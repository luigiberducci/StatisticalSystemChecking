Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.165s, episode steps: 100, steps per second: 608, episode reward: 194.233, mean reward: 1.942 [1.441, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.560, 10.217], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 194.797, mean reward: 1.948 [1.449, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.775, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 188.298, mean reward: 1.883 [1.493, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.005, 10.398], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 204.052, mean reward: 2.041 [1.526, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.199, 10.453], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 192.017, mean reward: 1.920 [1.499, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.655, 10.160], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.066s, episode steps: 100, steps per second: 1504, episode reward: 209.479, mean reward: 2.095 [1.476, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.059, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 193.894, mean reward: 1.939 [1.464, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.166, 10.272], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 185.705, mean reward: 1.857 [1.500, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.304, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 184.333, mean reward: 1.843 [1.465, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.536, 10.128], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 213.973, mean reward: 2.140 [1.484, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.876, 10.333], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 199.255, mean reward: 1.993 [1.434, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.294], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 180.791, mean reward: 1.808 [1.438, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.094, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 191.495, mean reward: 1.915 [1.473, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.236, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 193.996, mean reward: 1.940 [1.481, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.850, 10.193], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 185.937, mean reward: 1.859 [1.447, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.206, 10.099], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 202.419, mean reward: 2.024 [1.444, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.205, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 184.318, mean reward: 1.843 [1.467, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.291, 10.212], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.066s, episode steps: 100, steps per second: 1513, episode reward: 183.540, mean reward: 1.835 [1.476, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.304, 10.102], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 201.574, mean reward: 2.016 [1.465, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.271, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 187.309, mean reward: 1.873 [1.456, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.254], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 183.259, mean reward: 1.833 [1.439, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.650, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1575, episode reward: 197.276, mean reward: 1.973 [1.440, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.552, 10.249], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 204.153, mean reward: 2.042 [1.469, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.837, 10.188], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 208.011, mean reward: 2.080 [1.535, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.668, 10.240], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 185.310, mean reward: 1.853 [1.452, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 191.934, mean reward: 1.919 [1.476, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.462, 10.264], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.066s, episode steps: 100, steps per second: 1516, episode reward: 189.963, mean reward: 1.900 [1.472, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.914, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 189.394, mean reward: 1.894 [1.451, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.528, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 187.249, mean reward: 1.872 [1.480, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.479, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.065s, episode steps: 100, steps per second: 1538, episode reward: 202.727, mean reward: 2.027 [1.452, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.962, 10.390], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 186.376, mean reward: 1.864 [1.464, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.427, 10.173], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 192.881, mean reward: 1.929 [1.512, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.579, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 186.386, mean reward: 1.864 [1.450, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.108], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 189.632, mean reward: 1.896 [1.435, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.634, 10.142], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 194.922, mean reward: 1.949 [1.472, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 174.802, mean reward: 1.748 [1.456, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.010, 10.131], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 208.559, mean reward: 2.086 [1.500, 6.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.070s, episode steps: 100, steps per second: 1437, episode reward: 189.544, mean reward: 1.895 [1.509, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.188, 10.370], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.063s, episode steps: 100, steps per second: 1585, episode reward: 191.167, mean reward: 1.912 [1.437, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.619, 10.174], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.063s, episode steps: 100, steps per second: 1585, episode reward: 196.596, mean reward: 1.966 [1.462, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.542, 10.099], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 199.112, mean reward: 1.991 [1.443, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.407], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 180.059, mean reward: 1.801 [1.469, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.099, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 191.919, mean reward: 1.919 [1.432, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.870, 10.252], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 186.475, mean reward: 1.865 [1.456, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.962, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 182.864, mean reward: 1.829 [1.468, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.521, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 207.177, mean reward: 2.072 [1.472, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.749, 10.437], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 186.456, mean reward: 1.865 [1.511, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.842, 10.126], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.063s, episode steps: 100, steps per second: 1581, episode reward: 233.189, mean reward: 2.332 [1.454, 6.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.210, 10.500], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 184.134, mean reward: 1.841 [1.506, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.702, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 178.283, mean reward: 1.783 [1.459, 2.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.956, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 204.407, mean reward: 2.044 [1.468, 5.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.372, 10.098], loss: 0.166729, mae: 0.384033, mean_q: 2.845890
  5200/100000: episode: 52, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.449, mean reward: 1.924 [1.461, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.566, 10.129], loss: 0.116978, mae: 0.322415, mean_q: 3.228267
  5300/100000: episode: 53, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.570, mean reward: 1.876 [1.455, 6.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.003, 10.262], loss: 0.114259, mae: 0.316287, mean_q: 3.448520
  5400/100000: episode: 54, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.399, mean reward: 1.884 [1.444, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.636, 10.098], loss: 0.108391, mae: 0.308537, mean_q: 3.586990
  5500/100000: episode: 55, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 208.409, mean reward: 2.084 [1.467, 5.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.124, 10.262], loss: 0.126882, mae: 0.335466, mean_q: 3.681842
  5600/100000: episode: 56, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 202.629, mean reward: 2.026 [1.437, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.575, 10.098], loss: 0.094981, mae: 0.297018, mean_q: 3.731357
  5700/100000: episode: 57, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.427, mean reward: 1.944 [1.437, 7.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.063, 10.244], loss: 0.106858, mae: 0.316901, mean_q: 3.771574
  5800/100000: episode: 58, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 296.525, mean reward: 2.965 [1.450, 6.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.488], loss: 0.129137, mae: 0.332626, mean_q: 3.796135
  5900/100000: episode: 59, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 197.257, mean reward: 1.973 [1.482, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.060, 10.126], loss: 0.129433, mae: 0.320635, mean_q: 3.810134
  6000/100000: episode: 60, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.547, mean reward: 1.855 [1.473, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.495, 10.119], loss: 0.135149, mae: 0.340176, mean_q: 3.841081
  6100/100000: episode: 61, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.262, mean reward: 1.853 [1.469, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.775, 10.191], loss: 0.134269, mae: 0.327392, mean_q: 3.835038
  6200/100000: episode: 62, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 205.154, mean reward: 2.052 [1.456, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.163, 10.525], loss: 0.143226, mae: 0.341542, mean_q: 3.837652
  6300/100000: episode: 63, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 230.522, mean reward: 2.305 [1.565, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.639, 10.098], loss: 0.116244, mae: 0.310510, mean_q: 3.821319
  6400/100000: episode: 64, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.673, mean reward: 1.927 [1.454, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.205, 10.098], loss: 0.148566, mae: 0.352732, mean_q: 3.853560
  6500/100000: episode: 65, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.303, mean reward: 1.923 [1.459, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.557, 10.219], loss: 0.128169, mae: 0.337163, mean_q: 3.860008
  6600/100000: episode: 66, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.705, mean reward: 1.917 [1.472, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.519, 10.098], loss: 0.144430, mae: 0.340367, mean_q: 3.860281
  6700/100000: episode: 67, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.325, mean reward: 1.933 [1.456, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.978, 10.310], loss: 0.154032, mae: 0.354212, mean_q: 3.883356
  6800/100000: episode: 68, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 181.218, mean reward: 1.812 [1.477, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.461, 10.207], loss: 0.135374, mae: 0.341918, mean_q: 3.883939
  6900/100000: episode: 69, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.019, mean reward: 1.980 [1.445, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.380, 10.363], loss: 0.162321, mae: 0.348929, mean_q: 3.861670
  7000/100000: episode: 70, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 208.848, mean reward: 2.088 [1.455, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.255], loss: 0.149296, mae: 0.338565, mean_q: 3.877818
  7100/100000: episode: 71, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 184.504, mean reward: 1.845 [1.474, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.956, 10.212], loss: 0.136841, mae: 0.338382, mean_q: 3.887146
  7200/100000: episode: 72, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 218.767, mean reward: 2.188 [1.453, 6.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.404, 10.379], loss: 0.150357, mae: 0.348814, mean_q: 3.884624
  7300/100000: episode: 73, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.075, mean reward: 1.871 [1.450, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.103], loss: 0.143425, mae: 0.349934, mean_q: 3.901161
  7400/100000: episode: 74, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 198.239, mean reward: 1.982 [1.445, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.439, 10.230], loss: 0.142902, mae: 0.338982, mean_q: 3.868088
  7500/100000: episode: 75, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 206.545, mean reward: 2.065 [1.505, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.660, 10.098], loss: 0.139660, mae: 0.342169, mean_q: 3.890134
  7600/100000: episode: 76, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.403, mean reward: 1.934 [1.470, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.071, 10.280], loss: 0.138990, mae: 0.339701, mean_q: 3.862251
  7700/100000: episode: 77, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 206.259, mean reward: 2.063 [1.483, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.848, 10.404], loss: 0.163725, mae: 0.367103, mean_q: 3.909467
  7800/100000: episode: 78, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 201.983, mean reward: 2.020 [1.450, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.225], loss: 0.151108, mae: 0.360254, mean_q: 3.913811
  7900/100000: episode: 79, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.696, mean reward: 1.827 [1.435, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.098], loss: 0.166697, mae: 0.361639, mean_q: 3.912844
  8000/100000: episode: 80, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 188.918, mean reward: 1.889 [1.481, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.418, 10.098], loss: 0.121311, mae: 0.336737, mean_q: 3.908110
  8100/100000: episode: 81, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.814, mean reward: 1.848 [1.434, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.317, 10.098], loss: 0.155409, mae: 0.364958, mean_q: 3.916258
  8200/100000: episode: 82, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 211.250, mean reward: 2.113 [1.467, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.000, 10.211], loss: 0.143580, mae: 0.331476, mean_q: 3.908825
  8300/100000: episode: 83, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.541, mean reward: 1.935 [1.463, 5.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.201, 10.328], loss: 0.146985, mae: 0.355044, mean_q: 3.912498
  8400/100000: episode: 84, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.578, mean reward: 1.836 [1.483, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.146, 10.147], loss: 0.142978, mae: 0.351119, mean_q: 3.920877
  8500/100000: episode: 85, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: 198.547, mean reward: 1.985 [1.481, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.124, 10.098], loss: 0.136915, mae: 0.336983, mean_q: 3.904815
  8600/100000: episode: 86, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 182.553, mean reward: 1.826 [1.450, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.190, 10.098], loss: 0.133587, mae: 0.339464, mean_q: 3.902456
  8700/100000: episode: 87, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 174.230, mean reward: 1.742 [1.478, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.274, 10.210], loss: 0.127572, mae: 0.338114, mean_q: 3.888736
  8800/100000: episode: 88, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.071, mean reward: 1.891 [1.490, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.098], loss: 0.132708, mae: 0.333177, mean_q: 3.884618
  8900/100000: episode: 89, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 201.168, mean reward: 2.012 [1.450, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.856, 10.504], loss: 0.130795, mae: 0.344258, mean_q: 3.892781
  9000/100000: episode: 90, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.649, mean reward: 1.986 [1.478, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.141, 10.154], loss: 0.125706, mae: 0.338898, mean_q: 3.885910
  9100/100000: episode: 91, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 202.041, mean reward: 2.020 [1.486, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.245, 10.250], loss: 0.130094, mae: 0.340007, mean_q: 3.889917
  9200/100000: episode: 92, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.391, mean reward: 1.984 [1.508, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.306, 10.098], loss: 0.121638, mae: 0.337377, mean_q: 3.888097
  9300/100000: episode: 93, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.726, mean reward: 1.877 [1.443, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.639, 10.098], loss: 0.146248, mae: 0.351254, mean_q: 3.895507
  9400/100000: episode: 94, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.220, mean reward: 1.932 [1.458, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.616, 10.098], loss: 0.147951, mae: 0.359728, mean_q: 3.916065
  9500/100000: episode: 95, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.085, mean reward: 1.901 [1.468, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.461, 10.219], loss: 0.131285, mae: 0.345523, mean_q: 3.910111
  9600/100000: episode: 96, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.217, mean reward: 1.882 [1.460, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.236, 10.098], loss: 0.124534, mae: 0.332845, mean_q: 3.904800
  9700/100000: episode: 97, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 218.286, mean reward: 2.183 [1.449, 5.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.687, 10.133], loss: 0.129596, mae: 0.350718, mean_q: 3.900961
  9800/100000: episode: 98, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 203.000, mean reward: 2.030 [1.473, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.540, 10.296], loss: 0.135842, mae: 0.344338, mean_q: 3.896716
  9900/100000: episode: 99, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.436, mean reward: 1.834 [1.448, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.414, 10.098], loss: 0.133315, mae: 0.344931, mean_q: 3.915506
 10000/100000: episode: 100, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 204.234, mean reward: 2.042 [1.454, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.109], loss: 0.146264, mae: 0.352399, mean_q: 3.931565
 10100/100000: episode: 101, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 197.364, mean reward: 1.974 [1.492, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.467, 10.189], loss: 0.135453, mae: 0.349822, mean_q: 3.928176
 10200/100000: episode: 102, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 182.170, mean reward: 1.822 [1.461, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.537, 10.098], loss: 0.121283, mae: 0.329266, mean_q: 3.906991
 10300/100000: episode: 103, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.592, mean reward: 1.996 [1.524, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.331, 10.296], loss: 0.123724, mae: 0.329272, mean_q: 3.906877
 10400/100000: episode: 104, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.517, mean reward: 1.815 [1.472, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.098], loss: 0.128790, mae: 0.337973, mean_q: 3.890431
 10500/100000: episode: 105, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 204.198, mean reward: 2.042 [1.457, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.539, 10.165], loss: 0.142911, mae: 0.349483, mean_q: 3.913674
 10600/100000: episode: 106, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.991, mean reward: 1.880 [1.440, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.099], loss: 0.121234, mae: 0.330806, mean_q: 3.912102
 10700/100000: episode: 107, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.634, mean reward: 1.896 [1.471, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.098], loss: 0.134243, mae: 0.343175, mean_q: 3.900414
 10800/100000: episode: 108, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 187.589, mean reward: 1.876 [1.435, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.338, 10.122], loss: 0.107051, mae: 0.315036, mean_q: 3.886101
 10900/100000: episode: 109, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.475, mean reward: 1.965 [1.464, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.379, 10.141], loss: 0.102539, mae: 0.316369, mean_q: 3.865982
 11000/100000: episode: 110, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.726, mean reward: 1.937 [1.474, 5.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.437, 10.109], loss: 0.105579, mae: 0.316553, mean_q: 3.873709
 11100/100000: episode: 111, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 201.170, mean reward: 2.012 [1.475, 3.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.326, 10.220], loss: 0.109586, mae: 0.322831, mean_q: 3.882474
 11200/100000: episode: 112, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 188.744, mean reward: 1.887 [1.437, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.470, 10.098], loss: 0.108788, mae: 0.310760, mean_q: 3.862052
 11300/100000: episode: 113, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.852, mean reward: 1.959 [1.457, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.098], loss: 0.094964, mae: 0.301135, mean_q: 3.854874
 11400/100000: episode: 114, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 252.078, mean reward: 2.521 [1.450, 15.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.180, 10.681], loss: 0.127937, mae: 0.311371, mean_q: 3.867482
 11500/100000: episode: 115, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.137, mean reward: 1.981 [1.436, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.086, 10.098], loss: 0.116421, mae: 0.315001, mean_q: 3.854173
 11600/100000: episode: 116, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.591, mean reward: 1.856 [1.455, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.836, 10.098], loss: 0.161483, mae: 0.326890, mean_q: 3.861682
 11700/100000: episode: 117, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.337, mean reward: 1.853 [1.439, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.654, 10.098], loss: 0.182064, mae: 0.326947, mean_q: 3.884434
 11800/100000: episode: 118, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 225.744, mean reward: 2.257 [1.483, 5.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.295, 10.423], loss: 0.138258, mae: 0.331190, mean_q: 3.867036
 11900/100000: episode: 119, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 211.564, mean reward: 2.116 [1.514, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.036, 10.295], loss: 0.183203, mae: 0.336791, mean_q: 3.895538
 12000/100000: episode: 120, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 205.603, mean reward: 2.056 [1.439, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.452, 10.316], loss: 0.129012, mae: 0.327247, mean_q: 3.872853
 12100/100000: episode: 121, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.343, mean reward: 1.853 [1.441, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.149, 10.162], loss: 0.087134, mae: 0.296492, mean_q: 3.853133
 12200/100000: episode: 122, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 219.534, mean reward: 2.195 [1.482, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.967, 10.098], loss: 0.147207, mae: 0.325590, mean_q: 3.847089
 12300/100000: episode: 123, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 211.576, mean reward: 2.116 [1.457, 5.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.053, 10.098], loss: 0.175695, mae: 0.335607, mean_q: 3.886674
 12400/100000: episode: 124, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 182.706, mean reward: 1.827 [1.483, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.240, 10.098], loss: 0.204378, mae: 0.350355, mean_q: 3.898310
 12500/100000: episode: 125, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.151, mean reward: 1.992 [1.517, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.341, 10.261], loss: 0.124045, mae: 0.328560, mean_q: 3.876415
 12600/100000: episode: 126, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.562, mean reward: 1.866 [1.454, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.298], loss: 0.170597, mae: 0.339210, mean_q: 3.911597
 12700/100000: episode: 127, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.223, mean reward: 1.852 [1.473, 2.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.453, 10.098], loss: 0.100858, mae: 0.315647, mean_q: 3.889845
 12800/100000: episode: 128, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 183.496, mean reward: 1.835 [1.460, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.178], loss: 0.146236, mae: 0.328019, mean_q: 3.891638
 12900/100000: episode: 129, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 184.075, mean reward: 1.841 [1.458, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.194, 10.131], loss: 0.111225, mae: 0.315111, mean_q: 3.869727
 13000/100000: episode: 130, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 200.688, mean reward: 2.007 [1.456, 5.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.158, 10.422], loss: 0.145673, mae: 0.333658, mean_q: 3.888603
 13100/100000: episode: 131, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 195.824, mean reward: 1.958 [1.469, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.860, 10.172], loss: 0.152052, mae: 0.325520, mean_q: 3.896452
 13200/100000: episode: 132, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 189.845, mean reward: 1.898 [1.448, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.200, 10.116], loss: 0.119072, mae: 0.318239, mean_q: 3.886107
 13300/100000: episode: 133, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 209.406, mean reward: 2.094 [1.466, 7.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.098], loss: 0.170621, mae: 0.331016, mean_q: 3.900404
 13400/100000: episode: 134, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.682, mean reward: 1.977 [1.485, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.701, 10.098], loss: 0.159005, mae: 0.343158, mean_q: 3.903574
 13500/100000: episode: 135, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 188.340, mean reward: 1.883 [1.474, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.660, 10.098], loss: 0.128713, mae: 0.318925, mean_q: 3.881860
 13600/100000: episode: 136, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.606, mean reward: 1.896 [1.456, 5.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.806, 10.098], loss: 0.179867, mae: 0.330435, mean_q: 3.880231
 13700/100000: episode: 137, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 195.832, mean reward: 1.958 [1.446, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.098], loss: 0.203942, mae: 0.339723, mean_q: 3.898811
 13800/100000: episode: 138, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 177.314, mean reward: 1.773 [1.466, 2.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.502, 10.184], loss: 0.120884, mae: 0.326132, mean_q: 3.899161
 13900/100000: episode: 139, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.552, mean reward: 1.906 [1.513, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.572, 10.169], loss: 0.159941, mae: 0.330017, mean_q: 3.895435
 14000/100000: episode: 140, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 199.703, mean reward: 1.997 [1.462, 9.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.098], loss: 0.172118, mae: 0.331171, mean_q: 3.885453
 14100/100000: episode: 141, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.110, mean reward: 1.851 [1.473, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.776, 10.098], loss: 0.149235, mae: 0.340133, mean_q: 3.877944
 14200/100000: episode: 142, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 219.933, mean reward: 2.199 [1.464, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.249, 10.098], loss: 0.117157, mae: 0.316847, mean_q: 3.862844
 14300/100000: episode: 143, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.091, mean reward: 1.841 [1.435, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.835, 10.098], loss: 0.136619, mae: 0.326428, mean_q: 3.885077
 14400/100000: episode: 144, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 214.205, mean reward: 2.142 [1.444, 5.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.720, 10.098], loss: 0.131287, mae: 0.324461, mean_q: 3.869498
 14500/100000: episode: 145, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 201.163, mean reward: 2.012 [1.500, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.353, 10.098], loss: 0.161996, mae: 0.338902, mean_q: 3.895420
 14600/100000: episode: 146, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.859, mean reward: 1.889 [1.438, 6.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.409, 10.098], loss: 0.176019, mae: 0.342934, mean_q: 3.905205
 14700/100000: episode: 147, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 191.347, mean reward: 1.913 [1.452, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.304], loss: 0.173643, mae: 0.345151, mean_q: 3.914662
 14800/100000: episode: 148, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 186.812, mean reward: 1.868 [1.468, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.034, 10.328], loss: 0.125027, mae: 0.313261, mean_q: 3.875526
 14900/100000: episode: 149, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 184.291, mean reward: 1.843 [1.445, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.664, 10.154], loss: 0.143566, mae: 0.327763, mean_q: 3.879997
[Info] 1-TH LEVEL FOUND: 4.4794487953186035, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.117s, episode steps: 100, steps per second: 20, episode reward: 206.550, mean reward: 2.066 [1.526, 10.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.876, 10.098], loss: 0.129850, mae: 0.313563, mean_q: 3.868565
 15004/100000: episode: 151, duration: 0.037s, episode steps: 4, steps per second: 109, episode reward: 9.249, mean reward: 2.312 [2.135, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.340], loss: 0.086956, mae: 0.282197, mean_q: 3.843073
 15014/100000: episode: 152, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 26.951, mean reward: 2.695 [2.246, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.266, 10.418], loss: 0.132542, mae: 0.350007, mean_q: 3.901049
 15022/100000: episode: 153, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 31.092, mean reward: 3.886 [2.414, 6.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.633, 10.543], loss: 0.224659, mae: 0.337163, mean_q: 3.873929
 15026/100000: episode: 154, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 10.871, mean reward: 2.718 [2.035, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.364, 10.213], loss: 0.369827, mae: 0.405081, mean_q: 3.962980
 15053/100000: episode: 155, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 60.037, mean reward: 2.224 [1.699, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.722, 10.285], loss: 0.128854, mae: 0.341843, mean_q: 3.848103
 15066/100000: episode: 156, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 48.913, mean reward: 3.763 [2.586, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.585], loss: 0.087095, mae: 0.311987, mean_q: 3.889596
 15074/100000: episode: 157, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 31.022, mean reward: 3.878 [3.260, 4.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.354, 10.532], loss: 0.121131, mae: 0.341829, mean_q: 3.929943
 15087/100000: episode: 158, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 25.917, mean reward: 1.994 [1.694, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.329], loss: 0.091061, mae: 0.299101, mean_q: 3.807347
 15092/100000: episode: 159, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 14.126, mean reward: 2.825 [1.984, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.055, 10.202], loss: 0.249186, mae: 0.360385, mean_q: 3.972960
 15105/100000: episode: 160, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 37.092, mean reward: 2.853 [2.440, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.435], loss: 0.183520, mae: 0.372123, mean_q: 3.932615
 15108/100000: episode: 161, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 7.702, mean reward: 2.567 [2.175, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.427], loss: 0.137309, mae: 0.333450, mean_q: 3.781312
 15121/100000: episode: 162, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 26.113, mean reward: 2.009 [1.584, 2.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.359], loss: 0.144577, mae: 0.359512, mean_q: 3.881899
 15124/100000: episode: 163, duration: 0.018s, episode steps: 3, steps per second: 171, episode reward: 16.571, mean reward: 5.524 [4.222, 7.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.646], loss: 0.100964, mae: 0.324809, mean_q: 3.848600
 15134/100000: episode: 164, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 32.424, mean reward: 3.242 [2.880, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.523], loss: 0.156559, mae: 0.347962, mean_q: 3.957246
 15147/100000: episode: 165, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 27.492, mean reward: 2.115 [1.776, 2.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.147, 10.337], loss: 0.343711, mae: 0.375507, mean_q: 3.921580
 15160/100000: episode: 166, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 25.667, mean reward: 1.974 [1.664, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.302], loss: 0.222888, mae: 0.386485, mean_q: 3.935096
 15163/100000: episode: 167, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 14.240, mean reward: 4.747 [3.468, 5.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.604], loss: 0.196754, mae: 0.402891, mean_q: 3.944916
 15176/100000: episode: 168, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 40.686, mean reward: 3.130 [2.451, 3.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.421], loss: 0.136636, mae: 0.336282, mean_q: 3.874523
 15179/100000: episode: 169, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 21.642, mean reward: 7.214 [4.319, 11.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.545], loss: 0.120463, mae: 0.353601, mean_q: 3.875355
 15184/100000: episode: 170, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 13.395, mean reward: 2.679 [2.396, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.434], loss: 0.180480, mae: 0.344687, mean_q: 3.864994
 15192/100000: episode: 171, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 22.238, mean reward: 2.780 [2.301, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.427], loss: 0.127485, mae: 0.356213, mean_q: 3.937267
 15202/100000: episode: 172, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 29.396, mean reward: 2.940 [2.153, 5.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.381], loss: 0.151817, mae: 0.349905, mean_q: 3.885149
 15206/100000: episode: 173, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 15.022, mean reward: 3.755 [2.247, 5.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.743, 10.219], loss: 0.236112, mae: 0.361494, mean_q: 3.866456
 15219/100000: episode: 174, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 41.342, mean reward: 3.180 [2.781, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.949, 10.549], loss: 0.133651, mae: 0.334771, mean_q: 3.885272
 15232/100000: episode: 175, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 26.337, mean reward: 2.026 [1.696, 2.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.736, 10.306], loss: 0.214916, mae: 0.399489, mean_q: 3.951896
 15245/100000: episode: 176, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 32.416, mean reward: 2.494 [2.044, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.405, 10.380], loss: 0.163992, mae: 0.362831, mean_q: 3.925756
 15258/100000: episode: 177, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 25.862, mean reward: 1.989 [1.726, 2.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.325, 10.327], loss: 0.134074, mae: 0.344163, mean_q: 3.874815
 15261/100000: episode: 178, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 21.666, mean reward: 7.222 [4.109, 11.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.677], loss: 0.120611, mae: 0.341769, mean_q: 3.852895
 15274/100000: episode: 179, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 34.730, mean reward: 2.672 [2.259, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.982, 10.438], loss: 0.105986, mae: 0.327754, mean_q: 3.894851
 15287/100000: episode: 180, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 31.993, mean reward: 2.461 [2.048, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.127, 10.347], loss: 0.251594, mae: 0.408021, mean_q: 3.956227
 15300/100000: episode: 181, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 25.851, mean reward: 1.989 [1.692, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.270, 10.265], loss: 0.156862, mae: 0.352798, mean_q: 3.901663
 15303/100000: episode: 182, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 17.179, mean reward: 5.726 [4.616, 7.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.557], loss: 0.156216, mae: 0.403510, mean_q: 4.069713
 15316/100000: episode: 183, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 25.169, mean reward: 1.936 [1.677, 2.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.107, 10.210], loss: 0.233492, mae: 0.363555, mean_q: 3.929781
 15329/100000: episode: 184, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 45.493, mean reward: 3.499 [2.128, 9.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.403], loss: 0.255981, mae: 0.374341, mean_q: 3.878604
 15333/100000: episode: 185, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 8.573, mean reward: 2.143 [2.100, 2.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.276], loss: 0.322478, mae: 0.373196, mean_q: 3.960590
 15336/100000: episode: 186, duration: 0.024s, episode steps: 3, steps per second: 127, episode reward: 20.692, mean reward: 6.897 [5.272, 9.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.610], loss: 0.183319, mae: 0.380461, mean_q: 4.045758
 15339/100000: episode: 187, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 27.359, mean reward: 9.120 [7.172, 12.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.720], loss: 0.138858, mae: 0.366278, mean_q: 3.955070
 15366/100000: episode: 188, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 61.000, mean reward: 2.259 [1.547, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.264, 10.173], loss: 0.113875, mae: 0.325207, mean_q: 3.893556
 15379/100000: episode: 189, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 27.309, mean reward: 2.101 [1.760, 2.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.598, 10.289], loss: 0.330982, mae: 0.462533, mean_q: 4.106872
 15389/100000: episode: 190, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 22.680, mean reward: 2.268 [1.769, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.273], loss: 0.248649, mae: 0.363068, mean_q: 3.874128
 15392/100000: episode: 191, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 20.397, mean reward: 6.799 [5.648, 8.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.608], loss: 0.262907, mae: 0.458009, mean_q: 4.005469
 15395/100000: episode: 192, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 21.091, mean reward: 7.030 [5.886, 7.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.649], loss: 0.437338, mae: 0.493780, mean_q: 4.091784
 15422/100000: episode: 193, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 52.009, mean reward: 1.926 [1.496, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.494, 10.254], loss: 0.232211, mae: 0.393846, mean_q: 3.988690
 15449/100000: episode: 194, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 59.260, mean reward: 2.195 [1.488, 6.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.100], loss: 0.125671, mae: 0.343820, mean_q: 3.949072
 15453/100000: episode: 195, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 12.787, mean reward: 3.197 [2.423, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.827, 10.373], loss: 0.776321, mae: 0.547018, mean_q: 4.096300
 15480/100000: episode: 196, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 54.849, mean reward: 2.031 [1.575, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.172], loss: 0.321408, mae: 0.471739, mean_q: 4.029760
 15485/100000: episode: 197, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 11.082, mean reward: 2.216 [1.952, 2.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.365], loss: 0.140591, mae: 0.357347, mean_q: 3.923575
 15488/100000: episode: 198, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 5.598, mean reward: 1.866 [1.714, 1.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.188], loss: 0.140101, mae: 0.345572, mean_q: 3.788972
 15501/100000: episode: 199, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 29.607, mean reward: 2.277 [1.943, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.382], loss: 0.262493, mae: 0.417852, mean_q: 4.020214
 15509/100000: episode: 200, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 28.213, mean reward: 3.527 [3.041, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.666, 10.503], loss: 0.135435, mae: 0.345133, mean_q: 3.999868
 15512/100000: episode: 201, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 28.794, mean reward: 9.598 [4.793, 17.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.496], loss: 0.160864, mae: 0.369814, mean_q: 3.960851
 15525/100000: episode: 202, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 26.315, mean reward: 2.024 [1.862, 2.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-1.274, 10.332], loss: 0.318163, mae: 0.458264, mean_q: 4.052567
 15528/100000: episode: 203, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 6.939, mean reward: 2.313 [2.006, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.424], loss: 0.154475, mae: 0.345792, mean_q: 3.796463
 15531/100000: episode: 204, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 23.010, mean reward: 7.670 [6.981, 8.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.591], loss: 0.115120, mae: 0.337955, mean_q: 3.929971
 15544/100000: episode: 205, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 36.927, mean reward: 2.841 [2.471, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.366], loss: 0.387190, mae: 0.451225, mean_q: 4.030390
 15547/100000: episode: 206, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 6.515, mean reward: 2.172 [2.058, 2.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.318], loss: 0.137304, mae: 0.360293, mean_q: 3.878633
 15551/100000: episode: 207, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 7.848, mean reward: 1.962 [1.777, 2.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.297], loss: 0.151623, mae: 0.377451, mean_q: 4.073394
 15554/100000: episode: 208, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 6.723, mean reward: 2.241 [2.136, 2.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.325], loss: 0.289823, mae: 0.397581, mean_q: 3.993428
 15581/100000: episode: 209, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 56.847, mean reward: 2.105 [1.616, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.186, 10.166], loss: 0.205541, mae: 0.362576, mean_q: 3.964785
 15594/100000: episode: 210, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 24.723, mean reward: 1.902 [1.452, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.245, 10.171], loss: 0.823403, mae: 0.468558, mean_q: 3.990279
 15604/100000: episode: 211, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 28.330, mean reward: 2.833 [2.329, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.470, 10.402], loss: 0.348409, mae: 0.512896, mean_q: 4.178974
 15617/100000: episode: 212, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 47.100, mean reward: 3.623 [3.052, 5.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.564], loss: 0.210099, mae: 0.407205, mean_q: 4.015522
 15627/100000: episode: 213, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 25.661, mean reward: 2.566 [2.036, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.363, 10.439], loss: 0.234192, mae: 0.375388, mean_q: 3.883931
 15640/100000: episode: 214, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 32.743, mean reward: 2.519 [2.026, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.036, 10.419], loss: 0.255039, mae: 0.466294, mean_q: 4.125854
 15648/100000: episode: 215, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 27.745, mean reward: 3.468 [2.941, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.410, 10.520], loss: 0.367312, mae: 0.490215, mean_q: 3.995335
 15658/100000: episode: 216, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 26.636, mean reward: 2.664 [2.402, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.109, 10.448], loss: 0.828622, mae: 0.562474, mean_q: 4.235819
 15666/100000: episode: 217, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 23.548, mean reward: 2.944 [2.363, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.651, 10.450], loss: 0.187863, mae: 0.369003, mean_q: 3.943884
 15669/100000: episode: 218, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 8.561, mean reward: 2.854 [2.281, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.496], loss: 0.166400, mae: 0.384215, mean_q: 3.944062
 15696/100000: episode: 219, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 50.087, mean reward: 1.855 [1.473, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.724, 10.100], loss: 0.269811, mae: 0.397292, mean_q: 4.065499
 15699/100000: episode: 220, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 16.451, mean reward: 5.484 [4.867, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.588], loss: 0.143676, mae: 0.415850, mean_q: 4.195392
 15712/100000: episode: 221, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 28.704, mean reward: 2.208 [1.880, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.341], loss: 0.177896, mae: 0.368909, mean_q: 4.018939
 15725/100000: episode: 222, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 28.198, mean reward: 2.169 [1.789, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.405], loss: 0.344628, mae: 0.465163, mean_q: 4.153290
 15729/100000: episode: 223, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 10.635, mean reward: 2.659 [2.086, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.334], loss: 0.236150, mae: 0.442220, mean_q: 4.019091
 15737/100000: episode: 224, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 39.277, mean reward: 4.910 [3.592, 10.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.589], loss: 0.156991, mae: 0.355819, mean_q: 3.953643
 15740/100000: episode: 225, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 15.578, mean reward: 5.193 [3.583, 7.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.634], loss: 0.132438, mae: 0.374561, mean_q: 4.040475
 15743/100000: episode: 226, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 7.398, mean reward: 2.466 [2.385, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.200, 10.372], loss: 0.154763, mae: 0.397142, mean_q: 4.324383
 15747/100000: episode: 227, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 8.149, mean reward: 2.037 [1.934, 2.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.239], loss: 0.208935, mae: 0.441928, mean_q: 4.214462
 15760/100000: episode: 228, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 26.639, mean reward: 2.049 [1.763, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.275], loss: 0.298999, mae: 0.426152, mean_q: 3.915876
 15770/100000: episode: 229, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 33.501, mean reward: 3.350 [2.798, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.481], loss: 0.282992, mae: 0.457834, mean_q: 4.147616
 15773/100000: episode: 230, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 6.753, mean reward: 2.251 [1.896, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.399], loss: 0.163652, mae: 0.346907, mean_q: 3.819341
 15778/100000: episode: 231, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 12.898, mean reward: 2.580 [2.126, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.323], loss: 0.755008, mae: 0.439490, mean_q: 4.049229
 15788/100000: episode: 232, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 28.350, mean reward: 2.835 [2.144, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.319], loss: 0.531812, mae: 0.531269, mean_q: 4.111888
 15793/100000: episode: 233, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 10.981, mean reward: 2.196 [1.967, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.297], loss: 0.137283, mae: 0.383657, mean_q: 3.986356
 15796/100000: episode: 234, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 6.934, mean reward: 2.311 [2.221, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.373], loss: 0.117433, mae: 0.384559, mean_q: 4.117798
 15799/100000: episode: 235, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 18.006, mean reward: 6.002 [4.626, 8.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.572], loss: 0.208697, mae: 0.456034, mean_q: 4.111392
 15803/100000: episode: 236, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 11.434, mean reward: 2.859 [2.491, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.412], loss: 0.185373, mae: 0.369540, mean_q: 3.985744
 15807/100000: episode: 237, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 9.411, mean reward: 2.353 [2.060, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.172], loss: 0.159234, mae: 0.333365, mean_q: 3.939607
 15817/100000: episode: 238, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 26.693, mean reward: 2.669 [2.273, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.416], loss: 0.160017, mae: 0.411934, mean_q: 4.200605
 15830/100000: episode: 239, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 43.334, mean reward: 3.333 [2.359, 6.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.349], loss: 0.414935, mae: 0.472324, mean_q: 4.049850
[Info] 2-TH LEVEL FOUND: 5.409200668334961, Considering 10/90 traces
 15843/100000: episode: 240, duration: 4.313s, episode steps: 13, steps per second: 3, episode reward: 26.436, mean reward: 2.034 [1.725, 2.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.353], loss: 0.234644, mae: 0.395644, mean_q: 3.898695
 15848/100000: episode: 241, duration: 0.033s, episode steps: 5, steps per second: 149, episode reward: 16.321, mean reward: 3.264 [2.585, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.530], loss: 0.200194, mae: 0.428828, mean_q: 4.215726
 15852/100000: episode: 242, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 16.113, mean reward: 4.028 [2.935, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.771, 10.423], loss: 0.270748, mae: 0.475937, mean_q: 4.111907
 15857/100000: episode: 243, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 18.670, mean reward: 3.734 [3.011, 4.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.389], loss: 0.468356, mae: 0.484626, mean_q: 4.038277
 15862/100000: episode: 244, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 20.936, mean reward: 4.187 [2.855, 5.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.600], loss: 0.317444, mae: 0.486739, mean_q: 4.202784
 15866/100000: episode: 245, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 10.835, mean reward: 2.709 [2.628, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.436], loss: 0.119817, mae: 0.356610, mean_q: 4.163322
 15870/100000: episode: 246, duration: 0.028s, episode steps: 4, steps per second: 140, episode reward: 12.592, mean reward: 3.148 [2.847, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.324], loss: 0.314412, mae: 0.444260, mean_q: 3.890903
 15875/100000: episode: 247, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 16.326, mean reward: 3.265 [2.535, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.509], loss: 0.092731, mae: 0.326556, mean_q: 3.921424
 15880/100000: episode: 248, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 20.787, mean reward: 4.157 [3.229, 6.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.604], loss: 0.336815, mae: 0.463200, mean_q: 4.132567
 15884/100000: episode: 249, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 11.707, mean reward: 2.927 [2.685, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.433], loss: 0.281877, mae: 0.440594, mean_q: 4.168197
 15889/100000: episode: 250, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 12.395, mean reward: 2.479 [2.155, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.437], loss: 0.189475, mae: 0.408174, mean_q: 4.077737
 15892/100000: episode: 251, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.919, mean reward: 2.973 [2.869, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.441], loss: 0.432618, mae: 0.420478, mean_q: 4.046274
 15897/100000: episode: 252, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 14.888, mean reward: 2.978 [2.267, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.473], loss: 0.346954, mae: 0.428199, mean_q: 4.009080
 15901/100000: episode: 253, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 12.623, mean reward: 3.156 [2.691, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.348, 10.505], loss: 0.151174, mae: 0.386714, mean_q: 4.022937
 15906/100000: episode: 254, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 15.588, mean reward: 3.118 [2.766, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.431], loss: 0.156232, mae: 0.417322, mean_q: 4.092048
 15909/100000: episode: 255, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 11.638, mean reward: 3.879 [3.648, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.492], loss: 0.233524, mae: 0.410919, mean_q: 3.872011
 15914/100000: episode: 256, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 13.788, mean reward: 2.758 [2.139, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.412], loss: 0.261953, mae: 0.416721, mean_q: 4.090587
 15918/100000: episode: 257, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 13.558, mean reward: 3.390 [3.016, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.426], loss: 0.231198, mae: 0.461215, mean_q: 4.227248
 15922/100000: episode: 258, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 11.712, mean reward: 2.928 [2.621, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.485], loss: 0.225088, mae: 0.420368, mean_q: 4.041340
 15925/100000: episode: 259, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.876, mean reward: 2.959 [2.651, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.401], loss: 0.224785, mae: 0.376643, mean_q: 4.044755
 15930/100000: episode: 260, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 14.258, mean reward: 2.852 [2.530, 3.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.490], loss: 0.265526, mae: 0.416134, mean_q: 4.028643
 15935/100000: episode: 261, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 14.431, mean reward: 2.886 [2.701, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.458], loss: 0.488247, mae: 0.489287, mean_q: 4.213356
 15940/100000: episode: 262, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 13.503, mean reward: 2.701 [1.964, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.450], loss: 0.470809, mae: 0.583875, mean_q: 4.209164
 15945/100000: episode: 263, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 27.720, mean reward: 5.544 [4.104, 6.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.235, 10.495], loss: 0.505739, mae: 0.459062, mean_q: 4.043127
 15950/100000: episode: 264, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 17.086, mean reward: 3.417 [2.878, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.524], loss: 0.285704, mae: 0.442593, mean_q: 4.132656
 15955/100000: episode: 265, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 15.027, mean reward: 3.005 [2.755, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.092, 10.367], loss: 0.178285, mae: 0.457451, mean_q: 4.203382
 15960/100000: episode: 266, duration: 0.028s, episode steps: 5, steps per second: 182, episode reward: 16.022, mean reward: 3.204 [2.556, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.434], loss: 0.296046, mae: 0.390459, mean_q: 4.035147
 15963/100000: episode: 267, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 8.325, mean reward: 2.775 [2.449, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.494], loss: 0.142257, mae: 0.348935, mean_q: 3.978309
 15967/100000: episode: 268, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 14.078, mean reward: 3.520 [3.119, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.527, 10.530], loss: 0.204186, mae: 0.395548, mean_q: 4.059999
 15972/100000: episode: 269, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 13.591, mean reward: 2.718 [2.509, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.412], loss: 0.122184, mae: 0.377869, mean_q: 4.154585
 15975/100000: episode: 270, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 13.505, mean reward: 4.502 [3.547, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.235, 10.428], loss: 0.140725, mae: 0.377388, mean_q: 4.151989
 15979/100000: episode: 271, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 13.567, mean reward: 3.392 [2.973, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.405], loss: 0.157728, mae: 0.375191, mean_q: 3.939233
 15984/100000: episode: 272, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 18.394, mean reward: 3.679 [3.120, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.386], loss: 0.176389, mae: 0.358887, mean_q: 3.963165
 15988/100000: episode: 273, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 13.598, mean reward: 3.399 [3.024, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.468], loss: 0.968685, mae: 0.604128, mean_q: 4.301539
 15992/100000: episode: 274, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 11.759, mean reward: 2.940 [2.679, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.467], loss: 0.245060, mae: 0.501077, mean_q: 4.282776
 15996/100000: episode: 275, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 11.813, mean reward: 2.953 [2.794, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.453], loss: 0.268006, mae: 0.444739, mean_q: 4.098204
 15999/100000: episode: 276, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 12.290, mean reward: 4.097 [3.646, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.035, 10.569], loss: 0.150733, mae: 0.384328, mean_q: 3.992442
 16004/100000: episode: 277, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 16.777, mean reward: 3.355 [2.805, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.530], loss: 0.187975, mae: 0.409011, mean_q: 4.044549
 16008/100000: episode: 278, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 10.388, mean reward: 2.597 [2.371, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.415], loss: 0.502532, mae: 0.522000, mean_q: 4.110039
 16013/100000: episode: 279, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 24.984, mean reward: 4.997 [4.299, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.612], loss: 0.148222, mae: 0.424664, mean_q: 4.185764
 16017/100000: episode: 280, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 12.067, mean reward: 3.017 [2.684, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.505], loss: 0.573973, mae: 0.453551, mean_q: 3.998670
 16021/100000: episode: 281, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 10.278, mean reward: 2.570 [2.525, 2.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.402], loss: 0.162047, mae: 0.377413, mean_q: 4.003685
 16024/100000: episode: 282, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 9.063, mean reward: 3.021 [2.798, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.480], loss: 0.261034, mae: 0.458122, mean_q: 4.100441
 16029/100000: episode: 283, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 13.195, mean reward: 2.639 [2.427, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.273, 10.349], loss: 0.192276, mae: 0.358663, mean_q: 4.037763
 16034/100000: episode: 284, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 15.166, mean reward: 3.033 [2.566, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.475], loss: 0.185055, mae: 0.431960, mean_q: 4.037492
 16038/100000: episode: 285, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 10.844, mean reward: 2.711 [2.151, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.340], loss: 0.852096, mae: 0.574388, mean_q: 4.126757
 16042/100000: episode: 286, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 10.702, mean reward: 2.675 [2.285, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.144, 10.408], loss: 0.167939, mae: 0.458773, mean_q: 4.292295
 16047/100000: episode: 287, duration: 0.029s, episode steps: 5, steps per second: 169, episode reward: 24.914, mean reward: 4.983 [3.931, 6.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.506], loss: 0.450012, mae: 0.416766, mean_q: 3.966968
 16052/100000: episode: 288, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 23.190, mean reward: 4.638 [3.297, 6.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.611], loss: 0.133369, mae: 0.359666, mean_q: 4.102066
 16056/100000: episode: 289, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 12.795, mean reward: 3.199 [2.683, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.494], loss: 0.377416, mae: 0.518247, mean_q: 4.136717
 16061/100000: episode: 290, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 14.548, mean reward: 2.910 [2.662, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.414], loss: 0.342226, mae: 0.446206, mean_q: 4.020504
 16066/100000: episode: 291, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 15.567, mean reward: 3.113 [2.537, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.524], loss: 0.192884, mae: 0.442352, mean_q: 4.180332
 16070/100000: episode: 292, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 12.081, mean reward: 3.020 [2.778, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.448], loss: 0.159845, mae: 0.410297, mean_q: 4.244305
 16074/100000: episode: 293, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 11.428, mean reward: 2.857 [2.551, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.114, 10.380], loss: 0.242922, mae: 0.479301, mean_q: 4.119017
 16079/100000: episode: 294, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 14.810, mean reward: 2.962 [2.635, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.411, 10.466], loss: 0.267311, mae: 0.430143, mean_q: 4.063164
 16083/100000: episode: 295, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 14.203, mean reward: 3.551 [3.220, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.549], loss: 0.217222, mae: 0.462321, mean_q: 4.137358
 16088/100000: episode: 296, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 16.356, mean reward: 3.271 [3.004, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.417], loss: 0.272791, mae: 0.457247, mean_q: 4.179033
 16092/100000: episode: 297, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 15.184, mean reward: 3.796 [3.393, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.554], loss: 0.512840, mae: 0.555945, mean_q: 4.165641
 16096/100000: episode: 298, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 11.495, mean reward: 2.874 [2.523, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.499, 10.511], loss: 0.223763, mae: 0.476729, mean_q: 4.137203
 16101/100000: episode: 299, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 18.321, mean reward: 3.664 [3.060, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.470, 10.568], loss: 0.352944, mae: 0.468482, mean_q: 4.199163
 16106/100000: episode: 300, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 20.504, mean reward: 4.101 [3.785, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.549], loss: 0.146563, mae: 0.358767, mean_q: 4.034036
 16110/100000: episode: 301, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 12.141, mean reward: 3.035 [2.763, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.463], loss: 0.234003, mae: 0.450661, mean_q: 4.148065
 16115/100000: episode: 302, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 17.308, mean reward: 3.462 [3.090, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.462], loss: 0.520966, mae: 0.553594, mean_q: 4.277780
 16120/100000: episode: 303, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 14.487, mean reward: 2.897 [2.585, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.450], loss: 0.219103, mae: 0.454278, mean_q: 4.122355
 16123/100000: episode: 304, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 8.698, mean reward: 2.899 [2.798, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.443], loss: 0.190925, mae: 0.400776, mean_q: 3.958966
 16127/100000: episode: 305, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 10.420, mean reward: 2.605 [2.006, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.235], loss: 1.063340, mae: 0.526754, mean_q: 4.123820
 16132/100000: episode: 306, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 16.193, mean reward: 3.239 [2.826, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.498], loss: 0.142105, mae: 0.409640, mean_q: 4.212904
 16135/100000: episode: 307, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 9.505, mean reward: 3.168 [3.004, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.375], loss: 0.177707, mae: 0.398612, mean_q: 4.013859
 16140/100000: episode: 308, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 19.109, mean reward: 3.822 [2.746, 5.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.580], loss: 0.281232, mae: 0.404605, mean_q: 3.996322
 16144/100000: episode: 309, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 14.023, mean reward: 3.506 [3.096, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.481], loss: 0.526087, mae: 0.530911, mean_q: 4.309836
 16149/100000: episode: 310, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 14.535, mean reward: 2.907 [2.760, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.456], loss: 0.155513, mae: 0.389565, mean_q: 4.225689
 16152/100000: episode: 311, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 15.442, mean reward: 5.147 [4.790, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.396], loss: 0.270366, mae: 0.411638, mean_q: 3.990130
 16155/100000: episode: 312, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 9.588, mean reward: 3.196 [3.095, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.454], loss: 0.157988, mae: 0.397543, mean_q: 3.997318
 16160/100000: episode: 313, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 18.316, mean reward: 3.663 [3.407, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.506], loss: 0.804895, mae: 0.525881, mean_q: 4.318405
 16163/100000: episode: 314, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 10.610, mean reward: 3.537 [3.197, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.451], loss: 0.135216, mae: 0.365627, mean_q: 4.085463
 16167/100000: episode: 315, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 11.838, mean reward: 2.960 [2.751, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.478], loss: 0.238178, mae: 0.483130, mean_q: 4.062974
 16172/100000: episode: 316, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 18.276, mean reward: 3.655 [2.557, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.493], loss: 0.199540, mae: 0.434402, mean_q: 4.053773
 16176/100000: episode: 317, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 14.544, mean reward: 3.636 [2.867, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.539], loss: 0.848798, mae: 0.670498, mean_q: 4.120420
 16181/100000: episode: 318, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 29.742, mean reward: 5.948 [4.544, 7.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.578], loss: 0.255841, mae: 0.498993, mean_q: 4.404353
 16186/100000: episode: 319, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 16.599, mean reward: 3.320 [3.071, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.164, 10.445], loss: 0.162598, mae: 0.410489, mean_q: 4.056534
 16189/100000: episode: 320, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 10.279, mean reward: 3.426 [2.744, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.336, 10.444], loss: 0.108261, mae: 0.351564, mean_q: 4.049868
 16193/100000: episode: 321, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 14.614, mean reward: 3.654 [3.128, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.511], loss: 0.493481, mae: 0.597420, mean_q: 4.187594
 16197/100000: episode: 322, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 13.887, mean reward: 3.472 [3.319, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.502], loss: 0.240069, mae: 0.484855, mean_q: 4.335335
 16201/100000: episode: 323, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 11.373, mean reward: 2.843 [2.532, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.461], loss: 0.360811, mae: 0.535712, mean_q: 4.124543
 16206/100000: episode: 324, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 13.282, mean reward: 2.656 [2.545, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.418], loss: 0.172063, mae: 0.396164, mean_q: 4.036893
 16209/100000: episode: 325, duration: 0.027s, episode steps: 3, steps per second: 110, episode reward: 9.965, mean reward: 3.322 [2.995, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.515], loss: 0.116428, mae: 0.380473, mean_q: 4.206677
 16214/100000: episode: 326, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 16.138, mean reward: 3.228 [2.843, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.480], loss: 0.206939, mae: 0.473239, mean_q: 4.163936
 16218/100000: episode: 327, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 11.339, mean reward: 2.835 [2.691, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.465], loss: 0.767343, mae: 0.521217, mean_q: 4.183441
 16223/100000: episode: 328, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 15.502, mean reward: 3.100 [2.559, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.446], loss: 0.208196, mae: 0.458142, mean_q: 4.197401
 16228/100000: episode: 329, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 20.386, mean reward: 4.077 [3.891, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.556], loss: 0.217200, mae: 0.439772, mean_q: 4.044165
[Info] 3-TH LEVEL FOUND: 5.810241222381592, Considering 13/87 traces
 16233/100000: episode: 330, duration: 4.266s, episode steps: 5, steps per second: 1, episode reward: 30.165, mean reward: 6.033 [5.620, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.601], loss: 0.263650, mae: 0.443204, mean_q: 4.106271
 16236/100000: episode: 331, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 7.565, mean reward: 2.522 [2.397, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.044, 10.347], loss: 0.248598, mae: 0.522742, mean_q: 4.310334
 16239/100000: episode: 332, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 11.276, mean reward: 3.759 [3.462, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.534], loss: 0.266727, mae: 0.489858, mean_q: 4.393646
 16243/100000: episode: 333, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 12.622, mean reward: 3.155 [2.651, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.059, 10.443], loss: 0.132648, mae: 0.381966, mean_q: 4.141070
 16246/100000: episode: 334, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 11.123, mean reward: 3.708 [3.138, 4.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.399], loss: 0.245181, mae: 0.420510, mean_q: 4.172492
 16250/100000: episode: 335, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 12.244, mean reward: 3.061 [2.657, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.377], loss: 0.764203, mae: 0.670855, mean_q: 4.328260
 16254/100000: episode: 336, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 14.073, mean reward: 3.518 [3.257, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.495], loss: 0.314882, mae: 0.576230, mean_q: 4.403189
 16258/100000: episode: 337, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 9.718, mean reward: 2.430 [2.068, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.318], loss: 0.289557, mae: 0.491579, mean_q: 4.384649
 16261/100000: episode: 338, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 12.888, mean reward: 4.296 [3.799, 4.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.465], loss: 0.174832, mae: 0.374409, mean_q: 3.776690
 16265/100000: episode: 339, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 11.010, mean reward: 2.752 [2.490, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.556, 10.385], loss: 0.223957, mae: 0.333659, mean_q: 3.926521
 16268/100000: episode: 340, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 12.492, mean reward: 4.164 [3.617, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.860, 10.318], loss: 0.269870, mae: 0.440958, mean_q: 4.283834
 16272/100000: episode: 341, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 12.142, mean reward: 3.035 [2.873, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.310], loss: 0.279641, mae: 0.559325, mean_q: 4.348552
 16275/100000: episode: 342, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 8.420, mean reward: 2.807 [2.544, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.412], loss: 0.107917, mae: 0.358028, mean_q: 4.180017
 16278/100000: episode: 343, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 15.413, mean reward: 5.138 [4.671, 5.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.136, 10.566], loss: 0.319778, mae: 0.496265, mean_q: 3.849026
 16280/100000: episode: 344, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 8.695, mean reward: 4.348 [4.001, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.575], loss: 0.135937, mae: 0.368226, mean_q: 3.924179
 16284/100000: episode: 345, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 12.009, mean reward: 3.002 [2.626, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.488], loss: 0.489899, mae: 0.489651, mean_q: 4.124843
 16287/100000: episode: 346, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 9.965, mean reward: 3.322 [3.103, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.457], loss: 0.264854, mae: 0.477574, mean_q: 4.271781
 16291/100000: episode: 347, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 12.475, mean reward: 3.119 [2.957, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.459], loss: 0.228242, mae: 0.480265, mean_q: 4.276630
 16294/100000: episode: 348, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 10.307, mean reward: 3.436 [3.404, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.439], loss: 0.229426, mae: 0.449196, mean_q: 4.150445
 16297/100000: episode: 349, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 8.511, mean reward: 2.837 [2.354, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.303], loss: 0.270520, mae: 0.468728, mean_q: 4.227119
 16300/100000: episode: 350, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 14.039, mean reward: 4.680 [3.547, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.447], loss: 0.296778, mae: 0.497269, mean_q: 4.321495
 16303/100000: episode: 351, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 9.556, mean reward: 3.185 [2.956, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.473], loss: 0.210716, mae: 0.449063, mean_q: 4.317253
 16307/100000: episode: 352, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 11.987, mean reward: 2.997 [2.595, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.365], loss: 0.455573, mae: 0.488573, mean_q: 4.298510
 16311/100000: episode: 353, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 10.175, mean reward: 2.544 [2.480, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.409], loss: 0.385280, mae: 0.495795, mean_q: 4.151749
 16314/100000: episode: 354, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 12.737, mean reward: 4.246 [4.050, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.478], loss: 0.376153, mae: 0.518723, mean_q: 4.313845
 16317/100000: episode: 355, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 10.572, mean reward: 3.524 [3.354, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.457], loss: 0.216098, mae: 0.450695, mean_q: 4.363460
 16321/100000: episode: 356, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.594, mean reward: 2.898 [2.409, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.414], loss: 0.185980, mae: 0.401621, mean_q: 4.130411
 16323/100000: episode: 357, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 6.670, mean reward: 3.335 [3.116, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.491], loss: 0.253704, mae: 0.495996, mean_q: 4.195112
 16327/100000: episode: 358, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 11.352, mean reward: 2.838 [2.436, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.377], loss: 0.380646, mae: 0.536946, mean_q: 4.233311
 16331/100000: episode: 359, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 10.706, mean reward: 2.677 [2.509, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.385], loss: 0.278440, mae: 0.517643, mean_q: 4.304147
 16335/100000: episode: 360, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 10.287, mean reward: 2.572 [2.230, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.286], loss: 0.175907, mae: 0.363310, mean_q: 3.917378
 16339/100000: episode: 361, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 12.267, mean reward: 3.067 [2.513, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.285], loss: 0.223188, mae: 0.436947, mean_q: 4.051361
 16342/100000: episode: 362, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 14.339, mean reward: 4.780 [3.866, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.528], loss: 0.162739, mae: 0.410826, mean_q: 4.243025
 16346/100000: episode: 363, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 11.001, mean reward: 2.750 [2.391, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.450], loss: 0.524062, mae: 0.538441, mean_q: 4.201037
 16350/100000: episode: 364, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 12.356, mean reward: 3.089 [2.922, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.463], loss: 0.218479, mae: 0.474751, mean_q: 4.178663
 16354/100000: episode: 365, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 10.774, mean reward: 2.694 [2.306, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.881, 10.355], loss: 0.205113, mae: 0.476302, mean_q: 4.350756
 16358/100000: episode: 366, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 10.319, mean reward: 2.580 [2.296, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.432], loss: 0.246052, mae: 0.474922, mean_q: 4.171218
 16362/100000: episode: 367, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 12.454, mean reward: 3.114 [2.901, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.504], loss: 0.260600, mae: 0.469515, mean_q: 4.077826
 16364/100000: episode: 368, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 8.631, mean reward: 4.315 [3.873, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.274, 10.450], loss: 0.329706, mae: 0.463576, mean_q: 4.202776
 16366/100000: episode: 369, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 9.049, mean reward: 4.525 [4.030, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.493], loss: 0.523418, mae: 0.660180, mean_q: 4.470928
 16370/100000: episode: 370, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 12.921, mean reward: 3.230 [2.739, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.426], loss: 0.393944, mae: 0.509610, mean_q: 4.242491
 16373/100000: episode: 371, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 20.992, mean reward: 6.997 [6.508, 7.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.637], loss: 0.221074, mae: 0.461514, mean_q: 3.929371
 16375/100000: episode: 372, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 13.491, mean reward: 6.746 [5.272, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-1.167, 10.467], loss: 0.142037, mae: 0.371069, mean_q: 3.918490
 16378/100000: episode: 373, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 13.771, mean reward: 4.590 [4.332, 4.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.490], loss: 0.222085, mae: 0.475257, mean_q: 4.248384
 16381/100000: episode: 374, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 19.702, mean reward: 6.567 [5.969, 7.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.637], loss: 0.658569, mae: 0.537893, mean_q: 4.175856
 16384/100000: episode: 375, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 8.194, mean reward: 2.731 [2.660, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.342], loss: 0.137164, mae: 0.369167, mean_q: 4.215977
 16388/100000: episode: 376, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 11.083, mean reward: 2.771 [2.601, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-1.065, 10.428], loss: 0.279970, mae: 0.495171, mean_q: 4.131135
 16391/100000: episode: 377, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 20.498, mean reward: 6.833 [6.641, 7.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.599], loss: 1.221171, mae: 0.601728, mean_q: 4.316940
 16395/100000: episode: 378, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 13.288, mean reward: 3.322 [2.924, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.473], loss: 0.268986, mae: 0.517419, mean_q: 4.288297
 16399/100000: episode: 379, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 10.575, mean reward: 2.644 [2.472, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.388], loss: 0.392510, mae: 0.569433, mean_q: 4.390072
 16402/100000: episode: 380, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 7.626, mean reward: 2.542 [2.384, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.406], loss: 0.218055, mae: 0.453728, mean_q: 4.314076
 16406/100000: episode: 381, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 10.951, mean reward: 2.738 [2.668, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.428], loss: 0.294964, mae: 0.453979, mean_q: 4.094991
 16410/100000: episode: 382, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 12.999, mean reward: 3.250 [3.086, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.511], loss: 0.263284, mae: 0.468525, mean_q: 4.188003
 16412/100000: episode: 383, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 7.690, mean reward: 3.845 [3.410, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.555], loss: 0.154422, mae: 0.401155, mean_q: 4.209768
 16415/100000: episode: 384, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 15.020, mean reward: 5.007 [4.265, 5.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.449, 10.556], loss: 0.125388, mae: 0.371985, mean_q: 3.980746
 16417/100000: episode: 385, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 9.756, mean reward: 4.878 [4.056, 5.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.537], loss: 0.130684, mae: 0.358030, mean_q: 3.998222
 16421/100000: episode: 386, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 14.688, mean reward: 3.672 [3.159, 4.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.548], loss: 0.236493, mae: 0.472958, mean_q: 4.230709
 16425/100000: episode: 387, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 13.311, mean reward: 3.328 [2.855, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.505], loss: 0.324765, mae: 0.470005, mean_q: 4.234575
 16429/100000: episode: 388, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 11.045, mean reward: 2.761 [2.594, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.425], loss: 0.283280, mae: 0.531315, mean_q: 4.381209
 16431/100000: episode: 389, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 9.787, mean reward: 4.894 [4.285, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.543], loss: 0.511782, mae: 0.503388, mean_q: 4.175305
 16434/100000: episode: 390, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 22.766, mean reward: 7.589 [7.090, 8.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.573], loss: 1.376181, mae: 0.572869, mean_q: 4.170930
 16437/100000: episode: 391, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 10.313, mean reward: 3.438 [3.178, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.483], loss: 0.150638, mae: 0.379228, mean_q: 4.287665
 16439/100000: episode: 392, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 7.924, mean reward: 3.962 [3.611, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.035, 10.565], loss: 0.336781, mae: 0.504428, mean_q: 4.359384
 16442/100000: episode: 393, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 15.983, mean reward: 5.328 [4.584, 6.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.629], loss: 0.289726, mae: 0.523931, mean_q: 4.174209
 16445/100000: episode: 394, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 10.601, mean reward: 3.534 [3.350, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-1.200, 10.457], loss: 0.176136, mae: 0.407845, mean_q: 4.242929
 16449/100000: episode: 395, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 10.252, mean reward: 2.563 [2.285, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.363], loss: 0.190938, mae: 0.429197, mean_q: 4.247792
 16452/100000: episode: 396, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 20.024, mean reward: 6.675 [6.126, 7.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.628], loss: 0.413072, mae: 0.504497, mean_q: 4.232342
 16456/100000: episode: 397, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 16.073, mean reward: 4.018 [2.839, 5.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.583], loss: 0.294768, mae: 0.535490, mean_q: 4.307747
 16460/100000: episode: 398, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 16.976, mean reward: 4.244 [4.092, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.484], loss: 0.373687, mae: 0.529570, mean_q: 4.194240
 16462/100000: episode: 399, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 6.468, mean reward: 3.234 [3.047, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.427, 10.446], loss: 0.235002, mae: 0.487570, mean_q: 4.372747
 16466/100000: episode: 400, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 12.938, mean reward: 3.235 [3.144, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.450], loss: 0.995978, mae: 0.558910, mean_q: 4.334641
 16469/100000: episode: 401, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 10.170, mean reward: 3.390 [3.088, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.410], loss: 0.676744, mae: 0.620743, mean_q: 4.387156
 16472/100000: episode: 402, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 14.964, mean reward: 4.988 [4.827, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.576], loss: 0.459801, mae: 0.599132, mean_q: 4.353853
 16476/100000: episode: 403, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 10.396, mean reward: 2.599 [2.249, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.454], loss: 0.204093, mae: 0.407512, mean_q: 4.118005
 16480/100000: episode: 404, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 12.851, mean reward: 3.213 [2.650, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.073, 10.352], loss: 0.238194, mae: 0.439166, mean_q: 4.075686
 16484/100000: episode: 405, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 9.753, mean reward: 2.438 [2.061, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.380], loss: 0.256748, mae: 0.501709, mean_q: 4.463539
 16488/100000: episode: 406, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 13.011, mean reward: 3.253 [2.958, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.495], loss: 0.265016, mae: 0.432356, mean_q: 4.379682
 16492/100000: episode: 407, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 11.089, mean reward: 2.772 [2.641, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.391], loss: 0.327715, mae: 0.481237, mean_q: 4.223205
 16495/100000: episode: 408, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 9.283, mean reward: 3.094 [2.995, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.455], loss: 1.096914, mae: 0.709052, mean_q: 4.246511
 16498/100000: episode: 409, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 10.345, mean reward: 3.448 [2.982, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.381], loss: 0.436260, mae: 0.570922, mean_q: 4.458276
 16500/100000: episode: 410, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 6.353, mean reward: 3.176 [3.002, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.459], loss: 0.196093, mae: 0.438283, mean_q: 4.490377
 16503/100000: episode: 411, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 12.659, mean reward: 4.220 [4.030, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.557], loss: 0.506685, mae: 0.625485, mean_q: 4.322648
 16507/100000: episode: 412, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 12.338, mean reward: 3.084 [2.768, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.420], loss: 0.269872, mae: 0.504259, mean_q: 4.281336
 16511/100000: episode: 413, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 12.458, mean reward: 3.115 [2.652, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.406], loss: 0.332806, mae: 0.472501, mean_q: 4.065880
 16514/100000: episode: 414, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 11.421, mean reward: 3.807 [3.610, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.453], loss: 0.330190, mae: 0.533684, mean_q: 4.298120
 16517/100000: episode: 415, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 14.971, mean reward: 4.990 [4.512, 5.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.575], loss: 0.228575, mae: 0.496107, mean_q: 4.408927
 16519/100000: episode: 416, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 7.407, mean reward: 3.703 [3.475, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.375], loss: 0.370468, mae: 0.527778, mean_q: 4.304349
[Info] 4-TH LEVEL FOUND: 6.1006760597229, Considering 18/82 traces
 16522/100000: episode: 417, duration: 4.263s, episode steps: 3, steps per second: 1, episode reward: 9.540, mean reward: 3.180 [3.014, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.459], loss: 0.443422, mae: 0.543547, mean_q: 4.199242
 16524/100000: episode: 418, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 8.314, mean reward: 4.157 [4.137, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.450], loss: 0.711739, mae: 0.563536, mean_q: 4.177206
 16526/100000: episode: 419, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.042, mean reward: 5.021 [4.924, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.588], loss: 0.252424, mae: 0.499503, mean_q: 4.379152
 16528/100000: episode: 420, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 11.028, mean reward: 5.514 [5.146, 5.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.427], loss: 0.219438, mae: 0.458513, mean_q: 4.423861
 16530/100000: episode: 421, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 9.478, mean reward: 4.739 [4.294, 5.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.469], loss: 0.171278, mae: 0.415464, mean_q: 4.153212
 16532/100000: episode: 422, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 13.569, mean reward: 6.785 [6.508, 7.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.035, 10.656], loss: 0.722643, mae: 0.484229, mean_q: 4.066207
 16534/100000: episode: 423, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 25.708, mean reward: 12.854 [4.481, 21.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.245], loss: 0.397273, mae: 0.502854, mean_q: 3.959056
 16536/100000: episode: 424, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 9.153, mean reward: 4.576 [3.888, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.417], loss: 0.563310, mae: 0.624895, mean_q: 4.359392
 16538/100000: episode: 425, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 10.791, mean reward: 5.395 [5.319, 5.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.606], loss: 0.161824, mae: 0.427748, mean_q: 4.280750
 16540/100000: episode: 426, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 9.084, mean reward: 4.542 [4.465, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.553], loss: 0.306338, mae: 0.532655, mean_q: 4.406387
 16542/100000: episode: 427, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 13.790, mean reward: 6.895 [6.014, 7.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.667], loss: 0.163618, mae: 0.359550, mean_q: 4.005616
 16544/100000: episode: 428, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 9.250, mean reward: 4.625 [4.080, 5.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.529], loss: 0.203626, mae: 0.445510, mean_q: 4.225234
 16546/100000: episode: 429, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 12.098, mean reward: 6.049 [5.400, 6.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.591], loss: 0.985289, mae: 0.707238, mean_q: 4.557053
 16548/100000: episode: 430, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 7.729, mean reward: 3.864 [3.780, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.471], loss: 0.238443, mae: 0.516922, mean_q: 4.454630
 16550/100000: episode: 431, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 9.730, mean reward: 4.865 [4.839, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.555], loss: 0.480482, mae: 0.514621, mean_q: 4.271261
 16552/100000: episode: 432, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 12.164, mean reward: 6.082 [5.076, 7.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.434], loss: 0.590567, mae: 0.583617, mean_q: 4.215580
 16554/100000: episode: 433, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 10.970, mean reward: 5.485 [4.724, 6.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.035, 10.637], loss: 0.152267, mae: 0.431455, mean_q: 4.296227
 16556/100000: episode: 434, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 5.767, mean reward: 2.883 [2.819, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.419], loss: 2.450840, mae: 0.891606, mean_q: 4.625938
 16558/100000: episode: 435, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 11.533, mean reward: 5.766 [4.838, 6.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.420], loss: 1.797437, mae: 0.872868, mean_q: 4.716997
 16560/100000: episode: 436, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 18.144, mean reward: 9.072 [4.511, 13.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.427], loss: 0.576578, mae: 0.805161, mean_q: 5.001113
 16562/100000: episode: 437, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 7.865, mean reward: 3.932 [3.517, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.035, 10.422], loss: 0.344172, mae: 0.514249, mean_q: 4.268487
 16564/100000: episode: 438, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 8.718, mean reward: 4.359 [4.264, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.035, 10.566], loss: 0.285309, mae: 0.541583, mean_q: 3.894162
 16566/100000: episode: 439, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 8.156, mean reward: 4.078 [3.688, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.386], loss: 0.264434, mae: 0.523600, mean_q: 3.891339
 16568/100000: episode: 440, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 10.034, mean reward: 5.017 [4.877, 5.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.578], loss: 0.196273, mae: 0.417562, mean_q: 3.930810
 16570/100000: episode: 441, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 8.151, mean reward: 4.075 [3.952, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.428], loss: 0.387604, mae: 0.584218, mean_q: 4.463513
 16572/100000: episode: 442, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 6.521, mean reward: 3.261 [3.211, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.388], loss: 0.246577, mae: 0.517499, mean_q: 4.504230
 16574/100000: episode: 443, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 8.769, mean reward: 4.384 [4.152, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.559], loss: 0.327208, mae: 0.480365, mean_q: 4.301548
 16576/100000: episode: 444, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 6.901, mean reward: 3.450 [3.330, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.504], loss: 0.175082, mae: 0.393898, mean_q: 4.264608
 16578/100000: episode: 445, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 6.116, mean reward: 3.058 [2.803, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.425], loss: 0.301834, mae: 0.468908, mean_q: 4.248907
 16580/100000: episode: 446, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 6.565, mean reward: 3.283 [3.121, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.423], loss: 0.237026, mae: 0.503516, mean_q: 4.275091
 16582/100000: episode: 447, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 8.347, mean reward: 4.173 [3.771, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.516], loss: 0.178272, mae: 0.421637, mean_q: 3.994536
 16584/100000: episode: 448, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 26.462, mean reward: 13.231 [12.795, 13.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.455, 10.722], loss: 0.377501, mae: 0.546884, mean_q: 4.204664
 16586/100000: episode: 449, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 6.519, mean reward: 3.259 [3.243, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.446], loss: 0.197711, mae: 0.477459, mean_q: 4.409904
 16588/100000: episode: 450, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 11.726, mean reward: 5.863 [4.360, 7.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.415], loss: 0.267355, mae: 0.541232, mean_q: 4.529194
 16590/100000: episode: 451, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 7.606, mean reward: 3.803 [3.274, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.350], loss: 0.460687, mae: 0.650807, mean_q: 4.561374
 16592/100000: episode: 452, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 13.796, mean reward: 6.898 [6.713, 7.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.667, 10.643], loss: 0.324718, mae: 0.544935, mean_q: 4.216941
 16594/100000: episode: 453, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 17.750, mean reward: 8.875 [8.176, 9.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.670], loss: 0.328527, mae: 0.502746, mean_q: 4.362061
 16596/100000: episode: 454, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 8.857, mean reward: 4.429 [4.369, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.496], loss: 0.351703, mae: 0.510002, mean_q: 4.377474
 16598/100000: episode: 455, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 9.441, mean reward: 4.720 [4.485, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.563], loss: 0.270760, mae: 0.532942, mean_q: 4.315804
 16600/100000: episode: 456, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 12.681, mean reward: 6.340 [6.134, 6.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.638], loss: 0.429883, mae: 0.582342, mean_q: 4.588245
 16602/100000: episode: 457, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 9.138, mean reward: 4.569 [4.520, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.466], loss: 0.229901, mae: 0.405539, mean_q: 4.156898
 16604/100000: episode: 458, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 7.493, mean reward: 3.746 [3.627, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.480], loss: 0.243787, mae: 0.504993, mean_q: 4.163355
 16606/100000: episode: 459, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 6.567, mean reward: 3.284 [3.079, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.035, 10.510], loss: 0.262826, mae: 0.491759, mean_q: 4.327212
 16608/100000: episode: 460, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 9.241, mean reward: 4.621 [4.382, 4.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.035, 10.552], loss: 0.407387, mae: 0.550872, mean_q: 4.328886
 16610/100000: episode: 461, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.718, mean reward: 5.359 [5.333, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.537], loss: 0.334455, mae: 0.558880, mean_q: 4.461322
 16612/100000: episode: 462, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 6.236, mean reward: 3.118 [2.818, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.374], loss: 0.372727, mae: 0.481500, mean_q: 4.103638
 16614/100000: episode: 463, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 7.244, mean reward: 3.622 [3.320, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.437], loss: 0.930784, mae: 0.594404, mean_q: 4.160833
 16616/100000: episode: 464, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 7.603, mean reward: 3.802 [3.666, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.448], loss: 0.251994, mae: 0.474847, mean_q: 4.299311
 16618/100000: episode: 465, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 6.397, mean reward: 3.198 [3.067, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.424], loss: 0.210028, mae: 0.487514, mean_q: 4.354865
 16620/100000: episode: 466, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 6.361, mean reward: 3.180 [3.065, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.436], loss: 0.439565, mae: 0.556061, mean_q: 4.274529
 16622/100000: episode: 467, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 7.144, mean reward: 3.572 [3.125, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.387], loss: 0.349095, mae: 0.545015, mean_q: 4.469173
 16624/100000: episode: 468, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.206, mean reward: 5.103 [4.673, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.707, 10.587], loss: 0.223265, mae: 0.486587, mean_q: 4.414710
 16626/100000: episode: 469, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 6.728, mean reward: 3.364 [3.352, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.416], loss: 0.217501, mae: 0.531574, mean_q: 4.424881
 16628/100000: episode: 470, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 6.283, mean reward: 3.141 [3.009, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.035, 10.460], loss: 0.116669, mae: 0.355544, mean_q: 4.026795
 16630/100000: episode: 471, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 9.985, mean reward: 4.992 [4.914, 5.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.279, 10.582], loss: 0.277837, mae: 0.459533, mean_q: 3.935965
 16632/100000: episode: 472, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 9.709, mean reward: 4.855 [4.502, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.568], loss: 0.390977, mae: 0.488961, mean_q: 4.148377
 16634/100000: episode: 473, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 8.732, mean reward: 4.366 [3.819, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.315, 10.589], loss: 0.468884, mae: 0.633353, mean_q: 4.329283
 16636/100000: episode: 474, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 10.608, mean reward: 5.304 [4.797, 5.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.622], loss: 0.477226, mae: 0.657499, mean_q: 4.438476
 16638/100000: episode: 475, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 7.107, mean reward: 3.554 [3.527, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.471], loss: 0.348873, mae: 0.598579, mean_q: 4.447913
 16640/100000: episode: 476, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 8.514, mean reward: 4.257 [4.069, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.467], loss: 0.239674, mae: 0.486465, mean_q: 4.405944
 16642/100000: episode: 477, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 7.921, mean reward: 3.961 [3.465, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.484], loss: 0.244263, mae: 0.455530, mean_q: 4.030285
 16644/100000: episode: 478, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 6.115, mean reward: 3.057 [2.987, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.444], loss: 0.366681, mae: 0.577127, mean_q: 4.032793
 16646/100000: episode: 479, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 6.491, mean reward: 3.246 [2.955, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.377], loss: 1.751569, mae: 0.653386, mean_q: 4.161327
 16648/100000: episode: 480, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 12.287, mean reward: 6.143 [5.818, 6.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.598], loss: 0.126041, mae: 0.419273, mean_q: 4.315761
 16650/100000: episode: 481, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 6.627, mean reward: 3.313 [3.292, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.417], loss: 0.316694, mae: 0.609363, mean_q: 4.577795
 16652/100000: episode: 482, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 6.138, mean reward: 3.069 [3.047, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.470], loss: 0.396386, mae: 0.610294, mean_q: 4.147167
 16654/100000: episode: 483, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 7.425, mean reward: 3.713 [3.383, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.325], loss: 0.162461, mae: 0.398148, mean_q: 4.017506
 16656/100000: episode: 484, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 6.953, mean reward: 3.477 [3.177, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.530], loss: 0.185305, mae: 0.481394, mean_q: 4.329462
 16658/100000: episode: 485, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 6.207, mean reward: 3.104 [2.904, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.457], loss: 0.455256, mae: 0.646945, mean_q: 4.425542
 16660/100000: episode: 486, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 6.782, mean reward: 3.391 [3.287, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.463], loss: 0.522925, mae: 0.747489, mean_q: 4.592821
 16662/100000: episode: 487, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 6.442, mean reward: 3.221 [3.175, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.382], loss: 0.339125, mae: 0.559840, mean_q: 4.462656
 16664/100000: episode: 488, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 8.578, mean reward: 4.289 [4.268, 4.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.389], loss: 0.176801, mae: 0.387166, mean_q: 3.950913
 16666/100000: episode: 489, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 12.555, mean reward: 6.277 [4.878, 7.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.662], loss: 0.273387, mae: 0.499737, mean_q: 3.988019
 16668/100000: episode: 490, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 8.311, mean reward: 4.155 [3.466, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.367], loss: 0.331200, mae: 0.507330, mean_q: 4.399285
 16670/100000: episode: 491, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 7.383, mean reward: 3.691 [3.679, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.502], loss: 0.475573, mae: 0.642429, mean_q: 4.421715
 16672/100000: episode: 492, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 17.060, mean reward: 8.530 [7.815, 9.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.531], loss: 0.207408, mae: 0.430528, mean_q: 4.256809
 16674/100000: episode: 493, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 7.172, mean reward: 3.586 [3.430, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.513], loss: 0.135798, mae: 0.390279, mean_q: 4.086221
 16676/100000: episode: 494, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 10.369, mean reward: 5.185 [4.231, 6.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.437], loss: 0.147357, mae: 0.415374, mean_q: 3.938144
 16678/100000: episode: 495, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 13.475, mean reward: 6.738 [6.337, 7.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.608], loss: 0.922771, mae: 0.625216, mean_q: 4.122672
 16680/100000: episode: 496, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 8.387, mean reward: 4.193 [3.714, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.535], loss: 0.172674, mae: 0.447159, mean_q: 4.521727
 16682/100000: episode: 497, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 9.351, mean reward: 4.676 [3.613, 5.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.035, 10.517], loss: 0.267790, mae: 0.472422, mean_q: 4.361502
 16684/100000: episode: 498, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 11.222, mean reward: 5.611 [5.041, 6.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.348], loss: 0.312587, mae: 0.518433, mean_q: 4.409666
[Info] 5-TH LEVEL FOUND: 6.256218910217285, Considering 34/66 traces
 16686/100000: episode: 499, duration: 4.288s, episode steps: 2, steps per second: 0, episode reward: 7.954, mean reward: 3.977 [3.677, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.356], loss: 0.318029, mae: 0.442099, mean_q: 4.059057
 16688/100000: episode: 500, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 12.634, mean reward: 6.317 [6.036, 6.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.193, 10.628], loss: 0.724132, mae: 0.620022, mean_q: 4.170000
 16690/100000: episode: 501, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 9.384, mean reward: 4.692 [4.580, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.555], loss: 0.499833, mae: 0.732524, mean_q: 4.555936
 16692/100000: episode: 502, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 9.950, mean reward: 4.975 [4.659, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.524], loss: 0.353844, mae: 0.570067, mean_q: 4.591211
 16694/100000: episode: 503, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.704, mean reward: 5.352 [4.855, 5.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.604], loss: 0.214590, mae: 0.451453, mean_q: 4.347315
 16696/100000: episode: 504, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 9.705, mean reward: 4.853 [4.709, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.588], loss: 0.161516, mae: 0.423872, mean_q: 4.404486
 16698/100000: episode: 505, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.731, mean reward: 5.365 [4.912, 5.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.554], loss: 0.453936, mae: 0.635581, mean_q: 4.444026
 16700/100000: episode: 506, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 8.893, mean reward: 4.446 [4.085, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.202, 10.586], loss: 1.553401, mae: 0.740600, mean_q: 4.354273
 16702/100000: episode: 507, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 5.976, mean reward: 2.988 [2.749, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.355], loss: 0.401440, mae: 0.589963, mean_q: 4.431235
 16704/100000: episode: 508, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 9.057, mean reward: 4.528 [4.096, 4.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.401], loss: 0.951886, mae: 0.642738, mean_q: 4.405377
 16706/100000: episode: 509, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 12.815, mean reward: 6.407 [4.678, 8.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.089, 10.674], loss: 0.416604, mae: 0.558785, mean_q: 4.508375
 16708/100000: episode: 510, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 8.832, mean reward: 4.416 [3.897, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.577], loss: 0.244611, mae: 0.507391, mean_q: 4.575285
 16710/100000: episode: 511, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 9.417, mean reward: 4.708 [4.628, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.085, 10.426], loss: 0.365710, mae: 0.484291, mean_q: 4.280560
 16712/100000: episode: 512, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 7.077, mean reward: 3.539 [3.413, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.411], loss: 0.340038, mae: 0.524971, mean_q: 4.182608
 16714/100000: episode: 513, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 10.234, mean reward: 5.117 [4.897, 5.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.583], loss: 0.249648, mae: 0.438619, mean_q: 4.123235
 16716/100000: episode: 514, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 7.734, mean reward: 3.867 [3.692, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.531], loss: 0.213479, mae: 0.471234, mean_q: 4.499554
 16718/100000: episode: 515, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 8.393, mean reward: 4.196 [4.021, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.541], loss: 0.660808, mae: 0.719390, mean_q: 4.686954
 16720/100000: episode: 516, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 9.327, mean reward: 4.664 [4.285, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.595], loss: 0.643539, mae: 0.540913, mean_q: 4.506487
 16722/100000: episode: 517, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 7.855, mean reward: 3.928 [3.743, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.510], loss: 0.630373, mae: 0.645146, mean_q: 4.478310
 16724/100000: episode: 518, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 11.341, mean reward: 5.671 [4.580, 6.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.620], loss: 0.248248, mae: 0.492964, mean_q: 4.406028
 16726/100000: episode: 519, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 10.211, mean reward: 5.106 [5.089, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.531], loss: 0.398170, mae: 0.559714, mean_q: 4.334612
 16728/100000: episode: 520, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 12.220, mean reward: 6.110 [5.050, 7.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.035, 10.657], loss: 0.556821, mae: 0.633139, mean_q: 4.386649
 16730/100000: episode: 521, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 12.338, mean reward: 6.169 [5.680, 6.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.542], loss: 0.366655, mae: 0.617789, mean_q: 4.724098
 16732/100000: episode: 522, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 6.743, mean reward: 3.371 [2.995, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.395], loss: 0.571711, mae: 0.675896, mean_q: 4.600620
 16734/100000: episode: 523, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 11.533, mean reward: 5.767 [5.285, 6.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.638], loss: 0.306143, mae: 0.538920, mean_q: 4.537663
 16736/100000: episode: 524, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 10.105, mean reward: 5.053 [4.879, 5.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.585], loss: 0.305355, mae: 0.512890, mean_q: 4.073978
 16738/100000: episode: 525, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 7.046, mean reward: 3.523 [3.363, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.405], loss: 0.242852, mae: 0.476559, mean_q: 4.219496
 16740/100000: episode: 526, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 10.702, mean reward: 5.351 [5.104, 5.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.064, 10.574], loss: 0.259213, mae: 0.513453, mean_q: 4.661981
 16742/100000: episode: 527, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 9.299, mean reward: 4.649 [4.393, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.548], loss: 0.366946, mae: 0.554038, mean_q: 4.266469
 16744/100000: episode: 528, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 16.363, mean reward: 8.181 [3.457, 12.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.124, 10.407], loss: 0.241053, mae: 0.441891, mean_q: 4.209367
 16746/100000: episode: 529, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 6.434, mean reward: 3.217 [3.168, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.035, 10.459], loss: 0.259974, mae: 0.496007, mean_q: 4.277801
 16748/100000: episode: 530, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 11.006, mean reward: 5.503 [4.865, 6.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.571], loss: 0.757437, mae: 0.773123, mean_q: 4.325782
 16750/100000: episode: 531, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 9.256, mean reward: 4.628 [4.474, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.035, 10.578], loss: 0.286886, mae: 0.560498, mean_q: 4.622563
 16752/100000: episode: 532, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 9.537, mean reward: 4.769 [4.625, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.557], loss: 1.738516, mae: 0.922498, mean_q: 4.534146
 16754/100000: episode: 533, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 11.983, mean reward: 5.992 [5.730, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.634], loss: 0.707322, mae: 0.610827, mean_q: 4.422973
 16756/100000: episode: 534, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 14.480, mean reward: 7.240 [5.261, 9.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.358, 10.689], loss: 0.250574, mae: 0.515673, mean_q: 4.578718
 16758/100000: episode: 535, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 8.997, mean reward: 4.498 [3.508, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.035, 10.616], loss: 0.200642, mae: 0.396226, mean_q: 4.243202
 16760/100000: episode: 536, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 6.468, mean reward: 3.234 [3.171, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.382], loss: 0.483809, mae: 0.567398, mean_q: 4.182961
 16762/100000: episode: 537, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 32.897, mean reward: 16.449 [5.468, 27.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.390 [-0.035, 10.758], loss: 0.232620, mae: 0.473314, mean_q: 4.236691
 16764/100000: episode: 538, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 8.121, mean reward: 4.060 [3.797, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.149, 10.467], loss: 0.310895, mae: 0.491485, mean_q: 4.371894
 16766/100000: episode: 539, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 12.013, mean reward: 6.007 [5.798, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.607], loss: 0.752647, mae: 0.653356, mean_q: 4.564260
 16768/100000: episode: 540, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 8.615, mean reward: 4.308 [4.215, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.527], loss: 0.402510, mae: 0.594165, mean_q: 4.567919
 16770/100000: episode: 541, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 8.514, mean reward: 4.257 [4.156, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.532], loss: 0.548839, mae: 0.662493, mean_q: 4.702653
 16772/100000: episode: 542, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 10.848, mean reward: 5.424 [5.379, 5.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.596], loss: 0.520062, mae: 0.697561, mean_q: 4.609843
 16774/100000: episode: 543, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 14.131, mean reward: 7.065 [5.022, 9.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.676], loss: 0.212661, mae: 0.448195, mean_q: 4.281960
 16776/100000: episode: 544, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.652, mean reward: 5.326 [5.326, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.707, 10.605], loss: 0.237415, mae: 0.476791, mean_q: 4.305631
 16778/100000: episode: 545, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 13.396, mean reward: 6.698 [6.231, 7.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.593], loss: 0.329664, mae: 0.530380, mean_q: 4.226039
 16780/100000: episode: 546, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 7.421, mean reward: 3.710 [3.588, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.349], loss: 0.139831, mae: 0.363240, mean_q: 4.165726
 16782/100000: episode: 547, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 8.154, mean reward: 4.077 [3.911, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.870, 10.528], loss: 0.376814, mae: 0.563405, mean_q: 4.538218
 16784/100000: episode: 548, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 6.335, mean reward: 3.168 [3.021, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.378], loss: 0.335271, mae: 0.520933, mean_q: 4.127288
 16786/100000: episode: 549, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.746, mean reward: 5.373 [5.308, 5.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.601], loss: 0.865070, mae: 0.633368, mean_q: 4.227932
[Info] FALSIFICATION!
[Info] Levels: [4.479449, 5.4092007, 5.810241, 6.100676, 6.256219, 6.875524]
[Info] Cond. Prob: [0.1, 0.1, 0.13, 0.18, 0.34, 0.85]
[Info] Error Prob: 6.762600000000002e-05

 16788/100000: episode: 550, duration: 4.730s, episode steps: 2, steps per second: 0, episode reward: 106.864, mean reward: 53.432 [6.864, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.796], loss: 0.263962, mae: 0.490403, mean_q: 4.197912
 16888/100000: episode: 551, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 221.881, mean reward: 2.219 [1.476, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.112, 10.098], loss: 2.000937, mae: 0.639527, mean_q: 4.402977
 16988/100000: episode: 552, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 186.649, mean reward: 1.866 [1.458, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.118], loss: 1.923053, mae: 0.645525, mean_q: 4.421203
 17088/100000: episode: 553, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 205.691, mean reward: 2.057 [1.442, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.593, 10.375], loss: 2.158950, mae: 0.683451, mean_q: 4.420048
 17188/100000: episode: 554, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 198.606, mean reward: 1.986 [1.494, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.869, 10.098], loss: 0.701407, mae: 0.613793, mean_q: 4.431930
 17288/100000: episode: 555, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.278, mean reward: 1.963 [1.459, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.227], loss: 4.830732, mae: 0.792883, mean_q: 4.507727
 17388/100000: episode: 556, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 248.151, mean reward: 2.482 [1.438, 6.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.020, 10.637], loss: 0.451228, mae: 0.563100, mean_q: 4.376526
 17488/100000: episode: 557, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 188.814, mean reward: 1.888 [1.437, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.869, 10.270], loss: 3.338265, mae: 0.715706, mean_q: 4.453742
 17588/100000: episode: 558, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 198.645, mean reward: 1.986 [1.469, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.802, 10.098], loss: 4.838709, mae: 0.738522, mean_q: 4.554646
 17688/100000: episode: 559, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 230.565, mean reward: 2.306 [1.503, 7.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.164, 10.364], loss: 1.938894, mae: 0.643159, mean_q: 4.450822
 17788/100000: episode: 560, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 196.184, mean reward: 1.962 [1.438, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.809, 10.240], loss: 1.886189, mae: 0.626889, mean_q: 4.460668
 17888/100000: episode: 561, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 181.250, mean reward: 1.813 [1.448, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.521, 10.214], loss: 0.705374, mae: 0.614655, mean_q: 4.507000
 17988/100000: episode: 562, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.329, mean reward: 1.983 [1.505, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.967, 10.402], loss: 1.873885, mae: 0.620060, mean_q: 4.465611
 18088/100000: episode: 563, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 177.987, mean reward: 1.780 [1.440, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.826, 10.169], loss: 0.557762, mae: 0.577046, mean_q: 4.486189
 18188/100000: episode: 564, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 187.127, mean reward: 1.871 [1.462, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.993, 10.098], loss: 0.639351, mae: 0.592856, mean_q: 4.460393
 18288/100000: episode: 565, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.331, mean reward: 1.843 [1.435, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.496, 10.098], loss: 0.527122, mae: 0.555416, mean_q: 4.415812
 18388/100000: episode: 566, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 201.832, mean reward: 2.018 [1.441, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.871, 10.345], loss: 1.842846, mae: 0.619043, mean_q: 4.454375
 18488/100000: episode: 567, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 182.895, mean reward: 1.829 [1.446, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.008, 10.098], loss: 0.634163, mae: 0.565667, mean_q: 4.439936
 18588/100000: episode: 568, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 205.691, mean reward: 2.057 [1.482, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.098], loss: 1.961797, mae: 0.657776, mean_q: 4.509581
 18688/100000: episode: 569, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 211.812, mean reward: 2.118 [1.445, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.558, 10.399], loss: 4.583032, mae: 0.699493, mean_q: 4.496917
 18788/100000: episode: 570, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.602, mean reward: 1.916 [1.493, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.672, 10.339], loss: 1.957659, mae: 0.711766, mean_q: 4.498154
 18888/100000: episode: 571, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.241, mean reward: 1.972 [1.435, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.426, 10.098], loss: 3.455127, mae: 0.711972, mean_q: 4.498520
 18988/100000: episode: 572, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.279, mean reward: 1.873 [1.457, 2.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.438, 10.173], loss: 0.438915, mae: 0.561591, mean_q: 4.466365
 19088/100000: episode: 573, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 191.833, mean reward: 1.918 [1.438, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.837, 10.098], loss: 1.792408, mae: 0.601227, mean_q: 4.482580
 19188/100000: episode: 574, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: 190.422, mean reward: 1.904 [1.441, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.265, 10.230], loss: 1.978619, mae: 0.647692, mean_q: 4.463947
 19288/100000: episode: 575, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.359, mean reward: 1.934 [1.475, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.261, 10.098], loss: 2.127090, mae: 0.678255, mean_q: 4.540661
 19388/100000: episode: 576, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 190.296, mean reward: 1.903 [1.463, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.077, 10.098], loss: 1.899486, mae: 0.611565, mean_q: 4.428603
 19488/100000: episode: 577, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.309, mean reward: 1.903 [1.470, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.339, 10.251], loss: 4.541758, mae: 0.701077, mean_q: 4.543390
 19588/100000: episode: 578, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 196.908, mean reward: 1.969 [1.471, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.350, 10.180], loss: 0.594428, mae: 0.564623, mean_q: 4.459378
 19688/100000: episode: 579, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 208.635, mean reward: 2.086 [1.503, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.207], loss: 0.366668, mae: 0.521555, mean_q: 4.434561
 19788/100000: episode: 580, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 190.104, mean reward: 1.901 [1.461, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.632, 10.098], loss: 1.905710, mae: 0.612381, mean_q: 4.439555
 19888/100000: episode: 581, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 192.031, mean reward: 1.920 [1.505, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.489, 10.098], loss: 4.922793, mae: 0.756721, mean_q: 4.455138
 19988/100000: episode: 582, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 190.313, mean reward: 1.903 [1.454, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.126], loss: 0.394263, mae: 0.507179, mean_q: 4.336608
 20088/100000: episode: 583, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 184.980, mean reward: 1.850 [1.451, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.432, 10.159], loss: 3.197642, mae: 0.634493, mean_q: 4.465822
 20188/100000: episode: 584, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.069, mean reward: 1.901 [1.505, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.859, 10.098], loss: 4.479484, mae: 0.629231, mean_q: 4.409955
 20288/100000: episode: 585, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.803, mean reward: 1.938 [1.457, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.251], loss: 1.888100, mae: 0.545037, mean_q: 4.302912
 20388/100000: episode: 586, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 211.563, mean reward: 2.116 [1.458, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.495, 10.098], loss: 4.611091, mae: 0.732864, mean_q: 4.403693
 20488/100000: episode: 587, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 198.069, mean reward: 1.981 [1.437, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.587, 10.252], loss: 0.324468, mae: 0.479872, mean_q: 4.267157
 20588/100000: episode: 588, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 209.547, mean reward: 2.095 [1.459, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.563, 10.284], loss: 0.300881, mae: 0.472888, mean_q: 4.266218
 20688/100000: episode: 589, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 184.406, mean reward: 1.844 [1.447, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.769, 10.098], loss: 1.737811, mae: 0.512571, mean_q: 4.268129
 20788/100000: episode: 590, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 176.424, mean reward: 1.764 [1.446, 2.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.136, 10.098], loss: 0.344082, mae: 0.447342, mean_q: 4.207167
 20888/100000: episode: 591, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 228.861, mean reward: 2.289 [1.506, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.350, 10.278], loss: 0.360618, mae: 0.434731, mean_q: 4.157297
 20988/100000: episode: 592, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.152, mean reward: 1.992 [1.446, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.656, 10.098], loss: 1.712734, mae: 0.490538, mean_q: 4.173459
 21088/100000: episode: 593, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.524, mean reward: 1.935 [1.476, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.098], loss: 0.333858, mae: 0.421627, mean_q: 4.115339
 21188/100000: episode: 594, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 187.789, mean reward: 1.878 [1.439, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.369, 10.098], loss: 0.191849, mae: 0.385595, mean_q: 4.058603
 21288/100000: episode: 595, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 186.495, mean reward: 1.865 [1.451, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.476, 10.130], loss: 0.362079, mae: 0.377801, mean_q: 4.036503
 21388/100000: episode: 596, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 205.237, mean reward: 2.052 [1.481, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.320, 10.374], loss: 0.172968, mae: 0.356061, mean_q: 4.011136
 21488/100000: episode: 597, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 204.307, mean reward: 2.043 [1.459, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.219, 10.140], loss: 0.375542, mae: 0.379963, mean_q: 4.016037
 21588/100000: episode: 598, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 215.576, mean reward: 2.156 [1.460, 21.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.784, 10.295], loss: 4.327924, mae: 0.525081, mean_q: 4.057086
 21688/100000: episode: 599, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 177.903, mean reward: 1.779 [1.451, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.098], loss: 0.124335, mae: 0.328066, mean_q: 3.907183
 21788/100000: episode: 600, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: 183.919, mean reward: 1.839 [1.439, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.483, 10.139], loss: 1.502862, mae: 0.370486, mean_q: 3.907791
 21888/100000: episode: 601, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.279, mean reward: 1.853 [1.448, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.538, 10.098], loss: 0.100886, mae: 0.305004, mean_q: 3.879397
 21988/100000: episode: 602, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.688, mean reward: 1.917 [1.456, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.785, 10.098], loss: 0.158696, mae: 0.315894, mean_q: 3.891205
 22088/100000: episode: 603, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 195.254, mean reward: 1.953 [1.479, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.391, 10.098], loss: 0.216072, mae: 0.324666, mean_q: 3.880977
 22188/100000: episode: 604, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 225.258, mean reward: 2.253 [1.467, 5.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.920, 10.151], loss: 0.103474, mae: 0.314377, mean_q: 3.903795
 22288/100000: episode: 605, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 192.653, mean reward: 1.927 [1.446, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.894, 10.098], loss: 0.106059, mae: 0.316402, mean_q: 3.883543
 22388/100000: episode: 606, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 204.906, mean reward: 2.049 [1.459, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.256], loss: 0.156434, mae: 0.315554, mean_q: 3.880701
 22488/100000: episode: 607, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.179, mean reward: 1.892 [1.452, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.161, 10.149], loss: 0.101047, mae: 0.307388, mean_q: 3.882232
 22588/100000: episode: 608, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.692, mean reward: 1.977 [1.455, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.578, 10.379], loss: 0.148277, mae: 0.300695, mean_q: 3.872444
 22688/100000: episode: 609, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.009, mean reward: 2.030 [1.439, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.718, 10.363], loss: 0.140362, mae: 0.296806, mean_q: 3.865984
 22788/100000: episode: 610, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.214, mean reward: 1.892 [1.443, 5.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.666, 10.278], loss: 0.086277, mae: 0.294400, mean_q: 3.836087
 22888/100000: episode: 611, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.770, mean reward: 2.118 [1.490, 6.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.716, 10.222], loss: 0.198177, mae: 0.306142, mean_q: 3.862894
 22988/100000: episode: 612, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 185.445, mean reward: 1.854 [1.474, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.173], loss: 0.095417, mae: 0.308237, mean_q: 3.862775
 23088/100000: episode: 613, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.424, mean reward: 1.874 [1.439, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.106], loss: 0.214029, mae: 0.315841, mean_q: 3.875919
 23188/100000: episode: 614, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.805, mean reward: 1.868 [1.444, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.824, 10.138], loss: 0.161412, mae: 0.320114, mean_q: 3.880039
 23288/100000: episode: 615, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.177, mean reward: 1.922 [1.478, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.293, 10.098], loss: 0.084908, mae: 0.289678, mean_q: 3.869523
 23388/100000: episode: 616, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 183.944, mean reward: 1.839 [1.443, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.291, 10.098], loss: 0.089058, mae: 0.296925, mean_q: 3.870224
 23488/100000: episode: 617, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: 212.853, mean reward: 2.129 [1.470, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.039, 10.098], loss: 0.143766, mae: 0.296156, mean_q: 3.859510
 23588/100000: episode: 618, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.216, mean reward: 1.922 [1.454, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.760, 10.098], loss: 0.155859, mae: 0.311462, mean_q: 3.872133
 23688/100000: episode: 619, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.998, mean reward: 1.940 [1.463, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.761, 10.255], loss: 0.102447, mae: 0.313118, mean_q: 3.868443
 23788/100000: episode: 620, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 180.032, mean reward: 1.800 [1.477, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.330, 10.207], loss: 0.205030, mae: 0.312007, mean_q: 3.877438
 23888/100000: episode: 621, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.212, mean reward: 1.872 [1.475, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.607, 10.149], loss: 0.144773, mae: 0.301266, mean_q: 3.866762
 23988/100000: episode: 622, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 208.448, mean reward: 2.084 [1.479, 5.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.947, 10.414], loss: 0.097710, mae: 0.304056, mean_q: 3.872473
 24088/100000: episode: 623, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 185.756, mean reward: 1.858 [1.448, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.030, 10.098], loss: 0.208264, mae: 0.311708, mean_q: 3.868424
 24188/100000: episode: 624, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.808, mean reward: 1.888 [1.475, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.727, 10.233], loss: 0.280322, mae: 0.332790, mean_q: 3.870142
 24288/100000: episode: 625, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.645, mean reward: 1.936 [1.499, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.811, 10.116], loss: 0.087141, mae: 0.295237, mean_q: 3.847525
 24388/100000: episode: 626, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 229.754, mean reward: 2.298 [1.509, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.292, 10.485], loss: 0.149385, mae: 0.302395, mean_q: 3.852116
 24488/100000: episode: 627, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 175.128, mean reward: 1.751 [1.455, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.347, 10.098], loss: 0.145434, mae: 0.304014, mean_q: 3.887638
 24588/100000: episode: 628, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 214.384, mean reward: 2.144 [1.462, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.880, 10.359], loss: 0.154118, mae: 0.306908, mean_q: 3.869132
 24688/100000: episode: 629, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 196.931, mean reward: 1.969 [1.472, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.592, 10.313], loss: 0.150684, mae: 0.308485, mean_q: 3.867419
 24788/100000: episode: 630, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 191.685, mean reward: 1.917 [1.522, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.887, 10.156], loss: 0.090786, mae: 0.295070, mean_q: 3.855101
 24888/100000: episode: 631, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 208.258, mean reward: 2.083 [1.534, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.434, 10.522], loss: 0.148592, mae: 0.300872, mean_q: 3.854493
 24988/100000: episode: 632, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.533, mean reward: 1.855 [1.482, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.022, 10.336], loss: 0.087870, mae: 0.291274, mean_q: 3.854074
 25088/100000: episode: 633, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.779, mean reward: 1.918 [1.455, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.771, 10.120], loss: 0.098145, mae: 0.308575, mean_q: 3.868536
 25188/100000: episode: 634, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.884, mean reward: 1.949 [1.488, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.464, 10.098], loss: 0.087012, mae: 0.294552, mean_q: 3.855465
 25288/100000: episode: 635, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.018, mean reward: 1.920 [1.450, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.544, 10.253], loss: 0.082237, mae: 0.286404, mean_q: 3.844268
 25388/100000: episode: 636, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 193.969, mean reward: 1.940 [1.437, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.539, 10.147], loss: 0.144320, mae: 0.300915, mean_q: 3.852015
 25488/100000: episode: 637, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 192.434, mean reward: 1.924 [1.462, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.098], loss: 0.096679, mae: 0.306169, mean_q: 3.869312
 25588/100000: episode: 638, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.345, mean reward: 1.923 [1.456, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.286, 10.138], loss: 0.086726, mae: 0.289549, mean_q: 3.842792
 25688/100000: episode: 639, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 201.833, mean reward: 2.018 [1.514, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.046, 10.247], loss: 0.145048, mae: 0.296447, mean_q: 3.851500
 25788/100000: episode: 640, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 186.358, mean reward: 1.864 [1.451, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.942, 10.187], loss: 0.081703, mae: 0.287929, mean_q: 3.865639
 25888/100000: episode: 641, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 195.145, mean reward: 1.951 [1.442, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.539, 10.269], loss: 0.154046, mae: 0.305045, mean_q: 3.857810
 25988/100000: episode: 642, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 195.379, mean reward: 1.954 [1.485, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.511, 10.098], loss: 0.097823, mae: 0.302725, mean_q: 3.852918
 26088/100000: episode: 643, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 208.153, mean reward: 2.082 [1.452, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.368, 10.265], loss: 0.092653, mae: 0.297984, mean_q: 3.856881
 26188/100000: episode: 644, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 199.268, mean reward: 1.993 [1.444, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.485, 10.134], loss: 0.090164, mae: 0.293102, mean_q: 3.853420
 26288/100000: episode: 645, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 209.901, mean reward: 2.099 [1.477, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.576, 10.098], loss: 0.091896, mae: 0.302650, mean_q: 3.868556
 26388/100000: episode: 646, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 193.090, mean reward: 1.931 [1.477, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.653, 10.222], loss: 0.100180, mae: 0.300248, mean_q: 3.856762
 26488/100000: episode: 647, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 187.593, mean reward: 1.876 [1.458, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.668, 10.098], loss: 0.076836, mae: 0.279558, mean_q: 3.839327
 26588/100000: episode: 648, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 192.713, mean reward: 1.927 [1.441, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.526, 10.347], loss: 0.084670, mae: 0.287417, mean_q: 3.844007
 26688/100000: episode: 649, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.690, mean reward: 1.927 [1.470, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.041, 10.098], loss: 0.094802, mae: 0.302651, mean_q: 3.855594
[Info] 1-TH LEVEL FOUND: 5.154690742492676, Considering 10/90 traces
 26788/100000: episode: 650, duration: 4.729s, episode steps: 100, steps per second: 21, episode reward: 184.928, mean reward: 1.849 [1.433, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.661, 10.104], loss: 0.092789, mae: 0.299407, mean_q: 3.861501
 26835/100000: episode: 651, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 101.127, mean reward: 2.152 [1.502, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.393, 10.100], loss: 0.083238, mae: 0.293837, mean_q: 3.857834
 26882/100000: episode: 652, duration: 0.238s, episode steps: 47, steps per second: 198, episode reward: 114.053, mean reward: 2.427 [1.464, 5.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.226, 10.294], loss: 0.087757, mae: 0.287113, mean_q: 3.853895
 26893/100000: episode: 653, duration: 0.057s, episode steps: 11, steps per second: 195, episode reward: 30.574, mean reward: 2.779 [2.176, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.441], loss: 0.083751, mae: 0.291669, mean_q: 3.880955
 26926/100000: episode: 654, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 136.675, mean reward: 4.142 [2.275, 9.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.399, 10.100], loss: 0.094329, mae: 0.299383, mean_q: 3.876679
 26932/100000: episode: 655, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 15.909, mean reward: 2.651 [2.420, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.454], loss: 0.078679, mae: 0.289107, mean_q: 3.914153
 26944/100000: episode: 656, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 27.543, mean reward: 2.295 [1.732, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.263], loss: 0.127164, mae: 0.326341, mean_q: 3.872426
 26956/100000: episode: 657, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 30.047, mean reward: 2.504 [2.126, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.395], loss: 0.089931, mae: 0.295966, mean_q: 3.866485
 26968/100000: episode: 658, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 29.832, mean reward: 2.486 [1.801, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.460, 10.388], loss: 0.124865, mae: 0.345399, mean_q: 3.892834
 26987/100000: episode: 659, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 55.022, mean reward: 2.896 [2.253, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.947, 10.471], loss: 0.133621, mae: 0.345176, mean_q: 3.926055
 27006/100000: episode: 660, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 94.270, mean reward: 4.962 [2.034, 11.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.093, 10.666], loss: 0.073544, mae: 0.280564, mean_q: 3.890423
 27017/100000: episode: 661, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 23.071, mean reward: 2.097 [1.911, 2.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.313], loss: 0.156974, mae: 0.369224, mean_q: 3.986522
 27038/100000: episode: 662, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 72.677, mean reward: 3.461 [2.534, 5.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.489, 10.100], loss: 0.157289, mae: 0.322493, mean_q: 3.921587
 27050/100000: episode: 663, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 30.078, mean reward: 2.507 [2.169, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.344], loss: 0.144001, mae: 0.317227, mean_q: 3.963412
 27083/100000: episode: 664, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 81.545, mean reward: 2.471 [1.708, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.186, 10.100], loss: 0.112247, mae: 0.320553, mean_q: 3.932687
 27095/100000: episode: 665, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 35.570, mean reward: 2.964 [2.338, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.472], loss: 0.132524, mae: 0.309465, mean_q: 3.917424
 27114/100000: episode: 666, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 48.823, mean reward: 2.570 [1.944, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.373, 10.441], loss: 0.117071, mae: 0.317929, mean_q: 3.903482
 27147/100000: episode: 667, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 82.629, mean reward: 2.504 [1.972, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.496, 10.100], loss: 0.153189, mae: 0.348700, mean_q: 3.988440
 27158/100000: episode: 668, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 27.084, mean reward: 2.462 [2.189, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.394], loss: 0.102984, mae: 0.320310, mean_q: 3.984441
 27206/100000: episode: 669, duration: 0.234s, episode steps: 48, steps per second: 205, episode reward: 110.117, mean reward: 2.294 [1.543, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.283, 10.153], loss: 0.104436, mae: 0.311995, mean_q: 3.943926
 27217/100000: episode: 670, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 26.897, mean reward: 2.445 [2.076, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.865, 10.348], loss: 0.106414, mae: 0.314069, mean_q: 3.966201
 27264/100000: episode: 671, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 130.168, mean reward: 2.770 [1.630, 9.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.232, 10.327], loss: 0.085877, mae: 0.302774, mean_q: 3.959007
 27283/100000: episode: 672, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 55.229, mean reward: 2.907 [2.199, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.001, 10.393], loss: 0.153043, mae: 0.352865, mean_q: 4.018467
 27294/100000: episode: 673, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 41.870, mean reward: 3.806 [2.303, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.431], loss: 0.185048, mae: 0.317934, mean_q: 3.878793
 27313/100000: episode: 674, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 42.954, mean reward: 2.261 [1.841, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.578, 10.344], loss: 0.156372, mae: 0.363898, mean_q: 3.981858
 27334/100000: episode: 675, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 68.039, mean reward: 3.240 [1.985, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.613, 10.100], loss: 0.149036, mae: 0.348025, mean_q: 4.025238
 27345/100000: episode: 676, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 27.540, mean reward: 2.504 [2.232, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.356], loss: 0.096879, mae: 0.289612, mean_q: 3.924863
 27366/100000: episode: 677, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 47.292, mean reward: 2.252 [1.801, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.755, 10.100], loss: 0.105969, mae: 0.328513, mean_q: 3.997955
 27378/100000: episode: 678, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 33.917, mean reward: 2.826 [2.221, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.406], loss: 0.112071, mae: 0.311577, mean_q: 3.940820
 27389/100000: episode: 679, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 29.556, mean reward: 2.687 [2.174, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.468, 10.424], loss: 0.078248, mae: 0.295410, mean_q: 3.955473
 27437/100000: episode: 680, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 121.086, mean reward: 2.523 [1.857, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.473, 10.276], loss: 0.154519, mae: 0.343878, mean_q: 4.049922
 27485/100000: episode: 681, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 112.066, mean reward: 2.335 [1.437, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.273, 10.118], loss: 0.138899, mae: 0.354669, mean_q: 4.035333
 27504/100000: episode: 682, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 72.450, mean reward: 3.813 [2.075, 7.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.110, 10.510], loss: 0.127179, mae: 0.347173, mean_q: 4.086383
 27552/100000: episode: 683, duration: 0.236s, episode steps: 48, steps per second: 203, episode reward: 108.509, mean reward: 2.261 [1.567, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.197, 10.397], loss: 0.133208, mae: 0.344505, mean_q: 3.999381
 27599/100000: episode: 684, duration: 0.232s, episode steps: 47, steps per second: 203, episode reward: 121.774, mean reward: 2.591 [1.521, 6.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.757, 10.100], loss: 0.122466, mae: 0.335685, mean_q: 4.050933
 27610/100000: episode: 685, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 28.257, mean reward: 2.569 [2.163, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.450], loss: 0.087767, mae: 0.301265, mean_q: 4.044952
 27622/100000: episode: 686, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 33.335, mean reward: 2.778 [2.229, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.474], loss: 0.188687, mae: 0.381644, mean_q: 4.144905
 27633/100000: episode: 687, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 32.288, mean reward: 2.935 [2.279, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.551, 10.446], loss: 0.130393, mae: 0.336840, mean_q: 3.991942
 27644/100000: episode: 688, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 29.597, mean reward: 2.691 [2.202, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.425], loss: 0.140556, mae: 0.359337, mean_q: 4.156988
 27692/100000: episode: 689, duration: 0.253s, episode steps: 48, steps per second: 190, episode reward: 120.238, mean reward: 2.505 [1.609, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.291, 10.370], loss: 0.188715, mae: 0.371245, mean_q: 4.091084
 27703/100000: episode: 690, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 27.601, mean reward: 2.509 [2.062, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.342], loss: 0.224893, mae: 0.379217, mean_q: 3.921178
 27724/100000: episode: 691, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 59.143, mean reward: 2.816 [2.170, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.468, 10.100], loss: 0.161879, mae: 0.398588, mean_q: 4.058720
 27743/100000: episode: 692, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 63.431, mean reward: 3.338 [2.387, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.077, 10.456], loss: 0.148848, mae: 0.334474, mean_q: 3.985231
 27764/100000: episode: 693, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 85.319, mean reward: 4.063 [2.704, 7.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.727, 10.100], loss: 0.132771, mae: 0.351076, mean_q: 4.073590
 27797/100000: episode: 694, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 96.156, mean reward: 2.914 [1.779, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.092, 10.100], loss: 0.211815, mae: 0.386837, mean_q: 4.159908
 27845/100000: episode: 695, duration: 0.229s, episode steps: 48, steps per second: 209, episode reward: 114.858, mean reward: 2.393 [1.495, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-1.287, 10.216], loss: 0.168756, mae: 0.356795, mean_q: 4.128099
 27857/100000: episode: 696, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 33.498, mean reward: 2.791 [2.354, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.465], loss: 0.126999, mae: 0.362148, mean_q: 4.169635
 27868/100000: episode: 697, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 29.592, mean reward: 2.690 [2.354, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.107, 10.406], loss: 0.171353, mae: 0.404127, mean_q: 4.198252
 27874/100000: episode: 698, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 13.880, mean reward: 2.313 [2.109, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.385], loss: 0.119004, mae: 0.320765, mean_q: 4.061808
 27885/100000: episode: 699, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 30.787, mean reward: 2.799 [2.278, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.477], loss: 0.221013, mae: 0.422658, mean_q: 4.230400
 27896/100000: episode: 700, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 47.634, mean reward: 4.330 [2.830, 8.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.367], loss: 0.086147, mae: 0.284887, mean_q: 3.922729
 27907/100000: episode: 701, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 32.909, mean reward: 2.992 [2.501, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.502], loss: 0.108843, mae: 0.343799, mean_q: 4.155915
 27928/100000: episode: 702, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 52.644, mean reward: 2.507 [1.969, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.370, 10.100], loss: 0.156670, mae: 0.372395, mean_q: 4.191466
 27961/100000: episode: 703, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 92.306, mean reward: 2.797 [1.506, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.154, 10.100], loss: 0.139265, mae: 0.342567, mean_q: 4.127631
 27982/100000: episode: 704, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 51.861, mean reward: 2.470 [1.984, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.321, 10.100], loss: 0.162458, mae: 0.364482, mean_q: 4.215950
 28015/100000: episode: 705, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 124.412, mean reward: 3.770 [2.343, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.451, 10.100], loss: 0.180533, mae: 0.359238, mean_q: 4.220960
 28034/100000: episode: 706, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 51.234, mean reward: 2.697 [1.828, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.138, 10.328], loss: 0.140562, mae: 0.363334, mean_q: 4.180913
 28067/100000: episode: 707, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 93.664, mean reward: 2.838 [1.844, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.788, 10.100], loss: 0.149678, mae: 0.365853, mean_q: 4.238646
 28078/100000: episode: 708, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 34.463, mean reward: 3.133 [2.572, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.140, 10.484], loss: 0.166549, mae: 0.409952, mean_q: 4.197418
 28125/100000: episode: 709, duration: 0.229s, episode steps: 47, steps per second: 205, episode reward: 138.482, mean reward: 2.946 [1.752, 6.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.696, 10.281], loss: 0.138066, mae: 0.354220, mean_q: 4.161416
 28137/100000: episode: 710, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 37.408, mean reward: 3.117 [1.832, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.347], loss: 0.180029, mae: 0.411968, mean_q: 4.262982
 28143/100000: episode: 711, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 15.731, mean reward: 2.622 [2.307, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.115, 10.429], loss: 0.267513, mae: 0.418341, mean_q: 4.294809
 28149/100000: episode: 712, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 16.331, mean reward: 2.722 [2.371, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.444], loss: 0.146371, mae: 0.377537, mean_q: 4.371796
 28161/100000: episode: 713, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 30.065, mean reward: 2.505 [2.286, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.372], loss: 0.117186, mae: 0.341904, mean_q: 4.274739
 28182/100000: episode: 714, duration: 0.102s, episode steps: 21, steps per second: 207, episode reward: 68.545, mean reward: 3.264 [2.301, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.385, 10.100], loss: 0.151351, mae: 0.382122, mean_q: 4.233136
 28229/100000: episode: 715, duration: 0.231s, episode steps: 47, steps per second: 204, episode reward: 109.020, mean reward: 2.320 [1.532, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.452, 10.100], loss: 0.152751, mae: 0.365743, mean_q: 4.212555
 28248/100000: episode: 716, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 56.529, mean reward: 2.975 [2.450, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.457], loss: 0.171033, mae: 0.379771, mean_q: 4.207309
 28260/100000: episode: 717, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 26.748, mean reward: 2.229 [1.645, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.257], loss: 0.176651, mae: 0.413220, mean_q: 4.392887
 28271/100000: episode: 718, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 26.742, mean reward: 2.431 [2.058, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.363], loss: 0.183973, mae: 0.380031, mean_q: 4.312056
 28292/100000: episode: 719, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 44.932, mean reward: 2.140 [1.739, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.720, 10.100], loss: 0.154281, mae: 0.367384, mean_q: 4.250625
 28303/100000: episode: 720, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 38.393, mean reward: 3.490 [2.465, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.547], loss: 0.265527, mae: 0.422499, mean_q: 4.327862
 28336/100000: episode: 721, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 104.891, mean reward: 3.179 [2.221, 11.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.369, 10.100], loss: 0.175913, mae: 0.388020, mean_q: 4.327957
 28383/100000: episode: 722, duration: 0.233s, episode steps: 47, steps per second: 201, episode reward: 113.339, mean reward: 2.411 [1.582, 5.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.633, 10.227], loss: 0.156890, mae: 0.382020, mean_q: 4.299779
 28404/100000: episode: 723, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 51.000, mean reward: 2.429 [1.719, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.193, 10.100], loss: 0.262397, mae: 0.430335, mean_q: 4.331817
 28415/100000: episode: 724, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 36.747, mean reward: 3.341 [2.831, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.529], loss: 0.197455, mae: 0.396090, mean_q: 4.200006
 28434/100000: episode: 725, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 44.459, mean reward: 2.340 [1.806, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.115, 10.396], loss: 0.178885, mae: 0.394085, mean_q: 4.326982
 28482/100000: episode: 726, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 112.154, mean reward: 2.337 [1.517, 5.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.846, 10.219], loss: 0.214517, mae: 0.414763, mean_q: 4.354109
 28488/100000: episode: 727, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 16.181, mean reward: 2.697 [2.335, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.156, 10.467], loss: 0.125248, mae: 0.366780, mean_q: 4.254439
 28500/100000: episode: 728, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 32.111, mean reward: 2.676 [2.183, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.540, 10.391], loss: 0.197345, mae: 0.412060, mean_q: 4.364540
 28511/100000: episode: 729, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 33.917, mean reward: 3.083 [2.166, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.343], loss: 0.302090, mae: 0.464691, mean_q: 4.415606
 28523/100000: episode: 730, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 33.152, mean reward: 2.763 [1.700, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.471, 10.322], loss: 0.152371, mae: 0.414504, mean_q: 4.350873
 28542/100000: episode: 731, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 63.864, mean reward: 3.361 [2.605, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.521], loss: 0.163376, mae: 0.392796, mean_q: 4.344166
 28589/100000: episode: 732, duration: 0.239s, episode steps: 47, steps per second: 197, episode reward: 96.962, mean reward: 2.063 [1.496, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.782, 10.140], loss: 0.144617, mae: 0.383026, mean_q: 4.303571
 28601/100000: episode: 733, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 31.889, mean reward: 2.657 [2.312, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.485], loss: 0.202876, mae: 0.430850, mean_q: 4.369756
 28622/100000: episode: 734, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 47.709, mean reward: 2.272 [1.605, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.062, 10.100], loss: 0.120958, mae: 0.342695, mean_q: 4.298640
 28633/100000: episode: 735, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 36.428, mean reward: 3.312 [2.544, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.385, 10.578], loss: 0.162735, mae: 0.417503, mean_q: 4.487333
 28644/100000: episode: 736, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 37.835, mean reward: 3.440 [2.388, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.516], loss: 0.172324, mae: 0.382771, mean_q: 4.292472
 28692/100000: episode: 737, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 101.849, mean reward: 2.122 [1.503, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.997, 10.124], loss: 0.169572, mae: 0.402189, mean_q: 4.374656
 28713/100000: episode: 738, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 52.737, mean reward: 2.511 [2.279, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.281, 10.100], loss: 0.246729, mae: 0.398977, mean_q: 4.349214
 28734/100000: episode: 739, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 50.869, mean reward: 2.422 [1.874, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.980, 10.100], loss: 0.221770, mae: 0.437136, mean_q: 4.454524
[Info] 2-TH LEVEL FOUND: 7.6189494132995605, Considering 10/90 traces
 28740/100000: episode: 740, duration: 4.228s, episode steps: 6, steps per second: 1, episode reward: 14.750, mean reward: 2.458 [2.209, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.363], loss: 0.186020, mae: 0.429495, mean_q: 4.292512
 28751/100000: episode: 741, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 31.015, mean reward: 2.820 [2.409, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.156, 10.100], loss: 0.175632, mae: 0.399133, mean_q: 4.340762
 28764/100000: episode: 742, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 36.692, mean reward: 2.822 [2.440, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.450, 10.100], loss: 0.133431, mae: 0.378402, mean_q: 4.326599
 28768/100000: episode: 743, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 15.434, mean reward: 3.859 [3.139, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.496], loss: 0.151220, mae: 0.360331, mean_q: 4.205859
 28779/100000: episode: 744, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 30.066, mean reward: 2.733 [2.206, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.390, 10.100], loss: 0.220989, mae: 0.405676, mean_q: 4.421392
 28783/100000: episode: 745, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 13.674, mean reward: 3.419 [2.847, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.452], loss: 0.320667, mae: 0.387964, mean_q: 4.176777
 28786/100000: episode: 746, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 11.471, mean reward: 3.824 [3.705, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.475], loss: 0.177134, mae: 0.373266, mean_q: 4.430061
 28799/100000: episode: 747, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 39.129, mean reward: 3.010 [2.602, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.223, 10.100], loss: 0.177828, mae: 0.406206, mean_q: 4.454092
 28809/100000: episode: 748, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 54.408, mean reward: 5.441 [4.577, 7.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.670, 10.610], loss: 0.206458, mae: 0.440204, mean_q: 4.493270
 28820/100000: episode: 749, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 30.771, mean reward: 2.797 [2.304, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.270, 10.100], loss: 0.151353, mae: 0.396437, mean_q: 4.479106
 28830/100000: episode: 750, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 51.482, mean reward: 5.148 [3.648, 8.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.495, 10.606], loss: 0.215174, mae: 0.414527, mean_q: 4.309856
 28841/100000: episode: 751, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 32.276, mean reward: 2.934 [2.273, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.395, 10.100], loss: 0.133262, mae: 0.389925, mean_q: 4.424752
 28845/100000: episode: 752, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 20.698, mean reward: 5.174 [3.815, 7.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.411, 10.581], loss: 0.129135, mae: 0.358302, mean_q: 4.294557
 28868/100000: episode: 753, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 77.510, mean reward: 3.370 [2.713, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.322, 10.100], loss: 0.146387, mae: 0.358774, mean_q: 4.359664
 28872/100000: episode: 754, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 15.550, mean reward: 3.888 [3.384, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.174, 10.511], loss: 0.162082, mae: 0.439132, mean_q: 4.577226
 28876/100000: episode: 755, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 13.550, mean reward: 3.387 [2.932, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.380], loss: 0.130300, mae: 0.360747, mean_q: 4.105419
 28889/100000: episode: 756, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 42.027, mean reward: 3.233 [2.593, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.306, 10.100], loss: 0.184460, mae: 0.397143, mean_q: 4.429787
 28892/100000: episode: 757, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.246, mean reward: 3.749 [3.453, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.546], loss: 0.168744, mae: 0.414710, mean_q: 4.604044
 28915/100000: episode: 758, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 67.644, mean reward: 2.941 [2.252, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.483, 10.100], loss: 0.156191, mae: 0.384854, mean_q: 4.515243
 28938/100000: episode: 759, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 80.584, mean reward: 3.504 [2.498, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.719, 10.100], loss: 0.211000, mae: 0.420000, mean_q: 4.446776
 28961/100000: episode: 760, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 71.704, mean reward: 3.118 [2.177, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.145, 10.100], loss: 0.178295, mae: 0.387344, mean_q: 4.438086
 28971/100000: episode: 761, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 42.452, mean reward: 4.245 [2.652, 7.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.391, 10.453], loss: 0.117865, mae: 0.352312, mean_q: 4.414874
 28981/100000: episode: 762, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 63.670, mean reward: 6.367 [4.481, 9.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.575], loss: 0.160979, mae: 0.400452, mean_q: 4.420239
 28985/100000: episode: 763, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 16.266, mean reward: 4.067 [3.809, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.518], loss: 0.126301, mae: 0.362977, mean_q: 4.368257
 28998/100000: episode: 764, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 48.189, mean reward: 3.707 [2.841, 5.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.752, 10.100], loss: 0.184854, mae: 0.387586, mean_q: 4.451128
 29008/100000: episode: 765, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 53.334, mean reward: 5.333 [4.301, 8.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.517], loss: 0.259531, mae: 0.421846, mean_q: 4.466728
 29019/100000: episode: 766, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 27.690, mean reward: 2.517 [2.216, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.329, 10.100], loss: 0.223409, mae: 0.437746, mean_q: 4.520606
 29029/100000: episode: 767, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 50.891, mean reward: 5.089 [3.954, 7.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.503], loss: 0.156699, mae: 0.402888, mean_q: 4.407063
 29052/100000: episode: 768, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 148.084, mean reward: 6.438 [4.182, 11.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.403, 10.100], loss: 0.219660, mae: 0.437763, mean_q: 4.556082
 29075/100000: episode: 769, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 106.238, mean reward: 4.619 [2.291, 7.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.365, 10.100], loss: 0.282212, mae: 0.484747, mean_q: 4.600879
 29088/100000: episode: 770, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 39.269, mean reward: 3.021 [2.474, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.314, 10.100], loss: 0.283734, mae: 0.452221, mean_q: 4.591934
 29101/100000: episode: 771, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 52.171, mean reward: 4.013 [3.098, 5.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.340, 10.100], loss: 0.298533, mae: 0.460983, mean_q: 4.569145
 29105/100000: episode: 772, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 20.562, mean reward: 5.141 [3.331, 8.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.322], loss: 0.393440, mae: 0.548202, mean_q: 4.840219
 29128/100000: episode: 773, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 81.044, mean reward: 3.524 [2.011, 7.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.230, 10.100], loss: 0.238702, mae: 0.455518, mean_q: 4.612799
 29132/100000: episode: 774, duration: 0.027s, episode steps: 4, steps per second: 145, episode reward: 17.004, mean reward: 4.251 [3.400, 6.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.634], loss: 0.200150, mae: 0.443221, mean_q: 4.456340
 29145/100000: episode: 775, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 33.977, mean reward: 2.614 [2.190, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.299, 10.100], loss: 0.228907, mae: 0.453959, mean_q: 4.633600
 29158/100000: episode: 776, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 51.891, mean reward: 3.992 [2.710, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.889, 10.100], loss: 0.220515, mae: 0.456162, mean_q: 4.673738
 29169/100000: episode: 777, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 26.806, mean reward: 2.437 [2.010, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.452, 10.100], loss: 0.198689, mae: 0.436817, mean_q: 4.542336
 29173/100000: episode: 778, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 15.136, mean reward: 3.784 [3.388, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.549], loss: 0.347800, mae: 0.522282, mean_q: 4.811680
 29176/100000: episode: 779, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 10.594, mean reward: 3.531 [3.181, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.545], loss: 0.190739, mae: 0.459586, mean_q: 4.844801
 29180/100000: episode: 780, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 13.853, mean reward: 3.463 [3.143, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.388], loss: 0.368912, mae: 0.502658, mean_q: 4.657947
 29191/100000: episode: 781, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 33.974, mean reward: 3.089 [2.287, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.384, 10.100], loss: 0.257033, mae: 0.483272, mean_q: 4.700863
 29202/100000: episode: 782, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 30.070, mean reward: 2.734 [2.352, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.294, 10.100], loss: 0.318217, mae: 0.495083, mean_q: 4.718744
 29225/100000: episode: 783, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 84.368, mean reward: 3.668 [2.476, 6.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.332, 10.100], loss: 0.234007, mae: 0.464091, mean_q: 4.621597
 29236/100000: episode: 784, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 27.417, mean reward: 2.492 [2.008, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.311, 10.100], loss: 0.229390, mae: 0.454774, mean_q: 4.639161
 29240/100000: episode: 785, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 14.161, mean reward: 3.540 [3.061, 4.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.321], loss: 0.175689, mae: 0.433377, mean_q: 4.737104
 29244/100000: episode: 786, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 12.150, mean reward: 3.037 [2.933, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.374], loss: 0.223117, mae: 0.439545, mean_q: 4.607048
 29267/100000: episode: 787, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 69.466, mean reward: 3.020 [2.052, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.207, 10.100], loss: 0.304031, mae: 0.489025, mean_q: 4.761339
 29290/100000: episode: 788, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 82.929, mean reward: 3.606 [2.908, 6.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.433, 10.100], loss: 0.259845, mae: 0.459240, mean_q: 4.672700
 29313/100000: episode: 789, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 125.230, mean reward: 5.445 [3.585, 10.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.026, 10.100], loss: 0.264649, mae: 0.431794, mean_q: 4.603971
 29336/100000: episode: 790, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 93.259, mean reward: 4.055 [2.595, 5.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.774, 10.100], loss: 0.255473, mae: 0.457694, mean_q: 4.683478
 29347/100000: episode: 791, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 38.694, mean reward: 3.518 [2.637, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.360, 10.100], loss: 0.300499, mae: 0.482514, mean_q: 4.618137
 29358/100000: episode: 792, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 42.257, mean reward: 3.842 [3.368, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.319, 10.100], loss: 0.237444, mae: 0.470846, mean_q: 4.715868
 29369/100000: episode: 793, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 48.211, mean reward: 4.383 [3.027, 7.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.437, 10.100], loss: 0.297406, mae: 0.505989, mean_q: 4.710147
 29382/100000: episode: 794, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 54.015, mean reward: 4.155 [2.434, 8.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.458, 10.100], loss: 0.232538, mae: 0.475787, mean_q: 4.787750
 29386/100000: episode: 795, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 15.682, mean reward: 3.921 [3.621, 4.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.513], loss: 0.249502, mae: 0.440616, mean_q: 4.556384
 29389/100000: episode: 796, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 11.870, mean reward: 3.957 [3.296, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.428], loss: 0.308668, mae: 0.551589, mean_q: 4.848933
 29400/100000: episode: 797, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 34.032, mean reward: 3.094 [2.424, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.278, 10.100], loss: 0.204083, mae: 0.451594, mean_q: 4.686056
 29423/100000: episode: 798, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 80.739, mean reward: 3.510 [2.324, 8.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.368, 10.100], loss: 0.252452, mae: 0.470125, mean_q: 4.767039
 29434/100000: episode: 799, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 42.593, mean reward: 3.872 [2.826, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.343, 10.100], loss: 0.377947, mae: 0.504230, mean_q: 4.834148
 29437/100000: episode: 800, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 10.525, mean reward: 3.508 [3.451, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.321, 10.430], loss: 0.413910, mae: 0.571662, mean_q: 4.929154
 29447/100000: episode: 801, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 59.221, mean reward: 5.922 [4.278, 10.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.833, 10.495], loss: 0.353328, mae: 0.507000, mean_q: 4.635863
 29451/100000: episode: 802, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 34.541, mean reward: 8.635 [6.379, 13.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.035, 10.687], loss: 0.319750, mae: 0.519786, mean_q: 4.705434
 29474/100000: episode: 803, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 77.762, mean reward: 3.381 [2.434, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.257, 10.100], loss: 0.297310, mae: 0.484327, mean_q: 4.752532
 29478/100000: episode: 804, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 14.654, mean reward: 3.664 [2.864, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.552], loss: 0.179337, mae: 0.435519, mean_q: 4.749757
 29482/100000: episode: 805, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 21.918, mean reward: 5.479 [4.123, 7.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.065, 10.570], loss: 0.262171, mae: 0.450947, mean_q: 4.782010
 29495/100000: episode: 806, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 54.310, mean reward: 4.178 [2.521, 14.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.216, 10.100], loss: 0.278675, mae: 0.516005, mean_q: 4.801198
 29498/100000: episode: 807, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 12.958, mean reward: 4.319 [3.864, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.582], loss: 0.337546, mae: 0.552161, mean_q: 4.814257
 29508/100000: episode: 808, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 40.892, mean reward: 4.089 [3.183, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.566, 10.438], loss: 0.298137, mae: 0.527169, mean_q: 4.849895
 29511/100000: episode: 809, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 11.596, mean reward: 3.865 [3.405, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.817, 10.505], loss: 0.596366, mae: 0.578699, mean_q: 4.644456
 29521/100000: episode: 810, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 65.614, mean reward: 6.561 [4.589, 11.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.575], loss: 0.301514, mae: 0.470744, mean_q: 4.850851
 29544/100000: episode: 811, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 93.024, mean reward: 4.045 [2.967, 5.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.113, 10.100], loss: 0.310401, mae: 0.496537, mean_q: 4.935329
 29548/100000: episode: 812, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 13.111, mean reward: 3.278 [3.103, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.504], loss: 0.160067, mae: 0.379479, mean_q: 4.686070
 29559/100000: episode: 813, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 43.227, mean reward: 3.930 [2.508, 7.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.337, 10.100], loss: 0.301527, mae: 0.525492, mean_q: 4.960557
 29582/100000: episode: 814, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 63.031, mean reward: 2.740 [2.209, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.447, 10.100], loss: 0.329619, mae: 0.522577, mean_q: 4.989149
 29605/100000: episode: 815, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 75.074, mean reward: 3.264 [2.428, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.379, 10.100], loss: 0.278384, mae: 0.502728, mean_q: 4.915607
 29615/100000: episode: 816, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 37.275, mean reward: 3.727 [2.481, 6.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.099, 10.431], loss: 0.296637, mae: 0.491470, mean_q: 4.871774
 29638/100000: episode: 817, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 65.291, mean reward: 2.839 [2.130, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.155, 10.100], loss: 0.345164, mae: 0.507429, mean_q: 4.888554
 29651/100000: episode: 818, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 39.705, mean reward: 3.054 [2.296, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.473, 10.100], loss: 0.293673, mae: 0.507804, mean_q: 4.923081
 29662/100000: episode: 819, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 29.031, mean reward: 2.639 [2.156, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.217, 10.100], loss: 0.465111, mae: 0.548778, mean_q: 4.923980
 29685/100000: episode: 820, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 74.061, mean reward: 3.220 [2.446, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.604, 10.100], loss: 0.306979, mae: 0.502833, mean_q: 4.894971
 29689/100000: episode: 821, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 16.958, mean reward: 4.239 [2.768, 6.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.627], loss: 0.236754, mae: 0.443382, mean_q: 4.959648
 29702/100000: episode: 822, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 34.354, mean reward: 2.643 [2.244, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.337, 10.100], loss: 0.426410, mae: 0.572092, mean_q: 5.102314
 29725/100000: episode: 823, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 96.446, mean reward: 4.193 [2.234, 19.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.227, 10.100], loss: 0.225316, mae: 0.454212, mean_q: 4.961236
 29735/100000: episode: 824, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 31.959, mean reward: 3.196 [2.523, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.306], loss: 0.356165, mae: 0.561308, mean_q: 5.109804
 29746/100000: episode: 825, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 27.261, mean reward: 2.478 [2.076, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.362, 10.100], loss: 0.214609, mae: 0.452563, mean_q: 4.874717
 29750/100000: episode: 826, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 13.358, mean reward: 3.340 [3.232, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.506], loss: 0.593595, mae: 0.652895, mean_q: 5.243874
 29773/100000: episode: 827, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 96.285, mean reward: 4.186 [3.311, 5.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.421, 10.100], loss: 0.304003, mae: 0.517540, mean_q: 4.936023
 29783/100000: episode: 828, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 46.842, mean reward: 4.684 [3.253, 6.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.487], loss: 0.361434, mae: 0.544843, mean_q: 4.975392
 29806/100000: episode: 829, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 89.041, mean reward: 3.871 [3.105, 7.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.350, 10.100], loss: 0.269690, mae: 0.490512, mean_q: 4.988544
[Info] 3-TH LEVEL FOUND: 9.515542984008789, Considering 10/90 traces
 29810/100000: episode: 830, duration: 4.272s, episode steps: 4, steps per second: 1, episode reward: 17.045, mean reward: 4.261 [2.948, 5.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.615], loss: 0.456878, mae: 0.563747, mean_q: 5.015286
 29818/100000: episode: 831, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 40.264, mean reward: 5.033 [4.277, 7.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.493], loss: 0.331405, mae: 0.531322, mean_q: 5.061591
 29826/100000: episode: 832, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 48.193, mean reward: 6.024 [4.213, 8.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.502], loss: 0.380196, mae: 0.575827, mean_q: 5.040236
 29835/100000: episode: 833, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 60.610, mean reward: 6.734 [5.177, 9.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.560], loss: 0.282573, mae: 0.486629, mean_q: 5.043950
 29844/100000: episode: 834, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 42.144, mean reward: 4.683 [3.657, 5.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.539], loss: 0.436913, mae: 0.597773, mean_q: 5.315941
 29853/100000: episode: 835, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 60.253, mean reward: 6.695 [4.873, 10.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.385, 10.541], loss: 0.344429, mae: 0.566708, mean_q: 4.872990
 29861/100000: episode: 836, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 88.427, mean reward: 11.053 [6.150, 23.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.635], loss: 0.450678, mae: 0.608497, mean_q: 5.303222
 29870/100000: episode: 837, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 40.818, mean reward: 4.535 [2.996, 6.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.605], loss: 0.631868, mae: 0.659320, mean_q: 5.225603
 29878/100000: episode: 838, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 49.119, mean reward: 6.140 [4.087, 10.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.617], loss: 0.801562, mae: 0.559349, mean_q: 5.337022
 29886/100000: episode: 839, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 54.887, mean reward: 6.861 [4.662, 14.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.582], loss: 0.316577, mae: 0.533011, mean_q: 5.095221
 29894/100000: episode: 840, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 31.923, mean reward: 3.990 [3.352, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.454], loss: 0.453061, mae: 0.528210, mean_q: 4.895283
 29902/100000: episode: 841, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 62.246, mean reward: 7.781 [4.642, 12.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.212, 10.501], loss: 0.481034, mae: 0.565387, mean_q: 5.031524
 29911/100000: episode: 842, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 57.492, mean reward: 6.388 [5.090, 7.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.631], loss: 0.521696, mae: 0.593770, mean_q: 5.092448
 29919/100000: episode: 843, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 45.679, mean reward: 5.710 [4.232, 8.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.512], loss: 0.590744, mae: 0.594874, mean_q: 5.060034
 29927/100000: episode: 844, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 32.974, mean reward: 4.122 [3.263, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.451], loss: 0.379291, mae: 0.536578, mean_q: 5.012982
 29936/100000: episode: 845, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 67.794, mean reward: 7.533 [3.713, 13.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.682], loss: 0.883513, mae: 0.613408, mean_q: 5.140417
 29944/100000: episode: 846, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 64.024, mean reward: 8.003 [5.171, 12.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.658], loss: 1.068132, mae: 0.733596, mean_q: 5.380744
 29952/100000: episode: 847, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 68.435, mean reward: 8.554 [5.535, 13.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.571, 10.574], loss: 0.459256, mae: 0.588369, mean_q: 5.206752
 29961/100000: episode: 848, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 41.213, mean reward: 4.579 [3.729, 6.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.228, 10.601], loss: 0.295129, mae: 0.501751, mean_q: 5.299246
 29970/100000: episode: 849, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 82.423, mean reward: 9.158 [3.578, 19.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.564], loss: 0.272924, mae: 0.490760, mean_q: 5.110069
 29978/100000: episode: 850, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 29.450, mean reward: 3.681 [3.284, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.520], loss: 0.744707, mae: 0.576016, mean_q: 5.399122
 29986/100000: episode: 851, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 36.932, mean reward: 4.616 [3.540, 6.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.552], loss: 0.617032, mae: 0.692159, mean_q: 5.504272
 29995/100000: episode: 852, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 54.847, mean reward: 6.094 [4.550, 8.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.590], loss: 0.458753, mae: 0.610015, mean_q: 5.194475
 30004/100000: episode: 853, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 95.656, mean reward: 10.628 [3.903, 20.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.315, 10.655], loss: 0.346031, mae: 0.575779, mean_q: 5.511809
 30012/100000: episode: 854, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 35.383, mean reward: 4.423 [3.646, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.498], loss: 0.602832, mae: 0.673675, mean_q: 5.414429
 30020/100000: episode: 855, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 36.899, mean reward: 4.612 [3.191, 6.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.611], loss: 0.339413, mae: 0.550507, mean_q: 5.205684
 30029/100000: episode: 856, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 56.043, mean reward: 6.227 [4.397, 7.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.637], loss: 0.313658, mae: 0.522627, mean_q: 5.278801
 30038/100000: episode: 857, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 36.029, mean reward: 4.003 [3.333, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.543], loss: 0.694702, mae: 0.684534, mean_q: 5.424590
 30046/100000: episode: 858, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 30.364, mean reward: 3.796 [3.335, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.528], loss: 1.045335, mae: 0.870647, mean_q: 5.435747
 30054/100000: episode: 859, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 58.331, mean reward: 7.291 [3.762, 14.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.281, 10.610], loss: 0.752484, mae: 0.694659, mean_q: 5.469598
 30063/100000: episode: 860, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 60.653, mean reward: 6.739 [3.716, 13.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.092, 10.630], loss: 0.445375, mae: 0.590942, mean_q: 5.617762
 30071/100000: episode: 861, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 92.695, mean reward: 11.587 [5.388, 23.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.526], loss: 0.908333, mae: 0.624924, mean_q: 5.360582
 30079/100000: episode: 862, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 35.797, mean reward: 4.475 [3.892, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.379, 10.583], loss: 1.244898, mae: 0.735852, mean_q: 5.587724
 30087/100000: episode: 863, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 99.459, mean reward: 12.432 [3.947, 38.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.155, 10.543], loss: 0.417932, mae: 0.577003, mean_q: 5.328059
 30095/100000: episode: 864, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 62.011, mean reward: 7.751 [4.587, 11.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.586], loss: 0.316666, mae: 0.580151, mean_q: 5.660566
 30103/100000: episode: 865, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 30.114, mean reward: 3.764 [3.315, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.553], loss: 0.537390, mae: 0.646025, mean_q: 5.378634
 30112/100000: episode: 866, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 44.599, mean reward: 4.955 [3.758, 6.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.603, 10.552], loss: 0.600635, mae: 0.618008, mean_q: 5.402724
 30121/100000: episode: 867, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 47.401, mean reward: 5.267 [4.124, 7.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.583], loss: 0.954076, mae: 0.714016, mean_q: 5.583823
 30130/100000: episode: 868, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 82.268, mean reward: 9.141 [4.016, 19.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.411], loss: 1.013469, mae: 0.680673, mean_q: 5.134634
 30138/100000: episode: 869, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 60.279, mean reward: 7.535 [4.084, 10.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.535], loss: 0.683660, mae: 0.740066, mean_q: 5.715897
 30146/100000: episode: 870, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 46.474, mean reward: 5.809 [4.329, 9.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.910, 10.540], loss: 0.858060, mae: 0.685771, mean_q: 5.206762
 30154/100000: episode: 871, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 30.272, mean reward: 3.784 [3.150, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.463], loss: 1.679734, mae: 0.842171, mean_q: 5.692734
 30162/100000: episode: 872, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 33.254, mean reward: 4.157 [2.898, 5.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.354, 10.544], loss: 2.653665, mae: 0.848117, mean_q: 5.881729
 30171/100000: episode: 873, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 42.139, mean reward: 4.682 [3.619, 5.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.500], loss: 1.766086, mae: 1.007615, mean_q: 6.049389
 30179/100000: episode: 874, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 87.624, mean reward: 10.953 [8.240, 15.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.106, 10.689], loss: 0.846911, mae: 0.841416, mean_q: 5.577973
 30188/100000: episode: 875, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 112.368, mean reward: 12.485 [4.754, 26.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.719], loss: 0.800869, mae: 0.670097, mean_q: 5.231484
 30196/100000: episode: 876, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 40.456, mean reward: 5.057 [3.644, 6.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.481], loss: 1.131746, mae: 0.819595, mean_q: 5.683919
 30205/100000: episode: 877, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 47.183, mean reward: 5.243 [3.738, 7.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.120, 10.485], loss: 0.791761, mae: 0.785590, mean_q: 5.628391
 30213/100000: episode: 878, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 36.779, mean reward: 4.597 [3.655, 6.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.572], loss: 0.867948, mae: 0.792469, mean_q: 5.715505
 30222/100000: episode: 879, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 45.612, mean reward: 5.068 [4.523, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.578], loss: 0.738544, mae: 0.728052, mean_q: 5.643600
 30231/100000: episode: 880, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 64.231, mean reward: 7.137 [4.746, 10.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.659], loss: 0.910273, mae: 0.775027, mean_q: 5.761942
 30240/100000: episode: 881, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 40.813, mean reward: 4.535 [3.259, 6.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.518], loss: 0.418397, mae: 0.587029, mean_q: 5.510836
 30249/100000: episode: 882, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 36.787, mean reward: 4.087 [3.178, 5.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.490], loss: 0.359490, mae: 0.580477, mean_q: 5.586031
 30258/100000: episode: 883, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 48.044, mean reward: 5.338 [3.113, 8.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.470], loss: 3.311298, mae: 1.077467, mean_q: 5.949226
 30266/100000: episode: 884, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 88.095, mean reward: 11.012 [6.196, 29.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.612], loss: 0.741454, mae: 0.780267, mean_q: 5.762949
 30274/100000: episode: 885, duration: 0.054s, episode steps: 8, steps per second: 150, episode reward: 32.921, mean reward: 4.115 [2.857, 6.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.902, 10.490], loss: 0.574851, mae: 0.703044, mean_q: 5.428691
 30283/100000: episode: 886, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 289.712, mean reward: 32.190 [5.509, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.568, 10.736], loss: 4.224318, mae: 1.084083, mean_q: 6.069010
 30291/100000: episode: 887, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 30.077, mean reward: 3.760 [2.974, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.364, 10.570], loss: 18.170893, mae: 1.651029, mean_q: 6.846424
 30300/100000: episode: 888, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 104.948, mean reward: 11.661 [6.428, 28.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.633], loss: 1.207059, mae: 1.006242, mean_q: 5.300738
 30308/100000: episode: 889, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 33.232, mean reward: 4.154 [3.480, 5.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.532], loss: 1.328055, mae: 0.976162, mean_q: 5.335241
 30317/100000: episode: 890, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 73.223, mean reward: 8.136 [6.543, 11.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.632], loss: 1.910504, mae: 0.947527, mean_q: 5.850645
 30326/100000: episode: 891, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 38.386, mean reward: 4.265 [3.967, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.535], loss: 1.203569, mae: 0.838317, mean_q: 5.763692
 30335/100000: episode: 892, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 73.537, mean reward: 8.171 [6.316, 11.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.605], loss: 0.893066, mae: 0.836213, mean_q: 5.831003
 30343/100000: episode: 893, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 76.565, mean reward: 9.571 [5.435, 17.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.216, 10.706], loss: 1.109413, mae: 0.794699, mean_q: 5.943878
 30352/100000: episode: 894, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 55.004, mean reward: 6.112 [3.673, 9.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.642], loss: 0.900009, mae: 0.744360, mean_q: 5.822062
 30361/100000: episode: 895, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 52.231, mean reward: 5.803 [3.868, 10.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.677], loss: 0.781073, mae: 0.773764, mean_q: 5.958683
 30370/100000: episode: 896, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 91.851, mean reward: 10.206 [4.976, 18.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.599], loss: 0.839704, mae: 0.661786, mean_q: 5.509295
 30379/100000: episode: 897, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 47.528, mean reward: 5.281 [3.688, 8.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.602, 10.633], loss: 0.823256, mae: 0.780382, mean_q: 6.099375
[Info] FALSIFICATION!
[Info] Levels: [5.1546907, 7.6189494, 9.515543, 13.175355]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.23]
[Info] Error Prob: 0.00023000000000000006

 30388/100000: episode: 898, duration: 4.628s, episode steps: 9, steps per second: 2, episode reward: 173.263, mean reward: 19.251 [6.201, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.742], loss: 1.633684, mae: 0.867197, mean_q: 6.064997
 30488/100000: episode: 899, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 199.951, mean reward: 2.000 [1.462, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.081, 10.098], loss: 2.997613, mae: 0.991527, mean_q: 6.046499
 30588/100000: episode: 900, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 199.963, mean reward: 2.000 [1.449, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.687, 10.257], loss: 1.404613, mae: 0.835330, mean_q: 5.996502
 30688/100000: episode: 901, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 214.202, mean reward: 2.142 [1.462, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.645, 10.437], loss: 3.686803, mae: 1.013536, mean_q: 6.210308
 30788/100000: episode: 902, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 199.946, mean reward: 1.999 [1.476, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.098], loss: 1.986639, mae: 0.912091, mean_q: 6.095833
 30888/100000: episode: 903, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.228, mean reward: 1.932 [1.458, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.735, 10.098], loss: 3.178859, mae: 0.944777, mean_q: 6.148114
 30988/100000: episode: 904, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.044, mean reward: 1.850 [1.456, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.686, 10.098], loss: 2.793459, mae: 0.884487, mean_q: 6.091296
 31088/100000: episode: 905, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.437, mean reward: 1.894 [1.473, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.339, 10.098], loss: 5.227854, mae: 1.057731, mean_q: 6.223513
 31188/100000: episode: 906, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 208.765, mean reward: 2.088 [1.510, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.934, 10.260], loss: 1.725084, mae: 0.871219, mean_q: 6.210752
 31288/100000: episode: 907, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 199.870, mean reward: 1.999 [1.492, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.519, 10.170], loss: 1.471651, mae: 0.839883, mean_q: 6.009109
 31388/100000: episode: 908, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.682, mean reward: 1.817 [1.462, 2.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.095, 10.098], loss: 1.666218, mae: 0.843789, mean_q: 6.078290
 31488/100000: episode: 909, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.440, mean reward: 1.814 [1.456, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.098], loss: 6.692599, mae: 1.057367, mean_q: 6.149982
 31588/100000: episode: 910, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.772, mean reward: 1.888 [1.461, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.347, 10.305], loss: 3.135980, mae: 0.932405, mean_q: 6.053111
 31688/100000: episode: 911, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.715, mean reward: 1.857 [1.480, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.551, 10.098], loss: 6.904104, mae: 1.143670, mean_q: 6.191163
 31788/100000: episode: 912, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 178.592, mean reward: 1.786 [1.451, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.922, 10.098], loss: 2.573387, mae: 0.849719, mean_q: 6.049861
 31888/100000: episode: 913, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 182.412, mean reward: 1.824 [1.432, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.081, 10.270], loss: 3.683866, mae: 0.881031, mean_q: 5.869001
 31988/100000: episode: 914, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 176.805, mean reward: 1.768 [1.476, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.424, 10.464], loss: 5.894872, mae: 1.086544, mean_q: 6.019553
 32088/100000: episode: 915, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 206.412, mean reward: 2.064 [1.568, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.354, 10.098], loss: 5.191472, mae: 1.046215, mean_q: 6.107882
 32188/100000: episode: 916, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.018, mean reward: 1.810 [1.467, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.488, 10.364], loss: 2.562809, mae: 0.856034, mean_q: 5.974873
 32288/100000: episode: 917, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 230.705, mean reward: 2.307 [1.469, 6.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.045, 10.387], loss: 3.770577, mae: 0.910117, mean_q: 5.863467
 32388/100000: episode: 918, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.302, mean reward: 1.833 [1.471, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.098], loss: 4.054331, mae: 0.911438, mean_q: 5.861782
 32488/100000: episode: 919, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 196.478, mean reward: 1.965 [1.463, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.715, 10.214], loss: 4.206344, mae: 0.931183, mean_q: 5.863593
 32588/100000: episode: 920, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 211.278, mean reward: 2.113 [1.484, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.706, 10.254], loss: 7.642328, mae: 1.093095, mean_q: 5.903823
 32688/100000: episode: 921, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 197.955, mean reward: 1.980 [1.505, 4.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.098], loss: 4.137147, mae: 0.997776, mean_q: 5.857450
 32788/100000: episode: 922, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.792, mean reward: 1.858 [1.466, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.185], loss: 3.761759, mae: 0.841888, mean_q: 5.819026
 32888/100000: episode: 923, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 220.401, mean reward: 2.204 [1.472, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.785, 10.098], loss: 1.547740, mae: 0.786143, mean_q: 5.639865
 32988/100000: episode: 924, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.490, mean reward: 1.915 [1.490, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.314, 10.098], loss: 6.541113, mae: 0.953175, mean_q: 5.726334
 33088/100000: episode: 925, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 198.825, mean reward: 1.988 [1.481, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.240, 10.488], loss: 3.263927, mae: 0.841588, mean_q: 5.662584
 33188/100000: episode: 926, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 182.545, mean reward: 1.825 [1.455, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.345, 10.198], loss: 4.902771, mae: 0.952899, mean_q: 5.756208
 33288/100000: episode: 927, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 183.958, mean reward: 1.840 [1.494, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.858, 10.098], loss: 2.253798, mae: 0.752669, mean_q: 5.593962
 33388/100000: episode: 928, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.039, mean reward: 1.870 [1.501, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.197, 10.113], loss: 4.466388, mae: 0.843715, mean_q: 5.578675
 33488/100000: episode: 929, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 208.293, mean reward: 2.083 [1.497, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.767, 10.098], loss: 4.533392, mae: 0.939008, mean_q: 5.730598
 33588/100000: episode: 930, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 193.104, mean reward: 1.931 [1.462, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.098], loss: 1.404602, mae: 0.702629, mean_q: 5.422287
 33688/100000: episode: 931, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.979, mean reward: 1.880 [1.466, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.229, 10.165], loss: 1.069144, mae: 0.651218, mean_q: 5.356381
 33788/100000: episode: 932, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.864, mean reward: 1.989 [1.458, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.139, 10.226], loss: 1.861983, mae: 0.735089, mean_q: 5.467165
 33888/100000: episode: 933, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 196.839, mean reward: 1.968 [1.462, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.232, 10.098], loss: 1.843690, mae: 0.742836, mean_q: 5.437162
 33988/100000: episode: 934, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.337, mean reward: 2.053 [1.442, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.517, 10.113], loss: 1.669448, mae: 0.717817, mean_q: 5.358903
 34088/100000: episode: 935, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 223.457, mean reward: 2.235 [1.487, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.979, 10.388], loss: 0.973191, mae: 0.627715, mean_q: 5.192910
 34188/100000: episode: 936, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 188.750, mean reward: 1.888 [1.448, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.394, 10.199], loss: 3.120311, mae: 0.729311, mean_q: 5.317075
 34288/100000: episode: 937, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 200.733, mean reward: 2.007 [1.468, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.582, 10.098], loss: 5.155060, mae: 0.803355, mean_q: 5.157665
 34388/100000: episode: 938, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 211.266, mean reward: 2.113 [1.469, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.427, 10.353], loss: 2.014215, mae: 0.706727, mean_q: 5.134904
 34488/100000: episode: 939, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 184.480, mean reward: 1.845 [1.470, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.597, 10.098], loss: 4.608118, mae: 0.776700, mean_q: 5.029242
 34588/100000: episode: 940, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.795, mean reward: 1.918 [1.447, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.153], loss: 3.829480, mae: 0.676516, mean_q: 5.008996
 34688/100000: episode: 941, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 213.178, mean reward: 2.132 [1.436, 4.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.740, 10.098], loss: 3.050054, mae: 0.705837, mean_q: 4.909346
 34788/100000: episode: 942, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 202.610, mean reward: 2.026 [1.456, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.361], loss: 2.488813, mae: 0.668048, mean_q: 4.851049
 34888/100000: episode: 943, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 214.127, mean reward: 2.141 [1.478, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.025, 10.098], loss: 1.988290, mae: 0.585466, mean_q: 4.644101
 34988/100000: episode: 944, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 190.562, mean reward: 1.906 [1.480, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.401, 10.098], loss: 1.526162, mae: 0.570960, mean_q: 4.572462
 35088/100000: episode: 945, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.050, mean reward: 1.841 [1.457, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.901, 10.098], loss: 0.914704, mae: 0.432424, mean_q: 4.331213
 35188/100000: episode: 946, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 191.369, mean reward: 1.914 [1.468, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.771, 10.119], loss: 1.653774, mae: 0.411835, mean_q: 4.205040
 35288/100000: episode: 947, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 190.820, mean reward: 1.908 [1.472, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.931, 10.179], loss: 1.343866, mae: 0.375027, mean_q: 4.065766
 35388/100000: episode: 948, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.829, mean reward: 1.928 [1.433, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.364, 10.131], loss: 0.111403, mae: 0.305921, mean_q: 3.891416
 35488/100000: episode: 949, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.089, mean reward: 1.951 [1.474, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.334, 10.098], loss: 0.075859, mae: 0.277801, mean_q: 3.873429
 35588/100000: episode: 950, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 204.152, mean reward: 2.042 [1.450, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.453, 10.098], loss: 0.084598, mae: 0.283523, mean_q: 3.893286
 35688/100000: episode: 951, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 214.576, mean reward: 2.146 [1.495, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.098], loss: 0.082710, mae: 0.276362, mean_q: 3.870069
 35788/100000: episode: 952, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.985, mean reward: 1.880 [1.476, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.048, 10.198], loss: 0.076582, mae: 0.272843, mean_q: 3.854006
 35888/100000: episode: 953, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.545, mean reward: 1.825 [1.468, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.727, 10.133], loss: 0.066596, mae: 0.263453, mean_q: 3.856014
 35988/100000: episode: 954, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 182.484, mean reward: 1.825 [1.439, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.211, 10.183], loss: 0.081980, mae: 0.279525, mean_q: 3.859124
 36088/100000: episode: 955, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 208.595, mean reward: 2.086 [1.508, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.265, 10.098], loss: 0.073331, mae: 0.268453, mean_q: 3.864592
 36188/100000: episode: 956, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 201.687, mean reward: 2.017 [1.581, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.599, 10.279], loss: 0.074360, mae: 0.269703, mean_q: 3.865562
 36288/100000: episode: 957, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.899, mean reward: 1.889 [1.441, 5.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.574, 10.150], loss: 0.076701, mae: 0.282178, mean_q: 3.895427
 36388/100000: episode: 958, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.160, mean reward: 1.882 [1.462, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.696, 10.110], loss: 0.073342, mae: 0.273305, mean_q: 3.886573
 36488/100000: episode: 959, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.754, mean reward: 1.858 [1.436, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.045, 10.148], loss: 0.071688, mae: 0.273986, mean_q: 3.872237
 36588/100000: episode: 960, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 202.220, mean reward: 2.022 [1.455, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.485, 10.353], loss: 0.074444, mae: 0.278143, mean_q: 3.867218
 36688/100000: episode: 961, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.452, mean reward: 1.905 [1.490, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.075889, mae: 0.280118, mean_q: 3.885010
 36788/100000: episode: 962, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 198.415, mean reward: 1.984 [1.556, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.346, 10.098], loss: 0.074237, mae: 0.275215, mean_q: 3.885931
 36888/100000: episode: 963, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 178.694, mean reward: 1.787 [1.452, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.098], loss: 0.075900, mae: 0.281638, mean_q: 3.893523
 36988/100000: episode: 964, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 208.862, mean reward: 2.089 [1.479, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.996, 10.238], loss: 0.079605, mae: 0.277551, mean_q: 3.886693
 37088/100000: episode: 965, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 201.137, mean reward: 2.011 [1.513, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.372, 10.098], loss: 0.080304, mae: 0.285964, mean_q: 3.900444
 37188/100000: episode: 966, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 191.556, mean reward: 1.916 [1.474, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.783, 10.098], loss: 0.087712, mae: 0.290264, mean_q: 3.922249
 37288/100000: episode: 967, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 185.010, mean reward: 1.850 [1.481, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.496, 10.159], loss: 0.083064, mae: 0.288128, mean_q: 3.901066
 37388/100000: episode: 968, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 211.859, mean reward: 2.119 [1.470, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.175], loss: 0.073669, mae: 0.274355, mean_q: 3.886410
 37488/100000: episode: 969, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.707, mean reward: 1.917 [1.453, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.836, 10.252], loss: 0.083991, mae: 0.285608, mean_q: 3.884624
 37588/100000: episode: 970, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 210.076, mean reward: 2.101 [1.484, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.411, 10.144], loss: 0.079583, mae: 0.285758, mean_q: 3.902311
 37688/100000: episode: 971, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 205.877, mean reward: 2.059 [1.507, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.304], loss: 0.086147, mae: 0.284080, mean_q: 3.880554
 37788/100000: episode: 972, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.557, mean reward: 1.856 [1.473, 5.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.509, 10.112], loss: 0.077436, mae: 0.277951, mean_q: 3.887831
 37888/100000: episode: 973, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 198.897, mean reward: 1.989 [1.494, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.099], loss: 0.082960, mae: 0.289715, mean_q: 3.896765
 37988/100000: episode: 974, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 205.507, mean reward: 2.055 [1.503, 4.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.436, 10.098], loss: 0.078085, mae: 0.283409, mean_q: 3.901222
 38088/100000: episode: 975, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 212.499, mean reward: 2.125 [1.496, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.528, 10.098], loss: 0.085589, mae: 0.286168, mean_q: 3.898321
 38188/100000: episode: 976, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.304, mean reward: 1.933 [1.471, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.169, 10.174], loss: 0.074821, mae: 0.278325, mean_q: 3.891306
 38288/100000: episode: 977, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 205.836, mean reward: 2.058 [1.444, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.118], loss: 0.076843, mae: 0.273328, mean_q: 3.905902
 38388/100000: episode: 978, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 181.433, mean reward: 1.814 [1.455, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.000, 10.145], loss: 0.077352, mae: 0.278958, mean_q: 3.908670
 38488/100000: episode: 979, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.448, mean reward: 1.964 [1.449, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.195], loss: 0.078610, mae: 0.281123, mean_q: 3.916157
 38588/100000: episode: 980, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 230.263, mean reward: 2.303 [1.505, 6.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.098], loss: 0.086578, mae: 0.291309, mean_q: 3.904908
 38688/100000: episode: 981, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.240, mean reward: 1.842 [1.472, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.021, 10.098], loss: 0.088400, mae: 0.294874, mean_q: 3.904641
 38788/100000: episode: 982, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.996, mean reward: 1.930 [1.486, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.810, 10.310], loss: 0.088790, mae: 0.295976, mean_q: 3.922041
 38888/100000: episode: 983, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 216.386, mean reward: 2.164 [1.434, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.332, 10.098], loss: 0.087957, mae: 0.286514, mean_q: 3.906457
 38988/100000: episode: 984, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 232.512, mean reward: 2.325 [1.536, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.007, 10.098], loss: 0.088703, mae: 0.291487, mean_q: 3.920807
 39088/100000: episode: 985, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.164, mean reward: 1.882 [1.458, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.297, 10.191], loss: 0.084103, mae: 0.291285, mean_q: 3.921542
 39188/100000: episode: 986, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.569, mean reward: 1.856 [1.456, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.437, 10.283], loss: 0.073227, mae: 0.277213, mean_q: 3.897424
 39288/100000: episode: 987, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.319, mean reward: 1.953 [1.476, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.834, 10.108], loss: 0.080313, mae: 0.283102, mean_q: 3.892582
 39388/100000: episode: 988, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.899, mean reward: 1.859 [1.449, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.529, 10.108], loss: 0.081636, mae: 0.282499, mean_q: 3.888252
 39488/100000: episode: 989, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 203.489, mean reward: 2.035 [1.496, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.371, 10.098], loss: 0.083060, mae: 0.291843, mean_q: 3.909463
 39588/100000: episode: 990, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 197.400, mean reward: 1.974 [1.468, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.138], loss: 0.086828, mae: 0.291029, mean_q: 3.899075
 39688/100000: episode: 991, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 195.394, mean reward: 1.954 [1.455, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.545, 10.216], loss: 0.097370, mae: 0.298868, mean_q: 3.903816
 39788/100000: episode: 992, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 224.901, mean reward: 2.249 [1.484, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.496, 10.276], loss: 0.079083, mae: 0.282931, mean_q: 3.874316
 39888/100000: episode: 993, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 238.150, mean reward: 2.382 [1.456, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.734, 10.098], loss: 0.081720, mae: 0.289097, mean_q: 3.908869
 39988/100000: episode: 994, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.125, mean reward: 1.921 [1.463, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.893, 10.200], loss: 0.079269, mae: 0.284626, mean_q: 3.930254
 40088/100000: episode: 995, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 198.694, mean reward: 1.987 [1.449, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.118], loss: 0.088996, mae: 0.293597, mean_q: 3.918999
 40188/100000: episode: 996, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 219.607, mean reward: 2.196 [1.455, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.685, 10.411], loss: 0.094129, mae: 0.298358, mean_q: 3.930763
 40288/100000: episode: 997, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 205.449, mean reward: 2.054 [1.438, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.641, 10.098], loss: 0.083685, mae: 0.287988, mean_q: 3.940951
[Info] 1-TH LEVEL FOUND: 5.703334331512451, Considering 10/90 traces
 40388/100000: episode: 998, duration: 4.766s, episode steps: 100, steps per second: 21, episode reward: 183.170, mean reward: 1.832 [1.457, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.956, 10.098], loss: 0.091701, mae: 0.294158, mean_q: 3.950546
 40418/100000: episode: 999, duration: 0.147s, episode steps: 30, steps per second: 205, episode reward: 86.243, mean reward: 2.875 [1.933, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.092, 10.100], loss: 0.072295, mae: 0.275990, mean_q: 3.911584
 40439/100000: episode: 1000, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 48.993, mean reward: 2.333 [2.073, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.363, 10.100], loss: 0.101912, mae: 0.319854, mean_q: 4.004707
 40461/100000: episode: 1001, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 51.269, mean reward: 2.330 [1.857, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.226, 10.100], loss: 0.086218, mae: 0.275419, mean_q: 3.946247
 40487/100000: episode: 1002, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 53.501, mean reward: 2.058 [1.680, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.060, 10.100], loss: 0.072850, mae: 0.273244, mean_q: 3.935858
 40515/100000: episode: 1003, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 80.076, mean reward: 2.860 [2.171, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.261, 10.100], loss: 0.090082, mae: 0.297594, mean_q: 3.987133
 40553/100000: episode: 1004, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 89.340, mean reward: 2.351 [1.652, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.142, 10.100], loss: 0.078928, mae: 0.279263, mean_q: 3.947990
 40591/100000: episode: 1005, duration: 0.192s, episode steps: 38, steps per second: 197, episode reward: 88.545, mean reward: 2.330 [1.502, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.191], loss: 0.085164, mae: 0.298147, mean_q: 3.972037
 40618/100000: episode: 1006, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 56.987, mean reward: 2.111 [1.735, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.145, 10.100], loss: 0.093207, mae: 0.303948, mean_q: 4.020663
 40647/100000: episode: 1007, duration: 0.142s, episode steps: 29, steps per second: 205, episode reward: 63.146, mean reward: 2.177 [1.730, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.254, 10.100], loss: 0.097575, mae: 0.299968, mean_q: 3.944612
 40673/100000: episode: 1008, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 51.069, mean reward: 1.964 [1.451, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.732, 10.153], loss: 0.070168, mae: 0.283224, mean_q: 3.959425
 40700/100000: episode: 1009, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 64.617, mean reward: 2.393 [1.765, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.332, 10.100], loss: 0.078965, mae: 0.290750, mean_q: 3.956282
 40721/100000: episode: 1010, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 53.818, mean reward: 2.563 [2.121, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.520, 10.100], loss: 0.119298, mae: 0.327006, mean_q: 3.968143
 40742/100000: episode: 1011, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 57.052, mean reward: 2.717 [1.868, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.288, 10.100], loss: 0.098208, mae: 0.324870, mean_q: 3.976021
 40770/100000: episode: 1012, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 58.285, mean reward: 2.082 [1.618, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.876, 10.100], loss: 0.069865, mae: 0.271266, mean_q: 3.926155
 40792/100000: episode: 1013, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 52.037, mean reward: 2.365 [1.882, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.152, 10.100], loss: 0.092756, mae: 0.299040, mean_q: 3.995461
 40830/100000: episode: 1014, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 90.714, mean reward: 2.387 [1.818, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.607, 10.100], loss: 0.105669, mae: 0.310421, mean_q: 3.995421
 40858/100000: episode: 1015, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 67.998, mean reward: 2.429 [1.799, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.178, 10.100], loss: 0.076501, mae: 0.286504, mean_q: 3.977846
 40879/100000: episode: 1016, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 44.992, mean reward: 2.142 [1.728, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.183, 10.100], loss: 0.115649, mae: 0.339021, mean_q: 3.995186
 40906/100000: episode: 1017, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 73.284, mean reward: 2.714 [1.957, 7.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.234, 10.100], loss: 0.115471, mae: 0.337301, mean_q: 4.039055
 40934/100000: episode: 1018, duration: 0.140s, episode steps: 28, steps per second: 201, episode reward: 69.705, mean reward: 2.489 [1.503, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.609, 10.100], loss: 0.112525, mae: 0.311112, mean_q: 4.020076
 40956/100000: episode: 1019, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 48.408, mean reward: 2.200 [1.873, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.403, 10.100], loss: 0.091182, mae: 0.296940, mean_q: 4.025613
 40982/100000: episode: 1020, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 62.799, mean reward: 2.415 [1.870, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.377, 10.100], loss: 0.101075, mae: 0.307486, mean_q: 4.063787
 41004/100000: episode: 1021, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 62.481, mean reward: 2.840 [2.176, 6.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.425, 10.100], loss: 0.092823, mae: 0.290201, mean_q: 3.971157
 41034/100000: episode: 1022, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 86.829, mean reward: 2.894 [1.824, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.110, 10.100], loss: 0.085603, mae: 0.303907, mean_q: 4.049294
 41072/100000: episode: 1023, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 106.049, mean reward: 2.791 [2.317, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.272, 10.100], loss: 0.121128, mae: 0.332995, mean_q: 4.044766
 41099/100000: episode: 1024, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 76.338, mean reward: 2.827 [1.921, 5.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.188, 10.100], loss: 0.105046, mae: 0.313652, mean_q: 4.004510
 41127/100000: episode: 1025, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 66.883, mean reward: 2.389 [1.853, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.398, 10.100], loss: 0.118017, mae: 0.337339, mean_q: 4.080829
 41155/100000: episode: 1026, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 71.722, mean reward: 2.561 [1.765, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.001, 10.100], loss: 0.111123, mae: 0.321810, mean_q: 4.051484
 41185/100000: episode: 1027, duration: 0.150s, episode steps: 30, steps per second: 201, episode reward: 64.136, mean reward: 2.138 [1.596, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.283, 10.100], loss: 0.152266, mae: 0.355769, mean_q: 4.065759
 41211/100000: episode: 1028, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 73.705, mean reward: 2.835 [1.774, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.134, 10.100], loss: 0.097221, mae: 0.312598, mean_q: 4.028872
 41238/100000: episode: 1029, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 63.616, mean reward: 2.356 [1.742, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.380, 10.100], loss: 0.089026, mae: 0.301430, mean_q: 4.061794
 41268/100000: episode: 1030, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 68.485, mean reward: 2.283 [1.647, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.424, 10.100], loss: 0.088888, mae: 0.303941, mean_q: 4.059177
 41294/100000: episode: 1031, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 54.221, mean reward: 2.085 [1.588, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.190, 10.100], loss: 0.134171, mae: 0.334503, mean_q: 4.166503
 41324/100000: episode: 1032, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 78.183, mean reward: 2.606 [1.929, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.548, 10.100], loss: 0.103844, mae: 0.312455, mean_q: 4.078623
 41352/100000: episode: 1033, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 56.724, mean reward: 2.026 [1.443, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.811, 10.100], loss: 0.098577, mae: 0.312084, mean_q: 4.095799
 41373/100000: episode: 1034, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 42.899, mean reward: 2.043 [1.810, 2.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.086, 10.100], loss: 0.095967, mae: 0.316028, mean_q: 4.101576
 41400/100000: episode: 1035, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 67.317, mean reward: 2.493 [1.512, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.148], loss: 0.095755, mae: 0.304336, mean_q: 4.081746
 41430/100000: episode: 1036, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 74.741, mean reward: 2.491 [1.598, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.035, 10.100], loss: 0.109075, mae: 0.313041, mean_q: 4.136014
 41460/100000: episode: 1037, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 100.059, mean reward: 3.335 [2.336, 6.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.748, 10.100], loss: 0.107346, mae: 0.313444, mean_q: 4.125869
 41486/100000: episode: 1038, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 59.094, mean reward: 2.273 [1.714, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.229, 10.100], loss: 0.095947, mae: 0.314847, mean_q: 4.107952
 41513/100000: episode: 1039, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 61.578, mean reward: 2.281 [1.603, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.518, 10.100], loss: 0.096280, mae: 0.306305, mean_q: 4.128237
 41535/100000: episode: 1040, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 54.718, mean reward: 2.487 [2.102, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.389, 10.100], loss: 0.117226, mae: 0.335931, mean_q: 4.136028
 41561/100000: episode: 1041, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 71.619, mean reward: 2.755 [2.316, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.321, 10.100], loss: 0.108357, mae: 0.328527, mean_q: 4.128113
 41591/100000: episode: 1042, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 78.495, mean reward: 2.617 [1.985, 5.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.243, 10.100], loss: 0.115443, mae: 0.338591, mean_q: 4.181466
 41621/100000: episode: 1043, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 84.916, mean reward: 2.831 [1.936, 5.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.367, 10.100], loss: 0.110781, mae: 0.321140, mean_q: 4.170068
 41651/100000: episode: 1044, duration: 0.157s, episode steps: 30, steps per second: 192, episode reward: 122.187, mean reward: 4.073 [2.376, 7.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.468, 10.100], loss: 0.113248, mae: 0.330314, mean_q: 4.170794
 41681/100000: episode: 1045, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 80.309, mean reward: 2.677 [1.829, 5.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.265, 10.100], loss: 0.135715, mae: 0.343812, mean_q: 4.204644
 41702/100000: episode: 1046, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 57.094, mean reward: 2.719 [2.081, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.638, 10.100], loss: 0.126228, mae: 0.339624, mean_q: 4.162421
 41723/100000: episode: 1047, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 45.680, mean reward: 2.175 [1.731, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.159, 10.100], loss: 0.110213, mae: 0.337023, mean_q: 4.168303
 41749/100000: episode: 1048, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 124.633, mean reward: 4.794 [2.497, 12.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.900, 10.100], loss: 0.122898, mae: 0.352166, mean_q: 4.212542
 41776/100000: episode: 1049, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 61.446, mean reward: 2.276 [1.793, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.032, 10.100], loss: 0.182455, mae: 0.348842, mean_q: 4.172031
 41804/100000: episode: 1050, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 74.967, mean reward: 2.677 [2.110, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.898, 10.100], loss: 0.125532, mae: 0.333223, mean_q: 4.280366
 41833/100000: episode: 1051, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 76.280, mean reward: 2.630 [1.730, 5.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.297, 10.100], loss: 0.210027, mae: 0.375040, mean_q: 4.223799
 41859/100000: episode: 1052, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 75.711, mean reward: 2.912 [1.905, 6.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.867, 10.100], loss: 0.138895, mae: 0.348245, mean_q: 4.259617
 41889/100000: episode: 1053, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 81.180, mean reward: 2.706 [2.007, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.315, 10.100], loss: 0.144374, mae: 0.361586, mean_q: 4.304025
 41918/100000: episode: 1054, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 64.901, mean reward: 2.238 [1.780, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.923, 10.100], loss: 0.139204, mae: 0.355204, mean_q: 4.201529
 41939/100000: episode: 1055, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 46.171, mean reward: 2.199 [1.536, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.106, 10.102], loss: 0.186702, mae: 0.374699, mean_q: 4.324430
 41967/100000: episode: 1056, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 64.990, mean reward: 2.321 [1.814, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.220, 10.100], loss: 0.137936, mae: 0.358440, mean_q: 4.242087
 41989/100000: episode: 1057, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 42.496, mean reward: 1.932 [1.614, 2.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.123, 10.100], loss: 0.143071, mae: 0.372894, mean_q: 4.298010
 42011/100000: episode: 1058, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 54.884, mean reward: 2.495 [1.830, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.389, 10.100], loss: 0.137416, mae: 0.342731, mean_q: 4.275134
 42040/100000: episode: 1059, duration: 0.155s, episode steps: 29, steps per second: 188, episode reward: 77.934, mean reward: 2.687 [1.644, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.311, 10.100], loss: 0.126136, mae: 0.352989, mean_q: 4.261983
 42069/100000: episode: 1060, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 84.840, mean reward: 2.926 [2.018, 5.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.224, 10.100], loss: 0.174102, mae: 0.366426, mean_q: 4.359019
 42099/100000: episode: 1061, duration: 0.143s, episode steps: 30, steps per second: 210, episode reward: 70.271, mean reward: 2.342 [1.636, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.134, 10.100], loss: 0.147716, mae: 0.354510, mean_q: 4.249770
 42125/100000: episode: 1062, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 62.585, mean reward: 2.407 [1.771, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.125, 10.100], loss: 0.165034, mae: 0.375287, mean_q: 4.325613
 42147/100000: episode: 1063, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 57.997, mean reward: 2.636 [1.941, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.271, 10.100], loss: 0.180229, mae: 0.339643, mean_q: 4.298988
 42175/100000: episode: 1064, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 68.285, mean reward: 2.439 [2.037, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.654, 10.100], loss: 0.148684, mae: 0.355933, mean_q: 4.306138
 42203/100000: episode: 1065, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 71.013, mean reward: 2.536 [2.134, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.491, 10.100], loss: 0.159180, mae: 0.376336, mean_q: 4.326499
 42231/100000: episode: 1066, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 77.976, mean reward: 2.785 [2.113, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.341, 10.100], loss: 0.135554, mae: 0.347923, mean_q: 4.287065
 42257/100000: episode: 1067, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 54.484, mean reward: 2.096 [1.593, 5.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.841, 10.100], loss: 0.151827, mae: 0.379343, mean_q: 4.360191
 42285/100000: episode: 1068, duration: 0.133s, episode steps: 28, steps per second: 210, episode reward: 69.623, mean reward: 2.487 [1.996, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.691, 10.100], loss: 0.115803, mae: 0.335198, mean_q: 4.260842
 42311/100000: episode: 1069, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 88.839, mean reward: 3.417 [2.162, 7.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.278, 10.100], loss: 0.148602, mae: 0.359478, mean_q: 4.329714
 42333/100000: episode: 1070, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 45.559, mean reward: 2.071 [1.771, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.422, 10.100], loss: 0.146887, mae: 0.353805, mean_q: 4.419531
 42363/100000: episode: 1071, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 66.461, mean reward: 2.215 [1.518, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.420, 10.100], loss: 0.165849, mae: 0.346117, mean_q: 4.326673
 42385/100000: episode: 1072, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 51.616, mean reward: 2.346 [1.634, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.218, 10.100], loss: 0.117842, mae: 0.345099, mean_q: 4.361671
 42413/100000: episode: 1073, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 69.578, mean reward: 2.485 [1.789, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.303, 10.100], loss: 0.119945, mae: 0.323332, mean_q: 4.327193
 42435/100000: episode: 1074, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 59.064, mean reward: 2.685 [1.777, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.382, 10.100], loss: 0.134497, mae: 0.357055, mean_q: 4.362060
 42463/100000: episode: 1075, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 78.773, mean reward: 2.813 [2.044, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.488, 10.100], loss: 0.119847, mae: 0.326191, mean_q: 4.333188
 42491/100000: episode: 1076, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 61.116, mean reward: 2.183 [1.876, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.755, 10.100], loss: 0.178358, mae: 0.396264, mean_q: 4.405869
 42517/100000: episode: 1077, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 73.390, mean reward: 2.823 [2.211, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.936, 10.100], loss: 0.127214, mae: 0.350716, mean_q: 4.356747
 42539/100000: episode: 1078, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 56.808, mean reward: 2.582 [2.089, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.066, 10.100], loss: 0.163322, mae: 0.355637, mean_q: 4.277823
 42566/100000: episode: 1079, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 73.898, mean reward: 2.737 [2.323, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.412, 10.100], loss: 0.127732, mae: 0.347086, mean_q: 4.386742
 42596/100000: episode: 1080, duration: 0.158s, episode steps: 30, steps per second: 189, episode reward: 85.217, mean reward: 2.841 [1.660, 11.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.495, 10.100], loss: 0.122916, mae: 0.339916, mean_q: 4.372197
 42624/100000: episode: 1081, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 68.784, mean reward: 2.457 [1.986, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.131, 10.100], loss: 0.158852, mae: 0.364733, mean_q: 4.421940
[Info] FALSIFICATION!
[Info] Levels: [5.7033343, 7.292656]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 42650/100000: episode: 1082, duration: 4.491s, episode steps: 26, steps per second: 6, episode reward: 254.292, mean reward: 9.780 [2.643, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.757, 9.756], loss: 0.131418, mae: 0.344116, mean_q: 4.309170
 42750/100000: episode: 1083, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 219.219, mean reward: 2.192 [1.444, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.371, 10.098], loss: 1.640061, mae: 0.470707, mean_q: 4.446725
 42850/100000: episode: 1084, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 187.989, mean reward: 1.880 [1.443, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.024, 10.098], loss: 0.236520, mae: 0.376581, mean_q: 4.464551
 42950/100000: episode: 1085, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 229.214, mean reward: 2.292 [1.456, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.884, 10.507], loss: 1.595999, mae: 0.470285, mean_q: 4.496498
 43050/100000: episode: 1086, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 204.614, mean reward: 2.046 [1.463, 5.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.070, 10.417], loss: 1.569239, mae: 0.458226, mean_q: 4.506910
 43150/100000: episode: 1087, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.096, mean reward: 1.931 [1.439, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.323], loss: 1.529502, mae: 0.459782, mean_q: 4.471189
 43250/100000: episode: 1088, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 196.084, mean reward: 1.961 [1.453, 5.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.455, 10.098], loss: 0.240906, mae: 0.398745, mean_q: 4.472840
 43350/100000: episode: 1089, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 214.671, mean reward: 2.147 [1.495, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.391], loss: 1.639657, mae: 0.467416, mean_q: 4.494650
 43450/100000: episode: 1090, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 182.395, mean reward: 1.824 [1.473, 2.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.411, 10.210], loss: 1.619201, mae: 0.475851, mean_q: 4.524676
 43550/100000: episode: 1091, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.309, mean reward: 1.843 [1.443, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.472, 10.155], loss: 1.576730, mae: 0.475475, mean_q: 4.462415
 43650/100000: episode: 1092, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 220.231, mean reward: 2.202 [1.451, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.432], loss: 0.187683, mae: 0.381739, mean_q: 4.482660
 43750/100000: episode: 1093, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 183.174, mean reward: 1.832 [1.451, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.161], loss: 0.287575, mae: 0.411158, mean_q: 4.509770
 43850/100000: episode: 1094, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.850, mean reward: 1.928 [1.465, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.224, 10.098], loss: 1.573942, mae: 0.474050, mean_q: 4.510136
 43950/100000: episode: 1095, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 184.877, mean reward: 1.849 [1.463, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.240, 10.098], loss: 0.345212, mae: 0.448617, mean_q: 4.443841
 44050/100000: episode: 1096, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.660, mean reward: 1.837 [1.468, 2.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.246, 10.234], loss: 5.489462, mae: 0.625132, mean_q: 4.560791
 44150/100000: episode: 1097, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 182.218, mean reward: 1.822 [1.498, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.791, 10.107], loss: 1.643083, mae: 0.509940, mean_q: 4.501272
 44250/100000: episode: 1098, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 189.905, mean reward: 1.899 [1.444, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.098], loss: 2.822132, mae: 0.535565, mean_q: 4.525132
 44350/100000: episode: 1099, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 194.659, mean reward: 1.947 [1.473, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.177], loss: 0.189710, mae: 0.400675, mean_q: 4.413849
 44450/100000: episode: 1100, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 194.258, mean reward: 1.943 [1.456, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.404, 10.098], loss: 0.255597, mae: 0.408271, mean_q: 4.444366
 44550/100000: episode: 1101, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.797, mean reward: 1.988 [1.482, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.514, 10.241], loss: 1.540063, mae: 0.454564, mean_q: 4.435260
 44650/100000: episode: 1102, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.916, mean reward: 1.869 [1.456, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.498, 10.176], loss: 0.238000, mae: 0.411922, mean_q: 4.468354
 44750/100000: episode: 1103, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.300, mean reward: 1.913 [1.458, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.839, 10.281], loss: 2.836022, mae: 0.541043, mean_q: 4.486151
 44850/100000: episode: 1104, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.687, mean reward: 1.937 [1.459, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.622, 10.195], loss: 1.462890, mae: 0.453560, mean_q: 4.410081
 44950/100000: episode: 1105, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 178.496, mean reward: 1.785 [1.445, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.885, 10.098], loss: 2.822870, mae: 0.549515, mean_q: 4.460749
 45050/100000: episode: 1106, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 195.259, mean reward: 1.953 [1.455, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.493, 10.098], loss: 0.192810, mae: 0.389394, mean_q: 4.377337
 45150/100000: episode: 1107, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 193.802, mean reward: 1.938 [1.447, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.511, 10.300], loss: 0.164638, mae: 0.375019, mean_q: 4.374107
 45250/100000: episode: 1108, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 196.582, mean reward: 1.966 [1.451, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.308, 10.118], loss: 0.269613, mae: 0.387520, mean_q: 4.387290
 45350/100000: episode: 1109, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 213.355, mean reward: 2.134 [1.491, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.098], loss: 0.251014, mae: 0.417496, mean_q: 4.376373
 45450/100000: episode: 1110, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.325, mean reward: 1.943 [1.493, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.130], loss: 0.355138, mae: 0.426439, mean_q: 4.404325
 45550/100000: episode: 1111, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 179.749, mean reward: 1.797 [1.444, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.245, 10.098], loss: 1.463193, mae: 0.441663, mean_q: 4.409274
 45650/100000: episode: 1112, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.954, mean reward: 1.860 [1.471, 4.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.552, 10.124], loss: 0.256238, mae: 0.428458, mean_q: 4.345711
 45750/100000: episode: 1113, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 200.986, mean reward: 2.010 [1.481, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.615, 10.098], loss: 0.198074, mae: 0.390360, mean_q: 4.321736
 45850/100000: episode: 1114, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 188.133, mean reward: 1.881 [1.435, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.408, 10.098], loss: 0.180981, mae: 0.367186, mean_q: 4.299128
 45950/100000: episode: 1115, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 195.234, mean reward: 1.952 [1.499, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.815, 10.098], loss: 0.215519, mae: 0.378673, mean_q: 4.304967
 46050/100000: episode: 1116, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 203.008, mean reward: 2.030 [1.462, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.479, 10.098], loss: 0.220270, mae: 0.379922, mean_q: 4.282206
 46150/100000: episode: 1117, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 221.851, mean reward: 2.219 [1.490, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.284, 10.354], loss: 0.153158, mae: 0.353716, mean_q: 4.219400
 46250/100000: episode: 1118, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 193.056, mean reward: 1.931 [1.455, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.395, 10.183], loss: 0.188728, mae: 0.372878, mean_q: 4.231976
 46350/100000: episode: 1119, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.222, mean reward: 1.922 [1.469, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.624, 10.141], loss: 0.245437, mae: 0.384515, mean_q: 4.209860
 46450/100000: episode: 1120, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 183.465, mean reward: 1.835 [1.474, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.576, 10.098], loss: 1.466591, mae: 0.459076, mean_q: 4.182140
 46550/100000: episode: 1121, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 183.470, mean reward: 1.835 [1.437, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.157, 10.211], loss: 0.257106, mae: 0.389618, mean_q: 4.176085
 46650/100000: episode: 1122, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 200.124, mean reward: 2.001 [1.462, 5.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.489, 10.098], loss: 0.169691, mae: 0.342295, mean_q: 4.103135
 46750/100000: episode: 1123, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.787, mean reward: 1.918 [1.491, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.707, 10.098], loss: 0.147347, mae: 0.339319, mean_q: 4.074552
 46850/100000: episode: 1124, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 201.834, mean reward: 2.018 [1.472, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.169, 10.103], loss: 0.148819, mae: 0.330766, mean_q: 4.044873
 46950/100000: episode: 1125, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.148, mean reward: 1.911 [1.472, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.414, 10.098], loss: 0.214527, mae: 0.353495, mean_q: 4.016459
 47050/100000: episode: 1126, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 190.472, mean reward: 1.905 [1.487, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.233, 10.098], loss: 1.426735, mae: 0.430303, mean_q: 4.038949
 47150/100000: episode: 1127, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 194.190, mean reward: 1.942 [1.440, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.932, 10.098], loss: 0.134736, mae: 0.333227, mean_q: 3.973070
 47250/100000: episode: 1128, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.813, mean reward: 1.858 [1.440, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.681, 10.157], loss: 0.114424, mae: 0.317266, mean_q: 3.972709
 47350/100000: episode: 1129, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 213.827, mean reward: 2.138 [1.490, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.098], loss: 1.518019, mae: 0.441752, mean_q: 4.010940
 47450/100000: episode: 1130, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 194.851, mean reward: 1.949 [1.444, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.533, 10.098], loss: 1.344985, mae: 0.373539, mean_q: 3.961462
 47550/100000: episode: 1131, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.958, mean reward: 1.920 [1.503, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.098], loss: 1.438417, mae: 0.476758, mean_q: 3.960046
 47650/100000: episode: 1132, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 199.013, mean reward: 1.990 [1.449, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.602, 10.098], loss: 0.103176, mae: 0.314414, mean_q: 3.867899
 47750/100000: episode: 1133, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.719, mean reward: 1.877 [1.454, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.256], loss: 0.092607, mae: 0.297228, mean_q: 3.837752
 47850/100000: episode: 1134, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 227.085, mean reward: 2.271 [1.499, 7.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.088, 10.139], loss: 0.097478, mae: 0.307537, mean_q: 3.860912
 47950/100000: episode: 1135, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 184.531, mean reward: 1.845 [1.509, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.118, 10.137], loss: 0.094513, mae: 0.300079, mean_q: 3.855314
 48050/100000: episode: 1136, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 198.850, mean reward: 1.988 [1.444, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.392, 10.098], loss: 0.079575, mae: 0.288162, mean_q: 3.841391
 48150/100000: episode: 1137, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 230.278, mean reward: 2.303 [1.456, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.460], loss: 0.102130, mae: 0.309922, mean_q: 3.849995
 48250/100000: episode: 1138, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.289, mean reward: 1.813 [1.456, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.194], loss: 0.088629, mae: 0.292001, mean_q: 3.859841
 48350/100000: episode: 1139, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 178.974, mean reward: 1.790 [1.429, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.417, 10.223], loss: 0.085507, mae: 0.290324, mean_q: 3.847608
 48450/100000: episode: 1140, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 241.698, mean reward: 2.417 [1.578, 6.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.299, 10.098], loss: 0.093583, mae: 0.297503, mean_q: 3.844923
 48550/100000: episode: 1141, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.370, mean reward: 1.914 [1.502, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.498, 10.304], loss: 0.084612, mae: 0.287235, mean_q: 3.846733
 48650/100000: episode: 1142, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 192.920, mean reward: 1.929 [1.463, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.912, 10.098], loss: 0.084824, mae: 0.292236, mean_q: 3.826567
 48750/100000: episode: 1143, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 180.973, mean reward: 1.810 [1.488, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.098], loss: 0.102808, mae: 0.310576, mean_q: 3.863602
 48850/100000: episode: 1144, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.877, mean reward: 1.939 [1.475, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.439, 10.098], loss: 0.080009, mae: 0.284186, mean_q: 3.853391
 48950/100000: episode: 1145, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.195, mean reward: 1.842 [1.443, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.070, 10.098], loss: 0.082897, mae: 0.292563, mean_q: 3.845944
 49050/100000: episode: 1146, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 219.072, mean reward: 2.191 [1.488, 5.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.098], loss: 0.096302, mae: 0.303434, mean_q: 3.864884
 49150/100000: episode: 1147, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 191.152, mean reward: 1.912 [1.451, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.070, 10.098], loss: 0.114983, mae: 0.320807, mean_q: 3.874738
 49250/100000: episode: 1148, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 190.155, mean reward: 1.902 [1.479, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.604, 10.234], loss: 0.089102, mae: 0.295784, mean_q: 3.857347
 49350/100000: episode: 1149, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.672, mean reward: 1.827 [1.485, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.410, 10.098], loss: 0.092007, mae: 0.293353, mean_q: 3.849944
 49450/100000: episode: 1150, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 186.768, mean reward: 1.868 [1.477, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.068, 10.142], loss: 0.105548, mae: 0.316685, mean_q: 3.881674
 49550/100000: episode: 1151, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.551, mean reward: 1.856 [1.441, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.916, 10.098], loss: 0.090366, mae: 0.290957, mean_q: 3.847425
 49650/100000: episode: 1152, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 185.728, mean reward: 1.857 [1.451, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.556, 10.098], loss: 0.111161, mae: 0.316650, mean_q: 3.868190
 49750/100000: episode: 1153, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.131, mean reward: 1.891 [1.469, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.105], loss: 0.096663, mae: 0.307817, mean_q: 3.874846
 49850/100000: episode: 1154, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 197.003, mean reward: 1.970 [1.513, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.098], loss: 0.101012, mae: 0.303046, mean_q: 3.854751
 49950/100000: episode: 1155, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.865, mean reward: 1.969 [1.466, 5.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.910, 10.147], loss: 0.090711, mae: 0.297756, mean_q: 3.854673
 50050/100000: episode: 1156, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.054, mean reward: 1.971 [1.484, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.235, 10.315], loss: 0.105355, mae: 0.303776, mean_q: 3.876677
 50150/100000: episode: 1157, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 194.956, mean reward: 1.950 [1.448, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.804, 10.098], loss: 0.097773, mae: 0.300745, mean_q: 3.858727
 50250/100000: episode: 1158, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 201.145, mean reward: 2.011 [1.458, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.505, 10.206], loss: 0.101049, mae: 0.308235, mean_q: 3.856999
 50350/100000: episode: 1159, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 182.997, mean reward: 1.830 [1.445, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.210, 10.098], loss: 0.088820, mae: 0.294347, mean_q: 3.855579
 50450/100000: episode: 1160, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 183.887, mean reward: 1.839 [1.440, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.098], loss: 0.090432, mae: 0.297496, mean_q: 3.841887
 50550/100000: episode: 1161, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.880, mean reward: 1.969 [1.482, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.975, 10.098], loss: 0.101247, mae: 0.290096, mean_q: 3.841988
 50650/100000: episode: 1162, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.576, mean reward: 1.816 [1.451, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.250], loss: 0.082904, mae: 0.280267, mean_q: 3.853123
 50750/100000: episode: 1163, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 197.999, mean reward: 1.980 [1.480, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.594, 10.098], loss: 0.075743, mae: 0.282422, mean_q: 3.846358
 50850/100000: episode: 1164, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 217.261, mean reward: 2.173 [1.461, 6.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.627, 10.421], loss: 0.096364, mae: 0.299845, mean_q: 3.859275
 50950/100000: episode: 1165, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.026, mean reward: 1.810 [1.477, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.559, 10.130], loss: 0.084703, mae: 0.285828, mean_q: 3.850844
 51050/100000: episode: 1166, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 222.959, mean reward: 2.230 [1.492, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.742, 10.433], loss: 0.113694, mae: 0.310369, mean_q: 3.864480
 51150/100000: episode: 1167, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 179.401, mean reward: 1.794 [1.449, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.795, 10.179], loss: 0.108124, mae: 0.300418, mean_q: 3.871169
 51250/100000: episode: 1168, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.198, mean reward: 1.862 [1.460, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.237, 10.098], loss: 0.105180, mae: 0.302138, mean_q: 3.862959
 51350/100000: episode: 1169, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.937, mean reward: 1.819 [1.466, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.278, 10.098], loss: 0.086026, mae: 0.287049, mean_q: 3.835551
 51450/100000: episode: 1170, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.187, mean reward: 1.862 [1.456, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.621, 10.170], loss: 0.087174, mae: 0.290346, mean_q: 3.859439
 51550/100000: episode: 1171, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.393, mean reward: 1.904 [1.472, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.619, 10.115], loss: 0.079794, mae: 0.278353, mean_q: 3.822800
 51650/100000: episode: 1172, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.621, mean reward: 1.986 [1.474, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.160], loss: 0.079831, mae: 0.282026, mean_q: 3.820973
 51750/100000: episode: 1173, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 193.952, mean reward: 1.940 [1.449, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.124, 10.110], loss: 0.094536, mae: 0.294673, mean_q: 3.854241
 51850/100000: episode: 1174, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.935, mean reward: 1.899 [1.458, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.282, 10.098], loss: 0.086391, mae: 0.294194, mean_q: 3.854033
 51950/100000: episode: 1175, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 193.178, mean reward: 1.932 [1.530, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.098], loss: 0.092115, mae: 0.287872, mean_q: 3.838450
 52050/100000: episode: 1176, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 200.554, mean reward: 2.006 [1.464, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.064, 10.098], loss: 0.093514, mae: 0.294449, mean_q: 3.858531
 52150/100000: episode: 1177, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 188.324, mean reward: 1.883 [1.441, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.534, 10.117], loss: 0.087183, mae: 0.290671, mean_q: 3.854471
 52250/100000: episode: 1178, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 177.280, mean reward: 1.773 [1.441, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.642, 10.116], loss: 0.084476, mae: 0.287119, mean_q: 3.849451
 52350/100000: episode: 1179, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.825, mean reward: 1.898 [1.460, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.543, 10.280], loss: 0.080715, mae: 0.283460, mean_q: 3.840647
 52450/100000: episode: 1180, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 197.575, mean reward: 1.976 [1.471, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.468, 10.098], loss: 0.086665, mae: 0.280256, mean_q: 3.828924
 52550/100000: episode: 1181, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 194.386, mean reward: 1.944 [1.459, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.761, 10.098], loss: 0.083803, mae: 0.288639, mean_q: 3.826307
[Info] 1-TH LEVEL FOUND: 5.410340309143066, Considering 10/90 traces
 52650/100000: episode: 1182, duration: 4.710s, episode steps: 100, steps per second: 21, episode reward: 261.819, mean reward: 2.618 [1.463, 8.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.361, 10.098], loss: 0.092234, mae: 0.288398, mean_q: 3.857428
 52711/100000: episode: 1183, duration: 0.324s, episode steps: 61, steps per second: 188, episode reward: 126.588, mean reward: 2.075 [1.519, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-1.775, 10.236], loss: 0.090670, mae: 0.290343, mean_q: 3.879326
 52725/100000: episode: 1184, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 34.723, mean reward: 2.480 [1.974, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.313], loss: 0.081950, mae: 0.281731, mean_q: 3.829598
 52752/100000: episode: 1185, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 74.065, mean reward: 2.743 [1.768, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.831, 10.274], loss: 0.075038, mae: 0.277732, mean_q: 3.852273
 52758/100000: episode: 1186, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 15.270, mean reward: 2.545 [2.338, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.354], loss: 0.171240, mae: 0.346373, mean_q: 3.899115
 52820/100000: episode: 1187, duration: 0.313s, episode steps: 62, steps per second: 198, episode reward: 145.164, mean reward: 2.341 [1.763, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.785 [-1.723, 10.100], loss: 0.090329, mae: 0.293457, mean_q: 3.870984
 52826/100000: episode: 1188, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 17.643, mean reward: 2.940 [2.746, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.499], loss: 0.058668, mae: 0.256106, mean_q: 3.731616
 52832/100000: episode: 1189, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 13.568, mean reward: 2.261 [2.136, 2.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.377], loss: 0.124754, mae: 0.306840, mean_q: 3.914005
 52856/100000: episode: 1190, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 54.773, mean reward: 2.282 [1.833, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.180, 10.100], loss: 0.085767, mae: 0.305482, mean_q: 3.897954
 52909/100000: episode: 1191, duration: 0.263s, episode steps: 53, steps per second: 202, episode reward: 130.629, mean reward: 2.465 [1.838, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.337, 10.100], loss: 0.099906, mae: 0.301431, mean_q: 3.873759
 52971/100000: episode: 1192, duration: 0.296s, episode steps: 62, steps per second: 210, episode reward: 130.202, mean reward: 2.100 [1.527, 5.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.422, 10.265], loss: 0.085709, mae: 0.292202, mean_q: 3.881389
 53033/100000: episode: 1193, duration: 0.309s, episode steps: 62, steps per second: 201, episode reward: 116.458, mean reward: 1.878 [1.461, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-0.451, 10.152], loss: 0.091096, mae: 0.291032, mean_q: 3.902068
 53094/100000: episode: 1194, duration: 0.310s, episode steps: 61, steps per second: 197, episode reward: 114.934, mean reward: 1.884 [1.475, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.742, 10.100], loss: 0.090144, mae: 0.295003, mean_q: 3.873292
 53129/100000: episode: 1195, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 119.922, mean reward: 3.426 [2.276, 9.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.351, 10.100], loss: 0.099605, mae: 0.300211, mean_q: 3.886769
 53135/100000: episode: 1196, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 16.221, mean reward: 2.704 [2.388, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.479], loss: 0.113527, mae: 0.335197, mean_q: 3.909348
 53166/100000: episode: 1197, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 93.000, mean reward: 3.000 [2.252, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.818, 10.100], loss: 0.109576, mae: 0.305662, mean_q: 3.841038
 53172/100000: episode: 1198, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 15.229, mean reward: 2.538 [2.129, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.397], loss: 0.066884, mae: 0.271253, mean_q: 3.918849
 53196/100000: episode: 1199, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 56.207, mean reward: 2.342 [1.582, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.534, 10.100], loss: 0.076266, mae: 0.281941, mean_q: 3.888739
 53210/100000: episode: 1200, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 43.391, mean reward: 3.099 [2.388, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.604, 10.484], loss: 0.130274, mae: 0.343730, mean_q: 4.005077
 53271/100000: episode: 1201, duration: 0.308s, episode steps: 61, steps per second: 198, episode reward: 148.800, mean reward: 2.439 [1.501, 4.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.667, 10.100], loss: 0.103298, mae: 0.305172, mean_q: 3.910243
 53333/100000: episode: 1202, duration: 0.309s, episode steps: 62, steps per second: 201, episode reward: 145.300, mean reward: 2.344 [1.739, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.321, 10.100], loss: 0.106583, mae: 0.307563, mean_q: 3.903847
 53364/100000: episode: 1203, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 76.145, mean reward: 2.456 [1.834, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.208, 10.100], loss: 0.089057, mae: 0.292003, mean_q: 3.934160
 53380/100000: episode: 1204, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 44.580, mean reward: 2.786 [2.053, 5.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.299, 10.100], loss: 0.080037, mae: 0.293483, mean_q: 3.947913
 53442/100000: episode: 1205, duration: 0.319s, episode steps: 62, steps per second: 194, episode reward: 128.320, mean reward: 2.070 [1.476, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-0.602, 10.100], loss: 0.095315, mae: 0.302297, mean_q: 3.938370
 53448/100000: episode: 1206, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 16.863, mean reward: 2.810 [2.325, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.481], loss: 0.127527, mae: 0.374576, mean_q: 4.023397
 53479/100000: episode: 1207, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 81.664, mean reward: 2.634 [1.959, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.330, 10.100], loss: 0.097694, mae: 0.313255, mean_q: 3.952283
 53510/100000: episode: 1208, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 89.867, mean reward: 2.899 [2.077, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.247, 10.100], loss: 0.072600, mae: 0.283707, mean_q: 3.991914
 53563/100000: episode: 1209, duration: 0.252s, episode steps: 53, steps per second: 210, episode reward: 105.101, mean reward: 1.983 [1.464, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.951, 10.160], loss: 0.092632, mae: 0.295012, mean_q: 3.958565
 53594/100000: episode: 1210, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 91.743, mean reward: 2.959 [1.650, 6.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.174, 10.100], loss: 0.109284, mae: 0.296157, mean_q: 3.921101
 53618/100000: episode: 1211, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 63.757, mean reward: 2.657 [1.767, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.953, 10.100], loss: 0.117045, mae: 0.314193, mean_q: 3.975671
 53632/100000: episode: 1212, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 53.435, mean reward: 3.817 [2.807, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.529], loss: 0.072745, mae: 0.275842, mean_q: 3.961945
 53656/100000: episode: 1213, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 60.759, mean reward: 2.532 [1.804, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.982, 10.100], loss: 0.097177, mae: 0.301404, mean_q: 4.011558
 53680/100000: episode: 1214, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 65.112, mean reward: 2.713 [1.663, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.614, 10.100], loss: 0.095047, mae: 0.302505, mean_q: 4.013226
 53707/100000: episode: 1215, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 70.266, mean reward: 2.602 [2.146, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.827, 10.434], loss: 0.141686, mae: 0.338538, mean_q: 3.994836
 53723/100000: episode: 1216, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 45.156, mean reward: 2.822 [2.183, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.196, 10.100], loss: 0.106667, mae: 0.328763, mean_q: 3.992459
 53785/100000: episode: 1217, duration: 0.323s, episode steps: 62, steps per second: 192, episode reward: 144.914, mean reward: 2.337 [1.733, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.825, 10.342], loss: 0.118549, mae: 0.334104, mean_q: 4.020005
 53809/100000: episode: 1218, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 64.078, mean reward: 2.670 [1.903, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.564, 10.100], loss: 0.096138, mae: 0.295403, mean_q: 4.029076
 53836/100000: episode: 1219, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 64.583, mean reward: 2.392 [1.538, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.329, 10.131], loss: 0.132496, mae: 0.328082, mean_q: 4.017875
 53897/100000: episode: 1220, duration: 0.307s, episode steps: 61, steps per second: 199, episode reward: 113.764, mean reward: 1.865 [1.456, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.817 [-0.910, 10.135], loss: 0.124741, mae: 0.319721, mean_q: 4.005359
 53959/100000: episode: 1221, duration: 0.314s, episode steps: 62, steps per second: 197, episode reward: 126.456, mean reward: 2.040 [1.633, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-0.545, 10.176], loss: 0.130911, mae: 0.343272, mean_q: 4.040328
 53986/100000: episode: 1222, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 77.437, mean reward: 2.868 [2.245, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.377], loss: 0.101152, mae: 0.309178, mean_q: 4.070214
 54013/100000: episode: 1223, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 73.977, mean reward: 2.740 [2.133, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.530, 10.284], loss: 0.084036, mae: 0.292890, mean_q: 4.001623
 54066/100000: episode: 1224, duration: 0.266s, episode steps: 53, steps per second: 199, episode reward: 132.403, mean reward: 2.498 [1.479, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.486, 10.118], loss: 0.104138, mae: 0.307105, mean_q: 4.035076
 54082/100000: episode: 1225, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 50.697, mean reward: 3.169 [2.331, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.972, 10.100], loss: 0.133358, mae: 0.325570, mean_q: 4.092132
 54098/100000: episode: 1226, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 52.407, mean reward: 3.275 [2.285, 4.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.671, 10.100], loss: 0.128563, mae: 0.336092, mean_q: 4.057238
 54160/100000: episode: 1227, duration: 0.303s, episode steps: 62, steps per second: 205, episode reward: 143.154, mean reward: 2.309 [1.687, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-1.551, 10.100], loss: 0.095334, mae: 0.307811, mean_q: 4.032609
 54187/100000: episode: 1228, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 113.661, mean reward: 4.210 [2.162, 8.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.041, 10.434], loss: 0.123199, mae: 0.311820, mean_q: 4.037215
 54249/100000: episode: 1229, duration: 0.313s, episode steps: 62, steps per second: 198, episode reward: 118.161, mean reward: 1.906 [1.465, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.330, 10.100], loss: 0.129114, mae: 0.327649, mean_q: 4.103492
 54310/100000: episode: 1230, duration: 0.301s, episode steps: 61, steps per second: 203, episode reward: 111.401, mean reward: 1.826 [1.451, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.811 [-0.753, 10.100], loss: 0.096488, mae: 0.311694, mean_q: 4.073089
 54363/100000: episode: 1231, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 107.905, mean reward: 2.036 [1.465, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.523, 10.238], loss: 0.121938, mae: 0.328355, mean_q: 4.094285
 54387/100000: episode: 1232, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 64.111, mean reward: 2.671 [1.558, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.302, 10.100], loss: 0.112699, mae: 0.324335, mean_q: 4.110024
 54448/100000: episode: 1233, duration: 0.308s, episode steps: 61, steps per second: 198, episode reward: 118.478, mean reward: 1.942 [1.476, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.815 [-0.942, 10.307], loss: 0.106734, mae: 0.320556, mean_q: 4.115619
 54475/100000: episode: 1234, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 85.848, mean reward: 3.180 [2.181, 5.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.467], loss: 0.117212, mae: 0.327438, mean_q: 4.140263
 54499/100000: episode: 1235, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 64.925, mean reward: 2.705 [1.645, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.289, 10.100], loss: 0.103904, mae: 0.320496, mean_q: 4.120717
 54552/100000: episode: 1236, duration: 0.267s, episode steps: 53, steps per second: 198, episode reward: 125.251, mean reward: 2.363 [1.518, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-1.500, 10.100], loss: 0.126561, mae: 0.336656, mean_q: 4.131536
 54558/100000: episode: 1237, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 15.424, mean reward: 2.571 [2.367, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.402], loss: 0.110513, mae: 0.325108, mean_q: 4.094550
 54572/100000: episode: 1238, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 32.921, mean reward: 2.352 [1.895, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.137, 10.338], loss: 0.109715, mae: 0.320580, mean_q: 4.094119
 54588/100000: episode: 1239, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 46.061, mean reward: 2.879 [2.117, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.593, 10.100], loss: 0.102976, mae: 0.328910, mean_q: 4.146197
 54594/100000: episode: 1240, duration: 0.033s, episode steps: 6, steps per second: 179, episode reward: 17.698, mean reward: 2.950 [2.129, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.311], loss: 0.185729, mae: 0.372289, mean_q: 4.271568
 54625/100000: episode: 1241, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 85.508, mean reward: 2.758 [2.111, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.073, 10.100], loss: 0.100002, mae: 0.314579, mean_q: 4.044468
 54652/100000: episode: 1242, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 82.312, mean reward: 3.049 [1.788, 9.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.153, 10.306], loss: 0.131937, mae: 0.337492, mean_q: 4.207613
 54683/100000: episode: 1243, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 120.228, mean reward: 3.878 [2.525, 7.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.725, 10.100], loss: 0.145385, mae: 0.358216, mean_q: 4.193474
 54699/100000: episode: 1244, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 56.935, mean reward: 3.558 [2.407, 5.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.241, 10.100], loss: 0.121563, mae: 0.347847, mean_q: 4.224705
 54713/100000: episode: 1245, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 33.010, mean reward: 2.358 [1.916, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.356], loss: 0.132135, mae: 0.355095, mean_q: 4.211275
 54740/100000: episode: 1246, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 74.355, mean reward: 2.754 [2.098, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.428], loss: 0.195507, mae: 0.384406, mean_q: 4.279289
 54793/100000: episode: 1247, duration: 0.288s, episode steps: 53, steps per second: 184, episode reward: 96.096, mean reward: 1.813 [1.450, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.363, 10.100], loss: 0.132793, mae: 0.347106, mean_q: 4.174530
 54809/100000: episode: 1248, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 49.208, mean reward: 3.075 [2.428, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.444, 10.100], loss: 0.197364, mae: 0.391369, mean_q: 4.271817
 54815/100000: episode: 1249, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 14.433, mean reward: 2.406 [1.932, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.189], loss: 0.131402, mae: 0.335478, mean_q: 4.079676
 54829/100000: episode: 1250, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 46.572, mean reward: 3.327 [2.549, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.412], loss: 0.112144, mae: 0.339052, mean_q: 4.251853
 54856/100000: episode: 1251, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 62.265, mean reward: 2.306 [1.825, 3.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.363], loss: 0.165718, mae: 0.388387, mean_q: 4.262854
 54918/100000: episode: 1252, duration: 0.299s, episode steps: 62, steps per second: 207, episode reward: 126.161, mean reward: 2.035 [1.524, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.686, 10.100], loss: 0.139812, mae: 0.348218, mean_q: 4.239115
 54945/100000: episode: 1253, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 84.664, mean reward: 3.136 [2.477, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.106, 10.418], loss: 0.112577, mae: 0.328569, mean_q: 4.180918
 54976/100000: episode: 1254, duration: 0.150s, episode steps: 31, steps per second: 207, episode reward: 88.181, mean reward: 2.845 [2.273, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.336, 10.100], loss: 0.116137, mae: 0.335497, mean_q: 4.284693
 55007/100000: episode: 1255, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 87.067, mean reward: 2.809 [2.299, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.869, 10.100], loss: 0.178960, mae: 0.380662, mean_q: 4.315951
 55013/100000: episode: 1256, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 17.159, mean reward: 2.860 [2.628, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.273, 10.424], loss: 0.111431, mae: 0.336857, mean_q: 4.218754
 55074/100000: episode: 1257, duration: 0.293s, episode steps: 61, steps per second: 208, episode reward: 154.060, mean reward: 2.526 [1.602, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.817 [-0.596, 10.100], loss: 0.120000, mae: 0.346660, mean_q: 4.257823
 55101/100000: episode: 1258, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 88.381, mean reward: 3.273 [2.376, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.231, 10.444], loss: 0.120584, mae: 0.330293, mean_q: 4.183434
 55117/100000: episode: 1259, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 50.151, mean reward: 3.134 [2.719, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.251, 10.100], loss: 0.149453, mae: 0.376935, mean_q: 4.326471
 55123/100000: episode: 1260, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 16.946, mean reward: 2.824 [2.559, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.470], loss: 0.137456, mae: 0.346323, mean_q: 4.161905
 55185/100000: episode: 1261, duration: 0.315s, episode steps: 62, steps per second: 197, episode reward: 136.615, mean reward: 2.203 [1.621, 6.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.803 [-0.543, 10.100], loss: 0.153331, mae: 0.354277, mean_q: 4.295802
 55220/100000: episode: 1262, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 106.407, mean reward: 3.040 [2.360, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.277, 10.100], loss: 0.141892, mae: 0.359778, mean_q: 4.270607
 55236/100000: episode: 1263, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 53.200, mean reward: 3.325 [2.480, 4.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.724, 10.100], loss: 0.179230, mae: 0.401246, mean_q: 4.281505
 55289/100000: episode: 1264, duration: 0.267s, episode steps: 53, steps per second: 199, episode reward: 123.150, mean reward: 2.324 [1.616, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.909, 10.100], loss: 0.157678, mae: 0.366750, mean_q: 4.328501
 55305/100000: episode: 1265, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 37.998, mean reward: 2.375 [2.083, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.159, 10.100], loss: 0.203529, mae: 0.414462, mean_q: 4.447859
 55329/100000: episode: 1266, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 52.114, mean reward: 2.171 [1.760, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.186, 10.100], loss: 0.115450, mae: 0.335647, mean_q: 4.367622
 55356/100000: episode: 1267, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 132.094, mean reward: 4.892 [2.722, 24.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.724, 10.421], loss: 0.151295, mae: 0.372823, mean_q: 4.391073
 55418/100000: episode: 1268, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 134.137, mean reward: 2.163 [1.479, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.581, 10.349], loss: 0.176659, mae: 0.399999, mean_q: 4.390357
 55471/100000: episode: 1269, duration: 0.272s, episode steps: 53, steps per second: 195, episode reward: 120.603, mean reward: 2.276 [1.505, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.342, 10.138], loss: 0.430613, mae: 0.415831, mean_q: 4.379107
 55498/100000: episode: 1270, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 93.224, mean reward: 3.453 [2.578, 5.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.785, 10.514], loss: 0.177549, mae: 0.405315, mean_q: 4.422581
 55512/100000: episode: 1271, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 39.207, mean reward: 2.800 [2.268, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.436], loss: 0.156343, mae: 0.366608, mean_q: 4.396563
[Info] 2-TH LEVEL FOUND: 7.642341613769531, Considering 10/90 traces
 55547/100000: episode: 1272, duration: 4.418s, episode steps: 35, steps per second: 8, episode reward: 124.852, mean reward: 3.567 [2.445, 8.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.369, 10.100], loss: 0.152945, mae: 0.373135, mean_q: 4.392183
 55557/100000: episode: 1273, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 58.718, mean reward: 5.872 [3.240, 9.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.353, 10.496], loss: 0.128273, mae: 0.382632, mean_q: 4.406847
 55571/100000: episode: 1274, duration: 0.069s, episode steps: 14, steps per second: 201, episode reward: 61.009, mean reward: 4.358 [2.784, 9.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.502, 10.380], loss: 0.720667, mae: 0.460255, mean_q: 4.388575
 55581/100000: episode: 1275, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 81.784, mean reward: 8.178 [4.609, 10.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.183, 10.680], loss: 0.216556, mae: 0.475851, mean_q: 4.595093
 55611/100000: episode: 1276, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 220.138, mean reward: 7.338 [2.439, 22.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.484, 10.100], loss: 0.248020, mae: 0.437438, mean_q: 4.518781
 55641/100000: episode: 1277, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 71.897, mean reward: 2.397 [1.494, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.372, 10.135], loss: 0.236523, mae: 0.405421, mean_q: 4.485903
 55671/100000: episode: 1278, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 147.143, mean reward: 4.905 [3.147, 9.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.355, 10.100], loss: 0.245454, mae: 0.443793, mean_q: 4.493742
 55682/100000: episode: 1279, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 50.386, mean reward: 4.581 [3.404, 8.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.792, 10.100], loss: 0.206151, mae: 0.381118, mean_q: 4.494617
 55707/100000: episode: 1280, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 99.822, mean reward: 3.993 [2.203, 6.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.860, 10.100], loss: 0.622201, mae: 0.551544, mean_q: 4.649113
 55721/100000: episode: 1281, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 70.116, mean reward: 5.008 [3.486, 9.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.061, 10.488], loss: 0.341133, mae: 0.514735, mean_q: 4.492761
 55731/100000: episode: 1282, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 57.237, mean reward: 5.724 [4.271, 7.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.600], loss: 0.198349, mae: 0.454314, mean_q: 4.658884
 55745/100000: episode: 1283, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 46.731, mean reward: 3.338 [2.865, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.476], loss: 1.049014, mae: 0.545977, mean_q: 4.628936
 55772/100000: episode: 1284, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 87.171, mean reward: 3.229 [2.496, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.727, 10.100], loss: 0.272836, mae: 0.458253, mean_q: 4.538658
 55799/100000: episode: 1285, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 88.333, mean reward: 3.272 [2.232, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.528, 10.100], loss: 0.270074, mae: 0.448065, mean_q: 4.607418
 55810/100000: episode: 1286, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 39.251, mean reward: 3.568 [2.596, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.603, 10.100], loss: 0.292674, mae: 0.506614, mean_q: 4.801048
 55828/100000: episode: 1287, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 60.798, mean reward: 3.378 [2.514, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.313, 10.100], loss: 0.249033, mae: 0.465313, mean_q: 4.590964
 55842/100000: episode: 1288, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 71.768, mean reward: 5.126 [2.939, 7.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.528], loss: 0.554938, mae: 0.494276, mean_q: 4.719546
 55856/100000: episode: 1289, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 66.003, mean reward: 4.715 [3.408, 5.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.537], loss: 0.284698, mae: 0.461043, mean_q: 4.740495
 55870/100000: episode: 1290, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 40.025, mean reward: 2.859 [2.638, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.359, 10.462], loss: 0.207254, mae: 0.448525, mean_q: 4.703650
 55900/100000: episode: 1291, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 167.963, mean reward: 5.599 [2.840, 15.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.338, 10.100], loss: 0.386261, mae: 0.533853, mean_q: 4.648247
 55927/100000: episode: 1292, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 80.271, mean reward: 2.973 [2.042, 6.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.204, 10.100], loss: 0.621154, mae: 0.576638, mean_q: 4.834580
 55952/100000: episode: 1293, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 101.513, mean reward: 4.061 [2.886, 5.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.803, 10.100], loss: 0.534689, mae: 0.546529, mean_q: 4.813289
 55979/100000: episode: 1294, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 86.715, mean reward: 3.212 [2.520, 5.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.751, 10.100], loss: 0.231873, mae: 0.438371, mean_q: 4.680295
 55993/100000: episode: 1295, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 48.018, mean reward: 3.430 [2.612, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.523], loss: 0.572722, mae: 0.549491, mean_q: 4.847572
 56020/100000: episode: 1296, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 84.979, mean reward: 3.147 [2.242, 4.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.442, 10.100], loss: 0.328059, mae: 0.501560, mean_q: 4.801698
 56036/100000: episode: 1297, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 70.744, mean reward: 4.422 [2.617, 12.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.783, 10.100], loss: 0.288532, mae: 0.473776, mean_q: 4.701045
 56063/100000: episode: 1298, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 97.400, mean reward: 3.607 [2.516, 5.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.354, 10.100], loss: 0.589903, mae: 0.488751, mean_q: 4.760047
 56077/100000: episode: 1299, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 51.730, mean reward: 3.695 [2.406, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.524], loss: 0.847784, mae: 0.559334, mean_q: 4.945714
 56104/100000: episode: 1300, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 80.914, mean reward: 2.997 [2.363, 5.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.595, 10.100], loss: 0.283149, mae: 0.513542, mean_q: 4.824209
 56131/100000: episode: 1301, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 90.867, mean reward: 3.365 [2.602, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.513, 10.100], loss: 0.371014, mae: 0.483425, mean_q: 4.840842
 56145/100000: episode: 1302, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 69.218, mean reward: 4.944 [3.157, 9.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.526, 10.560], loss: 0.572629, mae: 0.626303, mean_q: 5.125813
 56155/100000: episode: 1303, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 48.888, mean reward: 4.889 [3.522, 6.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.261, 10.447], loss: 0.257699, mae: 0.529670, mean_q: 4.882016
 56180/100000: episode: 1304, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 74.933, mean reward: 2.997 [2.293, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.343, 10.100], loss: 0.407374, mae: 0.509369, mean_q: 4.896593
 56196/100000: episode: 1305, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 54.337, mean reward: 3.396 [2.672, 6.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.281, 10.100], loss: 0.318612, mae: 0.507923, mean_q: 4.885748
 56212/100000: episode: 1306, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 53.920, mean reward: 3.370 [2.425, 5.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.367, 10.100], loss: 0.504660, mae: 0.565544, mean_q: 5.049259
 56230/100000: episode: 1307, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 54.553, mean reward: 3.031 [2.443, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.609, 10.100], loss: 0.463610, mae: 0.515632, mean_q: 4.899312
 56248/100000: episode: 1308, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 68.791, mean reward: 3.822 [2.530, 5.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.391, 10.100], loss: 0.660580, mae: 0.535287, mean_q: 4.933745
 56258/100000: episode: 1309, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 46.516, mean reward: 4.652 [2.825, 8.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.043, 10.473], loss: 0.460192, mae: 0.537691, mean_q: 4.880012
 56288/100000: episode: 1310, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 115.532, mean reward: 3.851 [2.588, 6.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.663, 10.100], loss: 0.434206, mae: 0.540559, mean_q: 5.025597
 56304/100000: episode: 1311, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 64.905, mean reward: 4.057 [3.270, 5.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.460, 10.100], loss: 0.312294, mae: 0.518404, mean_q: 4.952714
 56322/100000: episode: 1312, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 65.860, mean reward: 3.659 [2.469, 8.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.268, 10.100], loss: 0.318657, mae: 0.485624, mean_q: 5.076485
 56336/100000: episode: 1313, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 51.181, mean reward: 3.656 [2.786, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.523], loss: 0.313027, mae: 0.502535, mean_q: 4.829284
 56346/100000: episode: 1314, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 53.244, mean reward: 5.324 [3.723, 9.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-1.715, 10.497], loss: 0.475444, mae: 0.556450, mean_q: 5.071776
 56373/100000: episode: 1315, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 93.044, mean reward: 3.446 [1.754, 5.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.347, 10.100], loss: 0.416938, mae: 0.545178, mean_q: 5.032657
 56389/100000: episode: 1316, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 53.219, mean reward: 3.326 [2.699, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.358, 10.100], loss: 0.789593, mae: 0.604766, mean_q: 5.113762
 56419/100000: episode: 1317, duration: 0.143s, episode steps: 30, steps per second: 210, episode reward: 91.944, mean reward: 3.065 [2.080, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.372, 10.100], loss: 0.527816, mae: 0.587812, mean_q: 5.111787
 56449/100000: episode: 1318, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 87.762, mean reward: 2.925 [2.384, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.370, 10.100], loss: 0.496173, mae: 0.590402, mean_q: 5.099404
 56460/100000: episode: 1319, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 38.237, mean reward: 3.476 [2.861, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.318, 10.100], loss: 0.361714, mae: 0.529977, mean_q: 5.062243
 56470/100000: episode: 1320, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 93.411, mean reward: 9.341 [3.607, 16.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.685], loss: 0.285376, mae: 0.532880, mean_q: 4.999582
 56495/100000: episode: 1321, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 73.329, mean reward: 2.933 [2.207, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.610, 10.100], loss: 0.290460, mae: 0.506120, mean_q: 5.098801
 56525/100000: episode: 1322, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 106.386, mean reward: 3.546 [2.026, 8.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.973, 10.100], loss: 0.492678, mae: 0.617485, mean_q: 5.201062
 56552/100000: episode: 1323, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 101.767, mean reward: 3.769 [2.060, 6.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.256, 10.100], loss: 0.702088, mae: 0.606505, mean_q: 5.203860
 56568/100000: episode: 1324, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 62.451, mean reward: 3.903 [3.097, 5.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.736, 10.100], loss: 0.446133, mae: 0.567965, mean_q: 4.973111
 56579/100000: episode: 1325, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 29.111, mean reward: 2.646 [1.865, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.286, 10.100], loss: 0.394086, mae: 0.502693, mean_q: 5.030356
 56609/100000: episode: 1326, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 90.981, mean reward: 3.033 [2.226, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.302, 10.100], loss: 0.420038, mae: 0.579473, mean_q: 5.266858
 56627/100000: episode: 1327, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 72.879, mean reward: 4.049 [2.631, 10.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.541, 10.100], loss: 0.442448, mae: 0.577745, mean_q: 5.094876
 56652/100000: episode: 1328, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 155.532, mean reward: 6.221 [2.916, 13.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.363, 10.100], loss: 0.420157, mae: 0.572722, mean_q: 5.281622
 56668/100000: episode: 1329, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 54.866, mean reward: 3.429 [2.890, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.445, 10.100], loss: 0.551685, mae: 0.593459, mean_q: 5.273817
 56693/100000: episode: 1330, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 73.154, mean reward: 2.926 [2.099, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.174, 10.100], loss: 0.571124, mae: 0.616099, mean_q: 5.379526
 56703/100000: episode: 1331, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 53.380, mean reward: 5.338 [2.816, 9.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.512], loss: 0.573906, mae: 0.590936, mean_q: 5.378879
 56719/100000: episode: 1332, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 59.286, mean reward: 3.705 [2.496, 6.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.209, 10.100], loss: 0.977359, mae: 0.706891, mean_q: 5.488140
 56744/100000: episode: 1333, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 90.768, mean reward: 3.631 [2.557, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.601, 10.100], loss: 0.535349, mae: 0.626118, mean_q: 5.356115
 56774/100000: episode: 1334, duration: 0.158s, episode steps: 30, steps per second: 189, episode reward: 151.784, mean reward: 5.059 [2.460, 7.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.368, 10.100], loss: 0.483957, mae: 0.566051, mean_q: 5.254718
 56801/100000: episode: 1335, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 176.435, mean reward: 6.535 [2.808, 19.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.032, 10.100], loss: 0.441248, mae: 0.582052, mean_q: 5.360111
 56828/100000: episode: 1336, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 86.393, mean reward: 3.200 [2.399, 4.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.241, 10.100], loss: 0.419837, mae: 0.579984, mean_q: 5.377006
 56858/100000: episode: 1337, duration: 0.148s, episode steps: 30, steps per second: 202, episode reward: 82.871, mean reward: 2.762 [1.888, 4.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.875, 10.100], loss: 0.710076, mae: 0.608869, mean_q: 5.339661
 56888/100000: episode: 1338, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 88.132, mean reward: 2.938 [1.767, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.705, 10.100], loss: 0.491426, mae: 0.575157, mean_q: 5.335390
 56913/100000: episode: 1339, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 108.053, mean reward: 4.322 [2.651, 6.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.369, 10.100], loss: 0.551388, mae: 0.628184, mean_q: 5.573356
 56923/100000: episode: 1340, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 50.797, mean reward: 5.080 [2.900, 10.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.516], loss: 0.332049, mae: 0.555459, mean_q: 5.252669
 56941/100000: episode: 1341, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 72.499, mean reward: 4.028 [2.765, 8.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.374, 10.100], loss: 0.805535, mae: 0.687692, mean_q: 5.560972
 56971/100000: episode: 1342, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 79.454, mean reward: 2.648 [1.983, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.279, 10.100], loss: 0.934361, mae: 0.741301, mean_q: 5.615175
 56998/100000: episode: 1343, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 92.710, mean reward: 3.434 [1.907, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.524, 10.100], loss: 0.651741, mae: 0.623522, mean_q: 5.408833
 57023/100000: episode: 1344, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 117.638, mean reward: 4.706 [2.800, 7.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.334, 10.100], loss: 0.796717, mae: 0.619475, mean_q: 5.588105
 57033/100000: episode: 1345, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 59.569, mean reward: 5.957 [4.444, 9.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.616], loss: 0.854640, mae: 0.758703, mean_q: 5.433492
 57058/100000: episode: 1346, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 84.156, mean reward: 3.366 [2.420, 6.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.171, 10.100], loss: 0.502575, mae: 0.645936, mean_q: 5.389385
 57085/100000: episode: 1347, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 104.364, mean reward: 3.865 [2.707, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.240, 10.100], loss: 0.730389, mae: 0.696654, mean_q: 5.582442
 57115/100000: episode: 1348, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 177.170, mean reward: 5.906 [2.763, 15.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.455, 10.100], loss: 0.480479, mae: 0.609442, mean_q: 5.596810
 57145/100000: episode: 1349, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 90.769, mean reward: 3.026 [2.084, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.864, 10.100], loss: 0.495668, mae: 0.656004, mean_q: 5.582334
 57163/100000: episode: 1350, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 68.513, mean reward: 3.806 [3.042, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.785, 10.100], loss: 0.624443, mae: 0.682880, mean_q: 5.669406
 57174/100000: episode: 1351, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 35.186, mean reward: 3.199 [2.304, 4.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.341, 10.100], loss: 0.525109, mae: 0.643978, mean_q: 5.630484
 57204/100000: episode: 1352, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 99.255, mean reward: 3.309 [2.108, 4.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.284, 10.100], loss: 0.569551, mae: 0.681603, mean_q: 5.699167
 57231/100000: episode: 1353, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 87.051, mean reward: 3.224 [1.956, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.844, 10.100], loss: 0.751818, mae: 0.709344, mean_q: 5.741514
 57258/100000: episode: 1354, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 78.127, mean reward: 2.894 [1.597, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.746, 10.100], loss: 0.738183, mae: 0.705466, mean_q: 5.637897
 57288/100000: episode: 1355, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 118.407, mean reward: 3.947 [2.889, 7.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.876, 10.100], loss: 1.245545, mae: 0.763392, mean_q: 5.747663
 57315/100000: episode: 1356, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 88.283, mean reward: 3.270 [2.448, 6.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.377, 10.100], loss: 1.027024, mae: 0.771406, mean_q: 5.758871
 57345/100000: episode: 1357, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 80.854, mean reward: 2.695 [1.866, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.399, 10.100], loss: 0.992666, mae: 0.680744, mean_q: 5.751612
 57372/100000: episode: 1358, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 108.870, mean reward: 4.032 [2.451, 6.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-2.087, 10.100], loss: 0.682808, mae: 0.738676, mean_q: 5.644269
 57399/100000: episode: 1359, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 76.843, mean reward: 2.846 [2.103, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.215, 10.100], loss: 0.544635, mae: 0.641263, mean_q: 5.716938
 57429/100000: episode: 1360, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 97.653, mean reward: 3.255 [1.967, 7.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.704, 10.100], loss: 0.581737, mae: 0.653609, mean_q: 5.808954
 57459/100000: episode: 1361, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 100.783, mean reward: 3.359 [2.114, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.681, 10.100], loss: 0.916432, mae: 0.679511, mean_q: 5.715859
[Info] 3-TH LEVEL FOUND: 11.759045600891113, Considering 10/90 traces
 57477/100000: episode: 1362, duration: 4.382s, episode steps: 18, steps per second: 4, episode reward: 54.755, mean reward: 3.042 [2.205, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.444, 10.100], loss: 0.943927, mae: 0.774154, mean_q: 5.960115
 57491/100000: episode: 1363, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 57.813, mean reward: 4.129 [2.660, 8.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.979, 10.100], loss: 0.780242, mae: 0.751106, mean_q: 5.773666
 57503/100000: episode: 1364, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 57.063, mean reward: 4.755 [2.795, 10.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.466, 10.100], loss: 0.689592, mae: 0.747128, mean_q: 5.872638
 57517/100000: episode: 1365, duration: 0.078s, episode steps: 14, steps per second: 181, episode reward: 86.362, mean reward: 6.169 [4.483, 8.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.455, 10.100], loss: 0.516952, mae: 0.618967, mean_q: 5.701791
 57526/100000: episode: 1366, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 55.760, mean reward: 6.196 [3.265, 10.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.565, 10.100], loss: 0.898035, mae: 0.709683, mean_q: 5.916141
 57539/100000: episode: 1367, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 55.034, mean reward: 4.233 [3.328, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.493, 10.100], loss: 0.718170, mae: 0.777162, mean_q: 5.747712
 57558/100000: episode: 1368, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 138.738, mean reward: 7.302 [4.217, 12.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.533, 10.100], loss: 1.180488, mae: 0.828599, mean_q: 5.746691
 57572/100000: episode: 1369, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 73.461, mean reward: 5.247 [3.878, 8.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.551, 10.100], loss: 0.990896, mae: 0.860768, mean_q: 5.826200
 57581/100000: episode: 1370, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 46.064, mean reward: 5.118 [3.805, 6.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.264, 10.100], loss: 0.671575, mae: 0.716553, mean_q: 5.801568
 57594/100000: episode: 1371, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 60.679, mean reward: 4.668 [3.265, 6.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.465, 10.100], loss: 0.549253, mae: 0.654000, mean_q: 5.855506
 57608/100000: episode: 1372, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 82.905, mean reward: 5.922 [2.911, 11.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.554, 10.100], loss: 0.624380, mae: 0.687015, mean_q: 5.823905
 57615/100000: episode: 1373, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 29.074, mean reward: 4.153 [3.376, 6.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.413, 10.100], loss: 0.587016, mae: 0.707699, mean_q: 5.850255
 57628/100000: episode: 1374, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 71.857, mean reward: 5.527 [2.871, 9.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.750, 10.100], loss: 0.976368, mae: 0.777960, mean_q: 5.963485
 57640/100000: episode: 1375, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 54.934, mean reward: 4.578 [3.713, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.370, 10.100], loss: 0.806019, mae: 0.807930, mean_q: 6.002722
 57653/100000: episode: 1376, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 77.870, mean reward: 5.990 [3.060, 15.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.615, 10.100], loss: 0.728449, mae: 0.740981, mean_q: 6.052025
 57672/100000: episode: 1377, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 272.795, mean reward: 14.358 [4.619, 29.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.534, 10.100], loss: 0.842329, mae: 0.721821, mean_q: 5.953065
 57686/100000: episode: 1378, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 206.263, mean reward: 14.733 [4.009, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.543, 10.100], loss: 0.951157, mae: 0.748925, mean_q: 5.806836
 57693/100000: episode: 1379, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 22.015, mean reward: 3.145 [2.714, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.505, 10.100], loss: 1.470069, mae: 0.800270, mean_q: 6.356036
 57707/100000: episode: 1380, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 55.606, mean reward: 3.972 [2.258, 10.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.495, 10.100], loss: 1.254537, mae: 0.789827, mean_q: 6.244334
 57725/100000: episode: 1381, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 135.807, mean reward: 7.545 [4.454, 27.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.736, 10.100], loss: 1.057982, mae: 0.866144, mean_q: 6.297088
 57739/100000: episode: 1382, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 46.227, mean reward: 3.302 [2.364, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.171, 10.100], loss: 0.531301, mae: 0.610755, mean_q: 5.841798
 57746/100000: episode: 1383, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 29.826, mean reward: 4.261 [3.478, 5.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.473, 10.100], loss: 0.589967, mae: 0.697949, mean_q: 6.166508
 57765/100000: episode: 1384, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 126.331, mean reward: 6.649 [3.748, 10.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.576, 10.100], loss: 1.095276, mae: 0.848392, mean_q: 6.145910
 57779/100000: episode: 1385, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 49.708, mean reward: 3.551 [2.922, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.366, 10.100], loss: 1.838586, mae: 1.017667, mean_q: 6.514736
 57786/100000: episode: 1386, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 33.294, mean reward: 4.756 [3.533, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.507, 10.100], loss: 2.809856, mae: 0.980807, mean_q: 5.834980
 57798/100000: episode: 1387, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 52.279, mean reward: 4.357 [3.816, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.335, 10.100], loss: 12.846368, mae: 1.653900, mean_q: 6.429327
 57807/100000: episode: 1388, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 129.371, mean reward: 14.375 [7.913, 22.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.492, 10.100], loss: 1.638548, mae: 1.102389, mean_q: 6.572502
 57826/100000: episode: 1389, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 110.749, mean reward: 5.829 [4.572, 7.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.683, 10.100], loss: 0.719256, mae: 0.731248, mean_q: 5.973487
 57842/100000: episode: 1390, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 65.202, mean reward: 4.075 [2.869, 6.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.386, 10.100], loss: 9.700682, mae: 1.003883, mean_q: 6.232926
 57856/100000: episode: 1391, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 55.401, mean reward: 3.957 [2.856, 7.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.203, 10.100], loss: 11.765959, mae: 1.706543, mean_q: 6.575497
 57863/100000: episode: 1392, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 27.899, mean reward: 3.986 [3.154, 5.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.535, 10.100], loss: 1.826049, mae: 1.187240, mean_q: 6.937842
 57877/100000: episode: 1393, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 41.456, mean reward: 2.961 [2.563, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.743, 10.100], loss: 1.713628, mae: 1.012455, mean_q: 6.152079
 57891/100000: episode: 1394, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 84.547, mean reward: 6.039 [3.137, 19.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.501, 10.100], loss: 1.351818, mae: 0.868955, mean_q: 6.452641
 57904/100000: episode: 1395, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 38.565, mean reward: 2.967 [2.439, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.775, 10.100], loss: 0.916023, mae: 0.724741, mean_q: 6.228149
 57918/100000: episode: 1396, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 80.232, mean reward: 5.731 [4.471, 9.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.386, 10.100], loss: 1.301387, mae: 0.858612, mean_q: 6.309848
 57934/100000: episode: 1397, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 81.697, mean reward: 5.106 [4.091, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.414, 10.100], loss: 10.206591, mae: 1.195123, mean_q: 6.742706
 57948/100000: episode: 1398, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 47.307, mean reward: 3.379 [2.293, 8.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.482, 10.100], loss: 1.721366, mae: 1.134021, mean_q: 6.239394
 57962/100000: episode: 1399, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 171.217, mean reward: 12.230 [2.988, 64.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.744, 10.100], loss: 1.616435, mae: 0.905155, mean_q: 6.351129
 57971/100000: episode: 1400, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 49.998, mean reward: 5.555 [4.205, 7.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.559, 10.100], loss: 1.303825, mae: 0.828080, mean_q: 6.352973
 57978/100000: episode: 1401, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 23.376, mean reward: 3.339 [3.051, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.409, 10.100], loss: 0.773531, mae: 0.801381, mean_q: 6.352096
 57997/100000: episode: 1402, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 94.390, mean reward: 4.968 [2.693, 6.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.231, 10.100], loss: 1.279492, mae: 0.859060, mean_q: 6.445041
 58009/100000: episode: 1403, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 56.213, mean reward: 4.684 [3.531, 6.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.732, 10.100], loss: 1.156370, mae: 0.813396, mean_q: 6.109373
 58016/100000: episode: 1404, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 38.923, mean reward: 5.560 [3.920, 9.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.522, 10.100], loss: 0.946393, mae: 0.885136, mean_q: 6.502446
 58030/100000: episode: 1405, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 96.425, mean reward: 6.887 [4.685, 12.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.582, 10.100], loss: 1.778042, mae: 0.997340, mean_q: 6.757599
 58044/100000: episode: 1406, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 68.480, mean reward: 4.891 [3.294, 8.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.438, 10.100], loss: 1.295238, mae: 0.890190, mean_q: 6.212121
 58051/100000: episode: 1407, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 26.576, mean reward: 3.797 [3.042, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.602, 10.100], loss: 0.884622, mae: 0.900740, mean_q: 6.638954
 58060/100000: episode: 1408, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 116.677, mean reward: 12.964 [7.496, 27.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.539, 10.100], loss: 1.137401, mae: 0.801931, mean_q: 6.588178
 58076/100000: episode: 1409, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 64.892, mean reward: 4.056 [2.427, 8.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.429, 10.100], loss: 1.517426, mae: 0.903992, mean_q: 6.529367
 58090/100000: episode: 1410, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 47.196, mean reward: 3.371 [2.106, 9.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.212, 10.100], loss: 1.542043, mae: 0.940577, mean_q: 6.472165
 58097/100000: episode: 1411, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 30.054, mean reward: 4.293 [3.763, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.335, 10.100], loss: 0.963391, mae: 0.862250, mean_q: 6.440768
 58110/100000: episode: 1412, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 44.935, mean reward: 3.457 [2.916, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.399, 10.100], loss: 1.570464, mae: 0.989152, mean_q: 6.664174
 58128/100000: episode: 1413, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 114.058, mean reward: 6.337 [3.807, 12.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.395, 10.100], loss: 1.408327, mae: 0.897892, mean_q: 6.559163
 58135/100000: episode: 1414, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 31.094, mean reward: 4.442 [3.546, 6.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.437, 10.100], loss: 0.703081, mae: 0.792269, mean_q: 6.614678
 58148/100000: episode: 1415, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 61.867, mean reward: 4.759 [2.947, 7.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.527, 10.100], loss: 1.496611, mae: 0.861200, mean_q: 6.496858
 58161/100000: episode: 1416, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 52.306, mean reward: 4.024 [3.101, 4.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.464, 10.100], loss: 1.263321, mae: 0.818853, mean_q: 6.487285
 58175/100000: episode: 1417, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 82.873, mean reward: 5.919 [3.238, 11.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.411, 10.100], loss: 1.932192, mae: 1.104400, mean_q: 6.844802
 58188/100000: episode: 1418, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 75.081, mean reward: 5.775 [3.248, 11.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.648, 10.100], loss: 1.600697, mae: 0.988496, mean_q: 6.810736
 58201/100000: episode: 1419, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 61.631, mean reward: 4.741 [3.195, 6.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.513, 10.100], loss: 3.031946, mae: 1.052755, mean_q: 6.486180
 58213/100000: episode: 1420, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 61.476, mean reward: 5.123 [3.746, 7.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.775, 10.100], loss: 2.031490, mae: 1.057882, mean_q: 6.725435
 58225/100000: episode: 1421, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 60.797, mean reward: 5.066 [3.017, 11.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.450, 10.100], loss: 2.299341, mae: 1.166658, mean_q: 6.972402
 58243/100000: episode: 1422, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 98.057, mean reward: 5.448 [4.185, 8.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.257, 10.100], loss: 2.740116, mae: 1.226437, mean_q: 6.938343
 58262/100000: episode: 1423, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 135.057, mean reward: 7.108 [3.761, 22.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.909, 10.100], loss: 5.255763, mae: 1.219927, mean_q: 7.065858
 58280/100000: episode: 1424, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 89.873, mean reward: 4.993 [3.849, 9.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.535, 10.100], loss: 0.993106, mae: 0.856278, mean_q: 6.380922
 58294/100000: episode: 1425, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 64.100, mean reward: 4.579 [2.892, 5.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.418, 10.100], loss: 1.615075, mae: 0.996432, mean_q: 6.958969
 58312/100000: episode: 1426, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 65.423, mean reward: 3.635 [2.420, 5.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.250, 10.100], loss: 1.289660, mae: 0.853095, mean_q: 6.485367
 58331/100000: episode: 1427, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 121.394, mean reward: 6.389 [2.676, 14.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.356, 10.100], loss: 1.521296, mae: 0.979892, mean_q: 6.877509
 58340/100000: episode: 1428, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 173.633, mean reward: 19.293 [5.239, 72.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.414, 10.100], loss: 1.314838, mae: 0.860224, mean_q: 6.386349
 58354/100000: episode: 1429, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 86.736, mean reward: 6.195 [3.314, 10.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.734, 10.100], loss: 2.304264, mae: 1.036938, mean_q: 7.033329
 58363/100000: episode: 1430, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 156.556, mean reward: 17.395 [5.392, 45.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.465, 10.100], loss: 10.245094, mae: 1.711779, mean_q: 7.648121
[Info] FALSIFICATION!
[Info] Levels: [5.4103403, 7.6423416, 11.759046, 12.07934]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.34]
[Info] Error Prob: 0.0003400000000000001

 58368/100000: episode: 1431, duration: 4.441s, episode steps: 5, steps per second: 1, episode reward: 148.104, mean reward: 29.621 [8.627, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.310, 10.067], loss: 1.462571, mae: 1.030350, mean_q: 6.375921
 58468/100000: episode: 1432, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.841, mean reward: 1.928 [1.464, 5.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.437, 10.195], loss: 7.740734, mae: 1.258777, mean_q: 7.076122
 58568/100000: episode: 1433, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 182.561, mean reward: 1.826 [1.450, 2.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.188], loss: 5.538431, mae: 1.252928, mean_q: 6.844559
 58668/100000: episode: 1434, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 181.877, mean reward: 1.819 [1.438, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.944, 10.098], loss: 5.408391, mae: 1.291019, mean_q: 6.973228
 58768/100000: episode: 1435, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 212.838, mean reward: 2.128 [1.467, 5.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.895, 10.313], loss: 4.241985, mae: 1.149383, mean_q: 6.958367
 58868/100000: episode: 1436, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 182.733, mean reward: 1.827 [1.476, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.694, 10.192], loss: 5.084634, mae: 1.166868, mean_q: 7.043954
 58968/100000: episode: 1437, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 211.221, mean reward: 2.112 [1.540, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.821, 10.313], loss: 8.345936, mae: 1.296167, mean_q: 7.032797
 59068/100000: episode: 1438, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.163, mean reward: 1.892 [1.450, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.713, 10.098], loss: 1.642459, mae: 0.969233, mean_q: 6.769056
 59168/100000: episode: 1439, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 202.684, mean reward: 2.027 [1.445, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.231, 10.112], loss: 2.367456, mae: 1.023314, mean_q: 6.790549
 59268/100000: episode: 1440, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 184.259, mean reward: 1.843 [1.443, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.474, 10.125], loss: 5.422483, mae: 1.155656, mean_q: 6.887099
 59368/100000: episode: 1441, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 192.820, mean reward: 1.928 [1.486, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.368, 10.098], loss: 4.267900, mae: 1.058564, mean_q: 6.777641
 59468/100000: episode: 1442, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.254, mean reward: 1.863 [1.483, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.767, 10.165], loss: 4.927394, mae: 1.087412, mean_q: 6.772603
 59568/100000: episode: 1443, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 253.444, mean reward: 2.534 [1.535, 5.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.744, 10.098], loss: 1.976090, mae: 1.008199, mean_q: 6.699452
 59668/100000: episode: 1444, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 203.535, mean reward: 2.035 [1.509, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.045, 10.098], loss: 5.030741, mae: 1.087150, mean_q: 6.815378
 59768/100000: episode: 1445, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.892, mean reward: 1.909 [1.456, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.112, 10.365], loss: 2.325537, mae: 0.957618, mean_q: 6.700028
 59868/100000: episode: 1446, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 181.598, mean reward: 1.816 [1.496, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.426, 10.098], loss: 2.602380, mae: 1.004340, mean_q: 6.684214
 59968/100000: episode: 1447, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 204.648, mean reward: 2.046 [1.447, 4.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.783, 10.331], loss: 2.445503, mae: 0.999710, mean_q: 6.692524
 60068/100000: episode: 1448, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 201.381, mean reward: 2.014 [1.516, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.524, 10.098], loss: 2.095793, mae: 0.982538, mean_q: 6.630717
 60168/100000: episode: 1449, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 208.737, mean reward: 2.087 [1.496, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.467, 10.328], loss: 4.284249, mae: 1.021364, mean_q: 6.651099
 60268/100000: episode: 1450, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 194.125, mean reward: 1.941 [1.452, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.678, 10.098], loss: 4.576237, mae: 1.007912, mean_q: 6.627744
 60368/100000: episode: 1451, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.545, mean reward: 1.825 [1.448, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.722, 10.214], loss: 4.569235, mae: 1.039235, mean_q: 6.627093
 60468/100000: episode: 1452, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.196, mean reward: 1.932 [1.479, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.015, 10.098], loss: 4.890879, mae: 1.033941, mean_q: 6.454058
 60568/100000: episode: 1453, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 189.459, mean reward: 1.895 [1.463, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.589, 10.137], loss: 7.334970, mae: 1.046947, mean_q: 6.458234
 60668/100000: episode: 1454, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 179.464, mean reward: 1.795 [1.450, 2.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.547, 10.098], loss: 5.520073, mae: 1.005530, mean_q: 6.311369
 60768/100000: episode: 1455, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 183.942, mean reward: 1.839 [1.434, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.658, 10.150], loss: 6.531743, mae: 1.171668, mean_q: 6.341649
 60868/100000: episode: 1456, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 230.285, mean reward: 2.303 [1.506, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.006, 10.098], loss: 1.412497, mae: 0.806136, mean_q: 6.039274
 60968/100000: episode: 1457, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.307, mean reward: 1.823 [1.468, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.133, 10.098], loss: 5.316632, mae: 1.000400, mean_q: 6.127385
 61068/100000: episode: 1458, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 198.921, mean reward: 1.989 [1.499, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.563, 10.098], loss: 3.832643, mae: 0.873965, mean_q: 6.091030
 61168/100000: episode: 1459, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.611, mean reward: 1.946 [1.455, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.166, 10.098], loss: 3.956999, mae: 0.855732, mean_q: 5.953210
 61268/100000: episode: 1460, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 181.230, mean reward: 1.812 [1.463, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.428, 10.098], loss: 5.606604, mae: 0.964457, mean_q: 5.995928
 61368/100000: episode: 1461, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 213.154, mean reward: 2.132 [1.522, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.250, 10.244], loss: 4.409441, mae: 0.929669, mean_q: 5.929969
 61468/100000: episode: 1462, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.108, mean reward: 1.911 [1.474, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.381, 10.098], loss: 3.496385, mae: 0.813438, mean_q: 5.723995
 61568/100000: episode: 1463, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.440, mean reward: 1.884 [1.467, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.486, 10.098], loss: 1.854797, mae: 0.737049, mean_q: 5.596048
 61668/100000: episode: 1464, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.552, mean reward: 1.836 [1.433, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.835, 10.132], loss: 2.709103, mae: 0.793336, mean_q: 5.538323
 61768/100000: episode: 1465, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.546, mean reward: 1.875 [1.449, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.744, 10.098], loss: 4.562533, mae: 0.828205, mean_q: 5.540056
 61868/100000: episode: 1466, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 200.723, mean reward: 2.007 [1.470, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.924, 10.320], loss: 2.274478, mae: 0.691435, mean_q: 5.397402
 61968/100000: episode: 1467, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.143, mean reward: 1.841 [1.478, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.289, 10.265], loss: 7.061546, mae: 0.970811, mean_q: 5.509329
 62068/100000: episode: 1468, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 202.815, mean reward: 2.028 [1.445, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.404, 10.098], loss: 2.959866, mae: 0.746004, mean_q: 5.343853
 62168/100000: episode: 1469, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.874, mean reward: 2.029 [1.459, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.736, 10.098], loss: 5.124782, mae: 0.816872, mean_q: 5.370329
 62268/100000: episode: 1470, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 191.525, mean reward: 1.915 [1.473, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.814, 10.098], loss: 7.145772, mae: 0.849904, mean_q: 5.260859
 62368/100000: episode: 1471, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 204.324, mean reward: 2.043 [1.526, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.683, 10.221], loss: 4.302562, mae: 0.772675, mean_q: 5.264224
 62468/100000: episode: 1472, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 192.838, mean reward: 1.928 [1.454, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.161, 10.316], loss: 5.348315, mae: 0.862846, mean_q: 5.316869
 62568/100000: episode: 1473, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 233.964, mean reward: 2.340 [1.472, 5.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.789, 10.280], loss: 3.741770, mae: 0.639639, mean_q: 4.900605
 62668/100000: episode: 1474, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 195.018, mean reward: 1.950 [1.482, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.988, 10.376], loss: 3.766614, mae: 0.609089, mean_q: 4.793583
 62768/100000: episode: 1475, duration: 0.466s, episode steps: 100, steps per second: 214, episode reward: 195.139, mean reward: 1.951 [1.455, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.231], loss: 3.242022, mae: 0.597422, mean_q: 4.724349
 62868/100000: episode: 1476, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 189.928, mean reward: 1.899 [1.463, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.870, 10.098], loss: 4.269283, mae: 0.681134, mean_q: 4.678461
 62968/100000: episode: 1477, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 201.569, mean reward: 2.016 [1.458, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.417, 10.210], loss: 3.547817, mae: 0.600182, mean_q: 4.497907
 63068/100000: episode: 1478, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 203.005, mean reward: 2.030 [1.484, 5.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.208], loss: 2.341195, mae: 0.477471, mean_q: 4.264017
 63168/100000: episode: 1479, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 201.701, mean reward: 2.017 [1.464, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.278, 10.349], loss: 0.824941, mae: 0.419368, mean_q: 4.243799
 63268/100000: episode: 1480, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.440, mean reward: 1.924 [1.462, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.932, 10.098], loss: 2.866389, mae: 0.433570, mean_q: 4.071712
 63368/100000: episode: 1481, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.604, mean reward: 1.886 [1.435, 6.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.338, 10.098], loss: 0.184354, mae: 0.346860, mean_q: 3.947060
 63468/100000: episode: 1482, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 247.070, mean reward: 2.471 [1.473, 8.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-1.457, 10.098], loss: 0.087725, mae: 0.287049, mean_q: 3.873204
 63568/100000: episode: 1483, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 186.240, mean reward: 1.862 [1.443, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.920, 10.174], loss: 0.093809, mae: 0.294850, mean_q: 3.908311
 63668/100000: episode: 1484, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 198.039, mean reward: 1.980 [1.475, 6.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.911, 10.135], loss: 0.088203, mae: 0.294929, mean_q: 3.908308
 63768/100000: episode: 1485, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 239.369, mean reward: 2.394 [1.454, 13.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.792, 10.098], loss: 0.092991, mae: 0.293411, mean_q: 3.919186
 63868/100000: episode: 1486, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 180.198, mean reward: 1.802 [1.451, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.563, 10.098], loss: 0.142069, mae: 0.314973, mean_q: 3.958434
 63968/100000: episode: 1487, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 191.342, mean reward: 1.913 [1.462, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.010, 10.195], loss: 0.122068, mae: 0.303481, mean_q: 3.918295
 64068/100000: episode: 1488, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.670, mean reward: 1.877 [1.447, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.565, 10.098], loss: 0.110283, mae: 0.293537, mean_q: 3.903613
 64168/100000: episode: 1489, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.477, mean reward: 1.845 [1.508, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.391, 10.120], loss: 0.128962, mae: 0.297663, mean_q: 3.908265
 64268/100000: episode: 1490, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.612, mean reward: 1.966 [1.447, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.748, 10.138], loss: 0.095936, mae: 0.294603, mean_q: 3.912567
 64368/100000: episode: 1491, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.142, mean reward: 1.891 [1.464, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.692, 10.098], loss: 0.104420, mae: 0.294588, mean_q: 3.908712
 64468/100000: episode: 1492, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 232.361, mean reward: 2.324 [1.524, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.189, 10.098], loss: 0.092507, mae: 0.298868, mean_q: 3.933922
 64568/100000: episode: 1493, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.739, mean reward: 1.837 [1.458, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.760, 10.195], loss: 0.094381, mae: 0.296907, mean_q: 3.907099
 64668/100000: episode: 1494, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 203.129, mean reward: 2.031 [1.489, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.753, 10.098], loss: 0.095337, mae: 0.290144, mean_q: 3.911998
 64768/100000: episode: 1495, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.076, mean reward: 1.971 [1.440, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.244, 10.098], loss: 0.100546, mae: 0.284589, mean_q: 3.888855
 64868/100000: episode: 1496, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 182.518, mean reward: 1.825 [1.430, 2.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.206, 10.098], loss: 0.109871, mae: 0.293934, mean_q: 3.913009
 64968/100000: episode: 1497, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 186.687, mean reward: 1.867 [1.489, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.128, 10.150], loss: 0.101957, mae: 0.292711, mean_q: 3.889399
 65068/100000: episode: 1498, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.804, mean reward: 1.928 [1.562, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.251, 10.098], loss: 0.112736, mae: 0.287396, mean_q: 3.911439
 65168/100000: episode: 1499, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 179.874, mean reward: 1.799 [1.470, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.555, 10.098], loss: 0.120808, mae: 0.285796, mean_q: 3.889487
 65268/100000: episode: 1500, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 223.207, mean reward: 2.232 [1.513, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.221, 10.098], loss: 0.111019, mae: 0.298961, mean_q: 3.905131
 65368/100000: episode: 1501, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.479, mean reward: 1.875 [1.434, 3.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.760, 10.135], loss: 0.085561, mae: 0.283696, mean_q: 3.908524
 65468/100000: episode: 1502, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 198.731, mean reward: 1.987 [1.495, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.581, 10.098], loss: 0.103074, mae: 0.288006, mean_q: 3.894181
 65568/100000: episode: 1503, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 212.086, mean reward: 2.121 [1.454, 7.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.466, 10.098], loss: 0.099209, mae: 0.289623, mean_q: 3.901047
 65668/100000: episode: 1504, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 196.780, mean reward: 1.968 [1.467, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.317, 10.098], loss: 0.088601, mae: 0.290521, mean_q: 3.908006
 65768/100000: episode: 1505, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.302, mean reward: 1.963 [1.451, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.486, 10.256], loss: 0.106844, mae: 0.285462, mean_q: 3.921720
 65868/100000: episode: 1506, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 222.063, mean reward: 2.221 [1.479, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.098], loss: 0.115365, mae: 0.303754, mean_q: 3.918075
 65968/100000: episode: 1507, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.568, mean reward: 1.976 [1.468, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.489, 10.098], loss: 0.092221, mae: 0.291506, mean_q: 3.901356
 66068/100000: episode: 1508, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 198.892, mean reward: 1.989 [1.471, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.282, 10.098], loss: 0.088612, mae: 0.290412, mean_q: 3.914859
 66168/100000: episode: 1509, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 194.534, mean reward: 1.945 [1.432, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.034, 10.285], loss: 0.085910, mae: 0.292353, mean_q: 3.898673
 66268/100000: episode: 1510, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 184.386, mean reward: 1.844 [1.453, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.098], loss: 0.085942, mae: 0.288137, mean_q: 3.911425
 66368/100000: episode: 1511, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.484, mean reward: 1.975 [1.461, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.243, 10.098], loss: 0.091863, mae: 0.296566, mean_q: 3.920736
 66468/100000: episode: 1512, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.154, mean reward: 1.832 [1.457, 2.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.748, 10.098], loss: 0.088934, mae: 0.290405, mean_q: 3.905539
 66568/100000: episode: 1513, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 178.139, mean reward: 1.781 [1.429, 2.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.656, 10.098], loss: 0.107918, mae: 0.295903, mean_q: 3.892444
 66668/100000: episode: 1514, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 215.654, mean reward: 2.157 [1.469, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.668, 10.098], loss: 0.086830, mae: 0.289309, mean_q: 3.910382
 66768/100000: episode: 1515, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.696, mean reward: 1.997 [1.497, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.672, 10.400], loss: 0.084325, mae: 0.287961, mean_q: 3.919115
 66868/100000: episode: 1516, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 183.869, mean reward: 1.839 [1.444, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.204, 10.193], loss: 0.111850, mae: 0.303044, mean_q: 3.919147
 66968/100000: episode: 1517, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 189.466, mean reward: 1.895 [1.452, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.086, 10.098], loss: 0.105742, mae: 0.307206, mean_q: 3.923091
 67068/100000: episode: 1518, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.371, mean reward: 1.914 [1.485, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.127, 10.098], loss: 0.093503, mae: 0.292703, mean_q: 3.917572
 67168/100000: episode: 1519, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.391, mean reward: 1.874 [1.451, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.293, 10.102], loss: 0.110290, mae: 0.296001, mean_q: 3.905927
 67268/100000: episode: 1520, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 194.901, mean reward: 1.949 [1.485, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.387, 10.178], loss: 0.104430, mae: 0.296599, mean_q: 3.890212
 67368/100000: episode: 1521, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 196.422, mean reward: 1.964 [1.481, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.409, 10.126], loss: 0.116720, mae: 0.306786, mean_q: 3.916213
 67468/100000: episode: 1522, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 189.871, mean reward: 1.899 [1.459, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.278, 10.098], loss: 0.085214, mae: 0.287880, mean_q: 3.912364
 67568/100000: episode: 1523, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 182.011, mean reward: 1.820 [1.443, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.326, 10.209], loss: 0.104869, mae: 0.294108, mean_q: 3.890569
 67668/100000: episode: 1524, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.060, mean reward: 1.851 [1.472, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.643, 10.245], loss: 0.093939, mae: 0.290865, mean_q: 3.876215
 67768/100000: episode: 1525, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 212.601, mean reward: 2.126 [1.437, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.098], loss: 0.117802, mae: 0.305289, mean_q: 3.898984
 67868/100000: episode: 1526, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.920, mean reward: 1.889 [1.447, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.730, 10.098], loss: 0.121886, mae: 0.302338, mean_q: 3.904829
 67968/100000: episode: 1527, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.184, mean reward: 1.862 [1.482, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 0.095764, mae: 0.300047, mean_q: 3.886253
 68068/100000: episode: 1528, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.288, mean reward: 1.933 [1.480, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.545, 10.266], loss: 0.121943, mae: 0.295011, mean_q: 3.860065
 68168/100000: episode: 1529, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 205.266, mean reward: 2.053 [1.515, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.534, 10.100], loss: 0.086101, mae: 0.280310, mean_q: 3.859028
 68268/100000: episode: 1530, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 180.462, mean reward: 1.805 [1.464, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.527, 10.111], loss: 0.101378, mae: 0.304443, mean_q: 3.895788
[Info] 1-TH LEVEL FOUND: 6.375091552734375, Considering 10/90 traces
 68368/100000: episode: 1531, duration: 4.723s, episode steps: 100, steps per second: 21, episode reward: 186.684, mean reward: 1.867 [1.458, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.086, 10.098], loss: 0.108107, mae: 0.297983, mean_q: 3.889348
 68393/100000: episode: 1532, duration: 0.127s, episode steps: 25, steps per second: 198, episode reward: 58.171, mean reward: 2.327 [1.857, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.401, 10.100], loss: 0.122071, mae: 0.308823, mean_q: 3.890899
 68415/100000: episode: 1533, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 68.742, mean reward: 3.125 [2.557, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.405, 10.100], loss: 0.244853, mae: 0.331441, mean_q: 3.914042
 68436/100000: episode: 1534, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 82.392, mean reward: 3.923 [2.593, 5.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.548], loss: 0.080043, mae: 0.282074, mean_q: 3.838579
 68457/100000: episode: 1535, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 51.347, mean reward: 2.445 [1.977, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.226, 10.100], loss: 0.075831, mae: 0.270505, mean_q: 3.878248
 68479/100000: episode: 1536, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 54.758, mean reward: 2.489 [1.544, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.200, 10.100], loss: 0.074312, mae: 0.276780, mean_q: 3.868106
 68502/100000: episode: 1537, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 57.638, mean reward: 2.506 [1.796, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.619, 10.100], loss: 0.096658, mae: 0.293853, mean_q: 3.886694
 68523/100000: episode: 1538, duration: 0.106s, episode steps: 21, steps per second: 197, episode reward: 67.380, mean reward: 3.209 [2.220, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.482, 10.100], loss: 0.128112, mae: 0.310360, mean_q: 3.882813
 68548/100000: episode: 1539, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 53.957, mean reward: 2.158 [1.611, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.181, 10.100], loss: 0.089720, mae: 0.302719, mean_q: 3.897450
 68569/100000: episode: 1540, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 55.633, mean reward: 2.649 [2.206, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.321, 10.100], loss: 0.166624, mae: 0.303698, mean_q: 3.938586
 68594/100000: episode: 1541, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 57.802, mean reward: 2.312 [1.680, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.100], loss: 0.089130, mae: 0.293281, mean_q: 3.903543
 68607/100000: episode: 1542, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 36.902, mean reward: 2.839 [1.824, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.855, 10.100], loss: 0.091618, mae: 0.302548, mean_q: 3.958370
 68633/100000: episode: 1543, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 62.268, mean reward: 2.395 [1.868, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.405, 10.100], loss: 0.090978, mae: 0.301832, mean_q: 3.919231
 68659/100000: episode: 1544, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 108.294, mean reward: 4.165 [2.614, 7.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.233, 10.100], loss: 0.108928, mae: 0.295754, mean_q: 3.888754
 68680/100000: episode: 1545, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 50.561, mean reward: 2.408 [1.838, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.202, 10.100], loss: 0.075825, mae: 0.277138, mean_q: 3.939723
 68701/100000: episode: 1546, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 58.925, mean reward: 2.806 [2.105, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.436, 10.100], loss: 0.090199, mae: 0.296329, mean_q: 3.947648
 68722/100000: episode: 1547, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 68.599, mean reward: 3.267 [2.432, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.769, 10.100], loss: 0.081957, mae: 0.287146, mean_q: 3.863702
 68745/100000: episode: 1548, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 60.198, mean reward: 2.617 [1.967, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.055, 10.100], loss: 0.078055, mae: 0.281956, mean_q: 3.943992
 68766/100000: episode: 1549, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 92.625, mean reward: 4.411 [3.101, 6.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.693, 10.508], loss: 0.072752, mae: 0.269592, mean_q: 3.909746
 68792/100000: episode: 1550, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 66.949, mean reward: 2.575 [1.642, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.979, 10.100], loss: 0.099269, mae: 0.309684, mean_q: 3.986276
 68817/100000: episode: 1551, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 64.057, mean reward: 2.562 [1.536, 6.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.100], loss: 0.125914, mae: 0.315447, mean_q: 4.014230
 68843/100000: episode: 1552, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 76.518, mean reward: 2.943 [1.953, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.253, 10.100], loss: 0.108654, mae: 0.304389, mean_q: 3.981525
 68869/100000: episode: 1553, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 68.472, mean reward: 2.634 [2.027, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.602, 10.100], loss: 0.088782, mae: 0.300085, mean_q: 3.947034
 68892/100000: episode: 1554, duration: 0.109s, episode steps: 23, steps per second: 211, episode reward: 59.171, mean reward: 2.573 [2.110, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.436, 10.100], loss: 0.091965, mae: 0.306362, mean_q: 3.988751
 68913/100000: episode: 1555, duration: 0.100s, episode steps: 21, steps per second: 209, episode reward: 62.847, mean reward: 2.993 [2.273, 8.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.264, 10.100], loss: 0.080976, mae: 0.294418, mean_q: 3.976303
 68938/100000: episode: 1556, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 61.385, mean reward: 2.455 [1.811, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.041, 10.100], loss: 0.131815, mae: 0.333282, mean_q: 4.003863
 68960/100000: episode: 1557, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 46.803, mean reward: 2.127 [1.525, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.124], loss: 0.093135, mae: 0.303728, mean_q: 3.988599
 68983/100000: episode: 1558, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 86.284, mean reward: 3.751 [2.593, 6.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.359, 10.100], loss: 0.120176, mae: 0.313471, mean_q: 4.016958
 69004/100000: episode: 1559, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 44.005, mean reward: 2.095 [1.488, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.864, 10.188], loss: 0.101596, mae: 0.322481, mean_q: 4.064268
 69017/100000: episode: 1560, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 42.350, mean reward: 3.258 [2.111, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.494, 10.100], loss: 0.097057, mae: 0.323299, mean_q: 4.088105
 69038/100000: episode: 1561, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 83.258, mean reward: 3.965 [3.036, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.594, 10.503], loss: 0.108872, mae: 0.305030, mean_q: 4.047552
 69060/100000: episode: 1562, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 66.570, mean reward: 3.026 [1.949, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.166, 10.100], loss: 0.173716, mae: 0.339970, mean_q: 4.068064
 69085/100000: episode: 1563, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 86.990, mean reward: 3.480 [2.397, 6.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.403, 10.100], loss: 0.112088, mae: 0.325760, mean_q: 4.068545
 69106/100000: episode: 1564, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 82.006, mean reward: 3.905 [2.489, 6.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.698, 10.525], loss: 0.121296, mae: 0.338372, mean_q: 4.104127
 69132/100000: episode: 1565, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 76.670, mean reward: 2.949 [2.358, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.851, 10.100], loss: 0.144780, mae: 0.365679, mean_q: 4.165878
 69154/100000: episode: 1566, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 49.763, mean reward: 2.262 [1.721, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.214, 10.100], loss: 0.131575, mae: 0.358737, mean_q: 4.113513
 69175/100000: episode: 1567, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 49.978, mean reward: 2.380 [1.732, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.311, 10.100], loss: 0.144272, mae: 0.346393, mean_q: 4.112786
 69198/100000: episode: 1568, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 65.964, mean reward: 2.868 [2.033, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.742, 10.100], loss: 0.107447, mae: 0.316950, mean_q: 4.076850
 69223/100000: episode: 1569, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 68.345, mean reward: 2.734 [2.027, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.389, 10.100], loss: 0.105442, mae: 0.318425, mean_q: 4.110459
 69249/100000: episode: 1570, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 63.522, mean reward: 2.443 [1.995, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.525, 10.100], loss: 0.119928, mae: 0.332284, mean_q: 4.152859
 69274/100000: episode: 1571, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 77.252, mean reward: 3.090 [2.616, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.562, 10.100], loss: 0.108444, mae: 0.317872, mean_q: 4.107546
 69295/100000: episode: 1572, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 74.062, mean reward: 3.527 [2.622, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.693, 10.100], loss: 0.135257, mae: 0.339112, mean_q: 4.147840
 69317/100000: episode: 1573, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 54.431, mean reward: 2.474 [1.803, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.592, 10.100], loss: 0.100479, mae: 0.319445, mean_q: 4.074036
 69330/100000: episode: 1574, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 28.980, mean reward: 2.229 [1.912, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.164, 10.100], loss: 0.108193, mae: 0.319682, mean_q: 4.128838
 69338/100000: episode: 1575, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 24.177, mean reward: 3.022 [2.473, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.285, 10.100], loss: 0.152300, mae: 0.326954, mean_q: 4.107183
 69363/100000: episode: 1576, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 63.589, mean reward: 2.544 [1.892, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.350, 10.100], loss: 0.136362, mae: 0.342505, mean_q: 4.157904
 69386/100000: episode: 1577, duration: 0.116s, episode steps: 23, steps per second: 197, episode reward: 59.951, mean reward: 2.607 [1.991, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.263, 10.100], loss: 0.120993, mae: 0.348180, mean_q: 4.244559
 69408/100000: episode: 1578, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 55.825, mean reward: 2.538 [1.578, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.507, 10.100], loss: 0.088953, mae: 0.298195, mean_q: 4.167531
 69416/100000: episode: 1579, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 25.092, mean reward: 3.136 [2.663, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.404, 10.100], loss: 0.115017, mae: 0.320030, mean_q: 4.321310
 69429/100000: episode: 1580, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 33.706, mean reward: 2.593 [2.113, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.291, 10.100], loss: 0.130970, mae: 0.347766, mean_q: 4.183147
 69450/100000: episode: 1581, duration: 0.107s, episode steps: 21, steps per second: 195, episode reward: 120.717, mean reward: 5.748 [2.536, 38.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.046, 10.100], loss: 0.118899, mae: 0.330933, mean_q: 4.141862
 69471/100000: episode: 1582, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 45.931, mean reward: 2.187 [1.676, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.644, 10.100], loss: 0.134316, mae: 0.343688, mean_q: 4.231864
 69494/100000: episode: 1583, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 61.459, mean reward: 2.672 [1.997, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.196, 10.100], loss: 0.109173, mae: 0.330136, mean_q: 4.205685
 69519/100000: episode: 1584, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 77.628, mean reward: 3.105 [2.496, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.164, 10.100], loss: 0.909920, mae: 0.402466, mean_q: 4.200298
 69527/100000: episode: 1585, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 25.278, mean reward: 3.160 [2.622, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.304, 10.100], loss: 2.647255, mae: 0.637175, mean_q: 4.555929
 69548/100000: episode: 1586, duration: 0.104s, episode steps: 21, steps per second: 203, episode reward: 59.862, mean reward: 2.851 [2.253, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.140, 10.100], loss: 0.157861, mae: 0.395387, mean_q: 4.187083
 69569/100000: episode: 1587, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 199.479, mean reward: 9.499 [2.440, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.700, 10.567], loss: 0.123461, mae: 0.353946, mean_q: 4.212583
 69582/100000: episode: 1588, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 35.272, mean reward: 2.713 [2.135, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.349, 10.100], loss: 0.163821, mae: 0.353945, mean_q: 4.218210
 69603/100000: episode: 1589, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 61.864, mean reward: 2.946 [1.921, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.276, 10.325], loss: 0.160683, mae: 0.384245, mean_q: 4.264097
 69629/100000: episode: 1590, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 89.001, mean reward: 3.423 [1.874, 7.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.855, 10.100], loss: 0.129844, mae: 0.356779, mean_q: 4.241935
 69654/100000: episode: 1591, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 68.337, mean reward: 2.733 [2.048, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.635, 10.100], loss: 6.146312, mae: 0.660467, mean_q: 4.475497
 69676/100000: episode: 1592, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 53.068, mean reward: 2.412 [2.007, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.136, 10.100], loss: 6.862838, mae: 0.601150, mean_q: 4.214888
 69684/100000: episode: 1593, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 23.606, mean reward: 2.951 [2.261, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.181, 10.100], loss: 0.131625, mae: 0.383483, mean_q: 4.350386
 69709/100000: episode: 1594, duration: 0.119s, episode steps: 25, steps per second: 209, episode reward: 58.669, mean reward: 2.347 [1.720, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.150, 10.100], loss: 0.899504, mae: 0.425160, mean_q: 4.203748
 69734/100000: episode: 1595, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 69.347, mean reward: 2.774 [2.350, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.878, 10.100], loss: 0.139552, mae: 0.362481, mean_q: 4.230281
[Info] FALSIFICATION!
[Info] Levels: [6.3750916, 10.838789]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 69755/100000: episode: 1596, duration: 4.685s, episode steps: 21, steps per second: 4, episode reward: 254.363, mean reward: 12.113 [3.376, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.567], loss: 0.135367, mae: 0.352603, mean_q: 4.267241
 69855/100000: episode: 1597, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.385, mean reward: 1.924 [1.461, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.977, 10.365], loss: 1.650671, mae: 0.417624, mean_q: 4.303805
 69955/100000: episode: 1598, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.082, mean reward: 2.111 [1.481, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.133, 10.098], loss: 0.210421, mae: 0.385048, mean_q: 4.327397
 70055/100000: episode: 1599, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 198.470, mean reward: 1.985 [1.459, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.469, 10.196], loss: 1.681368, mae: 0.462773, mean_q: 4.411838
 70155/100000: episode: 1600, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.261, mean reward: 1.783 [1.445, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.397, 10.123], loss: 0.284863, mae: 0.389513, mean_q: 4.360461
 70255/100000: episode: 1601, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 177.952, mean reward: 1.780 [1.468, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.351, 10.238], loss: 4.398752, mae: 0.576452, mean_q: 4.461363
 70355/100000: episode: 1602, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 183.888, mean reward: 1.839 [1.478, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.098], loss: 1.788788, mae: 0.518464, mean_q: 4.419147
 70455/100000: episode: 1603, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 192.760, mean reward: 1.928 [1.448, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.575, 10.098], loss: 1.699026, mae: 0.442428, mean_q: 4.402470
 70555/100000: episode: 1604, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 187.185, mean reward: 1.872 [1.438, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 3.059203, mae: 0.452285, mean_q: 4.365938
 70655/100000: episode: 1605, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.448, mean reward: 1.854 [1.455, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.212], loss: 0.204934, mae: 0.408650, mean_q: 4.346442
 70755/100000: episode: 1606, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 179.860, mean reward: 1.799 [1.459, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.733, 10.098], loss: 1.794382, mae: 0.436056, mean_q: 4.371571
 70855/100000: episode: 1607, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.852, mean reward: 1.889 [1.456, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.946, 10.312], loss: 4.430298, mae: 0.544831, mean_q: 4.445612
 70955/100000: episode: 1608, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.587, mean reward: 1.896 [1.452, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.125, 10.098], loss: 2.728710, mae: 0.473968, mean_q: 4.389036
 71055/100000: episode: 1609, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 193.724, mean reward: 1.937 [1.458, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.496, 10.170], loss: 1.735097, mae: 0.475952, mean_q: 4.424215
 71155/100000: episode: 1610, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.145, mean reward: 1.881 [1.488, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.181, 10.137], loss: 3.194089, mae: 0.535818, mean_q: 4.416780
 71255/100000: episode: 1611, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 190.136, mean reward: 1.901 [1.465, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.345, 10.198], loss: 3.003892, mae: 0.515181, mean_q: 4.390616
 71355/100000: episode: 1612, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 178.585, mean reward: 1.786 [1.445, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.098], loss: 4.298848, mae: 0.542731, mean_q: 4.399852
 71455/100000: episode: 1613, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 199.365, mean reward: 1.994 [1.466, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.540, 10.098], loss: 1.453756, mae: 0.467733, mean_q: 4.405115
 71555/100000: episode: 1614, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.152, mean reward: 1.942 [1.464, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.331, 10.285], loss: 2.839160, mae: 0.494198, mean_q: 4.388472
 71655/100000: episode: 1615, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 227.965, mean reward: 2.280 [1.553, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.682, 10.405], loss: 5.373313, mae: 0.594926, mean_q: 4.395974
 71755/100000: episode: 1616, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 196.443, mean reward: 1.964 [1.477, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.916, 10.287], loss: 1.664107, mae: 0.502485, mean_q: 4.430337
 71855/100000: episode: 1617, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 203.537, mean reward: 2.035 [1.458, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.527, 10.356], loss: 1.448967, mae: 0.442928, mean_q: 4.335976
 71955/100000: episode: 1618, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.193, mean reward: 1.862 [1.472, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.762, 10.271], loss: 0.439275, mae: 0.413509, mean_q: 4.381585
 72055/100000: episode: 1619, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 198.553, mean reward: 1.986 [1.503, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.446, 10.098], loss: 0.205926, mae: 0.397821, mean_q: 4.348995
 72155/100000: episode: 1620, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.879, mean reward: 1.929 [1.491, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.299, 10.098], loss: 0.774202, mae: 0.424422, mean_q: 4.345199
 72255/100000: episode: 1621, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 212.336, mean reward: 2.123 [1.523, 5.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.840, 10.098], loss: 1.683093, mae: 0.471949, mean_q: 4.426424
 72355/100000: episode: 1622, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 186.804, mean reward: 1.868 [1.479, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.523, 10.098], loss: 1.466125, mae: 0.456702, mean_q: 4.407287
 72455/100000: episode: 1623, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 241.260, mean reward: 2.413 [1.465, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.219, 10.098], loss: 1.927188, mae: 0.481251, mean_q: 4.394524
 72555/100000: episode: 1624, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.644, mean reward: 1.946 [1.488, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.000, 10.098], loss: 0.199978, mae: 0.404631, mean_q: 4.345786
 72655/100000: episode: 1625, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 197.117, mean reward: 1.971 [1.452, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.251, 10.098], loss: 1.683707, mae: 0.488025, mean_q: 4.406569
 72755/100000: episode: 1626, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.412, mean reward: 1.924 [1.444, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.691, 10.098], loss: 1.640478, mae: 0.468926, mean_q: 4.366823
 72855/100000: episode: 1627, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 198.775, mean reward: 1.988 [1.443, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.371], loss: 0.196939, mae: 0.383466, mean_q: 4.334724
 72955/100000: episode: 1628, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.034, mean reward: 1.810 [1.437, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.184, 10.098], loss: 1.488651, mae: 0.477321, mean_q: 4.450680
 73055/100000: episode: 1629, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 190.420, mean reward: 1.904 [1.485, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.906, 10.098], loss: 0.245256, mae: 0.394756, mean_q: 4.315632
 73155/100000: episode: 1630, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.737, mean reward: 1.907 [1.453, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.407, 10.098], loss: 2.848592, mae: 0.530706, mean_q: 4.443503
 73255/100000: episode: 1631, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 181.338, mean reward: 1.813 [1.467, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.223, 10.161], loss: 0.245684, mae: 0.391477, mean_q: 4.349173
 73355/100000: episode: 1632, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.413, mean reward: 1.984 [1.480, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.645, 10.245], loss: 0.630470, mae: 0.436537, mean_q: 4.386383
 73455/100000: episode: 1633, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 215.484, mean reward: 2.155 [1.454, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.097, 10.098], loss: 3.435498, mae: 0.534722, mean_q: 4.432988
 73555/100000: episode: 1634, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 222.102, mean reward: 2.221 [1.487, 14.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.098], loss: 1.428149, mae: 0.461292, mean_q: 4.334096
 73655/100000: episode: 1635, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 204.032, mean reward: 2.040 [1.453, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.172, 10.302], loss: 0.435484, mae: 0.392882, mean_q: 4.251838
 73755/100000: episode: 1636, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.118, mean reward: 1.921 [1.473, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.196, 10.098], loss: 1.909639, mae: 0.476155, mean_q: 4.305060
 73855/100000: episode: 1637, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 219.764, mean reward: 2.198 [1.477, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.408, 10.098], loss: 0.184153, mae: 0.364223, mean_q: 4.174277
 73955/100000: episode: 1638, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 234.593, mean reward: 2.346 [1.469, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.092, 10.098], loss: 1.859557, mae: 0.460184, mean_q: 4.217853
 74055/100000: episode: 1639, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 194.884, mean reward: 1.949 [1.459, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.298], loss: 4.325664, mae: 0.554776, mean_q: 4.248705
 74155/100000: episode: 1640, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.028, mean reward: 1.970 [1.475, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.332, 10.098], loss: 1.613448, mae: 0.411741, mean_q: 4.175388
 74255/100000: episode: 1641, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.113, mean reward: 1.861 [1.449, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.657, 10.198], loss: 2.437154, mae: 0.438260, mean_q: 4.146375
 74355/100000: episode: 1642, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.047, mean reward: 1.910 [1.480, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.244], loss: 2.837050, mae: 0.521465, mean_q: 4.106364
 74455/100000: episode: 1643, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 188.033, mean reward: 1.880 [1.442, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.866, 10.126], loss: 1.620337, mae: 0.421062, mean_q: 4.089453
 74555/100000: episode: 1644, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 188.498, mean reward: 1.885 [1.475, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.476, 10.098], loss: 1.340462, mae: 0.400780, mean_q: 4.017069
 74655/100000: episode: 1645, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.532, mean reward: 1.965 [1.443, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.168, 10.162], loss: 0.129826, mae: 0.321911, mean_q: 3.963943
 74755/100000: episode: 1646, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 201.814, mean reward: 2.018 [1.524, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.114, 10.374], loss: 0.142722, mae: 0.318775, mean_q: 3.921082
 74855/100000: episode: 1647, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 191.049, mean reward: 1.910 [1.463, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.258], loss: 0.123428, mae: 0.309740, mean_q: 3.895102
 74955/100000: episode: 1648, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 207.614, mean reward: 2.076 [1.465, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.806, 10.098], loss: 0.109937, mae: 0.309178, mean_q: 3.899105
 75055/100000: episode: 1649, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.336, mean reward: 1.993 [1.435, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.864, 10.105], loss: 0.110490, mae: 0.298543, mean_q: 3.875326
 75155/100000: episode: 1650, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.805, mean reward: 1.898 [1.483, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.653, 10.220], loss: 0.091543, mae: 0.297023, mean_q: 3.870121
 75255/100000: episode: 1651, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 194.224, mean reward: 1.942 [1.457, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.871, 10.150], loss: 0.095434, mae: 0.299444, mean_q: 3.881971
 75355/100000: episode: 1652, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 183.335, mean reward: 1.833 [1.457, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.199, 10.109], loss: 0.104683, mae: 0.291170, mean_q: 3.879830
 75455/100000: episode: 1653, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.917, mean reward: 1.819 [1.440, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.650, 10.098], loss: 0.114291, mae: 0.303958, mean_q: 3.891747
 75555/100000: episode: 1654, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 182.127, mean reward: 1.821 [1.450, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.498, 10.098], loss: 0.116837, mae: 0.305094, mean_q: 3.886567
 75655/100000: episode: 1655, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 200.117, mean reward: 2.001 [1.473, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.228, 10.098], loss: 0.115787, mae: 0.304441, mean_q: 3.886648
 75755/100000: episode: 1656, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 229.762, mean reward: 2.298 [1.444, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.252, 10.590], loss: 0.096673, mae: 0.303752, mean_q: 3.893762
 75855/100000: episode: 1657, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.626, mean reward: 1.846 [1.483, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.660, 10.125], loss: 0.097592, mae: 0.299134, mean_q: 3.902258
 75955/100000: episode: 1658, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.001, mean reward: 1.990 [1.502, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.895, 10.153], loss: 0.102869, mae: 0.306553, mean_q: 3.896335
 76055/100000: episode: 1659, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.020, mean reward: 2.030 [1.486, 7.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.388, 10.151], loss: 0.092093, mae: 0.293299, mean_q: 3.913697
 76155/100000: episode: 1660, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 193.879, mean reward: 1.939 [1.433, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.323, 10.287], loss: 0.114466, mae: 0.308828, mean_q: 3.911389
 76255/100000: episode: 1661, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 202.915, mean reward: 2.029 [1.451, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.730, 10.098], loss: 0.145598, mae: 0.315372, mean_q: 3.922199
 76355/100000: episode: 1662, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 218.197, mean reward: 2.182 [1.514, 6.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.762, 10.171], loss: 0.119168, mae: 0.309167, mean_q: 3.927075
 76455/100000: episode: 1663, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 204.804, mean reward: 2.048 [1.487, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.098], loss: 0.220902, mae: 0.341851, mean_q: 3.959522
 76555/100000: episode: 1664, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 185.398, mean reward: 1.854 [1.490, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.080, 10.098], loss: 0.127153, mae: 0.322210, mean_q: 3.938278
 76655/100000: episode: 1665, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.834, mean reward: 1.988 [1.456, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.033, 10.463], loss: 0.124138, mae: 0.308875, mean_q: 3.914202
 76755/100000: episode: 1666, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 216.666, mean reward: 2.167 [1.468, 5.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.816, 10.299], loss: 0.094987, mae: 0.303175, mean_q: 3.921304
 76855/100000: episode: 1667, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 181.580, mean reward: 1.816 [1.448, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.235, 10.111], loss: 0.105805, mae: 0.310394, mean_q: 3.937400
 76955/100000: episode: 1668, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 192.309, mean reward: 1.923 [1.441, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.198], loss: 0.107975, mae: 0.307087, mean_q: 3.923666
 77055/100000: episode: 1669, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.684, mean reward: 1.857 [1.469, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.777, 10.198], loss: 0.127014, mae: 0.315801, mean_q: 3.937723
 77155/100000: episode: 1670, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 170.800, mean reward: 1.708 [1.443, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.021, 10.205], loss: 0.119693, mae: 0.318306, mean_q: 3.921450
 77255/100000: episode: 1671, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.608, mean reward: 1.856 [1.453, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.468, 10.098], loss: 0.140532, mae: 0.309140, mean_q: 3.924112
 77355/100000: episode: 1672, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.685, mean reward: 1.937 [1.473, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.225, 10.098], loss: 0.093595, mae: 0.300361, mean_q: 3.913238
 77455/100000: episode: 1673, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.578, mean reward: 1.966 [1.449, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.326, 10.188], loss: 0.136828, mae: 0.313534, mean_q: 3.886620
 77555/100000: episode: 1674, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.395, mean reward: 1.934 [1.448, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.622, 10.129], loss: 0.114300, mae: 0.313723, mean_q: 3.889524
 77655/100000: episode: 1675, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 198.043, mean reward: 1.980 [1.450, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.634, 10.322], loss: 0.096827, mae: 0.301471, mean_q: 3.892497
 77755/100000: episode: 1676, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 210.018, mean reward: 2.100 [1.492, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.927, 10.255], loss: 0.107783, mae: 0.315259, mean_q: 3.885705
 77855/100000: episode: 1677, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 256.651, mean reward: 2.567 [1.482, 6.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.170, 10.098], loss: 0.099180, mae: 0.304861, mean_q: 3.900108
 77955/100000: episode: 1678, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.950, mean reward: 1.999 [1.481, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.762, 10.098], loss: 0.114357, mae: 0.321715, mean_q: 3.922386
 78055/100000: episode: 1679, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.112, mean reward: 1.881 [1.458, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.228, 10.286], loss: 0.104493, mae: 0.319641, mean_q: 3.946388
 78155/100000: episode: 1680, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.461, mean reward: 2.005 [1.482, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.241, 10.225], loss: 0.118460, mae: 0.316192, mean_q: 3.938241
 78255/100000: episode: 1681, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 195.353, mean reward: 1.954 [1.481, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.178, 10.253], loss: 0.137369, mae: 0.316277, mean_q: 3.920204
 78355/100000: episode: 1682, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.899, mean reward: 1.879 [1.456, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.896, 10.175], loss: 0.096964, mae: 0.311466, mean_q: 3.925489
 78455/100000: episode: 1683, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 183.125, mean reward: 1.831 [1.454, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.902, 10.098], loss: 0.121348, mae: 0.317469, mean_q: 3.940268
 78555/100000: episode: 1684, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 178.246, mean reward: 1.782 [1.453, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.088, 10.098], loss: 0.095043, mae: 0.301683, mean_q: 3.899921
 78655/100000: episode: 1685, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 200.537, mean reward: 2.005 [1.458, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.359, 10.098], loss: 0.099895, mae: 0.306973, mean_q: 3.916751
 78755/100000: episode: 1686, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 201.158, mean reward: 2.012 [1.489, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.098], loss: 0.105049, mae: 0.311133, mean_q: 3.907725
 78855/100000: episode: 1687, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 187.916, mean reward: 1.879 [1.470, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.535, 10.182], loss: 0.080716, mae: 0.282905, mean_q: 3.871204
 78955/100000: episode: 1688, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 184.631, mean reward: 1.846 [1.461, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.419, 10.278], loss: 0.091589, mae: 0.294979, mean_q: 3.881073
 79055/100000: episode: 1689, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 215.205, mean reward: 2.152 [1.513, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.741, 10.098], loss: 0.096139, mae: 0.298027, mean_q: 3.881740
 79155/100000: episode: 1690, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 217.295, mean reward: 2.173 [1.476, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.950, 10.249], loss: 0.086385, mae: 0.295913, mean_q: 3.869721
 79255/100000: episode: 1691, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.849, mean reward: 1.958 [1.448, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.780, 10.156], loss: 0.085879, mae: 0.281843, mean_q: 3.861297
 79355/100000: episode: 1692, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 187.526, mean reward: 1.875 [1.451, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.741, 10.329], loss: 0.087017, mae: 0.291674, mean_q: 3.864697
 79455/100000: episode: 1693, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 188.099, mean reward: 1.881 [1.461, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.519, 10.098], loss: 0.094020, mae: 0.298226, mean_q: 3.857971
 79555/100000: episode: 1694, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.834, mean reward: 2.028 [1.464, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.143], loss: 0.091281, mae: 0.299938, mean_q: 3.876342
 79655/100000: episode: 1695, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 177.817, mean reward: 1.778 [1.473, 2.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.178], loss: 0.091496, mae: 0.302214, mean_q: 3.889279
[Info] 1-TH LEVEL FOUND: 5.832976818084717, Considering 10/90 traces
 79755/100000: episode: 1696, duration: 4.772s, episode steps: 100, steps per second: 21, episode reward: 198.676, mean reward: 1.987 [1.442, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.615, 10.209], loss: 0.095926, mae: 0.302650, mean_q: 3.869300
 79786/100000: episode: 1697, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 81.277, mean reward: 2.622 [2.023, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.297, 10.100], loss: 0.129389, mae: 0.320023, mean_q: 3.930005
 79877/100000: episode: 1698, duration: 0.441s, episode steps: 91, steps per second: 206, episode reward: 183.073, mean reward: 2.012 [1.448, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.932, 10.192], loss: 0.085630, mae: 0.300483, mean_q: 3.899328
 79903/100000: episode: 1699, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 79.061, mean reward: 3.041 [2.306, 6.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.082, 10.385], loss: 0.088808, mae: 0.305691, mean_q: 3.893653
 79947/100000: episode: 1700, duration: 0.222s, episode steps: 44, steps per second: 198, episode reward: 114.185, mean reward: 2.595 [1.474, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.305, 10.180], loss: 0.092096, mae: 0.297078, mean_q: 3.901366
 79959/100000: episode: 1701, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 26.766, mean reward: 2.231 [1.993, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.353], loss: 0.109101, mae: 0.333023, mean_q: 3.910532
 79971/100000: episode: 1702, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 39.659, mean reward: 3.305 [2.264, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.510], loss: 0.104500, mae: 0.319184, mean_q: 3.896432
 80062/100000: episode: 1703, duration: 0.445s, episode steps: 91, steps per second: 205, episode reward: 165.525, mean reward: 1.819 [1.435, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-0.743, 10.102], loss: 0.103815, mae: 0.303082, mean_q: 3.933129
 80088/100000: episode: 1704, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 72.809, mean reward: 2.800 [2.020, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.425, 10.387], loss: 0.093114, mae: 0.302464, mean_q: 3.937734
 80119/100000: episode: 1705, duration: 0.147s, episode steps: 31, steps per second: 210, episode reward: 94.298, mean reward: 3.042 [1.680, 6.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.537, 10.100], loss: 0.104529, mae: 0.325733, mean_q: 3.939789
 80131/100000: episode: 1706, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 36.357, mean reward: 3.030 [2.446, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.475], loss: 0.097714, mae: 0.309260, mean_q: 3.915598
 80157/100000: episode: 1707, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 59.434, mean reward: 2.286 [1.549, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.384, 10.145], loss: 0.089878, mae: 0.300718, mean_q: 3.933551
 80248/100000: episode: 1708, duration: 0.454s, episode steps: 91, steps per second: 200, episode reward: 181.043, mean reward: 1.989 [1.476, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-0.158, 10.100], loss: 0.096445, mae: 0.304255, mean_q: 3.941164
 80264/100000: episode: 1709, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 29.386, mean reward: 1.837 [1.657, 2.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.285, 10.235], loss: 0.095513, mae: 0.306619, mean_q: 3.940109
 80355/100000: episode: 1710, duration: 0.452s, episode steps: 91, steps per second: 201, episode reward: 185.501, mean reward: 2.038 [1.458, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-1.306, 10.184], loss: 0.088642, mae: 0.301953, mean_q: 3.944962
 80362/100000: episode: 1711, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 16.768, mean reward: 2.395 [2.182, 2.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.377, 10.100], loss: 0.087354, mae: 0.302813, mean_q: 3.953675
 80378/100000: episode: 1712, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 46.315, mean reward: 2.895 [1.903, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.890, 10.499], loss: 0.082902, mae: 0.285936, mean_q: 3.890612
 80404/100000: episode: 1713, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 64.788, mean reward: 2.492 [1.848, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.445, 10.375], loss: 0.100007, mae: 0.297973, mean_q: 3.955693
 80446/100000: episode: 1714, duration: 0.205s, episode steps: 42, steps per second: 205, episode reward: 115.839, mean reward: 2.758 [1.511, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.137, 10.100], loss: 0.111446, mae: 0.317489, mean_q: 3.971078
 80537/100000: episode: 1715, duration: 0.453s, episode steps: 91, steps per second: 201, episode reward: 165.977, mean reward: 1.824 [1.456, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-1.058, 10.100], loss: 0.102731, mae: 0.312700, mean_q: 3.978007
 80549/100000: episode: 1716, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 34.831, mean reward: 2.903 [2.188, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.468], loss: 0.091745, mae: 0.298480, mean_q: 3.935802
 80640/100000: episode: 1717, duration: 0.465s, episode steps: 91, steps per second: 196, episode reward: 184.976, mean reward: 2.033 [1.491, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.544, 10.342], loss: 0.103638, mae: 0.309110, mean_q: 3.960091
 80731/100000: episode: 1718, duration: 0.444s, episode steps: 91, steps per second: 205, episode reward: 187.537, mean reward: 2.061 [1.498, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.417, 10.100], loss: 0.101306, mae: 0.313812, mean_q: 3.966530
 80775/100000: episode: 1719, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 98.299, mean reward: 2.234 [1.466, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.693, 10.100], loss: 0.087387, mae: 0.299350, mean_q: 3.927324
 80786/100000: episode: 1720, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 31.941, mean reward: 2.904 [2.597, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.442], loss: 0.110403, mae: 0.340033, mean_q: 4.045359
 80812/100000: episode: 1721, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 75.978, mean reward: 2.922 [2.374, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.475], loss: 0.079080, mae: 0.296162, mean_q: 3.994980
 80819/100000: episode: 1722, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 20.244, mean reward: 2.892 [2.165, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.206, 10.100], loss: 0.088339, mae: 0.295360, mean_q: 3.850616
 80830/100000: episode: 1723, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 32.482, mean reward: 2.953 [2.200, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.478], loss: 0.157465, mae: 0.384593, mean_q: 4.117249
 80846/100000: episode: 1724, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 48.945, mean reward: 3.059 [1.850, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.573, 10.466], loss: 0.105832, mae: 0.349263, mean_q: 4.069301
 80872/100000: episode: 1725, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 63.535, mean reward: 2.444 [1.942, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.491, 10.214], loss: 0.090675, mae: 0.309215, mean_q: 3.958831
 80898/100000: episode: 1726, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 74.800, mean reward: 2.877 [2.292, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.245, 10.369], loss: 0.083375, mae: 0.292209, mean_q: 4.012903
 80942/100000: episode: 1727, duration: 0.217s, episode steps: 44, steps per second: 203, episode reward: 135.130, mean reward: 3.071 [1.513, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.695, 10.125], loss: 0.094207, mae: 0.305162, mean_q: 4.019497
 80973/100000: episode: 1728, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 115.193, mean reward: 3.716 [2.336, 6.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.747, 10.100], loss: 0.089128, mae: 0.297997, mean_q: 4.011582
 80984/100000: episode: 1729, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 26.344, mean reward: 2.395 [1.994, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.386], loss: 0.094776, mae: 0.313890, mean_q: 4.046384
 81015/100000: episode: 1730, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 84.691, mean reward: 2.732 [1.583, 6.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.633, 10.100], loss: 0.096212, mae: 0.320111, mean_q: 3.997402
 81059/100000: episode: 1731, duration: 0.213s, episode steps: 44, steps per second: 206, episode reward: 203.429, mean reward: 4.623 [2.283, 8.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.840, 10.100], loss: 0.117069, mae: 0.335993, mean_q: 4.045100
 81085/100000: episode: 1732, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 65.501, mean reward: 2.519 [1.952, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.357, 10.430], loss: 0.114778, mae: 0.339621, mean_q: 4.139312
 81129/100000: episode: 1733, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 104.713, mean reward: 2.380 [1.558, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.234, 10.100], loss: 0.115608, mae: 0.337123, mean_q: 4.160193
 81141/100000: episode: 1734, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 36.930, mean reward: 3.077 [2.306, 5.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.108, 10.429], loss: 0.082460, mae: 0.294314, mean_q: 4.116515
 81153/100000: episode: 1735, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 29.202, mean reward: 2.433 [2.109, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.335], loss: 0.112037, mae: 0.361549, mean_q: 4.266325
 81165/100000: episode: 1736, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 34.084, mean reward: 2.840 [2.493, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.455], loss: 0.135359, mae: 0.363508, mean_q: 4.089989
 81172/100000: episode: 1737, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 15.685, mean reward: 2.241 [2.115, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.342, 10.100], loss: 0.134325, mae: 0.362921, mean_q: 4.206748
 81214/100000: episode: 1738, duration: 0.204s, episode steps: 42, steps per second: 206, episode reward: 99.118, mean reward: 2.360 [1.559, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-1.162, 10.100], loss: 0.100492, mae: 0.321239, mean_q: 4.121150
 81256/100000: episode: 1739, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 146.531, mean reward: 3.489 [1.937, 11.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.589, 10.100], loss: 0.142423, mae: 0.346852, mean_q: 4.150105
 81263/100000: episode: 1740, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 14.853, mean reward: 2.122 [1.873, 2.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.350, 10.100], loss: 0.096421, mae: 0.302861, mean_q: 4.267246
 81305/100000: episode: 1741, duration: 0.204s, episode steps: 42, steps per second: 206, episode reward: 106.288, mean reward: 2.531 [1.814, 7.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.262, 10.100], loss: 0.106981, mae: 0.326907, mean_q: 4.123728
 81312/100000: episode: 1742, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 16.106, mean reward: 2.301 [1.883, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.781, 10.100], loss: 0.265304, mae: 0.376970, mean_q: 4.000459
 81338/100000: episode: 1743, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 74.762, mean reward: 2.875 [1.794, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.162, 10.271], loss: 0.119665, mae: 0.326446, mean_q: 4.198852
 81345/100000: episode: 1744, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 16.431, mean reward: 2.347 [2.083, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.466, 10.100], loss: 0.155190, mae: 0.371930, mean_q: 4.025372
 81436/100000: episode: 1745, duration: 0.449s, episode steps: 91, steps per second: 203, episode reward: 172.083, mean reward: 1.891 [1.461, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-1.299, 10.100], loss: 0.119384, mae: 0.340967, mean_q: 4.149971
 81452/100000: episode: 1746, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 37.071, mean reward: 2.317 [1.921, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.894, 10.298], loss: 0.111413, mae: 0.337298, mean_q: 4.148702
 81468/100000: episode: 1747, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 30.054, mean reward: 1.878 [1.666, 2.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.054, 10.265], loss: 0.137821, mae: 0.354984, mean_q: 4.207790
 81479/100000: episode: 1748, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 25.706, mean reward: 2.337 [1.919, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.302], loss: 0.103286, mae: 0.300752, mean_q: 4.146437
 81521/100000: episode: 1749, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 109.428, mean reward: 2.605 [1.570, 5.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.456, 10.100], loss: 0.136695, mae: 0.338343, mean_q: 4.203222
 81547/100000: episode: 1750, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 93.914, mean reward: 3.612 [2.234, 8.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.588], loss: 0.118216, mae: 0.332773, mean_q: 4.187863
 81573/100000: episode: 1751, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 63.113, mean reward: 2.427 [2.014, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.846, 10.329], loss: 0.169145, mae: 0.380493, mean_q: 4.179282
 81580/100000: episode: 1752, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 15.526, mean reward: 2.218 [1.886, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.340, 10.100], loss: 0.116609, mae: 0.323993, mean_q: 4.146572
 81596/100000: episode: 1753, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 38.216, mean reward: 2.388 [1.764, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.478], loss: 0.170278, mae: 0.354708, mean_q: 4.194311
 81608/100000: episode: 1754, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 36.226, mean reward: 3.019 [2.323, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.223, 10.472], loss: 0.090036, mae: 0.300801, mean_q: 4.208630
 81650/100000: episode: 1755, duration: 0.205s, episode steps: 42, steps per second: 205, episode reward: 101.502, mean reward: 2.417 [1.820, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.658, 10.100], loss: 0.145218, mae: 0.350005, mean_q: 4.177067
 81661/100000: episode: 1756, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 32.035, mean reward: 2.912 [2.281, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.735, 10.359], loss: 0.179846, mae: 0.345394, mean_q: 4.160531
 81752/100000: episode: 1757, duration: 0.456s, episode steps: 91, steps per second: 200, episode reward: 180.716, mean reward: 1.986 [1.467, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-0.931, 10.146], loss: 0.148691, mae: 0.363308, mean_q: 4.236654
 81783/100000: episode: 1758, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 77.725, mean reward: 2.507 [1.937, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.214, 10.100], loss: 0.174352, mae: 0.372534, mean_q: 4.241216
 81827/100000: episode: 1759, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 150.744, mean reward: 3.426 [2.292, 5.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.633, 10.100], loss: 0.139065, mae: 0.359115, mean_q: 4.205776
 81843/100000: episode: 1760, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 32.710, mean reward: 2.044 [1.759, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.068, 10.274], loss: 0.121364, mae: 0.357179, mean_q: 4.237982
 81874/100000: episode: 1761, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 82.658, mean reward: 2.666 [1.692, 4.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.107, 10.100], loss: 0.129121, mae: 0.344306, mean_q: 4.254455
 81965/100000: episode: 1762, duration: 0.460s, episode steps: 91, steps per second: 198, episode reward: 163.177, mean reward: 1.793 [1.445, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.595, 10.137], loss: 0.129481, mae: 0.340600, mean_q: 4.229077
 81976/100000: episode: 1763, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 29.782, mean reward: 2.707 [2.293, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.461], loss: 0.170524, mae: 0.369820, mean_q: 4.280993
 81992/100000: episode: 1764, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 45.278, mean reward: 2.830 [1.644, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.156, 10.440], loss: 0.129462, mae: 0.349893, mean_q: 4.301477
 82036/100000: episode: 1765, duration: 0.217s, episode steps: 44, steps per second: 202, episode reward: 120.867, mean reward: 2.747 [1.895, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.425, 10.100], loss: 0.120313, mae: 0.349820, mean_q: 4.276122
 82047/100000: episode: 1766, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 28.877, mean reward: 2.625 [2.363, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.428], loss: 0.132626, mae: 0.371629, mean_q: 4.328977
 82063/100000: episode: 1767, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 35.909, mean reward: 2.244 [1.686, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.379, 10.381], loss: 0.135687, mae: 0.372848, mean_q: 4.234274
 82079/100000: episode: 1768, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 37.274, mean reward: 2.330 [1.839, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.140, 10.410], loss: 0.124054, mae: 0.346261, mean_q: 4.233076
 82095/100000: episode: 1769, duration: 0.084s, episode steps: 16, steps per second: 192, episode reward: 36.192, mean reward: 2.262 [1.699, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.052, 10.387], loss: 0.187854, mae: 0.373707, mean_q: 4.330758
 82111/100000: episode: 1770, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 36.303, mean reward: 2.269 [1.796, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.295, 10.334], loss: 0.169421, mae: 0.398503, mean_q: 4.319942
 82123/100000: episode: 1771, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 33.496, mean reward: 2.791 [2.304, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.036, 10.493], loss: 0.168120, mae: 0.377843, mean_q: 4.353758
 82134/100000: episode: 1772, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 31.178, mean reward: 2.834 [2.373, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.498], loss: 0.132569, mae: 0.354575, mean_q: 4.323803
 82178/100000: episode: 1773, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 105.683, mean reward: 2.402 [1.472, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.227, 10.100], loss: 0.143903, mae: 0.367253, mean_q: 4.313169
 82209/100000: episode: 1774, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 94.122, mean reward: 3.036 [2.014, 8.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.266, 10.100], loss: 0.110138, mae: 0.334970, mean_q: 4.265875
 82251/100000: episode: 1775, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 146.894, mean reward: 3.497 [2.514, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.456, 10.100], loss: 0.122047, mae: 0.336926, mean_q: 4.261866
 82342/100000: episode: 1776, duration: 0.463s, episode steps: 91, steps per second: 197, episode reward: 175.720, mean reward: 1.931 [1.445, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.365, 10.100], loss: 0.155480, mae: 0.368535, mean_q: 4.341093
 82368/100000: episode: 1777, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 63.043, mean reward: 2.425 [1.600, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.579, 10.231], loss: 0.135041, mae: 0.364946, mean_q: 4.365311
 82380/100000: episode: 1778, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 31.104, mean reward: 2.592 [2.170, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.429, 10.350], loss: 0.232964, mae: 0.407166, mean_q: 4.322795
 82406/100000: episode: 1779, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 65.047, mean reward: 2.502 [2.000, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.385], loss: 0.182351, mae: 0.381810, mean_q: 4.411178
 82448/100000: episode: 1780, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 110.177, mean reward: 2.623 [1.840, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.860, 10.100], loss: 0.182232, mae: 0.378759, mean_q: 4.396416
 82474/100000: episode: 1781, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 84.722, mean reward: 3.259 [2.016, 5.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.395, 10.493], loss: 0.159800, mae: 0.380508, mean_q: 4.387222
 82481/100000: episode: 1782, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 21.418, mean reward: 3.060 [2.538, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.260, 10.100], loss: 0.106127, mae: 0.336710, mean_q: 4.269660
 82488/100000: episode: 1783, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 16.198, mean reward: 2.314 [2.076, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.193, 10.100], loss: 0.259261, mae: 0.387701, mean_q: 4.351413
 82532/100000: episode: 1784, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 99.616, mean reward: 2.264 [1.677, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.811, 10.100], loss: 0.156543, mae: 0.377945, mean_q: 4.376515
 82576/100000: episode: 1785, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 122.146, mean reward: 2.776 [2.072, 6.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.576, 10.100], loss: 0.149889, mae: 0.374210, mean_q: 4.383512
[Info] 2-TH LEVEL FOUND: 7.66008186340332, Considering 10/90 traces
 82620/100000: episode: 1786, duration: 4.512s, episode steps: 44, steps per second: 10, episode reward: 102.076, mean reward: 2.320 [1.723, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.951, 10.100], loss: 0.155034, mae: 0.369078, mean_q: 4.372232
 82653/100000: episode: 1787, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 87.999, mean reward: 2.667 [1.483, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.472, 10.126], loss: 0.188577, mae: 0.393424, mean_q: 4.420277
 82687/100000: episode: 1788, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 187.294, mean reward: 5.509 [2.735, 29.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.314, 10.100], loss: 0.131904, mae: 0.347920, mean_q: 4.413145
 82714/100000: episode: 1789, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 102.064, mean reward: 3.780 [2.172, 6.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.318, 10.100], loss: 0.159625, mae: 0.381831, mean_q: 4.421263
 82746/100000: episode: 1790, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 83.710, mean reward: 2.616 [1.726, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.668, 10.100], loss: 0.149712, mae: 0.368001, mean_q: 4.421370
 82771/100000: episode: 1791, duration: 0.127s, episode steps: 25, steps per second: 198, episode reward: 95.417, mean reward: 3.817 [2.137, 6.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.356, 10.100], loss: 0.137937, mae: 0.354079, mean_q: 4.450237
 82803/100000: episode: 1792, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 121.041, mean reward: 3.783 [2.503, 6.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.333, 10.100], loss: 0.199866, mae: 0.396183, mean_q: 4.405440
 82827/100000: episode: 1793, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 60.372, mean reward: 2.516 [1.949, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.558, 10.100], loss: 0.251098, mae: 0.421757, mean_q: 4.470967
 82860/100000: episode: 1794, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 119.163, mean reward: 3.611 [2.357, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.540, 10.100], loss: 0.146780, mae: 0.367101, mean_q: 4.406292
 82885/100000: episode: 1795, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 67.025, mean reward: 2.681 [2.115, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.923, 10.100], loss: 0.701993, mae: 0.465622, mean_q: 4.515311
 82918/100000: episode: 1796, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 109.681, mean reward: 3.324 [2.049, 5.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.389, 10.100], loss: 0.170293, mae: 0.418841, mean_q: 4.469271
 82942/100000: episode: 1797, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 67.335, mean reward: 2.806 [2.354, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.824, 10.100], loss: 0.148687, mae: 0.366854, mean_q: 4.444768
 82969/100000: episode: 1798, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 79.865, mean reward: 2.958 [1.616, 5.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.100], loss: 0.192694, mae: 0.411688, mean_q: 4.508098
 83005/100000: episode: 1799, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 119.539, mean reward: 3.321 [1.876, 9.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.253, 10.100], loss: 0.184949, mae: 0.392691, mean_q: 4.515624
 83038/100000: episode: 1800, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 99.313, mean reward: 3.009 [1.625, 6.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.664, 10.100], loss: 0.168781, mae: 0.396840, mean_q: 4.575600
 83063/100000: episode: 1801, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 84.833, mean reward: 3.393 [2.639, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.755, 10.100], loss: 0.209028, mae: 0.400131, mean_q: 4.616627
 83096/100000: episode: 1802, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 136.867, mean reward: 4.147 [2.462, 8.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.718, 10.100], loss: 0.209886, mae: 0.405146, mean_q: 4.597466
 83132/100000: episode: 1803, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 179.431, mean reward: 4.984 [3.531, 8.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.870, 10.100], loss: 0.185398, mae: 0.394752, mean_q: 4.543962
 83156/100000: episode: 1804, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 66.249, mean reward: 2.760 [2.226, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.314, 10.100], loss: 0.211866, mae: 0.417623, mean_q: 4.647465
 83183/100000: episode: 1805, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 89.741, mean reward: 3.324 [2.506, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.540, 10.100], loss: 0.202852, mae: 0.411920, mean_q: 4.555699
 83210/100000: episode: 1806, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 139.820, mean reward: 5.179 [2.910, 9.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.538, 10.100], loss: 0.188858, mae: 0.409574, mean_q: 4.639168
 83243/100000: episode: 1807, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 94.574, mean reward: 2.866 [1.514, 10.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.585, 10.314], loss: 0.205629, mae: 0.435645, mean_q: 4.700877
 83278/100000: episode: 1808, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 141.319, mean reward: 4.038 [2.665, 8.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.399, 10.100], loss: 0.662443, mae: 0.517172, mean_q: 4.785975
 83305/100000: episode: 1809, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 101.560, mean reward: 3.761 [2.609, 5.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.397, 10.100], loss: 0.247245, mae: 0.472542, mean_q: 4.770437
 83339/100000: episode: 1810, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 107.074, mean reward: 3.149 [2.358, 6.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.396, 10.100], loss: 0.204497, mae: 0.409550, mean_q: 4.789668
 83371/100000: episode: 1811, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 86.679, mean reward: 2.709 [1.947, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.229, 10.100], loss: 0.237382, mae: 0.433415, mean_q: 4.840878
 83407/100000: episode: 1812, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 121.085, mean reward: 3.363 [1.593, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.525, 10.100], loss: 0.292672, mae: 0.475310, mean_q: 4.988726
 83440/100000: episode: 1813, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 92.223, mean reward: 2.795 [2.131, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.414, 10.100], loss: 0.212834, mae: 0.412421, mean_q: 4.837835
 83464/100000: episode: 1814, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 75.435, mean reward: 3.143 [2.188, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.140, 10.100], loss: 0.301332, mae: 0.500354, mean_q: 4.835807
 83498/100000: episode: 1815, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 118.804, mean reward: 3.494 [1.626, 6.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.700, 10.100], loss: 0.291668, mae: 0.484118, mean_q: 4.899601
 83523/100000: episode: 1816, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 71.266, mean reward: 2.851 [2.068, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.344, 10.100], loss: 0.299914, mae: 0.485390, mean_q: 4.924102
 83556/100000: episode: 1817, duration: 0.165s, episode steps: 33, steps per second: 199, episode reward: 86.094, mean reward: 2.609 [2.053, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.265, 10.100], loss: 0.232211, mae: 0.462512, mean_q: 4.947038
 83592/100000: episode: 1818, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 139.647, mean reward: 3.879 [2.579, 10.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.672, 10.100], loss: 0.289529, mae: 0.483246, mean_q: 4.898300
 83626/100000: episode: 1819, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 105.304, mean reward: 3.097 [2.000, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.677, 10.100], loss: 0.226057, mae: 0.457455, mean_q: 4.923578
 83661/100000: episode: 1820, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 126.652, mean reward: 3.619 [2.011, 13.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.129, 10.100], loss: 0.265384, mae: 0.448865, mean_q: 4.860265
 83695/100000: episode: 1821, duration: 0.164s, episode steps: 34, steps per second: 208, episode reward: 102.209, mean reward: 3.006 [2.210, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.632, 10.100], loss: 0.268168, mae: 0.461493, mean_q: 4.967952
 83722/100000: episode: 1822, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 85.969, mean reward: 3.184 [2.113, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.403, 10.100], loss: 0.202722, mae: 0.434013, mean_q: 4.976580
 83755/100000: episode: 1823, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 122.166, mean reward: 3.702 [2.245, 5.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.158, 10.100], loss: 0.562171, mae: 0.494831, mean_q: 4.991532
 83788/100000: episode: 1824, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 112.015, mean reward: 3.394 [2.063, 6.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.473, 10.100], loss: 0.216904, mae: 0.428767, mean_q: 4.986048
 83821/100000: episode: 1825, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 117.663, mean reward: 3.566 [2.463, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.439, 10.100], loss: 0.252287, mae: 0.487536, mean_q: 4.988022
 83855/100000: episode: 1826, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 117.669, mean reward: 3.461 [2.179, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.709, 10.100], loss: 0.278731, mae: 0.467570, mean_q: 5.008173
 83890/100000: episode: 1827, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 105.379, mean reward: 3.011 [2.114, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.692, 10.100], loss: 0.251177, mae: 0.458943, mean_q: 5.087114
 83917/100000: episode: 1828, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 145.503, mean reward: 5.389 [2.584, 11.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.282, 10.100], loss: 0.316064, mae: 0.471461, mean_q: 4.987276
 83942/100000: episode: 1829, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 97.274, mean reward: 3.891 [2.749, 5.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.074, 10.100], loss: 0.382865, mae: 0.519256, mean_q: 5.065444
 83966/100000: episode: 1830, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 59.679, mean reward: 2.487 [1.486, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.134, 10.100], loss: 0.395615, mae: 0.575910, mean_q: 5.214022
 84000/100000: episode: 1831, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: 102.205, mean reward: 3.006 [1.939, 4.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.265, 10.100], loss: 0.288041, mae: 0.498269, mean_q: 5.118837
 84034/100000: episode: 1832, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 148.619, mean reward: 4.371 [2.725, 10.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.375, 10.100], loss: 0.350434, mae: 0.524314, mean_q: 5.118541
 84059/100000: episode: 1833, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 67.317, mean reward: 2.693 [1.939, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.214, 10.100], loss: 0.304391, mae: 0.454206, mean_q: 5.129142
 84092/100000: episode: 1834, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 128.327, mean reward: 3.889 [1.696, 8.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.111, 10.100], loss: 0.322713, mae: 0.501018, mean_q: 5.156148
 84116/100000: episode: 1835, duration: 0.116s, episode steps: 24, steps per second: 206, episode reward: 81.398, mean reward: 3.392 [2.770, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.305, 10.100], loss: 0.306547, mae: 0.502824, mean_q: 5.199026
 84150/100000: episode: 1836, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 94.957, mean reward: 2.793 [2.126, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.212, 10.100], loss: 0.402098, mae: 0.558562, mean_q: 5.190599
 84184/100000: episode: 1837, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 116.617, mean reward: 3.430 [2.583, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.561, 10.100], loss: 0.342491, mae: 0.527753, mean_q: 5.239680
 84217/100000: episode: 1838, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 138.188, mean reward: 4.188 [2.640, 5.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.222, 10.100], loss: 0.287738, mae: 0.477382, mean_q: 5.213789
 84250/100000: episode: 1839, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 86.800, mean reward: 2.630 [1.476, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.035, 10.170], loss: 0.226744, mae: 0.458317, mean_q: 5.221439
 84286/100000: episode: 1840, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 161.381, mean reward: 4.483 [2.583, 6.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-1.363, 10.100], loss: 0.309769, mae: 0.493870, mean_q: 5.239239
 84310/100000: episode: 1841, duration: 0.118s, episode steps: 24, steps per second: 204, episode reward: 103.714, mean reward: 4.321 [2.476, 16.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.425, 10.100], loss: 1.182649, mae: 0.595763, mean_q: 5.430292
 84345/100000: episode: 1842, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 99.630, mean reward: 2.847 [2.203, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.076, 10.100], loss: 0.674091, mae: 0.558905, mean_q: 5.236429
 84369/100000: episode: 1843, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 88.578, mean reward: 3.691 [1.958, 5.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.572, 10.100], loss: 0.723727, mae: 0.523236, mean_q: 5.387775
 84402/100000: episode: 1844, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 89.979, mean reward: 2.727 [1.962, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.181, 10.100], loss: 0.345550, mae: 0.528751, mean_q: 5.364972
 84426/100000: episode: 1845, duration: 0.114s, episode steps: 24, steps per second: 210, episode reward: 72.439, mean reward: 3.018 [2.141, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.335, 10.100], loss: 0.355719, mae: 0.517432, mean_q: 5.417521
 84451/100000: episode: 1846, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 94.672, mean reward: 3.787 [2.355, 7.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.459, 10.100], loss: 0.338045, mae: 0.516650, mean_q: 5.415652
 84478/100000: episode: 1847, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 117.962, mean reward: 4.369 [3.392, 6.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.244, 10.100], loss: 0.289650, mae: 0.521344, mean_q: 5.406058
 84510/100000: episode: 1848, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 120.455, mean reward: 3.764 [2.699, 5.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.571, 10.100], loss: 0.297553, mae: 0.517143, mean_q: 5.451642
 84543/100000: episode: 1849, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 211.813, mean reward: 6.419 [3.075, 16.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.458, 10.100], loss: 0.343811, mae: 0.530051, mean_q: 5.452987
 84579/100000: episode: 1850, duration: 0.180s, episode steps: 36, steps per second: 199, episode reward: 126.509, mean reward: 3.514 [2.535, 6.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.229, 10.100], loss: 0.681003, mae: 0.585019, mean_q: 5.682516
 84606/100000: episode: 1851, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 88.763, mean reward: 3.288 [2.041, 5.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.530, 10.100], loss: 0.373500, mae: 0.522602, mean_q: 5.595393
 84642/100000: episode: 1852, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 166.040, mean reward: 4.612 [2.735, 18.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.425, 10.100], loss: 0.397798, mae: 0.526999, mean_q: 5.510158
 84676/100000: episode: 1853, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 168.054, mean reward: 4.943 [1.924, 9.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.908, 10.100], loss: 0.319216, mae: 0.541127, mean_q: 5.577779
 84711/100000: episode: 1854, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 205.989, mean reward: 5.885 [2.914, 34.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.526, 10.100], loss: 0.912054, mae: 0.602901, mean_q: 5.547955
 84736/100000: episode: 1855, duration: 0.122s, episode steps: 25, steps per second: 204, episode reward: 88.659, mean reward: 3.546 [2.180, 5.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.338, 10.100], loss: 0.446619, mae: 0.596935, mean_q: 5.613732
 84769/100000: episode: 1856, duration: 0.156s, episode steps: 33, steps per second: 211, episode reward: 148.596, mean reward: 4.503 [2.132, 8.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.084, 10.100], loss: 0.867291, mae: 0.640768, mean_q: 5.651242
 84793/100000: episode: 1857, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 64.878, mean reward: 2.703 [2.019, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.582, 10.100], loss: 0.891133, mae: 0.577033, mean_q: 5.625545
 84828/100000: episode: 1858, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 94.373, mean reward: 2.696 [1.715, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.761, 10.100], loss: 0.516512, mae: 0.621222, mean_q: 5.714848
 84855/100000: episode: 1859, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 118.477, mean reward: 4.388 [3.527, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.275, 10.100], loss: 0.588361, mae: 0.587887, mean_q: 5.701131
 84891/100000: episode: 1860, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 111.393, mean reward: 3.094 [2.222, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.930, 10.100], loss: 0.857664, mae: 0.684550, mean_q: 5.789444
 84924/100000: episode: 1861, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 90.150, mean reward: 2.732 [2.055, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.234, 10.100], loss: 0.482816, mae: 0.609394, mean_q: 5.830898
 84957/100000: episode: 1862, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 110.987, mean reward: 3.363 [1.617, 7.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.078, 10.100], loss: 0.856150, mae: 0.653524, mean_q: 5.749208
 84992/100000: episode: 1863, duration: 0.176s, episode steps: 35, steps per second: 198, episode reward: 108.878, mean reward: 3.111 [2.200, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.328, 10.100], loss: 0.770820, mae: 0.612180, mean_q: 5.677497
 85028/100000: episode: 1864, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 99.488, mean reward: 2.764 [1.674, 5.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.494, 10.100], loss: 0.759636, mae: 0.665801, mean_q: 5.824355
 85064/100000: episode: 1865, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 113.832, mean reward: 3.162 [2.000, 8.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.212, 10.100], loss: 0.348650, mae: 0.569501, mean_q: 5.735013
 85097/100000: episode: 1866, duration: 0.170s, episode steps: 33, steps per second: 195, episode reward: 105.071, mean reward: 3.184 [2.149, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.338, 10.100], loss: 0.496493, mae: 0.586239, mean_q: 5.834035
 85130/100000: episode: 1867, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 108.536, mean reward: 3.289 [1.811, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.313, 10.100], loss: 1.227215, mae: 0.659852, mean_q: 5.839525
[Info] FALSIFICATION!
[Info] Levels: [5.832977, 7.660082, 14.189515]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 85140/100000: episode: 1868, duration: 4.590s, episode steps: 10, steps per second: 2, episode reward: 151.893, mean reward: 15.189 [3.936, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.161, 9.404], loss: 0.835732, mae: 0.688923, mean_q: 5.766886
 85240/100000: episode: 1869, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.992, mean reward: 1.980 [1.489, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.443, 10.243], loss: 1.115033, mae: 0.782951, mean_q: 5.871503
 85340/100000: episode: 1870, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.857, mean reward: 1.839 [1.487, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.274, 10.098], loss: 0.596199, mae: 0.597539, mean_q: 5.875800
 85440/100000: episode: 1871, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.278, mean reward: 1.943 [1.463, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.319, 10.212], loss: 3.004300, mae: 0.777582, mean_q: 5.862275
 85540/100000: episode: 1872, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 195.257, mean reward: 1.953 [1.510, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.779, 10.128], loss: 1.461584, mae: 0.638911, mean_q: 5.803562
 85640/100000: episode: 1873, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 198.408, mean reward: 1.984 [1.496, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.098], loss: 0.464838, mae: 0.614697, mean_q: 5.887102
 85740/100000: episode: 1874, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.819, mean reward: 1.878 [1.446, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.282, 10.208], loss: 1.669637, mae: 0.729400, mean_q: 5.820212
 85840/100000: episode: 1875, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.189, mean reward: 1.922 [1.445, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.986, 10.112], loss: 0.489785, mae: 0.597713, mean_q: 5.773788
 85940/100000: episode: 1876, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 191.217, mean reward: 1.912 [1.440, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.600, 10.140], loss: 0.386890, mae: 0.560707, mean_q: 5.667125
 86040/100000: episode: 1877, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 178.488, mean reward: 1.785 [1.445, 2.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.098], loss: 0.732710, mae: 0.626530, mean_q: 5.720414
 86140/100000: episode: 1878, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 179.265, mean reward: 1.793 [1.465, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.781, 10.172], loss: 0.499669, mae: 0.568385, mean_q: 5.631289
 86240/100000: episode: 1879, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.043, mean reward: 1.910 [1.467, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.477, 10.173], loss: 1.903311, mae: 0.682341, mean_q: 5.604124
 86340/100000: episode: 1880, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.760, mean reward: 1.818 [1.438, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.483, 10.098], loss: 1.843605, mae: 0.687496, mean_q: 5.626351
 86440/100000: episode: 1881, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.772, mean reward: 1.818 [1.443, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.720, 10.098], loss: 0.796778, mae: 0.591481, mean_q: 5.635341
 86540/100000: episode: 1882, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 216.898, mean reward: 2.169 [1.433, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.709, 10.422], loss: 0.394963, mae: 0.549982, mean_q: 5.599230
 86640/100000: episode: 1883, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 198.446, mean reward: 1.984 [1.471, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.806, 10.098], loss: 2.770609, mae: 0.669461, mean_q: 5.578439
 86740/100000: episode: 1884, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.737, mean reward: 1.897 [1.440, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.617, 10.152], loss: 0.562215, mae: 0.570000, mean_q: 5.503638
 86840/100000: episode: 1885, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.284, mean reward: 1.873 [1.445, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.523, 10.098], loss: 0.615865, mae: 0.571715, mean_q: 5.551569
 86940/100000: episode: 1886, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 180.131, mean reward: 1.801 [1.488, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.433, 10.098], loss: 0.897536, mae: 0.602157, mean_q: 5.468580
 87040/100000: episode: 1887, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 190.671, mean reward: 1.907 [1.435, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.437, 10.245], loss: 1.672144, mae: 0.655259, mean_q: 5.510725
 87140/100000: episode: 1888, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 234.101, mean reward: 2.341 [1.582, 9.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.368, 10.098], loss: 1.627194, mae: 0.653769, mean_q: 5.525463
 87240/100000: episode: 1889, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 183.865, mean reward: 1.839 [1.438, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.252, 10.098], loss: 1.669124, mae: 0.667701, mean_q: 5.483336
 87340/100000: episode: 1890, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 211.612, mean reward: 2.116 [1.518, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.256, 10.098], loss: 0.726869, mae: 0.585403, mean_q: 5.438887
 87440/100000: episode: 1891, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.129, mean reward: 1.901 [1.480, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.709, 10.186], loss: 2.750097, mae: 0.682531, mean_q: 5.389096
 87540/100000: episode: 1892, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 195.004, mean reward: 1.950 [1.437, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.448, 10.304], loss: 0.692325, mae: 0.582919, mean_q: 5.481175
 87640/100000: episode: 1893, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.681, mean reward: 1.847 [1.454, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.357, 10.232], loss: 1.530782, mae: 0.602252, mean_q: 5.315085
 87740/100000: episode: 1894, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 259.885, mean reward: 2.599 [1.458, 6.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.199, 10.507], loss: 3.555241, mae: 0.718800, mean_q: 5.373305
 87840/100000: episode: 1895, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.156, mean reward: 1.882 [1.446, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.098], loss: 1.515127, mae: 0.631473, mean_q: 5.249313
 87940/100000: episode: 1896, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 225.292, mean reward: 2.253 [1.467, 10.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.629, 10.098], loss: 0.526791, mae: 0.541586, mean_q: 5.206095
 88040/100000: episode: 1897, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.112, mean reward: 1.841 [1.451, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.504, 10.098], loss: 0.611716, mae: 0.553508, mean_q: 5.149466
 88140/100000: episode: 1898, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 216.972, mean reward: 2.170 [1.480, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.567, 10.098], loss: 1.464495, mae: 0.580224, mean_q: 5.106617
 88240/100000: episode: 1899, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 190.829, mean reward: 1.908 [1.467, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.512, 10.098], loss: 1.351515, mae: 0.522442, mean_q: 5.075863
 88340/100000: episode: 1900, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 193.727, mean reward: 1.937 [1.476, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.120], loss: 0.309005, mae: 0.472204, mean_q: 4.940803
 88440/100000: episode: 1901, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 211.010, mean reward: 2.110 [1.462, 5.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.632, 10.098], loss: 1.405838, mae: 0.542901, mean_q: 4.966133
 88540/100000: episode: 1902, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.599, mean reward: 1.896 [1.466, 6.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.338, 10.340], loss: 0.375550, mae: 0.457494, mean_q: 4.858219
 88640/100000: episode: 1903, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 181.843, mean reward: 1.818 [1.463, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.721, 10.098], loss: 0.223015, mae: 0.422397, mean_q: 4.827163
 88740/100000: episode: 1904, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 201.561, mean reward: 2.016 [1.478, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.337, 10.199], loss: 3.240581, mae: 0.589099, mean_q: 4.927333
 88840/100000: episode: 1905, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 195.928, mean reward: 1.959 [1.531, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.583, 10.098], loss: 0.333681, mae: 0.455026, mean_q: 4.761344
 88940/100000: episode: 1906, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.471, mean reward: 1.885 [1.474, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.734, 10.320], loss: 2.195706, mae: 0.521701, mean_q: 4.669934
 89040/100000: episode: 1907, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.589, mean reward: 1.916 [1.442, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.711, 10.098], loss: 0.502260, mae: 0.449851, mean_q: 4.638916
 89140/100000: episode: 1908, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.539, mean reward: 1.915 [1.517, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.245, 10.098], loss: 2.343783, mae: 0.556506, mean_q: 4.556590
 89240/100000: episode: 1909, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 214.258, mean reward: 2.143 [1.511, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.199, 10.223], loss: 0.284224, mae: 0.431829, mean_q: 4.452566
 89340/100000: episode: 1910, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.344, mean reward: 1.883 [1.442, 3.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.206, 10.098], loss: 0.165651, mae: 0.368682, mean_q: 4.369976
 89440/100000: episode: 1911, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 183.218, mean reward: 1.832 [1.438, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.430, 10.263], loss: 0.288623, mae: 0.379500, mean_q: 4.301116
 89540/100000: episode: 1912, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 185.717, mean reward: 1.857 [1.488, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.041, 10.176], loss: 1.290259, mae: 0.451031, mean_q: 4.279191
 89640/100000: episode: 1913, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 206.488, mean reward: 2.065 [1.436, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.188, 10.233], loss: 0.157548, mae: 0.357087, mean_q: 4.174892
 89740/100000: episode: 1914, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.498, mean reward: 2.025 [1.452, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.144, 10.098], loss: 1.996210, mae: 0.418258, mean_q: 4.117629
 89840/100000: episode: 1915, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 185.309, mean reward: 1.853 [1.460, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.304, 10.147], loss: 0.130944, mae: 0.326422, mean_q: 3.988612
 89940/100000: episode: 1916, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 220.100, mean reward: 2.201 [1.502, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.675, 10.199], loss: 1.001108, mae: 0.377580, mean_q: 4.006794
 90040/100000: episode: 1917, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 190.264, mean reward: 1.903 [1.463, 2.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.719, 10.157], loss: 0.106121, mae: 0.315573, mean_q: 3.885299
 90140/100000: episode: 1918, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 193.687, mean reward: 1.937 [1.460, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.435], loss: 0.101227, mae: 0.299638, mean_q: 3.856797
 90240/100000: episode: 1919, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.119, mean reward: 1.961 [1.457, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.617, 10.154], loss: 0.076127, mae: 0.279523, mean_q: 3.835576
 90340/100000: episode: 1920, duration: 0.487s, episode steps: 100, steps per second: 206, episode reward: 193.555, mean reward: 1.936 [1.454, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.932, 10.098], loss: 0.104685, mae: 0.292822, mean_q: 3.861429
 90440/100000: episode: 1921, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.250, mean reward: 1.833 [1.435, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.189, 10.191], loss: 0.108373, mae: 0.310097, mean_q: 3.870978
 90540/100000: episode: 1922, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.545, mean reward: 1.835 [1.467, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.004, 10.098], loss: 0.084902, mae: 0.292055, mean_q: 3.855400
 90640/100000: episode: 1923, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 185.240, mean reward: 1.852 [1.496, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.211, 10.098], loss: 0.091849, mae: 0.295663, mean_q: 3.870685
 90740/100000: episode: 1924, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.142, mean reward: 1.811 [1.442, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.699, 10.114], loss: 0.087170, mae: 0.293973, mean_q: 3.867019
 90840/100000: episode: 1925, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 191.880, mean reward: 1.919 [1.456, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.432, 10.166], loss: 0.108100, mae: 0.305337, mean_q: 3.853419
 90940/100000: episode: 1926, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.095, mean reward: 1.811 [1.439, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.614, 10.174], loss: 0.107470, mae: 0.305053, mean_q: 3.854399
 91040/100000: episode: 1927, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 202.372, mean reward: 2.024 [1.485, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.392, 10.251], loss: 0.095679, mae: 0.290709, mean_q: 3.854057
 91140/100000: episode: 1928, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.422, mean reward: 1.874 [1.451, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.693, 10.098], loss: 0.086861, mae: 0.294467, mean_q: 3.853842
 91240/100000: episode: 1929, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.828, mean reward: 1.938 [1.473, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.248, 10.098], loss: 0.100405, mae: 0.304444, mean_q: 3.866924
 91340/100000: episode: 1930, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.595, mean reward: 1.906 [1.473, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.264, 10.332], loss: 0.105094, mae: 0.301357, mean_q: 3.864628
 91440/100000: episode: 1931, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 180.818, mean reward: 1.808 [1.446, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.342, 10.184], loss: 0.119368, mae: 0.303337, mean_q: 3.878432
 91540/100000: episode: 1932, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 211.101, mean reward: 2.111 [1.486, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.085581, mae: 0.290144, mean_q: 3.843812
 91640/100000: episode: 1933, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 187.751, mean reward: 1.878 [1.465, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.326, 10.098], loss: 0.093750, mae: 0.289481, mean_q: 3.862498
 91740/100000: episode: 1934, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 180.970, mean reward: 1.810 [1.442, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.098], loss: 0.117809, mae: 0.301180, mean_q: 3.897982
 91840/100000: episode: 1935, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 194.794, mean reward: 1.948 [1.464, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.588, 10.098], loss: 0.103072, mae: 0.291787, mean_q: 3.859930
 91940/100000: episode: 1936, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.789, mean reward: 1.888 [1.455, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.410, 10.098], loss: 0.105145, mae: 0.298688, mean_q: 3.881195
 92040/100000: episode: 1937, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 198.452, mean reward: 1.985 [1.490, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.098], loss: 0.095197, mae: 0.301057, mean_q: 3.888826
 92140/100000: episode: 1938, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 195.406, mean reward: 1.954 [1.457, 5.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.641, 10.146], loss: 0.086866, mae: 0.287671, mean_q: 3.861079
 92240/100000: episode: 1939, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.023, mean reward: 1.800 [1.453, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.291, 10.098], loss: 0.090475, mae: 0.292174, mean_q: 3.849501
 92340/100000: episode: 1940, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 181.113, mean reward: 1.811 [1.482, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.230], loss: 0.101726, mae: 0.304026, mean_q: 3.885206
 92440/100000: episode: 1941, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 196.315, mean reward: 1.963 [1.477, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.135], loss: 0.082752, mae: 0.290605, mean_q: 3.856826
 92540/100000: episode: 1942, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 181.939, mean reward: 1.819 [1.466, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.078360, mae: 0.284107, mean_q: 3.840636
 92640/100000: episode: 1943, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 220.500, mean reward: 2.205 [1.452, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.834, 10.098], loss: 0.080026, mae: 0.280046, mean_q: 3.855216
 92740/100000: episode: 1944, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 193.239, mean reward: 1.932 [1.516, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.098], loss: 0.087877, mae: 0.286453, mean_q: 3.841685
 92840/100000: episode: 1945, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.444, mean reward: 1.804 [1.460, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.955, 10.098], loss: 0.096912, mae: 0.282396, mean_q: 3.812449
 92940/100000: episode: 1946, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.034, mean reward: 1.850 [1.493, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.232], loss: 0.078186, mae: 0.282023, mean_q: 3.829021
 93040/100000: episode: 1947, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.946, mean reward: 1.999 [1.474, 6.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.903, 10.098], loss: 0.084738, mae: 0.287599, mean_q: 3.811540
 93140/100000: episode: 1948, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 183.968, mean reward: 1.840 [1.447, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.995, 10.098], loss: 0.074100, mae: 0.275996, mean_q: 3.808952
 93240/100000: episode: 1949, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 228.574, mean reward: 2.286 [1.464, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.276], loss: 0.079850, mae: 0.274864, mean_q: 3.799622
 93340/100000: episode: 1950, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 181.367, mean reward: 1.814 [1.439, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.143], loss: 0.085390, mae: 0.283399, mean_q: 3.804914
 93440/100000: episode: 1951, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 173.640, mean reward: 1.736 [1.432, 2.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.998, 10.214], loss: 0.081355, mae: 0.276803, mean_q: 3.800334
 93540/100000: episode: 1952, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.462, mean reward: 1.975 [1.460, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.557, 10.255], loss: 0.074900, mae: 0.274184, mean_q: 3.803818
 93640/100000: episode: 1953, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 192.028, mean reward: 1.920 [1.476, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.779, 10.278], loss: 0.073545, mae: 0.275249, mean_q: 3.793748
 93740/100000: episode: 1954, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 183.870, mean reward: 1.839 [1.443, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.213, 10.098], loss: 0.075151, mae: 0.272189, mean_q: 3.783871
 93840/100000: episode: 1955, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 191.558, mean reward: 1.916 [1.482, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.655, 10.098], loss: 0.072622, mae: 0.275408, mean_q: 3.788222
 93940/100000: episode: 1956, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.637, mean reward: 1.966 [1.443, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.560, 10.098], loss: 0.070474, mae: 0.271428, mean_q: 3.792981
 94040/100000: episode: 1957, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.112, mean reward: 1.941 [1.455, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.330], loss: 0.074123, mae: 0.275122, mean_q: 3.806886
 94140/100000: episode: 1958, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 188.521, mean reward: 1.885 [1.446, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.562, 10.098], loss: 0.081970, mae: 0.286737, mean_q: 3.799788
 94240/100000: episode: 1959, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.099, mean reward: 1.831 [1.461, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.545, 10.098], loss: 0.074782, mae: 0.273079, mean_q: 3.792393
 94340/100000: episode: 1960, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 180.942, mean reward: 1.809 [1.440, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.075106, mae: 0.276724, mean_q: 3.803403
 94440/100000: episode: 1961, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 216.309, mean reward: 2.163 [1.440, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.211, 10.396], loss: 0.086492, mae: 0.284864, mean_q: 3.796428
 94540/100000: episode: 1962, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 181.697, mean reward: 1.817 [1.459, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.165, 10.098], loss: 0.077578, mae: 0.278117, mean_q: 3.803611
 94640/100000: episode: 1963, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 206.252, mean reward: 2.063 [1.486, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.712, 10.098], loss: 0.077896, mae: 0.279464, mean_q: 3.783298
 94740/100000: episode: 1964, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 185.960, mean reward: 1.860 [1.481, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.288, 10.258], loss: 0.064448, mae: 0.262631, mean_q: 3.782300
 94840/100000: episode: 1965, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.620, mean reward: 1.926 [1.456, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.947, 10.098], loss: 0.074305, mae: 0.269375, mean_q: 3.762043
 94940/100000: episode: 1966, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.353, mean reward: 1.874 [1.434, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.324, 10.098], loss: 0.066301, mae: 0.267967, mean_q: 3.793837
 95040/100000: episode: 1967, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 180.976, mean reward: 1.810 [1.456, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.676, 10.165], loss: 0.072302, mae: 0.274252, mean_q: 3.793960
[Info] 1-TH LEVEL FOUND: 5.330318450927734, Considering 10/90 traces
 95140/100000: episode: 1968, duration: 4.755s, episode steps: 100, steps per second: 21, episode reward: 183.234, mean reward: 1.832 [1.448, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.829, 10.198], loss: 0.075737, mae: 0.274566, mean_q: 3.784182
 95158/100000: episode: 1969, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 40.965, mean reward: 2.276 [1.865, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.294, 10.100], loss: 0.103887, mae: 0.297206, mean_q: 3.804944
 95178/100000: episode: 1970, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 53.925, mean reward: 2.696 [2.137, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.371, 10.100], loss: 0.070241, mae: 0.270594, mean_q: 3.765662
 95196/100000: episode: 1971, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 41.634, mean reward: 2.313 [1.942, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.285, 10.100], loss: 0.080468, mae: 0.295319, mean_q: 3.785507
 95243/100000: episode: 1972, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 117.311, mean reward: 2.496 [1.882, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.703, 10.100], loss: 0.068392, mae: 0.263198, mean_q: 3.779194
 95261/100000: episode: 1973, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 60.000, mean reward: 3.333 [2.044, 5.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.334, 10.100], loss: 0.065548, mae: 0.267456, mean_q: 3.780529
 95267/100000: episode: 1974, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 13.514, mean reward: 2.252 [1.835, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.323, 10.100], loss: 0.079374, mae: 0.261653, mean_q: 3.734158
 95280/100000: episode: 1975, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 43.662, mean reward: 3.359 [2.384, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.824, 10.482], loss: 0.083288, mae: 0.274243, mean_q: 3.806877
 95307/100000: episode: 1976, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 64.619, mean reward: 2.393 [1.743, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.048, 10.100], loss: 0.076098, mae: 0.281601, mean_q: 3.811066
 95313/100000: episode: 1977, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 18.434, mean reward: 3.072 [2.355, 4.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.750, 10.100], loss: 0.056683, mae: 0.258074, mean_q: 3.828393
 95347/100000: episode: 1978, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 84.912, mean reward: 2.497 [2.138, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.683, 10.417], loss: 0.090242, mae: 0.276235, mean_q: 3.822570
 95358/100000: episode: 1979, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 28.075, mean reward: 2.552 [2.052, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.459, 10.100], loss: 0.105135, mae: 0.291921, mean_q: 3.803594
 95364/100000: episode: 1980, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 14.811, mean reward: 2.468 [2.204, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.493, 10.100], loss: 0.079038, mae: 0.296057, mean_q: 3.828011
 95391/100000: episode: 1981, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 67.887, mean reward: 2.514 [2.054, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.639, 10.100], loss: 0.088746, mae: 0.281543, mean_q: 3.830419
 95425/100000: episode: 1982, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 91.956, mean reward: 2.705 [2.088, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.654, 10.322], loss: 0.076200, mae: 0.280518, mean_q: 3.828612
 95431/100000: episode: 1983, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 19.506, mean reward: 3.251 [2.413, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.210, 10.100], loss: 0.104634, mae: 0.292166, mean_q: 3.751673
 95444/100000: episode: 1984, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 41.913, mean reward: 3.224 [2.586, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.371, 10.498], loss: 0.086177, mae: 0.300131, mean_q: 3.883814
 95478/100000: episode: 1985, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 130.002, mean reward: 3.824 [2.065, 6.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.444, 10.448], loss: 0.069336, mae: 0.271111, mean_q: 3.842752
 95491/100000: episode: 1986, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 34.186, mean reward: 2.630 [2.318, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.360], loss: 0.106068, mae: 0.335936, mean_q: 3.970781
 95502/100000: episode: 1987, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 26.231, mean reward: 2.385 [1.941, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.301, 10.100], loss: 0.079228, mae: 0.277089, mean_q: 3.790255
 95549/100000: episode: 1988, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 105.133, mean reward: 2.237 [1.456, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.235, 10.218], loss: 0.096924, mae: 0.298925, mean_q: 3.905983
 95560/100000: episode: 1989, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 27.015, mean reward: 2.456 [1.818, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.390, 10.100], loss: 0.125688, mae: 0.310101, mean_q: 3.867059
 95588/100000: episode: 1990, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 92.286, mean reward: 3.296 [2.149, 6.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.305, 10.100], loss: 0.099463, mae: 0.303102, mean_q: 3.889170
 95616/100000: episode: 1991, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 61.659, mean reward: 2.202 [1.794, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.302, 10.100], loss: 0.111394, mae: 0.316008, mean_q: 3.908842
 95650/100000: episode: 1992, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 101.805, mean reward: 2.994 [1.954, 5.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.512, 10.588], loss: 0.087425, mae: 0.300737, mean_q: 3.918257
 95684/100000: episode: 1993, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 88.462, mean reward: 2.602 [1.623, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.412, 10.239], loss: 0.096130, mae: 0.304156, mean_q: 3.890055
 95718/100000: episode: 1994, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 88.930, mean reward: 2.616 [1.997, 4.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.383], loss: 0.089353, mae: 0.301513, mean_q: 3.913608
 95729/100000: episode: 1995, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 24.884, mean reward: 2.262 [1.911, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.244, 10.100], loss: 0.087367, mae: 0.309343, mean_q: 3.962942
 95764/100000: episode: 1996, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 76.969, mean reward: 2.199 [1.761, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.499, 10.382], loss: 0.089651, mae: 0.288375, mean_q: 3.902019
 95811/100000: episode: 1997, duration: 0.232s, episode steps: 47, steps per second: 202, episode reward: 98.699, mean reward: 2.100 [1.558, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.194, 10.100], loss: 0.097210, mae: 0.308500, mean_q: 3.939316
 95845/100000: episode: 1998, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 144.008, mean reward: 4.236 [2.227, 8.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.037, 10.530], loss: 0.101123, mae: 0.318828, mean_q: 3.995846
 95856/100000: episode: 1999, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 29.003, mean reward: 2.637 [1.874, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.395, 10.100], loss: 0.098259, mae: 0.288964, mean_q: 3.888458
 95903/100000: episode: 2000, duration: 0.242s, episode steps: 47, steps per second: 195, episode reward: 102.389, mean reward: 2.178 [1.492, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.242, 10.137], loss: 0.096128, mae: 0.304480, mean_q: 3.980556
 95909/100000: episode: 2001, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 14.519, mean reward: 2.420 [2.104, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.234, 10.100], loss: 0.164221, mae: 0.395414, mean_q: 4.096373
 95943/100000: episode: 2002, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 123.354, mean reward: 3.628 [2.256, 6.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.607, 10.499], loss: 0.127590, mae: 0.338165, mean_q: 4.005128
 95963/100000: episode: 2003, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 47.955, mean reward: 2.398 [1.973, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.127, 10.100], loss: 0.088622, mae: 0.301665, mean_q: 4.000902
 95991/100000: episode: 2004, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 86.257, mean reward: 3.081 [2.253, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.303, 10.100], loss: 0.109493, mae: 0.323452, mean_q: 4.019044
 96009/100000: episode: 2005, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 54.745, mean reward: 3.041 [2.068, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.374, 10.100], loss: 0.101209, mae: 0.312195, mean_q: 4.053024
 96036/100000: episode: 2006, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 74.987, mean reward: 2.777 [1.852, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.153, 10.100], loss: 0.104086, mae: 0.307123, mean_q: 4.077033
 96054/100000: episode: 2007, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 52.128, mean reward: 2.896 [2.341, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.391, 10.100], loss: 0.101922, mae: 0.328232, mean_q: 4.092653
 96081/100000: episode: 2008, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 82.818, mean reward: 3.067 [1.924, 6.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.317, 10.100], loss: 0.080166, mae: 0.289743, mean_q: 3.992478
 96087/100000: episode: 2009, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 14.238, mean reward: 2.373 [1.858, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.422, 10.100], loss: 0.100850, mae: 0.301707, mean_q: 4.020813
 96115/100000: episode: 2010, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 74.745, mean reward: 2.669 [2.134, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.721, 10.100], loss: 0.102938, mae: 0.321835, mean_q: 4.107915
 96135/100000: episode: 2011, duration: 0.117s, episode steps: 20, steps per second: 170, episode reward: 53.991, mean reward: 2.700 [2.169, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.443, 10.100], loss: 0.118262, mae: 0.331605, mean_q: 4.050437
 96170/100000: episode: 2012, duration: 0.175s, episode steps: 35, steps per second: 201, episode reward: 146.116, mean reward: 4.175 [2.217, 7.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.304, 10.475], loss: 0.119439, mae: 0.331798, mean_q: 4.064230
 96183/100000: episode: 2013, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 37.051, mean reward: 2.850 [2.204, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.437, 10.355], loss: 0.119283, mae: 0.329965, mean_q: 4.024077
 96211/100000: episode: 2014, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 72.702, mean reward: 2.597 [2.088, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.816, 10.100], loss: 0.143841, mae: 0.339095, mean_q: 4.135231
 96229/100000: episode: 2015, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 44.290, mean reward: 2.461 [2.105, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.311, 10.100], loss: 0.094553, mae: 0.310705, mean_q: 4.104612
 96235/100000: episode: 2016, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 16.314, mean reward: 2.719 [2.035, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.313, 10.100], loss: 0.101308, mae: 0.311295, mean_q: 4.067111
 96269/100000: episode: 2017, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 94.865, mean reward: 2.790 [1.863, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.998, 10.257], loss: 0.130871, mae: 0.342396, mean_q: 4.150806
 96282/100000: episode: 2018, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 33.791, mean reward: 2.599 [2.293, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.984, 10.418], loss: 0.110294, mae: 0.325217, mean_q: 4.061444
 96302/100000: episode: 2019, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 78.860, mean reward: 3.943 [2.730, 7.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.192, 10.100], loss: 0.164302, mae: 0.375559, mean_q: 4.227500
 96320/100000: episode: 2020, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 46.088, mean reward: 2.560 [2.065, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.239, 10.100], loss: 0.124814, mae: 0.337327, mean_q: 4.119174
 96367/100000: episode: 2021, duration: 0.237s, episode steps: 47, steps per second: 198, episode reward: 110.281, mean reward: 2.346 [1.550, 4.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.835, 10.100], loss: 0.123732, mae: 0.346893, mean_q: 4.128216
 96394/100000: episode: 2022, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 88.439, mean reward: 3.276 [1.945, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.163, 10.100], loss: 0.112476, mae: 0.334501, mean_q: 4.100186
 96400/100000: episode: 2023, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 14.532, mean reward: 2.422 [1.977, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.186, 10.100], loss: 0.148195, mae: 0.377998, mean_q: 4.228831
 96413/100000: episode: 2024, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 31.276, mean reward: 2.406 [2.015, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.362], loss: 0.135023, mae: 0.362624, mean_q: 4.184999
 96460/100000: episode: 2025, duration: 0.232s, episode steps: 47, steps per second: 203, episode reward: 109.409, mean reward: 2.328 [1.577, 4.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.912, 10.100], loss: 0.123699, mae: 0.334416, mean_q: 4.165853
 96466/100000: episode: 2026, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 13.971, mean reward: 2.328 [2.121, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.600, 10.100], loss: 0.128185, mae: 0.326122, mean_q: 4.116233
 96472/100000: episode: 2027, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 16.271, mean reward: 2.712 [2.076, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.442, 10.100], loss: 0.196046, mae: 0.371289, mean_q: 4.197932
 96485/100000: episode: 2028, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 35.368, mean reward: 2.721 [2.363, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.409], loss: 0.133523, mae: 0.346583, mean_q: 4.198489
 96519/100000: episode: 2029, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 105.432, mean reward: 3.101 [2.060, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.420, 10.397], loss: 0.125179, mae: 0.343954, mean_q: 4.203999
 96553/100000: episode: 2030, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 103.860, mean reward: 3.055 [1.923, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.443], loss: 0.131541, mae: 0.347808, mean_q: 4.215562
 96573/100000: episode: 2031, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 96.455, mean reward: 4.823 [2.602, 33.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.163, 10.100], loss: 0.112387, mae: 0.334724, mean_q: 4.176896
 96620/100000: episode: 2032, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 155.857, mean reward: 3.316 [2.204, 5.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.514, 10.100], loss: 0.733763, mae: 0.398403, mean_q: 4.224956
 96647/100000: episode: 2033, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 70.261, mean reward: 2.602 [2.035, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.331, 10.100], loss: 0.167596, mae: 0.379790, mean_q: 4.232350
 96694/100000: episode: 2034, duration: 0.236s, episode steps: 47, steps per second: 200, episode reward: 113.562, mean reward: 2.416 [1.730, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.545, 10.100], loss: 0.173360, mae: 0.377605, mean_q: 4.288910
 96728/100000: episode: 2035, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 92.207, mean reward: 2.712 [1.836, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.264, 10.334], loss: 0.126203, mae: 0.342716, mean_q: 4.288933
 96739/100000: episode: 2036, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 29.008, mean reward: 2.637 [2.233, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.547, 10.100], loss: 0.118966, mae: 0.346663, mean_q: 4.369570
 96750/100000: episode: 2037, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 26.605, mean reward: 2.419 [2.065, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.356, 10.100], loss: 0.144426, mae: 0.355934, mean_q: 4.275091
 96768/100000: episode: 2038, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 56.049, mean reward: 3.114 [2.012, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.839, 10.100], loss: 0.144273, mae: 0.356469, mean_q: 4.244905
 96803/100000: episode: 2039, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 89.446, mean reward: 2.556 [1.858, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.485, 10.282], loss: 0.143627, mae: 0.378180, mean_q: 4.319216
 96821/100000: episode: 2040, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 53.531, mean reward: 2.974 [2.096, 5.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.446, 10.100], loss: 0.153430, mae: 0.352417, mean_q: 4.332282
 96849/100000: episode: 2041, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 80.626, mean reward: 2.880 [1.820, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.575, 10.100], loss: 0.196889, mae: 0.386754, mean_q: 4.339913
 96855/100000: episode: 2042, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 13.360, mean reward: 2.227 [2.095, 2.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.387, 10.100], loss: 0.184948, mae: 0.417984, mean_q: 4.379560
 96882/100000: episode: 2043, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 68.211, mean reward: 2.526 [2.178, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.075, 10.100], loss: 0.138747, mae: 0.362289, mean_q: 4.347315
 96910/100000: episode: 2044, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 71.999, mean reward: 2.571 [1.754, 4.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.943, 10.100], loss: 0.134755, mae: 0.360869, mean_q: 4.296455
 96916/100000: episode: 2045, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 15.509, mean reward: 2.585 [2.042, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.285, 10.100], loss: 0.090045, mae: 0.302382, mean_q: 4.407045
 96929/100000: episode: 2046, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 43.891, mean reward: 3.376 [2.320, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.562, 10.546], loss: 0.141144, mae: 0.357292, mean_q: 4.394131
 96964/100000: episode: 2047, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 96.711, mean reward: 2.763 [1.862, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.741, 10.428], loss: 0.153673, mae: 0.371750, mean_q: 4.340892
 96992/100000: episode: 2048, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 74.426, mean reward: 2.658 [2.011, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.756, 10.100], loss: 0.202233, mae: 0.403118, mean_q: 4.359941
 97039/100000: episode: 2049, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 131.815, mean reward: 2.805 [1.706, 4.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.762, 10.100], loss: 0.148220, mae: 0.374635, mean_q: 4.412576
 97073/100000: episode: 2050, duration: 0.173s, episode steps: 34, steps per second: 196, episode reward: 90.512, mean reward: 2.662 [1.963, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.842, 10.386], loss: 0.124606, mae: 0.354689, mean_q: 4.340253
 97107/100000: episode: 2051, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 89.132, mean reward: 2.622 [1.895, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.065, 10.437], loss: 0.566522, mae: 0.413203, mean_q: 4.420266
 97142/100000: episode: 2052, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 76.646, mean reward: 2.190 [1.625, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.585, 10.301], loss: 0.132029, mae: 0.368105, mean_q: 4.407640
 97148/100000: episode: 2053, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 14.017, mean reward: 2.336 [2.165, 2.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.226, 10.100], loss: 0.128615, mae: 0.383765, mean_q: 4.489363
 97182/100000: episode: 2054, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 93.097, mean reward: 2.738 [1.971, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.273, 10.352], loss: 0.601667, mae: 0.435546, mean_q: 4.450259
 97209/100000: episode: 2055, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 117.358, mean reward: 4.347 [2.220, 11.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.619, 10.100], loss: 0.149721, mae: 0.378217, mean_q: 4.404080
 97227/100000: episode: 2056, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 54.360, mean reward: 3.020 [2.504, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.505, 10.100], loss: 0.140896, mae: 0.372825, mean_q: 4.320625
 97261/100000: episode: 2057, duration: 0.168s, episode steps: 34, steps per second: 203, episode reward: 84.255, mean reward: 2.478 [1.665, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.425, 10.191], loss: 0.172125, mae: 0.395645, mean_q: 4.451881
[Info] 2-TH LEVEL FOUND: 7.547628879547119, Considering 10/90 traces
 97279/100000: episode: 2058, duration: 4.297s, episode steps: 18, steps per second: 4, episode reward: 46.145, mean reward: 2.564 [1.957, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.794, 10.100], loss: 0.182435, mae: 0.408653, mean_q: 4.566507
 97287/100000: episode: 2059, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 27.523, mean reward: 3.440 [2.900, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.066, 10.335], loss: 0.130009, mae: 0.369070, mean_q: 4.526202
 97310/100000: episode: 2060, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 81.999, mean reward: 3.565 [1.958, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.490, 10.329], loss: 0.154530, mae: 0.391912, mean_q: 4.469092
 97325/100000: episode: 2061, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 99.530, mean reward: 6.635 [3.331, 13.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.606, 10.100], loss: 0.133043, mae: 0.374930, mean_q: 4.479040
 97333/100000: episode: 2062, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 37.433, mean reward: 4.679 [4.045, 5.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.559], loss: 0.239555, mae: 0.419197, mean_q: 4.422951
 97341/100000: episode: 2063, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 31.101, mean reward: 3.888 [3.218, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.521], loss: 0.163669, mae: 0.384625, mean_q: 4.384813
 97364/100000: episode: 2064, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 71.494, mean reward: 3.108 [2.043, 5.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.306], loss: 0.218442, mae: 0.420818, mean_q: 4.428685
 97384/100000: episode: 2065, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 67.170, mean reward: 3.358 [2.407, 4.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.762, 10.426], loss: 0.209565, mae: 0.414765, mean_q: 4.536513
 97410/100000: episode: 2066, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 77.745, mean reward: 2.990 [2.162, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.480], loss: 0.219512, mae: 0.414026, mean_q: 4.573298
 97418/100000: episode: 2067, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 29.885, mean reward: 3.736 [3.190, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.547], loss: 0.160805, mae: 0.415971, mean_q: 4.597780
 97444/100000: episode: 2068, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 105.216, mean reward: 4.047 [3.191, 4.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.459], loss: 0.760432, mae: 0.448720, mean_q: 4.530650
 97457/100000: episode: 2069, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 54.998, mean reward: 4.231 [2.709, 5.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.093, 10.596], loss: 1.222593, mae: 0.447302, mean_q: 4.578223
 97483/100000: episode: 2070, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 90.044, mean reward: 3.463 [2.395, 5.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.216, 10.542], loss: 0.179327, mae: 0.412650, mean_q: 4.609792
 97491/100000: episode: 2071, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 35.836, mean reward: 4.480 [2.914, 8.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.675, 10.365], loss: 0.130547, mae: 0.373851, mean_q: 4.556237
 97514/100000: episode: 2072, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 150.049, mean reward: 6.524 [4.413, 10.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.571], loss: 0.176257, mae: 0.400044, mean_q: 4.522449
 97538/100000: episode: 2073, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 75.593, mean reward: 3.150 [2.048, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.377, 10.358], loss: 0.163741, mae: 0.382238, mean_q: 4.620508
 97551/100000: episode: 2074, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 43.372, mean reward: 3.336 [2.811, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.477], loss: 0.231274, mae: 0.428560, mean_q: 4.564904
 97571/100000: episode: 2075, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 52.147, mean reward: 2.607 [1.521, 5.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.207, 10.201], loss: 0.237685, mae: 0.437832, mean_q: 4.756493
 97595/100000: episode: 2076, duration: 0.120s, episode steps: 24, steps per second: 199, episode reward: 78.677, mean reward: 3.278 [2.381, 4.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.706, 10.525], loss: 0.194268, mae: 0.407615, mean_q: 4.675066
 97613/100000: episode: 2077, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 53.117, mean reward: 2.951 [2.630, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.300, 10.380], loss: 1.032053, mae: 0.516491, mean_q: 4.691520
 97633/100000: episode: 2078, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 85.228, mean reward: 4.261 [3.263, 5.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.241, 10.517], loss: 0.295654, mae: 0.511630, mean_q: 4.582866
 97656/100000: episode: 2079, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 58.205, mean reward: 2.531 [1.852, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.369, 10.341], loss: 0.218234, mae: 0.463937, mean_q: 4.750439
 97679/100000: episode: 2080, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 102.865, mean reward: 4.472 [2.979, 6.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.490, 10.515], loss: 0.230566, mae: 0.421766, mean_q: 4.589488
 97703/100000: episode: 2081, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 77.373, mean reward: 3.224 [2.402, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.455], loss: 0.823694, mae: 0.486181, mean_q: 4.720199
 97728/100000: episode: 2082, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 108.139, mean reward: 4.326 [3.429, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.550], loss: 0.234628, mae: 0.451036, mean_q: 4.769345
 97752/100000: episode: 2083, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 74.041, mean reward: 3.085 [2.194, 5.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.386], loss: 0.255310, mae: 0.459871, mean_q: 4.872984
[Info] FALSIFICATION!
[Info] Levels: [5.3303185, 7.547629, 9.984227]
[Info] Cond. Prob: [0.1, 0.1, 0.05]
[Info] Error Prob: 0.0005000000000000001

 97766/100000: episode: 2084, duration: 4.445s, episode steps: 14, steps per second: 3, episode reward: 225.943, mean reward: 16.139 [4.051, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.796, 10.098], loss: 1.305311, mae: 0.592222, mean_q: 4.883875
 97866/100000: episode: 2085, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 184.632, mean reward: 1.846 [1.445, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.055, 10.098], loss: 1.895531, mae: 0.601534, mean_q: 4.820976
 97966/100000: episode: 2086, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 192.605, mean reward: 1.926 [1.484, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.229, 10.106], loss: 0.291645, mae: 0.465920, mean_q: 4.773086
 98066/100000: episode: 2087, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.664, mean reward: 1.887 [1.435, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.559, 10.098], loss: 0.422302, mae: 0.487427, mean_q: 4.852653
 98166/100000: episode: 2088, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.057, mean reward: 1.971 [1.471, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.286], loss: 0.299330, mae: 0.454124, mean_q: 4.840008
 98266/100000: episode: 2089, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 201.357, mean reward: 2.014 [1.447, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.579, 10.344], loss: 1.732183, mae: 0.584351, mean_q: 4.765977
 98366/100000: episode: 2090, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 206.781, mean reward: 2.068 [1.491, 4.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.537, 10.127], loss: 1.905701, mae: 0.548812, mean_q: 4.892177
 98466/100000: episode: 2091, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 187.076, mean reward: 1.871 [1.449, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.043, 10.130], loss: 1.742051, mae: 0.629583, mean_q: 4.908752
 98566/100000: episode: 2092, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 216.934, mean reward: 2.169 [1.578, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.070, 10.335], loss: 2.005662, mae: 0.625980, mean_q: 4.945390
 98666/100000: episode: 2093, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 192.104, mean reward: 1.921 [1.459, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.532, 10.127], loss: 0.371352, mae: 0.485463, mean_q: 4.871359
 98766/100000: episode: 2094, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 189.195, mean reward: 1.892 [1.467, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.794, 10.098], loss: 4.280657, mae: 0.734354, mean_q: 4.967738
 98866/100000: episode: 2095, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 197.388, mean reward: 1.974 [1.505, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.417], loss: 0.398283, mae: 0.493883, mean_q: 4.896828
 98966/100000: episode: 2096, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 189.790, mean reward: 1.898 [1.470, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.098], loss: 1.863297, mae: 0.556169, mean_q: 4.925951
 99066/100000: episode: 2097, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.337, mean reward: 1.963 [1.451, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.573, 10.098], loss: 1.685133, mae: 0.616306, mean_q: 4.931445
 99166/100000: episode: 2098, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 221.880, mean reward: 2.219 [1.526, 4.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.192, 10.204], loss: 1.647596, mae: 0.562366, mean_q: 4.928945
 99266/100000: episode: 2099, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.540, mean reward: 1.935 [1.450, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.931, 10.143], loss: 1.699817, mae: 0.550188, mean_q: 4.913536
 99366/100000: episode: 2100, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 175.008, mean reward: 1.750 [1.451, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.982, 10.185], loss: 2.817438, mae: 0.663243, mean_q: 4.887009
 99466/100000: episode: 2101, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 190.020, mean reward: 1.900 [1.467, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.058, 10.098], loss: 4.164367, mae: 0.665146, mean_q: 5.000077
 99566/100000: episode: 2102, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.024, mean reward: 1.840 [1.461, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.399, 10.098], loss: 2.101408, mae: 0.660611, mean_q: 5.010190
 99666/100000: episode: 2103, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.016, mean reward: 1.860 [1.442, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.271, 10.098], loss: 2.814769, mae: 0.621515, mean_q: 4.963453
 99766/100000: episode: 2104, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 196.620, mean reward: 1.966 [1.512, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.222, 10.325], loss: 1.630325, mae: 0.568631, mean_q: 4.915847
 99866/100000: episode: 2105, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 207.177, mean reward: 2.072 [1.448, 5.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.476, 10.296], loss: 0.315953, mae: 0.489702, mean_q: 4.861904
 99966/100000: episode: 2106, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 191.044, mean reward: 1.910 [1.461, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.612, 10.151], loss: 1.865317, mae: 0.561110, mean_q: 4.951931
done, took 589.089 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
