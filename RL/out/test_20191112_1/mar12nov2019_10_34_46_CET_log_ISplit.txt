Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.158s, episode steps: 100, steps per second: 634, episode reward: 204.426, mean reward: 2.044 [1.446, 6.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.427], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 185.305, mean reward: 1.853 [1.482, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.497, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 183.654, mean reward: 1.837 [1.460, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.421, 10.233], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 214.555, mean reward: 2.146 [1.468, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.404, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 203.843, mean reward: 2.038 [1.437, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.791, 10.343], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 186.035, mean reward: 1.860 [1.476, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.457, 10.243], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.073s, episode steps: 100, steps per second: 1376, episode reward: 203.486, mean reward: 2.035 [1.441, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.518, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 182.587, mean reward: 1.826 [1.487, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.436, 10.206], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 191.723, mean reward: 1.917 [1.448, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.986, 10.261], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 185.030, mean reward: 1.850 [1.451, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.708, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 225.619, mean reward: 2.256 [1.457, 5.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.046, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.067s, episode steps: 100, steps per second: 1487, episode reward: 180.501, mean reward: 1.805 [1.483, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 186.620, mean reward: 1.866 [1.466, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.050, 10.140], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.061s, episode steps: 100, steps per second: 1630, episode reward: 190.300, mean reward: 1.903 [1.460, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.584, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 197.068, mean reward: 1.971 [1.458, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.540, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.071s, episode steps: 100, steps per second: 1408, episode reward: 200.719, mean reward: 2.007 [1.470, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.371, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 181.259, mean reward: 1.813 [1.430, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.145], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.074s, episode steps: 100, steps per second: 1351, episode reward: 189.339, mean reward: 1.893 [1.528, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.281], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.061s, episode steps: 100, steps per second: 1653, episode reward: 215.085, mean reward: 2.151 [1.444, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.129, 10.423], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 201.591, mean reward: 2.016 [1.497, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.185], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 193.233, mean reward: 1.932 [1.481, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.676, 10.336], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 189.375, mean reward: 1.894 [1.442, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.979, 10.144], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.059s, episode steps: 100, steps per second: 1690, episode reward: 190.004, mean reward: 1.900 [1.460, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.239], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.062s, episode steps: 100, steps per second: 1621, episode reward: 188.313, mean reward: 1.883 [1.443, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.065s, episode steps: 100, steps per second: 1536, episode reward: 198.045, mean reward: 1.980 [1.458, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.080, 10.133], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: 222.251, mean reward: 2.223 [1.466, 13.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.540, 10.285], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 201.651, mean reward: 2.017 [1.458, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.610, 10.233], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 196.327, mean reward: 1.963 [1.451, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.207], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 200.167, mean reward: 2.002 [1.488, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.972, 10.242], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.068s, episode steps: 100, steps per second: 1478, episode reward: 226.455, mean reward: 2.265 [1.486, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.877, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.066s, episode steps: 100, steps per second: 1514, episode reward: 178.607, mean reward: 1.786 [1.445, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.965, 10.181], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.060s, episode steps: 100, steps per second: 1672, episode reward: 184.971, mean reward: 1.850 [1.517, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.887, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 192.915, mean reward: 1.929 [1.508, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.126, 10.150], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 228.785, mean reward: 2.288 [1.508, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.403, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.071s, episode steps: 100, steps per second: 1418, episode reward: 179.450, mean reward: 1.794 [1.450, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.690, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 195.663, mean reward: 1.957 [1.447, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.960, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.061s, episode steps: 100, steps per second: 1640, episode reward: 195.775, mean reward: 1.958 [1.447, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.351, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 189.450, mean reward: 1.894 [1.475, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.762, 10.175], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 193.209, mean reward: 1.932 [1.447, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.156, 10.175], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 191.598, mean reward: 1.916 [1.482, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.252, 10.209], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 185.442, mean reward: 1.854 [1.449, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.013, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 190.799, mean reward: 1.908 [1.432, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.145], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 183.623, mean reward: 1.836 [1.448, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.642, 10.109], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.061s, episode steps: 100, steps per second: 1631, episode reward: 198.209, mean reward: 1.982 [1.454, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.964, 10.166], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.061s, episode steps: 100, steps per second: 1631, episode reward: 179.602, mean reward: 1.796 [1.450, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.458, 10.284], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.070s, episode steps: 100, steps per second: 1419, episode reward: 193.102, mean reward: 1.931 [1.459, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.432, 10.100], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.068s, episode steps: 100, steps per second: 1469, episode reward: 192.388, mean reward: 1.924 [1.481, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.868, 10.177], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 201.223, mean reward: 2.012 [1.439, 5.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.352, 10.330], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 187.819, mean reward: 1.878 [1.467, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.793, 10.297], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 181.459, mean reward: 1.815 [1.453, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.589, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 182.176, mean reward: 1.822 [1.490, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.705, 10.098], loss: 0.181537, mae: 0.398179, mean_q: 3.547607
  5200/100000: episode: 52, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 191.204, mean reward: 1.912 [1.447, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.804, 10.127], loss: 0.140613, mae: 0.344559, mean_q: 3.635529
  5300/100000: episode: 53, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 219.096, mean reward: 2.191 [1.455, 6.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.098], loss: 0.180730, mae: 0.349889, mean_q: 3.726768
  5400/100000: episode: 54, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 191.081, mean reward: 1.911 [1.453, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.900, 10.098], loss: 0.121338, mae: 0.331682, mean_q: 3.764153
  5500/100000: episode: 55, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 191.188, mean reward: 1.912 [1.450, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.608, 10.098], loss: 0.110803, mae: 0.311933, mean_q: 3.759727
  5600/100000: episode: 56, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.399, mean reward: 1.924 [1.497, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.750, 10.196], loss: 0.153637, mae: 0.327067, mean_q: 3.798107
  5700/100000: episode: 57, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.020, mean reward: 1.980 [1.457, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.840, 10.416], loss: 0.150435, mae: 0.315640, mean_q: 3.803301
  5800/100000: episode: 58, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.999, mean reward: 1.850 [1.495, 2.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.292, 10.098], loss: 0.124712, mae: 0.324027, mean_q: 3.826674
  5900/100000: episode: 59, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.572, mean reward: 1.826 [1.451, 5.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.439, 10.098], loss: 0.121148, mae: 0.319609, mean_q: 3.812809
  6000/100000: episode: 60, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.373, mean reward: 1.904 [1.449, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.434], loss: 0.115537, mae: 0.323082, mean_q: 3.842618
  6100/100000: episode: 61, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.673, mean reward: 1.967 [1.489, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.215, 10.098], loss: 0.145686, mae: 0.333622, mean_q: 3.853329
  6200/100000: episode: 62, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.924, mean reward: 1.929 [1.436, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.535, 10.176], loss: 0.118310, mae: 0.323719, mean_q: 3.835763
  6300/100000: episode: 63, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 191.050, mean reward: 1.911 [1.459, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.309, 10.098], loss: 0.145297, mae: 0.331421, mean_q: 3.862294
  6400/100000: episode: 64, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 203.820, mean reward: 2.038 [1.487, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.711, 10.098], loss: 0.136638, mae: 0.315268, mean_q: 3.856124
  6500/100000: episode: 65, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 197.279, mean reward: 1.973 [1.453, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.216, 10.098], loss: 0.145722, mae: 0.324614, mean_q: 3.851569
  6600/100000: episode: 66, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.245, mean reward: 1.842 [1.452, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.973, 10.125], loss: 0.148392, mae: 0.323024, mean_q: 3.845017
  6700/100000: episode: 67, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 198.181, mean reward: 1.982 [1.453, 8.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.597, 10.098], loss: 0.117917, mae: 0.321902, mean_q: 3.846352
  6800/100000: episode: 68, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 214.236, mean reward: 2.142 [1.493, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.003, 10.144], loss: 0.139942, mae: 0.324103, mean_q: 3.858086
  6900/100000: episode: 69, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 205.750, mean reward: 2.057 [1.467, 5.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.098], loss: 0.143303, mae: 0.325135, mean_q: 3.849869
  7000/100000: episode: 70, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.124, mean reward: 1.921 [1.484, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.546, 10.098], loss: 0.151989, mae: 0.337648, mean_q: 3.849502
  7100/100000: episode: 71, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 211.787, mean reward: 2.118 [1.477, 5.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.486, 10.098], loss: 0.131559, mae: 0.321965, mean_q: 3.851673
  7200/100000: episode: 72, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.074, mean reward: 1.971 [1.482, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.674, 10.151], loss: 0.140326, mae: 0.338660, mean_q: 3.862272
  7300/100000: episode: 73, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.515, mean reward: 1.915 [1.485, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.470, 10.463], loss: 0.137101, mae: 0.321706, mean_q: 3.854970
  7400/100000: episode: 74, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.576, mean reward: 1.916 [1.465, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.785, 10.180], loss: 0.163326, mae: 0.337064, mean_q: 3.862540
  7500/100000: episode: 75, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 177.649, mean reward: 1.776 [1.460, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.098], loss: 0.121331, mae: 0.327674, mean_q: 3.857975
  7600/100000: episode: 76, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 216.148, mean reward: 2.161 [1.445, 6.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.098], loss: 0.107714, mae: 0.312218, mean_q: 3.851291
  7700/100000: episode: 77, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.976, mean reward: 1.920 [1.483, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.936, 10.191], loss: 0.126291, mae: 0.328369, mean_q: 3.851022
  7800/100000: episode: 78, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 218.368, mean reward: 2.184 [1.553, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.172, 10.098], loss: 0.100712, mae: 0.317083, mean_q: 3.852513
  7900/100000: episode: 79, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 220.642, mean reward: 2.206 [1.484, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.427], loss: 0.099099, mae: 0.313104, mean_q: 3.837068
  8000/100000: episode: 80, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 209.534, mean reward: 2.095 [1.464, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.098], loss: 0.136702, mae: 0.332188, mean_q: 3.863990
  8100/100000: episode: 81, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.871, mean reward: 1.849 [1.434, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.133, 10.171], loss: 0.105500, mae: 0.313031, mean_q: 3.856522
  8200/100000: episode: 82, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 180.544, mean reward: 1.805 [1.438, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.707, 10.098], loss: 0.123651, mae: 0.330250, mean_q: 3.845268
  8300/100000: episode: 83, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 199.814, mean reward: 1.998 [1.513, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.392, 10.292], loss: 0.124480, mae: 0.334437, mean_q: 3.875240
  8400/100000: episode: 84, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 182.941, mean reward: 1.829 [1.472, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.413, 10.247], loss: 0.107492, mae: 0.318297, mean_q: 3.854006
  8500/100000: episode: 85, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.975, mean reward: 2.000 [1.457, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.561, 10.098], loss: 0.126506, mae: 0.328993, mean_q: 3.844690
  8600/100000: episode: 86, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 182.423, mean reward: 1.824 [1.439, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.681, 10.098], loss: 0.100411, mae: 0.307301, mean_q: 3.840836
  8700/100000: episode: 87, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 173.548, mean reward: 1.735 [1.451, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.544, 10.190], loss: 0.105730, mae: 0.311774, mean_q: 3.828918
  8800/100000: episode: 88, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.835, mean reward: 1.848 [1.486, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.661, 10.098], loss: 0.112948, mae: 0.313732, mean_q: 3.840173
  8900/100000: episode: 89, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.457, mean reward: 1.895 [1.464, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.120], loss: 0.102567, mae: 0.312635, mean_q: 3.840761
  9000/100000: episode: 90, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.679, mean reward: 1.917 [1.463, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.206, 10.257], loss: 0.098550, mae: 0.310275, mean_q: 3.816993
  9100/100000: episode: 91, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 187.298, mean reward: 1.873 [1.504, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.470, 10.207], loss: 0.107802, mae: 0.314981, mean_q: 3.843208
  9200/100000: episode: 92, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.424, mean reward: 2.034 [1.471, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.864, 10.295], loss: 0.112531, mae: 0.307092, mean_q: 3.838149
  9300/100000: episode: 93, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 181.741, mean reward: 1.817 [1.438, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.564, 10.098], loss: 0.114450, mae: 0.311378, mean_q: 3.842009
  9400/100000: episode: 94, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 176.284, mean reward: 1.763 [1.458, 2.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.098], loss: 0.108475, mae: 0.314309, mean_q: 3.839996
  9500/100000: episode: 95, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 194.816, mean reward: 1.948 [1.482, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.169, 10.204], loss: 0.113400, mae: 0.311539, mean_q: 3.848541
  9600/100000: episode: 96, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.978, mean reward: 1.920 [1.468, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.252], loss: 0.114483, mae: 0.316742, mean_q: 3.839288
  9700/100000: episode: 97, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 208.053, mean reward: 2.081 [1.458, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.752, 10.098], loss: 0.098437, mae: 0.306523, mean_q: 3.837928
  9800/100000: episode: 98, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.481, mean reward: 1.955 [1.484, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.702, 10.098], loss: 0.098597, mae: 0.298777, mean_q: 3.821856
  9900/100000: episode: 99, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 183.035, mean reward: 1.830 [1.439, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.128], loss: 0.107264, mae: 0.304903, mean_q: 3.841537
 10000/100000: episode: 100, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.182, mean reward: 1.872 [1.487, 4.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.431, 10.098], loss: 0.109270, mae: 0.314522, mean_q: 3.841106
 10100/100000: episode: 101, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.982, mean reward: 1.960 [1.478, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.692, 10.160], loss: 0.115465, mae: 0.322590, mean_q: 3.842971
 10200/100000: episode: 102, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 200.548, mean reward: 2.005 [1.434, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.823, 10.351], loss: 0.086469, mae: 0.297989, mean_q: 3.841226
 10300/100000: episode: 103, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.655, mean reward: 1.857 [1.464, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.975, 10.098], loss: 0.097405, mae: 0.306898, mean_q: 3.825392
 10400/100000: episode: 104, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 195.121, mean reward: 1.951 [1.448, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.207, 10.098], loss: 0.106671, mae: 0.315046, mean_q: 3.842889
 10500/100000: episode: 105, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.728, mean reward: 1.877 [1.459, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.214], loss: 0.098364, mae: 0.302814, mean_q: 3.835753
 10600/100000: episode: 106, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 212.500, mean reward: 2.125 [1.510, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.367, 10.098], loss: 0.106205, mae: 0.306912, mean_q: 3.839030
 10700/100000: episode: 107, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 177.484, mean reward: 1.775 [1.464, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.747, 10.127], loss: 0.106367, mae: 0.306022, mean_q: 3.835870
 10800/100000: episode: 108, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 196.494, mean reward: 1.965 [1.440, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.805, 10.098], loss: 0.106745, mae: 0.304523, mean_q: 3.825124
 10900/100000: episode: 109, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.519, mean reward: 1.945 [1.503, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.198, 10.157], loss: 0.106177, mae: 0.312309, mean_q: 3.848684
 11000/100000: episode: 110, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.086, mean reward: 1.831 [1.441, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.290], loss: 0.099436, mae: 0.308394, mean_q: 3.841443
 11100/100000: episode: 111, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 214.505, mean reward: 2.145 [1.502, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.479, 10.351], loss: 0.108448, mae: 0.313003, mean_q: 3.845104
 11200/100000: episode: 112, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.594, mean reward: 1.956 [1.488, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.330, 10.098], loss: 0.098730, mae: 0.307188, mean_q: 3.838058
 11300/100000: episode: 113, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.248, mean reward: 1.842 [1.454, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.182, 10.154], loss: 0.099056, mae: 0.303069, mean_q: 3.844016
 11400/100000: episode: 114, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 204.180, mean reward: 2.042 [1.471, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.622, 10.098], loss: 0.094746, mae: 0.308306, mean_q: 3.851355
 11500/100000: episode: 115, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 184.824, mean reward: 1.848 [1.437, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.344, 10.098], loss: 0.093903, mae: 0.305127, mean_q: 3.834510
 11600/100000: episode: 116, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 192.751, mean reward: 1.928 [1.500, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.098], loss: 0.095125, mae: 0.308197, mean_q: 3.848272
 11700/100000: episode: 117, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 217.871, mean reward: 2.179 [1.522, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.532, 10.098], loss: 0.097153, mae: 0.299551, mean_q: 3.845034
 11800/100000: episode: 118, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 197.587, mean reward: 1.976 [1.461, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.575, 10.098], loss: 0.095618, mae: 0.310237, mean_q: 3.851214
 11900/100000: episode: 119, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 210.847, mean reward: 2.108 [1.473, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.513, 10.098], loss: 0.092003, mae: 0.303347, mean_q: 3.856399
 12000/100000: episode: 120, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.125, mean reward: 1.811 [1.448, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.797, 10.115], loss: 0.094921, mae: 0.303586, mean_q: 3.854427
 12100/100000: episode: 121, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.510, mean reward: 1.835 [1.478, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.244, 10.098], loss: 0.102828, mae: 0.310680, mean_q: 3.849231
 12200/100000: episode: 122, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.453, mean reward: 1.875 [1.480, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.098], loss: 0.090442, mae: 0.300158, mean_q: 3.842111
 12300/100000: episode: 123, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.991, mean reward: 1.880 [1.490, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.798, 10.123], loss: 0.095994, mae: 0.308033, mean_q: 3.863546
 12400/100000: episode: 124, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.308, mean reward: 1.803 [1.453, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.789, 10.098], loss: 0.087680, mae: 0.299980, mean_q: 3.832556
 12500/100000: episode: 125, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 202.810, mean reward: 2.028 [1.447, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.705, 10.276], loss: 0.086257, mae: 0.291290, mean_q: 3.837427
 12600/100000: episode: 126, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 191.822, mean reward: 1.918 [1.444, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.597, 10.098], loss: 0.076623, mae: 0.279908, mean_q: 3.808447
 12700/100000: episode: 127, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 182.943, mean reward: 1.829 [1.472, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.976, 10.098], loss: 0.090955, mae: 0.298455, mean_q: 3.827682
 12800/100000: episode: 128, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.424, mean reward: 1.834 [1.477, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.159, 10.154], loss: 0.076867, mae: 0.281124, mean_q: 3.807218
 12900/100000: episode: 129, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.994, mean reward: 1.850 [1.480, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.098], loss: 0.081060, mae: 0.288028, mean_q: 3.794950
 13000/100000: episode: 130, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.552, mean reward: 1.886 [1.481, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.159], loss: 0.077315, mae: 0.280913, mean_q: 3.783824
 13100/100000: episode: 131, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.465, mean reward: 1.945 [1.472, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.707, 10.098], loss: 0.080028, mae: 0.285330, mean_q: 3.806075
 13200/100000: episode: 132, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.545, mean reward: 1.975 [1.529, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.098, 10.098], loss: 0.082738, mae: 0.291782, mean_q: 3.794754
 13300/100000: episode: 133, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 192.561, mean reward: 1.926 [1.502, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.482, 10.104], loss: 0.074298, mae: 0.272767, mean_q: 3.784038
 13400/100000: episode: 134, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.049, mean reward: 1.890 [1.534, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.453, 10.098], loss: 0.076883, mae: 0.282315, mean_q: 3.797187
 13500/100000: episode: 135, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 188.250, mean reward: 1.882 [1.465, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.286, 10.098], loss: 0.073163, mae: 0.280350, mean_q: 3.784934
 13600/100000: episode: 136, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 186.501, mean reward: 1.865 [1.464, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.190, 10.098], loss: 0.075684, mae: 0.278706, mean_q: 3.795167
 13700/100000: episode: 137, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.064, mean reward: 1.891 [1.455, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.048, 10.098], loss: 0.070657, mae: 0.265338, mean_q: 3.780544
 13800/100000: episode: 138, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 183.430, mean reward: 1.834 [1.480, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.464, 10.102], loss: 0.078685, mae: 0.284147, mean_q: 3.787356
 13900/100000: episode: 139, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.046, mean reward: 1.860 [1.441, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.194, 10.164], loss: 0.083496, mae: 0.289705, mean_q: 3.782014
 14000/100000: episode: 140, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.320, mean reward: 1.863 [1.459, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.617, 10.103], loss: 0.075009, mae: 0.279777, mean_q: 3.795761
 14100/100000: episode: 141, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 217.279, mean reward: 2.173 [1.447, 6.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.098], loss: 0.079801, mae: 0.285440, mean_q: 3.790594
 14200/100000: episode: 142, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 232.899, mean reward: 2.329 [1.443, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.341], loss: 0.080996, mae: 0.285456, mean_q: 3.795823
 14300/100000: episode: 143, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 233.699, mean reward: 2.337 [1.465, 5.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.709, 10.227], loss: 0.080806, mae: 0.283208, mean_q: 3.807603
 14400/100000: episode: 144, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.355, mean reward: 1.834 [1.453, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.700, 10.108], loss: 0.086378, mae: 0.299117, mean_q: 3.805894
 14500/100000: episode: 145, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 204.697, mean reward: 2.047 [1.469, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.387, 10.161], loss: 0.085147, mae: 0.296158, mean_q: 3.813224
 14600/100000: episode: 146, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 222.072, mean reward: 2.221 [1.527, 5.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.101, 10.308], loss: 0.090743, mae: 0.302646, mean_q: 3.824106
 14700/100000: episode: 147, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 187.999, mean reward: 1.880 [1.451, 8.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.058, 10.098], loss: 0.086528, mae: 0.288114, mean_q: 3.822524
 14800/100000: episode: 148, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.042, mean reward: 1.870 [1.490, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.996, 10.098], loss: 0.097341, mae: 0.305733, mean_q: 3.831640
 14900/100000: episode: 149, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.600, mean reward: 1.816 [1.460, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.796, 10.163], loss: 0.083463, mae: 0.293384, mean_q: 3.813938
[Info] 1-TH LEVEL FOUND: 4.405862331390381, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.167s, episode steps: 100, steps per second: 19, episode reward: 182.374, mean reward: 1.824 [1.469, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.328, 10.098], loss: 0.108283, mae: 0.311713, mean_q: 3.847695
 15011/100000: episode: 151, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 35.578, mean reward: 3.234 [2.267, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.090, 10.100], loss: 0.122889, mae: 0.331874, mean_q: 3.851551
 15022/100000: episode: 152, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 24.391, mean reward: 2.217 [1.793, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.309, 10.100], loss: 0.095643, mae: 0.308625, mean_q: 3.815977
[Info] FALSIFICATION!
[Info] Levels: [4.4058623, 6.2501965]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 15039/100000: episode: 153, duration: 4.683s, episode steps: 17, steps per second: 4, episode reward: 256.838, mean reward: 15.108 [3.207, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.563, 10.100], loss: 0.075191, mae: 0.290600, mean_q: 3.850030
 15139/100000: episode: 154, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.878, mean reward: 1.949 [1.458, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.019, 10.292], loss: 0.158595, mae: 0.331505, mean_q: 3.866534
 15239/100000: episode: 155, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 218.077, mean reward: 2.181 [1.498, 5.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.676, 10.098], loss: 3.150316, mae: 0.503803, mean_q: 3.937002
 15339/100000: episode: 156, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.682, mean reward: 1.957 [1.454, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.224, 10.098], loss: 1.670047, mae: 0.412514, mean_q: 3.932759
 15439/100000: episode: 157, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.086, mean reward: 1.821 [1.442, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.533, 10.098], loss: 0.190073, mae: 0.340242, mean_q: 3.847966
 15539/100000: episode: 158, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 178.511, mean reward: 1.785 [1.474, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.098], loss: 0.323871, mae: 0.361011, mean_q: 3.886814
 15639/100000: episode: 159, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.616, mean reward: 1.906 [1.457, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.830, 10.098], loss: 0.158774, mae: 0.329522, mean_q: 3.847487
 15739/100000: episode: 160, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.189, mean reward: 1.942 [1.466, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.273, 10.130], loss: 0.297718, mae: 0.340396, mean_q: 3.893711
 15839/100000: episode: 161, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 187.180, mean reward: 1.872 [1.475, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.312, 10.169], loss: 1.537866, mae: 0.416163, mean_q: 3.904037
 15939/100000: episode: 162, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 185.824, mean reward: 1.858 [1.479, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.730, 10.131], loss: 0.164698, mae: 0.328809, mean_q: 3.883429
 16039/100000: episode: 163, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.767, mean reward: 1.878 [1.442, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.457, 10.202], loss: 0.112145, mae: 0.308896, mean_q: 3.861672
 16139/100000: episode: 164, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 180.207, mean reward: 1.802 [1.441, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.040, 10.098], loss: 2.786723, mae: 0.475467, mean_q: 3.956397
 16239/100000: episode: 165, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 179.883, mean reward: 1.799 [1.440, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.717, 10.232], loss: 0.334221, mae: 0.347373, mean_q: 3.877099
 16339/100000: episode: 166, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.428, mean reward: 1.824 [1.458, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.004, 10.125], loss: 0.439125, mae: 0.347080, mean_q: 3.877303
 16439/100000: episode: 167, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 191.396, mean reward: 1.914 [1.505, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.769, 10.134], loss: 2.690655, mae: 0.441280, mean_q: 3.886936
 16539/100000: episode: 168, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 204.527, mean reward: 2.045 [1.490, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.311, 10.098], loss: 2.902537, mae: 0.550777, mean_q: 3.979581
 16639/100000: episode: 169, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 187.055, mean reward: 1.871 [1.488, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.098], loss: 0.171172, mae: 0.312853, mean_q: 3.864370
 16739/100000: episode: 170, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 238.840, mean reward: 2.388 [1.502, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.951, 10.246], loss: 1.371974, mae: 0.382251, mean_q: 3.886753
 16839/100000: episode: 171, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.954, mean reward: 1.870 [1.443, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.691, 10.120], loss: 1.510890, mae: 0.420605, mean_q: 3.902639
 16939/100000: episode: 172, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 189.001, mean reward: 1.890 [1.521, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.619, 10.170], loss: 0.191173, mae: 0.344494, mean_q: 3.898775
 17039/100000: episode: 173, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 176.235, mean reward: 1.762 [1.464, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.239, 10.098], loss: 2.477053, mae: 0.422398, mean_q: 3.936879
 17139/100000: episode: 174, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 207.294, mean reward: 2.073 [1.473, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.621, 10.098], loss: 3.655582, mae: 0.522567, mean_q: 3.996001
 17239/100000: episode: 175, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 184.160, mean reward: 1.842 [1.457, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.634, 10.098], loss: 0.473310, mae: 0.397101, mean_q: 3.961337
 17339/100000: episode: 176, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.323, mean reward: 1.883 [1.449, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.195], loss: 0.311181, mae: 0.350865, mean_q: 3.910216
 17439/100000: episode: 177, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.781, mean reward: 1.838 [1.466, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.098], loss: 2.447015, mae: 0.442030, mean_q: 3.942868
 17539/100000: episode: 178, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.348, mean reward: 1.953 [1.505, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.361, 10.098], loss: 0.193307, mae: 0.353945, mean_q: 3.862525
 17639/100000: episode: 179, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.107, mean reward: 1.971 [1.485, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.527, 10.098], loss: 1.384358, mae: 0.422092, mean_q: 3.905399
 17739/100000: episode: 180, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 219.872, mean reward: 2.199 [1.479, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.391, 10.098], loss: 0.313020, mae: 0.360404, mean_q: 3.906048
 17839/100000: episode: 181, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 204.555, mean reward: 2.046 [1.480, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.152, 10.098], loss: 1.228330, mae: 0.373523, mean_q: 3.905630
 17939/100000: episode: 182, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.162, mean reward: 1.902 [1.468, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.364], loss: 0.329154, mae: 0.389381, mean_q: 3.921309
 18039/100000: episode: 183, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.455, mean reward: 1.945 [1.442, 4.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.479, 10.310], loss: 3.535731, mae: 0.576663, mean_q: 3.998672
 18139/100000: episode: 184, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 190.873, mean reward: 1.909 [1.486, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.098], loss: 1.393777, mae: 0.413584, mean_q: 3.994460
 18239/100000: episode: 185, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 198.796, mean reward: 1.988 [1.493, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.605, 10.098], loss: 2.328661, mae: 0.473726, mean_q: 3.979197
 18339/100000: episode: 186, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.946, mean reward: 2.019 [1.493, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.671, 10.260], loss: 0.219042, mae: 0.373448, mean_q: 3.945916
 18439/100000: episode: 187, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.561, mean reward: 1.856 [1.432, 2.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.332, 10.161], loss: 2.068529, mae: 0.505326, mean_q: 4.014664
 18539/100000: episode: 188, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 211.104, mean reward: 2.111 [1.456, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.922, 10.098], loss: 0.171390, mae: 0.331989, mean_q: 3.899504
 18639/100000: episode: 189, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 237.353, mean reward: 2.374 [1.452, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.528, 10.399], loss: 0.318436, mae: 0.360144, mean_q: 3.905109
 18739/100000: episode: 190, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.216, mean reward: 1.882 [1.446, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.707, 10.106], loss: 0.182373, mae: 0.351513, mean_q: 3.954144
 18839/100000: episode: 191, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 204.522, mean reward: 2.045 [1.452, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.830, 10.240], loss: 0.299285, mae: 0.359795, mean_q: 3.964445
 18939/100000: episode: 192, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 180.543, mean reward: 1.805 [1.437, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.841, 10.098], loss: 3.078059, mae: 0.537679, mean_q: 4.059057
 19039/100000: episode: 193, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 208.817, mean reward: 2.088 [1.491, 4.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.347, 10.325], loss: 1.145002, mae: 0.448908, mean_q: 3.978215
 19139/100000: episode: 194, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.508, mean reward: 1.945 [1.524, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.254, 10.098], loss: 0.225753, mae: 0.375980, mean_q: 3.909705
 19239/100000: episode: 195, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 179.817, mean reward: 1.798 [1.452, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.164, 10.160], loss: 0.196920, mae: 0.343906, mean_q: 3.925457
 19339/100000: episode: 196, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.347, mean reward: 1.883 [1.458, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.485, 10.099], loss: 0.314828, mae: 0.366121, mean_q: 3.924392
 19439/100000: episode: 197, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.399, mean reward: 1.894 [1.465, 4.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.556, 10.363], loss: 1.178457, mae: 0.411741, mean_q: 3.922336
 19539/100000: episode: 198, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 202.068, mean reward: 2.021 [1.479, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.687, 10.440], loss: 1.074193, mae: 0.366585, mean_q: 3.894598
 19639/100000: episode: 199, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 197.335, mean reward: 1.973 [1.435, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.045, 10.268], loss: 0.361957, mae: 0.399292, mean_q: 3.930724
 19739/100000: episode: 200, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 192.384, mean reward: 1.924 [1.477, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.224, 10.098], loss: 0.144308, mae: 0.322005, mean_q: 3.887029
 19839/100000: episode: 201, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.464, mean reward: 1.895 [1.497, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.712, 10.337], loss: 1.171370, mae: 0.446478, mean_q: 3.943132
 19939/100000: episode: 202, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.496, mean reward: 1.975 [1.451, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.343, 10.259], loss: 0.176867, mae: 0.333308, mean_q: 3.883370
 20039/100000: episode: 203, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 218.327, mean reward: 2.183 [1.505, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.293, 10.098], loss: 0.123830, mae: 0.325780, mean_q: 3.876906
 20139/100000: episode: 204, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 201.121, mean reward: 2.011 [1.479, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.755, 10.194], loss: 0.098088, mae: 0.309204, mean_q: 3.836025
 20239/100000: episode: 205, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 196.162, mean reward: 1.962 [1.442, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.721, 10.098], loss: 0.105803, mae: 0.321553, mean_q: 3.854661
 20339/100000: episode: 206, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 192.097, mean reward: 1.921 [1.452, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.553, 10.098], loss: 0.111461, mae: 0.312252, mean_q: 3.858438
 20439/100000: episode: 207, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 206.816, mean reward: 2.068 [1.460, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.098], loss: 0.120775, mae: 0.328616, mean_q: 3.875341
 20539/100000: episode: 208, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.080, mean reward: 1.971 [1.466, 4.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.900, 10.154], loss: 0.101504, mae: 0.314411, mean_q: 3.852672
 20639/100000: episode: 209, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.060, mean reward: 1.971 [1.479, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.094, 10.241], loss: 0.111202, mae: 0.324094, mean_q: 3.867815
 20739/100000: episode: 210, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.713, mean reward: 1.877 [1.472, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.439, 10.256], loss: 0.103043, mae: 0.315193, mean_q: 3.874854
 20839/100000: episode: 211, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.886, mean reward: 1.889 [1.466, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.846, 10.195], loss: 0.111069, mae: 0.322365, mean_q: 3.862836
 20939/100000: episode: 212, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.071, mean reward: 1.931 [1.438, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.326, 10.098], loss: 0.101234, mae: 0.314159, mean_q: 3.877754
 21039/100000: episode: 213, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.707, mean reward: 1.787 [1.457, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.479, 10.181], loss: 0.100630, mae: 0.315151, mean_q: 3.883911
 21139/100000: episode: 214, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 192.578, mean reward: 1.926 [1.490, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.502, 10.185], loss: 0.101946, mae: 0.319164, mean_q: 3.870337
 21239/100000: episode: 215, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 206.661, mean reward: 2.067 [1.503, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.774, 10.287], loss: 0.109359, mae: 0.325999, mean_q: 3.898521
 21339/100000: episode: 216, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 211.630, mean reward: 2.116 [1.478, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.052, 10.265], loss: 0.101243, mae: 0.317584, mean_q: 3.887350
 21439/100000: episode: 217, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 177.899, mean reward: 1.779 [1.462, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.055, 10.098], loss: 0.107032, mae: 0.321239, mean_q: 3.897266
 21539/100000: episode: 218, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 182.236, mean reward: 1.822 [1.457, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.203, 10.098], loss: 0.099096, mae: 0.313745, mean_q: 3.872326
 21639/100000: episode: 219, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.993, mean reward: 2.060 [1.445, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.082, 10.098], loss: 0.102310, mae: 0.313951, mean_q: 3.869452
 21739/100000: episode: 220, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.940, mean reward: 1.919 [1.437, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.219, 10.098], loss: 0.094890, mae: 0.307767, mean_q: 3.861606
 21839/100000: episode: 221, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 185.297, mean reward: 1.853 [1.452, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.592, 10.296], loss: 0.101262, mae: 0.319060, mean_q: 3.881498
 21939/100000: episode: 222, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.445, mean reward: 1.964 [1.435, 5.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.234], loss: 0.107135, mae: 0.325483, mean_q: 3.879269
 22039/100000: episode: 223, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 187.763, mean reward: 1.878 [1.442, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.525, 10.098], loss: 0.099202, mae: 0.316278, mean_q: 3.882595
 22139/100000: episode: 224, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.618, mean reward: 1.836 [1.447, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.245, 10.191], loss: 0.096417, mae: 0.312990, mean_q: 3.873011
 22239/100000: episode: 225, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.927, mean reward: 1.849 [1.454, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.571, 10.100], loss: 0.087924, mae: 0.294501, mean_q: 3.869249
 22339/100000: episode: 226, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 200.176, mean reward: 2.002 [1.502, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.098], loss: 0.094209, mae: 0.307607, mean_q: 3.876073
 22439/100000: episode: 227, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 199.246, mean reward: 1.992 [1.479, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.573, 10.263], loss: 0.089709, mae: 0.308784, mean_q: 3.891279
 22539/100000: episode: 228, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 180.047, mean reward: 1.800 [1.439, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.098], loss: 0.112383, mae: 0.332774, mean_q: 3.903430
 22639/100000: episode: 229, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 183.135, mean reward: 1.831 [1.494, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.098], loss: 0.088597, mae: 0.300042, mean_q: 3.869261
 22739/100000: episode: 230, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 216.861, mean reward: 2.169 [1.448, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.553, 10.098], loss: 0.094693, mae: 0.308392, mean_q: 3.873821
 22839/100000: episode: 231, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.252, mean reward: 1.913 [1.448, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.360, 10.098], loss: 0.090850, mae: 0.302746, mean_q: 3.862633
 22939/100000: episode: 232, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 204.171, mean reward: 2.042 [1.475, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.268, 10.289], loss: 0.090485, mae: 0.299014, mean_q: 3.852320
 23039/100000: episode: 233, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 191.737, mean reward: 1.917 [1.459, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.455, 10.098], loss: 0.094085, mae: 0.307993, mean_q: 3.857422
 23139/100000: episode: 234, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.955, mean reward: 1.880 [1.490, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.215], loss: 0.100091, mae: 0.313175, mean_q: 3.894888
 23239/100000: episode: 235, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 184.713, mean reward: 1.847 [1.479, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.585, 10.101], loss: 0.091778, mae: 0.298826, mean_q: 3.864813
 23339/100000: episode: 236, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.227, mean reward: 1.862 [1.455, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.488, 10.145], loss: 0.100749, mae: 0.311358, mean_q: 3.858526
 23439/100000: episode: 237, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 187.034, mean reward: 1.870 [1.443, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.176], loss: 0.098516, mae: 0.312791, mean_q: 3.852292
 23539/100000: episode: 238, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.329, mean reward: 1.893 [1.439, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.639, 10.105], loss: 0.086163, mae: 0.299249, mean_q: 3.849881
 23639/100000: episode: 239, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.428, mean reward: 1.994 [1.525, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.135, 10.098], loss: 0.092759, mae: 0.301680, mean_q: 3.846879
 23739/100000: episode: 240, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 223.580, mean reward: 2.236 [1.469, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.965, 10.098], loss: 0.085482, mae: 0.297939, mean_q: 3.851648
 23839/100000: episode: 241, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 216.475, mean reward: 2.165 [1.512, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.121, 10.105], loss: 0.079474, mae: 0.284770, mean_q: 3.830223
 23939/100000: episode: 242, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 251.132, mean reward: 2.511 [1.483, 8.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.147, 10.098], loss: 0.093300, mae: 0.303165, mean_q: 3.828981
 24039/100000: episode: 243, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 198.916, mean reward: 1.989 [1.441, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.107], loss: 0.104935, mae: 0.310089, mean_q: 3.849677
 24139/100000: episode: 244, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.602, mean reward: 2.016 [1.494, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.322, 10.098], loss: 0.118090, mae: 0.318533, mean_q: 3.863376
 24239/100000: episode: 245, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 213.627, mean reward: 2.136 [1.489, 5.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.919, 10.098], loss: 0.101958, mae: 0.309541, mean_q: 3.859964
 24339/100000: episode: 246, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 201.128, mean reward: 2.011 [1.464, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.098], loss: 0.132467, mae: 0.335644, mean_q: 3.887490
 24439/100000: episode: 247, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.217, mean reward: 1.862 [1.520, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.098], loss: 0.126760, mae: 0.328751, mean_q: 3.892766
 24539/100000: episode: 248, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 198.512, mean reward: 1.985 [1.497, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.218, 10.098], loss: 0.133371, mae: 0.336496, mean_q: 3.901198
 24639/100000: episode: 249, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 191.102, mean reward: 1.911 [1.472, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.184, 10.261], loss: 0.116558, mae: 0.325684, mean_q: 3.880590
 24739/100000: episode: 250, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.320, mean reward: 1.873 [1.438, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.005, 10.157], loss: 0.125102, mae: 0.321629, mean_q: 3.884749
 24839/100000: episode: 251, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 198.482, mean reward: 1.985 [1.504, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.062, 10.171], loss: 0.109436, mae: 0.321844, mean_q: 3.891211
 24939/100000: episode: 252, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 242.864, mean reward: 2.429 [1.459, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.493, 10.515], loss: 0.107950, mae: 0.317553, mean_q: 3.883963
[Info] 1-TH LEVEL FOUND: 4.593966007232666, Considering 10/90 traces
 25039/100000: episode: 253, duration: 4.704s, episode steps: 100, steps per second: 21, episode reward: 194.916, mean reward: 1.949 [1.485, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.694, 10.098], loss: 0.106110, mae: 0.308135, mean_q: 3.868846
 25128/100000: episode: 254, duration: 0.490s, episode steps: 89, steps per second: 182, episode reward: 170.114, mean reward: 1.911 [1.451, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.563 [-0.488, 10.100], loss: 0.129401, mae: 0.329583, mean_q: 3.878388
 25223/100000: episode: 255, duration: 0.512s, episode steps: 95, steps per second: 186, episode reward: 197.099, mean reward: 2.075 [1.462, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.276, 10.274], loss: 0.114648, mae: 0.326637, mean_q: 3.884623
 25314/100000: episode: 256, duration: 0.476s, episode steps: 91, steps per second: 191, episode reward: 171.347, mean reward: 1.883 [1.465, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.707, 10.186], loss: 0.123094, mae: 0.331025, mean_q: 3.890020
 25409/100000: episode: 257, duration: 0.515s, episode steps: 95, steps per second: 185, episode reward: 189.060, mean reward: 1.990 [1.439, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.754, 10.258], loss: 0.106075, mae: 0.315724, mean_q: 3.896009
 25503/100000: episode: 258, duration: 0.499s, episode steps: 94, steps per second: 189, episode reward: 179.789, mean reward: 1.913 [1.473, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.881, 10.153], loss: 0.127650, mae: 0.335622, mean_q: 3.914091
 25596/100000: episode: 259, duration: 0.508s, episode steps: 93, steps per second: 183, episode reward: 182.139, mean reward: 1.958 [1.445, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-1.150, 10.100], loss: 0.121748, mae: 0.327586, mean_q: 3.883514
 25691/100000: episode: 260, duration: 0.523s, episode steps: 95, steps per second: 182, episode reward: 181.509, mean reward: 1.911 [1.465, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.707, 10.372], loss: 0.114352, mae: 0.316581, mean_q: 3.866944
 25785/100000: episode: 261, duration: 0.505s, episode steps: 94, steps per second: 186, episode reward: 191.640, mean reward: 2.039 [1.477, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-1.037, 10.137], loss: 0.109233, mae: 0.316464, mean_q: 3.894120
 25875/100000: episode: 262, duration: 0.468s, episode steps: 90, steps per second: 192, episode reward: 191.101, mean reward: 2.123 [1.463, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.454, 10.100], loss: 0.125908, mae: 0.328385, mean_q: 3.905372
 25965/100000: episode: 263, duration: 0.486s, episode steps: 90, steps per second: 185, episode reward: 181.187, mean reward: 2.013 [1.533, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.762, 10.100], loss: 0.119093, mae: 0.331134, mean_q: 3.908618
 25981/100000: episode: 264, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 52.432, mean reward: 3.277 [2.200, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.554, 10.100], loss: 0.086933, mae: 0.289577, mean_q: 3.880385
 26075/100000: episode: 265, duration: 0.497s, episode steps: 94, steps per second: 189, episode reward: 177.322, mean reward: 1.886 [1.446, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.750, 10.187], loss: 0.126187, mae: 0.324569, mean_q: 3.917771
 26170/100000: episode: 266, duration: 0.491s, episode steps: 95, steps per second: 193, episode reward: 170.934, mean reward: 1.799 [1.453, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.572, 10.100], loss: 0.122227, mae: 0.325299, mean_q: 3.913441
 26186/100000: episode: 267, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 37.011, mean reward: 2.313 [1.788, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.063, 10.100], loss: 0.127010, mae: 0.340075, mean_q: 3.934122
 26202/100000: episode: 268, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 40.942, mean reward: 2.559 [2.161, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.436, 10.100], loss: 0.110081, mae: 0.312497, mean_q: 3.861158
 26296/100000: episode: 269, duration: 0.486s, episode steps: 94, steps per second: 193, episode reward: 175.294, mean reward: 1.865 [1.484, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.547, 10.100], loss: 0.131722, mae: 0.342619, mean_q: 3.919317
 26312/100000: episode: 270, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 41.468, mean reward: 2.592 [2.148, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.208, 10.100], loss: 0.106054, mae: 0.327223, mean_q: 3.922446
 26405/100000: episode: 271, duration: 0.507s, episode steps: 93, steps per second: 183, episode reward: 174.936, mean reward: 1.881 [1.458, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-0.962, 10.100], loss: 0.132180, mae: 0.329475, mean_q: 3.908046
 26424/100000: episode: 272, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 48.529, mean reward: 2.554 [1.591, 5.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.100], loss: 0.116136, mae: 0.321125, mean_q: 3.899282
 26514/100000: episode: 273, duration: 0.481s, episode steps: 90, steps per second: 187, episode reward: 204.157, mean reward: 2.268 [1.470, 12.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-1.123, 10.174], loss: 0.130425, mae: 0.327662, mean_q: 3.916404
 26604/100000: episode: 274, duration: 0.462s, episode steps: 90, steps per second: 195, episode reward: 172.696, mean reward: 1.919 [1.473, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.583, 10.100], loss: 0.109077, mae: 0.326710, mean_q: 3.914912
 26695/100000: episode: 275, duration: 0.478s, episode steps: 91, steps per second: 190, episode reward: 184.081, mean reward: 2.023 [1.460, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.536, 10.245], loss: 0.106142, mae: 0.325490, mean_q: 3.929881
 26786/100000: episode: 276, duration: 0.485s, episode steps: 91, steps per second: 188, episode reward: 173.946, mean reward: 1.911 [1.435, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.703, 10.134], loss: 0.144844, mae: 0.338554, mean_q: 3.942521
 26875/100000: episode: 277, duration: 0.480s, episode steps: 89, steps per second: 185, episode reward: 168.885, mean reward: 1.898 [1.477, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.561 [-1.862, 10.100], loss: 0.162715, mae: 0.343297, mean_q: 3.927336
 26965/100000: episode: 278, duration: 0.499s, episode steps: 90, steps per second: 180, episode reward: 173.020, mean reward: 1.922 [1.440, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.737, 10.100], loss: 0.108688, mae: 0.312937, mean_q: 3.909585
 26981/100000: episode: 279, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 37.390, mean reward: 2.337 [2.058, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.143, 10.100], loss: 0.238084, mae: 0.363535, mean_q: 3.924916
 26997/100000: episode: 280, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 39.128, mean reward: 2.446 [2.024, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.143, 10.100], loss: 0.119561, mae: 0.328390, mean_q: 3.970313
 27087/100000: episode: 281, duration: 0.485s, episode steps: 90, steps per second: 186, episode reward: 185.027, mean reward: 2.056 [1.480, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.809, 10.100], loss: 0.121992, mae: 0.338010, mean_q: 3.935611
 27177/100000: episode: 282, duration: 0.493s, episode steps: 90, steps per second: 183, episode reward: 173.086, mean reward: 1.923 [1.493, 6.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-1.425, 10.115], loss: 0.108926, mae: 0.314548, mean_q: 3.905764
 27267/100000: episode: 283, duration: 0.485s, episode steps: 90, steps per second: 186, episode reward: 171.624, mean reward: 1.907 [1.450, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.585, 10.100], loss: 0.126239, mae: 0.326210, mean_q: 3.932446
 27358/100000: episode: 284, duration: 0.483s, episode steps: 91, steps per second: 188, episode reward: 180.585, mean reward: 1.984 [1.449, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.776, 10.190], loss: 0.127129, mae: 0.331932, mean_q: 3.923691
 27449/100000: episode: 285, duration: 0.476s, episode steps: 91, steps per second: 191, episode reward: 178.775, mean reward: 1.965 [1.460, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.683, 10.254], loss: 0.139379, mae: 0.342814, mean_q: 3.928000
 27468/100000: episode: 286, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 61.650, mean reward: 3.245 [1.882, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.100], loss: 0.109986, mae: 0.334727, mean_q: 3.929247
 27561/100000: episode: 287, duration: 0.504s, episode steps: 93, steps per second: 184, episode reward: 214.016, mean reward: 2.301 [1.476, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-1.250, 10.100], loss: 0.130376, mae: 0.348856, mean_q: 3.932001
 27656/100000: episode: 288, duration: 0.502s, episode steps: 95, steps per second: 189, episode reward: 190.281, mean reward: 2.003 [1.484, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.167, 10.147], loss: 0.146259, mae: 0.345049, mean_q: 3.937005
 27675/100000: episode: 289, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 63.727, mean reward: 3.354 [1.900, 7.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.199, 10.100], loss: 0.104360, mae: 0.323174, mean_q: 3.884615
 27768/100000: episode: 290, duration: 0.501s, episode steps: 93, steps per second: 186, episode reward: 179.040, mean reward: 1.925 [1.501, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.342, 10.252], loss: 0.132483, mae: 0.346730, mean_q: 3.947610
 27784/100000: episode: 291, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 38.788, mean reward: 2.424 [1.896, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.188, 10.100], loss: 0.133221, mae: 0.357138, mean_q: 3.985115
 27874/100000: episode: 292, duration: 0.468s, episode steps: 90, steps per second: 192, episode reward: 180.097, mean reward: 2.001 [1.434, 6.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-1.882, 10.100], loss: 0.121049, mae: 0.329410, mean_q: 3.959591
 27964/100000: episode: 293, duration: 0.456s, episode steps: 90, steps per second: 197, episode reward: 177.231, mean reward: 1.969 [1.470, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-0.624, 10.268], loss: 0.169673, mae: 0.355165, mean_q: 3.983164
 28053/100000: episode: 294, duration: 0.454s, episode steps: 89, steps per second: 196, episode reward: 185.461, mean reward: 2.084 [1.467, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.364, 10.100], loss: 0.140350, mae: 0.343350, mean_q: 3.965703
 28146/100000: episode: 295, duration: 0.485s, episode steps: 93, steps per second: 192, episode reward: 169.352, mean reward: 1.821 [1.444, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.600, 10.100], loss: 0.156529, mae: 0.347147, mean_q: 3.980366
 28236/100000: episode: 296, duration: 0.477s, episode steps: 90, steps per second: 189, episode reward: 176.907, mean reward: 1.966 [1.478, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-1.280, 10.100], loss: 0.138105, mae: 0.350398, mean_q: 3.995958
 28325/100000: episode: 297, duration: 0.480s, episode steps: 89, steps per second: 186, episode reward: 177.851, mean reward: 1.998 [1.483, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.568 [-1.416, 10.178], loss: 0.140961, mae: 0.355955, mean_q: 3.986752
 28418/100000: episode: 298, duration: 0.501s, episode steps: 93, steps per second: 186, episode reward: 172.901, mean reward: 1.859 [1.454, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-1.719, 10.152], loss: 0.134756, mae: 0.352892, mean_q: 3.973992
 28508/100000: episode: 299, duration: 0.487s, episode steps: 90, steps per second: 185, episode reward: 169.093, mean reward: 1.879 [1.475, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-0.125, 10.100], loss: 0.129122, mae: 0.334424, mean_q: 3.980862
 28527/100000: episode: 300, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 52.211, mean reward: 2.748 [1.968, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.131, 10.100], loss: 0.156666, mae: 0.355425, mean_q: 4.036114
 28617/100000: episode: 301, duration: 0.475s, episode steps: 90, steps per second: 190, episode reward: 171.887, mean reward: 1.910 [1.493, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-0.788, 10.227], loss: 0.159390, mae: 0.353948, mean_q: 3.992562
 28636/100000: episode: 302, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 48.173, mean reward: 2.535 [1.718, 4.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.334, 10.105], loss: 0.129450, mae: 0.329310, mean_q: 3.998256
 28731/100000: episode: 303, duration: 0.528s, episode steps: 95, steps per second: 180, episode reward: 200.868, mean reward: 2.114 [1.484, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.452, 10.233], loss: 0.139957, mae: 0.353888, mean_q: 4.015316
 28826/100000: episode: 304, duration: 0.497s, episode steps: 95, steps per second: 191, episode reward: 176.206, mean reward: 1.855 [1.497, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.493, 10.100], loss: 0.128130, mae: 0.337713, mean_q: 3.972553
 28915/100000: episode: 305, duration: 0.470s, episode steps: 89, steps per second: 189, episode reward: 184.601, mean reward: 2.074 [1.451, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.540, 10.414], loss: 0.126632, mae: 0.345307, mean_q: 3.981993
 29005/100000: episode: 306, duration: 0.493s, episode steps: 90, steps per second: 183, episode reward: 179.570, mean reward: 1.995 [1.458, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.626, 10.300], loss: 0.104160, mae: 0.316781, mean_q: 3.975431
 29100/100000: episode: 307, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 181.426, mean reward: 1.910 [1.455, 7.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.754, 10.134], loss: 0.121913, mae: 0.334673, mean_q: 3.959991
 29189/100000: episode: 308, duration: 0.491s, episode steps: 89, steps per second: 181, episode reward: 218.725, mean reward: 2.458 [1.529, 10.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-1.153, 10.154], loss: 0.106732, mae: 0.318265, mean_q: 3.945021
 29279/100000: episode: 309, duration: 0.475s, episode steps: 90, steps per second: 190, episode reward: 188.972, mean reward: 2.100 [1.510, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-1.496, 10.100], loss: 0.119330, mae: 0.335589, mean_q: 3.972301
 29295/100000: episode: 310, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 37.537, mean reward: 2.346 [1.976, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.209, 10.100], loss: 0.214190, mae: 0.347656, mean_q: 3.931831
 29314/100000: episode: 311, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 63.374, mean reward: 3.335 [2.040, 5.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.137, 10.100], loss: 0.119420, mae: 0.345127, mean_q: 4.004153
 29407/100000: episode: 312, duration: 0.468s, episode steps: 93, steps per second: 199, episode reward: 175.966, mean reward: 1.892 [1.462, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-0.600, 10.186], loss: 0.142159, mae: 0.344125, mean_q: 3.965342
 29498/100000: episode: 313, duration: 0.470s, episode steps: 91, steps per second: 194, episode reward: 174.613, mean reward: 1.919 [1.451, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.872, 10.109], loss: 0.146004, mae: 0.345241, mean_q: 3.987896
 29589/100000: episode: 314, duration: 0.497s, episode steps: 91, steps per second: 183, episode reward: 191.851, mean reward: 2.108 [1.494, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-1.062, 10.310], loss: 0.154316, mae: 0.341175, mean_q: 4.000365
 29684/100000: episode: 315, duration: 0.494s, episode steps: 95, steps per second: 192, episode reward: 172.849, mean reward: 1.819 [1.474, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.836, 10.100], loss: 0.147057, mae: 0.337035, mean_q: 3.988324
 29773/100000: episode: 316, duration: 0.463s, episode steps: 89, steps per second: 192, episode reward: 162.199, mean reward: 1.822 [1.468, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.672, 10.166], loss: 0.124198, mae: 0.326151, mean_q: 3.993191
 29866/100000: episode: 317, duration: 0.532s, episode steps: 93, steps per second: 175, episode reward: 177.325, mean reward: 1.907 [1.481, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.197, 10.100], loss: 0.155246, mae: 0.355681, mean_q: 3.990448
 29885/100000: episode: 318, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 66.333, mean reward: 3.491 [2.088, 10.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.365, 10.100], loss: 0.109719, mae: 0.315168, mean_q: 3.952798
 29975/100000: episode: 319, duration: 0.479s, episode steps: 90, steps per second: 188, episode reward: 169.759, mean reward: 1.886 [1.480, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.761, 10.176], loss: 0.118408, mae: 0.321451, mean_q: 3.985360
 30068/100000: episode: 320, duration: 0.498s, episode steps: 93, steps per second: 187, episode reward: 189.457, mean reward: 2.037 [1.493, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.683, 10.167], loss: 0.163957, mae: 0.349257, mean_q: 3.998943
 30158/100000: episode: 321, duration: 0.474s, episode steps: 90, steps per second: 190, episode reward: 180.721, mean reward: 2.008 [1.447, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-1.159, 10.155], loss: 0.155733, mae: 0.343729, mean_q: 3.989095
 30253/100000: episode: 322, duration: 0.525s, episode steps: 95, steps per second: 181, episode reward: 175.459, mean reward: 1.847 [1.446, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.714, 10.230], loss: 0.156234, mae: 0.341650, mean_q: 3.965014
 30343/100000: episode: 323, duration: 0.492s, episode steps: 90, steps per second: 183, episode reward: 177.493, mean reward: 1.972 [1.498, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.591, 10.100], loss: 0.120792, mae: 0.324981, mean_q: 3.948694
 30434/100000: episode: 324, duration: 0.492s, episode steps: 91, steps per second: 185, episode reward: 185.803, mean reward: 2.042 [1.453, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-1.608, 10.100], loss: 0.151270, mae: 0.352103, mean_q: 3.941931
 30527/100000: episode: 325, duration: 0.491s, episode steps: 93, steps per second: 190, episode reward: 184.146, mean reward: 1.980 [1.507, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.541, 10.100], loss: 0.107595, mae: 0.325524, mean_q: 3.960737
 30620/100000: episode: 326, duration: 0.489s, episode steps: 93, steps per second: 190, episode reward: 180.241, mean reward: 1.938 [1.477, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-0.471, 10.100], loss: 0.124550, mae: 0.334839, mean_q: 3.942912
 30710/100000: episode: 327, duration: 0.493s, episode steps: 90, steps per second: 183, episode reward: 176.247, mean reward: 1.958 [1.480, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.531, 10.100], loss: 0.159272, mae: 0.344883, mean_q: 3.969471
 30801/100000: episode: 328, duration: 0.486s, episode steps: 91, steps per second: 187, episode reward: 166.159, mean reward: 1.826 [1.481, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.928, 10.158], loss: 0.122176, mae: 0.321863, mean_q: 3.942562
 30896/100000: episode: 329, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 186.856, mean reward: 1.967 [1.462, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.590, 10.100], loss: 0.148168, mae: 0.337028, mean_q: 3.964069
 30915/100000: episode: 330, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 59.870, mean reward: 3.151 [1.796, 5.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.086, 10.100], loss: 0.097539, mae: 0.300373, mean_q: 3.909510
 31006/100000: episode: 331, duration: 0.462s, episode steps: 91, steps per second: 197, episode reward: 173.556, mean reward: 1.907 [1.444, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-1.371, 10.100], loss: 0.120055, mae: 0.323493, mean_q: 3.938000
 31101/100000: episode: 332, duration: 0.498s, episode steps: 95, steps per second: 191, episode reward: 201.356, mean reward: 2.120 [1.443, 6.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-2.038, 10.205], loss: 0.143308, mae: 0.333518, mean_q: 3.971756
 31117/100000: episode: 333, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 44.177, mean reward: 2.761 [2.012, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.597, 10.100], loss: 0.114135, mae: 0.332006, mean_q: 3.947252
 31210/100000: episode: 334, duration: 0.505s, episode steps: 93, steps per second: 184, episode reward: 170.663, mean reward: 1.835 [1.455, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.372, 10.100], loss: 0.162603, mae: 0.341801, mean_q: 3.957110
 31304/100000: episode: 335, duration: 0.500s, episode steps: 94, steps per second: 188, episode reward: 186.986, mean reward: 1.989 [1.477, 4.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.833, 10.140], loss: 0.128683, mae: 0.330747, mean_q: 3.948829
 31399/100000: episode: 336, duration: 0.517s, episode steps: 95, steps per second: 184, episode reward: 194.583, mean reward: 2.048 [1.469, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.316, 10.196], loss: 0.143409, mae: 0.335544, mean_q: 3.962859
 31494/100000: episode: 337, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 186.369, mean reward: 1.962 [1.474, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.698, 10.100], loss: 0.113211, mae: 0.318153, mean_q: 3.931897
 31585/100000: episode: 338, duration: 0.470s, episode steps: 91, steps per second: 194, episode reward: 168.608, mean reward: 1.853 [1.457, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.415, 10.218], loss: 0.106935, mae: 0.318935, mean_q: 3.929148
 31678/100000: episode: 339, duration: 0.485s, episode steps: 93, steps per second: 192, episode reward: 176.394, mean reward: 1.897 [1.462, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.760, 10.363], loss: 0.136088, mae: 0.333908, mean_q: 3.969673
 31773/100000: episode: 340, duration: 0.479s, episode steps: 95, steps per second: 198, episode reward: 182.356, mean reward: 1.920 [1.471, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.746, 10.180], loss: 0.130261, mae: 0.332784, mean_q: 3.934832
 31792/100000: episode: 341, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 49.648, mean reward: 2.613 [1.556, 6.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.106], loss: 0.099896, mae: 0.326327, mean_q: 3.931904
 31811/100000: episode: 342, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 65.912, mean reward: 3.469 [2.255, 7.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.312, 10.100], loss: 0.160261, mae: 0.357462, mean_q: 3.970543
[Info] 2-TH LEVEL FOUND: 6.310044288635254, Considering 11/89 traces
 31906/100000: episode: 343, duration: 4.682s, episode steps: 95, steps per second: 20, episode reward: 183.790, mean reward: 1.935 [1.488, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.117, 10.240], loss: 0.143722, mae: 0.340956, mean_q: 3.967186
 31925/100000: episode: 344, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 77.055, mean reward: 4.056 [2.492, 12.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.109, 10.100], loss: 0.153271, mae: 0.378207, mean_q: 4.040616
 31944/100000: episode: 345, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 57.374, mean reward: 3.020 [2.288, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.876, 10.100], loss: 0.138884, mae: 0.332951, mean_q: 3.981759
 31963/100000: episode: 346, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 141.487, mean reward: 7.447 [4.394, 16.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.387, 10.100], loss: 0.123666, mae: 0.337361, mean_q: 4.010133
 31982/100000: episode: 347, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 54.499, mean reward: 2.868 [1.757, 6.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.131, 10.100], loss: 0.129880, mae: 0.341798, mean_q: 3.993562
 32001/100000: episode: 348, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 88.423, mean reward: 4.654 [2.335, 8.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.229, 10.100], loss: 0.291789, mae: 0.398990, mean_q: 4.041521
 32020/100000: episode: 349, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 77.437, mean reward: 4.076 [2.183, 8.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.135, 10.100], loss: 0.300294, mae: 0.415980, mean_q: 3.961359
 32039/100000: episode: 350, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 49.410, mean reward: 2.601 [1.622, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.045, 10.100], loss: 0.229183, mae: 0.411103, mean_q: 4.098683
 32058/100000: episode: 351, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 57.221, mean reward: 3.012 [2.238, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.353, 10.100], loss: 0.331677, mae: 0.404058, mean_q: 4.038271
 32077/100000: episode: 352, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 53.530, mean reward: 2.817 [1.677, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.447, 10.100], loss: 0.147192, mae: 0.363430, mean_q: 3.990463
 32096/100000: episode: 353, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 60.433, mean reward: 3.181 [2.084, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.100, 10.100], loss: 0.184221, mae: 0.335907, mean_q: 3.981922
 32115/100000: episode: 354, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 83.027, mean reward: 4.370 [3.241, 5.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.354, 10.100], loss: 0.177547, mae: 0.370366, mean_q: 4.075539
 32134/100000: episode: 355, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 95.095, mean reward: 5.005 [2.592, 7.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.247, 10.100], loss: 0.177346, mae: 0.356117, mean_q: 4.020926
 32153/100000: episode: 356, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 62.428, mean reward: 3.286 [2.039, 7.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.269, 10.100], loss: 0.150098, mae: 0.355350, mean_q: 4.103524
 32172/100000: episode: 357, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 170.380, mean reward: 8.967 [2.142, 91.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.074, 10.100], loss: 0.522556, mae: 0.521148, mean_q: 4.197890
 32191/100000: episode: 358, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 57.136, mean reward: 3.007 [2.190, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.127, 10.100], loss: 0.313906, mae: 0.481606, mean_q: 4.111469
 32210/100000: episode: 359, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 58.599, mean reward: 3.084 [1.796, 6.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.802, 10.100], loss: 0.350463, mae: 0.487691, mean_q: 4.176533
 32229/100000: episode: 360, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 58.682, mean reward: 3.089 [1.619, 5.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.100], loss: 0.320410, mae: 0.386316, mean_q: 4.138920
 32248/100000: episode: 361, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 70.274, mean reward: 3.699 [1.907, 11.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.073, 10.100], loss: 0.279956, mae: 0.466123, mean_q: 4.251177
[Info] FALSIFICATION!
[Info] Levels: [4.593966, 6.3100443, 8.033305]
[Info] Cond. Prob: [0.1, 0.11, 0.02]
[Info] Error Prob: 0.00022000000000000003

 32250/100000: episode: 362, duration: 4.438s, episode steps: 2, steps per second: 0, episode reward: 106.096, mean reward: 53.048 [6.096, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.659, 9.404], loss: 0.185176, mae: 0.412726, mean_q: 4.322675
 32350/100000: episode: 363, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 197.485, mean reward: 1.975 [1.454, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.270], loss: 4.309143, mae: 0.648404, mean_q: 4.287459
 32450/100000: episode: 364, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.281, mean reward: 1.913 [1.447, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.092, 10.106], loss: 0.303395, mae: 0.439223, mean_q: 4.132463
 32550/100000: episode: 365, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 181.127, mean reward: 1.811 [1.480, 2.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.140], loss: 2.724777, mae: 0.507834, mean_q: 4.186648
 32650/100000: episode: 366, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 192.505, mean reward: 1.925 [1.448, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.239, 10.229], loss: 1.473892, mae: 0.429331, mean_q: 4.127419
 32750/100000: episode: 367, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 183.134, mean reward: 1.831 [1.451, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.624, 10.187], loss: 1.461250, mae: 0.444427, mean_q: 4.221254
 32850/100000: episode: 368, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.601, mean reward: 1.906 [1.447, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.886, 10.166], loss: 1.610568, mae: 0.458442, mean_q: 4.183342
 32950/100000: episode: 369, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 237.982, mean reward: 2.380 [1.475, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.098], loss: 0.180805, mae: 0.364769, mean_q: 4.119268
 33050/100000: episode: 370, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 192.528, mean reward: 1.925 [1.458, 5.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.840, 10.111], loss: 0.156617, mae: 0.349132, mean_q: 4.139975
 33150/100000: episode: 371, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 201.504, mean reward: 2.015 [1.463, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.739, 10.098], loss: 4.132214, mae: 0.599416, mean_q: 4.212573
 33250/100000: episode: 372, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 222.708, mean reward: 2.227 [1.482, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.899, 10.098], loss: 4.211544, mae: 0.592085, mean_q: 4.266340
 33350/100000: episode: 373, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.857, mean reward: 1.919 [1.467, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.279], loss: 4.206704, mae: 0.590500, mean_q: 4.264812
 33450/100000: episode: 374, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 180.943, mean reward: 1.809 [1.457, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.856, 10.265], loss: 1.444996, mae: 0.439161, mean_q: 4.212222
 33550/100000: episode: 375, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 194.309, mean reward: 1.943 [1.456, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.922, 10.098], loss: 1.455404, mae: 0.440575, mean_q: 4.197883
 33650/100000: episode: 376, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.077, mean reward: 1.841 [1.467, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.165, 10.108], loss: 2.815043, mae: 0.541110, mean_q: 4.247567
 33750/100000: episode: 377, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.845, mean reward: 1.818 [1.480, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.780, 10.098], loss: 0.250500, mae: 0.395181, mean_q: 4.177472
 33850/100000: episode: 378, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 208.875, mean reward: 2.089 [1.466, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.886, 10.404], loss: 1.565966, mae: 0.448987, mean_q: 4.178945
 33950/100000: episode: 379, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 187.981, mean reward: 1.880 [1.471, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.646, 10.098], loss: 1.443172, mae: 0.447890, mean_q: 4.169578
 34050/100000: episode: 380, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.948, mean reward: 1.919 [1.469, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.964, 10.098], loss: 0.174937, mae: 0.353697, mean_q: 4.087607
 34150/100000: episode: 381, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.319, mean reward: 1.883 [1.468, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.036, 10.370], loss: 0.153102, mae: 0.351454, mean_q: 4.118799
 34250/100000: episode: 382, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 229.755, mean reward: 2.298 [1.461, 8.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.596, 10.347], loss: 4.130713, mae: 0.552892, mean_q: 4.217813
 34350/100000: episode: 383, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.706, mean reward: 1.907 [1.447, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.943, 10.098], loss: 2.721445, mae: 0.512195, mean_q: 4.143185
 34450/100000: episode: 384, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 196.279, mean reward: 1.963 [1.461, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.407], loss: 1.534188, mae: 0.450966, mean_q: 4.180952
 34550/100000: episode: 385, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 186.622, mean reward: 1.866 [1.462, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.098], loss: 0.285676, mae: 0.430310, mean_q: 4.194045
 34650/100000: episode: 386, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.253, mean reward: 1.833 [1.494, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.959, 10.098], loss: 3.913763, mae: 0.512594, mean_q: 4.199677
 34750/100000: episode: 387, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 190.404, mean reward: 1.904 [1.459, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.909, 10.098], loss: 1.544688, mae: 0.472378, mean_q: 4.180397
 34850/100000: episode: 388, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.369, mean reward: 1.954 [1.448, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.851, 10.098], loss: 1.540956, mae: 0.434680, mean_q: 4.202894
 34950/100000: episode: 389, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 192.201, mean reward: 1.922 [1.458, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.479, 10.211], loss: 2.751172, mae: 0.509403, mean_q: 4.170362
 35050/100000: episode: 390, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 210.809, mean reward: 2.108 [1.509, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.221, 10.098], loss: 0.247346, mae: 0.379456, mean_q: 4.158968
 35150/100000: episode: 391, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 194.433, mean reward: 1.944 [1.432, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.269, 10.147], loss: 4.006019, mae: 0.547479, mean_q: 4.232855
 35250/100000: episode: 392, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 199.080, mean reward: 1.991 [1.447, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.127], loss: 1.589874, mae: 0.488620, mean_q: 4.104846
 35350/100000: episode: 393, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 178.221, mean reward: 1.782 [1.463, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.098], loss: 0.216963, mae: 0.374124, mean_q: 4.121716
 35450/100000: episode: 394, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 201.402, mean reward: 2.014 [1.445, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.232, 10.098], loss: 2.775601, mae: 0.524622, mean_q: 4.158472
 35550/100000: episode: 395, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.039, mean reward: 1.890 [1.434, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.284], loss: 1.463607, mae: 0.403293, mean_q: 4.112697
 35650/100000: episode: 396, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 231.840, mean reward: 2.318 [1.452, 5.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.809, 10.121], loss: 1.446619, mae: 0.385544, mean_q: 4.085606
 35750/100000: episode: 397, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 182.801, mean reward: 1.828 [1.465, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.755, 10.132], loss: 0.227562, mae: 0.395942, mean_q: 4.096877
 35850/100000: episode: 398, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 194.801, mean reward: 1.948 [1.454, 3.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.367, 10.134], loss: 1.395742, mae: 0.411765, mean_q: 4.131060
 35950/100000: episode: 399, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 196.365, mean reward: 1.964 [1.445, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.449, 10.299], loss: 0.182643, mae: 0.359910, mean_q: 4.100296
 36050/100000: episode: 400, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 199.709, mean reward: 1.997 [1.485, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.942, 10.098], loss: 0.193043, mae: 0.367927, mean_q: 4.084140
 36150/100000: episode: 401, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.967, mean reward: 1.910 [1.441, 5.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.962, 10.185], loss: 1.535151, mae: 0.444367, mean_q: 4.146593
 36250/100000: episode: 402, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 185.393, mean reward: 1.854 [1.463, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.565, 10.288], loss: 4.069771, mae: 0.550049, mean_q: 4.160618
 36350/100000: episode: 403, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 206.772, mean reward: 2.068 [1.440, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.283, 10.098], loss: 0.238636, mae: 0.377053, mean_q: 4.096461
 36450/100000: episode: 404, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 195.137, mean reward: 1.951 [1.582, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.878, 10.235], loss: 2.749378, mae: 0.493911, mean_q: 4.183319
 36550/100000: episode: 405, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.547, mean reward: 1.885 [1.459, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.118], loss: 1.443368, mae: 0.439282, mean_q: 4.108535
 36650/100000: episode: 406, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 202.511, mean reward: 2.025 [1.469, 6.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.171], loss: 2.763322, mae: 0.446709, mean_q: 4.133664
 36750/100000: episode: 407, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.532, mean reward: 1.925 [1.475, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.098], loss: 2.780451, mae: 0.531854, mean_q: 4.172870
 36850/100000: episode: 408, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 202.079, mean reward: 2.021 [1.476, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.247, 10.211], loss: 1.439113, mae: 0.431777, mean_q: 4.113961
 36950/100000: episode: 409, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 194.352, mean reward: 1.944 [1.471, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.146, 10.098], loss: 3.836865, mae: 0.525710, mean_q: 4.113955
 37050/100000: episode: 410, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.721, mean reward: 1.827 [1.444, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.042, 10.105], loss: 1.400740, mae: 0.418586, mean_q: 4.044217
 37150/100000: episode: 411, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.058, mean reward: 1.891 [1.470, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.920, 10.098], loss: 0.146854, mae: 0.351219, mean_q: 3.943154
 37250/100000: episode: 412, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 196.667, mean reward: 1.967 [1.444, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.855, 10.098], loss: 2.708042, mae: 0.434620, mean_q: 3.947222
 37350/100000: episode: 413, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 179.854, mean reward: 1.799 [1.445, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.194], loss: 0.109377, mae: 0.312967, mean_q: 3.880501
 37450/100000: episode: 414, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.719, mean reward: 1.907 [1.439, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.937, 10.311], loss: 0.098759, mae: 0.298895, mean_q: 3.862468
 37550/100000: episode: 415, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.887, mean reward: 1.939 [1.469, 5.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.762, 10.280], loss: 0.101819, mae: 0.297327, mean_q: 3.865941
 37650/100000: episode: 416, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 206.546, mean reward: 2.065 [1.447, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.138], loss: 0.102499, mae: 0.297710, mean_q: 3.864485
 37750/100000: episode: 417, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 199.866, mean reward: 1.999 [1.510, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.283], loss: 0.104656, mae: 0.305159, mean_q: 3.868074
 37850/100000: episode: 418, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 189.672, mean reward: 1.897 [1.455, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.149, 10.098], loss: 0.093302, mae: 0.296449, mean_q: 3.864227
 37950/100000: episode: 419, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.127, mean reward: 1.881 [1.496, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.441, 10.098], loss: 0.104881, mae: 0.295460, mean_q: 3.866794
 38050/100000: episode: 420, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.781, mean reward: 2.008 [1.443, 3.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.986, 10.148], loss: 0.087551, mae: 0.287476, mean_q: 3.858013
 38150/100000: episode: 421, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.726, mean reward: 1.887 [1.451, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.493, 10.186], loss: 0.077775, mae: 0.279527, mean_q: 3.862216
 38250/100000: episode: 422, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 203.937, mean reward: 2.039 [1.446, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.824, 10.325], loss: 0.091446, mae: 0.280938, mean_q: 3.843002
 38350/100000: episode: 423, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 175.956, mean reward: 1.760 [1.469, 2.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.413, 10.098], loss: 0.084850, mae: 0.287052, mean_q: 3.844164
 38450/100000: episode: 424, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 190.048, mean reward: 1.900 [1.445, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.098], loss: 0.101722, mae: 0.292228, mean_q: 3.840393
 38550/100000: episode: 425, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 215.644, mean reward: 2.156 [1.448, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.454, 10.098], loss: 0.089810, mae: 0.296264, mean_q: 3.844668
 38650/100000: episode: 426, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 186.072, mean reward: 1.861 [1.444, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.135], loss: 0.093352, mae: 0.290251, mean_q: 3.844716
 38750/100000: episode: 427, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.757, mean reward: 1.888 [1.444, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.415, 10.380], loss: 0.091247, mae: 0.289312, mean_q: 3.848332
 38850/100000: episode: 428, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 189.162, mean reward: 1.892 [1.455, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.312, 10.226], loss: 0.106712, mae: 0.301504, mean_q: 3.863056
 38950/100000: episode: 429, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.609, mean reward: 1.986 [1.471, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.453, 10.098], loss: 0.083688, mae: 0.281101, mean_q: 3.842984
 39050/100000: episode: 430, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.403, mean reward: 1.884 [1.489, 2.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.002, 10.098], loss: 0.091104, mae: 0.294415, mean_q: 3.865501
 39150/100000: episode: 431, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 181.662, mean reward: 1.817 [1.448, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.804, 10.098], loss: 0.090183, mae: 0.282629, mean_q: 3.851084
 39250/100000: episode: 432, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.725, mean reward: 1.777 [1.435, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.683, 10.098], loss: 0.079522, mae: 0.274633, mean_q: 3.850913
 39350/100000: episode: 433, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 179.647, mean reward: 1.796 [1.444, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.762, 10.162], loss: 0.078254, mae: 0.273667, mean_q: 3.825918
 39450/100000: episode: 434, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.377, mean reward: 1.924 [1.509, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.011, 10.098], loss: 0.079495, mae: 0.269345, mean_q: 3.826367
 39550/100000: episode: 435, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.747, mean reward: 1.857 [1.476, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.365], loss: 0.081956, mae: 0.277048, mean_q: 3.819263
 39650/100000: episode: 436, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.852, mean reward: 1.929 [1.448, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.098], loss: 0.083273, mae: 0.277088, mean_q: 3.841672
 39750/100000: episode: 437, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.589, mean reward: 2.116 [1.510, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.671, 10.098], loss: 0.076947, mae: 0.274610, mean_q: 3.829603
 39850/100000: episode: 438, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.457, mean reward: 1.915 [1.447, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.049, 10.098], loss: 0.080691, mae: 0.278214, mean_q: 3.836820
 39950/100000: episode: 439, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.581, mean reward: 1.926 [1.492, 5.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.790, 10.172], loss: 0.085557, mae: 0.285920, mean_q: 3.830610
 40050/100000: episode: 440, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.309, mean reward: 1.903 [1.455, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.645, 10.230], loss: 0.073028, mae: 0.269252, mean_q: 3.822567
 40150/100000: episode: 441, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.514, mean reward: 1.925 [1.484, 5.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.522, 10.252], loss: 0.080919, mae: 0.274197, mean_q: 3.835417
 40250/100000: episode: 442, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 183.992, mean reward: 1.840 [1.469, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.719, 10.229], loss: 0.074925, mae: 0.268505, mean_q: 3.808802
 40350/100000: episode: 443, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 178.444, mean reward: 1.784 [1.456, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.737, 10.179], loss: 0.085742, mae: 0.275871, mean_q: 3.823376
 40450/100000: episode: 444, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 255.132, mean reward: 2.551 [1.473, 12.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.098], loss: 0.092375, mae: 0.277190, mean_q: 3.831877
 40550/100000: episode: 445, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 203.257, mean reward: 2.033 [1.454, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.395], loss: 0.088730, mae: 0.279598, mean_q: 3.827186
 40650/100000: episode: 446, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 189.378, mean reward: 1.894 [1.511, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.172, 10.098], loss: 0.094119, mae: 0.285719, mean_q: 3.847751
 40750/100000: episode: 447, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 230.541, mean reward: 2.305 [1.499, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.572, 10.098], loss: 0.083875, mae: 0.281541, mean_q: 3.855109
 40850/100000: episode: 448, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.616, mean reward: 1.906 [1.484, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.191], loss: 0.096450, mae: 0.279638, mean_q: 3.849409
 40950/100000: episode: 449, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.162, mean reward: 1.812 [1.474, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.445, 10.098], loss: 0.109191, mae: 0.295293, mean_q: 3.848715
 41050/100000: episode: 450, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.271, mean reward: 1.893 [1.485, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.261, 10.170], loss: 0.095105, mae: 0.280048, mean_q: 3.842320
 41150/100000: episode: 451, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 187.825, mean reward: 1.878 [1.443, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.654, 10.244], loss: 0.082130, mae: 0.271319, mean_q: 3.803824
 41250/100000: episode: 452, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 198.169, mean reward: 1.982 [1.490, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.552, 10.152], loss: 0.093680, mae: 0.276684, mean_q: 3.822629
 41350/100000: episode: 453, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.064, mean reward: 1.811 [1.501, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.873, 10.294], loss: 0.101865, mae: 0.285987, mean_q: 3.815776
 41450/100000: episode: 454, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.993, mean reward: 1.870 [1.444, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.098], loss: 0.073018, mae: 0.267562, mean_q: 3.817787
 41550/100000: episode: 455, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 190.872, mean reward: 1.909 [1.461, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.177], loss: 0.115627, mae: 0.288777, mean_q: 3.836479
 41650/100000: episode: 456, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 191.185, mean reward: 1.912 [1.463, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.549, 10.230], loss: 0.071934, mae: 0.261208, mean_q: 3.821714
 41750/100000: episode: 457, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 209.123, mean reward: 2.091 [1.471, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.669, 10.458], loss: 0.086809, mae: 0.265799, mean_q: 3.801661
 41850/100000: episode: 458, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 202.940, mean reward: 2.029 [1.454, 5.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.496, 10.128], loss: 0.078308, mae: 0.266003, mean_q: 3.789817
 41950/100000: episode: 459, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 198.139, mean reward: 1.981 [1.469, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.941, 10.353], loss: 0.086284, mae: 0.278191, mean_q: 3.816453
 42050/100000: episode: 460, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.558, mean reward: 1.856 [1.459, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.435, 10.098], loss: 0.119691, mae: 0.300646, mean_q: 3.833617
 42150/100000: episode: 461, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 186.866, mean reward: 1.869 [1.471, 2.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.784, 10.118], loss: 0.095341, mae: 0.284012, mean_q: 3.827336
[Info] 1-TH LEVEL FOUND: 5.258397579193115, Considering 10/90 traces
 42250/100000: episode: 462, duration: 4.696s, episode steps: 100, steps per second: 21, episode reward: 191.369, mean reward: 1.914 [1.474, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.285, 10.140], loss: 0.092426, mae: 0.284495, mean_q: 3.829972
 42269/100000: episode: 463, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 66.639, mean reward: 3.507 [2.104, 7.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.346, 10.100], loss: 0.064322, mae: 0.245465, mean_q: 3.779107
 42295/100000: episode: 464, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 55.071, mean reward: 2.118 [1.527, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.179, 10.100], loss: 0.092435, mae: 0.296009, mean_q: 3.854604
 42324/100000: episode: 465, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 101.405, mean reward: 3.497 [2.874, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.658, 10.100], loss: 0.088434, mae: 0.290320, mean_q: 3.814781
 42350/100000: episode: 466, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 64.728, mean reward: 2.490 [1.861, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.310, 10.100], loss: 0.091944, mae: 0.288651, mean_q: 3.878392
 42379/100000: episode: 467, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 72.533, mean reward: 2.501 [1.696, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.400, 10.100], loss: 0.082187, mae: 0.280809, mean_q: 3.880106
 42398/100000: episode: 468, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 54.168, mean reward: 2.851 [2.244, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.171, 10.100], loss: 0.096719, mae: 0.301865, mean_q: 3.896601
 42415/100000: episode: 469, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 46.197, mean reward: 2.717 [1.956, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.493, 10.100], loss: 0.100330, mae: 0.309590, mean_q: 3.883974
 42437/100000: episode: 470, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 93.525, mean reward: 4.251 [2.667, 7.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.527], loss: 0.149467, mae: 0.304449, mean_q: 3.927754
 42455/100000: episode: 471, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 47.730, mean reward: 2.652 [1.964, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.149, 10.100], loss: 0.088131, mae: 0.289020, mean_q: 3.844882
 42477/100000: episode: 472, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 72.742, mean reward: 3.306 [1.988, 4.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.619, 10.394], loss: 0.121869, mae: 0.331274, mean_q: 3.915315
 42494/100000: episode: 473, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 39.193, mean reward: 2.305 [1.939, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.222, 10.100], loss: 0.102617, mae: 0.308601, mean_q: 3.914390
 42519/100000: episode: 474, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 58.007, mean reward: 2.320 [1.840, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.564, 10.100], loss: 0.106671, mae: 0.313043, mean_q: 3.903200
 42537/100000: episode: 475, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 53.171, mean reward: 2.954 [2.102, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.422, 10.100], loss: 0.159132, mae: 0.347108, mean_q: 3.930236
 42563/100000: episode: 476, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 54.669, mean reward: 2.103 [1.541, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.050, 10.100], loss: 0.123986, mae: 0.314873, mean_q: 3.905190
 42588/100000: episode: 477, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 61.566, mean reward: 2.463 [1.809, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.307, 10.100], loss: 0.105945, mae: 0.309425, mean_q: 3.907216
 42606/100000: episode: 478, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 39.094, mean reward: 2.172 [1.752, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.205, 10.100], loss: 0.101188, mae: 0.295946, mean_q: 3.891135
 42631/100000: episode: 479, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 54.356, mean reward: 2.174 [1.707, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.738, 10.100], loss: 0.089497, mae: 0.286677, mean_q: 3.878197
 42649/100000: episode: 480, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 46.376, mean reward: 2.576 [2.194, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.083, 10.100], loss: 0.094789, mae: 0.298341, mean_q: 3.908960
 42674/100000: episode: 481, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 77.321, mean reward: 3.093 [1.926, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.430, 10.100], loss: 0.078714, mae: 0.283012, mean_q: 3.864650
 42692/100000: episode: 482, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 42.272, mean reward: 2.348 [1.869, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.366, 10.100], loss: 0.083828, mae: 0.293314, mean_q: 3.891016
 42717/100000: episode: 483, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 68.322, mean reward: 2.733 [2.270, 5.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.297, 10.100], loss: 0.103743, mae: 0.306021, mean_q: 3.980788
 42743/100000: episode: 484, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 74.361, mean reward: 2.860 [2.300, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.307, 10.100], loss: 0.130142, mae: 0.316132, mean_q: 3.970776
 42769/100000: episode: 485, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 57.846, mean reward: 2.225 [1.558, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.213, 10.130], loss: 0.119808, mae: 0.314498, mean_q: 3.918621
 42796/100000: episode: 486, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 63.349, mean reward: 2.346 [1.701, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.103, 10.100], loss: 0.119838, mae: 0.331277, mean_q: 3.951906
 42822/100000: episode: 487, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 68.929, mean reward: 2.651 [1.822, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.880, 10.100], loss: 0.116421, mae: 0.317160, mean_q: 4.007905
 42841/100000: episode: 488, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 45.787, mean reward: 2.410 [1.838, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.193, 10.100], loss: 0.171846, mae: 0.377780, mean_q: 4.005918
 42868/100000: episode: 489, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 49.371, mean reward: 1.829 [1.503, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.357, 10.138], loss: 0.123647, mae: 0.333441, mean_q: 4.007391
 42894/100000: episode: 490, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 75.650, mean reward: 2.910 [2.184, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.415, 10.100], loss: 0.106823, mae: 0.305790, mean_q: 3.994038
 42919/100000: episode: 491, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 67.451, mean reward: 2.698 [1.732, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.481, 10.100], loss: 0.105192, mae: 0.311136, mean_q: 3.997015
 42945/100000: episode: 492, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 65.093, mean reward: 2.504 [2.016, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.499, 10.100], loss: 0.078660, mae: 0.280709, mean_q: 3.942281
 42974/100000: episode: 493, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 119.398, mean reward: 4.117 [2.609, 6.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.475, 10.100], loss: 0.195063, mae: 0.344082, mean_q: 4.070105
 42999/100000: episode: 494, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 54.105, mean reward: 2.164 [1.724, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.508, 10.100], loss: 0.109409, mae: 0.307712, mean_q: 4.010114
 43021/100000: episode: 495, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 75.460, mean reward: 3.430 [1.766, 9.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.008, 10.295], loss: 0.086674, mae: 0.287332, mean_q: 4.037403
 43046/100000: episode: 496, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 59.599, mean reward: 2.384 [1.912, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.486, 10.100], loss: 0.172467, mae: 0.322047, mean_q: 4.037324
 43075/100000: episode: 497, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 78.456, mean reward: 2.705 [1.974, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.207, 10.100], loss: 0.132984, mae: 0.347713, mean_q: 4.136686
 43104/100000: episode: 498, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 97.749, mean reward: 3.371 [2.718, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.139, 10.100], loss: 0.133199, mae: 0.326224, mean_q: 4.065056
 43133/100000: episode: 499, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 87.392, mean reward: 3.014 [2.323, 5.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.658, 10.100], loss: 0.147776, mae: 0.352966, mean_q: 4.101659
 43151/100000: episode: 500, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 48.361, mean reward: 2.687 [2.027, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.208, 10.100], loss: 0.123976, mae: 0.320838, mean_q: 4.079168
 43177/100000: episode: 501, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 77.323, mean reward: 2.974 [2.393, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.682, 10.100], loss: 0.168690, mae: 0.317429, mean_q: 4.072409
 43195/100000: episode: 502, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 42.225, mean reward: 2.346 [1.603, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.116, 10.100], loss: 0.135220, mae: 0.337699, mean_q: 4.125483
 43217/100000: episode: 503, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 73.560, mean reward: 3.344 [2.618, 6.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.504, 10.479], loss: 0.122670, mae: 0.352605, mean_q: 4.119957
 43235/100000: episode: 504, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 61.369, mean reward: 3.409 [2.680, 9.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.468, 10.100], loss: 0.129279, mae: 0.338608, mean_q: 4.098995
 43262/100000: episode: 505, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 85.120, mean reward: 3.153 [2.426, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.186, 10.100], loss: 0.157347, mae: 0.355821, mean_q: 4.149216
 43288/100000: episode: 506, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 75.480, mean reward: 2.903 [2.225, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.244, 10.100], loss: 0.097658, mae: 0.303967, mean_q: 4.043959
 43305/100000: episode: 507, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 38.859, mean reward: 2.286 [1.850, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.252, 10.100], loss: 0.080620, mae: 0.303055, mean_q: 4.171209
 43331/100000: episode: 508, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 59.695, mean reward: 2.296 [1.978, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.195, 10.100], loss: 0.097847, mae: 0.306264, mean_q: 4.116899
 43357/100000: episode: 509, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 65.786, mean reward: 2.530 [1.897, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.302, 10.100], loss: 0.194178, mae: 0.382141, mean_q: 4.203772
 43382/100000: episode: 510, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 53.315, mean reward: 2.133 [1.541, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.833, 10.100], loss: 0.132374, mae: 0.349488, mean_q: 4.117167
 43411/100000: episode: 511, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 92.633, mean reward: 3.194 [2.532, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.735, 10.100], loss: 0.133523, mae: 0.333895, mean_q: 4.161859
 43429/100000: episode: 512, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 36.840, mean reward: 2.047 [1.535, 2.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.412, 10.100], loss: 0.107448, mae: 0.315498, mean_q: 4.064065
 43458/100000: episode: 513, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 85.971, mean reward: 2.965 [1.953, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.486, 10.100], loss: 0.121484, mae: 0.344780, mean_q: 4.189054
 43484/100000: episode: 514, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 55.852, mean reward: 2.148 [1.693, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.469, 10.100], loss: 0.115916, mae: 0.332126, mean_q: 4.196583
 43506/100000: episode: 515, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 105.932, mean reward: 4.815 [2.845, 15.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.007, 10.491], loss: 0.119699, mae: 0.312183, mean_q: 4.162765
 43528/100000: episode: 516, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 102.410, mean reward: 4.655 [2.016, 11.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.928, 10.367], loss: 0.236578, mae: 0.387666, mean_q: 4.246692
 43545/100000: episode: 517, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 37.287, mean reward: 2.193 [1.822, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.328, 10.100], loss: 0.307301, mae: 0.403101, mean_q: 4.278635
 43571/100000: episode: 518, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 62.245, mean reward: 2.394 [1.958, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.854, 10.100], loss: 0.132760, mae: 0.316347, mean_q: 4.193341
 43596/100000: episode: 519, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 65.656, mean reward: 2.626 [1.697, 6.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.116, 10.100], loss: 0.164217, mae: 0.347272, mean_q: 4.147746
 43622/100000: episode: 520, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 59.299, mean reward: 2.281 [1.660, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.112, 10.221], loss: 0.279991, mae: 0.378066, mean_q: 4.238341
 43648/100000: episode: 521, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 60.012, mean reward: 2.308 [1.957, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.428, 10.100], loss: 0.305264, mae: 0.405527, mean_q: 4.341952
 43667/100000: episode: 522, duration: 0.094s, episode steps: 19, steps per second: 201, episode reward: 37.327, mean reward: 1.965 [1.657, 2.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.089, 10.100], loss: 0.211149, mae: 0.375339, mean_q: 4.264674
 43694/100000: episode: 523, duration: 0.137s, episode steps: 27, steps per second: 196, episode reward: 56.137, mean reward: 2.079 [1.611, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.721, 10.100], loss: 0.170109, mae: 0.347447, mean_q: 4.186822
 43712/100000: episode: 524, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 39.097, mean reward: 2.172 [1.818, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.234, 10.100], loss: 0.150928, mae: 0.362380, mean_q: 4.227395
 43737/100000: episode: 525, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 73.061, mean reward: 2.922 [1.951, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.236, 10.100], loss: 0.140365, mae: 0.338475, mean_q: 4.207058
 43755/100000: episode: 526, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 48.742, mean reward: 2.708 [2.258, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.254, 10.100], loss: 0.115762, mae: 0.321600, mean_q: 4.237060
 43784/100000: episode: 527, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 76.674, mean reward: 2.644 [1.578, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.130, 10.100], loss: 0.173777, mae: 0.356576, mean_q: 4.248456
 43809/100000: episode: 528, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 58.618, mean reward: 2.345 [1.651, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.124, 10.100], loss: 0.139355, mae: 0.351524, mean_q: 4.282399
 43835/100000: episode: 529, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 78.142, mean reward: 3.005 [2.392, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.212, 10.100], loss: 0.132638, mae: 0.338878, mean_q: 4.201130
 43860/100000: episode: 530, duration: 0.156s, episode steps: 25, steps per second: 161, episode reward: 62.681, mean reward: 2.507 [1.933, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.138, 10.100], loss: 0.296450, mae: 0.417432, mean_q: 4.320900
 43886/100000: episode: 531, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 67.943, mean reward: 2.613 [1.810, 5.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.290, 10.100], loss: 0.238645, mae: 0.409479, mean_q: 4.368172
 43911/100000: episode: 532, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 51.860, mean reward: 2.074 [1.516, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.137, 10.100], loss: 0.205594, mae: 0.420071, mean_q: 4.330586
 43937/100000: episode: 533, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 66.081, mean reward: 2.542 [1.868, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.464, 10.100], loss: 0.216373, mae: 0.364015, mean_q: 4.277806
 43962/100000: episode: 534, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 55.752, mean reward: 2.230 [1.839, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.462, 10.100], loss: 0.187527, mae: 0.355797, mean_q: 4.244865
 43987/100000: episode: 535, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 85.066, mean reward: 3.403 [2.245, 6.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.504, 10.100], loss: 0.185917, mae: 0.353810, mean_q: 4.304392
 44016/100000: episode: 536, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 115.490, mean reward: 3.982 [2.827, 6.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.741, 10.100], loss: 0.164746, mae: 0.380352, mean_q: 4.286563
 44035/100000: episode: 537, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 42.173, mean reward: 2.220 [1.940, 2.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.369, 10.100], loss: 0.173767, mae: 0.366549, mean_q: 4.348340
 44060/100000: episode: 538, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 63.486, mean reward: 2.539 [1.830, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.385, 10.100], loss: 0.198822, mae: 0.347789, mean_q: 4.276419
 44087/100000: episode: 539, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 66.385, mean reward: 2.459 [1.920, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.911, 10.100], loss: 0.148037, mae: 0.344800, mean_q: 4.346768
 44113/100000: episode: 540, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 80.123, mean reward: 3.082 [2.245, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.313, 10.100], loss: 0.200186, mae: 0.396239, mean_q: 4.373055
 44142/100000: episode: 541, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 71.276, mean reward: 2.458 [1.723, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.768, 10.100], loss: 0.131074, mae: 0.343060, mean_q: 4.339171
 44169/100000: episode: 542, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 58.766, mean reward: 2.177 [1.715, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.197, 10.100], loss: 0.259174, mae: 0.393872, mean_q: 4.403104
 44198/100000: episode: 543, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 126.343, mean reward: 4.357 [2.330, 7.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.348, 10.100], loss: 0.174074, mae: 0.359955, mean_q: 4.367697
 44225/100000: episode: 544, duration: 0.167s, episode steps: 27, steps per second: 161, episode reward: 74.029, mean reward: 2.742 [1.743, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.336, 10.100], loss: 0.213319, mae: 0.396422, mean_q: 4.394236
 44250/100000: episode: 545, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 61.982, mean reward: 2.479 [1.777, 4.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.339, 10.100], loss: 0.187245, mae: 0.383068, mean_q: 4.340118
 44277/100000: episode: 546, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 63.949, mean reward: 2.368 [1.745, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.377, 10.100], loss: 0.187558, mae: 0.367979, mean_q: 4.414145
 44302/100000: episode: 547, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 47.978, mean reward: 1.919 [1.465, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.348, 10.100], loss: 0.265132, mae: 0.383276, mean_q: 4.404183
 44321/100000: episode: 548, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 43.537, mean reward: 2.291 [1.955, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.249, 10.100], loss: 0.213341, mae: 0.411273, mean_q: 4.529135
 44346/100000: episode: 549, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 48.927, mean reward: 1.957 [1.589, 2.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.303, 10.100], loss: 0.242902, mae: 0.399033, mean_q: 4.394031
 44365/100000: episode: 550, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 62.148, mean reward: 3.271 [2.267, 12.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.727, 10.100], loss: 0.212755, mae: 0.384431, mean_q: 4.371755
 44384/100000: episode: 551, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 52.234, mean reward: 2.749 [1.967, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.456, 10.100], loss: 0.245482, mae: 0.454144, mean_q: 4.543358
[Info] 2-TH LEVEL FOUND: 8.453256607055664, Considering 10/90 traces
 44411/100000: episode: 552, duration: 4.306s, episode steps: 27, steps per second: 6, episode reward: 61.575, mean reward: 2.281 [1.663, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.107, 10.100], loss: 0.214164, mae: 0.410997, mean_q: 4.457738
 44433/100000: episode: 553, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 98.264, mean reward: 4.467 [2.603, 8.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.753, 10.331], loss: 0.265850, mae: 0.396146, mean_q: 4.433785
 44455/100000: episode: 554, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 72.837, mean reward: 3.311 [1.673, 4.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.320, 10.264], loss: 0.193874, mae: 0.382121, mean_q: 4.513331
 44477/100000: episode: 555, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 78.988, mean reward: 3.590 [2.092, 6.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.306, 10.397], loss: 0.227071, mae: 0.415321, mean_q: 4.533327
[Info] FALSIFICATION!
[Info] Levels: [5.2583976, 8.453257, 9.574174]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 44498/100000: episode: 556, duration: 4.630s, episode steps: 21, steps per second: 5, episode reward: 198.770, mean reward: 9.465 [3.219, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.486, 10.562], loss: 0.275180, mae: 0.415196, mean_q: 4.569173
 44598/100000: episode: 557, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 195.053, mean reward: 1.951 [1.467, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.769, 10.185], loss: 1.633085, mae: 0.458406, mean_q: 4.525679
 44698/100000: episode: 558, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 179.770, mean reward: 1.798 [1.445, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.071, 10.098], loss: 0.272500, mae: 0.445229, mean_q: 4.538164
 44798/100000: episode: 559, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 202.666, mean reward: 2.027 [1.450, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.122], loss: 0.254463, mae: 0.445866, mean_q: 4.535463
 44898/100000: episode: 560, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.257, mean reward: 1.963 [1.490, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.755, 10.098], loss: 0.189213, mae: 0.389804, mean_q: 4.494806
 44998/100000: episode: 561, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 186.668, mean reward: 1.867 [1.468, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.098], loss: 4.511549, mae: 0.594468, mean_q: 4.596093
 45098/100000: episode: 562, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 180.408, mean reward: 1.804 [1.446, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.535, 10.098], loss: 3.063691, mae: 0.598261, mean_q: 4.613250
 45198/100000: episode: 563, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 203.831, mean reward: 2.038 [1.468, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.863, 10.098], loss: 0.305328, mae: 0.468332, mean_q: 4.596473
 45298/100000: episode: 564, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 199.889, mean reward: 1.999 [1.499, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.496, 10.098], loss: 0.269269, mae: 0.426075, mean_q: 4.540425
 45398/100000: episode: 565, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 183.645, mean reward: 1.836 [1.462, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.993, 10.098], loss: 1.687595, mae: 0.530027, mean_q: 4.526309
 45498/100000: episode: 566, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 224.217, mean reward: 2.242 [1.445, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.689, 10.103], loss: 0.188749, mae: 0.396523, mean_q: 4.505629
 45598/100000: episode: 567, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.000, mean reward: 1.870 [1.454, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.280], loss: 0.210018, mae: 0.398579, mean_q: 4.486642
 45698/100000: episode: 568, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 191.925, mean reward: 1.919 [1.464, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.624, 10.261], loss: 1.638262, mae: 0.490714, mean_q: 4.498446
 45798/100000: episode: 569, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 207.432, mean reward: 2.074 [1.467, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.442, 10.473], loss: 0.208751, mae: 0.411282, mean_q: 4.443592
 45898/100000: episode: 570, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.168, mean reward: 1.882 [1.448, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.017, 10.098], loss: 1.604468, mae: 0.452334, mean_q: 4.469769
 45998/100000: episode: 571, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 177.840, mean reward: 1.778 [1.477, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.121], loss: 0.235199, mae: 0.423815, mean_q: 4.508898
 46098/100000: episode: 572, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.587, mean reward: 1.976 [1.481, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.967, 10.238], loss: 0.222124, mae: 0.411665, mean_q: 4.487239
 46198/100000: episode: 573, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.533, mean reward: 1.885 [1.440, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.849, 10.394], loss: 1.615349, mae: 0.481895, mean_q: 4.512406
 46298/100000: episode: 574, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 199.590, mean reward: 1.996 [1.458, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.179, 10.224], loss: 0.198004, mae: 0.389978, mean_q: 4.519000
 46398/100000: episode: 575, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 194.749, mean reward: 1.947 [1.567, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.108, 10.166], loss: 0.188304, mae: 0.399056, mean_q: 4.526360
 46498/100000: episode: 576, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 221.650, mean reward: 2.216 [1.480, 4.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.064, 10.395], loss: 0.181125, mae: 0.384782, mean_q: 4.466228
 46598/100000: episode: 577, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.745, mean reward: 1.817 [1.451, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.098], loss: 5.709497, mae: 0.685318, mean_q: 4.652213
 46698/100000: episode: 578, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 219.136, mean reward: 2.191 [1.488, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.366, 10.098], loss: 1.580750, mae: 0.465620, mean_q: 4.557753
 46798/100000: episode: 579, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 198.264, mean reward: 1.983 [1.468, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.454, 10.336], loss: 1.598889, mae: 0.479821, mean_q: 4.556387
 46898/100000: episode: 580, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.098, mean reward: 1.931 [1.472, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.513, 10.098], loss: 1.564701, mae: 0.474623, mean_q: 4.538369
 46998/100000: episode: 581, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.339, mean reward: 1.863 [1.446, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.181, 10.102], loss: 1.618709, mae: 0.527076, mean_q: 4.535956
 47098/100000: episode: 582, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.440, mean reward: 1.814 [1.464, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.125, 10.178], loss: 2.989065, mae: 0.591623, mean_q: 4.521708
 47198/100000: episode: 583, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.012, mean reward: 1.840 [1.466, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.585, 10.098], loss: 2.865324, mae: 0.542725, mean_q: 4.548730
 47298/100000: episode: 584, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.278, mean reward: 1.963 [1.489, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.115], loss: 0.224239, mae: 0.433903, mean_q: 4.429307
 47398/100000: episode: 585, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 176.729, mean reward: 1.767 [1.449, 2.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.033, 10.179], loss: 2.913171, mae: 0.503418, mean_q: 4.472602
 47498/100000: episode: 586, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.435, mean reward: 1.974 [1.469, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.433, 10.208], loss: 0.236750, mae: 0.426200, mean_q: 4.420569
 47598/100000: episode: 587, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 192.134, mean reward: 1.921 [1.513, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.627, 10.098], loss: 2.821760, mae: 0.503976, mean_q: 4.444834
 47698/100000: episode: 588, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 196.228, mean reward: 1.962 [1.445, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.109, 10.098], loss: 2.935338, mae: 0.559936, mean_q: 4.432101
 47798/100000: episode: 589, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 206.794, mean reward: 2.068 [1.505, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.082, 10.236], loss: 0.230683, mae: 0.410149, mean_q: 4.324949
 47898/100000: episode: 590, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.725, mean reward: 1.847 [1.469, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.098], loss: 1.556604, mae: 0.466306, mean_q: 4.362796
 47998/100000: episode: 591, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.382, mean reward: 1.894 [1.486, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.366, 10.230], loss: 0.189167, mae: 0.385242, mean_q: 4.307481
 48098/100000: episode: 592, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.324, mean reward: 1.863 [1.455, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.636, 10.238], loss: 2.908321, mae: 0.503933, mean_q: 4.293908
 48198/100000: episode: 593, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 214.749, mean reward: 2.147 [1.438, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.145, 10.098], loss: 1.520878, mae: 0.436828, mean_q: 4.275746
 48298/100000: episode: 594, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.055, mean reward: 1.981 [1.465, 4.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.398, 10.200], loss: 0.178255, mae: 0.368710, mean_q: 4.221171
 48398/100000: episode: 595, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.457, mean reward: 1.955 [1.434, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.840, 10.278], loss: 0.152435, mae: 0.352545, mean_q: 4.129738
 48498/100000: episode: 596, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.650, mean reward: 1.896 [1.434, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.101, 10.098], loss: 0.170406, mae: 0.362633, mean_q: 4.143113
 48598/100000: episode: 597, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.012, mean reward: 2.010 [1.465, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.103], loss: 1.532814, mae: 0.418630, mean_q: 4.150544
 48698/100000: episode: 598, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.434, mean reward: 1.834 [1.433, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.259, 10.238], loss: 0.125089, mae: 0.345200, mean_q: 4.109564
 48798/100000: episode: 599, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 181.435, mean reward: 1.814 [1.441, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.430, 10.245], loss: 0.141582, mae: 0.346549, mean_q: 4.068204
 48898/100000: episode: 600, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.571, mean reward: 1.956 [1.450, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.860, 10.098], loss: 0.133461, mae: 0.332239, mean_q: 4.049395
 48998/100000: episode: 601, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 204.711, mean reward: 2.047 [1.537, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.966, 10.098], loss: 0.106499, mae: 0.314721, mean_q: 3.962651
 49098/100000: episode: 602, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 241.459, mean reward: 2.415 [1.479, 5.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.895, 10.098], loss: 1.512671, mae: 0.386022, mean_q: 4.019735
 49198/100000: episode: 603, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.876, mean reward: 1.939 [1.470, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.098], loss: 1.483444, mae: 0.381072, mean_q: 4.002602
 49298/100000: episode: 604, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.824, mean reward: 1.928 [1.444, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.774, 10.256], loss: 2.808580, mae: 0.438895, mean_q: 3.996036
 49398/100000: episode: 605, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.244, mean reward: 1.932 [1.467, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.041, 10.317], loss: 0.131441, mae: 0.333688, mean_q: 3.945187
 49498/100000: episode: 606, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.409, mean reward: 1.984 [1.485, 4.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.359, 10.098], loss: 0.101696, mae: 0.305620, mean_q: 3.874285
 49598/100000: episode: 607, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 176.653, mean reward: 1.767 [1.457, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.098], loss: 0.088957, mae: 0.298012, mean_q: 3.874169
 49698/100000: episode: 608, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 202.314, mean reward: 2.023 [1.486, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.961, 10.148], loss: 0.096009, mae: 0.303899, mean_q: 3.888214
 49798/100000: episode: 609, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 213.162, mean reward: 2.132 [1.515, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.572, 10.406], loss: 0.086692, mae: 0.293989, mean_q: 3.860645
 49898/100000: episode: 610, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.566, mean reward: 1.966 [1.453, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.450, 10.259], loss: 0.081172, mae: 0.290873, mean_q: 3.869717
 49998/100000: episode: 611, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.620, mean reward: 1.956 [1.471, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.550, 10.186], loss: 0.087278, mae: 0.299674, mean_q: 3.886133
 50098/100000: episode: 612, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 205.579, mean reward: 2.056 [1.458, 6.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.290, 10.098], loss: 0.082306, mae: 0.291782, mean_q: 3.870949
 50198/100000: episode: 613, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.287, mean reward: 1.903 [1.463, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.779, 10.343], loss: 0.084071, mae: 0.289010, mean_q: 3.857117
 50298/100000: episode: 614, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.861, mean reward: 1.899 [1.462, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.748, 10.136], loss: 0.083710, mae: 0.289985, mean_q: 3.884875
 50398/100000: episode: 615, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 186.975, mean reward: 1.870 [1.432, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.175, 10.098], loss: 0.098854, mae: 0.304528, mean_q: 3.865341
 50498/100000: episode: 616, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 195.230, mean reward: 1.952 [1.508, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.766, 10.098], loss: 0.086125, mae: 0.295661, mean_q: 3.876002
 50598/100000: episode: 617, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 174.859, mean reward: 1.749 [1.457, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.123, 10.120], loss: 0.091572, mae: 0.293915, mean_q: 3.874911
 50698/100000: episode: 618, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 207.725, mean reward: 2.077 [1.502, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.147, 10.098], loss: 0.080081, mae: 0.287526, mean_q: 3.871435
 50798/100000: episode: 619, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 178.374, mean reward: 1.784 [1.449, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.409, 10.098], loss: 0.078139, mae: 0.284417, mean_q: 3.847683
 50898/100000: episode: 620, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.434, mean reward: 1.984 [1.475, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.098], loss: 0.089635, mae: 0.295863, mean_q: 3.857022
 50998/100000: episode: 621, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 199.505, mean reward: 1.995 [1.499, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.861, 10.231], loss: 0.086518, mae: 0.287686, mean_q: 3.861451
 51098/100000: episode: 622, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.694, mean reward: 1.977 [1.440, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.023, 10.156], loss: 0.087975, mae: 0.295818, mean_q: 3.861370
 51198/100000: episode: 623, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.335, mean reward: 1.873 [1.468, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.276, 10.127], loss: 0.092862, mae: 0.294513, mean_q: 3.853026
 51298/100000: episode: 624, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.574, mean reward: 1.926 [1.469, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.884, 10.296], loss: 0.106252, mae: 0.310624, mean_q: 3.872144
 51398/100000: episode: 625, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 183.369, mean reward: 1.834 [1.432, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.986, 10.098], loss: 0.089143, mae: 0.299636, mean_q: 3.835224
 51498/100000: episode: 626, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 189.968, mean reward: 1.900 [1.457, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.274, 10.150], loss: 0.093667, mae: 0.303178, mean_q: 3.845243
 51598/100000: episode: 627, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.552, mean reward: 1.866 [1.436, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.214, 10.098], loss: 0.093804, mae: 0.304827, mean_q: 3.850334
 51698/100000: episode: 628, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.046, mean reward: 1.900 [1.447, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.994, 10.207], loss: 0.088330, mae: 0.291723, mean_q: 3.847752
 51798/100000: episode: 629, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 184.611, mean reward: 1.846 [1.458, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.080, 10.098], loss: 0.090992, mae: 0.296746, mean_q: 3.815696
 51898/100000: episode: 630, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 197.043, mean reward: 1.970 [1.445, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.724, 10.355], loss: 0.082091, mae: 0.286067, mean_q: 3.814761
 51998/100000: episode: 631, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.749, mean reward: 1.897 [1.436, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.308, 10.098], loss: 0.079235, mae: 0.285210, mean_q: 3.823060
 52098/100000: episode: 632, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 201.502, mean reward: 2.015 [1.496, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.098], loss: 0.100346, mae: 0.304276, mean_q: 3.824384
 52198/100000: episode: 633, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 198.396, mean reward: 1.984 [1.455, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.098], loss: 0.082039, mae: 0.285605, mean_q: 3.829845
 52298/100000: episode: 634, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 205.711, mean reward: 2.057 [1.477, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.288, 10.262], loss: 0.095967, mae: 0.301437, mean_q: 3.854091
 52398/100000: episode: 635, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 211.733, mean reward: 2.117 [1.502, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.112], loss: 0.092160, mae: 0.293086, mean_q: 3.843077
 52498/100000: episode: 636, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 184.087, mean reward: 1.841 [1.458, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.830, 10.098], loss: 0.082869, mae: 0.287411, mean_q: 3.851338
 52598/100000: episode: 637, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.768, mean reward: 1.998 [1.466, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.097, 10.345], loss: 0.088638, mae: 0.294818, mean_q: 3.857630
 52698/100000: episode: 638, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.096, mean reward: 1.781 [1.435, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.726, 10.098], loss: 0.097772, mae: 0.307577, mean_q: 3.873829
 52798/100000: episode: 639, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 233.829, mean reward: 2.338 [1.466, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.393], loss: 0.094345, mae: 0.288180, mean_q: 3.848158
 52898/100000: episode: 640, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 189.967, mean reward: 1.900 [1.469, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.815, 10.098], loss: 0.081482, mae: 0.279745, mean_q: 3.846914
 52998/100000: episode: 641, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 181.147, mean reward: 1.811 [1.452, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.098], loss: 0.085694, mae: 0.291840, mean_q: 3.844863
 53098/100000: episode: 642, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 204.303, mean reward: 2.043 [1.462, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.023, 10.237], loss: 0.089850, mae: 0.295173, mean_q: 3.850792
 53198/100000: episode: 643, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.223, mean reward: 1.852 [1.494, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.333, 10.098], loss: 0.102905, mae: 0.304357, mean_q: 3.873413
 53298/100000: episode: 644, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 213.438, mean reward: 2.134 [1.466, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.531, 10.098], loss: 0.098487, mae: 0.300825, mean_q: 3.871048
 53398/100000: episode: 645, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 191.662, mean reward: 1.917 [1.460, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.098], loss: 0.092778, mae: 0.300394, mean_q: 3.879391
 53498/100000: episode: 646, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.243, mean reward: 1.912 [1.458, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.082, 10.098], loss: 0.095568, mae: 0.296017, mean_q: 3.865886
 53598/100000: episode: 647, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 222.143, mean reward: 2.221 [1.431, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.572, 10.098], loss: 0.082600, mae: 0.281413, mean_q: 3.844030
 53698/100000: episode: 648, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.589, mean reward: 1.846 [1.441, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.084, 10.099], loss: 0.087274, mae: 0.290504, mean_q: 3.870080
 53798/100000: episode: 649, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 197.514, mean reward: 1.975 [1.525, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.172], loss: 0.096861, mae: 0.298723, mean_q: 3.859389
 53898/100000: episode: 650, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 215.756, mean reward: 2.158 [1.480, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.419, 10.142], loss: 0.088105, mae: 0.293631, mean_q: 3.873980
 53998/100000: episode: 651, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 183.308, mean reward: 1.833 [1.503, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.588, 10.143], loss: 0.099375, mae: 0.305748, mean_q: 3.889173
 54098/100000: episode: 652, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 206.426, mean reward: 2.064 [1.520, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.064, 10.173], loss: 0.078215, mae: 0.285040, mean_q: 3.851101
 54198/100000: episode: 653, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.712, mean reward: 1.907 [1.481, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.719, 10.155], loss: 0.091704, mae: 0.297602, mean_q: 3.879327
 54298/100000: episode: 654, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 213.583, mean reward: 2.136 [1.575, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.098], loss: 0.093863, mae: 0.298970, mean_q: 3.865216
 54398/100000: episode: 655, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.872, mean reward: 1.939 [1.464, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.185, 10.201], loss: 0.081860, mae: 0.287069, mean_q: 3.857894
[Info] 1-TH LEVEL FOUND: 5.559656143188477, Considering 10/90 traces
 54498/100000: episode: 656, duration: 4.767s, episode steps: 100, steps per second: 21, episode reward: 198.819, mean reward: 1.988 [1.505, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.448, 10.285], loss: 0.089286, mae: 0.298561, mean_q: 3.873161
 54520/100000: episode: 657, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 59.460, mean reward: 2.703 [2.129, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.427], loss: 0.089522, mae: 0.301943, mean_q: 3.872839
 54605/100000: episode: 658, duration: 0.440s, episode steps: 85, steps per second: 193, episode reward: 175.137, mean reward: 2.060 [1.455, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.598 [-0.845, 10.464], loss: 0.096272, mae: 0.298997, mean_q: 3.886051
 54611/100000: episode: 659, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 19.879, mean reward: 3.313 [3.086, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.472], loss: 0.136054, mae: 0.321355, mean_q: 3.891951
 54696/100000: episode: 660, duration: 0.452s, episode steps: 85, steps per second: 188, episode reward: 163.123, mean reward: 1.919 [1.474, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.605 [-0.826, 10.257], loss: 0.086224, mae: 0.292266, mean_q: 3.872736
 54709/100000: episode: 661, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 36.531, mean reward: 2.810 [2.101, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.890, 10.100], loss: 0.074245, mae: 0.277373, mean_q: 3.826958
 54749/100000: episode: 662, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 142.117, mean reward: 3.553 [2.367, 8.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.724, 10.100], loss: 0.095063, mae: 0.298930, mean_q: 3.887511
 54776/100000: episode: 663, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 96.239, mean reward: 3.564 [2.284, 6.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.312, 10.100], loss: 0.095715, mae: 0.300605, mean_q: 3.891024
 54810/100000: episode: 664, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 76.505, mean reward: 2.250 [1.484, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.337, 10.131], loss: 0.094248, mae: 0.300345, mean_q: 3.910933
 54826/100000: episode: 665, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 45.398, mean reward: 2.837 [2.028, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.261, 10.259], loss: 0.131204, mae: 0.333856, mean_q: 3.939236
 54848/100000: episode: 666, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 59.014, mean reward: 2.682 [2.004, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.356], loss: 0.094812, mae: 0.303644, mean_q: 3.914332
 54854/100000: episode: 667, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 20.242, mean reward: 3.374 [2.733, 4.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.297, 10.439], loss: 0.181287, mae: 0.390230, mean_q: 4.080175
 54878/100000: episode: 668, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 62.225, mean reward: 2.593 [1.943, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.796, 10.357], loss: 0.135822, mae: 0.357575, mean_q: 3.981303
 54963/100000: episode: 669, duration: 0.449s, episode steps: 85, steps per second: 189, episode reward: 167.853, mean reward: 1.975 [1.452, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.592 [-0.609, 10.100], loss: 0.098343, mae: 0.314173, mean_q: 3.924489
 54983/100000: episode: 670, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 56.755, mean reward: 2.838 [1.792, 5.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.218, 10.317], loss: 0.100127, mae: 0.312590, mean_q: 3.964336
 55005/100000: episode: 671, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 55.321, mean reward: 2.515 [2.088, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.820, 10.248], loss: 0.088404, mae: 0.302961, mean_q: 3.949033
 55045/100000: episode: 672, duration: 0.228s, episode steps: 40, steps per second: 175, episode reward: 103.243, mean reward: 2.581 [2.103, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.516, 10.100], loss: 0.115878, mae: 0.330343, mean_q: 3.975029
 55051/100000: episode: 673, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 16.140, mean reward: 2.690 [2.485, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.414], loss: 0.095764, mae: 0.321290, mean_q: 3.943693
 55057/100000: episode: 674, duration: 0.044s, episode steps: 6, steps per second: 136, episode reward: 20.148, mean reward: 3.358 [3.054, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.547], loss: 0.073572, mae: 0.276414, mean_q: 3.869133
 55084/100000: episode: 675, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 68.644, mean reward: 2.542 [1.884, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.430, 10.100], loss: 0.122521, mae: 0.324127, mean_q: 3.989792
 55111/100000: episode: 676, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 141.248, mean reward: 5.231 [2.568, 21.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.753, 10.100], loss: 0.094839, mae: 0.310375, mean_q: 3.971294
 55131/100000: episode: 677, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 55.492, mean reward: 2.775 [2.171, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.349], loss: 0.106413, mae: 0.308524, mean_q: 3.954332
 55171/100000: episode: 678, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 111.349, mean reward: 2.784 [2.188, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.443, 10.100], loss: 0.094869, mae: 0.308096, mean_q: 3.947659
 55256/100000: episode: 679, duration: 0.447s, episode steps: 85, steps per second: 190, episode reward: 164.321, mean reward: 1.933 [1.521, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-2.087, 10.191], loss: 0.206810, mae: 0.375763, mean_q: 4.019712
 55280/100000: episode: 680, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 65.466, mean reward: 2.728 [2.040, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.760, 10.337], loss: 0.692851, mae: 0.462456, mean_q: 4.061060
 55286/100000: episode: 681, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 19.268, mean reward: 3.211 [2.718, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.479], loss: 1.100652, mae: 0.481178, mean_q: 4.036904
 55306/100000: episode: 682, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 47.585, mean reward: 2.379 [1.879, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.093, 10.334], loss: 0.132912, mae: 0.366406, mean_q: 3.960394
 55319/100000: episode: 683, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 41.068, mean reward: 3.159 [2.501, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.416, 10.100], loss: 0.113382, mae: 0.340073, mean_q: 3.967598
 55341/100000: episode: 684, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 53.440, mean reward: 2.429 [2.026, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.384], loss: 0.140426, mae: 0.339021, mean_q: 4.004540
 55368/100000: episode: 685, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 67.640, mean reward: 2.505 [2.079, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.264, 10.100], loss: 0.112404, mae: 0.333624, mean_q: 4.069887
 55408/100000: episode: 686, duration: 0.224s, episode steps: 40, steps per second: 178, episode reward: 98.209, mean reward: 2.455 [1.644, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.170, 10.100], loss: 0.099695, mae: 0.308431, mean_q: 4.016531
 55424/100000: episode: 687, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 38.320, mean reward: 2.395 [2.012, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.444, 10.411], loss: 0.119687, mae: 0.330192, mean_q: 4.043906
 55458/100000: episode: 688, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 71.395, mean reward: 2.100 [1.754, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.790, 10.100], loss: 0.114033, mae: 0.320213, mean_q: 4.015296
 55478/100000: episode: 689, duration: 0.128s, episode steps: 20, steps per second: 156, episode reward: 57.187, mean reward: 2.859 [2.294, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.538, 10.489], loss: 0.097959, mae: 0.315174, mean_q: 4.033234
 55500/100000: episode: 690, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 61.085, mean reward: 2.777 [1.511, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.300, 10.256], loss: 0.110547, mae: 0.327539, mean_q: 4.030208
 55540/100000: episode: 691, duration: 0.209s, episode steps: 40, steps per second: 191, episode reward: 102.296, mean reward: 2.557 [1.771, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.348, 10.100], loss: 0.116706, mae: 0.325474, mean_q: 4.012351
 55567/100000: episode: 692, duration: 0.133s, episode steps: 27, steps per second: 202, episode reward: 97.133, mean reward: 3.598 [2.294, 7.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.795, 10.100], loss: 0.117954, mae: 0.347484, mean_q: 4.140235
 55591/100000: episode: 693, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 88.572, mean reward: 3.690 [2.665, 11.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.720, 10.530], loss: 0.329265, mae: 0.374808, mean_q: 4.124367
 55604/100000: episode: 694, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 34.781, mean reward: 2.675 [2.110, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.225, 10.100], loss: 0.459536, mae: 0.374797, mean_q: 4.060114
 55631/100000: episode: 695, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 82.339, mean reward: 3.050 [1.746, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.440, 10.100], loss: 0.296677, mae: 0.388908, mean_q: 4.148435
 55671/100000: episode: 696, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 110.492, mean reward: 2.762 [1.555, 6.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.648, 10.100], loss: 0.241916, mae: 0.351335, mean_q: 4.106596
 55698/100000: episode: 697, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 67.181, mean reward: 2.488 [1.873, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.214, 10.100], loss: 0.510860, mae: 0.416626, mean_q: 4.200865
 55711/100000: episode: 698, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 30.349, mean reward: 2.335 [1.824, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.235, 10.100], loss: 0.169559, mae: 0.404335, mean_q: 4.060087
 55724/100000: episode: 699, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 37.628, mean reward: 2.894 [2.554, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.317, 10.100], loss: 0.143684, mae: 0.386096, mean_q: 4.213334
 55746/100000: episode: 700, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 56.043, mean reward: 2.547 [1.859, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.059, 10.256], loss: 0.127853, mae: 0.353382, mean_q: 4.146080
 55786/100000: episode: 701, duration: 0.217s, episode steps: 40, steps per second: 185, episode reward: 135.394, mean reward: 3.385 [2.255, 6.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.472, 10.100], loss: 0.225472, mae: 0.351866, mean_q: 4.173070
 55799/100000: episode: 702, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 30.437, mean reward: 2.341 [1.957, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.307, 10.100], loss: 0.222097, mae: 0.440441, mean_q: 4.135529
 55839/100000: episode: 703, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 147.475, mean reward: 3.687 [1.859, 9.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.341, 10.100], loss: 0.168542, mae: 0.407765, mean_q: 4.197598
 55859/100000: episode: 704, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 62.490, mean reward: 3.124 [2.307, 7.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.590, 10.370], loss: 0.120528, mae: 0.345784, mean_q: 4.227886
 55875/100000: episode: 705, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 37.724, mean reward: 2.358 [2.023, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.319, 10.366], loss: 0.134006, mae: 0.366955, mean_q: 4.189795
 55909/100000: episode: 706, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 119.374, mean reward: 3.511 [2.110, 9.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.467, 10.100], loss: 0.314393, mae: 0.416065, mean_q: 4.254014
 55936/100000: episode: 707, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 54.280, mean reward: 2.010 [1.648, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.039, 10.100], loss: 0.177982, mae: 0.380430, mean_q: 4.316801
 55952/100000: episode: 708, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 37.846, mean reward: 2.365 [1.876, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.337], loss: 0.181542, mae: 0.376774, mean_q: 4.277752
 55992/100000: episode: 709, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 120.747, mean reward: 3.019 [2.319, 5.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.372, 10.100], loss: 0.222077, mae: 0.405954, mean_q: 4.294442
 56012/100000: episode: 710, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 49.516, mean reward: 2.476 [2.084, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.626, 10.361], loss: 0.349300, mae: 0.377893, mean_q: 4.280715
 56039/100000: episode: 711, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 82.938, mean reward: 3.072 [1.730, 7.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.829, 10.100], loss: 0.212247, mae: 0.372446, mean_q: 4.215462
 56073/100000: episode: 712, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 76.092, mean reward: 2.238 [1.752, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.195, 10.100], loss: 0.204517, mae: 0.394400, mean_q: 4.301063
 56100/100000: episode: 713, duration: 0.150s, episode steps: 27, steps per second: 179, episode reward: 69.793, mean reward: 2.585 [2.042, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.130, 10.100], loss: 0.302632, mae: 0.379038, mean_q: 4.240582
 56120/100000: episode: 714, duration: 0.133s, episode steps: 20, steps per second: 151, episode reward: 48.303, mean reward: 2.415 [1.859, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.390], loss: 0.185387, mae: 0.353518, mean_q: 4.246081
 56140/100000: episode: 715, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 56.190, mean reward: 2.809 [2.109, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.451], loss: 0.169768, mae: 0.351656, mean_q: 4.300117
 56180/100000: episode: 716, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 110.231, mean reward: 2.756 [1.951, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.244, 10.100], loss: 0.130230, mae: 0.357550, mean_q: 4.263630
 56265/100000: episode: 717, duration: 0.450s, episode steps: 85, steps per second: 189, episode reward: 173.327, mean reward: 2.039 [1.548, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.590 [-0.518, 10.223], loss: 0.226450, mae: 0.377662, mean_q: 4.327041
 56285/100000: episode: 718, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 48.568, mean reward: 2.428 [2.060, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.401], loss: 0.138581, mae: 0.328433, mean_q: 4.299184
 56301/100000: episode: 719, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 39.454, mean reward: 2.466 [1.821, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.052, 10.352], loss: 0.168445, mae: 0.363172, mean_q: 4.271355
 56335/100000: episode: 720, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 70.215, mean reward: 2.065 [1.527, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.459, 10.100], loss: 0.122985, mae: 0.339746, mean_q: 4.229183
 56420/100000: episode: 721, duration: 0.469s, episode steps: 85, steps per second: 181, episode reward: 178.204, mean reward: 2.097 [1.469, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.595 [-0.271, 10.100], loss: 0.158081, mae: 0.376628, mean_q: 4.335621
 56436/100000: episode: 722, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 42.455, mean reward: 2.653 [2.022, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.336], loss: 0.138819, mae: 0.344528, mean_q: 4.340478
 56476/100000: episode: 723, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 210.775, mean reward: 5.269 [2.234, 14.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.404, 10.100], loss: 0.126439, mae: 0.344954, mean_q: 4.351327
 56492/100000: episode: 724, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 55.256, mean reward: 3.454 [2.592, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.531, 10.556], loss: 0.291749, mae: 0.414663, mean_q: 4.441026
 56526/100000: episode: 725, duration: 0.194s, episode steps: 34, steps per second: 176, episode reward: 113.240, mean reward: 3.331 [2.388, 5.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.386, 10.100], loss: 0.215581, mae: 0.397851, mean_q: 4.358228
 56548/100000: episode: 726, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 59.129, mean reward: 2.688 [1.898, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.817, 10.332], loss: 0.209967, mae: 0.404728, mean_q: 4.360257
 56633/100000: episode: 727, duration: 0.459s, episode steps: 85, steps per second: 185, episode reward: 160.937, mean reward: 1.893 [1.472, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.594 [-0.567, 10.262], loss: 0.214780, mae: 0.395425, mean_q: 4.429552
 56718/100000: episode: 728, duration: 0.455s, episode steps: 85, steps per second: 187, episode reward: 195.113, mean reward: 2.295 [1.503, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-1.132, 10.100], loss: 0.230857, mae: 0.412109, mean_q: 4.428370
 56734/100000: episode: 729, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 45.179, mean reward: 2.824 [2.049, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.447, 10.388], loss: 0.179501, mae: 0.369853, mean_q: 4.355067
 56761/100000: episode: 730, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 85.639, mean reward: 3.172 [2.155, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.243, 10.100], loss: 0.280572, mae: 0.434623, mean_q: 4.556213
 56785/100000: episode: 731, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 89.005, mean reward: 3.709 [2.610, 5.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.445], loss: 0.404850, mae: 0.440005, mean_q: 4.449076
 56798/100000: episode: 732, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 33.483, mean reward: 2.576 [2.340, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.523, 10.100], loss: 0.119471, mae: 0.338317, mean_q: 4.360423
 56818/100000: episode: 733, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 64.306, mean reward: 3.215 [2.447, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.075, 10.476], loss: 0.123599, mae: 0.350714, mean_q: 4.392571
 56845/100000: episode: 734, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 80.175, mean reward: 2.969 [1.771, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.206, 10.100], loss: 0.294027, mae: 0.395906, mean_q: 4.444464
 56867/100000: episode: 735, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 64.186, mean reward: 2.918 [2.242, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.318, 10.504], loss: 0.317920, mae: 0.440535, mean_q: 4.423734
 56880/100000: episode: 736, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 55.280, mean reward: 4.252 [3.034, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.625, 10.100], loss: 0.551370, mae: 0.495360, mean_q: 4.584934
 56965/100000: episode: 737, duration: 0.452s, episode steps: 85, steps per second: 188, episode reward: 187.040, mean reward: 2.200 [1.452, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.598 [-1.597, 10.409], loss: 0.194048, mae: 0.386574, mean_q: 4.510251
 56999/100000: episode: 738, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 130.568, mean reward: 3.840 [1.842, 14.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.106, 10.100], loss: 0.191793, mae: 0.407150, mean_q: 4.533276
 57033/100000: episode: 739, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 81.777, mean reward: 2.405 [1.838, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.631, 10.100], loss: 0.203973, mae: 0.399543, mean_q: 4.488975
 57053/100000: episode: 740, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 64.344, mean reward: 3.217 [1.723, 6.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.689, 10.224], loss: 0.170011, mae: 0.398572, mean_q: 4.527715
 57087/100000: episode: 741, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 104.113, mean reward: 3.062 [2.246, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.725, 10.100], loss: 0.271867, mae: 0.430485, mean_q: 4.588391
 57127/100000: episode: 742, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 111.175, mean reward: 2.779 [2.214, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.734, 10.100], loss: 0.243835, mae: 0.409110, mean_q: 4.559719
 57212/100000: episode: 743, duration: 0.437s, episode steps: 85, steps per second: 194, episode reward: 175.191, mean reward: 2.061 [1.459, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-0.674, 10.100], loss: 0.278590, mae: 0.453435, mean_q: 4.593181
 57246/100000: episode: 744, duration: 0.201s, episode steps: 34, steps per second: 169, episode reward: 88.231, mean reward: 2.595 [1.937, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.891, 10.100], loss: 0.171741, mae: 0.389506, mean_q: 4.487135
 57259/100000: episode: 745, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 36.874, mean reward: 2.836 [2.242, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.390, 10.100], loss: 0.202015, mae: 0.400768, mean_q: 4.666107
[Info] 2-TH LEVEL FOUND: 8.464042663574219, Considering 10/90 traces
 57293/100000: episode: 746, duration: 4.348s, episode steps: 34, steps per second: 8, episode reward: 91.028, mean reward: 2.677 [1.997, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.738, 10.100], loss: 0.222480, mae: 0.409193, mean_q: 4.492354
 57315/100000: episode: 747, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 94.910, mean reward: 4.314 [2.537, 9.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.027, 10.100], loss: 0.227690, mae: 0.401791, mean_q: 4.659864
 57324/100000: episode: 748, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 29.310, mean reward: 3.257 [2.879, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.327, 10.100], loss: 0.280610, mae: 0.446359, mean_q: 4.617428
 57333/100000: episode: 749, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 44.979, mean reward: 4.998 [3.452, 8.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.051, 10.100], loss: 0.157296, mae: 0.400027, mean_q: 4.564650
 57347/100000: episode: 750, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 43.642, mean reward: 3.117 [2.315, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.248, 10.100], loss: 0.683198, mae: 0.544511, mean_q: 4.677919
 57379/100000: episode: 751, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 152.855, mean reward: 4.777 [2.411, 9.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.322, 10.100], loss: 0.292588, mae: 0.468648, mean_q: 4.676018
 57390/100000: episode: 752, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 88.154, mean reward: 8.014 [3.567, 15.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.511, 10.100], loss: 0.229076, mae: 0.470633, mean_q: 4.742031
 57407/100000: episode: 753, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 95.774, mean reward: 5.634 [3.177, 10.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.055, 10.100], loss: 0.240393, mae: 0.421217, mean_q: 4.770796
 57416/100000: episode: 754, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 27.877, mean reward: 3.097 [2.772, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.396, 10.100], loss: 0.170601, mae: 0.402084, mean_q: 4.604851
 57430/100000: episode: 755, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 77.441, mean reward: 5.532 [3.921, 8.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.339, 10.100], loss: 0.260276, mae: 0.411139, mean_q: 4.575521
 57450/100000: episode: 756, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 86.787, mean reward: 4.339 [2.717, 9.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.338, 10.100], loss: 0.242926, mae: 0.472083, mean_q: 4.810934
 57464/100000: episode: 757, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 37.907, mean reward: 2.708 [1.809, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.293, 10.100], loss: 0.194472, mae: 0.434935, mean_q: 4.491390
 57475/100000: episode: 758, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 52.971, mean reward: 4.816 [2.792, 7.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.368, 10.100], loss: 0.198099, mae: 0.435255, mean_q: 4.647059
 57507/100000: episode: 759, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 237.785, mean reward: 7.431 [3.988, 16.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.532, 10.100], loss: 0.335588, mae: 0.487195, mean_q: 4.712905
 57531/100000: episode: 760, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 105.070, mean reward: 4.378 [3.057, 11.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.389, 10.100], loss: 0.242437, mae: 0.452395, mean_q: 4.708904
[Info] FALSIFICATION!
[Info] Levels: [5.559656, 8.464043, 10.950699]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 57541/100000: episode: 761, duration: 4.471s, episode steps: 10, steps per second: 2, episode reward: 183.377, mean reward: 18.338 [3.903, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.008, 8.942], loss: 0.326847, mae: 0.493289, mean_q: 4.753400
 57641/100000: episode: 762, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 189.064, mean reward: 1.891 [1.462, 5.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.956, 10.222], loss: 0.352994, mae: 0.490720, mean_q: 4.844991
 57741/100000: episode: 763, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 186.288, mean reward: 1.863 [1.460, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.377, 10.098], loss: 0.340957, mae: 0.477502, mean_q: 4.806218
 57841/100000: episode: 764, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.820, mean reward: 1.918 [1.458, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.309, 10.315], loss: 0.411483, mae: 0.512126, mean_q: 4.848097
 57941/100000: episode: 765, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.080, mean reward: 1.971 [1.465, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.712, 10.098], loss: 2.910155, mae: 0.649790, mean_q: 4.944945
 58041/100000: episode: 766, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 182.484, mean reward: 1.825 [1.462, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.098], loss: 1.620921, mae: 0.647018, mean_q: 4.929310
 58141/100000: episode: 767, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 208.299, mean reward: 2.083 [1.463, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.686, 10.504], loss: 0.494200, mae: 0.528390, mean_q: 4.944129
 58241/100000: episode: 768, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.654, mean reward: 1.857 [1.459, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.858, 10.228], loss: 0.374052, mae: 0.494341, mean_q: 4.937316
 58341/100000: episode: 769, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.089, mean reward: 1.831 [1.452, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.920, 10.206], loss: 0.343901, mae: 0.474697, mean_q: 4.863381
 58441/100000: episode: 770, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 196.527, mean reward: 1.965 [1.458, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.144, 10.130], loss: 1.537932, mae: 0.562092, mean_q: 4.893180
 58541/100000: episode: 771, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.808, mean reward: 1.918 [1.471, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.138, 10.098], loss: 0.459041, mae: 0.533098, mean_q: 4.959660
 58641/100000: episode: 772, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 184.956, mean reward: 1.850 [1.471, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.789, 10.098], loss: 1.617474, mae: 0.614636, mean_q: 4.906466
 58741/100000: episode: 773, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 180.057, mean reward: 1.801 [1.440, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.254], loss: 2.659449, mae: 0.586798, mean_q: 4.933107
 58841/100000: episode: 774, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 207.507, mean reward: 2.075 [1.452, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.558, 10.098], loss: 1.571162, mae: 0.576990, mean_q: 4.879153
 58941/100000: episode: 775, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.691, mean reward: 1.947 [1.444, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.627, 10.130], loss: 0.335747, mae: 0.478543, mean_q: 4.817332
 59041/100000: episode: 776, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 182.226, mean reward: 1.822 [1.439, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.569, 10.148], loss: 0.358471, mae: 0.483935, mean_q: 4.909443
 59141/100000: episode: 777, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.499, mean reward: 1.975 [1.439, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.098], loss: 0.341813, mae: 0.494796, mean_q: 4.875133
 59241/100000: episode: 778, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 186.712, mean reward: 1.867 [1.462, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.348, 10.098], loss: 1.454315, mae: 0.529605, mean_q: 4.856371
 59341/100000: episode: 779, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 205.186, mean reward: 2.052 [1.452, 6.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.485, 10.293], loss: 0.418655, mae: 0.520891, mean_q: 4.844282
 59441/100000: episode: 780, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 206.937, mean reward: 2.069 [1.464, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.306, 10.364], loss: 1.506420, mae: 0.538250, mean_q: 4.875423
 59541/100000: episode: 781, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.826, mean reward: 1.978 [1.502, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.347], loss: 2.608247, mae: 0.646301, mean_q: 4.845433
 59641/100000: episode: 782, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 214.375, mean reward: 2.144 [1.509, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.483, 10.230], loss: 1.479450, mae: 0.569092, mean_q: 4.929291
 59741/100000: episode: 783, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 195.650, mean reward: 1.956 [1.470, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.391, 10.098], loss: 0.378635, mae: 0.486610, mean_q: 4.769340
 59841/100000: episode: 784, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 201.300, mean reward: 2.013 [1.449, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.689, 10.173], loss: 1.502101, mae: 0.562583, mean_q: 4.806392
 59941/100000: episode: 785, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.150, mean reward: 1.902 [1.494, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.550, 10.236], loss: 0.353562, mae: 0.443772, mean_q: 4.728047
 60041/100000: episode: 786, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.491, mean reward: 1.945 [1.445, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.243], loss: 0.351535, mae: 0.479048, mean_q: 4.731311
 60141/100000: episode: 787, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.555, mean reward: 1.936 [1.452, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.903, 10.259], loss: 1.446817, mae: 0.560330, mean_q: 4.675606
 60241/100000: episode: 788, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 200.533, mean reward: 2.005 [1.494, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.068, 10.365], loss: 0.264643, mae: 0.447712, mean_q: 4.620537
 60341/100000: episode: 789, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 201.302, mean reward: 2.013 [1.449, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.098], loss: 0.427788, mae: 0.486921, mean_q: 4.696742
 60441/100000: episode: 790, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.332, mean reward: 1.933 [1.509, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.602, 10.098], loss: 0.382571, mae: 0.479805, mean_q: 4.628082
 60541/100000: episode: 791, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.908, mean reward: 1.839 [1.454, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.879, 10.098], loss: 0.293167, mae: 0.448069, mean_q: 4.604101
 60641/100000: episode: 792, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.306, mean reward: 1.883 [1.442, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.099], loss: 0.364587, mae: 0.470525, mean_q: 4.601200
 60741/100000: episode: 793, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.926, mean reward: 1.939 [1.484, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.169, 10.284], loss: 1.412226, mae: 0.502158, mean_q: 4.597404
 60841/100000: episode: 794, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.609, mean reward: 1.966 [1.435, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.159, 10.098], loss: 0.226716, mae: 0.400911, mean_q: 4.372673
 60941/100000: episode: 795, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.907, mean reward: 2.009 [1.478, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.619, 10.098], loss: 0.303113, mae: 0.426796, mean_q: 4.500894
 61041/100000: episode: 796, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 200.011, mean reward: 2.000 [1.442, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.130], loss: 0.315395, mae: 0.420613, mean_q: 4.410916
 61141/100000: episode: 797, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.527, mean reward: 1.905 [1.475, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.076, 10.276], loss: 0.223213, mae: 0.406601, mean_q: 4.400731
 61241/100000: episode: 798, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.281, mean reward: 1.863 [1.444, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.098], loss: 1.344153, mae: 0.485832, mean_q: 4.436152
 61341/100000: episode: 799, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 200.406, mean reward: 2.004 [1.483, 4.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.433, 10.171], loss: 1.337803, mae: 0.467555, mean_q: 4.422966
 61441/100000: episode: 800, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.568, mean reward: 1.876 [1.441, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.293, 10.098], loss: 1.501604, mae: 0.549346, mean_q: 4.391856
 61541/100000: episode: 801, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.714, mean reward: 1.917 [1.452, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.693, 10.098], loss: 1.325655, mae: 0.488176, mean_q: 4.297729
 61641/100000: episode: 802, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 206.606, mean reward: 2.066 [1.491, 3.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.355, 10.259], loss: 1.349359, mae: 0.472582, mean_q: 4.394696
 61741/100000: episode: 803, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 194.013, mean reward: 1.940 [1.447, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.419, 10.134], loss: 1.279082, mae: 0.484636, mean_q: 4.290992
 61841/100000: episode: 804, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 192.155, mean reward: 1.922 [1.465, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.478, 10.243], loss: 1.404696, mae: 0.492877, mean_q: 4.306467
 61941/100000: episode: 805, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.555, mean reward: 1.836 [1.470, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.735, 10.098], loss: 0.221003, mae: 0.377815, mean_q: 4.191068
 62041/100000: episode: 806, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.719, mean reward: 1.987 [1.450, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.645, 10.267], loss: 0.166768, mae: 0.353019, mean_q: 4.127155
 62141/100000: episode: 807, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 197.433, mean reward: 1.974 [1.443, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.474, 10.098], loss: 0.178580, mae: 0.350566, mean_q: 4.134392
 62241/100000: episode: 808, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 183.979, mean reward: 1.840 [1.463, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.792, 10.179], loss: 1.394030, mae: 0.496292, mean_q: 4.235084
 62341/100000: episode: 809, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 212.497, mean reward: 2.125 [1.450, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.512, 10.101], loss: 0.197596, mae: 0.353691, mean_q: 4.079266
 62441/100000: episode: 810, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 202.051, mean reward: 2.021 [1.459, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.256, 10.098], loss: 0.137478, mae: 0.335495, mean_q: 3.970261
 62541/100000: episode: 811, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 213.391, mean reward: 2.134 [1.462, 6.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.246, 10.098], loss: 0.131476, mae: 0.326929, mean_q: 3.902179
 62641/100000: episode: 812, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.980, mean reward: 1.950 [1.446, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.563, 10.201], loss: 0.090823, mae: 0.300663, mean_q: 3.845912
 62741/100000: episode: 813, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.720, mean reward: 1.847 [1.519, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.575, 10.098], loss: 0.101443, mae: 0.310096, mean_q: 3.854472
 62841/100000: episode: 814, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.739, mean reward: 1.897 [1.519, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.827, 10.129], loss: 0.087478, mae: 0.300079, mean_q: 3.842646
 62941/100000: episode: 815, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 181.399, mean reward: 1.814 [1.500, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.211, 10.233], loss: 0.086101, mae: 0.293843, mean_q: 3.842995
 63041/100000: episode: 816, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 188.106, mean reward: 1.881 [1.457, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.054, 10.136], loss: 0.082891, mae: 0.287573, mean_q: 3.844168
 63141/100000: episode: 817, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.000, mean reward: 1.950 [1.441, 4.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.256], loss: 0.096463, mae: 0.305073, mean_q: 3.849643
 63241/100000: episode: 818, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.143, mean reward: 1.941 [1.448, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.028, 10.335], loss: 0.096857, mae: 0.309619, mean_q: 3.858505
 63341/100000: episode: 819, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 191.599, mean reward: 1.916 [1.446, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.201, 10.138], loss: 0.096811, mae: 0.303727, mean_q: 3.861969
 63441/100000: episode: 820, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 208.453, mean reward: 2.085 [1.501, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.105], loss: 0.088381, mae: 0.290094, mean_q: 3.840220
 63541/100000: episode: 821, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 194.212, mean reward: 1.942 [1.433, 3.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.542, 10.193], loss: 0.080986, mae: 0.289249, mean_q: 3.831422
 63641/100000: episode: 822, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 227.018, mean reward: 2.270 [1.476, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.072, 10.098], loss: 0.099601, mae: 0.304474, mean_q: 3.876234
 63741/100000: episode: 823, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 192.802, mean reward: 1.928 [1.465, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.817, 10.098], loss: 0.086218, mae: 0.288825, mean_q: 3.859429
 63841/100000: episode: 824, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.855, mean reward: 1.889 [1.481, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.264, 10.098], loss: 0.079917, mae: 0.287617, mean_q: 3.863229
 63941/100000: episode: 825, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 214.517, mean reward: 2.145 [1.470, 5.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.399, 10.098], loss: 0.087121, mae: 0.288447, mean_q: 3.853421
 64041/100000: episode: 826, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 198.663, mean reward: 1.987 [1.491, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.294], loss: 0.084290, mae: 0.296276, mean_q: 3.886834
 64141/100000: episode: 827, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 226.620, mean reward: 2.266 [1.438, 5.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.404, 10.098], loss: 0.093150, mae: 0.298092, mean_q: 3.889213
 64241/100000: episode: 828, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 203.701, mean reward: 2.037 [1.509, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.118, 10.240], loss: 0.089988, mae: 0.301502, mean_q: 3.923976
 64341/100000: episode: 829, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 213.136, mean reward: 2.131 [1.452, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.240, 10.098], loss: 0.089079, mae: 0.295579, mean_q: 3.892440
 64441/100000: episode: 830, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 184.840, mean reward: 1.848 [1.460, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.120, 10.290], loss: 0.088889, mae: 0.298840, mean_q: 3.901375
 64541/100000: episode: 831, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 195.512, mean reward: 1.955 [1.464, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.357, 10.098], loss: 0.083998, mae: 0.293906, mean_q: 3.886039
 64641/100000: episode: 832, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 190.120, mean reward: 1.901 [1.447, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.591, 10.261], loss: 0.080992, mae: 0.290177, mean_q: 3.886937
 64741/100000: episode: 833, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.706, mean reward: 1.917 [1.469, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.139], loss: 0.077381, mae: 0.285648, mean_q: 3.879199
 64841/100000: episode: 834, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 185.789, mean reward: 1.858 [1.441, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.218, 10.234], loss: 0.093008, mae: 0.300867, mean_q: 3.871191
 64941/100000: episode: 835, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 179.326, mean reward: 1.793 [1.436, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.092, 10.207], loss: 0.089842, mae: 0.296010, mean_q: 3.877535
 65041/100000: episode: 836, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 189.831, mean reward: 1.898 [1.464, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.508, 10.098], loss: 0.072861, mae: 0.274175, mean_q: 3.853266
 65141/100000: episode: 837, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 205.151, mean reward: 2.052 [1.462, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.444, 10.150], loss: 0.083799, mae: 0.292661, mean_q: 3.884150
 65241/100000: episode: 838, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 180.904, mean reward: 1.809 [1.446, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.137], loss: 0.083481, mae: 0.284946, mean_q: 3.872346
 65341/100000: episode: 839, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.944, mean reward: 1.849 [1.497, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.328, 10.098], loss: 0.086654, mae: 0.295526, mean_q: 3.864952
 65441/100000: episode: 840, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 182.420, mean reward: 1.824 [1.441, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.614, 10.249], loss: 0.081119, mae: 0.285186, mean_q: 3.864778
 65541/100000: episode: 841, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 210.863, mean reward: 2.109 [1.430, 4.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.940, 10.344], loss: 0.082852, mae: 0.286102, mean_q: 3.856164
 65641/100000: episode: 842, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 207.736, mean reward: 2.077 [1.464, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.226, 10.098], loss: 0.077690, mae: 0.280350, mean_q: 3.861341
 65741/100000: episode: 843, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 206.983, mean reward: 2.070 [1.462, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.876, 10.098], loss: 0.085371, mae: 0.285688, mean_q: 3.875797
 65841/100000: episode: 844, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.471, mean reward: 1.865 [1.447, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.575, 10.098], loss: 0.087972, mae: 0.293741, mean_q: 3.880409
 65941/100000: episode: 845, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.425, mean reward: 1.874 [1.466, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.207, 10.098], loss: 0.089134, mae: 0.291612, mean_q: 3.873837
 66041/100000: episode: 846, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 178.591, mean reward: 1.786 [1.437, 3.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.384, 10.098], loss: 0.083744, mae: 0.287870, mean_q: 3.862964
 66141/100000: episode: 847, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 209.185, mean reward: 2.092 [1.453, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.236, 10.098], loss: 0.094489, mae: 0.302874, mean_q: 3.869759
 66241/100000: episode: 848, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.931, mean reward: 2.009 [1.437, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.592, 10.098], loss: 0.081885, mae: 0.287029, mean_q: 3.875839
 66341/100000: episode: 849, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.702, mean reward: 2.017 [1.443, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.566, 10.146], loss: 0.087744, mae: 0.291384, mean_q: 3.879494
 66441/100000: episode: 850, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 207.521, mean reward: 2.075 [1.466, 6.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.401, 10.098], loss: 0.082996, mae: 0.285200, mean_q: 3.885387
 66541/100000: episode: 851, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 250.583, mean reward: 2.506 [1.515, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.152, 10.394], loss: 0.088337, mae: 0.292474, mean_q: 3.887692
 66641/100000: episode: 852, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.446, mean reward: 1.894 [1.440, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.806, 10.127], loss: 0.084865, mae: 0.293558, mean_q: 3.897048
 66741/100000: episode: 853, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 209.136, mean reward: 2.091 [1.490, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: 0.092106, mae: 0.298085, mean_q: 3.897186
 66841/100000: episode: 854, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 183.809, mean reward: 1.838 [1.460, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.124], loss: 0.090641, mae: 0.292883, mean_q: 3.925289
 66941/100000: episode: 855, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 219.189, mean reward: 2.192 [1.461, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.098], loss: 0.081909, mae: 0.286886, mean_q: 3.917245
 67041/100000: episode: 856, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 207.368, mean reward: 2.074 [1.496, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.446, 10.098], loss: 0.096724, mae: 0.298428, mean_q: 3.936802
 67141/100000: episode: 857, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 212.867, mean reward: 2.129 [1.511, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.256, 10.431], loss: 0.093167, mae: 0.300358, mean_q: 3.941897
 67241/100000: episode: 858, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 215.820, mean reward: 2.158 [1.481, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.307, 10.098], loss: 0.094024, mae: 0.296764, mean_q: 3.938966
 67341/100000: episode: 859, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.664, mean reward: 1.897 [1.446, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.101, 10.293], loss: 0.089665, mae: 0.300536, mean_q: 3.950635
 67441/100000: episode: 860, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 181.926, mean reward: 1.819 [1.440, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.444, 10.098], loss: 0.100846, mae: 0.302578, mean_q: 3.918706
[Info] 1-TH LEVEL FOUND: 5.640432357788086, Considering 10/90 traces
 67541/100000: episode: 861, duration: 4.742s, episode steps: 100, steps per second: 21, episode reward: 187.825, mean reward: 1.878 [1.449, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.684, 10.098], loss: 0.080186, mae: 0.287802, mean_q: 3.928640
 67559/100000: episode: 862, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 47.100, mean reward: 2.617 [1.938, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.227, 10.100], loss: 0.079981, mae: 0.279564, mean_q: 3.918695
 67623/100000: episode: 863, duration: 0.340s, episode steps: 64, steps per second: 188, episode reward: 236.847, mean reward: 3.701 [2.398, 6.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.774 [-0.368, 10.100], loss: 0.099860, mae: 0.305615, mean_q: 3.958377
 67657/100000: episode: 864, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 84.372, mean reward: 2.482 [1.524, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.080, 10.228], loss: 0.101451, mae: 0.314476, mean_q: 3.960346
 67691/100000: episode: 865, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 83.342, mean reward: 2.451 [1.989, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.393], loss: 0.093253, mae: 0.303490, mean_q: 3.985503
 67725/100000: episode: 866, duration: 0.206s, episode steps: 34, steps per second: 165, episode reward: 71.779, mean reward: 2.111 [1.555, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.270, 10.177], loss: 0.101438, mae: 0.301912, mean_q: 3.937022
 67736/100000: episode: 867, duration: 0.065s, episode steps: 11, steps per second: 171, episode reward: 28.219, mean reward: 2.565 [1.789, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.407, 10.288], loss: 0.082534, mae: 0.278494, mean_q: 3.903067
 67770/100000: episode: 868, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 74.161, mean reward: 2.181 [1.439, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.665, 10.153], loss: 0.111195, mae: 0.321921, mean_q: 4.003536
 67804/100000: episode: 869, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 83.469, mean reward: 2.455 [1.501, 6.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.167, 10.225], loss: 0.130336, mae: 0.329656, mean_q: 3.990479
 67815/100000: episode: 870, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 29.163, mean reward: 2.651 [2.043, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.396], loss: 0.156531, mae: 0.360626, mean_q: 4.026710
 67845/100000: episode: 871, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 59.223, mean reward: 1.974 [1.509, 2.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.813, 10.209], loss: 0.128276, mae: 0.343894, mean_q: 4.050849
 67909/100000: episode: 872, duration: 0.344s, episode steps: 64, steps per second: 186, episode reward: 150.849, mean reward: 2.357 [1.589, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.784 [-0.450, 10.100], loss: 0.120487, mae: 0.336625, mean_q: 4.005936
 67927/100000: episode: 873, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 49.422, mean reward: 2.746 [2.356, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.282, 10.100], loss: 0.107300, mae: 0.321368, mean_q: 4.026445
 67968/100000: episode: 874, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 90.910, mean reward: 2.217 [1.502, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.264, 10.122], loss: 0.123380, mae: 0.332482, mean_q: 4.042524
 68032/100000: episode: 875, duration: 0.361s, episode steps: 64, steps per second: 177, episode reward: 176.174, mean reward: 2.753 [1.475, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.787 [-0.212, 10.211], loss: 0.089767, mae: 0.304822, mean_q: 4.012823
 68096/100000: episode: 876, duration: 0.354s, episode steps: 64, steps per second: 181, episode reward: 162.752, mean reward: 2.543 [1.492, 5.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-1.180, 10.184], loss: 0.096499, mae: 0.307173, mean_q: 4.035596
 68107/100000: episode: 877, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 36.615, mean reward: 3.329 [2.432, 5.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.707, 10.400], loss: 0.143842, mae: 0.349649, mean_q: 4.117562
 68141/100000: episode: 878, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 64.409, mean reward: 1.894 [1.473, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.193, 10.100], loss: 0.107642, mae: 0.333518, mean_q: 4.083891
 68175/100000: episode: 879, duration: 0.197s, episode steps: 34, steps per second: 172, episode reward: 78.960, mean reward: 2.322 [1.838, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.278, 10.435], loss: 0.132418, mae: 0.356862, mean_q: 4.071638
 68219/100000: episode: 880, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 102.027, mean reward: 2.319 [1.463, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.391, 10.143], loss: 0.116748, mae: 0.337414, mean_q: 4.074183
 68253/100000: episode: 881, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 89.235, mean reward: 2.625 [2.127, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.450, 10.445], loss: 0.123115, mae: 0.343714, mean_q: 4.089257
 68264/100000: episode: 882, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 29.216, mean reward: 2.656 [2.518, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.527, 10.100], loss: 0.108660, mae: 0.329962, mean_q: 4.098994
 68298/100000: episode: 883, duration: 0.188s, episode steps: 34, steps per second: 180, episode reward: 113.928, mean reward: 3.351 [1.826, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.345], loss: 0.104051, mae: 0.312683, mean_q: 4.077847
 68318/100000: episode: 884, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 52.550, mean reward: 2.628 [1.988, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.745, 10.325], loss: 0.103868, mae: 0.312213, mean_q: 4.043192
 68362/100000: episode: 885, duration: 0.230s, episode steps: 44, steps per second: 192, episode reward: 98.449, mean reward: 2.237 [1.536, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.366, 10.127], loss: 0.107616, mae: 0.324003, mean_q: 4.080942
 68373/100000: episode: 886, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 25.579, mean reward: 2.325 [1.953, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.167, 10.100], loss: 0.111642, mae: 0.322832, mean_q: 4.139058
 68391/100000: episode: 887, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 48.178, mean reward: 2.677 [2.111, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.754, 10.100], loss: 0.121850, mae: 0.311118, mean_q: 4.097391
 68421/100000: episode: 888, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 89.127, mean reward: 2.971 [2.123, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.210, 10.388], loss: 0.135789, mae: 0.361509, mean_q: 4.144172
 68455/100000: episode: 889, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 79.174, mean reward: 2.329 [1.605, 4.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.197, 10.234], loss: 0.135118, mae: 0.360629, mean_q: 4.102706
 68499/100000: episode: 890, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 104.460, mean reward: 2.374 [1.491, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.519, 10.143], loss: 0.116276, mae: 0.327481, mean_q: 4.118577
 68517/100000: episode: 891, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 64.304, mean reward: 3.572 [2.308, 6.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.903, 10.100], loss: 0.118755, mae: 0.345176, mean_q: 4.101824
 68537/100000: episode: 892, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 61.027, mean reward: 3.051 [2.400, 6.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.458, 10.319], loss: 0.106554, mae: 0.308539, mean_q: 4.138490
 68548/100000: episode: 893, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 32.148, mean reward: 2.923 [2.529, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.470], loss: 0.152483, mae: 0.365525, mean_q: 4.114382
 68582/100000: episode: 894, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 92.809, mean reward: 2.730 [2.172, 5.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.523, 10.395], loss: 0.127436, mae: 0.340139, mean_q: 4.148018
 68623/100000: episode: 895, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 111.879, mean reward: 2.729 [2.033, 5.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.369, 10.372], loss: 0.116118, mae: 0.326714, mean_q: 4.121775
 68664/100000: episode: 896, duration: 0.234s, episode steps: 41, steps per second: 175, episode reward: 108.783, mean reward: 2.653 [1.892, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.353, 10.374], loss: 0.144020, mae: 0.362885, mean_q: 4.214509
 68698/100000: episode: 897, duration: 0.188s, episode steps: 34, steps per second: 180, episode reward: 68.905, mean reward: 2.027 [1.497, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.468, 10.104], loss: 0.111473, mae: 0.343527, mean_q: 4.158629
 68716/100000: episode: 898, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 54.353, mean reward: 3.020 [2.239, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.069, 10.100], loss: 0.162645, mae: 0.384101, mean_q: 4.185161
 68734/100000: episode: 899, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 43.412, mean reward: 2.412 [1.679, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.326, 10.100], loss: 0.128762, mae: 0.360711, mean_q: 4.158599
 68768/100000: episode: 900, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 90.271, mean reward: 2.655 [2.069, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.430, 10.386], loss: 0.101841, mae: 0.325072, mean_q: 4.142376
 68798/100000: episode: 901, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 75.479, mean reward: 2.516 [1.985, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.717, 10.310], loss: 0.103190, mae: 0.327811, mean_q: 4.186568
 68828/100000: episode: 902, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 69.131, mean reward: 2.304 [1.653, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.311, 10.220], loss: 0.115812, mae: 0.331321, mean_q: 4.205706
 68846/100000: episode: 903, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 44.728, mean reward: 2.485 [2.023, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.372, 10.100], loss: 0.127781, mae: 0.349975, mean_q: 4.249590
 68876/100000: episode: 904, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 70.310, mean reward: 2.344 [1.766, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.652, 10.324], loss: 0.148037, mae: 0.365446, mean_q: 4.198411
 68910/100000: episode: 905, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 67.149, mean reward: 1.975 [1.481, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.117, 10.103], loss: 0.117044, mae: 0.332887, mean_q: 4.206339
 68930/100000: episode: 906, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 43.322, mean reward: 2.166 [1.709, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.440, 10.260], loss: 0.102003, mae: 0.321034, mean_q: 4.138468
 68950/100000: episode: 907, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 66.065, mean reward: 3.303 [2.100, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.126, 10.354], loss: 0.115711, mae: 0.327324, mean_q: 4.167621
 69014/100000: episode: 908, duration: 0.326s, episode steps: 64, steps per second: 196, episode reward: 155.736, mean reward: 2.433 [1.819, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.785 [-0.351, 10.100], loss: 0.115702, mae: 0.333051, mean_q: 4.206569
 69048/100000: episode: 909, duration: 0.217s, episode steps: 34, steps per second: 157, episode reward: 97.700, mean reward: 2.874 [1.981, 6.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.169, 10.409], loss: 0.140192, mae: 0.352433, mean_q: 4.202570
 69082/100000: episode: 910, duration: 0.201s, episode steps: 34, steps per second: 169, episode reward: 80.768, mean reward: 2.376 [1.725, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.339], loss: 0.104981, mae: 0.315336, mean_q: 4.245332
 69112/100000: episode: 911, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 93.599, mean reward: 3.120 [2.294, 6.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.254, 10.341], loss: 0.143704, mae: 0.364573, mean_q: 4.271384
 69123/100000: episode: 912, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 29.795, mean reward: 2.709 [2.134, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.332, 10.100], loss: 0.109276, mae: 0.341532, mean_q: 4.183414
 69167/100000: episode: 913, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 160.243, mean reward: 3.642 [2.002, 10.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.572, 10.355], loss: 0.135877, mae: 0.355609, mean_q: 4.286639
 69201/100000: episode: 914, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 78.586, mean reward: 2.311 [1.679, 4.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.035, 10.346], loss: 0.138977, mae: 0.357920, mean_q: 4.255756
 69231/100000: episode: 915, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 67.317, mean reward: 2.244 [1.682, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.438], loss: 0.143360, mae: 0.370648, mean_q: 4.305765
 69251/100000: episode: 916, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 66.561, mean reward: 3.328 [2.621, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.502], loss: 0.130360, mae: 0.357255, mean_q: 4.254323
 69315/100000: episode: 917, duration: 0.357s, episode steps: 64, steps per second: 179, episode reward: 154.448, mean reward: 2.413 [1.568, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.786 [-0.797, 10.100], loss: 0.146274, mae: 0.370766, mean_q: 4.304852
 69349/100000: episode: 918, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 76.755, mean reward: 2.258 [1.572, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.895, 10.245], loss: 0.128585, mae: 0.341636, mean_q: 4.285995
 69360/100000: episode: 919, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 28.250, mean reward: 2.568 [2.137, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.190, 10.100], loss: 0.134386, mae: 0.350181, mean_q: 4.277895
 69378/100000: episode: 920, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 64.824, mean reward: 3.601 [2.003, 7.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.496, 10.100], loss: 0.190269, mae: 0.402033, mean_q: 4.299398
 69412/100000: episode: 921, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 74.668, mean reward: 2.196 [1.649, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.941, 10.167], loss: 0.137673, mae: 0.365274, mean_q: 4.267553
 69423/100000: episode: 922, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 25.851, mean reward: 2.350 [1.906, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.234, 10.100], loss: 0.147572, mae: 0.376112, mean_q: 4.200472
 69464/100000: episode: 923, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 104.576, mean reward: 2.551 [1.655, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.282, 10.270], loss: 0.126691, mae: 0.348729, mean_q: 4.296284
 69475/100000: episode: 924, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 33.365, mean reward: 3.033 [2.370, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.284, 10.100], loss: 0.137126, mae: 0.348882, mean_q: 4.223154
 69493/100000: episode: 925, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 70.192, mean reward: 3.900 [2.370, 6.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.360, 10.100], loss: 0.169086, mae: 0.360629, mean_q: 4.352553
 69527/100000: episode: 926, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 88.859, mean reward: 2.614 [2.067, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.733, 10.329], loss: 0.225106, mae: 0.411704, mean_q: 4.390227
 69568/100000: episode: 927, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 85.274, mean reward: 2.080 [1.619, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.016, 10.268], loss: 0.149124, mae: 0.368311, mean_q: 4.362119
 69579/100000: episode: 928, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 29.322, mean reward: 2.666 [2.424, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.447, 10.370], loss: 0.204858, mae: 0.412713, mean_q: 4.375015
 69613/100000: episode: 929, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 83.567, mean reward: 2.458 [1.763, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.316, 10.444], loss: 0.122936, mae: 0.343728, mean_q: 4.353144
 69631/100000: episode: 930, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 47.324, mean reward: 2.629 [2.210, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.704, 10.100], loss: 0.140577, mae: 0.362023, mean_q: 4.314560
 69695/100000: episode: 931, duration: 0.333s, episode steps: 64, steps per second: 192, episode reward: 263.493, mean reward: 4.117 [1.592, 9.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.777 [-0.591, 10.100], loss: 0.184322, mae: 0.385944, mean_q: 4.371134
 69715/100000: episode: 932, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 52.464, mean reward: 2.623 [1.503, 6.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.774, 10.215], loss: 0.139192, mae: 0.362432, mean_q: 4.310928
 69756/100000: episode: 933, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 137.793, mean reward: 3.361 [2.607, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.234, 10.468], loss: 0.150234, mae: 0.378469, mean_q: 4.379245
 69790/100000: episode: 934, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 74.333, mean reward: 2.186 [1.604, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.232], loss: 0.148660, mae: 0.383086, mean_q: 4.483615
 69801/100000: episode: 935, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 34.788, mean reward: 3.163 [2.526, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.283, 10.100], loss: 0.200378, mae: 0.419300, mean_q: 4.424108
 69842/100000: episode: 936, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 132.179, mean reward: 3.224 [1.730, 5.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.102, 10.334], loss: 0.173249, mae: 0.401284, mean_q: 4.442430
 69860/100000: episode: 937, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 45.460, mean reward: 2.526 [2.126, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.303, 10.100], loss: 0.175359, mae: 0.392317, mean_q: 4.542788
 69871/100000: episode: 938, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 34.686, mean reward: 3.153 [2.738, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.460], loss: 0.155289, mae: 0.369053, mean_q: 4.410990
 69901/100000: episode: 939, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 115.005, mean reward: 3.834 [2.516, 5.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.740, 10.535], loss: 0.187008, mae: 0.401946, mean_q: 4.475717
 69942/100000: episode: 940, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 106.844, mean reward: 2.606 [1.996, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.641, 10.384], loss: 0.176802, mae: 0.397772, mean_q: 4.521077
 69953/100000: episode: 941, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 28.592, mean reward: 2.599 [2.211, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.838, 10.100], loss: 0.125540, mae: 0.373832, mean_q: 4.647799
 69987/100000: episode: 942, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 97.019, mean reward: 2.854 [1.998, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.322], loss: 0.193649, mae: 0.414363, mean_q: 4.573997
 70021/100000: episode: 943, duration: 0.169s, episode steps: 34, steps per second: 202, episode reward: 82.598, mean reward: 2.429 [1.451, 4.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.925, 10.124], loss: 0.193981, mae: 0.417119, mean_q: 4.645212
 70032/100000: episode: 944, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 32.738, mean reward: 2.976 [1.987, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.518, 10.422], loss: 0.172477, mae: 0.399750, mean_q: 4.552751
 70062/100000: episode: 945, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 70.794, mean reward: 2.360 [1.865, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.085, 10.419], loss: 0.175396, mae: 0.400569, mean_q: 4.565637
 70082/100000: episode: 946, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 67.756, mean reward: 3.388 [2.860, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.337, 10.480], loss: 0.183773, mae: 0.396244, mean_q: 4.604297
 70093/100000: episode: 947, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 27.772, mean reward: 2.525 [2.141, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.314, 10.100], loss: 0.271654, mae: 0.456302, mean_q: 4.518977
 70127/100000: episode: 948, duration: 0.205s, episode steps: 34, steps per second: 166, episode reward: 70.637, mean reward: 2.078 [1.588, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.806, 10.223], loss: 0.203234, mae: 0.412736, mean_q: 4.535293
 70145/100000: episode: 949, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 46.879, mean reward: 2.604 [1.812, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.294, 10.100], loss: 0.190877, mae: 0.417446, mean_q: 4.600209
 70156/100000: episode: 950, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 40.202, mean reward: 3.655 [2.752, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.529], loss: 0.188259, mae: 0.420645, mean_q: 4.632161
[Info] 2-TH LEVEL FOUND: 7.618673324584961, Considering 10/90 traces
 70197/100000: episode: 951, duration: 4.327s, episode steps: 41, steps per second: 9, episode reward: 97.638, mean reward: 2.381 [1.941, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.138, 10.333], loss: 0.198627, mae: 0.409800, mean_q: 4.613690
 70260/100000: episode: 952, duration: 0.334s, episode steps: 63, steps per second: 189, episode reward: 210.448, mean reward: 3.340 [2.364, 9.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-0.271, 10.100], loss: 0.221105, mae: 0.437885, mean_q: 4.648152
 70323/100000: episode: 953, duration: 0.329s, episode steps: 63, steps per second: 192, episode reward: 145.962, mean reward: 2.317 [1.535, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-0.600, 10.386], loss: 0.212636, mae: 0.421083, mean_q: 4.682501
 70386/100000: episode: 954, duration: 0.343s, episode steps: 63, steps per second: 184, episode reward: 144.833, mean reward: 2.299 [1.568, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.835, 10.304], loss: 0.224979, mae: 0.443225, mean_q: 4.676605
 70446/100000: episode: 955, duration: 0.305s, episode steps: 60, steps per second: 197, episode reward: 139.505, mean reward: 2.325 [1.529, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.730, 10.100], loss: 0.191413, mae: 0.418908, mean_q: 4.690155
 70471/100000: episode: 956, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 93.012, mean reward: 3.720 [3.056, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.791, 10.546], loss: 0.200551, mae: 0.432002, mean_q: 4.652633
 70495/100000: episode: 957, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 94.936, mean reward: 3.956 [2.419, 11.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.973, 10.390], loss: 0.192539, mae: 0.416338, mean_q: 4.687191
 70558/100000: episode: 958, duration: 0.341s, episode steps: 63, steps per second: 185, episode reward: 200.809, mean reward: 3.187 [2.073, 6.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.773 [-0.705, 10.100], loss: 0.234835, mae: 0.430398, mean_q: 4.708786
 70618/100000: episode: 959, duration: 0.306s, episode steps: 60, steps per second: 196, episode reward: 141.189, mean reward: 2.353 [1.469, 9.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.800, 10.121], loss: 0.217933, mae: 0.437731, mean_q: 4.792531
 70643/100000: episode: 960, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 68.297, mean reward: 2.732 [2.254, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.757, 10.369], loss: 0.273050, mae: 0.479270, mean_q: 4.755644
 70706/100000: episode: 961, duration: 0.348s, episode steps: 63, steps per second: 181, episode reward: 166.142, mean reward: 2.637 [1.675, 5.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.788 [-0.291, 10.100], loss: 0.205458, mae: 0.429919, mean_q: 4.727562
 70766/100000: episode: 962, duration: 0.321s, episode steps: 60, steps per second: 187, episode reward: 165.345, mean reward: 2.756 [1.938, 4.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.946, 10.100], loss: 0.195177, mae: 0.433238, mean_q: 4.743967
 70791/100000: episode: 963, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 80.637, mean reward: 3.225 [1.849, 5.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.344], loss: 0.212672, mae: 0.437174, mean_q: 4.793113
 70816/100000: episode: 964, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 74.562, mean reward: 2.982 [2.069, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.615, 10.396], loss: 0.216063, mae: 0.440711, mean_q: 4.845910
 70879/100000: episode: 965, duration: 0.337s, episode steps: 63, steps per second: 187, episode reward: 154.678, mean reward: 2.455 [1.440, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.035, 10.167], loss: 0.205471, mae: 0.421317, mean_q: 4.796721
 70939/100000: episode: 966, duration: 0.314s, episode steps: 60, steps per second: 191, episode reward: 161.600, mean reward: 2.693 [1.735, 7.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.811 [-0.517, 10.100], loss: 0.190178, mae: 0.430986, mean_q: 4.778936
 70968/100000: episode: 967, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 199.748, mean reward: 6.888 [3.522, 19.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.181, 10.617], loss: 0.162360, mae: 0.397700, mean_q: 4.853085
 71028/100000: episode: 968, duration: 0.312s, episode steps: 60, steps per second: 192, episode reward: 213.434, mean reward: 3.557 [1.976, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-0.269, 10.100], loss: 0.280975, mae: 0.467210, mean_q: 4.869498
 71088/100000: episode: 969, duration: 0.305s, episode steps: 60, steps per second: 197, episode reward: 117.418, mean reward: 1.957 [1.446, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.823 [-1.268, 10.200], loss: 0.265849, mae: 0.459844, mean_q: 4.886807
 71151/100000: episode: 970, duration: 0.319s, episode steps: 63, steps per second: 198, episode reward: 149.293, mean reward: 2.370 [1.513, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.644, 10.100], loss: 0.279544, mae: 0.471032, mean_q: 4.876431
 71214/100000: episode: 971, duration: 0.328s, episode steps: 63, steps per second: 192, episode reward: 161.606, mean reward: 2.565 [1.472, 14.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.855, 10.156], loss: 0.257842, mae: 0.465105, mean_q: 4.901561
 71277/100000: episode: 972, duration: 0.343s, episode steps: 63, steps per second: 184, episode reward: 251.454, mean reward: 3.991 [1.912, 7.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-0.687, 10.100], loss: 0.290163, mae: 0.487704, mean_q: 4.915462
 71339/100000: episode: 973, duration: 0.313s, episode steps: 62, steps per second: 198, episode reward: 193.351, mean reward: 3.119 [1.491, 8.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.669, 10.156], loss: 0.327344, mae: 0.473621, mean_q: 4.981663
 71363/100000: episode: 974, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 74.800, mean reward: 3.117 [2.490, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.857, 10.452], loss: 0.211693, mae: 0.471707, mean_q: 4.904659
 71387/100000: episode: 975, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 63.662, mean reward: 2.653 [1.638, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.374, 10.286], loss: 0.377787, mae: 0.542448, mean_q: 5.136277
 71412/100000: episode: 976, duration: 0.142s, episode steps: 25, steps per second: 175, episode reward: 76.155, mean reward: 3.046 [2.206, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.214, 10.355], loss: 0.241222, mae: 0.474999, mean_q: 4.996717
 71475/100000: episode: 977, duration: 0.334s, episode steps: 63, steps per second: 188, episode reward: 240.668, mean reward: 3.820 [1.837, 11.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-1.333, 10.100], loss: 0.324285, mae: 0.498898, mean_q: 5.028796
 71537/100000: episode: 978, duration: 0.317s, episode steps: 62, steps per second: 196, episode reward: 158.728, mean reward: 2.560 [1.646, 4.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.580, 10.100], loss: 0.361206, mae: 0.531038, mean_q: 5.077921
 71600/100000: episode: 979, duration: 0.335s, episode steps: 63, steps per second: 188, episode reward: 120.735, mean reward: 1.916 [1.453, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.793 [-0.910, 10.100], loss: 0.288128, mae: 0.495323, mean_q: 5.085582
 71660/100000: episode: 980, duration: 0.330s, episode steps: 60, steps per second: 182, episode reward: 142.340, mean reward: 2.372 [1.805, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-0.422, 10.100], loss: 0.310248, mae: 0.502245, mean_q: 5.036807
 71685/100000: episode: 981, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 79.986, mean reward: 3.199 [2.370, 4.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.452], loss: 0.297007, mae: 0.496274, mean_q: 5.039465
 71745/100000: episode: 982, duration: 0.325s, episode steps: 60, steps per second: 185, episode reward: 120.190, mean reward: 2.003 [1.451, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-0.778, 10.100], loss: 0.372323, mae: 0.528052, mean_q: 5.092952
 71808/100000: episode: 983, duration: 0.361s, episode steps: 63, steps per second: 174, episode reward: 155.125, mean reward: 2.462 [1.501, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.119, 10.157], loss: 0.339011, mae: 0.518910, mean_q: 5.084037
 71871/100000: episode: 984, duration: 0.311s, episode steps: 63, steps per second: 203, episode reward: 152.398, mean reward: 2.419 [1.595, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.145, 10.100], loss: 0.270782, mae: 0.486390, mean_q: 5.058541
 71931/100000: episode: 985, duration: 0.326s, episode steps: 60, steps per second: 184, episode reward: 137.979, mean reward: 2.300 [1.534, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.035, 10.268], loss: 0.357107, mae: 0.518707, mean_q: 5.127333
 71991/100000: episode: 986, duration: 0.314s, episode steps: 60, steps per second: 191, episode reward: 129.272, mean reward: 2.155 [1.452, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.823 [-0.035, 10.104], loss: 0.349497, mae: 0.541791, mean_q: 5.181336
 72016/100000: episode: 987, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 85.291, mean reward: 3.412 [2.098, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.350, 10.558], loss: 0.262483, mae: 0.500418, mean_q: 5.187860
 72079/100000: episode: 988, duration: 0.347s, episode steps: 63, steps per second: 182, episode reward: 130.340, mean reward: 2.069 [1.493, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.488, 10.120], loss: 0.345509, mae: 0.504582, mean_q: 5.101441
 72103/100000: episode: 989, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 104.541, mean reward: 4.356 [2.718, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.831, 10.558], loss: 0.222969, mae: 0.448011, mean_q: 5.091906
 72128/100000: episode: 990, duration: 0.155s, episode steps: 25, steps per second: 162, episode reward: 78.317, mean reward: 3.133 [2.071, 10.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.259, 10.280], loss: 0.317280, mae: 0.496210, mean_q: 5.138952
 72191/100000: episode: 991, duration: 0.340s, episode steps: 63, steps per second: 185, episode reward: 145.489, mean reward: 2.309 [1.546, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.864, 10.322], loss: 0.355396, mae: 0.532700, mean_q: 5.213222
 72220/100000: episode: 992, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 69.704, mean reward: 2.404 [1.500, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.070, 10.104], loss: 0.531784, mae: 0.561002, mean_q: 5.182935
 72283/100000: episode: 993, duration: 0.345s, episode steps: 63, steps per second: 183, episode reward: 152.003, mean reward: 2.413 [1.468, 5.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-1.193, 10.274], loss: 0.395441, mae: 0.561543, mean_q: 5.196176
 72346/100000: episode: 994, duration: 0.336s, episode steps: 63, steps per second: 187, episode reward: 156.666, mean reward: 2.487 [1.472, 6.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.373, 10.250], loss: 0.363821, mae: 0.526081, mean_q: 5.184413
 72409/100000: episode: 995, duration: 0.314s, episode steps: 63, steps per second: 200, episode reward: 148.174, mean reward: 2.352 [1.801, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.792 [-0.670, 10.100], loss: 0.459376, mae: 0.591597, mean_q: 5.234575
 72472/100000: episode: 996, duration: 0.334s, episode steps: 63, steps per second: 189, episode reward: 159.467, mean reward: 2.531 [1.758, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.795 [-1.074, 10.100], loss: 0.442117, mae: 0.539139, mean_q: 5.197729
 72532/100000: episode: 997, duration: 0.299s, episode steps: 60, steps per second: 201, episode reward: 176.907, mean reward: 2.948 [1.877, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-0.278, 10.100], loss: 0.415327, mae: 0.542027, mean_q: 5.220750
 72557/100000: episode: 998, duration: 0.160s, episode steps: 25, steps per second: 156, episode reward: 90.565, mean reward: 3.623 [1.990, 10.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.780, 10.363], loss: 0.363642, mae: 0.524925, mean_q: 5.158816
 72582/100000: episode: 999, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 75.943, mean reward: 3.038 [2.343, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.492], loss: 0.379976, mae: 0.519294, mean_q: 5.155844
 72645/100000: episode: 1000, duration: 0.345s, episode steps: 63, steps per second: 183, episode reward: 148.391, mean reward: 2.355 [1.550, 6.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.120, 10.100], loss: 0.388626, mae: 0.519282, mean_q: 5.255443
 72670/100000: episode: 1001, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 123.938, mean reward: 4.958 [3.195, 10.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.611], loss: 0.227613, mae: 0.471255, mean_q: 5.137953
 72730/100000: episode: 1002, duration: 0.347s, episode steps: 60, steps per second: 173, episode reward: 125.365, mean reward: 2.089 [1.509, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.819 [-1.302, 10.100], loss: 0.338934, mae: 0.522977, mean_q: 5.309677
 72790/100000: episode: 1003, duration: 0.334s, episode steps: 60, steps per second: 180, episode reward: 130.139, mean reward: 2.169 [1.437, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-2.358, 10.298], loss: 0.374950, mae: 0.524724, mean_q: 5.296196
 72853/100000: episode: 1004, duration: 0.353s, episode steps: 63, steps per second: 179, episode reward: 142.716, mean reward: 2.265 [1.465, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.873, 10.141], loss: 0.334180, mae: 0.508379, mean_q: 5.182432
 72916/100000: episode: 1005, duration: 0.318s, episode steps: 63, steps per second: 198, episode reward: 162.295, mean reward: 2.576 [1.572, 9.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-1.744, 10.100], loss: 0.412681, mae: 0.530769, mean_q: 5.171248
 72945/100000: episode: 1006, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 146.318, mean reward: 5.045 [2.552, 9.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.155, 10.458], loss: 0.361411, mae: 0.534925, mean_q: 5.300590
 73008/100000: episode: 1007, duration: 0.321s, episode steps: 63, steps per second: 196, episode reward: 135.206, mean reward: 2.146 [1.439, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.796 [-0.702, 10.100], loss: 0.366902, mae: 0.526490, mean_q: 5.178249
 73068/100000: episode: 1008, duration: 0.318s, episode steps: 60, steps per second: 189, episode reward: 150.353, mean reward: 2.506 [1.754, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.814 [-0.508, 10.100], loss: 0.364368, mae: 0.518562, mean_q: 5.198552
 73093/100000: episode: 1009, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 151.958, mean reward: 6.078 [3.157, 16.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.808, 10.641], loss: 0.273450, mae: 0.517532, mean_q: 5.186306
 73153/100000: episode: 1010, duration: 0.340s, episode steps: 60, steps per second: 176, episode reward: 136.504, mean reward: 2.275 [1.451, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.827 [-0.650, 10.110], loss: 0.396906, mae: 0.529235, mean_q: 5.278443
 73182/100000: episode: 1011, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 88.672, mean reward: 3.058 [2.281, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.377], loss: 0.561969, mae: 0.577416, mean_q: 5.259022
 73206/100000: episode: 1012, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 125.120, mean reward: 5.213 [2.955, 8.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.506], loss: 0.633054, mae: 0.642249, mean_q: 5.414028
 73269/100000: episode: 1013, duration: 0.346s, episode steps: 63, steps per second: 182, episode reward: 131.571, mean reward: 2.088 [1.523, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.814 [-0.109, 10.293], loss: 0.318267, mae: 0.510624, mean_q: 5.257412
 73332/100000: episode: 1014, duration: 0.346s, episode steps: 63, steps per second: 182, episode reward: 139.291, mean reward: 2.211 [1.521, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-1.065, 10.207], loss: 0.356902, mae: 0.554995, mean_q: 5.321973
 73395/100000: episode: 1015, duration: 0.373s, episode steps: 63, steps per second: 169, episode reward: 126.353, mean reward: 2.006 [1.446, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.355, 10.312], loss: 0.476174, mae: 0.586217, mean_q: 5.350065
 73419/100000: episode: 1016, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 72.763, mean reward: 3.032 [2.469, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.230, 10.453], loss: 0.302488, mae: 0.479329, mean_q: 5.173098
 73479/100000: episode: 1017, duration: 0.311s, episode steps: 60, steps per second: 193, episode reward: 146.612, mean reward: 2.444 [1.696, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.812 [-0.861, 10.100], loss: 0.523235, mae: 0.577592, mean_q: 5.341723
 73504/100000: episode: 1018, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 75.654, mean reward: 3.026 [1.706, 6.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.249], loss: 0.350249, mae: 0.550311, mean_q: 5.288889
 73528/100000: episode: 1019, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 80.797, mean reward: 3.367 [2.138, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.092, 10.549], loss: 0.380917, mae: 0.537647, mean_q: 5.255052
 73591/100000: episode: 1020, duration: 0.330s, episode steps: 63, steps per second: 191, episode reward: 151.905, mean reward: 2.411 [1.541, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.791 [-0.950, 10.100], loss: 0.348100, mae: 0.538775, mean_q: 5.274180
 73616/100000: episode: 1021, duration: 0.161s, episode steps: 25, steps per second: 155, episode reward: 189.129, mean reward: 7.565 [3.740, 62.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.296, 10.543], loss: 0.420783, mae: 0.530447, mean_q: 5.306927
 73679/100000: episode: 1022, duration: 0.321s, episode steps: 63, steps per second: 196, episode reward: 150.873, mean reward: 2.395 [1.482, 7.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.204, 10.314], loss: 0.333137, mae: 0.507533, mean_q: 5.305445
 73703/100000: episode: 1023, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 63.336, mean reward: 2.639 [1.752, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.137, 10.348], loss: 0.451621, mae: 0.615867, mean_q: 5.276750
 73763/100000: episode: 1024, duration: 0.341s, episode steps: 60, steps per second: 176, episode reward: 169.698, mean reward: 2.828 [1.823, 5.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.819 [-0.552, 10.100], loss: 1.341362, mae: 0.599344, mean_q: 5.260392
 73823/100000: episode: 1025, duration: 0.302s, episode steps: 60, steps per second: 198, episode reward: 136.666, mean reward: 2.278 [1.525, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-0.177, 10.100], loss: 1.247272, mae: 0.586764, mean_q: 5.240591
 73848/100000: episode: 1026, duration: 0.159s, episode steps: 25, steps per second: 157, episode reward: 79.618, mean reward: 3.185 [2.348, 6.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.730, 10.502], loss: 4.867617, mae: 0.780265, mean_q: 5.385814
 73908/100000: episode: 1027, duration: 0.310s, episode steps: 60, steps per second: 194, episode reward: 160.646, mean reward: 2.677 [1.557, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.817 [-0.392, 10.100], loss: 1.437693, mae: 0.722290, mean_q: 5.352895
 73937/100000: episode: 1028, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 87.353, mean reward: 3.012 [2.031, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.117, 10.254], loss: 2.311661, mae: 0.641182, mean_q: 5.334898
 73962/100000: episode: 1029, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 74.546, mean reward: 2.982 [2.312, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.464], loss: 0.531124, mae: 0.624847, mean_q: 5.412517
 74024/100000: episode: 1030, duration: 0.334s, episode steps: 62, steps per second: 186, episode reward: 149.046, mean reward: 2.404 [1.455, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.815 [-0.098, 10.100], loss: 0.457429, mae: 0.574923, mean_q: 5.327642
 74087/100000: episode: 1031, duration: 0.341s, episode steps: 63, steps per second: 185, episode reward: 125.954, mean reward: 1.999 [1.444, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-0.832, 10.120], loss: 0.357694, mae: 0.518093, mean_q: 5.324157
 74147/100000: episode: 1032, duration: 0.348s, episode steps: 60, steps per second: 172, episode reward: 138.394, mean reward: 2.307 [1.498, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.753, 10.142], loss: 0.407913, mae: 0.550267, mean_q: 5.313362
 74171/100000: episode: 1033, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 67.747, mean reward: 2.823 [2.262, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.408], loss: 0.363090, mae: 0.516772, mean_q: 5.270919
 74234/100000: episode: 1034, duration: 0.317s, episode steps: 63, steps per second: 199, episode reward: 157.852, mean reward: 2.506 [1.521, 6.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.810 [-1.597, 10.291], loss: 0.338410, mae: 0.505905, mean_q: 5.300746
 74297/100000: episode: 1035, duration: 0.337s, episode steps: 63, steps per second: 187, episode reward: 171.499, mean reward: 2.722 [1.779, 6.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.787 [-1.225, 10.100], loss: 0.426052, mae: 0.546135, mean_q: 5.363065
 74360/100000: episode: 1036, duration: 0.327s, episode steps: 63, steps per second: 193, episode reward: 144.824, mean reward: 2.299 [1.447, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.803 [-0.560, 10.105], loss: 0.379226, mae: 0.519831, mean_q: 5.242682
 74385/100000: episode: 1037, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 78.115, mean reward: 3.125 [1.871, 4.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.257], loss: 0.310573, mae: 0.547226, mean_q: 5.294959
 74447/100000: episode: 1038, duration: 0.334s, episode steps: 62, steps per second: 186, episode reward: 156.701, mean reward: 2.527 [1.472, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-0.553, 10.164], loss: 1.321099, mae: 0.618027, mean_q: 5.375895
 74507/100000: episode: 1039, duration: 0.329s, episode steps: 60, steps per second: 182, episode reward: 132.597, mean reward: 2.210 [1.505, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.776, 10.255], loss: 0.359043, mae: 0.534460, mean_q: 5.277545
 74532/100000: episode: 1040, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 68.591, mean reward: 2.744 [1.709, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.864, 10.226], loss: 2.637331, mae: 0.665964, mean_q: 5.233797
[Info] 3-TH LEVEL FOUND: 9.224786758422852, Considering 10/90 traces
 74592/100000: episode: 1041, duration: 4.410s, episode steps: 60, steps per second: 14, episode reward: 130.364, mean reward: 2.173 [1.669, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-1.877, 10.100], loss: 0.497977, mae: 0.570650, mean_q: 5.213918
 74613/100000: episode: 1042, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 116.603, mean reward: 5.553 [3.622, 9.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.532], loss: 0.257912, mae: 0.494617, mean_q: 5.205882
 74632/100000: episode: 1043, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 205.268, mean reward: 10.804 [4.011, 75.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.199, 10.568], loss: 0.458529, mae: 0.579044, mean_q: 5.397894
 74647/100000: episode: 1044, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 55.827, mean reward: 3.722 [2.597, 8.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.388], loss: 0.270117, mae: 0.472971, mean_q: 5.158740
 74666/100000: episode: 1045, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 59.366, mean reward: 3.125 [2.208, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.230, 10.482], loss: 0.509773, mae: 0.600835, mean_q: 5.546463
 74679/100000: episode: 1046, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 69.625, mean reward: 5.356 [3.785, 7.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.646], loss: 0.598534, mae: 0.592167, mean_q: 5.325754
 74693/100000: episode: 1047, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 55.689, mean reward: 3.978 [2.995, 6.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.197, 10.421], loss: 0.317122, mae: 0.527333, mean_q: 5.397855
 74711/100000: episode: 1048, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 80.068, mean reward: 4.448 [3.003, 6.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.050, 10.468], loss: 0.434528, mae: 0.523159, mean_q: 5.225631
 74724/100000: episode: 1049, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 58.751, mean reward: 4.519 [3.682, 6.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.533], loss: 0.644560, mae: 0.553187, mean_q: 5.342463
 74727/100000: episode: 1050, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 16.772, mean reward: 5.591 [4.370, 6.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.622], loss: 0.247266, mae: 0.478718, mean_q: 5.185957
 74745/100000: episode: 1051, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 67.055, mean reward: 3.725 [3.151, 5.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.971, 10.533], loss: 4.894106, mae: 0.822317, mean_q: 5.491906
 74763/100000: episode: 1052, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 53.223, mean reward: 2.957 [2.285, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.172, 10.420], loss: 0.342160, mae: 0.564836, mean_q: 5.079429
 74766/100000: episode: 1053, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 12.962, mean reward: 4.321 [3.929, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.473], loss: 0.651574, mae: 0.645619, mean_q: 5.596062
 74784/100000: episode: 1054, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 94.118, mean reward: 5.229 [3.272, 8.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.187, 10.451], loss: 0.480560, mae: 0.572591, mean_q: 5.378037
 74832/100000: episode: 1055, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 167.585, mean reward: 3.491 [1.974, 6.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-1.063, 10.100], loss: 0.365858, mae: 0.516461, mean_q: 5.332605
 74845/100000: episode: 1056, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 57.187, mean reward: 4.399 [3.010, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.211, 10.535], loss: 0.506690, mae: 0.565904, mean_q: 5.381286
 74848/100000: episode: 1057, duration: 0.026s, episode steps: 3, steps per second: 114, episode reward: 14.505, mean reward: 4.835 [4.000, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-1.347, 10.612], loss: 0.127770, mae: 0.369561, mean_q: 5.281490
 74869/100000: episode: 1058, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 75.430, mean reward: 3.592 [2.216, 5.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.165, 10.428], loss: 0.456269, mae: 0.553874, mean_q: 5.277172
 74884/100000: episode: 1059, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 115.662, mean reward: 7.711 [5.387, 11.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.173, 10.610], loss: 0.899674, mae: 0.710988, mean_q: 5.455592
 74897/100000: episode: 1060, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 77.163, mean reward: 5.936 [3.562, 11.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.382, 10.706], loss: 0.465267, mae: 0.642044, mean_q: 5.521293
 74900/100000: episode: 1061, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 13.816, mean reward: 4.605 [4.160, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.061, 10.579], loss: 0.361560, mae: 0.500399, mean_q: 5.513186
 74914/100000: episode: 1062, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 45.051, mean reward: 3.218 [2.513, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.966, 10.394], loss: 0.392328, mae: 0.572079, mean_q: 5.509708
 74934/100000: episode: 1063, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 69.332, mean reward: 3.467 [2.387, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.452], loss: 0.387291, mae: 0.543779, mean_q: 5.296732
 74954/100000: episode: 1064, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 114.259, mean reward: 5.713 [3.771, 12.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.715], loss: 0.574488, mae: 0.613271, mean_q: 5.424409
 75002/100000: episode: 1065, duration: 0.277s, episode steps: 48, steps per second: 173, episode reward: 155.730, mean reward: 3.244 [1.511, 9.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.732, 10.100], loss: 0.496759, mae: 0.604003, mean_q: 5.491974
 75022/100000: episode: 1066, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 69.139, mean reward: 3.457 [2.705, 4.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.074, 10.560], loss: 0.468666, mae: 0.603689, mean_q: 5.495121
 75041/100000: episode: 1067, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 132.967, mean reward: 6.998 [3.614, 12.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.128, 10.562], loss: 3.551424, mae: 0.887084, mean_q: 5.591208
 75059/100000: episode: 1068, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 89.622, mean reward: 4.979 [3.505, 7.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.563], loss: 0.641116, mae: 0.733658, mean_q: 5.445264
 75072/100000: episode: 1069, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 57.875, mean reward: 4.452 [2.887, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.057, 10.477], loss: 0.550533, mae: 0.655279, mean_q: 5.459455
 75091/100000: episode: 1070, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 88.288, mean reward: 4.647 [3.407, 6.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.692, 10.571], loss: 0.400821, mae: 0.559016, mean_q: 5.534956
 75109/100000: episode: 1071, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 64.983, mean reward: 3.610 [3.071, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.122, 10.545], loss: 0.618484, mae: 0.655859, mean_q: 5.694038
 75157/100000: episode: 1072, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 149.327, mean reward: 3.111 [1.542, 10.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.339, 10.100], loss: 2.312139, mae: 0.777474, mean_q: 5.555627
 75171/100000: episode: 1073, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 75.120, mean reward: 5.366 [3.402, 10.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.531], loss: 4.560214, mae: 0.938749, mean_q: 5.829597
 75174/100000: episode: 1074, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 13.036, mean reward: 4.345 [3.798, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.567], loss: 0.419276, mae: 0.646713, mean_q: 5.741905
 75195/100000: episode: 1075, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 144.567, mean reward: 6.884 [3.384, 21.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.054, 10.591], loss: 0.655262, mae: 0.700902, mean_q: 5.629197
 75214/100000: episode: 1076, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 180.576, mean reward: 9.504 [2.361, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.703, 10.365], loss: 5.009274, mae: 0.948128, mean_q: 5.826049
 75217/100000: episode: 1077, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 12.451, mean reward: 4.150 [3.899, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.561], loss: 47.949738, mae: 1.695107, mean_q: 5.899446
 75237/100000: episode: 1078, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 109.919, mean reward: 5.496 [3.473, 11.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.206, 10.557], loss: 12.125339, mae: 1.378892, mean_q: 5.816089
 75256/100000: episode: 1079, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 148.391, mean reward: 7.810 [3.104, 35.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.496, 10.447], loss: 0.811280, mae: 0.821809, mean_q: 5.607500
 75274/100000: episode: 1080, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 74.292, mean reward: 4.127 [2.407, 6.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.837, 10.543], loss: 0.530388, mae: 0.700635, mean_q: 5.399195
 75295/100000: episode: 1081, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 83.110, mean reward: 3.958 [2.679, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.121, 10.533], loss: 0.759721, mae: 0.770952, mean_q: 5.715631
 75315/100000: episode: 1082, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 83.696, mean reward: 4.185 [2.392, 8.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.458, 10.408], loss: 7.742215, mae: 0.910570, mean_q: 5.798696
 75318/100000: episode: 1083, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 13.951, mean reward: 4.650 [4.127, 5.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.590], loss: 0.536246, mae: 0.735577, mean_q: 5.872496
 75333/100000: episode: 1084, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 72.670, mean reward: 4.845 [3.852, 6.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.536, 10.560], loss: 0.512290, mae: 0.677113, mean_q: 5.468493
 75347/100000: episode: 1085, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 77.001, mean reward: 5.500 [3.475, 10.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.975, 10.527], loss: 1.586641, mae: 0.751515, mean_q: 5.986392
 75350/100000: episode: 1086, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 11.506, mean reward: 3.835 [3.594, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.481], loss: 0.568607, mae: 0.653307, mean_q: 5.931790
 75368/100000: episode: 1087, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 64.788, mean reward: 3.599 [2.176, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.576, 10.441], loss: 7.437063, mae: 1.033916, mean_q: 6.188848
[Info] FALSIFICATION!
[Info] Levels: [5.6404324, 7.6186733, 9.224787, 11.278678]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.09]
[Info] Error Prob: 9.000000000000002e-05

 75370/100000: episode: 1088, duration: 4.326s, episode steps: 2, steps per second: 0, episode reward: 110.087, mean reward: 55.044 [10.087, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.014, 10.493], loss: 1.479527, mae: 0.898092, mean_q: 5.789707
 75470/100000: episode: 1089, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 195.428, mean reward: 1.954 [1.447, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.544, 10.098], loss: 2.244729, mae: 0.769319, mean_q: 5.777899
 75570/100000: episode: 1090, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 196.209, mean reward: 1.962 [1.501, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.026, 10.128], loss: 3.955808, mae: 0.786533, mean_q: 5.788945
 75670/100000: episode: 1091, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 206.801, mean reward: 2.068 [1.439, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.225, 10.208], loss: 2.043511, mae: 0.697940, mean_q: 5.655303
 75770/100000: episode: 1092, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.149, mean reward: 1.971 [1.475, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.771, 10.099], loss: 3.314126, mae: 0.787615, mean_q: 5.711052
 75870/100000: episode: 1093, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 206.244, mean reward: 2.062 [1.516, 7.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.378, 10.098], loss: 7.679672, mae: 0.962476, mean_q: 5.786775
 75970/100000: episode: 1094, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.822, mean reward: 1.948 [1.463, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.063, 10.098], loss: 2.656371, mae: 0.726879, mean_q: 5.613811
 76070/100000: episode: 1095, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 216.693, mean reward: 2.167 [1.515, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.782, 10.098], loss: 3.414448, mae: 0.732565, mean_q: 5.668711
 76170/100000: episode: 1096, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 206.228, mean reward: 2.062 [1.468, 5.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.741, 10.198], loss: 3.091833, mae: 0.768278, mean_q: 5.617414
 76270/100000: episode: 1097, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 195.065, mean reward: 1.951 [1.455, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.671, 10.098], loss: 3.232992, mae: 0.739286, mean_q: 5.477925
 76370/100000: episode: 1098, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 203.509, mean reward: 2.035 [1.468, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.260, 10.098], loss: 0.362820, mae: 0.517430, mean_q: 5.395822
 76470/100000: episode: 1099, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 229.370, mean reward: 2.294 [1.503, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.370, 10.375], loss: 1.393038, mae: 0.635858, mean_q: 5.442503
 76570/100000: episode: 1100, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 180.873, mean reward: 1.809 [1.464, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.081, 10.098], loss: 2.864549, mae: 0.688165, mean_q: 5.475925
 76670/100000: episode: 1101, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 201.366, mean reward: 2.014 [1.450, 5.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.425, 10.098], loss: 4.917812, mae: 0.775257, mean_q: 5.483393
 76770/100000: episode: 1102, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 200.936, mean reward: 2.009 [1.524, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.548, 10.306], loss: 4.612207, mae: 0.751808, mean_q: 5.435690
 76870/100000: episode: 1103, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.496, mean reward: 1.925 [1.465, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.306], loss: 4.866619, mae: 0.764544, mean_q: 5.525705
 76970/100000: episode: 1104, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 204.716, mean reward: 2.047 [1.459, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.708, 10.098], loss: 4.384494, mae: 0.696111, mean_q: 5.447185
 77070/100000: episode: 1105, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 180.493, mean reward: 1.805 [1.474, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.926, 10.252], loss: 3.543001, mae: 0.650022, mean_q: 5.336747
 77170/100000: episode: 1106, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.583, mean reward: 1.916 [1.431, 5.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.173], loss: 6.039209, mae: 0.808761, mean_q: 5.422204
 77270/100000: episode: 1107, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 202.364, mean reward: 2.024 [1.521, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.289, 10.098], loss: 1.710303, mae: 0.611618, mean_q: 5.236484
 77370/100000: episode: 1108, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 208.053, mean reward: 2.081 [1.550, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.577, 10.098], loss: 0.935756, mae: 0.524816, mean_q: 5.186591
 77470/100000: episode: 1109, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.009, mean reward: 1.850 [1.457, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.408, 10.098], loss: 3.294692, mae: 0.656314, mean_q: 5.311419
 77570/100000: episode: 1110, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.350, mean reward: 1.854 [1.471, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.255, 10.098], loss: 1.866705, mae: 0.559742, mean_q: 5.088740
 77670/100000: episode: 1111, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 182.607, mean reward: 1.826 [1.456, 2.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.239], loss: 3.426066, mae: 0.702770, mean_q: 5.209691
 77770/100000: episode: 1112, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.849, mean reward: 1.898 [1.430, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.138, 10.098], loss: 7.892289, mae: 0.839932, mean_q: 5.356761
 77870/100000: episode: 1113, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 218.629, mean reward: 2.186 [1.479, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.129], loss: 4.216886, mae: 0.684716, mean_q: 5.177677
 77970/100000: episode: 1114, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.112, mean reward: 1.841 [1.458, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.280], loss: 2.486545, mae: 0.566167, mean_q: 5.044472
 78070/100000: episode: 1115, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.635, mean reward: 1.866 [1.492, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.660, 10.114], loss: 4.458399, mae: 0.651023, mean_q: 5.083982
 78170/100000: episode: 1116, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 186.601, mean reward: 1.866 [1.455, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.717, 10.098], loss: 1.675108, mae: 0.573047, mean_q: 4.966966
 78270/100000: episode: 1117, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 180.455, mean reward: 1.805 [1.471, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.098], loss: 7.469290, mae: 0.757656, mean_q: 5.158815
 78370/100000: episode: 1118, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.886, mean reward: 1.909 [1.491, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.229], loss: 2.583346, mae: 0.602660, mean_q: 4.996025
 78470/100000: episode: 1119, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.629, mean reward: 1.906 [1.469, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.563, 10.244], loss: 3.293191, mae: 0.660071, mean_q: 5.006674
 78570/100000: episode: 1120, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 187.837, mean reward: 1.878 [1.458, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.148, 10.098], loss: 0.624406, mae: 0.497713, mean_q: 4.904494
 78670/100000: episode: 1121, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 217.563, mean reward: 2.176 [1.492, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.639, 10.285], loss: 0.536857, mae: 0.491558, mean_q: 4.778834
 78770/100000: episode: 1122, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 200.826, mean reward: 2.008 [1.463, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.675, 10.292], loss: 4.942381, mae: 0.631796, mean_q: 4.906151
 78870/100000: episode: 1123, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 185.850, mean reward: 1.859 [1.471, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.045, 10.350], loss: 7.324504, mae: 0.753473, mean_q: 4.969629
 78970/100000: episode: 1124, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 196.264, mean reward: 1.963 [1.474, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.810, 10.149], loss: 2.350260, mae: 0.559503, mean_q: 4.809447
 79070/100000: episode: 1125, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 201.625, mean reward: 2.016 [1.530, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.661, 10.098], loss: 2.720447, mae: 0.584399, mean_q: 4.803542
 79170/100000: episode: 1126, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 195.097, mean reward: 1.951 [1.497, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.380, 10.098], loss: 1.720883, mae: 0.497626, mean_q: 4.754379
 79270/100000: episode: 1127, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 176.465, mean reward: 1.765 [1.470, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.121], loss: 1.916314, mae: 0.527017, mean_q: 4.712132
 79370/100000: episode: 1128, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 204.261, mean reward: 2.043 [1.481, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.508, 10.098], loss: 6.505751, mae: 0.640529, mean_q: 4.773421
 79470/100000: episode: 1129, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.523, mean reward: 1.935 [1.470, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.185, 10.172], loss: 3.184713, mae: 0.598911, mean_q: 4.781332
 79570/100000: episode: 1130, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 180.784, mean reward: 1.808 [1.467, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.197], loss: 2.973930, mae: 0.540355, mean_q: 4.707748
 79670/100000: episode: 1131, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 188.858, mean reward: 1.889 [1.434, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.953, 10.311], loss: 1.888896, mae: 0.500103, mean_q: 4.579323
 79770/100000: episode: 1132, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 177.194, mean reward: 1.772 [1.440, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.156], loss: 1.862523, mae: 0.496534, mean_q: 4.501190
 79870/100000: episode: 1133, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.015, mean reward: 1.930 [1.458, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.346], loss: 2.725962, mae: 0.503155, mean_q: 4.467203
 79970/100000: episode: 1134, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.137, mean reward: 1.911 [1.439, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.103, 10.098], loss: 4.123503, mae: 0.505939, mean_q: 4.316832
 80070/100000: episode: 1135, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.173, mean reward: 1.902 [1.471, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.666, 10.098], loss: 1.948100, mae: 0.566617, mean_q: 4.139054
 80170/100000: episode: 1136, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 212.049, mean reward: 2.120 [1.463, 5.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.659, 10.403], loss: 0.348724, mae: 0.365214, mean_q: 4.087204
 80270/100000: episode: 1137, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 211.908, mean reward: 2.119 [1.471, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.062, 10.098], loss: 1.375383, mae: 0.345637, mean_q: 3.973901
 80370/100000: episode: 1138, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.741, mean reward: 1.977 [1.433, 7.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.103, 10.395], loss: 0.100355, mae: 0.295872, mean_q: 3.877012
 80470/100000: episode: 1139, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.422, mean reward: 1.954 [1.475, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.303, 10.098], loss: 0.088008, mae: 0.286838, mean_q: 3.879421
 80570/100000: episode: 1140, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 181.868, mean reward: 1.819 [1.496, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.839, 10.098], loss: 0.089232, mae: 0.289292, mean_q: 3.857733
 80670/100000: episode: 1141, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 191.404, mean reward: 1.914 [1.434, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.543, 10.098], loss: 0.117402, mae: 0.294661, mean_q: 3.870729
 80770/100000: episode: 1142, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 202.628, mean reward: 2.026 [1.448, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.098], loss: 0.105179, mae: 0.298731, mean_q: 3.865179
 80870/100000: episode: 1143, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.760, mean reward: 1.998 [1.502, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.053, 10.212], loss: 0.087176, mae: 0.287898, mean_q: 3.869965
 80970/100000: episode: 1144, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 188.442, mean reward: 1.884 [1.485, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.142, 10.098], loss: 0.085265, mae: 0.279575, mean_q: 3.877439
 81070/100000: episode: 1145, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 195.853, mean reward: 1.959 [1.503, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.577, 10.098], loss: 0.083843, mae: 0.278740, mean_q: 3.852118
 81170/100000: episode: 1146, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 184.820, mean reward: 1.848 [1.462, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.111], loss: 0.086763, mae: 0.277582, mean_q: 3.834331
 81270/100000: episode: 1147, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.780, mean reward: 1.948 [1.450, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.383, 10.098], loss: 0.096376, mae: 0.283922, mean_q: 3.859791
 81370/100000: episode: 1148, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 181.012, mean reward: 1.810 [1.437, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.680, 10.098], loss: 0.091960, mae: 0.281008, mean_q: 3.852662
 81470/100000: episode: 1149, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.991, mean reward: 2.010 [1.442, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.140, 10.458], loss: 0.089323, mae: 0.273299, mean_q: 3.830050
 81570/100000: episode: 1150, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 203.373, mean reward: 2.034 [1.475, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.674, 10.155], loss: 0.081599, mae: 0.269445, mean_q: 3.812461
 81670/100000: episode: 1151, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 179.657, mean reward: 1.797 [1.485, 2.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.248], loss: 0.072603, mae: 0.267008, mean_q: 3.828672
 81770/100000: episode: 1152, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.976, mean reward: 1.980 [1.487, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.885, 10.180], loss: 0.086800, mae: 0.274432, mean_q: 3.803801
 81870/100000: episode: 1153, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 220.771, mean reward: 2.208 [1.451, 5.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.555, 10.318], loss: 0.081123, mae: 0.268412, mean_q: 3.798584
 81970/100000: episode: 1154, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.701, mean reward: 1.847 [1.464, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.499, 10.098], loss: 0.083455, mae: 0.282880, mean_q: 3.839452
 82070/100000: episode: 1155, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.685, mean reward: 2.067 [1.454, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.702, 10.098], loss: 0.104639, mae: 0.288810, mean_q: 3.832230
 82170/100000: episode: 1156, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 213.081, mean reward: 2.131 [1.479, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.340, 10.098], loss: 0.088019, mae: 0.278651, mean_q: 3.829780
 82270/100000: episode: 1157, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.463, mean reward: 2.035 [1.442, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.271, 10.209], loss: 0.083436, mae: 0.282276, mean_q: 3.842520
 82370/100000: episode: 1158, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.349, mean reward: 1.933 [1.469, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.169], loss: 0.101976, mae: 0.292056, mean_q: 3.837621
 82470/100000: episode: 1159, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 193.340, mean reward: 1.933 [1.483, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.470, 10.110], loss: 0.093208, mae: 0.281188, mean_q: 3.841664
 82570/100000: episode: 1160, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 199.873, mean reward: 1.999 [1.500, 5.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.099851, mae: 0.288827, mean_q: 3.857696
 82670/100000: episode: 1161, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.292, mean reward: 1.963 [1.451, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.467, 10.343], loss: 0.100837, mae: 0.285774, mean_q: 3.850629
 82770/100000: episode: 1162, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.271, mean reward: 1.873 [1.447, 2.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.143, 10.098], loss: 0.088591, mae: 0.275236, mean_q: 3.830596
 82870/100000: episode: 1163, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 188.823, mean reward: 1.888 [1.443, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.329, 10.098], loss: 0.077397, mae: 0.266390, mean_q: 3.823478
 82970/100000: episode: 1164, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 183.227, mean reward: 1.832 [1.480, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.854, 10.361], loss: 0.087723, mae: 0.275475, mean_q: 3.824170
 83070/100000: episode: 1165, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 211.852, mean reward: 2.119 [1.482, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.942, 10.123], loss: 0.091050, mae: 0.279799, mean_q: 3.847519
 83170/100000: episode: 1166, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 189.890, mean reward: 1.899 [1.465, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.865, 10.152], loss: 0.087874, mae: 0.280596, mean_q: 3.851285
 83270/100000: episode: 1167, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 197.847, mean reward: 1.978 [1.452, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.848, 10.098], loss: 0.106650, mae: 0.292734, mean_q: 3.861389
 83370/100000: episode: 1168, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 204.454, mean reward: 2.045 [1.493, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.934, 10.098], loss: 0.101342, mae: 0.287692, mean_q: 3.852791
 83470/100000: episode: 1169, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 209.507, mean reward: 2.095 [1.444, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.205], loss: 0.102998, mae: 0.292633, mean_q: 3.866611
 83570/100000: episode: 1170, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.219, mean reward: 1.882 [1.516, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.903, 10.213], loss: 0.111599, mae: 0.290352, mean_q: 3.864932
 83670/100000: episode: 1171, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 189.292, mean reward: 1.893 [1.441, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.353, 10.215], loss: 0.098721, mae: 0.287646, mean_q: 3.868985
 83770/100000: episode: 1172, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 208.294, mean reward: 2.083 [1.468, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.098], loss: 0.105497, mae: 0.293393, mean_q: 3.865765
 83870/100000: episode: 1173, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.130, mean reward: 1.861 [1.471, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.129, 10.221], loss: 0.105280, mae: 0.292069, mean_q: 3.862624
 83970/100000: episode: 1174, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 196.246, mean reward: 1.962 [1.434, 6.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.098], loss: 0.094651, mae: 0.297984, mean_q: 3.880544
 84070/100000: episode: 1175, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 216.927, mean reward: 2.169 [1.462, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.471], loss: 0.104460, mae: 0.299236, mean_q: 3.870654
 84170/100000: episode: 1176, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.831, mean reward: 1.998 [1.471, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.063, 10.098], loss: 0.109716, mae: 0.303255, mean_q: 3.883490
 84270/100000: episode: 1177, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.885, mean reward: 1.819 [1.475, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.700, 10.098], loss: 0.098342, mae: 0.295455, mean_q: 3.884374
 84370/100000: episode: 1178, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 199.558, mean reward: 1.996 [1.446, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.297, 10.228], loss: 0.096443, mae: 0.297303, mean_q: 3.875195
 84470/100000: episode: 1179, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 195.874, mean reward: 1.959 [1.436, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.393, 10.315], loss: 0.091485, mae: 0.294242, mean_q: 3.876060
 84570/100000: episode: 1180, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.866, mean reward: 1.949 [1.452, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.100, 10.104], loss: 0.091518, mae: 0.289891, mean_q: 3.874811
 84670/100000: episode: 1181, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.214, mean reward: 1.862 [1.432, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.008, 10.194], loss: 0.111137, mae: 0.301932, mean_q: 3.901659
 84770/100000: episode: 1182, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 199.917, mean reward: 1.999 [1.457, 12.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.934, 10.098], loss: 0.092661, mae: 0.284459, mean_q: 3.869016
 84870/100000: episode: 1183, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 185.173, mean reward: 1.852 [1.455, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.967, 10.098], loss: 0.124278, mae: 0.307174, mean_q: 3.893376
 84970/100000: episode: 1184, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.554, mean reward: 1.846 [1.450, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.104, 10.243], loss: 0.106348, mae: 0.298662, mean_q: 3.890829
 85070/100000: episode: 1185, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 229.778, mean reward: 2.298 [1.460, 32.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.158, 10.150], loss: 0.102088, mae: 0.296836, mean_q: 3.878227
 85170/100000: episode: 1186, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.226, mean reward: 1.932 [1.443, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.956, 10.240], loss: 0.441742, mae: 0.353265, mean_q: 3.884358
 85270/100000: episode: 1187, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.026, mean reward: 1.960 [1.470, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.722, 10.185], loss: 0.094403, mae: 0.287090, mean_q: 3.865532
[Info] 1-TH LEVEL FOUND: 5.397293567657471, Considering 10/90 traces
 85370/100000: episode: 1188, duration: 4.733s, episode steps: 100, steps per second: 21, episode reward: 190.212, mean reward: 1.902 [1.452, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.567, 10.240], loss: 0.129787, mae: 0.303415, mean_q: 3.878684
 85427/100000: episode: 1189, duration: 0.303s, episode steps: 57, steps per second: 188, episode reward: 124.223, mean reward: 2.179 [1.732, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.763, 10.100], loss: 0.090749, mae: 0.281956, mean_q: 3.871145
 85491/100000: episode: 1190, duration: 0.329s, episode steps: 64, steps per second: 195, episode reward: 134.896, mean reward: 2.108 [1.510, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.792 [-0.553, 10.100], loss: 0.134376, mae: 0.308984, mean_q: 3.879958
 85548/100000: episode: 1191, duration: 0.285s, episode steps: 57, steps per second: 200, episode reward: 114.183, mean reward: 2.003 [1.507, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.157, 10.170], loss: 0.855025, mae: 0.378980, mean_q: 3.939228
 85578/100000: episode: 1192, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 101.533, mean reward: 3.384 [2.303, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.460, 10.554], loss: 0.105522, mae: 0.299932, mean_q: 3.895052
 85635/100000: episode: 1193, duration: 0.301s, episode steps: 57, steps per second: 189, episode reward: 107.638, mean reward: 1.888 [1.484, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.532, 10.100], loss: 0.131374, mae: 0.306244, mean_q: 3.872132
 85692/100000: episode: 1194, duration: 0.301s, episode steps: 57, steps per second: 189, episode reward: 117.390, mean reward: 2.059 [1.512, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.812, 10.116], loss: 0.130840, mae: 0.304804, mean_q: 3.932571
 85756/100000: episode: 1195, duration: 0.316s, episode steps: 64, steps per second: 203, episode reward: 124.784, mean reward: 1.950 [1.531, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-1.072, 10.151], loss: 0.102226, mae: 0.294639, mean_q: 3.896358
 85771/100000: episode: 1196, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 34.700, mean reward: 2.313 [1.975, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.374], loss: 0.085261, mae: 0.292890, mean_q: 3.862570
 85801/100000: episode: 1197, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 69.596, mean reward: 2.320 [1.612, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.128, 10.253], loss: 0.090431, mae: 0.295513, mean_q: 3.884048
 85820/100000: episode: 1198, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 51.592, mean reward: 2.715 [2.108, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.387, 10.100], loss: 0.084200, mae: 0.278145, mean_q: 3.889173
 85836/100000: episode: 1199, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 33.282, mean reward: 2.080 [1.523, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.977, 10.134], loss: 0.080549, mae: 0.269933, mean_q: 3.855871
 85917/100000: episode: 1200, duration: 0.412s, episode steps: 81, steps per second: 197, episode reward: 169.715, mean reward: 2.095 [1.487, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.632 [-0.805, 10.100], loss: 0.463916, mae: 0.313598, mean_q: 3.877837
 85936/100000: episode: 1201, duration: 0.090s, episode steps: 19, steps per second: 210, episode reward: 63.908, mean reward: 3.364 [2.449, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.279, 10.100], loss: 0.984699, mae: 0.500049, mean_q: 4.064594
 85951/100000: episode: 1202, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 34.601, mean reward: 2.307 [2.067, 2.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.368], loss: 0.096450, mae: 0.305074, mean_q: 3.814505
 86026/100000: episode: 1203, duration: 0.404s, episode steps: 75, steps per second: 186, episode reward: 141.718, mean reward: 1.890 [1.495, 4.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.699 [-0.335, 10.350], loss: 0.292183, mae: 0.329002, mean_q: 3.939267
 86101/100000: episode: 1204, duration: 0.404s, episode steps: 75, steps per second: 186, episode reward: 176.921, mean reward: 2.359 [1.481, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.699 [-0.225, 10.233], loss: 0.327188, mae: 0.331539, mean_q: 3.929138
 86131/100000: episode: 1205, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 100.382, mean reward: 3.346 [2.282, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.440, 10.527], loss: 0.097724, mae: 0.304609, mean_q: 3.879332
 86147/100000: episode: 1206, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 46.926, mean reward: 2.933 [1.774, 10.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.683, 10.212], loss: 0.186528, mae: 0.343002, mean_q: 3.974553
 86228/100000: episode: 1207, duration: 0.418s, episode steps: 81, steps per second: 194, episode reward: 160.684, mean reward: 1.984 [1.459, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.647, 10.238], loss: 0.358818, mae: 0.348553, mean_q: 3.979675
 86292/100000: episode: 1208, duration: 0.352s, episode steps: 64, steps per second: 182, episode reward: 128.617, mean reward: 2.010 [1.455, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.741, 10.100], loss: 0.155135, mae: 0.319957, mean_q: 3.940643
 86311/100000: episode: 1209, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 47.783, mean reward: 2.515 [1.924, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.336, 10.100], loss: 0.100212, mae: 0.296734, mean_q: 3.946024
 86386/100000: episode: 1210, duration: 0.389s, episode steps: 75, steps per second: 193, episode reward: 257.888, mean reward: 3.439 [1.474, 17.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-1.213, 10.165], loss: 0.311772, mae: 0.335685, mean_q: 3.970348
 86416/100000: episode: 1211, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 89.328, mean reward: 2.978 [1.996, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.421], loss: 0.205625, mae: 0.350067, mean_q: 3.945445
 86480/100000: episode: 1212, duration: 0.349s, episode steps: 64, steps per second: 183, episode reward: 156.584, mean reward: 2.447 [1.512, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.792 [-0.954, 10.275], loss: 0.195793, mae: 0.354322, mean_q: 4.016521
 86555/100000: episode: 1213, duration: 0.409s, episode steps: 75, steps per second: 183, episode reward: 148.340, mean reward: 1.978 [1.440, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.880, 10.257], loss: 0.141219, mae: 0.329674, mean_q: 4.001808
 86570/100000: episode: 1214, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 39.497, mean reward: 2.633 [2.246, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.165, 10.483], loss: 0.173755, mae: 0.361753, mean_q: 4.019506
 86645/100000: episode: 1215, duration: 0.402s, episode steps: 75, steps per second: 186, episode reward: 155.345, mean reward: 2.071 [1.458, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.685 [-0.922, 10.101], loss: 0.383568, mae: 0.362468, mean_q: 4.038155
 86720/100000: episode: 1216, duration: 0.429s, episode steps: 75, steps per second: 175, episode reward: 176.214, mean reward: 2.350 [1.604, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.492, 10.100], loss: 0.505349, mae: 0.355422, mean_q: 4.036614
 86736/100000: episode: 1217, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 48.275, mean reward: 3.017 [2.432, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.492], loss: 0.142690, mae: 0.366405, mean_q: 4.081406
 86800/100000: episode: 1218, duration: 0.339s, episode steps: 64, steps per second: 189, episode reward: 137.408, mean reward: 2.147 [1.507, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-0.857, 10.114], loss: 0.199678, mae: 0.363428, mean_q: 4.053366
 86864/100000: episode: 1219, duration: 0.332s, episode steps: 64, steps per second: 193, episode reward: 185.541, mean reward: 2.899 [1.954, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.774 [-0.531, 10.100], loss: 0.417155, mae: 0.378095, mean_q: 4.044055
 86928/100000: episode: 1220, duration: 0.347s, episode steps: 64, steps per second: 184, episode reward: 135.561, mean reward: 2.118 [1.471, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.793 [-1.314, 10.312], loss: 0.150979, mae: 0.331661, mean_q: 4.093920
 86947/100000: episode: 1221, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 82.030, mean reward: 4.317 [2.869, 7.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.740, 10.100], loss: 0.129452, mae: 0.335887, mean_q: 4.055303
 86973/100000: episode: 1222, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 75.740, mean reward: 2.913 [2.013, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.368], loss: 0.179889, mae: 0.335636, mean_q: 4.057419
 86988/100000: episode: 1223, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 36.730, mean reward: 2.449 [1.944, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.440], loss: 0.072630, mae: 0.287636, mean_q: 4.068272
 87052/100000: episode: 1224, duration: 0.317s, episode steps: 64, steps per second: 202, episode reward: 136.732, mean reward: 2.136 [1.466, 4.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-0.779, 10.100], loss: 0.097250, mae: 0.302732, mean_q: 4.056701
[Info] FALSIFICATION!
[Info] Levels: [5.3972936, 6.6351705]
[Info] Cond. Prob: [0.1, 0.09]
[Info] Error Prob: 0.009

 87067/100000: episode: 1225, duration: 4.570s, episode steps: 15, steps per second: 3, episode reward: 149.727, mean reward: 9.982 [2.298, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-1.384, 10.681], loss: 0.102139, mae: 0.321203, mean_q: 4.117137
 87167/100000: episode: 1226, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 199.090, mean reward: 1.991 [1.456, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.142, 10.351], loss: 0.223439, mae: 0.361545, mean_q: 4.107445
 87267/100000: episode: 1227, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 189.987, mean reward: 1.900 [1.477, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.224, 10.127], loss: 0.468694, mae: 0.386202, mean_q: 4.130670
 87367/100000: episode: 1228, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 209.216, mean reward: 2.092 [1.509, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.406, 10.172], loss: 1.626284, mae: 0.392043, mean_q: 4.109440
 87467/100000: episode: 1229, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 190.023, mean reward: 1.900 [1.491, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.439, 10.378], loss: 0.155926, mae: 0.343702, mean_q: 4.126794
 87567/100000: episode: 1230, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.772, mean reward: 1.978 [1.465, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.684, 10.221], loss: 0.142786, mae: 0.332268, mean_q: 4.136955
 87667/100000: episode: 1231, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.583, mean reward: 1.976 [1.461, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.188], loss: 0.251199, mae: 0.374657, mean_q: 4.120164
 87767/100000: episode: 1232, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 190.094, mean reward: 1.901 [1.494, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.980, 10.154], loss: 0.126987, mae: 0.330117, mean_q: 4.093115
 87867/100000: episode: 1233, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 181.788, mean reward: 1.818 [1.449, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.135], loss: 0.350061, mae: 0.386744, mean_q: 4.139467
 87967/100000: episode: 1234, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 203.862, mean reward: 2.039 [1.537, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.117, 10.098], loss: 1.671628, mae: 0.433310, mean_q: 4.136940
 88067/100000: episode: 1235, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 196.925, mean reward: 1.969 [1.470, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.170], loss: 1.577437, mae: 0.407772, mean_q: 4.122575
 88167/100000: episode: 1236, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 175.683, mean reward: 1.757 [1.463, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.115], loss: 1.793995, mae: 0.460600, mean_q: 4.147283
 88267/100000: episode: 1237, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.596, mean reward: 1.846 [1.468, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.506, 10.098], loss: 0.171160, mae: 0.341591, mean_q: 4.121198
 88367/100000: episode: 1238, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.882, mean reward: 1.879 [1.461, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.611, 10.243], loss: 1.592369, mae: 0.388763, mean_q: 4.142552
 88467/100000: episode: 1239, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.793, mean reward: 1.858 [1.434, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.419, 10.120], loss: 0.139358, mae: 0.331420, mean_q: 4.077411
 88567/100000: episode: 1240, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.015, mean reward: 2.050 [1.470, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.837, 10.233], loss: 1.611806, mae: 0.395547, mean_q: 4.103021
 88667/100000: episode: 1241, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.063, mean reward: 1.971 [1.480, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.891, 10.294], loss: 0.631698, mae: 0.406865, mean_q: 4.143424
 88767/100000: episode: 1242, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 206.163, mean reward: 2.062 [1.473, 7.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.255, 10.098], loss: 0.191492, mae: 0.350905, mean_q: 4.086723
 88867/100000: episode: 1243, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.987, mean reward: 1.900 [1.464, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.547, 10.098], loss: 3.170045, mae: 0.480040, mean_q: 4.169045
 88967/100000: episode: 1244, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 207.667, mean reward: 2.077 [1.442, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.745, 10.332], loss: 0.165778, mae: 0.345458, mean_q: 4.128966
 89067/100000: episode: 1245, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 204.759, mean reward: 2.048 [1.538, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.711, 10.101], loss: 1.568716, mae: 0.381733, mean_q: 4.127376
 89167/100000: episode: 1246, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 218.707, mean reward: 2.187 [1.449, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.287, 10.375], loss: 0.171749, mae: 0.358105, mean_q: 4.132834
 89267/100000: episode: 1247, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 194.488, mean reward: 1.945 [1.467, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.358, 10.098], loss: 0.259049, mae: 0.337931, mean_q: 4.095134
 89367/100000: episode: 1248, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 211.282, mean reward: 2.113 [1.476, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.902, 10.394], loss: 1.643268, mae: 0.440925, mean_q: 4.144614
 89467/100000: episode: 1249, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 181.130, mean reward: 1.811 [1.434, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.760, 10.147], loss: 0.152908, mae: 0.348249, mean_q: 4.107386
 89567/100000: episode: 1250, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 183.353, mean reward: 1.834 [1.446, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.589, 10.098], loss: 0.143137, mae: 0.327450, mean_q: 4.073770
 89667/100000: episode: 1251, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 213.949, mean reward: 2.139 [1.457, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.168, 10.116], loss: 1.707320, mae: 0.420120, mean_q: 4.114056
 89767/100000: episode: 1252, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 198.092, mean reward: 1.981 [1.454, 4.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.463, 10.196], loss: 1.706926, mae: 0.417159, mean_q: 4.146770
 89867/100000: episode: 1253, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.531, mean reward: 1.905 [1.469, 2.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.272, 10.153], loss: 2.999908, mae: 0.453633, mean_q: 4.215088
 89967/100000: episode: 1254, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.484, mean reward: 1.935 [1.437, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.693, 10.098], loss: 1.757725, mae: 0.429003, mean_q: 4.157698
 90067/100000: episode: 1255, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 198.628, mean reward: 1.986 [1.459, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.084, 10.098], loss: 0.183327, mae: 0.375338, mean_q: 4.140491
 90167/100000: episode: 1256, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.273, mean reward: 1.853 [1.480, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.530, 10.220], loss: 0.146484, mae: 0.354841, mean_q: 4.142758
 90267/100000: episode: 1257, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.733, mean reward: 1.937 [1.457, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.906, 10.196], loss: 2.973695, mae: 0.426397, mean_q: 4.133897
 90367/100000: episode: 1258, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.344, mean reward: 1.883 [1.442, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.102, 10.098], loss: 1.568373, mae: 0.403001, mean_q: 4.195463
 90467/100000: episode: 1259, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.070, mean reward: 1.941 [1.458, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.531, 10.168], loss: 0.131614, mae: 0.335990, mean_q: 4.107818
 90567/100000: episode: 1260, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 199.025, mean reward: 1.990 [1.462, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.931, 10.098], loss: 0.158844, mae: 0.338941, mean_q: 4.099629
 90667/100000: episode: 1261, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 197.321, mean reward: 1.973 [1.455, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.090, 10.098], loss: 1.617389, mae: 0.422866, mean_q: 4.149525
 90767/100000: episode: 1262, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.844, mean reward: 1.898 [1.465, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.173], loss: 2.962647, mae: 0.453357, mean_q: 4.187621
 90867/100000: episode: 1263, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 195.808, mean reward: 1.958 [1.439, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.022, 10.205], loss: 0.122688, mae: 0.326232, mean_q: 4.072691
 90967/100000: episode: 1264, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 203.647, mean reward: 2.036 [1.473, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.422, 10.098], loss: 0.150975, mae: 0.342846, mean_q: 4.098641
 91067/100000: episode: 1265, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.330, mean reward: 1.913 [1.540, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.227, 10.158], loss: 1.616532, mae: 0.399405, mean_q: 4.112473
 91167/100000: episode: 1266, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.504, mean reward: 1.925 [1.471, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.769, 10.279], loss: 1.581638, mae: 0.415608, mean_q: 4.102016
 91267/100000: episode: 1267, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.293, mean reward: 1.973 [1.433, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.288, 10.138], loss: 0.112937, mae: 0.326936, mean_q: 4.020632
 91367/100000: episode: 1268, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.695, mean reward: 1.927 [1.444, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.581, 10.309], loss: 1.542589, mae: 0.374700, mean_q: 4.021544
 91467/100000: episode: 1269, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.744, mean reward: 1.947 [1.459, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.550, 10.126], loss: 1.527758, mae: 0.377727, mean_q: 3.995644
 91567/100000: episode: 1270, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.337, mean reward: 1.883 [1.452, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.723, 10.173], loss: 0.118474, mae: 0.318983, mean_q: 3.946774
 91667/100000: episode: 1271, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 195.531, mean reward: 1.955 [1.464, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.212, 10.098], loss: 0.107267, mae: 0.304063, mean_q: 3.917234
 91767/100000: episode: 1272, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 211.483, mean reward: 2.115 [1.555, 4.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.373, 10.098], loss: 0.107989, mae: 0.314555, mean_q: 3.920682
 91867/100000: episode: 1273, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.794, mean reward: 1.948 [1.431, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.051, 10.098], loss: 0.119941, mae: 0.312779, mean_q: 3.904504
 91967/100000: episode: 1274, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 235.059, mean reward: 2.351 [1.469, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.131, 10.098], loss: 0.094424, mae: 0.304932, mean_q: 3.896311
 92067/100000: episode: 1275, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.430, mean reward: 1.884 [1.469, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.801, 10.098], loss: 0.088337, mae: 0.296513, mean_q: 3.894067
 92167/100000: episode: 1276, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 189.956, mean reward: 1.900 [1.450, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.781, 10.143], loss: 0.090241, mae: 0.300272, mean_q: 3.873972
 92267/100000: episode: 1277, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.636, mean reward: 1.896 [1.457, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.168], loss: 0.095772, mae: 0.297813, mean_q: 3.874119
 92367/100000: episode: 1278, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.803, mean reward: 1.988 [1.444, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.494, 10.098], loss: 0.088421, mae: 0.297675, mean_q: 3.874859
 92467/100000: episode: 1279, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 187.316, mean reward: 1.873 [1.434, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.172, 10.098], loss: 0.093442, mae: 0.303972, mean_q: 3.865638
 92567/100000: episode: 1280, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 183.270, mean reward: 1.833 [1.450, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.898, 10.156], loss: 0.088336, mae: 0.296742, mean_q: 3.874339
 92667/100000: episode: 1281, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 202.224, mean reward: 2.022 [1.477, 6.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.530, 10.269], loss: 0.099148, mae: 0.304821, mean_q: 3.873981
 92767/100000: episode: 1282, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 191.305, mean reward: 1.913 [1.481, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.298, 10.126], loss: 0.095969, mae: 0.303241, mean_q: 3.876201
 92867/100000: episode: 1283, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 188.716, mean reward: 1.887 [1.462, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.137, 10.277], loss: 0.091169, mae: 0.302722, mean_q: 3.863075
 92967/100000: episode: 1284, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 206.172, mean reward: 2.062 [1.463, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.813, 10.330], loss: 0.112569, mae: 0.320041, mean_q: 3.900528
 93067/100000: episode: 1285, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.738, mean reward: 1.897 [1.492, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.645, 10.177], loss: 0.106368, mae: 0.313045, mean_q: 3.868012
 93167/100000: episode: 1286, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.268, mean reward: 1.883 [1.489, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.561, 10.316], loss: 0.083751, mae: 0.293349, mean_q: 3.860258
 93267/100000: episode: 1287, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.675, mean reward: 1.897 [1.490, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.136, 10.203], loss: 0.090885, mae: 0.298363, mean_q: 3.880226
 93367/100000: episode: 1288, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.820, mean reward: 1.868 [1.465, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.755, 10.098], loss: 0.081504, mae: 0.289629, mean_q: 3.869232
 93467/100000: episode: 1289, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 184.963, mean reward: 1.850 [1.460, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.098], loss: 0.086756, mae: 0.294979, mean_q: 3.871929
 93567/100000: episode: 1290, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 247.551, mean reward: 2.476 [1.489, 5.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.889, 10.098], loss: 0.104826, mae: 0.310531, mean_q: 3.870707
 93667/100000: episode: 1291, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.154, mean reward: 1.832 [1.445, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.975, 10.106], loss: 0.096521, mae: 0.303190, mean_q: 3.874030
 93767/100000: episode: 1292, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 191.409, mean reward: 1.914 [1.485, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.117, 10.393], loss: 0.089569, mae: 0.297583, mean_q: 3.865066
 93867/100000: episode: 1293, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 184.583, mean reward: 1.846 [1.447, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.396, 10.122], loss: 0.095138, mae: 0.297622, mean_q: 3.866743
 93967/100000: episode: 1294, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 182.447, mean reward: 1.824 [1.467, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.098], loss: 0.091990, mae: 0.300640, mean_q: 3.856556
 94067/100000: episode: 1295, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.947, mean reward: 1.989 [1.486, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.680, 10.098], loss: 0.088660, mae: 0.297507, mean_q: 3.861273
 94167/100000: episode: 1296, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.992, mean reward: 1.950 [1.468, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.258], loss: 0.088547, mae: 0.302174, mean_q: 3.877913
 94267/100000: episode: 1297, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 209.829, mean reward: 2.098 [1.481, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.769, 10.369], loss: 0.092912, mae: 0.299501, mean_q: 3.860676
 94367/100000: episode: 1298, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 183.891, mean reward: 1.839 [1.442, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.140], loss: 0.094843, mae: 0.296701, mean_q: 3.860578
 94467/100000: episode: 1299, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 191.157, mean reward: 1.912 [1.456, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.230, 10.154], loss: 0.080652, mae: 0.284058, mean_q: 3.851169
 94567/100000: episode: 1300, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.054, mean reward: 1.931 [1.438, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.643, 10.098], loss: 0.088703, mae: 0.298143, mean_q: 3.874158
 94667/100000: episode: 1301, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 191.881, mean reward: 1.919 [1.436, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.152, 10.157], loss: 0.087466, mae: 0.292435, mean_q: 3.854026
 94767/100000: episode: 1302, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 202.127, mean reward: 2.021 [1.436, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.244, 10.098], loss: 0.081772, mae: 0.284937, mean_q: 3.828369
 94867/100000: episode: 1303, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.283, mean reward: 1.853 [1.511, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.131], loss: 0.092121, mae: 0.304993, mean_q: 3.863802
 94967/100000: episode: 1304, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 185.526, mean reward: 1.855 [1.467, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.893, 10.318], loss: 0.086113, mae: 0.287655, mean_q: 3.850024
 95067/100000: episode: 1305, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.410, mean reward: 1.854 [1.439, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.770, 10.098], loss: 0.086873, mae: 0.294756, mean_q: 3.861084
 95167/100000: episode: 1306, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 208.285, mean reward: 2.083 [1.482, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.204, 10.194], loss: 0.083237, mae: 0.289277, mean_q: 3.847874
 95267/100000: episode: 1307, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 186.105, mean reward: 1.861 [1.456, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.253], loss: 0.087498, mae: 0.297928, mean_q: 3.880957
 95367/100000: episode: 1308, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 194.259, mean reward: 1.943 [1.443, 6.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.755, 10.306], loss: 0.074050, mae: 0.279997, mean_q: 3.838645
 95467/100000: episode: 1309, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 197.873, mean reward: 1.979 [1.500, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.515, 10.147], loss: 0.085090, mae: 0.289397, mean_q: 3.860999
 95567/100000: episode: 1310, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.198, mean reward: 1.902 [1.443, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.130, 10.098], loss: 0.086950, mae: 0.297802, mean_q: 3.845500
 95667/100000: episode: 1311, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.892, mean reward: 1.869 [1.509, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.870, 10.180], loss: 0.080991, mae: 0.283493, mean_q: 3.831275
 95767/100000: episode: 1312, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 211.203, mean reward: 2.112 [1.447, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.336, 10.318], loss: 0.089460, mae: 0.288057, mean_q: 3.844362
 95867/100000: episode: 1313, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.542, mean reward: 1.885 [1.482, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.118, 10.098], loss: 0.080854, mae: 0.286075, mean_q: 3.839285
 95967/100000: episode: 1314, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.124, mean reward: 1.821 [1.465, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.110], loss: 0.089628, mae: 0.284956, mean_q: 3.845138
 96067/100000: episode: 1315, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 192.454, mean reward: 1.925 [1.489, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.098], loss: 0.062750, mae: 0.265018, mean_q: 3.822747
 96167/100000: episode: 1316, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.031, mean reward: 1.950 [1.462, 4.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.760, 10.198], loss: 0.083429, mae: 0.289539, mean_q: 3.833388
 96267/100000: episode: 1317, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.071, mean reward: 1.911 [1.476, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.185], loss: 0.075353, mae: 0.275294, mean_q: 3.820418
 96367/100000: episode: 1318, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.040, mean reward: 1.870 [1.450, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.703, 10.238], loss: 0.073733, mae: 0.270175, mean_q: 3.817861
 96467/100000: episode: 1319, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.418, mean reward: 1.944 [1.439, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.698, 10.098], loss: 0.081456, mae: 0.283010, mean_q: 3.824356
 96567/100000: episode: 1320, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 192.269, mean reward: 1.923 [1.455, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.720, 10.348], loss: 0.079234, mae: 0.281787, mean_q: 3.819731
 96667/100000: episode: 1321, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.669, mean reward: 1.937 [1.439, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.627, 10.318], loss: 0.087417, mae: 0.287516, mean_q: 3.847250
 96767/100000: episode: 1322, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 190.027, mean reward: 1.900 [1.456, 5.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.588, 10.198], loss: 0.096276, mae: 0.291074, mean_q: 3.849061
 96867/100000: episode: 1323, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.614, mean reward: 1.886 [1.506, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.420, 10.098], loss: 0.088492, mae: 0.286876, mean_q: 3.833875
 96967/100000: episode: 1324, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.942, mean reward: 1.899 [1.444, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.927, 10.177], loss: 0.075763, mae: 0.276570, mean_q: 3.807420
[Info] 1-TH LEVEL FOUND: 5.176376819610596, Considering 10/90 traces
 97067/100000: episode: 1325, duration: 4.712s, episode steps: 100, steps per second: 21, episode reward: 184.608, mean reward: 1.846 [1.444, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.130, 10.201], loss: 0.074457, mae: 0.272626, mean_q: 3.807048
 97103/100000: episode: 1326, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 81.742, mean reward: 2.271 [1.604, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.035, 10.218], loss: 0.070266, mae: 0.273049, mean_q: 3.794468
 97146/100000: episode: 1327, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 145.585, mean reward: 3.386 [1.794, 9.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.014, 10.100], loss: 0.079200, mae: 0.283525, mean_q: 3.802027
 97156/100000: episode: 1328, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 22.929, mean reward: 2.293 [2.065, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.372, 10.100], loss: 0.123538, mae: 0.291652, mean_q: 3.801706
 97173/100000: episode: 1329, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 41.894, mean reward: 2.464 [1.891, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.371, 10.498], loss: 0.094088, mae: 0.304155, mean_q: 3.850981
 97204/100000: episode: 1330, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 104.436, mean reward: 3.369 [2.221, 9.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.076, 10.436], loss: 0.135742, mae: 0.317571, mean_q: 3.877125
 97239/100000: episode: 1331, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 101.126, mean reward: 2.889 [1.734, 5.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.216], loss: 0.087660, mae: 0.295462, mean_q: 3.834611
 97275/100000: episode: 1332, duration: 0.185s, episode steps: 36, steps per second: 195, episode reward: 72.037, mean reward: 2.001 [1.471, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.061, 10.145], loss: 0.101923, mae: 0.300375, mean_q: 3.873884
 97310/100000: episode: 1333, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 69.975, mean reward: 1.999 [1.432, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.403, 10.100], loss: 0.068015, mae: 0.265994, mean_q: 3.798357
 97341/100000: episode: 1334, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 68.439, mean reward: 2.208 [1.923, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.094, 10.299], loss: 0.101299, mae: 0.298800, mean_q: 3.877726
 97424/100000: episode: 1335, duration: 0.444s, episode steps: 83, steps per second: 187, episode reward: 180.849, mean reward: 2.179 [1.521, 6.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.620 [-1.107, 10.268], loss: 0.088724, mae: 0.287450, mean_q: 3.853274
 97441/100000: episode: 1336, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 48.463, mean reward: 2.851 [2.114, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.272, 10.431], loss: 0.066659, mae: 0.261496, mean_q: 3.833064
 97462/100000: episode: 1337, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 48.776, mean reward: 2.323 [1.534, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.157], loss: 0.096285, mae: 0.308376, mean_q: 3.836493
 97497/100000: episode: 1338, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 94.641, mean reward: 2.704 [1.973, 4.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.451, 10.362], loss: 0.106219, mae: 0.307872, mean_q: 3.879991
 97528/100000: episode: 1339, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 65.289, mean reward: 2.106 [1.719, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.508, 10.363], loss: 0.089008, mae: 0.289103, mean_q: 3.854198
 97563/100000: episode: 1340, duration: 0.184s, episode steps: 35, steps per second: 191, episode reward: 73.151, mean reward: 2.090 [1.597, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.715, 10.205], loss: 0.092012, mae: 0.309292, mean_q: 3.921068
 97598/100000: episode: 1341, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 85.620, mean reward: 2.446 [1.962, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.895, 10.377], loss: 0.087043, mae: 0.295146, mean_q: 3.903693
 97634/100000: episode: 1342, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 88.557, mean reward: 2.460 [1.833, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.405, 10.298], loss: 0.084146, mae: 0.286259, mean_q: 3.869495
 97677/100000: episode: 1343, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 100.029, mean reward: 2.326 [1.646, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.783, 10.100], loss: 0.108822, mae: 0.302940, mean_q: 3.928286
 97698/100000: episode: 1344, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 56.940, mean reward: 2.711 [2.245, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.512], loss: 0.108489, mae: 0.328833, mean_q: 3.928417
 97734/100000: episode: 1345, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 96.059, mean reward: 2.668 [2.105, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.485, 10.378], loss: 0.136940, mae: 0.329554, mean_q: 3.926861
 97777/100000: episode: 1346, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 194.144, mean reward: 4.515 [2.253, 9.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.500, 10.100], loss: 0.102939, mae: 0.326456, mean_q: 3.967449
 97827/100000: episode: 1347, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 113.633, mean reward: 2.273 [1.529, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.142, 10.100], loss: 0.111409, mae: 0.318304, mean_q: 3.982636
 97877/100000: episode: 1348, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 97.653, mean reward: 1.953 [1.462, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.195, 10.196], loss: 0.111386, mae: 0.324228, mean_q: 3.988902
 97960/100000: episode: 1349, duration: 0.442s, episode steps: 83, steps per second: 188, episode reward: 168.159, mean reward: 2.026 [1.519, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.614 [-0.797, 10.100], loss: 0.102634, mae: 0.303763, mean_q: 3.950823
 97981/100000: episode: 1350, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 48.162, mean reward: 2.293 [1.770, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.125, 10.359], loss: 0.120206, mae: 0.320355, mean_q: 3.977040
 97991/100000: episode: 1351, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 27.628, mean reward: 2.763 [2.207, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.162, 10.100], loss: 0.084218, mae: 0.286986, mean_q: 3.926585
 98041/100000: episode: 1352, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 128.898, mean reward: 2.578 [1.659, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.397, 10.100], loss: 0.120271, mae: 0.315312, mean_q: 3.992963
 98077/100000: episode: 1353, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 102.569, mean reward: 2.849 [2.149, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.395], loss: 0.106424, mae: 0.306180, mean_q: 4.006142
 98112/100000: episode: 1354, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 104.853, mean reward: 2.996 [2.165, 6.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.416, 10.454], loss: 0.132389, mae: 0.318914, mean_q: 4.038522
 98122/100000: episode: 1355, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 25.288, mean reward: 2.529 [2.118, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.141, 10.100], loss: 0.188154, mae: 0.359598, mean_q: 3.979050
[Info] FALSIFICATION!
[Info] Levels: [5.176377, 10.800927]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

 98159/100000: episode: 1356, duration: 4.459s, episode steps: 37, steps per second: 8, episode reward: 349.526, mean reward: 9.447 [2.656, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-1.004, 10.020], loss: 0.103759, mae: 0.309341, mean_q: 4.038805
 98259/100000: episode: 1357, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 208.555, mean reward: 2.086 [1.444, 5.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.098], loss: 0.142297, mae: 0.337292, mean_q: 4.059572
 98359/100000: episode: 1358, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 190.309, mean reward: 1.903 [1.450, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.819, 10.098], loss: 2.085495, mae: 0.461605, mean_q: 4.091337
 98459/100000: episode: 1359, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 207.520, mean reward: 2.075 [1.450, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.211, 10.098], loss: 3.919765, mae: 0.633304, mean_q: 4.104118
 98559/100000: episode: 1360, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 195.266, mean reward: 1.953 [1.446, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.996, 10.098], loss: 1.344624, mae: 0.456297, mean_q: 4.045231
 98659/100000: episode: 1361, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.696, mean reward: 1.977 [1.526, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.319, 10.098], loss: 2.016771, mae: 0.448386, mean_q: 4.130818
 98759/100000: episode: 1362, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 183.539, mean reward: 1.835 [1.454, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.875, 10.122], loss: 1.252523, mae: 0.425387, mean_q: 4.131662
 98859/100000: episode: 1363, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 184.827, mean reward: 1.848 [1.491, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.200], loss: 1.413241, mae: 0.410885, mean_q: 4.093337
 98959/100000: episode: 1364, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 185.511, mean reward: 1.855 [1.479, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.694, 10.098], loss: 1.340687, mae: 0.411914, mean_q: 4.138698
 99059/100000: episode: 1365, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.989, mean reward: 1.870 [1.434, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.895, 10.098], loss: 1.947452, mae: 0.424018, mean_q: 4.116993
 99159/100000: episode: 1366, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 182.640, mean reward: 1.826 [1.448, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.040, 10.285], loss: 1.063151, mae: 0.424823, mean_q: 4.135039
 99259/100000: episode: 1367, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.542, mean reward: 1.855 [1.451, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.437, 10.170], loss: 1.974506, mae: 0.440742, mean_q: 4.145998
 99359/100000: episode: 1368, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.664, mean reward: 1.917 [1.441, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.451, 10.098], loss: 0.457595, mae: 0.370416, mean_q: 4.124343
 99459/100000: episode: 1369, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.704, mean reward: 1.837 [1.457, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.751, 10.098], loss: 0.734253, mae: 0.418620, mean_q: 4.180826
 99559/100000: episode: 1370, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 197.389, mean reward: 1.974 [1.460, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.098], loss: 0.410501, mae: 0.371331, mean_q: 4.089397
 99659/100000: episode: 1371, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 193.073, mean reward: 1.931 [1.446, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.308, 10.098], loss: 0.117239, mae: 0.318529, mean_q: 4.056019
 99759/100000: episode: 1372, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.228, mean reward: 1.892 [1.439, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.130, 10.163], loss: 0.121287, mae: 0.321263, mean_q: 4.023861
 99859/100000: episode: 1373, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 192.387, mean reward: 1.924 [1.520, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.592, 10.098], loss: 1.046668, mae: 0.391383, mean_q: 4.116950
 99959/100000: episode: 1374, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.549, mean reward: 1.895 [1.449, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.447, 10.194], loss: 1.048420, mae: 0.407730, mean_q: 4.109948
done, took 594.638 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
