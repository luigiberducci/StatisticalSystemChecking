Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.158s, episode steps: 100, steps per second: 635, episode reward: 190.628, mean reward: 1.906 [1.461, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.358, 10.205], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 206.897, mean reward: 2.069 [1.499, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.518, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.078s, episode steps: 100, steps per second: 1282, episode reward: 187.190, mean reward: 1.872 [1.477, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.975, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 195.454, mean reward: 1.955 [1.472, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.355, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 192.358, mean reward: 1.924 [1.464, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.539, 10.364], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.074s, episode steps: 100, steps per second: 1357, episode reward: 182.525, mean reward: 1.825 [1.476, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.382, 10.103], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.069s, episode steps: 100, steps per second: 1453, episode reward: 194.098, mean reward: 1.941 [1.483, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.675, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 183.489, mean reward: 1.835 [1.465, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.211], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 190.364, mean reward: 1.904 [1.481, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.162, 10.205], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 192.502, mean reward: 1.925 [1.455, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.078s, episode steps: 100, steps per second: 1275, episode reward: 192.750, mean reward: 1.928 [1.443, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.430, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 187.809, mean reward: 1.878 [1.460, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.242, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.071s, episode steps: 100, steps per second: 1410, episode reward: 187.447, mean reward: 1.874 [1.464, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.110, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.070s, episode steps: 100, steps per second: 1427, episode reward: 248.111, mean reward: 2.481 [1.447, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.040, 10.466], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: 203.833, mean reward: 2.038 [1.443, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.427, 10.373], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 190.311, mean reward: 1.903 [1.471, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.125], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.060s, episode steps: 100, steps per second: 1671, episode reward: 190.136, mean reward: 1.901 [1.472, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.405, 10.374], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.060s, episode steps: 100, steps per second: 1668, episode reward: 184.252, mean reward: 1.843 [1.471, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.625, 10.210], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.060s, episode steps: 100, steps per second: 1669, episode reward: 172.635, mean reward: 1.726 [1.457, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.040, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.060s, episode steps: 100, steps per second: 1662, episode reward: 187.091, mean reward: 1.871 [1.489, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.168, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 193.391, mean reward: 1.934 [1.467, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.336, 10.118], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.069s, episode steps: 100, steps per second: 1458, episode reward: 192.480, mean reward: 1.925 [1.454, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.388, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 190.683, mean reward: 1.907 [1.463, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.933, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 189.701, mean reward: 1.897 [1.440, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.932, 10.310], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.067s, episode steps: 100, steps per second: 1502, episode reward: 237.364, mean reward: 2.374 [1.473, 5.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.855, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.073s, episode steps: 100, steps per second: 1367, episode reward: 186.745, mean reward: 1.867 [1.442, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.971, 10.133], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 262.269, mean reward: 2.623 [1.546, 6.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.322, 10.167], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 210.869, mean reward: 2.109 [1.523, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.350, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.067s, episode steps: 100, steps per second: 1498, episode reward: 236.345, mean reward: 2.363 [1.492, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.772, 10.302], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 181.607, mean reward: 1.816 [1.447, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.145, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.070s, episode steps: 100, steps per second: 1422, episode reward: 198.748, mean reward: 1.987 [1.459, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.286, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.059s, episode steps: 100, steps per second: 1682, episode reward: 178.335, mean reward: 1.783 [1.440, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.123], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 227.178, mean reward: 2.272 [1.481, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.158, 10.341], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.061s, episode steps: 100, steps per second: 1629, episode reward: 186.237, mean reward: 1.862 [1.481, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.507, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.068s, episode steps: 100, steps per second: 1473, episode reward: 193.526, mean reward: 1.935 [1.482, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.694, 10.144], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 200.680, mean reward: 2.007 [1.455, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.271], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 195.505, mean reward: 1.955 [1.480, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.150, 10.257], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.070s, episode steps: 100, steps per second: 1432, episode reward: 185.241, mean reward: 1.852 [1.452, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.292, 10.279], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 187.515, mean reward: 1.875 [1.431, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.159, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 190.290, mean reward: 1.903 [1.462, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.484, 10.107], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.061s, episode steps: 100, steps per second: 1639, episode reward: 209.863, mean reward: 2.099 [1.467, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.872, 10.499], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.066s, episode steps: 100, steps per second: 1523, episode reward: 201.679, mean reward: 2.017 [1.466, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.267, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.067s, episode steps: 100, steps per second: 1500, episode reward: 191.850, mean reward: 1.919 [1.444, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.643, 10.224], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 189.894, mean reward: 1.899 [1.440, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.221, 10.123], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.061s, episode steps: 100, steps per second: 1649, episode reward: 208.595, mean reward: 2.086 [1.458, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.443], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 198.605, mean reward: 1.986 [1.454, 4.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.709, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.059s, episode steps: 100, steps per second: 1682, episode reward: 204.405, mean reward: 2.044 [1.503, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.999, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1663, episode reward: 224.294, mean reward: 2.243 [1.456, 4.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.927, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 219.504, mean reward: 2.195 [1.462, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.604, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 196.835, mean reward: 1.968 [1.452, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.108, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.261s, episode steps: 100, steps per second: 79, episode reward: 208.882, mean reward: 2.089 [1.485, 4.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.715, 10.193], loss: 0.229077, mae: 0.486854, mean_q: 2.419203
  5200/100000: episode: 52, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 209.165, mean reward: 2.092 [1.458, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.083, 10.236], loss: 0.118195, mae: 0.346288, mean_q: 3.038007
  5300/100000: episode: 53, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.840, mean reward: 2.018 [1.487, 10.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.203, 10.530], loss: 0.131345, mae: 0.344378, mean_q: 3.343701
  5400/100000: episode: 54, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.666, mean reward: 1.847 [1.485, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.898, 10.139], loss: 0.133065, mae: 0.347036, mean_q: 3.573255
  5500/100000: episode: 55, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.233, mean reward: 1.952 [1.458, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.165, 10.098], loss: 0.138822, mae: 0.348643, mean_q: 3.714952
  5600/100000: episode: 56, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.969, mean reward: 1.960 [1.486, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.361, 10.098], loss: 0.127827, mae: 0.345264, mean_q: 3.812935
  5700/100000: episode: 57, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 185.743, mean reward: 1.857 [1.445, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.718, 10.199], loss: 0.128144, mae: 0.340379, mean_q: 3.847901
  5800/100000: episode: 58, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 199.568, mean reward: 1.996 [1.456, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.098], loss: 0.121024, mae: 0.350679, mean_q: 3.918740
  5900/100000: episode: 59, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.024, mean reward: 1.840 [1.462, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.112], loss: 0.117213, mae: 0.331180, mean_q: 3.921740
  6000/100000: episode: 60, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.401, mean reward: 1.964 [1.437, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.226], loss: 0.127699, mae: 0.341438, mean_q: 3.917375
  6100/100000: episode: 61, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.653, mean reward: 1.927 [1.472, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.213, 10.145], loss: 0.139154, mae: 0.361878, mean_q: 3.964484
  6200/100000: episode: 62, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.998, mean reward: 1.830 [1.477, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.499, 10.107], loss: 0.140513, mae: 0.349616, mean_q: 3.926881
  6300/100000: episode: 63, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.988, mean reward: 1.980 [1.458, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.968, 10.098], loss: 0.116824, mae: 0.341348, mean_q: 3.923234
  6400/100000: episode: 64, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 225.433, mean reward: 2.254 [1.453, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.773, 10.098], loss: 0.112741, mae: 0.335293, mean_q: 3.954519
  6500/100000: episode: 65, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 193.626, mean reward: 1.936 [1.450, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.553, 10.291], loss: 0.117094, mae: 0.334094, mean_q: 3.941670
  6600/100000: episode: 66, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 182.823, mean reward: 1.828 [1.452, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.181], loss: 0.124815, mae: 0.336705, mean_q: 3.922977
  6700/100000: episode: 67, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 195.617, mean reward: 1.956 [1.476, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.354, 10.246], loss: 0.127036, mae: 0.338697, mean_q: 3.941665
  6800/100000: episode: 68, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 207.261, mean reward: 2.073 [1.471, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.255, 10.098], loss: 0.124021, mae: 0.335103, mean_q: 3.937196
  6900/100000: episode: 69, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 190.022, mean reward: 1.900 [1.476, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.133], loss: 0.137234, mae: 0.354950, mean_q: 3.954851
  7000/100000: episode: 70, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 181.199, mean reward: 1.812 [1.481, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.958, 10.098], loss: 0.130746, mae: 0.344433, mean_q: 3.973112
  7100/100000: episode: 71, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.225, mean reward: 1.882 [1.438, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.171], loss: 0.121070, mae: 0.330239, mean_q: 3.932032
  7200/100000: episode: 72, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 194.647, mean reward: 1.946 [1.481, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.396, 10.098], loss: 0.116599, mae: 0.337481, mean_q: 3.927384
  7300/100000: episode: 73, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 201.608, mean reward: 2.016 [1.430, 9.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.484, 10.098], loss: 0.124335, mae: 0.344702, mean_q: 3.934283
  7400/100000: episode: 74, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 207.779, mean reward: 2.078 [1.477, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.987, 10.098], loss: 0.128516, mae: 0.347561, mean_q: 3.965701
  7500/100000: episode: 75, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 182.008, mean reward: 1.820 [1.445, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.122], loss: 0.114304, mae: 0.334370, mean_q: 3.945084
  7600/100000: episode: 76, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.580, mean reward: 1.936 [1.470, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.901, 10.178], loss: 0.123029, mae: 0.330410, mean_q: 3.934728
  7700/100000: episode: 77, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 178.785, mean reward: 1.788 [1.454, 2.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.838, 10.134], loss: 0.119393, mae: 0.327454, mean_q: 3.923147
  7800/100000: episode: 78, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 203.517, mean reward: 2.035 [1.484, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.427, 10.363], loss: 0.086513, mae: 0.300346, mean_q: 3.898121
  7900/100000: episode: 79, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 245.407, mean reward: 2.454 [1.455, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.406 [-1.226, 10.176], loss: 0.099998, mae: 0.314656, mean_q: 3.909679
  8000/100000: episode: 80, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 192.201, mean reward: 1.922 [1.436, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.475, 10.222], loss: 0.152541, mae: 0.341057, mean_q: 3.913142
  8100/100000: episode: 81, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.187, mean reward: 1.892 [1.476, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.554, 10.098], loss: 0.120196, mae: 0.329776, mean_q: 3.920749
  8200/100000: episode: 82, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 205.660, mean reward: 2.057 [1.479, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.392], loss: 0.125750, mae: 0.332515, mean_q: 3.927979
  8300/100000: episode: 83, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.631, mean reward: 1.876 [1.479, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.098], loss: 0.131558, mae: 0.337358, mean_q: 3.929125
  8400/100000: episode: 84, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.992, mean reward: 1.840 [1.449, 3.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.405, 10.213], loss: 0.136025, mae: 0.330088, mean_q: 3.912605
  8500/100000: episode: 85, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 212.417, mean reward: 2.124 [1.497, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.962, 10.178], loss: 0.115510, mae: 0.324006, mean_q: 3.906661
  8600/100000: episode: 86, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 193.004, mean reward: 1.930 [1.474, 4.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.472, 10.199], loss: 0.104205, mae: 0.320054, mean_q: 3.915769
  8700/100000: episode: 87, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 181.069, mean reward: 1.811 [1.479, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.122], loss: 0.121530, mae: 0.329026, mean_q: 3.898923
  8800/100000: episode: 88, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 226.888, mean reward: 2.269 [1.523, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.098], loss: 0.112073, mae: 0.327511, mean_q: 3.910167
  8900/100000: episode: 89, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.600, mean reward: 1.906 [1.446, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.436, 10.098], loss: 0.126892, mae: 0.332661, mean_q: 3.907434
  9000/100000: episode: 90, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.538, mean reward: 2.055 [1.445, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.516, 10.230], loss: 0.133184, mae: 0.342049, mean_q: 3.936663
  9100/100000: episode: 91, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 176.810, mean reward: 1.768 [1.438, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.727, 10.098], loss: 0.129111, mae: 0.343783, mean_q: 3.946769
  9200/100000: episode: 92, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.972, mean reward: 2.020 [1.454, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.667, 10.098], loss: 0.163678, mae: 0.359488, mean_q: 3.941687
  9300/100000: episode: 93, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.771, mean reward: 1.818 [1.464, 2.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.386, 10.098], loss: 0.115551, mae: 0.328863, mean_q: 3.938144
  9400/100000: episode: 94, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.494, mean reward: 1.875 [1.460, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.546, 10.220], loss: 0.125792, mae: 0.334613, mean_q: 3.918466
  9500/100000: episode: 95, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.366, mean reward: 1.994 [1.456, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.723, 10.098], loss: 0.127712, mae: 0.333575, mean_q: 3.909302
  9600/100000: episode: 96, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 202.810, mean reward: 2.028 [1.481, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.456, 10.098], loss: 0.146422, mae: 0.336536, mean_q: 3.915904
  9700/100000: episode: 97, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.213, mean reward: 1.872 [1.435, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.879, 10.098], loss: 0.110099, mae: 0.323698, mean_q: 3.910006
  9800/100000: episode: 98, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.538, mean reward: 1.835 [1.471, 2.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.851, 10.289], loss: 0.128280, mae: 0.327496, mean_q: 3.902779
  9900/100000: episode: 99, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 211.957, mean reward: 2.120 [1.490, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.528, 10.192], loss: 0.123674, mae: 0.331657, mean_q: 3.906948
 10000/100000: episode: 100, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 194.057, mean reward: 1.941 [1.463, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.465, 10.322], loss: 0.133425, mae: 0.322327, mean_q: 3.883523
 10100/100000: episode: 101, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 219.865, mean reward: 2.199 [1.487, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.911, 10.098], loss: 0.106779, mae: 0.318284, mean_q: 3.873903
 10200/100000: episode: 102, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.911, mean reward: 1.909 [1.457, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.706, 10.110], loss: 0.134716, mae: 0.330928, mean_q: 3.893880
 10300/100000: episode: 103, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 198.441, mean reward: 1.984 [1.493, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.098], loss: 0.101074, mae: 0.317525, mean_q: 3.882243
 10400/100000: episode: 104, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 185.044, mean reward: 1.850 [1.457, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.189], loss: 0.100527, mae: 0.318191, mean_q: 3.881251
 10500/100000: episode: 105, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.789, mean reward: 2.048 [1.477, 4.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.694, 10.098], loss: 0.120826, mae: 0.327664, mean_q: 3.891802
 10600/100000: episode: 106, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.758, mean reward: 1.898 [1.463, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.491, 10.098], loss: 0.118699, mae: 0.327592, mean_q: 3.881385
 10700/100000: episode: 107, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 177.965, mean reward: 1.780 [1.460, 2.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.868, 10.191], loss: 0.121401, mae: 0.328883, mean_q: 3.879081
 10800/100000: episode: 108, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.924, mean reward: 1.899 [1.452, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.051, 10.157], loss: 0.100357, mae: 0.312674, mean_q: 3.855328
 10900/100000: episode: 109, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.218, mean reward: 1.882 [1.473, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.684, 10.311], loss: 0.101736, mae: 0.317644, mean_q: 3.881585
 11000/100000: episode: 110, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.148, mean reward: 1.921 [1.462, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.936, 10.098], loss: 0.102480, mae: 0.318461, mean_q: 3.864455
 11100/100000: episode: 111, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.127, mean reward: 1.981 [1.530, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.428, 10.395], loss: 0.094155, mae: 0.307025, mean_q: 3.856475
 11200/100000: episode: 112, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.290, mean reward: 1.973 [1.452, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.857, 10.098], loss: 0.101957, mae: 0.321428, mean_q: 3.871979
 11300/100000: episode: 113, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 223.884, mean reward: 2.239 [1.436, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.150, 10.485], loss: 0.106526, mae: 0.331174, mean_q: 3.881802
 11400/100000: episode: 114, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.837, mean reward: 1.998 [1.442, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.564, 10.189], loss: 0.111594, mae: 0.330249, mean_q: 3.897776
 11500/100000: episode: 115, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.430, mean reward: 1.964 [1.443, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.098], loss: 0.100521, mae: 0.316274, mean_q: 3.880728
 11600/100000: episode: 116, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.372, mean reward: 1.924 [1.514, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.676, 10.098], loss: 0.103067, mae: 0.322517, mean_q: 3.896097
 11700/100000: episode: 117, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 174.369, mean reward: 1.744 [1.453, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.688, 10.118], loss: 0.130746, mae: 0.346890, mean_q: 3.899774
 11800/100000: episode: 118, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 188.119, mean reward: 1.881 [1.455, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.138, 10.213], loss: 0.100078, mae: 0.315045, mean_q: 3.876113
 11900/100000: episode: 119, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.983, mean reward: 1.870 [1.463, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.257], loss: 0.106065, mae: 0.317134, mean_q: 3.872857
 12000/100000: episode: 120, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.373, mean reward: 1.924 [1.445, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.173, 10.130], loss: 0.102584, mae: 0.318268, mean_q: 3.883075
 12100/100000: episode: 121, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.276, mean reward: 1.843 [1.506, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.881, 10.098], loss: 0.106318, mae: 0.313889, mean_q: 3.868031
 12200/100000: episode: 122, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 177.404, mean reward: 1.774 [1.446, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.140, 10.148], loss: 0.103635, mae: 0.310611, mean_q: 3.845126
 12300/100000: episode: 123, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.853, mean reward: 1.849 [1.481, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.128], loss: 0.093448, mae: 0.306037, mean_q: 3.850511
 12400/100000: episode: 124, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 180.307, mean reward: 1.803 [1.479, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.599, 10.185], loss: 0.092672, mae: 0.301770, mean_q: 3.846353
 12500/100000: episode: 125, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.074, mean reward: 1.851 [1.441, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.475, 10.098], loss: 0.095786, mae: 0.306744, mean_q: 3.846044
 12600/100000: episode: 126, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 203.447, mean reward: 2.034 [1.478, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.802, 10.219], loss: 0.099784, mae: 0.312611, mean_q: 3.843507
 12700/100000: episode: 127, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.895, mean reward: 1.949 [1.477, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.511, 10.177], loss: 0.105771, mae: 0.316948, mean_q: 3.858675
 12800/100000: episode: 128, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.546, mean reward: 1.845 [1.458, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.504, 10.098], loss: 0.096654, mae: 0.306046, mean_q: 3.846348
 12900/100000: episode: 129, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 184.634, mean reward: 1.846 [1.459, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.156, 10.144], loss: 0.096079, mae: 0.310359, mean_q: 3.856122
 13000/100000: episode: 130, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.488, mean reward: 1.895 [1.449, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.394, 10.098], loss: 0.084692, mae: 0.298020, mean_q: 3.827638
 13100/100000: episode: 131, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 200.348, mean reward: 2.003 [1.439, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.499, 10.243], loss: 0.085287, mae: 0.295134, mean_q: 3.833545
 13200/100000: episode: 132, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.864, mean reward: 1.899 [1.493, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.274], loss: 0.090369, mae: 0.303624, mean_q: 3.816305
 13300/100000: episode: 133, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.795, mean reward: 1.898 [1.473, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.098], loss: 0.102046, mae: 0.311900, mean_q: 3.808569
 13400/100000: episode: 134, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 184.844, mean reward: 1.848 [1.454, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.850, 10.109], loss: 0.095848, mae: 0.313261, mean_q: 3.828398
 13500/100000: episode: 135, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 220.704, mean reward: 2.207 [1.469, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.657, 10.098], loss: 0.088353, mae: 0.301037, mean_q: 3.808800
 13600/100000: episode: 136, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.484, mean reward: 1.915 [1.480, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.051, 10.206], loss: 0.094795, mae: 0.309989, mean_q: 3.822053
 13700/100000: episode: 137, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 201.286, mean reward: 2.013 [1.484, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.198, 10.098], loss: 0.086359, mae: 0.302160, mean_q: 3.809917
 13800/100000: episode: 138, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.535, mean reward: 1.875 [1.459, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.130], loss: 0.087345, mae: 0.293554, mean_q: 3.815043
 13900/100000: episode: 139, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.897, mean reward: 1.949 [1.546, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.121, 10.098], loss: 0.082354, mae: 0.292010, mean_q: 3.816809
 14000/100000: episode: 140, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 191.500, mean reward: 1.915 [1.465, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.856, 10.098], loss: 0.085907, mae: 0.296068, mean_q: 3.821391
 14100/100000: episode: 141, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.603, mean reward: 2.006 [1.470, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.730, 10.348], loss: 0.088391, mae: 0.300394, mean_q: 3.811496
 14200/100000: episode: 142, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 187.576, mean reward: 1.876 [1.454, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.160, 10.223], loss: 0.079497, mae: 0.292948, mean_q: 3.818228
 14300/100000: episode: 143, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 200.330, mean reward: 2.003 [1.451, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.306, 10.098], loss: 0.075729, mae: 0.283132, mean_q: 3.809940
 14400/100000: episode: 144, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 197.247, mean reward: 1.972 [1.487, 4.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.305], loss: 0.081268, mae: 0.295380, mean_q: 3.824549
 14500/100000: episode: 145, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.877, mean reward: 1.789 [1.460, 2.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.631, 10.137], loss: 0.080652, mae: 0.290041, mean_q: 3.808743
 14600/100000: episode: 146, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.805, mean reward: 1.918 [1.435, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.221, 10.098], loss: 0.080145, mae: 0.288958, mean_q: 3.821153
 14700/100000: episode: 147, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 182.802, mean reward: 1.828 [1.460, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.267, 10.129], loss: 0.074943, mae: 0.278315, mean_q: 3.800312
 14800/100000: episode: 148, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.935, mean reward: 1.949 [1.503, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.082, 10.098], loss: 0.083919, mae: 0.292827, mean_q: 3.809884
 14900/100000: episode: 149, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.195, mean reward: 1.922 [1.470, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.229, 10.098], loss: 0.085662, mae: 0.295847, mean_q: 3.812596
[Info] 1-TH LEVEL FOUND: 4.321181774139404, Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.997s, episode steps: 100, steps per second: 20, episode reward: 190.809, mean reward: 1.908 [1.489, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.435, 10.175], loss: 0.084713, mae: 0.299799, mean_q: 3.821079
 15019/100000: episode: 151, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 35.294, mean reward: 1.858 [1.465, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.238, 10.162], loss: 0.085279, mae: 0.300128, mean_q: 3.793472
 15068/100000: episode: 152, duration: 0.249s, episode steps: 49, steps per second: 197, episode reward: 93.891, mean reward: 1.916 [1.485, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.215, 10.157], loss: 0.067688, mae: 0.269795, mean_q: 3.785180
 15119/100000: episode: 153, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 103.100, mean reward: 2.022 [1.560, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.312, 10.263], loss: 0.068485, mae: 0.268607, mean_q: 3.777379
 15169/100000: episode: 154, duration: 0.298s, episode steps: 50, steps per second: 168, episode reward: 94.278, mean reward: 1.886 [1.472, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.544, 10.213], loss: 0.073094, mae: 0.280095, mean_q: 3.795431
 15225/100000: episode: 155, duration: 0.306s, episode steps: 56, steps per second: 183, episode reward: 147.572, mean reward: 2.635 [1.497, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.538, 10.226], loss: 0.081755, mae: 0.285568, mean_q: 3.775294
 15244/100000: episode: 156, duration: 0.119s, episode steps: 19, steps per second: 159, episode reward: 34.578, mean reward: 1.820 [1.491, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.155], loss: 0.066642, mae: 0.270273, mean_q: 3.782495
 15294/100000: episode: 157, duration: 0.284s, episode steps: 50, steps per second: 176, episode reward: 117.827, mean reward: 2.357 [1.466, 5.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.240, 10.229], loss: 0.083720, mae: 0.287697, mean_q: 3.788587
 15316/100000: episode: 158, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 41.036, mean reward: 1.865 [1.567, 2.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.358, 10.100], loss: 0.074163, mae: 0.276374, mean_q: 3.785166
 15335/100000: episode: 159, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 34.085, mean reward: 1.794 [1.493, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.049, 10.193], loss: 0.084747, mae: 0.290248, mean_q: 3.819158
 15428/100000: episode: 160, duration: 0.524s, episode steps: 93, steps per second: 177, episode reward: 177.624, mean reward: 1.910 [1.494, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.535, 10.100], loss: 0.094234, mae: 0.302103, mean_q: 3.799622
 15478/100000: episode: 161, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 100.252, mean reward: 2.005 [1.481, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.969, 10.100], loss: 0.085957, mae: 0.290163, mean_q: 3.794498
 15529/100000: episode: 162, duration: 0.310s, episode steps: 51, steps per second: 165, episode reward: 128.211, mean reward: 2.514 [1.836, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.479, 10.375], loss: 0.077276, mae: 0.280697, mean_q: 3.772509
 15579/100000: episode: 163, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 101.353, mean reward: 2.027 [1.445, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.809, 10.100], loss: 0.096990, mae: 0.307310, mean_q: 3.813766
 15631/100000: episode: 164, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 109.204, mean reward: 2.100 [1.638, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-1.385, 10.191], loss: 0.093766, mae: 0.306836, mean_q: 3.825062
 15687/100000: episode: 165, duration: 0.307s, episode steps: 56, steps per second: 183, episode reward: 142.551, mean reward: 2.546 [1.526, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.474, 10.252], loss: 0.095609, mae: 0.309014, mean_q: 3.859806
 15736/100000: episode: 166, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 115.601, mean reward: 2.359 [1.629, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.243, 10.423], loss: 0.093632, mae: 0.307233, mean_q: 3.833149
 15788/100000: episode: 167, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 111.852, mean reward: 2.151 [1.633, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.825, 10.206], loss: 0.097560, mae: 0.310608, mean_q: 3.859182
 15881/100000: episode: 168, duration: 0.506s, episode steps: 93, steps per second: 184, episode reward: 179.121, mean reward: 1.926 [1.458, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-0.619, 10.100], loss: 0.086700, mae: 0.295328, mean_q: 3.854631
 15933/100000: episode: 169, duration: 0.291s, episode steps: 52, steps per second: 179, episode reward: 123.767, mean reward: 2.380 [1.694, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.827, 10.327], loss: 0.100247, mae: 0.315954, mean_q: 3.863171
 15985/100000: episode: 170, duration: 0.263s, episode steps: 52, steps per second: 197, episode reward: 134.710, mean reward: 2.591 [1.996, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.612, 10.254], loss: 0.098422, mae: 0.309558, mean_q: 3.860575
 16036/100000: episode: 171, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 98.749, mean reward: 1.936 [1.471, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.236, 10.198], loss: 0.094956, mae: 0.309990, mean_q: 3.863912
 16055/100000: episode: 172, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 33.837, mean reward: 1.781 [1.461, 2.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.247], loss: 0.104132, mae: 0.316887, mean_q: 3.882135
 16107/100000: episode: 173, duration: 0.277s, episode steps: 52, steps per second: 188, episode reward: 108.501, mean reward: 2.087 [1.434, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.436, 10.165], loss: 0.090171, mae: 0.307883, mean_q: 3.887680
 16156/100000: episode: 174, duration: 0.282s, episode steps: 49, steps per second: 173, episode reward: 103.896, mean reward: 2.120 [1.537, 8.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.529, 10.178], loss: 0.104651, mae: 0.336191, mean_q: 3.895086
 16207/100000: episode: 175, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 116.882, mean reward: 2.292 [1.576, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.904, 10.407], loss: 0.120213, mae: 0.330555, mean_q: 3.879069
 16257/100000: episode: 176, duration: 0.295s, episode steps: 50, steps per second: 170, episode reward: 99.391, mean reward: 1.988 [1.525, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.613, 10.100], loss: 0.108913, mae: 0.319743, mean_q: 3.889949
 16308/100000: episode: 177, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 102.431, mean reward: 2.008 [1.487, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.251, 10.100], loss: 0.116210, mae: 0.320591, mean_q: 3.899212
 16359/100000: episode: 178, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 94.357, mean reward: 1.850 [1.439, 2.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.257, 10.104], loss: 0.089574, mae: 0.305295, mean_q: 3.899729
 16415/100000: episode: 179, duration: 0.276s, episode steps: 56, steps per second: 203, episode reward: 136.522, mean reward: 2.438 [1.534, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.328, 10.100], loss: 0.101998, mae: 0.309264, mean_q: 3.885136
 16466/100000: episode: 180, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 104.833, mean reward: 2.056 [1.552, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.707, 10.100], loss: 0.093801, mae: 0.294034, mean_q: 3.883593
 16560/100000: episode: 181, duration: 0.510s, episode steps: 94, steps per second: 184, episode reward: 174.974, mean reward: 1.861 [1.456, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.696, 10.220], loss: 0.100498, mae: 0.308084, mean_q: 3.902373
 16609/100000: episode: 182, duration: 0.292s, episode steps: 49, steps per second: 168, episode reward: 98.647, mean reward: 2.013 [1.477, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.633, 10.215], loss: 0.119695, mae: 0.328669, mean_q: 3.919290
 16658/100000: episode: 183, duration: 0.281s, episode steps: 49, steps per second: 175, episode reward: 112.617, mean reward: 2.298 [1.657, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.207, 10.340], loss: 0.099412, mae: 0.304677, mean_q: 3.878214
 16752/100000: episode: 184, duration: 0.548s, episode steps: 94, steps per second: 172, episode reward: 194.110, mean reward: 2.065 [1.492, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-1.579, 10.307], loss: 0.114454, mae: 0.326157, mean_q: 3.895956
 16803/100000: episode: 185, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 129.005, mean reward: 2.530 [1.728, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.096, 10.432], loss: 0.090276, mae: 0.305454, mean_q: 3.884014
 16854/100000: episode: 186, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 106.195, mean reward: 2.082 [1.599, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-2.322, 10.262], loss: 0.112230, mae: 0.322769, mean_q: 3.913071
 16906/100000: episode: 187, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 117.307, mean reward: 2.256 [1.630, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.311, 10.426], loss: 0.095655, mae: 0.313581, mean_q: 3.894243
 16957/100000: episode: 188, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 93.416, mean reward: 1.832 [1.519, 2.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.535, 10.100], loss: 0.102140, mae: 0.319386, mean_q: 3.936281
 17008/100000: episode: 189, duration: 0.296s, episode steps: 51, steps per second: 172, episode reward: 119.859, mean reward: 2.350 [1.806, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.727, 10.369], loss: 0.107176, mae: 0.327952, mean_q: 3.957260
 17058/100000: episode: 190, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 116.009, mean reward: 2.320 [1.635, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.787, 10.203], loss: 0.115793, mae: 0.326743, mean_q: 3.933759
 17151/100000: episode: 191, duration: 0.485s, episode steps: 93, steps per second: 192, episode reward: 171.040, mean reward: 1.839 [1.473, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.568, 10.340], loss: 0.104472, mae: 0.319646, mean_q: 3.958182
 17200/100000: episode: 192, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 100.592, mean reward: 2.053 [1.684, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.414, 10.287], loss: 0.092442, mae: 0.306419, mean_q: 3.937801
 17219/100000: episode: 193, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 36.468, mean reward: 1.919 [1.478, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.084, 10.100], loss: 0.125492, mae: 0.328727, mean_q: 3.985900
 17270/100000: episode: 194, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 104.901, mean reward: 2.057 [1.490, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.244, 10.217], loss: 0.135726, mae: 0.340871, mean_q: 3.963299
 17364/100000: episode: 195, duration: 0.497s, episode steps: 94, steps per second: 189, episode reward: 200.457, mean reward: 2.133 [1.479, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-2.012, 10.342], loss: 0.102228, mae: 0.315546, mean_q: 3.962332
 17414/100000: episode: 196, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 104.470, mean reward: 2.089 [1.593, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.657, 10.219], loss: 0.108630, mae: 0.329930, mean_q: 3.977551
 17463/100000: episode: 197, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 104.718, mean reward: 2.137 [1.471, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.294, 10.249], loss: 0.112034, mae: 0.329532, mean_q: 3.964892
 17485/100000: episode: 198, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 45.477, mean reward: 2.067 [1.634, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.858, 10.100], loss: 0.096718, mae: 0.319450, mean_q: 3.995267
 17541/100000: episode: 199, duration: 0.319s, episode steps: 56, steps per second: 175, episode reward: 127.449, mean reward: 2.276 [1.583, 4.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.775, 10.262], loss: 0.116650, mae: 0.337940, mean_q: 3.997966
 17563/100000: episode: 200, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 41.741, mean reward: 1.897 [1.479, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.568, 10.100], loss: 0.101465, mae: 0.326217, mean_q: 3.973297
 17656/100000: episode: 201, duration: 0.497s, episode steps: 93, steps per second: 187, episode reward: 198.391, mean reward: 2.133 [1.440, 5.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-1.025, 10.400], loss: 0.111246, mae: 0.330797, mean_q: 3.973866
 17749/100000: episode: 202, duration: 0.480s, episode steps: 93, steps per second: 194, episode reward: 176.315, mean reward: 1.896 [1.459, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.905, 10.100], loss: 0.097455, mae: 0.317989, mean_q: 3.990242
 17842/100000: episode: 203, duration: 0.490s, episode steps: 93, steps per second: 190, episode reward: 172.194, mean reward: 1.852 [1.452, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-1.115, 10.286], loss: 0.117634, mae: 0.328311, mean_q: 4.002430
 17892/100000: episode: 204, duration: 0.277s, episode steps: 50, steps per second: 181, episode reward: 94.605, mean reward: 1.892 [1.537, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.670, 10.100], loss: 0.102182, mae: 0.321916, mean_q: 3.991717
 17944/100000: episode: 205, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 112.815, mean reward: 2.170 [1.583, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.531, 10.267], loss: 0.109189, mae: 0.327448, mean_q: 3.961712
 17995/100000: episode: 206, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 120.301, mean reward: 2.359 [1.571, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.630, 10.285], loss: 0.113808, mae: 0.336390, mean_q: 3.996871
 18045/100000: episode: 207, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 104.347, mean reward: 2.087 [1.559, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.368, 10.100], loss: 0.145524, mae: 0.355544, mean_q: 4.014776
 18139/100000: episode: 208, duration: 0.529s, episode steps: 94, steps per second: 178, episode reward: 257.810, mean reward: 2.743 [1.474, 7.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-2.532, 10.100], loss: 0.114171, mae: 0.337337, mean_q: 4.000344
 18190/100000: episode: 209, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 93.532, mean reward: 1.834 [1.491, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.245, 10.191], loss: 0.131753, mae: 0.337538, mean_q: 4.027034
 18240/100000: episode: 210, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 104.370, mean reward: 2.087 [1.488, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.461, 10.447], loss: 0.112442, mae: 0.335198, mean_q: 3.996440
 18259/100000: episode: 211, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 33.662, mean reward: 1.772 [1.519, 2.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.340, 10.100], loss: 0.152787, mae: 0.369738, mean_q: 4.032429
 18352/100000: episode: 212, duration: 0.498s, episode steps: 93, steps per second: 187, episode reward: 176.939, mean reward: 1.903 [1.508, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.576, 10.100], loss: 0.135556, mae: 0.363054, mean_q: 4.047922
 18404/100000: episode: 213, duration: 0.281s, episode steps: 52, steps per second: 185, episode reward: 149.125, mean reward: 2.868 [1.874, 6.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.696, 10.551], loss: 0.173784, mae: 0.371999, mean_q: 4.060222
 18456/100000: episode: 214, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 131.219, mean reward: 2.523 [1.870, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.341, 10.424], loss: 0.124501, mae: 0.338631, mean_q: 4.030901
 18550/100000: episode: 215, duration: 0.522s, episode steps: 94, steps per second: 180, episode reward: 175.171, mean reward: 1.864 [1.441, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.508, 10.176], loss: 0.138691, mae: 0.352239, mean_q: 4.062406
 18572/100000: episode: 216, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 40.648, mean reward: 1.848 [1.588, 2.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.700, 10.231], loss: 0.152542, mae: 0.350781, mean_q: 4.045563
 18623/100000: episode: 217, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 102.700, mean reward: 2.014 [1.550, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.630, 10.100], loss: 0.130988, mae: 0.337819, mean_q: 3.995847
 18717/100000: episode: 218, duration: 0.494s, episode steps: 94, steps per second: 190, episode reward: 185.457, mean reward: 1.973 [1.441, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.880, 10.229], loss: 0.136273, mae: 0.348108, mean_q: 4.042515
 18767/100000: episode: 219, duration: 0.283s, episode steps: 50, steps per second: 177, episode reward: 95.013, mean reward: 1.900 [1.520, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.093, 10.187], loss: 0.145421, mae: 0.352022, mean_q: 4.025780
 18860/100000: episode: 220, duration: 0.514s, episode steps: 93, steps per second: 181, episode reward: 184.567, mean reward: 1.985 [1.565, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.255, 10.100], loss: 0.142141, mae: 0.359959, mean_q: 4.043993
 18911/100000: episode: 221, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 121.646, mean reward: 2.385 [1.887, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.413, 10.356], loss: 0.135698, mae: 0.364858, mean_q: 4.078938
 18962/100000: episode: 222, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 95.931, mean reward: 1.881 [1.470, 2.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.372, 10.151], loss: 0.125367, mae: 0.325157, mean_q: 4.011659
 19018/100000: episode: 223, duration: 0.300s, episode steps: 56, steps per second: 187, episode reward: 165.575, mean reward: 2.957 [1.908, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-1.554, 10.366], loss: 0.111251, mae: 0.338420, mean_q: 4.040937
 19069/100000: episode: 224, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 115.421, mean reward: 2.263 [1.684, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.805, 10.386], loss: 0.145013, mae: 0.356436, mean_q: 4.035956
 19120/100000: episode: 225, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 96.570, mean reward: 1.894 [1.530, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.489, 10.100], loss: 0.155406, mae: 0.369476, mean_q: 4.053693
 19176/100000: episode: 226, duration: 0.321s, episode steps: 56, steps per second: 174, episode reward: 159.788, mean reward: 2.853 [2.316, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.466, 10.374], loss: 0.161765, mae: 0.381488, mean_q: 4.101138
 19225/100000: episode: 227, duration: 0.287s, episode steps: 49, steps per second: 171, episode reward: 91.952, mean reward: 1.877 [1.499, 2.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.715, 10.106], loss: 0.148087, mae: 0.378366, mean_q: 4.077254
 19275/100000: episode: 228, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 99.314, mean reward: 1.986 [1.472, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.803, 10.100], loss: 0.136346, mae: 0.359083, mean_q: 4.130410
 19294/100000: episode: 229, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 33.891, mean reward: 1.784 [1.512, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.121, 10.100], loss: 0.182821, mae: 0.393773, mean_q: 4.099625
 19344/100000: episode: 230, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 85.284, mean reward: 1.706 [1.444, 2.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.777, 10.139], loss: 0.153212, mae: 0.386732, mean_q: 4.167432
 19396/100000: episode: 231, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 113.413, mean reward: 2.181 [1.739, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.950, 10.258], loss: 0.142652, mae: 0.361396, mean_q: 4.125948
 19446/100000: episode: 232, duration: 0.292s, episode steps: 50, steps per second: 171, episode reward: 121.574, mean reward: 2.431 [1.511, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.649, 10.139], loss: 0.155450, mae: 0.368616, mean_q: 4.085911
 19465/100000: episode: 233, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 34.020, mean reward: 1.791 [1.454, 2.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.803, 10.100], loss: 0.139142, mae: 0.355942, mean_q: 4.079187
 19558/100000: episode: 234, duration: 0.485s, episode steps: 93, steps per second: 192, episode reward: 170.634, mean reward: 1.835 [1.443, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.669, 10.163], loss: 0.135227, mae: 0.358174, mean_q: 4.112638
 19577/100000: episode: 235, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 36.398, mean reward: 1.916 [1.499, 2.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.070, 10.100], loss: 0.139311, mae: 0.361246, mean_q: 4.112469
 19628/100000: episode: 236, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 108.019, mean reward: 2.118 [1.495, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.197, 10.100], loss: 0.129920, mae: 0.360315, mean_q: 4.151685
 19721/100000: episode: 237, duration: 0.512s, episode steps: 93, steps per second: 182, episode reward: 195.657, mean reward: 2.104 [1.509, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.646, 10.100], loss: 0.126613, mae: 0.355557, mean_q: 4.080197
 19815/100000: episode: 238, duration: 0.527s, episode steps: 94, steps per second: 178, episode reward: 187.419, mean reward: 1.994 [1.488, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.645, 10.100], loss: 0.141331, mae: 0.370330, mean_q: 4.135234
 19867/100000: episode: 239, duration: 0.297s, episode steps: 52, steps per second: 175, episode reward: 106.830, mean reward: 2.054 [1.534, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.941, 10.100], loss: 0.137493, mae: 0.354036, mean_q: 4.095361
[Info] 2-TH LEVEL FOUND: 5.955633640289307, Considering 10/90 traces
 19918/100000: episode: 240, duration: 4.306s, episode steps: 51, steps per second: 12, episode reward: 110.139, mean reward: 2.160 [1.477, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.595, 10.100], loss: 0.121602, mae: 0.352487, mean_q: 4.121365
 19969/100000: episode: 241, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 105.509, mean reward: 2.069 [1.441, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.326, 10.100], loss: 0.119775, mae: 0.349790, mean_q: 4.136242
 20023/100000: episode: 242, duration: 0.293s, episode steps: 54, steps per second: 184, episode reward: 167.209, mean reward: 3.096 [1.538, 7.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.879, 10.184], loss: 0.128437, mae: 0.355699, mean_q: 4.120027
 20074/100000: episode: 243, duration: 0.310s, episode steps: 51, steps per second: 164, episode reward: 131.819, mean reward: 2.585 [1.539, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.344, 10.181], loss: 0.130378, mae: 0.357903, mean_q: 4.132932
 20128/100000: episode: 244, duration: 0.286s, episode steps: 54, steps per second: 189, episode reward: 123.563, mean reward: 2.288 [1.501, 5.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.494, 10.265], loss: 0.158676, mae: 0.377297, mean_q: 4.189025
 20177/100000: episode: 245, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 140.996, mean reward: 2.877 [1.483, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.138, 10.194], loss: 0.120911, mae: 0.351171, mean_q: 4.171997
 20231/100000: episode: 246, duration: 0.310s, episode steps: 54, steps per second: 174, episode reward: 121.784, mean reward: 2.255 [1.579, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-1.501, 10.320], loss: 0.138464, mae: 0.355353, mean_q: 4.161626
 20282/100000: episode: 247, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 108.196, mean reward: 2.121 [1.513, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.322, 10.100], loss: 0.142175, mae: 0.363139, mean_q: 4.175564
 20331/100000: episode: 248, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 131.958, mean reward: 2.693 [1.887, 5.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.415, 10.438], loss: 0.162949, mae: 0.386984, mean_q: 4.197149
 20382/100000: episode: 249, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 136.731, mean reward: 2.681 [1.726, 4.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.860, 10.326], loss: 0.143034, mae: 0.363988, mean_q: 4.194534
 20431/100000: episode: 250, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 146.162, mean reward: 2.983 [1.990, 14.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.126, 10.414], loss: 0.181827, mae: 0.366512, mean_q: 4.188356
 20479/100000: episode: 251, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 107.663, mean reward: 2.243 [1.509, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.320, 10.100], loss: 0.147243, mae: 0.375291, mean_q: 4.227815
 20527/100000: episode: 252, duration: 0.262s, episode steps: 48, steps per second: 183, episode reward: 122.584, mean reward: 2.554 [2.033, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.315, 10.408], loss: 0.128141, mae: 0.363290, mean_q: 4.232293
 20581/100000: episode: 253, duration: 0.285s, episode steps: 54, steps per second: 189, episode reward: 142.175, mean reward: 2.633 [1.518, 6.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.426, 10.180], loss: 0.251926, mae: 0.412301, mean_q: 4.248520
 20629/100000: episode: 254, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 114.638, mean reward: 2.388 [1.533, 5.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.947, 10.234], loss: 0.201416, mae: 0.387124, mean_q: 4.257917
 20678/100000: episode: 255, duration: 0.248s, episode steps: 49, steps per second: 198, episode reward: 114.879, mean reward: 2.344 [1.521, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.084, 10.110], loss: 0.146959, mae: 0.375861, mean_q: 4.270820
 20726/100000: episode: 256, duration: 0.265s, episode steps: 48, steps per second: 181, episode reward: 133.434, mean reward: 2.780 [1.678, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.313, 10.187], loss: 0.129799, mae: 0.357830, mean_q: 4.242952
 20780/100000: episode: 257, duration: 0.292s, episode steps: 54, steps per second: 185, episode reward: 153.188, mean reward: 2.837 [1.460, 6.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.678, 10.100], loss: 0.166057, mae: 0.376084, mean_q: 4.245193
 20829/100000: episode: 258, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 131.148, mean reward: 2.676 [2.052, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.086, 10.362], loss: 0.196928, mae: 0.407862, mean_q: 4.280963
 20883/100000: episode: 259, duration: 0.310s, episode steps: 54, steps per second: 174, episode reward: 126.662, mean reward: 2.346 [1.614, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.394, 10.307], loss: 0.164799, mae: 0.361687, mean_q: 4.251341
 20934/100000: episode: 260, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 116.770, mean reward: 2.290 [1.617, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.558, 10.100], loss: 0.226757, mae: 0.393932, mean_q: 4.256484
 20983/100000: episode: 261, duration: 0.244s, episode steps: 49, steps per second: 201, episode reward: 155.600, mean reward: 3.176 [1.853, 6.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.322, 10.367], loss: 0.172122, mae: 0.403136, mean_q: 4.302790
 21032/100000: episode: 262, duration: 0.243s, episode steps: 49, steps per second: 202, episode reward: 136.445, mean reward: 2.785 [1.936, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.247, 10.337], loss: 0.166454, mae: 0.400490, mean_q: 4.345932
 21086/100000: episode: 263, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 130.228, mean reward: 2.412 [1.697, 4.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.860, 10.351], loss: 0.158741, mae: 0.395362, mean_q: 4.314940
 21127/100000: episode: 264, duration: 0.222s, episode steps: 41, steps per second: 184, episode reward: 89.402, mean reward: 2.181 [1.580, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.304, 10.199], loss: 0.205435, mae: 0.399202, mean_q: 4.328859
 21178/100000: episode: 265, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 120.444, mean reward: 2.362 [1.517, 7.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.795, 10.100], loss: 0.206853, mae: 0.407548, mean_q: 4.339458
 21232/100000: episode: 266, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 130.413, mean reward: 2.415 [1.533, 5.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.348, 10.133], loss: 0.185472, mae: 0.417213, mean_q: 4.355425
 21280/100000: episode: 267, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 173.788, mean reward: 3.621 [1.960, 8.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.321, 10.484], loss: 0.160840, mae: 0.411807, mean_q: 4.325713
 21334/100000: episode: 268, duration: 0.306s, episode steps: 54, steps per second: 176, episode reward: 122.519, mean reward: 2.269 [1.515, 5.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.611, 10.149], loss: 0.188601, mae: 0.430786, mean_q: 4.374559
 21388/100000: episode: 269, duration: 0.307s, episode steps: 54, steps per second: 176, episode reward: 128.962, mean reward: 2.388 [1.726, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.660, 10.266], loss: 0.186703, mae: 0.388081, mean_q: 4.347104
 21439/100000: episode: 270, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 97.989, mean reward: 1.921 [1.474, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.551, 10.304], loss: 0.159644, mae: 0.400994, mean_q: 4.322834
 21493/100000: episode: 271, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 132.765, mean reward: 2.459 [1.812, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.477, 10.313], loss: 0.158577, mae: 0.393814, mean_q: 4.358235
 21544/100000: episode: 272, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 121.156, mean reward: 2.376 [1.474, 4.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.715, 10.198], loss: 0.191203, mae: 0.424071, mean_q: 4.414350
 21595/100000: episode: 273, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 112.822, mean reward: 2.212 [1.528, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.341, 10.100], loss: 0.190789, mae: 0.397363, mean_q: 4.374468
 21646/100000: episode: 274, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 114.061, mean reward: 2.236 [1.561, 4.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.949, 10.383], loss: 0.158256, mae: 0.402025, mean_q: 4.392883
 21694/100000: episode: 275, duration: 0.253s, episode steps: 48, steps per second: 189, episode reward: 157.662, mean reward: 3.285 [2.329, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.254, 10.514], loss: 0.229694, mae: 0.426690, mean_q: 4.405918
 21748/100000: episode: 276, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 173.566, mean reward: 3.214 [1.927, 6.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.426, 10.375], loss: 0.231686, mae: 0.444112, mean_q: 4.410546
 21802/100000: episode: 277, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 120.761, mean reward: 2.236 [1.725, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.328, 10.281], loss: 0.204589, mae: 0.408485, mean_q: 4.457896
 21851/100000: episode: 278, duration: 0.255s, episode steps: 49, steps per second: 193, episode reward: 124.567, mean reward: 2.542 [1.661, 5.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.901, 10.368], loss: 0.212849, mae: 0.452768, mean_q: 4.473434
 21905/100000: episode: 279, duration: 0.291s, episode steps: 54, steps per second: 185, episode reward: 159.027, mean reward: 2.945 [1.719, 12.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-1.478, 10.363], loss: 0.214061, mae: 0.424445, mean_q: 4.437439
 21954/100000: episode: 280, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 103.393, mean reward: 2.110 [1.570, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.766, 10.100], loss: 0.253316, mae: 0.451959, mean_q: 4.481996
 21995/100000: episode: 281, duration: 0.225s, episode steps: 41, steps per second: 183, episode reward: 119.366, mean reward: 2.911 [1.776, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.127, 10.386], loss: 0.178221, mae: 0.405247, mean_q: 4.476682
 22043/100000: episode: 282, duration: 0.260s, episode steps: 48, steps per second: 184, episode reward: 122.014, mean reward: 2.542 [2.005, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.302, 10.358], loss: 0.169542, mae: 0.403712, mean_q: 4.449410
 22097/100000: episode: 283, duration: 0.295s, episode steps: 54, steps per second: 183, episode reward: 129.865, mean reward: 2.405 [1.894, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.442, 10.322], loss: 0.243919, mae: 0.451991, mean_q: 4.512051
 22138/100000: episode: 284, duration: 0.243s, episode steps: 41, steps per second: 169, episode reward: 117.845, mean reward: 2.874 [1.745, 5.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.495, 10.290], loss: 0.179043, mae: 0.410074, mean_q: 4.511091
 22192/100000: episode: 285, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 164.387, mean reward: 3.044 [1.954, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.231, 10.482], loss: 0.185342, mae: 0.413849, mean_q: 4.526356
 22240/100000: episode: 286, duration: 0.262s, episode steps: 48, steps per second: 184, episode reward: 89.256, mean reward: 1.859 [1.517, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.351, 10.153], loss: 0.205643, mae: 0.454220, mean_q: 4.540921
 22291/100000: episode: 287, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 112.430, mean reward: 2.205 [1.655, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.331, 10.186], loss: 0.179794, mae: 0.418477, mean_q: 4.560999
 22345/100000: episode: 288, duration: 0.284s, episode steps: 54, steps per second: 190, episode reward: 129.245, mean reward: 2.393 [1.488, 6.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-1.105, 10.100], loss: 0.232186, mae: 0.436229, mean_q: 4.551176
 22396/100000: episode: 289, duration: 0.258s, episode steps: 51, steps per second: 197, episode reward: 116.179, mean reward: 2.278 [1.571, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.593, 10.216], loss: 0.211484, mae: 0.422525, mean_q: 4.546977
 22450/100000: episode: 290, duration: 0.286s, episode steps: 54, steps per second: 189, episode reward: 133.890, mean reward: 2.479 [1.452, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.359, 10.235], loss: 0.156207, mae: 0.397917, mean_q: 4.442521
 22501/100000: episode: 291, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 113.271, mean reward: 2.221 [1.606, 5.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.373, 10.100], loss: 0.174522, mae: 0.419623, mean_q: 4.534431
 22555/100000: episode: 292, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 137.518, mean reward: 2.547 [1.556, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.271, 10.133], loss: 0.266651, mae: 0.479123, mean_q: 4.556628
 22606/100000: episode: 293, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 113.265, mean reward: 2.221 [1.534, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.634, 10.100], loss: 0.226732, mae: 0.450867, mean_q: 4.575105
 22657/100000: episode: 294, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 111.580, mean reward: 2.188 [1.737, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.394, 10.216], loss: 0.189250, mae: 0.430521, mean_q: 4.621343
 22711/100000: episode: 295, duration: 0.311s, episode steps: 54, steps per second: 173, episode reward: 158.800, mean reward: 2.941 [1.757, 10.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.397, 10.365], loss: 0.163381, mae: 0.405510, mean_q: 4.566107
 22759/100000: episode: 296, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 94.336, mean reward: 1.965 [1.443, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.484, 10.247], loss: 0.202742, mae: 0.417299, mean_q: 4.564589
 22813/100000: episode: 297, duration: 0.303s, episode steps: 54, steps per second: 178, episode reward: 172.057, mean reward: 3.186 [1.971, 6.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-1.068, 10.415], loss: 0.148707, mae: 0.387995, mean_q: 4.550217
 22867/100000: episode: 298, duration: 0.308s, episode steps: 54, steps per second: 175, episode reward: 133.878, mean reward: 2.479 [1.475, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.865, 10.100], loss: 0.182418, mae: 0.432725, mean_q: 4.589557
 22921/100000: episode: 299, duration: 0.288s, episode steps: 54, steps per second: 188, episode reward: 127.958, mean reward: 2.370 [1.465, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.736, 10.155], loss: 0.223016, mae: 0.430730, mean_q: 4.604235
 22969/100000: episode: 300, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 114.133, mean reward: 2.378 [1.876, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.475, 10.301], loss: 0.188620, mae: 0.416904, mean_q: 4.591861
 23020/100000: episode: 301, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 106.054, mean reward: 2.079 [1.482, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.681, 10.124], loss: 0.180059, mae: 0.420268, mean_q: 4.608397
 23074/100000: episode: 302, duration: 0.312s, episode steps: 54, steps per second: 173, episode reward: 134.935, mean reward: 2.499 [1.598, 5.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-1.025, 10.100], loss: 0.206373, mae: 0.430657, mean_q: 4.601591
 23125/100000: episode: 303, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 133.815, mean reward: 2.624 [1.899, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.493, 10.346], loss: 0.156273, mae: 0.411492, mean_q: 4.580596
 23179/100000: episode: 304, duration: 0.297s, episode steps: 54, steps per second: 182, episode reward: 117.098, mean reward: 2.168 [1.485, 4.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-1.088, 10.139], loss: 0.174549, mae: 0.402357, mean_q: 4.584731
 23233/100000: episode: 305, duration: 0.289s, episode steps: 54, steps per second: 187, episode reward: 153.474, mean reward: 2.842 [1.469, 9.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.147, 10.168], loss: 0.179916, mae: 0.415689, mean_q: 4.626399
 23287/100000: episode: 306, duration: 0.289s, episode steps: 54, steps per second: 187, episode reward: 168.757, mean reward: 3.125 [1.732, 5.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.398, 10.550], loss: 0.225107, mae: 0.440678, mean_q: 4.668992
 23341/100000: episode: 307, duration: 0.306s, episode steps: 54, steps per second: 177, episode reward: 123.506, mean reward: 2.287 [1.460, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.445, 10.100], loss: 0.261607, mae: 0.438239, mean_q: 4.663505
 23390/100000: episode: 308, duration: 0.279s, episode steps: 49, steps per second: 176, episode reward: 118.627, mean reward: 2.421 [1.913, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-1.136, 10.421], loss: 0.281168, mae: 0.435198, mean_q: 4.702080
 23444/100000: episode: 309, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 132.782, mean reward: 2.459 [1.561, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-1.583, 10.209], loss: 0.250603, mae: 0.454909, mean_q: 4.689930
 23498/100000: episode: 310, duration: 0.299s, episode steps: 54, steps per second: 181, episode reward: 127.865, mean reward: 2.368 [1.681, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.344, 10.413], loss: 0.246613, mae: 0.432272, mean_q: 4.701751
 23546/100000: episode: 311, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 139.710, mean reward: 2.911 [2.184, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.325, 10.369], loss: 0.207917, mae: 0.437001, mean_q: 4.683676
 23597/100000: episode: 312, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 100.609, mean reward: 1.973 [1.496, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.982, 10.129], loss: 0.166445, mae: 0.401484, mean_q: 4.647000
 23645/100000: episode: 313, duration: 0.238s, episode steps: 48, steps per second: 202, episode reward: 127.350, mean reward: 2.653 [2.096, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.669, 10.282], loss: 0.164162, mae: 0.406819, mean_q: 4.658888
 23696/100000: episode: 314, duration: 0.289s, episode steps: 51, steps per second: 176, episode reward: 145.573, mean reward: 2.854 [2.081, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.589, 10.376], loss: 0.198833, mae: 0.437345, mean_q: 4.689836
 23750/100000: episode: 315, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 125.146, mean reward: 2.318 [1.472, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.980, 10.207], loss: 0.208750, mae: 0.447996, mean_q: 4.712762
 23801/100000: episode: 316, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 105.440, mean reward: 2.067 [1.536, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.424, 10.105], loss: 0.175432, mae: 0.416698, mean_q: 4.766896
[Info] FALSIFICATION!
[Info] Levels: [4.321182, 5.9556336, 11.779379]
[Info] Cond. Prob: [0.1, 0.1, 1.0]
[Info] Error Prob: 0.010000000000000002

 23830/100000: episode: 317, duration: 4.673s, episode steps: 29, steps per second: 6, episode reward: 225.410, mean reward: 7.773 [2.274, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.741 [-0.734, 9.893], loss: 0.271540, mae: 0.435386, mean_q: 4.696481
 23930/100000: episode: 318, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 203.675, mean reward: 2.037 [1.453, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.530, 10.444], loss: 1.636143, mae: 0.551445, mean_q: 4.754048
 24030/100000: episode: 319, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 198.399, mean reward: 1.984 [1.459, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.533, 10.100], loss: 3.000461, mae: 0.649142, mean_q: 4.746288
 24130/100000: episode: 320, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 197.111, mean reward: 1.971 [1.442, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.786, 10.137], loss: 1.606397, mae: 0.539263, mean_q: 4.612817
 24230/100000: episode: 321, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 186.670, mean reward: 1.867 [1.452, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.473, 10.253], loss: 0.218407, mae: 0.436582, mean_q: 4.664409
 24330/100000: episode: 322, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 181.377, mean reward: 1.814 [1.470, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.526, 10.140], loss: 0.262819, mae: 0.441049, mean_q: 4.678932
 24430/100000: episode: 323, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 216.641, mean reward: 2.166 [1.447, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.249, 10.098], loss: 1.591950, mae: 0.527178, mean_q: 4.710351
 24530/100000: episode: 324, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 207.505, mean reward: 2.075 [1.534, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.437], loss: 0.271732, mae: 0.432901, mean_q: 4.676825
 24630/100000: episode: 325, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 187.679, mean reward: 1.877 [1.460, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.098], loss: 1.635365, mae: 0.541041, mean_q: 4.748855
 24730/100000: episode: 326, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 171.836, mean reward: 1.718 [1.446, 2.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.098], loss: 1.579907, mae: 0.538247, mean_q: 4.747399
 24830/100000: episode: 327, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.772, mean reward: 1.918 [1.482, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.504, 10.098], loss: 0.249553, mae: 0.425450, mean_q: 4.711904
 24930/100000: episode: 328, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 217.548, mean reward: 2.175 [1.499, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.927, 10.255], loss: 1.579564, mae: 0.507967, mean_q: 4.687546
 25030/100000: episode: 329, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 217.786, mean reward: 2.178 [1.501, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.266, 10.098], loss: 1.546589, mae: 0.480184, mean_q: 4.650389
 25130/100000: episode: 330, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.932, mean reward: 1.899 [1.447, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.143, 10.098], loss: 0.221675, mae: 0.431540, mean_q: 4.651140
 25230/100000: episode: 331, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.235, mean reward: 2.012 [1.471, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.142, 10.098], loss: 0.227739, mae: 0.438355, mean_q: 4.657809
 25330/100000: episode: 332, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.331, mean reward: 1.933 [1.462, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.223, 10.098], loss: 0.191210, mae: 0.413365, mean_q: 4.612626
 25430/100000: episode: 333, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 193.694, mean reward: 1.937 [1.448, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.708, 10.099], loss: 1.572678, mae: 0.520672, mean_q: 4.644058
 25530/100000: episode: 334, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 209.481, mean reward: 2.095 [1.448, 5.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.787, 10.188], loss: 1.537742, mae: 0.501905, mean_q: 4.550403
 25630/100000: episode: 335, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.256, mean reward: 1.883 [1.496, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.754, 10.098], loss: 0.177563, mae: 0.409956, mean_q: 4.519057
 25730/100000: episode: 336, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.731, mean reward: 1.897 [1.458, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.192, 10.118], loss: 0.171075, mae: 0.401570, mean_q: 4.526548
 25830/100000: episode: 337, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 217.654, mean reward: 2.177 [1.509, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.300], loss: 0.229496, mae: 0.431121, mean_q: 4.550146
 25930/100000: episode: 338, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.731, mean reward: 2.047 [1.474, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.098], loss: 1.540215, mae: 0.487361, mean_q: 4.511417
 26030/100000: episode: 339, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 215.732, mean reward: 2.157 [1.437, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.760, 10.098], loss: 0.211052, mae: 0.417234, mean_q: 4.453997
 26130/100000: episode: 340, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.380, mean reward: 2.044 [1.445, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.649, 10.098], loss: 0.198045, mae: 0.413461, mean_q: 4.459172
 26230/100000: episode: 341, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 189.798, mean reward: 1.898 [1.457, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.785, 10.098], loss: 0.191611, mae: 0.413209, mean_q: 4.431589
 26330/100000: episode: 342, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 195.683, mean reward: 1.957 [1.487, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.534, 10.098], loss: 0.208951, mae: 0.457939, mean_q: 4.431459
 26430/100000: episode: 343, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 195.949, mean reward: 1.959 [1.467, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.098], loss: 1.520319, mae: 0.490669, mean_q: 4.430914
 26530/100000: episode: 344, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 187.993, mean reward: 1.880 [1.458, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.098], loss: 0.174286, mae: 0.392566, mean_q: 4.329371
 26630/100000: episode: 345, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.643, mean reward: 2.046 [1.453, 7.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.174, 10.098], loss: 0.168134, mae: 0.388194, mean_q: 4.335325
 26730/100000: episode: 346, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 191.032, mean reward: 1.910 [1.451, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.997, 10.098], loss: 1.505061, mae: 0.462753, mean_q: 4.330603
 26830/100000: episode: 347, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.937, mean reward: 1.869 [1.467, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.453, 10.167], loss: 0.151906, mae: 0.379560, mean_q: 4.310037
 26930/100000: episode: 348, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 224.778, mean reward: 2.248 [1.496, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.225, 10.206], loss: 0.156836, mae: 0.392018, mean_q: 4.304574
 27030/100000: episode: 349, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 214.010, mean reward: 2.140 [1.434, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.300], loss: 0.150180, mae: 0.374870, mean_q: 4.291584
 27130/100000: episode: 350, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.434, mean reward: 1.904 [1.459, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.175, 10.098], loss: 0.136687, mae: 0.363452, mean_q: 4.259111
 27230/100000: episode: 351, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 193.216, mean reward: 1.932 [1.445, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.107], loss: 1.493841, mae: 0.460593, mean_q: 4.237141
 27330/100000: episode: 352, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.572, mean reward: 1.826 [1.448, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.852, 10.148], loss: 0.170510, mae: 0.394224, mean_q: 4.236393
 27430/100000: episode: 353, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 183.336, mean reward: 1.833 [1.447, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.321], loss: 0.159699, mae: 0.377566, mean_q: 4.200962
 27530/100000: episode: 354, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.648, mean reward: 1.846 [1.440, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.580, 10.192], loss: 0.159645, mae: 0.386342, mean_q: 4.213558
 27630/100000: episode: 355, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 181.495, mean reward: 1.815 [1.450, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.848, 10.230], loss: 2.804507, mae: 0.501551, mean_q: 4.193943
 27730/100000: episode: 356, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 189.093, mean reward: 1.891 [1.490, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.098], loss: 2.824795, mae: 0.557379, mean_q: 4.223085
 27830/100000: episode: 357, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 189.581, mean reward: 1.896 [1.480, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.735, 10.098], loss: 0.139096, mae: 0.369485, mean_q: 4.127270
 27930/100000: episode: 358, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 190.659, mean reward: 1.907 [1.469, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.768, 10.098], loss: 0.130773, mae: 0.361066, mean_q: 4.098572
 28030/100000: episode: 359, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.713, mean reward: 1.877 [1.452, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.755, 10.277], loss: 0.127232, mae: 0.344626, mean_q: 4.053706
 28130/100000: episode: 360, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.555, mean reward: 1.936 [1.473, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.108, 10.224], loss: 2.776465, mae: 0.548047, mean_q: 4.124281
 28230/100000: episode: 361, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.462, mean reward: 1.865 [1.492, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.418, 10.098], loss: 1.462868, mae: 0.441439, mean_q: 4.024503
 28330/100000: episode: 362, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.221, mean reward: 1.872 [1.450, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.592, 10.169], loss: 1.451211, mae: 0.439995, mean_q: 4.035979
 28430/100000: episode: 363, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 192.532, mean reward: 1.925 [1.481, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.231, 10.245], loss: 0.133863, mae: 0.352350, mean_q: 4.004691
 28530/100000: episode: 364, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.918, mean reward: 1.949 [1.477, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.906, 10.098], loss: 0.113598, mae: 0.333103, mean_q: 3.966779
 28630/100000: episode: 365, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 190.653, mean reward: 1.907 [1.454, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.737, 10.098], loss: 0.112790, mae: 0.328157, mean_q: 3.942278
 28730/100000: episode: 366, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 185.357, mean reward: 1.854 [1.449, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.005, 10.106], loss: 0.108004, mae: 0.324189, mean_q: 3.897187
 28830/100000: episode: 367, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 201.978, mean reward: 2.020 [1.462, 5.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.588, 10.194], loss: 0.103053, mae: 0.323568, mean_q: 3.873557
 28930/100000: episode: 368, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 188.584, mean reward: 1.886 [1.435, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.079, 10.276], loss: 0.115471, mae: 0.330688, mean_q: 3.880701
 29030/100000: episode: 369, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 206.172, mean reward: 2.062 [1.460, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.068, 10.260], loss: 0.116886, mae: 0.326707, mean_q: 3.872573
 29130/100000: episode: 370, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.071, mean reward: 1.901 [1.466, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.569, 10.098], loss: 0.103003, mae: 0.318572, mean_q: 3.879700
 29230/100000: episode: 371, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.499, mean reward: 1.915 [1.473, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.497, 10.381], loss: 0.117813, mae: 0.323151, mean_q: 3.878696
 29330/100000: episode: 372, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.689, mean reward: 2.007 [1.451, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.352, 10.185], loss: 0.100087, mae: 0.316572, mean_q: 3.874162
 29430/100000: episode: 373, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 187.416, mean reward: 1.874 [1.488, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.657, 10.210], loss: 0.096813, mae: 0.309062, mean_q: 3.866790
 29530/100000: episode: 374, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 243.866, mean reward: 2.439 [1.446, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.080, 10.098], loss: 0.096257, mae: 0.307106, mean_q: 3.864398
 29630/100000: episode: 375, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 197.020, mean reward: 1.970 [1.470, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.189], loss: 0.111631, mae: 0.327923, mean_q: 3.870750
 29730/100000: episode: 376, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.386, mean reward: 1.894 [1.456, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.258, 10.098], loss: 0.106233, mae: 0.326001, mean_q: 3.876374
 29830/100000: episode: 377, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.179, mean reward: 1.902 [1.452, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.537, 10.098], loss: 0.104803, mae: 0.320560, mean_q: 3.891495
 29930/100000: episode: 378, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 181.286, mean reward: 1.813 [1.450, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.865, 10.141], loss: 0.090994, mae: 0.307204, mean_q: 3.869895
 30030/100000: episode: 379, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 237.077, mean reward: 2.371 [1.502, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.685, 10.098], loss: 0.110075, mae: 0.323864, mean_q: 3.880725
 30130/100000: episode: 380, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 204.527, mean reward: 2.045 [1.454, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.834, 10.359], loss: 0.102790, mae: 0.314946, mean_q: 3.864002
 30230/100000: episode: 381, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 181.270, mean reward: 1.813 [1.460, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.288], loss: 0.104535, mae: 0.314267, mean_q: 3.871592
 30330/100000: episode: 382, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 196.390, mean reward: 1.964 [1.459, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.201], loss: 0.118923, mae: 0.331476, mean_q: 3.863738
 30430/100000: episode: 383, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 220.968, mean reward: 2.210 [1.458, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.940, 10.098], loss: 0.114706, mae: 0.325962, mean_q: 3.907167
 30530/100000: episode: 384, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.020, mean reward: 1.830 [1.449, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.944, 10.118], loss: 0.093463, mae: 0.303976, mean_q: 3.892887
 30630/100000: episode: 385, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 177.752, mean reward: 1.778 [1.447, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.203, 10.098], loss: 0.095017, mae: 0.315364, mean_q: 3.880552
 30730/100000: episode: 386, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.851, mean reward: 2.009 [1.448, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.386, 10.127], loss: 0.103478, mae: 0.316131, mean_q: 3.867460
 30830/100000: episode: 387, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 187.969, mean reward: 1.880 [1.481, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.410, 10.184], loss: 0.107785, mae: 0.333864, mean_q: 3.876263
 30930/100000: episode: 388, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 179.653, mean reward: 1.797 [1.433, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.033, 10.179], loss: 0.096635, mae: 0.311854, mean_q: 3.867351
 31030/100000: episode: 389, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 186.837, mean reward: 1.868 [1.447, 2.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.418, 10.192], loss: 0.092569, mae: 0.307273, mean_q: 3.857885
 31130/100000: episode: 390, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 288.974, mean reward: 2.890 [1.535, 25.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.905, 10.597], loss: 0.089514, mae: 0.303273, mean_q: 3.849656
 31230/100000: episode: 391, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.552, mean reward: 1.906 [1.443, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.184, 10.098], loss: 0.231552, mae: 0.351844, mean_q: 3.893275
 31330/100000: episode: 392, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.238, mean reward: 1.852 [1.492, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.098], loss: 0.211351, mae: 0.336880, mean_q: 3.875411
 31430/100000: episode: 393, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 204.435, mean reward: 2.044 [1.474, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.306, 10.098], loss: 0.238763, mae: 0.360149, mean_q: 3.896588
 31530/100000: episode: 394, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 227.137, mean reward: 2.271 [1.504, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.810, 10.577], loss: 0.316164, mae: 0.369815, mean_q: 3.890002
 31630/100000: episode: 395, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 197.140, mean reward: 1.971 [1.446, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.512, 10.098], loss: 0.220407, mae: 0.339833, mean_q: 3.909893
 31730/100000: episode: 396, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 182.235, mean reward: 1.822 [1.464, 2.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.489, 10.109], loss: 0.125766, mae: 0.314450, mean_q: 3.883789
 31830/100000: episode: 397, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 192.800, mean reward: 1.928 [1.468, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.366], loss: 0.207844, mae: 0.345349, mean_q: 3.909899
 31930/100000: episode: 398, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 217.708, mean reward: 2.177 [1.489, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.530, 10.325], loss: 0.254882, mae: 0.323857, mean_q: 3.880646
 32030/100000: episode: 399, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 187.136, mean reward: 1.871 [1.467, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.472, 10.098], loss: 0.085387, mae: 0.291209, mean_q: 3.848334
 32130/100000: episode: 400, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 192.294, mean reward: 1.923 [1.439, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.098], loss: 0.118817, mae: 0.317367, mean_q: 3.846347
 32230/100000: episode: 401, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 193.278, mean reward: 1.933 [1.444, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.269], loss: 0.119874, mae: 0.316208, mean_q: 3.873730
 32330/100000: episode: 402, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.327, mean reward: 1.873 [1.444, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.817, 10.098], loss: 0.125458, mae: 0.321151, mean_q: 3.882987
 32430/100000: episode: 403, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.084, mean reward: 1.891 [1.497, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.300], loss: 0.157048, mae: 0.329601, mean_q: 3.890134
 32530/100000: episode: 404, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 197.728, mean reward: 1.977 [1.451, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.398, 10.098], loss: 0.238343, mae: 0.352398, mean_q: 3.894317
 32630/100000: episode: 405, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 186.956, mean reward: 1.870 [1.506, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.878, 10.098], loss: 0.209531, mae: 0.352102, mean_q: 3.912164
 32730/100000: episode: 406, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 217.100, mean reward: 2.171 [1.459, 5.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.424, 10.577], loss: 0.252958, mae: 0.361794, mean_q: 3.915870
 32830/100000: episode: 407, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.461, mean reward: 1.835 [1.445, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.867, 10.098], loss: 0.281857, mae: 0.360177, mean_q: 3.915018
 32930/100000: episode: 408, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 194.689, mean reward: 1.947 [1.513, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.795, 10.344], loss: 0.188124, mae: 0.334019, mean_q: 3.883844
 33030/100000: episode: 409, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.021, mean reward: 1.880 [1.488, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.317, 10.098], loss: 0.139645, mae: 0.326537, mean_q: 3.895639
 33130/100000: episode: 410, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.307, mean reward: 1.923 [1.449, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.364, 10.098], loss: 0.130741, mae: 0.320700, mean_q: 3.885156
 33230/100000: episode: 411, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 223.510, mean reward: 2.235 [1.519, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.481, 10.295], loss: 0.251635, mae: 0.363554, mean_q: 3.927956
 33330/100000: episode: 412, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 187.673, mean reward: 1.877 [1.469, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.666, 10.193], loss: 0.142081, mae: 0.337342, mean_q: 3.954499
 33430/100000: episode: 413, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.607, mean reward: 1.906 [1.457, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.098], loss: 0.128361, mae: 0.326104, mean_q: 3.921131
 33530/100000: episode: 414, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 190.322, mean reward: 1.903 [1.443, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.220], loss: 0.137545, mae: 0.328342, mean_q: 3.928708
 33630/100000: episode: 415, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.411, mean reward: 1.814 [1.466, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.943, 10.135], loss: 0.130454, mae: 0.330764, mean_q: 3.940958
 33730/100000: episode: 416, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 204.218, mean reward: 2.042 [1.475, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.521, 10.279], loss: 0.131392, mae: 0.320664, mean_q: 3.924491
[Info] 1-TH LEVEL FOUND: 5.224785804748535, Considering 10/90 traces
 33830/100000: episode: 417, duration: 4.628s, episode steps: 100, steps per second: 22, episode reward: 189.435, mean reward: 1.894 [1.467, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.882, 10.187], loss: 0.124370, mae: 0.324716, mean_q: 3.922342
 33857/100000: episode: 418, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 51.550, mean reward: 1.909 [1.478, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.120, 10.142], loss: 0.679253, mae: 0.440717, mean_q: 3.956038
 33867/100000: episode: 419, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 26.845, mean reward: 2.685 [1.915, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.295], loss: 0.142903, mae: 0.396413, mean_q: 3.830173
 33877/100000: episode: 420, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 22.999, mean reward: 2.300 [1.770, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.077, 10.294], loss: 0.919324, mae: 0.476794, mean_q: 4.096995
 33928/100000: episode: 421, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 104.128, mean reward: 2.042 [1.502, 8.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.281, 10.144], loss: 0.140280, mae: 0.338752, mean_q: 3.923344
 33938/100000: episode: 422, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 26.131, mean reward: 2.613 [1.981, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.366], loss: 0.116743, mae: 0.343995, mean_q: 3.927179
 33970/100000: episode: 423, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 74.321, mean reward: 2.323 [1.778, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.220], loss: 0.184073, mae: 0.342256, mean_q: 3.946897
 33997/100000: episode: 424, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 88.141, mean reward: 3.264 [1.981, 5.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.925, 10.615], loss: 0.109165, mae: 0.301773, mean_q: 3.864477
 34049/100000: episode: 425, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 113.169, mean reward: 2.176 [1.564, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-1.150, 10.100], loss: 0.414986, mae: 0.392213, mean_q: 3.977943
 34100/100000: episode: 426, duration: 0.294s, episode steps: 51, steps per second: 174, episode reward: 103.982, mean reward: 2.039 [1.471, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.826, 10.100], loss: 0.120933, mae: 0.316351, mean_q: 3.930064
 34110/100000: episode: 427, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 23.847, mean reward: 2.385 [2.082, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.331], loss: 0.089890, mae: 0.315700, mean_q: 3.906777
 34142/100000: episode: 428, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 114.154, mean reward: 3.567 [2.559, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.242, 10.541], loss: 0.139460, mae: 0.315794, mean_q: 3.947851
 34193/100000: episode: 429, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 120.257, mean reward: 2.358 [1.820, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.355, 10.400], loss: 0.135667, mae: 0.342278, mean_q: 3.974164
 34203/100000: episode: 430, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 20.308, mean reward: 2.031 [1.711, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.254], loss: 0.159566, mae: 0.338423, mean_q: 4.036765
 34219/100000: episode: 431, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 35.785, mean reward: 2.237 [1.784, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.234], loss: 0.088198, mae: 0.308783, mean_q: 3.905535
 34251/100000: episode: 432, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 137.847, mean reward: 4.308 [2.139, 7.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.345, 10.669], loss: 0.127076, mae: 0.325248, mean_q: 4.001830
 34282/100000: episode: 433, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 81.999, mean reward: 2.645 [1.829, 8.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.737, 10.387], loss: 0.108298, mae: 0.322060, mean_q: 3.972294
 34336/100000: episode: 434, duration: 0.279s, episode steps: 54, steps per second: 194, episode reward: 116.623, mean reward: 2.160 [1.539, 7.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-1.265, 10.100], loss: 0.150418, mae: 0.341000, mean_q: 4.007253
 34350/100000: episode: 435, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 38.828, mean reward: 2.773 [2.194, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.509], loss: 0.137543, mae: 0.381186, mean_q: 4.061610
 34356/100000: episode: 436, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 13.473, mean reward: 2.245 [2.031, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.312], loss: 1.432489, mae: 0.592028, mean_q: 4.271515
 34408/100000: episode: 437, duration: 0.312s, episode steps: 52, steps per second: 167, episode reward: 108.140, mean reward: 2.080 [1.505, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.316, 10.128], loss: 0.362284, mae: 0.432204, mean_q: 4.043591
 34440/100000: episode: 438, duration: 0.174s, episode steps: 32, steps per second: 183, episode reward: 92.718, mean reward: 2.897 [2.074, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.993, 10.429], loss: 0.140385, mae: 0.352320, mean_q: 4.041174
 34472/100000: episode: 439, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 79.588, mean reward: 2.487 [1.728, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.338, 10.260], loss: 0.119205, mae: 0.314625, mean_q: 4.021420
 34504/100000: episode: 440, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 75.101, mean reward: 2.347 [1.957, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.035, 10.286], loss: 0.135071, mae: 0.323411, mean_q: 3.993217
 34535/100000: episode: 441, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 87.749, mean reward: 2.831 [1.565, 5.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.123, 10.127], loss: 0.135426, mae: 0.351359, mean_q: 4.006433
 34545/100000: episode: 442, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 26.031, mean reward: 2.603 [2.103, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.356], loss: 0.109879, mae: 0.330901, mean_q: 3.925067
 34572/100000: episode: 443, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 57.778, mean reward: 2.140 [1.778, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.486, 10.301], loss: 0.179001, mae: 0.368624, mean_q: 4.121716
 34624/100000: episode: 444, duration: 0.295s, episode steps: 52, steps per second: 176, episode reward: 126.327, mean reward: 2.429 [1.553, 5.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.324, 10.100], loss: 0.169390, mae: 0.368481, mean_q: 4.057905
 34678/100000: episode: 445, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 117.231, mean reward: 2.171 [1.507, 5.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.254, 10.100], loss: 0.123357, mae: 0.338573, mean_q: 4.033616
 34705/100000: episode: 446, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 62.156, mean reward: 2.302 [1.804, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.494, 10.313], loss: 0.105563, mae: 0.314949, mean_q: 4.010471
 34715/100000: episode: 447, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 22.337, mean reward: 2.234 [1.785, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.348], loss: 0.184751, mae: 0.374021, mean_q: 4.112411
 34746/100000: episode: 448, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 72.714, mean reward: 2.346 [1.475, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.198, 10.130], loss: 0.163420, mae: 0.359040, mean_q: 4.050215
 34797/100000: episode: 449, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 106.718, mean reward: 2.093 [1.538, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.516, 10.231], loss: 0.135201, mae: 0.329309, mean_q: 4.049866
 34828/100000: episode: 450, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 92.922, mean reward: 2.997 [2.169, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.234, 10.361], loss: 0.171025, mae: 0.352128, mean_q: 4.026905
 34838/100000: episode: 451, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 23.259, mean reward: 2.326 [1.624, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.194], loss: 0.120352, mae: 0.337667, mean_q: 4.058178
 34892/100000: episode: 452, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 131.060, mean reward: 2.427 [1.909, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-1.498, 10.300], loss: 0.277845, mae: 0.356589, mean_q: 4.060015
 34943/100000: episode: 453, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 98.930, mean reward: 1.940 [1.481, 4.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.264, 10.100], loss: 0.130665, mae: 0.355517, mean_q: 4.050688
 34953/100000: episode: 454, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 23.597, mean reward: 2.360 [1.810, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.296], loss: 0.305330, mae: 0.431462, mean_q: 4.291046
 34985/100000: episode: 455, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 81.117, mean reward: 2.535 [1.998, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.140, 10.366], loss: 0.139356, mae: 0.355388, mean_q: 4.045609
 35017/100000: episode: 456, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 91.882, mean reward: 2.871 [1.897, 5.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.387, 10.403], loss: 0.378662, mae: 0.392673, mean_q: 4.152245
 35031/100000: episode: 457, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 36.550, mean reward: 2.611 [2.247, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.468], loss: 0.109168, mae: 0.306948, mean_q: 4.065960
 35063/100000: episode: 458, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 74.565, mean reward: 2.330 [1.587, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.374, 10.184], loss: 0.149396, mae: 0.370311, mean_q: 4.077803
 35073/100000: episode: 459, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 28.357, mean reward: 2.836 [2.170, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.434], loss: 0.154759, mae: 0.358013, mean_q: 4.020440
 35083/100000: episode: 460, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 26.924, mean reward: 2.692 [2.446, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.417], loss: 0.344683, mae: 0.448846, mean_q: 4.325443
 35114/100000: episode: 461, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 71.615, mean reward: 2.310 [1.833, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.715, 10.332], loss: 0.163552, mae: 0.363660, mean_q: 4.085546
 35128/100000: episode: 462, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 47.575, mean reward: 3.398 [2.636, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.553], loss: 0.691038, mae: 0.446892, mean_q: 4.094409
 35155/100000: episode: 463, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 62.844, mean reward: 2.328 [1.952, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.273, 10.425], loss: 0.166521, mae: 0.378013, mean_q: 4.103814
 35161/100000: episode: 464, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 15.566, mean reward: 2.594 [2.181, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.447], loss: 0.139838, mae: 0.357793, mean_q: 3.913195
 35167/100000: episode: 465, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 14.490, mean reward: 2.415 [2.177, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.254], loss: 0.153111, mae: 0.400657, mean_q: 4.176073
 35219/100000: episode: 466, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 131.229, mean reward: 2.524 [1.566, 4.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.931, 10.183], loss: 0.144391, mae: 0.361865, mean_q: 4.156591
 35229/100000: episode: 467, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 21.935, mean reward: 2.193 [1.799, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-1.029, 10.294], loss: 0.128877, mae: 0.306094, mean_q: 4.059866
 35280/100000: episode: 468, duration: 0.287s, episode steps: 51, steps per second: 177, episode reward: 107.147, mean reward: 2.101 [1.669, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.807, 10.242], loss: 0.138305, mae: 0.336623, mean_q: 4.048659
 35334/100000: episode: 469, duration: 0.318s, episode steps: 54, steps per second: 170, episode reward: 102.357, mean reward: 1.895 [1.452, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-1.063, 10.100], loss: 0.282505, mae: 0.385994, mean_q: 4.130163
 35386/100000: episode: 470, duration: 0.273s, episode steps: 52, steps per second: 191, episode reward: 99.050, mean reward: 1.905 [1.451, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.534, 10.115], loss: 0.293069, mae: 0.380607, mean_q: 4.153367
 35413/100000: episode: 471, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 75.157, mean reward: 2.784 [2.032, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.353], loss: 0.191500, mae: 0.378406, mean_q: 4.083388
 35440/100000: episode: 472, duration: 0.155s, episode steps: 27, steps per second: 175, episode reward: 79.506, mean reward: 2.945 [2.045, 5.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.095, 10.500], loss: 0.155180, mae: 0.375316, mean_q: 4.126502
 35450/100000: episode: 473, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 26.093, mean reward: 2.609 [2.098, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.535, 10.262], loss: 0.192150, mae: 0.368704, mean_q: 4.190863
 35502/100000: episode: 474, duration: 0.295s, episode steps: 52, steps per second: 176, episode reward: 117.042, mean reward: 2.251 [1.504, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.838, 10.266], loss: 0.152278, mae: 0.357525, mean_q: 4.205967
 35556/100000: episode: 475, duration: 0.296s, episode steps: 54, steps per second: 182, episode reward: 142.917, mean reward: 2.647 [1.521, 9.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.630, 10.100], loss: 0.270756, mae: 0.386715, mean_q: 4.192463
 35566/100000: episode: 476, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 29.003, mean reward: 2.900 [2.070, 5.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.101, 10.339], loss: 0.192435, mae: 0.345288, mean_q: 4.118149
 35617/100000: episode: 477, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 108.328, mean reward: 2.124 [1.497, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.369, 10.203], loss: 0.168762, mae: 0.356949, mean_q: 4.142543
 35669/100000: episode: 478, duration: 0.274s, episode steps: 52, steps per second: 190, episode reward: 101.884, mean reward: 1.959 [1.524, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.357, 10.103], loss: 0.242646, mae: 0.399508, mean_q: 4.159982
 35723/100000: episode: 479, duration: 0.294s, episode steps: 54, steps per second: 183, episode reward: 110.348, mean reward: 2.043 [1.501, 5.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.312, 10.100], loss: 0.150470, mae: 0.357266, mean_q: 4.173468
 35750/100000: episode: 480, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 67.815, mean reward: 2.512 [1.896, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.298, 10.307], loss: 0.154939, mae: 0.357495, mean_q: 4.198654
 35804/100000: episode: 481, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 113.227, mean reward: 2.097 [1.532, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.548, 10.192], loss: 0.163577, mae: 0.366421, mean_q: 4.152975
 35855/100000: episode: 482, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 107.981, mean reward: 2.117 [1.553, 5.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.274, 10.321], loss: 0.158445, mae: 0.361461, mean_q: 4.194233
 35909/100000: episode: 483, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 130.268, mean reward: 2.412 [1.714, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.363, 10.299], loss: 0.183703, mae: 0.382813, mean_q: 4.204214
 35940/100000: episode: 484, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 66.276, mean reward: 2.138 [1.588, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.393, 10.291], loss: 0.172775, mae: 0.355156, mean_q: 4.168365
 35956/100000: episode: 485, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 34.276, mean reward: 2.142 [1.544, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.256], loss: 0.142772, mae: 0.390521, mean_q: 4.244531
 35983/100000: episode: 486, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 62.626, mean reward: 2.319 [1.582, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.274, 10.110], loss: 0.158701, mae: 0.364727, mean_q: 4.196761
 36014/100000: episode: 487, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 87.345, mean reward: 2.818 [2.104, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.879, 10.390], loss: 0.099472, mae: 0.319593, mean_q: 4.155532
 36020/100000: episode: 488, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 15.015, mean reward: 2.503 [2.379, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.367], loss: 0.234354, mae: 0.381977, mean_q: 4.354612
 36072/100000: episode: 489, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 135.727, mean reward: 2.610 [1.458, 13.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.634, 10.220], loss: 0.144115, mae: 0.357269, mean_q: 4.171686
 36088/100000: episode: 490, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 42.406, mean reward: 2.650 [1.653, 7.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.073, 10.212], loss: 0.186952, mae: 0.370398, mean_q: 4.236669
 36140/100000: episode: 491, duration: 0.279s, episode steps: 52, steps per second: 187, episode reward: 109.368, mean reward: 2.103 [1.493, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.347, 10.224], loss: 0.129657, mae: 0.358256, mean_q: 4.195319
 36171/100000: episode: 492, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 78.133, mean reward: 2.520 [1.886, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.367], loss: 0.126630, mae: 0.353136, mean_q: 4.242392
 36198/100000: episode: 493, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 61.418, mean reward: 2.275 [1.872, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.076, 10.345], loss: 0.183615, mae: 0.375566, mean_q: 4.183262
 36204/100000: episode: 494, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 16.207, mean reward: 2.701 [2.231, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.241], loss: 0.096989, mae: 0.299209, mean_q: 4.063383
 36220/100000: episode: 495, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 36.636, mean reward: 2.290 [1.967, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.452, 10.340], loss: 0.177192, mae: 0.387391, mean_q: 4.256687
 36271/100000: episode: 496, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 100.891, mean reward: 1.978 [1.558, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.420, 10.290], loss: 0.163670, mae: 0.351645, mean_q: 4.206832
 36302/100000: episode: 497, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 74.147, mean reward: 2.392 [1.573, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.616, 10.225], loss: 0.214508, mae: 0.365986, mean_q: 4.229645
 36333/100000: episode: 498, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 100.972, mean reward: 3.257 [2.517, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-2.143, 10.439], loss: 0.169326, mae: 0.353744, mean_q: 4.210566
 36347/100000: episode: 499, duration: 0.087s, episode steps: 14, steps per second: 162, episode reward: 32.475, mean reward: 2.320 [2.052, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.338], loss: 0.119275, mae: 0.334967, mean_q: 4.273038
 36357/100000: episode: 500, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 23.141, mean reward: 2.314 [1.961, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.313], loss: 0.117262, mae: 0.354450, mean_q: 4.304786
 36384/100000: episode: 501, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 79.793, mean reward: 2.955 [2.111, 6.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.726, 10.473], loss: 0.181437, mae: 0.396284, mean_q: 4.289849
 36394/100000: episode: 502, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 23.219, mean reward: 2.322 [2.090, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.369], loss: 0.125122, mae: 0.310126, mean_q: 4.089007
 36425/100000: episode: 503, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 80.878, mean reward: 2.609 [1.641, 7.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.174, 10.184], loss: 0.145133, mae: 0.361124, mean_q: 4.268920
 36479/100000: episode: 504, duration: 0.293s, episode steps: 54, steps per second: 184, episode reward: 139.661, mean reward: 2.586 [1.813, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-1.144, 10.422], loss: 0.155693, mae: 0.347295, mean_q: 4.250942
 36531/100000: episode: 505, duration: 0.294s, episode steps: 52, steps per second: 177, episode reward: 125.897, mean reward: 2.421 [1.778, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.444, 10.336], loss: 0.291971, mae: 0.420677, mean_q: 4.269639
 36563/100000: episode: 506, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 70.879, mean reward: 2.215 [1.716, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.725, 10.288], loss: 0.253149, mae: 0.380266, mean_q: 4.273439
[Info] 2-TH LEVEL FOUND: 6.584854602813721, Considering 10/90 traces
 36590/100000: episode: 507, duration: 4.219s, episode steps: 27, steps per second: 6, episode reward: 76.158, mean reward: 2.821 [1.712, 4.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.100, 10.509], loss: 0.219381, mae: 0.398648, mean_q: 4.310324
 36607/100000: episode: 508, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 55.590, mean reward: 3.270 [2.224, 4.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.832, 10.527], loss: 0.106350, mae: 0.340216, mean_q: 4.219966
 36624/100000: episode: 509, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 51.803, mean reward: 3.047 [2.129, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.200, 10.472], loss: 0.210754, mae: 0.391920, mean_q: 4.320641
 36641/100000: episode: 510, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 38.991, mean reward: 2.294 [1.675, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.567, 10.303], loss: 0.151819, mae: 0.369577, mean_q: 4.323428
 36655/100000: episode: 511, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 42.378, mean reward: 3.027 [2.489, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.404, 10.491], loss: 0.178129, mae: 0.344004, mean_q: 4.264035
 36672/100000: episode: 512, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 66.739, mean reward: 3.926 [2.886, 6.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.763, 10.486], loss: 0.117716, mae: 0.345678, mean_q: 4.338009
 36697/100000: episode: 513, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 82.090, mean reward: 3.284 [2.412, 5.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.502, 10.501], loss: 0.128747, mae: 0.357968, mean_q: 4.310742
 36714/100000: episode: 514, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 49.076, mean reward: 2.887 [2.353, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.252, 10.542], loss: 0.319215, mae: 0.456394, mean_q: 4.388920
 36739/100000: episode: 515, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 78.667, mean reward: 3.147 [2.291, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.470], loss: 0.123405, mae: 0.361185, mean_q: 4.385589
 36763/100000: episode: 516, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 53.950, mean reward: 2.248 [1.711, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.315], loss: 0.165217, mae: 0.395684, mean_q: 4.362010
 36805/100000: episode: 517, duration: 0.233s, episode steps: 42, steps per second: 181, episode reward: 99.394, mean reward: 2.367 [1.600, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.559, 10.358], loss: 0.226615, mae: 0.396252, mean_q: 4.370553
 36835/100000: episode: 518, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 86.333, mean reward: 2.878 [2.152, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.359], loss: 0.144378, mae: 0.363514, mean_q: 4.354484
 36849/100000: episode: 519, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 49.395, mean reward: 3.528 [2.549, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.474], loss: 0.317239, mae: 0.438144, mean_q: 4.420233
 36891/100000: episode: 520, duration: 0.228s, episode steps: 42, steps per second: 185, episode reward: 117.864, mean reward: 2.806 [1.563, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.357, 10.186], loss: 0.142527, mae: 0.362323, mean_q: 4.336578
 36933/100000: episode: 521, duration: 0.234s, episode steps: 42, steps per second: 180, episode reward: 122.740, mean reward: 2.922 [1.811, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.398, 10.272], loss: 0.212347, mae: 0.405945, mean_q: 4.417272
 36957/100000: episode: 522, duration: 0.147s, episode steps: 24, steps per second: 164, episode reward: 83.143, mean reward: 3.464 [2.980, 4.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.039, 10.479], loss: 0.181476, mae: 0.402592, mean_q: 4.408899
 36971/100000: episode: 523, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 42.331, mean reward: 3.024 [1.979, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.148, 10.410], loss: 0.140614, mae: 0.373196, mean_q: 4.403917
 37013/100000: episode: 524, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 92.599, mean reward: 2.205 [1.540, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.735, 10.100], loss: 0.127290, mae: 0.354643, mean_q: 4.416412
 37030/100000: episode: 525, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 59.892, mean reward: 3.523 [2.407, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.859, 10.504], loss: 0.180360, mae: 0.374593, mean_q: 4.446630
 37054/100000: episode: 526, duration: 0.138s, episode steps: 24, steps per second: 175, episode reward: 60.615, mean reward: 2.526 [1.876, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.464, 10.339], loss: 0.158075, mae: 0.359899, mean_q: 4.436568
 37079/100000: episode: 527, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 76.706, mean reward: 3.068 [2.419, 5.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.587], loss: 0.134798, mae: 0.346345, mean_q: 4.367795
 37121/100000: episode: 528, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 109.665, mean reward: 2.611 [1.719, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.792, 10.286], loss: 0.192784, mae: 0.375439, mean_q: 4.428997
 37145/100000: episode: 529, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 141.837, mean reward: 5.910 [2.554, 15.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.118, 10.625], loss: 0.158639, mae: 0.366903, mean_q: 4.408981
 37169/100000: episode: 530, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 69.570, mean reward: 2.899 [2.109, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.399], loss: 0.338292, mae: 0.418663, mean_q: 4.500958
 37194/100000: episode: 531, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 75.650, mean reward: 3.026 [2.400, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.167, 10.553], loss: 0.185870, mae: 0.414980, mean_q: 4.471755
 37218/100000: episode: 532, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 59.765, mean reward: 2.490 [1.720, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.306], loss: 0.131591, mae: 0.369086, mean_q: 4.571248
 37242/100000: episode: 533, duration: 0.134s, episode steps: 24, steps per second: 178, episode reward: 71.230, mean reward: 2.968 [2.127, 8.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.043, 10.340], loss: 0.241159, mae: 0.406499, mean_q: 4.491070
 37266/100000: episode: 534, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 88.935, mean reward: 3.706 [2.640, 7.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.330, 10.545], loss: 0.269475, mae: 0.384629, mean_q: 4.556987
 37295/100000: episode: 535, duration: 0.185s, episode steps: 29, steps per second: 157, episode reward: 73.251, mean reward: 2.526 [1.678, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.510, 10.344], loss: 0.193988, mae: 0.417999, mean_q: 4.617698
 37337/100000: episode: 536, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 104.181, mean reward: 2.480 [1.443, 4.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.466, 10.315], loss: 0.176942, mae: 0.380275, mean_q: 4.445965
 37368/100000: episode: 537, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 95.075, mean reward: 3.067 [2.309, 4.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.434, 10.386], loss: 0.286885, mae: 0.448216, mean_q: 4.533813
 37397/100000: episode: 538, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 97.966, mean reward: 3.378 [2.652, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.958, 10.487], loss: 0.254670, mae: 0.426096, mean_q: 4.634360
 37414/100000: episode: 539, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 63.957, mean reward: 3.762 [2.503, 7.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.606], loss: 0.166722, mae: 0.383552, mean_q: 4.537797
 37439/100000: episode: 540, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 75.952, mean reward: 3.038 [2.623, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.430], loss: 0.157934, mae: 0.368650, mean_q: 4.523607
 37470/100000: episode: 541, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 87.030, mean reward: 2.807 [2.202, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.239, 10.401], loss: 0.251437, mae: 0.405911, mean_q: 4.556625
 37500/100000: episode: 542, duration: 0.177s, episode steps: 30, steps per second: 170, episode reward: 96.421, mean reward: 3.214 [1.550, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.274, 10.213], loss: 0.265363, mae: 0.440719, mean_q: 4.676099
 37514/100000: episode: 543, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 48.625, mean reward: 3.473 [2.821, 4.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.495], loss: 0.448757, mae: 0.507922, mean_q: 4.539001
 37556/100000: episode: 544, duration: 0.234s, episode steps: 42, steps per second: 180, episode reward: 143.894, mean reward: 3.426 [2.027, 6.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.219, 10.362], loss: 0.327774, mae: 0.451298, mean_q: 4.691853
 37581/100000: episode: 545, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 82.668, mean reward: 3.307 [2.512, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.437, 10.444], loss: 0.235095, mae: 0.426354, mean_q: 4.624554
 37612/100000: episode: 546, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 84.225, mean reward: 2.717 [1.919, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.261, 10.399], loss: 0.227984, mae: 0.413072, mean_q: 4.640008
 37629/100000: episode: 547, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 47.808, mean reward: 2.812 [2.170, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.233, 10.520], loss: 0.230456, mae: 0.415801, mean_q: 4.701350
 37660/100000: episode: 548, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 136.464, mean reward: 4.402 [2.643, 10.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.101, 10.553], loss: 0.236197, mae: 0.429579, mean_q: 4.715674
 37674/100000: episode: 549, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 54.117, mean reward: 3.866 [2.012, 14.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.537, 10.362], loss: 0.216147, mae: 0.423214, mean_q: 4.744126
 37691/100000: episode: 550, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 52.212, mean reward: 3.071 [2.484, 5.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.447], loss: 0.158763, mae: 0.396624, mean_q: 4.727894
 37708/100000: episode: 551, duration: 0.114s, episode steps: 17, steps per second: 150, episode reward: 61.665, mean reward: 3.627 [2.581, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.243, 10.517], loss: 0.157030, mae: 0.383253, mean_q: 4.667673
 37739/100000: episode: 552, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 78.101, mean reward: 2.519 [1.988, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.384], loss: 0.239600, mae: 0.428617, mean_q: 4.769801
 37763/100000: episode: 553, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 75.487, mean reward: 3.145 [2.594, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.200, 10.537], loss: 0.273557, mae: 0.448842, mean_q: 4.807120
 37780/100000: episode: 554, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 54.656, mean reward: 3.215 [2.373, 5.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.393], loss: 0.218167, mae: 0.420949, mean_q: 4.717453
 37797/100000: episode: 555, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 58.623, mean reward: 3.448 [2.364, 4.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.542, 10.521], loss: 0.346528, mae: 0.501012, mean_q: 4.791410
 37828/100000: episode: 556, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 93.798, mean reward: 3.026 [2.308, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.076, 10.339], loss: 0.304649, mae: 0.460719, mean_q: 4.783790
 37845/100000: episode: 557, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 42.254, mean reward: 2.486 [1.905, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.453], loss: 0.244851, mae: 0.433650, mean_q: 4.704915
[Info] FALSIFICATION!
[Info] Levels: [5.224786, 6.5848546, 8.855746]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 37862/100000: episode: 558, duration: 4.479s, episode steps: 17, steps per second: 4, episode reward: 202.679, mean reward: 11.922 [2.929, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.810, 8.617], loss: 0.207632, mae: 0.438436, mean_q: 4.727276
 37962/100000: episode: 559, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 211.390, mean reward: 2.114 [1.467, 7.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.379, 10.098], loss: 1.683689, mae: 0.545056, mean_q: 4.890481
 38062/100000: episode: 560, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 196.872, mean reward: 1.969 [1.437, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.756, 10.249], loss: 1.706539, mae: 0.580609, mean_q: 4.882524
 38162/100000: episode: 561, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.511, mean reward: 1.935 [1.453, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.588, 10.098], loss: 1.657959, mae: 0.569141, mean_q: 4.904297
 38262/100000: episode: 562, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 181.149, mean reward: 1.811 [1.452, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.588, 10.107], loss: 2.903449, mae: 0.580354, mean_q: 4.949650
 38362/100000: episode: 563, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 195.685, mean reward: 1.957 [1.451, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.303, 10.098], loss: 0.348607, mae: 0.511625, mean_q: 4.878190
 38462/100000: episode: 564, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 194.258, mean reward: 1.943 [1.471, 4.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.480, 10.146], loss: 1.616023, mae: 0.563646, mean_q: 4.875910
 38562/100000: episode: 565, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 197.993, mean reward: 1.980 [1.467, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.811, 10.098], loss: 0.295710, mae: 0.447040, mean_q: 4.844851
 38662/100000: episode: 566, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 221.135, mean reward: 2.211 [1.511, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.850, 10.098], loss: 1.576811, mae: 0.515562, mean_q: 4.925553
 38762/100000: episode: 567, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 205.070, mean reward: 2.051 [1.496, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.769, 10.106], loss: 0.395240, mae: 0.498242, mean_q: 4.845465
 38862/100000: episode: 568, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 202.664, mean reward: 2.027 [1.472, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.794, 10.324], loss: 0.266615, mae: 0.426730, mean_q: 4.818151
 38962/100000: episode: 569, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 187.029, mean reward: 1.870 [1.446, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.150, 10.130], loss: 0.344563, mae: 0.467529, mean_q: 4.806551
 39062/100000: episode: 570, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 234.215, mean reward: 2.342 [1.447, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.828, 10.255], loss: 4.130298, mae: 0.659064, mean_q: 4.882887
 39162/100000: episode: 571, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 178.871, mean reward: 1.789 [1.458, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.567, 10.098], loss: 0.234750, mae: 0.450566, mean_q: 4.806524
 39262/100000: episode: 572, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 211.971, mean reward: 2.120 [1.476, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.685, 10.370], loss: 0.247364, mae: 0.429973, mean_q: 4.802076
 39362/100000: episode: 573, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 180.520, mean reward: 1.805 [1.451, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.346], loss: 0.260267, mae: 0.432810, mean_q: 4.772667
 39462/100000: episode: 574, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 189.712, mean reward: 1.897 [1.489, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.201], loss: 1.525645, mae: 0.494431, mean_q: 4.729830
 39562/100000: episode: 575, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 196.117, mean reward: 1.961 [1.453, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.530, 10.305], loss: 0.286885, mae: 0.432280, mean_q: 4.730421
 39662/100000: episode: 576, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.098, mean reward: 1.921 [1.463, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.540, 10.357], loss: 0.234324, mae: 0.429746, mean_q: 4.700909
 39762/100000: episode: 577, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 192.511, mean reward: 1.925 [1.482, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.618, 10.098], loss: 1.568696, mae: 0.517147, mean_q: 4.708773
 39862/100000: episode: 578, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 210.941, mean reward: 2.109 [1.465, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.352], loss: 2.803574, mae: 0.546687, mean_q: 4.723673
 39962/100000: episode: 579, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 178.708, mean reward: 1.787 [1.454, 2.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.154], loss: 2.799200, mae: 0.551341, mean_q: 4.711494
 40062/100000: episode: 580, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 197.844, mean reward: 1.978 [1.464, 4.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.817, 10.098], loss: 0.220636, mae: 0.440859, mean_q: 4.645281
 40162/100000: episode: 581, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 180.581, mean reward: 1.806 [1.471, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.818, 10.098], loss: 0.286482, mae: 0.433051, mean_q: 4.644548
 40262/100000: episode: 582, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.303, mean reward: 1.953 [1.437, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.113, 10.206], loss: 1.498980, mae: 0.473892, mean_q: 4.640456
 40362/100000: episode: 583, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 209.280, mean reward: 2.093 [1.458, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.642, 10.098], loss: 0.265384, mae: 0.423237, mean_q: 4.635606
 40462/100000: episode: 584, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 188.147, mean reward: 1.881 [1.488, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.577, 10.098], loss: 0.209927, mae: 0.407802, mean_q: 4.612305
 40562/100000: episode: 585, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 197.038, mean reward: 1.970 [1.462, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.389, 10.370], loss: 2.783850, mae: 0.559745, mean_q: 4.620526
 40662/100000: episode: 586, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.072, mean reward: 1.771 [1.498, 2.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.152, 10.098], loss: 0.306642, mae: 0.446006, mean_q: 4.641387
 40762/100000: episode: 587, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.562, mean reward: 1.916 [1.435, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.124, 10.142], loss: 0.238421, mae: 0.398789, mean_q: 4.530495
 40862/100000: episode: 588, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 203.440, mean reward: 2.034 [1.458, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.722, 10.364], loss: 0.275337, mae: 0.431480, mean_q: 4.584056
 40962/100000: episode: 589, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 206.816, mean reward: 2.068 [1.449, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.665, 10.098], loss: 1.474641, mae: 0.451894, mean_q: 4.509756
 41062/100000: episode: 590, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.111, mean reward: 1.931 [1.449, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.988, 10.098], loss: 1.449999, mae: 0.467931, mean_q: 4.507794
 41162/100000: episode: 591, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 194.640, mean reward: 1.946 [1.491, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.250, 10.098], loss: 1.431551, mae: 0.452461, mean_q: 4.527179
 41262/100000: episode: 592, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 210.008, mean reward: 2.100 [1.455, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.336, 10.160], loss: 1.467930, mae: 0.479737, mean_q: 4.530338
 41362/100000: episode: 593, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 209.204, mean reward: 2.092 [1.495, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.099], loss: 1.495637, mae: 0.471564, mean_q: 4.527914
 41462/100000: episode: 594, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.355, mean reward: 2.034 [1.438, 4.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.504, 10.313], loss: 0.203170, mae: 0.402128, mean_q: 4.441404
 41562/100000: episode: 595, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.842, mean reward: 2.008 [1.461, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.412, 10.430], loss: 0.233275, mae: 0.399055, mean_q: 4.424947
 41662/100000: episode: 596, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 177.373, mean reward: 1.774 [1.490, 2.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.764, 10.163], loss: 0.194191, mae: 0.399237, mean_q: 4.418592
 41762/100000: episode: 597, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 180.985, mean reward: 1.810 [1.447, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.428, 10.369], loss: 0.228576, mae: 0.390129, mean_q: 4.351655
 41862/100000: episode: 598, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 206.571, mean reward: 2.066 [1.510, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.098], loss: 0.140408, mae: 0.358147, mean_q: 4.266274
 41962/100000: episode: 599, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 201.221, mean reward: 2.012 [1.444, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.501, 10.142], loss: 0.257685, mae: 0.409502, mean_q: 4.314979
 42062/100000: episode: 600, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 182.779, mean reward: 1.828 [1.440, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.876, 10.125], loss: 2.678010, mae: 0.479571, mean_q: 4.318841
 42162/100000: episode: 601, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 198.267, mean reward: 1.983 [1.469, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.847, 10.159], loss: 2.639925, mae: 0.545953, mean_q: 4.292546
 42262/100000: episode: 602, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 198.358, mean reward: 1.984 [1.470, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.583, 10.276], loss: 2.649808, mae: 0.522970, mean_q: 4.268799
 42362/100000: episode: 603, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 181.100, mean reward: 1.811 [1.435, 5.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.796, 10.164], loss: 0.150002, mae: 0.365134, mean_q: 4.146639
 42462/100000: episode: 604, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 194.331, mean reward: 1.943 [1.481, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.208, 10.107], loss: 0.207145, mae: 0.365286, mean_q: 4.103179
 42562/100000: episode: 605, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 191.222, mean reward: 1.912 [1.450, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.515, 10.340], loss: 0.131843, mae: 0.337650, mean_q: 4.061223
 42662/100000: episode: 606, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 207.642, mean reward: 2.076 [1.450, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.973, 10.098], loss: 1.312857, mae: 0.392503, mean_q: 4.038938
 42762/100000: episode: 607, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.886, mean reward: 1.879 [1.465, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.518, 10.258], loss: 1.310281, mae: 0.398175, mean_q: 3.975837
 42862/100000: episode: 608, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.951, mean reward: 2.090 [1.508, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.256], loss: 0.147019, mae: 0.319338, mean_q: 3.933911
 42962/100000: episode: 609, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 202.544, mean reward: 2.025 [1.443, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.818, 10.098], loss: 0.087733, mae: 0.300747, mean_q: 3.892349
 43062/100000: episode: 610, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 258.135, mean reward: 2.581 [1.489, 7.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.408 [-0.771, 10.098], loss: 0.095496, mae: 0.311549, mean_q: 3.900024
 43162/100000: episode: 611, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 225.379, mean reward: 2.254 [1.454, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.408, 10.406], loss: 0.107366, mae: 0.324624, mean_q: 3.916816
 43262/100000: episode: 612, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 190.645, mean reward: 1.906 [1.501, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.458, 10.130], loss: 0.115708, mae: 0.319956, mean_q: 3.908034
 43362/100000: episode: 613, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.909, mean reward: 1.879 [1.449, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.829, 10.273], loss: 0.112776, mae: 0.329085, mean_q: 3.928890
 43462/100000: episode: 614, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 185.164, mean reward: 1.852 [1.474, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.199, 10.099], loss: 0.100637, mae: 0.315682, mean_q: 3.926276
 43562/100000: episode: 615, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.314, mean reward: 1.913 [1.438, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.609, 10.151], loss: 0.103990, mae: 0.319167, mean_q: 3.899224
 43662/100000: episode: 616, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 216.947, mean reward: 2.169 [1.541, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.271], loss: 0.109208, mae: 0.320331, mean_q: 3.920249
 43762/100000: episode: 617, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 214.777, mean reward: 2.148 [1.546, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.156, 10.098], loss: 0.097626, mae: 0.309444, mean_q: 3.913396
 43862/100000: episode: 618, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.049, mean reward: 1.880 [1.439, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.485, 10.132], loss: 0.113348, mae: 0.319075, mean_q: 3.920296
 43962/100000: episode: 619, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 197.888, mean reward: 1.979 [1.444, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.118], loss: 0.099953, mae: 0.305449, mean_q: 3.888589
 44062/100000: episode: 620, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 176.670, mean reward: 1.767 [1.440, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.676, 10.098], loss: 0.099743, mae: 0.301039, mean_q: 3.911725
 44162/100000: episode: 621, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.200, mean reward: 2.002 [1.473, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.609, 10.098], loss: 0.108449, mae: 0.317627, mean_q: 3.911079
 44262/100000: episode: 622, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 178.724, mean reward: 1.787 [1.480, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.381, 10.224], loss: 0.117775, mae: 0.319501, mean_q: 3.925785
 44362/100000: episode: 623, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 187.617, mean reward: 1.876 [1.433, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.279, 10.108], loss: 0.111294, mae: 0.322914, mean_q: 3.929543
 44462/100000: episode: 624, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 188.599, mean reward: 1.886 [1.451, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.820, 10.144], loss: 0.094641, mae: 0.297672, mean_q: 3.898195
 44562/100000: episode: 625, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 237.904, mean reward: 2.379 [1.487, 6.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.860, 10.098], loss: 0.105580, mae: 0.312163, mean_q: 3.922581
 44662/100000: episode: 626, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.682, mean reward: 1.947 [1.445, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.668, 10.277], loss: 0.110696, mae: 0.315629, mean_q: 3.922304
 44762/100000: episode: 627, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 176.463, mean reward: 1.765 [1.458, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.897, 10.145], loss: 0.104548, mae: 0.311468, mean_q: 3.916747
 44862/100000: episode: 628, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 193.281, mean reward: 1.933 [1.495, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.266], loss: 0.108085, mae: 0.314290, mean_q: 3.921685
 44962/100000: episode: 629, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 190.364, mean reward: 1.904 [1.462, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.568, 10.144], loss: 0.100262, mae: 0.306064, mean_q: 3.908653
 45062/100000: episode: 630, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 177.760, mean reward: 1.778 [1.448, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.740, 10.183], loss: 0.095175, mae: 0.301994, mean_q: 3.879461
 45162/100000: episode: 631, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 206.320, mean reward: 2.063 [1.481, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.582, 10.098], loss: 0.099371, mae: 0.306009, mean_q: 3.912169
 45262/100000: episode: 632, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 200.349, mean reward: 2.003 [1.450, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.186], loss: 0.105773, mae: 0.314187, mean_q: 3.910163
 45362/100000: episode: 633, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 185.819, mean reward: 1.858 [1.436, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.887, 10.198], loss: 0.095710, mae: 0.303032, mean_q: 3.882939
 45462/100000: episode: 634, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 184.086, mean reward: 1.841 [1.464, 2.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.153], loss: 0.103275, mae: 0.311137, mean_q: 3.892359
 45562/100000: episode: 635, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.780, mean reward: 1.918 [1.444, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.638, 10.220], loss: 0.084701, mae: 0.292619, mean_q: 3.873760
 45662/100000: episode: 636, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 197.418, mean reward: 1.974 [1.463, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.514, 10.098], loss: 0.098830, mae: 0.308082, mean_q: 3.899979
 45762/100000: episode: 637, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 175.773, mean reward: 1.758 [1.442, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.660, 10.098], loss: 0.114276, mae: 0.317187, mean_q: 3.896867
 45862/100000: episode: 638, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.809, mean reward: 1.868 [1.458, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.190, 10.260], loss: 0.095562, mae: 0.304445, mean_q: 3.895079
 45962/100000: episode: 639, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 211.508, mean reward: 2.115 [1.447, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.754, 10.098], loss: 0.094774, mae: 0.303903, mean_q: 3.888661
 46062/100000: episode: 640, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.664, mean reward: 1.897 [1.431, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.722, 10.114], loss: 0.101285, mae: 0.309456, mean_q: 3.891086
 46162/100000: episode: 641, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.354, mean reward: 1.914 [1.458, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.569, 10.098], loss: 0.101170, mae: 0.306318, mean_q: 3.875909
 46262/100000: episode: 642, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 213.135, mean reward: 2.131 [1.532, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.819, 10.331], loss: 0.098209, mae: 0.309480, mean_q: 3.891521
 46362/100000: episode: 643, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.115, mean reward: 1.851 [1.461, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.202], loss: 0.091341, mae: 0.300438, mean_q: 3.874225
 46462/100000: episode: 644, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 189.174, mean reward: 1.892 [1.475, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.277, 10.393], loss: 0.100294, mae: 0.308039, mean_q: 3.879205
 46562/100000: episode: 645, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 197.534, mean reward: 1.975 [1.492, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.353, 10.098], loss: 0.103683, mae: 0.305846, mean_q: 3.870223
 46662/100000: episode: 646, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 190.002, mean reward: 1.900 [1.471, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.304, 10.267], loss: 0.102202, mae: 0.310080, mean_q: 3.890155
 46762/100000: episode: 647, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 205.808, mean reward: 2.058 [1.464, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.057, 10.124], loss: 0.093329, mae: 0.296835, mean_q: 3.863503
 46862/100000: episode: 648, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.873, mean reward: 1.919 [1.494, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.233, 10.098], loss: 0.102380, mae: 0.307191, mean_q: 3.874977
 46962/100000: episode: 649, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.475, mean reward: 1.855 [1.482, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.039, 10.186], loss: 0.088242, mae: 0.291180, mean_q: 3.863153
 47062/100000: episode: 650, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 184.791, mean reward: 1.848 [1.464, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.098], loss: 0.091308, mae: 0.301298, mean_q: 3.879266
 47162/100000: episode: 651, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 201.862, mean reward: 2.019 [1.460, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.076, 10.208], loss: 0.102459, mae: 0.307500, mean_q: 3.873079
 47262/100000: episode: 652, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.390, mean reward: 2.004 [1.483, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.246], loss: 0.101684, mae: 0.311451, mean_q: 3.886419
 47362/100000: episode: 653, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.679, mean reward: 1.947 [1.479, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.469, 10.098], loss: 0.103094, mae: 0.305992, mean_q: 3.914059
 47462/100000: episode: 654, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 181.404, mean reward: 1.814 [1.436, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.117], loss: 0.098101, mae: 0.304629, mean_q: 3.890819
 47562/100000: episode: 655, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.162, mean reward: 1.832 [1.457, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.144], loss: 0.098511, mae: 0.308095, mean_q: 3.883448
 47662/100000: episode: 656, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 186.692, mean reward: 1.867 [1.496, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.087, 10.098], loss: 0.088462, mae: 0.296227, mean_q: 3.868272
 47762/100000: episode: 657, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 181.063, mean reward: 1.811 [1.449, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.340, 10.098], loss: 0.091155, mae: 0.298598, mean_q: 3.877281
[Info] 1-TH LEVEL FOUND: 5.432631015777588, Considering 10/90 traces
 47862/100000: episode: 658, duration: 4.623s, episode steps: 100, steps per second: 22, episode reward: 186.920, mean reward: 1.869 [1.481, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.098], loss: 0.087870, mae: 0.296027, mean_q: 3.855287
 47898/100000: episode: 659, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 104.610, mean reward: 2.906 [2.104, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.330, 10.381], loss: 0.088354, mae: 0.292358, mean_q: 3.875659
 47934/100000: episode: 660, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 94.408, mean reward: 2.622 [1.655, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.672, 10.100], loss: 0.088225, mae: 0.292745, mean_q: 3.868997
 47952/100000: episode: 661, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 42.309, mean reward: 2.351 [1.780, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.237], loss: 0.126308, mae: 0.326545, mean_q: 3.898145
 47959/100000: episode: 662, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 20.673, mean reward: 2.953 [2.432, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.985, 10.345], loss: 0.056496, mae: 0.269546, mean_q: 3.858162
 47978/100000: episode: 663, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 56.943, mean reward: 2.997 [2.409, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.276, 10.100], loss: 0.069582, mae: 0.267775, mean_q: 3.839137
 47997/100000: episode: 664, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 49.133, mean reward: 2.586 [1.906, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.233, 10.402], loss: 0.073973, mae: 0.276562, mean_q: 3.846827
 48015/100000: episode: 665, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 45.355, mean reward: 2.520 [2.092, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.101, 10.388], loss: 0.115851, mae: 0.330663, mean_q: 3.897523
 48051/100000: episode: 666, duration: 0.222s, episode steps: 36, steps per second: 162, episode reward: 85.299, mean reward: 2.369 [1.837, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.421], loss: 0.088921, mae: 0.302194, mean_q: 3.872001
 48070/100000: episode: 667, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 52.956, mean reward: 2.787 [2.257, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.623, 10.370], loss: 0.089985, mae: 0.316414, mean_q: 3.895121
 48089/100000: episode: 668, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 80.877, mean reward: 4.257 [2.991, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.299, 10.100], loss: 0.095010, mae: 0.301318, mean_q: 3.895643
 48125/100000: episode: 669, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 79.802, mean reward: 2.217 [1.599, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.690, 10.100], loss: 0.082814, mae: 0.296404, mean_q: 3.881027
 48144/100000: episode: 670, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 40.873, mean reward: 2.151 [1.880, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.318], loss: 0.085707, mae: 0.301918, mean_q: 3.890287
 48170/100000: episode: 671, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 70.903, mean reward: 2.727 [1.948, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.538, 10.432], loss: 0.095393, mae: 0.310335, mean_q: 3.886454
 48206/100000: episode: 672, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 72.520, mean reward: 2.014 [1.495, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.045, 10.100], loss: 0.089066, mae: 0.299984, mean_q: 3.939385
 48242/100000: episode: 673, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 94.315, mean reward: 2.620 [1.970, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.085, 10.367], loss: 0.089963, mae: 0.295652, mean_q: 3.897912
 48261/100000: episode: 674, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 77.304, mean reward: 4.069 [3.325, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.821, 10.100], loss: 0.084979, mae: 0.291510, mean_q: 3.920815
 48282/100000: episode: 675, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 46.153, mean reward: 2.198 [1.782, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.271], loss: 0.080492, mae: 0.295958, mean_q: 3.902533
 48301/100000: episode: 676, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 54.868, mean reward: 2.888 [2.253, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.209, 10.414], loss: 0.087578, mae: 0.298543, mean_q: 3.899334
 48322/100000: episode: 677, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 48.974, mean reward: 2.332 [1.785, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.251, 10.449], loss: 0.102891, mae: 0.310363, mean_q: 3.912156
 48341/100000: episode: 678, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 54.594, mean reward: 2.873 [2.202, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.181, 10.100], loss: 0.092370, mae: 0.313362, mean_q: 3.934683
 48377/100000: episode: 679, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 85.459, mean reward: 2.374 [1.704, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.291, 10.510], loss: 0.100138, mae: 0.316422, mean_q: 3.921134
 48403/100000: episode: 680, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 65.692, mean reward: 2.527 [2.119, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.291, 10.359], loss: 0.097071, mae: 0.314615, mean_q: 3.955593
 48422/100000: episode: 681, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 70.433, mean reward: 3.707 [2.616, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.475, 10.100], loss: 0.100581, mae: 0.319041, mean_q: 3.915965
 48458/100000: episode: 682, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 82.180, mean reward: 2.283 [1.688, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.446, 10.252], loss: 0.104435, mae: 0.319019, mean_q: 3.984216
 48494/100000: episode: 683, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 75.849, mean reward: 2.107 [1.543, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.289, 10.199], loss: 0.091053, mae: 0.308979, mean_q: 3.951688
 48530/100000: episode: 684, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 75.049, mean reward: 2.085 [1.708, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.346, 10.285], loss: 0.090300, mae: 0.307832, mean_q: 3.955224
 48556/100000: episode: 685, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 65.561, mean reward: 2.522 [1.917, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.739, 10.335], loss: 0.115142, mae: 0.337010, mean_q: 3.984021
 48575/100000: episode: 686, duration: 0.120s, episode steps: 19, steps per second: 159, episode reward: 40.197, mean reward: 2.116 [1.734, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.391, 10.297], loss: 0.093823, mae: 0.312935, mean_q: 3.981971
 48593/100000: episode: 687, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 43.436, mean reward: 2.413 [2.161, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.389], loss: 0.087234, mae: 0.301532, mean_q: 3.938523
 48611/100000: episode: 688, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 36.923, mean reward: 2.051 [1.610, 2.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.639, 10.187], loss: 0.096910, mae: 0.311078, mean_q: 3.950414
 48620/100000: episode: 689, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 18.783, mean reward: 2.087 [1.913, 2.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.328], loss: 0.081998, mae: 0.309682, mean_q: 4.027553
 48656/100000: episode: 690, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 71.343, mean reward: 1.982 [1.443, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.195, 10.100], loss: 0.095373, mae: 0.316556, mean_q: 3.965713
 48692/100000: episode: 691, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 99.288, mean reward: 2.758 [1.836, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.366], loss: 0.094282, mae: 0.309641, mean_q: 3.926585
 48699/100000: episode: 692, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 16.486, mean reward: 2.355 [2.055, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.406], loss: 0.083885, mae: 0.288160, mean_q: 3.973288
 48718/100000: episode: 693, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 46.734, mean reward: 2.460 [1.951, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.728, 10.342], loss: 0.101999, mae: 0.332554, mean_q: 3.982889
 48725/100000: episode: 694, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 18.948, mean reward: 2.707 [2.430, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.416], loss: 0.153496, mae: 0.361606, mean_q: 3.985936
 48743/100000: episode: 695, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 51.027, mean reward: 2.835 [1.971, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.089, 10.520], loss: 0.089340, mae: 0.301205, mean_q: 3.998357
 48779/100000: episode: 696, duration: 0.206s, episode steps: 36, steps per second: 174, episode reward: 71.787, mean reward: 1.994 [1.550, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.614, 10.331], loss: 0.115440, mae: 0.337814, mean_q: 4.019908
 48798/100000: episode: 697, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 49.367, mean reward: 2.598 [1.933, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.173, 10.100], loss: 0.093272, mae: 0.302611, mean_q: 3.916885
 48805/100000: episode: 698, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 20.226, mean reward: 2.889 [2.407, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.399], loss: 0.137706, mae: 0.360921, mean_q: 3.923951
 48812/100000: episode: 699, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 21.869, mean reward: 3.124 [2.743, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.344], loss: 0.108979, mae: 0.381889, mean_q: 4.141888
 48833/100000: episode: 700, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 49.821, mean reward: 2.372 [1.903, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.377], loss: 0.140159, mae: 0.342241, mean_q: 3.997158
 48869/100000: episode: 701, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 96.769, mean reward: 2.688 [1.837, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.439, 10.452], loss: 0.121889, mae: 0.349356, mean_q: 3.989235
 48888/100000: episode: 702, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 46.718, mean reward: 2.459 [2.062, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.609, 10.430], loss: 0.108048, mae: 0.341192, mean_q: 4.091241
 48909/100000: episode: 703, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 49.575, mean reward: 2.361 [1.898, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.875, 10.284], loss: 0.093329, mae: 0.316618, mean_q: 3.971992
 48916/100000: episode: 704, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 20.408, mean reward: 2.915 [2.516, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.443], loss: 0.079132, mae: 0.278229, mean_q: 4.005784
 48935/100000: episode: 705, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 41.808, mean reward: 2.200 [1.811, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.759, 10.312], loss: 0.092333, mae: 0.311561, mean_q: 3.984936
 48971/100000: episode: 706, duration: 0.210s, episode steps: 36, steps per second: 171, episode reward: 98.056, mean reward: 2.724 [1.972, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.361, 10.329], loss: 0.116264, mae: 0.338602, mean_q: 4.012332
 48997/100000: episode: 707, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 66.103, mean reward: 2.542 [2.048, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.821, 10.310], loss: 0.118144, mae: 0.331318, mean_q: 4.030827
 49006/100000: episode: 708, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 19.094, mean reward: 2.122 [1.695, 2.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.267], loss: 0.115095, mae: 0.327802, mean_q: 4.020209
 49025/100000: episode: 709, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 40.467, mean reward: 2.130 [1.661, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.036, 10.247], loss: 0.103868, mae: 0.319097, mean_q: 4.016045
 49032/100000: episode: 710, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 21.358, mean reward: 3.051 [2.728, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.540, 10.457], loss: 0.111650, mae: 0.340242, mean_q: 4.051291
 49053/100000: episode: 711, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 46.498, mean reward: 2.214 [1.705, 2.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.247, 10.199], loss: 0.101622, mae: 0.318416, mean_q: 4.016790
 49072/100000: episode: 712, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 76.608, mean reward: 4.032 [3.003, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.299, 10.100], loss: 0.110680, mae: 0.335584, mean_q: 4.060627
 49091/100000: episode: 713, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 64.121, mean reward: 3.375 [2.591, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.175, 10.100], loss: 0.118293, mae: 0.340052, mean_q: 4.071971
 49098/100000: episode: 714, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 18.203, mean reward: 2.600 [2.348, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.349, 10.362], loss: 0.094041, mae: 0.312803, mean_q: 3.880372
 49117/100000: episode: 715, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 99.416, mean reward: 5.232 [2.663, 11.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.367, 10.100], loss: 0.113255, mae: 0.330611, mean_q: 4.036922
 49126/100000: episode: 716, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 25.960, mean reward: 2.884 [2.130, 5.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.325], loss: 0.121401, mae: 0.329042, mean_q: 4.063902
 49135/100000: episode: 717, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 20.372, mean reward: 2.264 [1.985, 2.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.382], loss: 0.099744, mae: 0.322615, mean_q: 4.112489
 49156/100000: episode: 718, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 67.902, mean reward: 3.233 [1.778, 7.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.532], loss: 0.124123, mae: 0.349053, mean_q: 4.080669
 49192/100000: episode: 719, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 76.310, mean reward: 2.120 [1.507, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.980, 10.151], loss: 0.156080, mae: 0.360400, mean_q: 4.082846
 49210/100000: episode: 720, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 36.198, mean reward: 2.011 [1.607, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.241], loss: 0.101573, mae: 0.318391, mean_q: 4.084398
 49231/100000: episode: 721, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 48.888, mean reward: 2.328 [1.993, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.113, 10.340], loss: 0.100939, mae: 0.318724, mean_q: 4.066407
 49238/100000: episode: 722, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 19.139, mean reward: 2.734 [2.447, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.456], loss: 0.118954, mae: 0.342418, mean_q: 4.114809
 49264/100000: episode: 723, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 60.518, mean reward: 2.328 [1.944, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.284, 10.429], loss: 0.098575, mae: 0.312748, mean_q: 4.031149
 49273/100000: episode: 724, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 18.654, mean reward: 2.073 [1.944, 2.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.358], loss: 0.145346, mae: 0.385342, mean_q: 4.193875
 49282/100000: episode: 725, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 26.676, mean reward: 2.964 [2.116, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.533], loss: 0.309260, mae: 0.416876, mean_q: 4.024978
 49318/100000: episode: 726, duration: 0.210s, episode steps: 36, steps per second: 172, episode reward: 73.280, mean reward: 2.036 [1.490, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.109, 10.276], loss: 0.119286, mae: 0.347076, mean_q: 4.087255
 49354/100000: episode: 727, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 83.476, mean reward: 2.319 [1.648, 4.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.276], loss: 0.123620, mae: 0.358579, mean_q: 4.088930
 49375/100000: episode: 728, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 63.832, mean reward: 3.040 [2.160, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.540, 10.552], loss: 0.169369, mae: 0.387810, mean_q: 4.159672
 49401/100000: episode: 729, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 68.334, mean reward: 2.628 [2.080, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.486], loss: 0.119552, mae: 0.354045, mean_q: 4.183557
 49422/100000: episode: 730, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 46.011, mean reward: 2.191 [1.688, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.360, 10.443], loss: 0.145969, mae: 0.382261, mean_q: 4.168689
 49441/100000: episode: 731, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 59.590, mean reward: 3.136 [2.194, 5.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.207, 10.100], loss: 0.130510, mae: 0.353970, mean_q: 4.091040
 49477/100000: episode: 732, duration: 0.210s, episode steps: 36, steps per second: 171, episode reward: 89.723, mean reward: 2.492 [1.861, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.295, 10.297], loss: 0.145088, mae: 0.362077, mean_q: 4.134840
 49513/100000: episode: 733, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 105.178, mean reward: 2.922 [1.952, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.579, 10.580], loss: 0.130081, mae: 0.356301, mean_q: 4.168562
 49539/100000: episode: 734, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 67.650, mean reward: 2.602 [2.121, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.429, 10.511], loss: 0.126208, mae: 0.360994, mean_q: 4.110911
 49557/100000: episode: 735, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 54.615, mean reward: 3.034 [2.109, 5.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-2.025, 10.412], loss: 0.107990, mae: 0.334356, mean_q: 4.093952
 49578/100000: episode: 736, duration: 0.122s, episode steps: 21, steps per second: 173, episode reward: 54.850, mean reward: 2.612 [1.937, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.472], loss: 0.165502, mae: 0.349266, mean_q: 4.149201
 49597/100000: episode: 737, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 73.122, mean reward: 3.849 [2.649, 5.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.749, 10.100], loss: 0.112538, mae: 0.348719, mean_q: 4.144238
 49604/100000: episode: 738, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 18.953, mean reward: 2.708 [2.176, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.299], loss: 0.075304, mae: 0.289966, mean_q: 4.114564
 49640/100000: episode: 739, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 77.405, mean reward: 2.150 [1.800, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.316, 10.249], loss: 0.114149, mae: 0.340009, mean_q: 4.156594
 49661/100000: episode: 740, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 50.291, mean reward: 2.395 [1.784, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.546, 10.332], loss: 0.161742, mae: 0.403893, mean_q: 4.240381
 49670/100000: episode: 741, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 20.075, mean reward: 2.231 [2.066, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.325], loss: 0.180759, mae: 0.372958, mean_q: 4.168102
 49688/100000: episode: 742, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 36.976, mean reward: 2.054 [1.557, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.223, 10.289], loss: 0.144729, mae: 0.340429, mean_q: 4.203234
 49706/100000: episode: 743, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 38.272, mean reward: 2.126 [1.730, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.285], loss: 0.151278, mae: 0.382726, mean_q: 4.240586
 49724/100000: episode: 744, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 42.657, mean reward: 2.370 [1.917, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.212, 10.310], loss: 0.126451, mae: 0.356669, mean_q: 4.159899
 49743/100000: episode: 745, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 48.929, mean reward: 2.575 [2.128, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.177, 10.402], loss: 0.184832, mae: 0.377697, mean_q: 4.209901
 49750/100000: episode: 746, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 18.670, mean reward: 2.667 [1.917, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.052, 10.254], loss: 0.120858, mae: 0.314972, mean_q: 3.977232
 49759/100000: episode: 747, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 23.043, mean reward: 2.560 [2.191, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.433], loss: 0.149425, mae: 0.375156, mean_q: 4.242429
[Info] 2-TH LEVEL FOUND: 6.826877117156982, Considering 10/90 traces
 49778/100000: episode: 748, duration: 4.461s, episode steps: 19, steps per second: 4, episode reward: 71.648, mean reward: 3.771 [2.825, 5.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.410, 10.100], loss: 0.113381, mae: 0.338445, mean_q: 4.236263
 49794/100000: episode: 749, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 48.196, mean reward: 3.012 [2.325, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.336, 10.100], loss: 0.168684, mae: 0.361526, mean_q: 4.250310
 49809/100000: episode: 750, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 41.060, mean reward: 2.737 [2.340, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.339, 10.100], loss: 0.195524, mae: 0.403301, mean_q: 4.255435
 49827/100000: episode: 751, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 55.439, mean reward: 3.080 [2.419, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.382, 10.100], loss: 0.156533, mae: 0.366286, mean_q: 4.195521
 49844/100000: episode: 752, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 53.569, mean reward: 3.151 [2.125, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.219, 10.100], loss: 0.123730, mae: 0.356765, mean_q: 4.305241
 49862/100000: episode: 753, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 114.660, mean reward: 6.370 [3.204, 8.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.594, 10.100], loss: 0.149898, mae: 0.377433, mean_q: 4.239611
 49869/100000: episode: 754, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 28.826, mean reward: 4.118 [3.418, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.446], loss: 0.233790, mae: 0.394210, mean_q: 4.248108
 49876/100000: episode: 755, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 35.206, mean reward: 5.029 [4.001, 6.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.952, 10.520], loss: 0.146762, mae: 0.395515, mean_q: 4.150787
 49883/100000: episode: 756, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 26.266, mean reward: 3.752 [3.292, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.551], loss: 0.150546, mae: 0.372519, mean_q: 4.205442
 49900/100000: episode: 757, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 61.128, mean reward: 3.596 [2.150, 6.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.517, 10.100], loss: 0.152262, mae: 0.388938, mean_q: 4.316957
 49917/100000: episode: 758, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 61.851, mean reward: 3.638 [2.590, 5.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.375, 10.100], loss: 0.216256, mae: 0.407471, mean_q: 4.351811
 49935/100000: episode: 759, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 71.731, mean reward: 3.985 [3.108, 5.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.634, 10.100], loss: 0.189318, mae: 0.426148, mean_q: 4.374693
 49951/100000: episode: 760, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 57.296, mean reward: 3.581 [2.854, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.452, 10.100], loss: 0.185372, mae: 0.409463, mean_q: 4.300593
 49966/100000: episode: 761, duration: 0.104s, episode steps: 15, steps per second: 144, episode reward: 41.237, mean reward: 2.749 [1.911, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.040, 10.100], loss: 0.188605, mae: 0.391108, mean_q: 4.380254
 49983/100000: episode: 762, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 100.925, mean reward: 5.937 [3.714, 13.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.145, 10.100], loss: 0.164221, mae: 0.394102, mean_q: 4.323428
 50001/100000: episode: 763, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 62.280, mean reward: 3.460 [1.775, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.133, 10.100], loss: 0.196738, mae: 0.403155, mean_q: 4.338377
 50018/100000: episode: 764, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 78.667, mean reward: 4.627 [3.025, 6.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.302, 10.100], loss: 0.154066, mae: 0.386292, mean_q: 4.357899
 50035/100000: episode: 765, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 55.282, mean reward: 3.252 [2.571, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.149, 10.100], loss: 0.186789, mae: 0.424497, mean_q: 4.483838
 50053/100000: episode: 766, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 87.135, mean reward: 4.841 [3.128, 7.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.263, 10.100], loss: 0.188112, mae: 0.393280, mean_q: 4.401884
 50069/100000: episode: 767, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 52.149, mean reward: 3.259 [2.342, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.216, 10.100], loss: 0.200222, mae: 0.410885, mean_q: 4.348129
 50086/100000: episode: 768, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 71.621, mean reward: 4.213 [3.035, 5.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.244, 10.100], loss: 0.283031, mae: 0.474919, mean_q: 4.560308
 50104/100000: episode: 769, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 56.816, mean reward: 3.156 [2.265, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.190, 10.100], loss: 0.256602, mae: 0.439474, mean_q: 4.456491
 50120/100000: episode: 770, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 63.617, mean reward: 3.976 [2.361, 12.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.268, 10.100], loss: 0.134323, mae: 0.377878, mean_q: 4.329350
 50136/100000: episode: 771, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 85.019, mean reward: 5.314 [2.955, 21.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.467, 10.100], loss: 0.252503, mae: 0.447159, mean_q: 4.401392
 50154/100000: episode: 772, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 84.539, mean reward: 4.697 [3.708, 6.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.259, 10.100], loss: 0.214675, mae: 0.436748, mean_q: 4.549482
 50161/100000: episode: 773, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 28.724, mean reward: 4.103 [3.290, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.537, 10.494], loss: 0.144083, mae: 0.384358, mean_q: 4.453441
 50177/100000: episode: 774, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 51.324, mean reward: 3.208 [2.469, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.619, 10.100], loss: 0.169092, mae: 0.362198, mean_q: 4.414624
 50194/100000: episode: 775, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 51.763, mean reward: 3.045 [2.220, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.297, 10.100], loss: 0.154850, mae: 0.377750, mean_q: 4.514054
 50211/100000: episode: 776, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 50.430, mean reward: 2.966 [2.547, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.245, 10.100], loss: 0.154257, mae: 0.391925, mean_q: 4.437809
 50226/100000: episode: 777, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 60.570, mean reward: 4.038 [3.070, 7.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.514, 10.100], loss: 0.233895, mae: 0.434357, mean_q: 4.440847
 50241/100000: episode: 778, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 44.282, mean reward: 2.952 [2.536, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.328, 10.100], loss: 0.207521, mae: 0.467140, mean_q: 4.486994
 50258/100000: episode: 779, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 51.781, mean reward: 3.046 [1.957, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.823, 10.100], loss: 0.162307, mae: 0.406473, mean_q: 4.432324
 50274/100000: episode: 780, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 53.016, mean reward: 3.313 [2.397, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.404, 10.100], loss: 0.210811, mae: 0.419154, mean_q: 4.528812
 50291/100000: episode: 781, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 78.978, mean reward: 4.646 [3.637, 6.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.440, 10.100], loss: 0.571806, mae: 0.513174, mean_q: 4.642284
 50309/100000: episode: 782, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 87.568, mean reward: 4.865 [2.584, 12.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.465, 10.100], loss: 0.199327, mae: 0.422014, mean_q: 4.567596
 50327/100000: episode: 783, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 66.039, mean reward: 3.669 [2.477, 6.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.262, 10.100], loss: 0.182174, mae: 0.404923, mean_q: 4.474434
 50345/100000: episode: 784, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 69.565, mean reward: 3.865 [2.866, 8.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.428, 10.100], loss: 0.194034, mae: 0.423614, mean_q: 4.580409
 50362/100000: episode: 785, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 52.696, mean reward: 3.100 [2.600, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.837, 10.100], loss: 0.282180, mae: 0.455536, mean_q: 4.699831
 50369/100000: episode: 786, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 25.744, mean reward: 3.678 [2.952, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.095, 10.579], loss: 0.240620, mae: 0.438175, mean_q: 4.549390
 50387/100000: episode: 787, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 66.364, mean reward: 3.687 [2.850, 7.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.344, 10.100], loss: 0.288550, mae: 0.461473, mean_q: 4.725440
 50404/100000: episode: 788, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 60.093, mean reward: 3.535 [2.967, 5.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.343, 10.100], loss: 0.233434, mae: 0.442414, mean_q: 4.485731
 50421/100000: episode: 789, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 82.065, mean reward: 4.827 [3.386, 7.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.664, 10.100], loss: 0.527165, mae: 0.491995, mean_q: 4.653963
 50437/100000: episode: 790, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 56.247, mean reward: 3.515 [2.921, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.315, 10.100], loss: 0.355157, mae: 0.493712, mean_q: 4.727382
 50455/100000: episode: 791, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 67.586, mean reward: 3.755 [2.931, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.429, 10.100], loss: 0.293899, mae: 0.469129, mean_q: 4.649050
 50462/100000: episode: 792, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 41.460, mean reward: 5.923 [3.108, 9.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.269, 10.648], loss: 0.155745, mae: 0.392591, mean_q: 4.612758
 50477/100000: episode: 793, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 48.599, mean reward: 3.240 [2.548, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.498, 10.100], loss: 0.300565, mae: 0.467719, mean_q: 4.781446
 50495/100000: episode: 794, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 72.971, mean reward: 4.054 [2.765, 5.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.807, 10.100], loss: 0.270495, mae: 0.426834, mean_q: 4.646728
 50511/100000: episode: 795, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 59.119, mean reward: 3.695 [2.652, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.071, 10.100], loss: 0.324086, mae: 0.497661, mean_q: 4.780591
 50527/100000: episode: 796, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 46.932, mean reward: 2.933 [2.288, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.445, 10.100], loss: 0.179591, mae: 0.416642, mean_q: 4.810718
 50542/100000: episode: 797, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 67.000, mean reward: 4.467 [2.858, 6.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.344, 10.100], loss: 0.211098, mae: 0.431198, mean_q: 4.740147
 50559/100000: episode: 798, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 78.810, mean reward: 4.636 [3.043, 6.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.235, 10.100], loss: 0.230350, mae: 0.462663, mean_q: 4.652531
 50574/100000: episode: 799, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 62.269, mean reward: 4.151 [2.679, 6.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.299, 10.100], loss: 0.281997, mae: 0.484115, mean_q: 4.820190
 50591/100000: episode: 800, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 57.982, mean reward: 3.411 [1.663, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.184, 10.100], loss: 0.172363, mae: 0.426369, mean_q: 4.825746
 50609/100000: episode: 801, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 112.733, mean reward: 6.263 [3.026, 14.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.448, 10.100], loss: 0.255394, mae: 0.491937, mean_q: 4.798865
[Info] FALSIFICATION!
[Info] Levels: [5.432631, 6.826877, 10.260513]
[Info] Cond. Prob: [0.1, 0.1, 0.03]
[Info] Error Prob: 0.00030000000000000003

 50619/100000: episode: 802, duration: 4.361s, episode steps: 10, steps per second: 2, episode reward: 172.770, mean reward: 17.277 [3.520, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.154, 9.952], loss: 0.818825, mae: 0.553292, mean_q: 4.880530
 50719/100000: episode: 803, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.020, mean reward: 2.020 [1.466, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.490, 10.098], loss: 1.660170, mae: 0.575773, mean_q: 4.878741
 50819/100000: episode: 804, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.506, mean reward: 2.005 [1.444, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.124], loss: 0.400129, mae: 0.498542, mean_q: 4.876803
 50919/100000: episode: 805, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 206.523, mean reward: 2.065 [1.481, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.158], loss: 1.667091, mae: 0.572632, mean_q: 4.883102
 51019/100000: episode: 806, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.927, mean reward: 1.899 [1.459, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.752, 10.098], loss: 2.875138, mae: 0.605683, mean_q: 4.907351
 51119/100000: episode: 807, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 183.726, mean reward: 1.837 [1.491, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.044, 10.098], loss: 0.313795, mae: 0.502959, mean_q: 4.850549
 51219/100000: episode: 808, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 200.924, mean reward: 2.009 [1.467, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.271], loss: 0.368490, mae: 0.486124, mean_q: 4.874640
 51319/100000: episode: 809, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 187.917, mean reward: 1.879 [1.474, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.400, 10.100], loss: 2.895349, mae: 0.629902, mean_q: 4.854349
 51419/100000: episode: 810, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.838, mean reward: 1.868 [1.477, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.230], loss: 0.313457, mae: 0.467563, mean_q: 4.923692
 51519/100000: episode: 811, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 188.786, mean reward: 1.888 [1.492, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.193], loss: 0.359872, mae: 0.494827, mean_q: 4.866255
 51619/100000: episode: 812, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.243, mean reward: 1.922 [1.479, 4.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.113], loss: 0.264448, mae: 0.460165, mean_q: 4.857450
 51719/100000: episode: 813, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 220.467, mean reward: 2.205 [1.449, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.454, 10.370], loss: 1.676122, mae: 0.534764, mean_q: 4.921596
 51819/100000: episode: 814, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 210.348, mean reward: 2.103 [1.496, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.537, 10.098], loss: 0.320160, mae: 0.505122, mean_q: 4.843507
 51919/100000: episode: 815, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 203.052, mean reward: 2.031 [1.468, 4.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.553, 10.098], loss: 1.616030, mae: 0.549351, mean_q: 4.935922
 52019/100000: episode: 816, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.342, mean reward: 1.823 [1.443, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.030, 10.237], loss: 0.318237, mae: 0.490870, mean_q: 4.966723
 52119/100000: episode: 817, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.970, mean reward: 1.840 [1.463, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.102, 10.284], loss: 1.607497, mae: 0.572755, mean_q: 4.904819
 52219/100000: episode: 818, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 179.870, mean reward: 1.799 [1.455, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.468, 10.297], loss: 1.654125, mae: 0.592094, mean_q: 4.966700
 52319/100000: episode: 819, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.468, mean reward: 1.995 [1.463, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.067, 10.098], loss: 2.997462, mae: 0.673804, mean_q: 4.999996
 52419/100000: episode: 820, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 208.945, mean reward: 2.089 [1.498, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.098], loss: 1.660871, mae: 0.602044, mean_q: 4.960912
 52519/100000: episode: 821, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.017, mean reward: 1.980 [1.451, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.904, 10.284], loss: 1.592258, mae: 0.549010, mean_q: 4.893792
 52619/100000: episode: 822, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 197.321, mean reward: 1.973 [1.454, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.262, 10.098], loss: 1.568026, mae: 0.558619, mean_q: 4.933411
 52719/100000: episode: 823, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 215.675, mean reward: 2.157 [1.501, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.368, 10.279], loss: 1.591843, mae: 0.571261, mean_q: 4.990511
 52819/100000: episode: 824, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.249, mean reward: 2.002 [1.460, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.296, 10.116], loss: 2.780877, mae: 0.572635, mean_q: 4.864999
 52919/100000: episode: 825, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.960, mean reward: 2.010 [1.447, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.599, 10.261], loss: 0.357021, mae: 0.524127, mean_q: 4.887186
 53019/100000: episode: 826, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.145, mean reward: 1.901 [1.458, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.543, 10.273], loss: 5.270122, mae: 0.699693, mean_q: 4.952155
 53119/100000: episode: 827, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 204.740, mean reward: 2.047 [1.461, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.168, 10.098], loss: 1.604833, mae: 0.552108, mean_q: 4.894740
 53219/100000: episode: 828, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.494, mean reward: 1.865 [1.473, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.385, 10.098], loss: 1.660865, mae: 0.562031, mean_q: 4.807541
 53319/100000: episode: 829, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.831, mean reward: 2.078 [1.453, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.754, 10.162], loss: 0.382095, mae: 0.522717, mean_q: 4.832817
 53419/100000: episode: 830, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 216.053, mean reward: 2.161 [1.469, 5.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.098], loss: 1.560293, mae: 0.545078, mean_q: 4.759779
 53519/100000: episode: 831, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.956, mean reward: 1.960 [1.507, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.472, 10.411], loss: 1.599565, mae: 0.555026, mean_q: 4.773248
 53619/100000: episode: 832, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 191.859, mean reward: 1.919 [1.494, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.541, 10.098], loss: 0.448537, mae: 0.530753, mean_q: 4.843162
 53719/100000: episode: 833, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 203.622, mean reward: 2.036 [1.488, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.514, 10.098], loss: 0.329348, mae: 0.460305, mean_q: 4.709273
 53819/100000: episode: 834, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 193.438, mean reward: 1.934 [1.457, 4.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.098], loss: 0.315490, mae: 0.467763, mean_q: 4.713936
 53919/100000: episode: 835, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 190.035, mean reward: 1.900 [1.453, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.371, 10.408], loss: 0.234384, mae: 0.436102, mean_q: 4.668768
 54019/100000: episode: 836, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 210.153, mean reward: 2.102 [1.458, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.062, 10.145], loss: 0.268040, mae: 0.440890, mean_q: 4.684642
 54119/100000: episode: 837, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.618, mean reward: 1.836 [1.449, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.098], loss: 2.817015, mae: 0.586320, mean_q: 4.708754
 54219/100000: episode: 838, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 193.468, mean reward: 1.935 [1.471, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.616, 10.133], loss: 0.277864, mae: 0.468407, mean_q: 4.685932
 54319/100000: episode: 839, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 190.832, mean reward: 1.908 [1.450, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.695, 10.192], loss: 0.358554, mae: 0.455117, mean_q: 4.590965
 54419/100000: episode: 840, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 204.770, mean reward: 2.048 [1.484, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.762, 10.098], loss: 0.197455, mae: 0.405212, mean_q: 4.547949
 54519/100000: episode: 841, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 219.034, mean reward: 2.190 [1.492, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.641, 10.236], loss: 0.357998, mae: 0.458971, mean_q: 4.587011
 54619/100000: episode: 842, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 186.257, mean reward: 1.863 [1.468, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.283, 10.098], loss: 0.256217, mae: 0.433016, mean_q: 4.532421
 54719/100000: episode: 843, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.026, mean reward: 2.040 [1.475, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.790, 10.281], loss: 2.838722, mae: 0.565636, mean_q: 4.575073
 54819/100000: episode: 844, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 193.934, mean reward: 1.939 [1.454, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.207, 10.234], loss: 0.219366, mae: 0.404622, mean_q: 4.377182
 54919/100000: episode: 845, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 183.551, mean reward: 1.836 [1.456, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.821, 10.140], loss: 0.275381, mae: 0.407285, mean_q: 4.382377
 55019/100000: episode: 846, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.559, mean reward: 1.876 [1.452, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.265, 10.098], loss: 2.711704, mae: 0.485909, mean_q: 4.348665
 55119/100000: episode: 847, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 211.133, mean reward: 2.111 [1.456, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.247], loss: 0.246369, mae: 0.408245, mean_q: 4.271754
 55219/100000: episode: 848, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 194.803, mean reward: 1.948 [1.445, 4.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.513, 10.098], loss: 0.196213, mae: 0.378951, mean_q: 4.191922
 55319/100000: episode: 849, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 193.145, mean reward: 1.931 [1.443, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.210], loss: 1.428989, mae: 0.436735, mean_q: 4.169991
 55419/100000: episode: 850, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 206.401, mean reward: 2.064 [1.440, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.912, 10.098], loss: 1.381214, mae: 0.408014, mean_q: 4.108931
 55519/100000: episode: 851, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.199, mean reward: 1.902 [1.477, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.232, 10.335], loss: 0.130737, mae: 0.331645, mean_q: 3.977528
 55619/100000: episode: 852, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.138, mean reward: 1.861 [1.455, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.873, 10.167], loss: 0.106650, mae: 0.311746, mean_q: 3.893967
 55719/100000: episode: 853, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 192.013, mean reward: 1.920 [1.443, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.148], loss: 0.096101, mae: 0.310509, mean_q: 3.890717
 55819/100000: episode: 854, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 179.451, mean reward: 1.795 [1.458, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.865, 10.122], loss: 0.105128, mae: 0.322271, mean_q: 3.900809
 55919/100000: episode: 855, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.455, mean reward: 1.985 [1.478, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.389, 10.127], loss: 0.100684, mae: 0.311223, mean_q: 3.872755
 56019/100000: episode: 856, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 186.306, mean reward: 1.863 [1.457, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.828, 10.098], loss: 0.094694, mae: 0.307080, mean_q: 3.880140
 56119/100000: episode: 857, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 179.777, mean reward: 1.798 [1.446, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.100, 10.278], loss: 0.098555, mae: 0.311909, mean_q: 3.877219
 56219/100000: episode: 858, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 190.375, mean reward: 1.904 [1.447, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.846, 10.098], loss: 0.093633, mae: 0.301925, mean_q: 3.866290
 56319/100000: episode: 859, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.922, mean reward: 1.979 [1.453, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.043, 10.098], loss: 0.087807, mae: 0.304003, mean_q: 3.875911
 56419/100000: episode: 860, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.185, mean reward: 1.812 [1.433, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.072, 10.098], loss: 0.090987, mae: 0.303792, mean_q: 3.885059
 56519/100000: episode: 861, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 179.978, mean reward: 1.800 [1.474, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.619, 10.130], loss: 0.092259, mae: 0.302105, mean_q: 3.874999
 56619/100000: episode: 862, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.103, mean reward: 1.901 [1.464, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.030, 10.281], loss: 0.091768, mae: 0.305281, mean_q: 3.867575
 56719/100000: episode: 863, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.187, mean reward: 1.982 [1.459, 6.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.960, 10.098], loss: 0.082087, mae: 0.291951, mean_q: 3.841290
 56819/100000: episode: 864, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.797, mean reward: 1.938 [1.445, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.414, 10.239], loss: 0.089353, mae: 0.293051, mean_q: 3.823374
 56919/100000: episode: 865, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.831, mean reward: 1.978 [1.480, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.250, 10.427], loss: 0.094564, mae: 0.297969, mean_q: 3.854794
 57019/100000: episode: 866, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.782, mean reward: 1.858 [1.442, 4.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.043, 10.329], loss: 0.080561, mae: 0.286805, mean_q: 3.844540
 57119/100000: episode: 867, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 177.862, mean reward: 1.779 [1.437, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.814, 10.098], loss: 0.084238, mae: 0.298571, mean_q: 3.859928
 57219/100000: episode: 868, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 192.487, mean reward: 1.925 [1.459, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.086, 10.098], loss: 0.086657, mae: 0.296122, mean_q: 3.863518
 57319/100000: episode: 869, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.693, mean reward: 1.897 [1.432, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.331, 10.242], loss: 0.092880, mae: 0.300990, mean_q: 3.872740
 57419/100000: episode: 870, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.284, mean reward: 1.913 [1.465, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.127], loss: 0.095080, mae: 0.303545, mean_q: 3.854887
 57519/100000: episode: 871, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.457, mean reward: 1.905 [1.465, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.791, 10.311], loss: 0.082366, mae: 0.291044, mean_q: 3.842780
 57619/100000: episode: 872, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 181.180, mean reward: 1.812 [1.445, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.306, 10.098], loss: 0.084970, mae: 0.295918, mean_q: 3.852959
 57719/100000: episode: 873, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 253.763, mean reward: 2.538 [1.466, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.813, 10.098], loss: 0.082185, mae: 0.288874, mean_q: 3.835127
 57819/100000: episode: 874, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 186.746, mean reward: 1.867 [1.450, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.463, 10.098], loss: 0.091770, mae: 0.301370, mean_q: 3.875516
 57919/100000: episode: 875, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 203.707, mean reward: 2.037 [1.453, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.029, 10.098], loss: 0.072874, mae: 0.275926, mean_q: 3.840920
 58019/100000: episode: 876, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 198.105, mean reward: 1.981 [1.453, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.978, 10.184], loss: 0.084442, mae: 0.287608, mean_q: 3.841654
 58119/100000: episode: 877, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.866, mean reward: 1.859 [1.463, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.550, 10.378], loss: 0.090717, mae: 0.295049, mean_q: 3.839119
 58219/100000: episode: 878, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.491, mean reward: 1.845 [1.458, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.628, 10.190], loss: 0.083447, mae: 0.289027, mean_q: 3.837006
 58319/100000: episode: 879, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 212.775, mean reward: 2.128 [1.457, 6.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.080, 10.355], loss: 0.092354, mae: 0.301492, mean_q: 3.856285
 58419/100000: episode: 880, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 179.231, mean reward: 1.792 [1.453, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.105], loss: 0.090223, mae: 0.292632, mean_q: 3.827370
 58519/100000: episode: 881, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 205.850, mean reward: 2.059 [1.449, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.098], loss: 0.086243, mae: 0.297497, mean_q: 3.845755
 58619/100000: episode: 882, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 195.309, mean reward: 1.953 [1.506, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.541, 10.336], loss: 0.093159, mae: 0.296596, mean_q: 3.824432
 58719/100000: episode: 883, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 203.300, mean reward: 2.033 [1.535, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.897, 10.098], loss: 0.099317, mae: 0.304711, mean_q: 3.839356
 58819/100000: episode: 884, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.403, mean reward: 1.924 [1.480, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.098], loss: 0.087202, mae: 0.290973, mean_q: 3.830581
 58919/100000: episode: 885, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 215.733, mean reward: 2.157 [1.463, 5.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.469, 10.431], loss: 0.081070, mae: 0.288695, mean_q: 3.827169
 59019/100000: episode: 886, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 184.082, mean reward: 1.841 [1.507, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.131, 10.279], loss: 0.088785, mae: 0.298035, mean_q: 3.842824
 59119/100000: episode: 887, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.210, mean reward: 1.822 [1.435, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.098], loss: 0.088203, mae: 0.293022, mean_q: 3.846305
 59219/100000: episode: 888, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 209.587, mean reward: 2.096 [1.463, 9.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.856, 10.098], loss: 0.084251, mae: 0.291074, mean_q: 3.828902
 59319/100000: episode: 889, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 201.770, mean reward: 2.018 [1.449, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.895, 10.098], loss: 0.090436, mae: 0.297919, mean_q: 3.824960
 59419/100000: episode: 890, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 221.166, mean reward: 2.212 [1.508, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.524, 10.387], loss: 0.097471, mae: 0.310130, mean_q: 3.831815
 59519/100000: episode: 891, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 181.867, mean reward: 1.819 [1.450, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.146, 10.098], loss: 0.092613, mae: 0.302572, mean_q: 3.811602
 59619/100000: episode: 892, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 178.325, mean reward: 1.783 [1.438, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.433, 10.158], loss: 0.092248, mae: 0.307346, mean_q: 3.840818
 59719/100000: episode: 893, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 193.619, mean reward: 1.936 [1.444, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.098], loss: 0.085942, mae: 0.298477, mean_q: 3.826819
 59819/100000: episode: 894, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 213.179, mean reward: 2.132 [1.480, 5.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.921, 10.562], loss: 0.105620, mae: 0.313428, mean_q: 3.828057
 59919/100000: episode: 895, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.371, mean reward: 2.004 [1.471, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.098], loss: 0.088511, mae: 0.291831, mean_q: 3.821612
 60019/100000: episode: 896, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 194.164, mean reward: 1.942 [1.450, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.951, 10.098], loss: 0.086722, mae: 0.297330, mean_q: 3.831864
 60119/100000: episode: 897, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 203.757, mean reward: 2.038 [1.493, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.724, 10.218], loss: 0.094901, mae: 0.300247, mean_q: 3.817537
 60219/100000: episode: 898, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.117, mean reward: 1.921 [1.453, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.066, 10.098], loss: 0.080848, mae: 0.285945, mean_q: 3.816393
 60319/100000: episode: 899, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.385, mean reward: 1.894 [1.446, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.814, 10.146], loss: 0.094627, mae: 0.288857, mean_q: 3.821168
 60419/100000: episode: 900, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.597, mean reward: 1.926 [1.466, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.279, 10.098], loss: 0.089093, mae: 0.297576, mean_q: 3.835669
 60519/100000: episode: 901, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.086, mean reward: 1.911 [1.457, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.830, 10.098], loss: 0.099712, mae: 0.307331, mean_q: 3.837442
[Info] 1-TH LEVEL FOUND: 5.011612892150879, Considering 10/90 traces
 60619/100000: episode: 902, duration: 4.637s, episode steps: 100, steps per second: 22, episode reward: 193.471, mean reward: 1.935 [1.437, 7.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.400, 10.098], loss: 0.099103, mae: 0.306829, mean_q: 3.843148
 60629/100000: episode: 903, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 23.789, mean reward: 2.379 [2.047, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.153, 10.100], loss: 0.067150, mae: 0.272602, mean_q: 3.831435
 60679/100000: episode: 904, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 111.417, mean reward: 2.228 [1.463, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.977, 10.119], loss: 0.114048, mae: 0.311416, mean_q: 3.838682
 60729/100000: episode: 905, duration: 0.256s, episode steps: 50, steps per second: 196, episode reward: 155.507, mean reward: 3.110 [2.287, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.350, 10.381], loss: 0.103046, mae: 0.295715, mean_q: 3.848887
 60747/100000: episode: 906, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 51.160, mean reward: 2.842 [1.905, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.604, 10.100], loss: 0.096507, mae: 0.311464, mean_q: 3.883425
 60763/100000: episode: 907, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 37.061, mean reward: 2.316 [1.987, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.229, 10.100], loss: 0.094367, mae: 0.298666, mean_q: 3.879766
 60773/100000: episode: 908, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 21.356, mean reward: 2.136 [1.948, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.307, 10.100], loss: 0.113268, mae: 0.292426, mean_q: 3.850129
 60864/100000: episode: 909, duration: 0.477s, episode steps: 91, steps per second: 191, episode reward: 200.920, mean reward: 2.208 [1.449, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-2.249, 10.475], loss: 0.116857, mae: 0.322222, mean_q: 3.889499
 60901/100000: episode: 910, duration: 0.201s, episode steps: 37, steps per second: 185, episode reward: 96.714, mean reward: 2.614 [1.891, 4.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.314], loss: 0.113168, mae: 0.311962, mean_q: 3.914348
 60939/100000: episode: 911, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 100.768, mean reward: 2.652 [2.006, 6.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.457, 10.443], loss: 0.103361, mae: 0.292463, mean_q: 3.906458
 60957/100000: episode: 912, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 49.211, mean reward: 2.734 [1.949, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.384, 10.100], loss: 0.116792, mae: 0.309947, mean_q: 3.939411
 60990/100000: episode: 913, duration: 0.188s, episode steps: 33, steps per second: 175, episode reward: 74.296, mean reward: 2.251 [1.575, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.067, 10.285], loss: 0.127034, mae: 0.329199, mean_q: 3.900418
 61000/100000: episode: 914, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 26.217, mean reward: 2.622 [2.374, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.106, 10.100], loss: 0.121824, mae: 0.334365, mean_q: 3.895165
 61033/100000: episode: 915, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 58.136, mean reward: 1.762 [1.480, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.643, 10.177], loss: 0.143881, mae: 0.344079, mean_q: 3.896492
 61070/100000: episode: 916, duration: 0.226s, episode steps: 37, steps per second: 164, episode reward: 78.195, mean reward: 2.113 [1.844, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.571, 10.366], loss: 0.084687, mae: 0.297720, mean_q: 3.915017
 61120/100000: episode: 917, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 163.008, mean reward: 3.260 [2.326, 5.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.351, 10.403], loss: 0.124823, mae: 0.328513, mean_q: 3.964418
 61211/100000: episode: 918, duration: 0.514s, episode steps: 91, steps per second: 177, episode reward: 185.132, mean reward: 2.034 [1.446, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.849, 10.100], loss: 0.119833, mae: 0.326313, mean_q: 3.948280
 61302/100000: episode: 919, duration: 0.501s, episode steps: 91, steps per second: 182, episode reward: 173.796, mean reward: 1.910 [1.460, 4.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.651, 10.111], loss: 0.098264, mae: 0.311481, mean_q: 3.918869
 61339/100000: episode: 920, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 77.686, mean reward: 2.100 [1.493, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.899, 10.224], loss: 0.142929, mae: 0.327430, mean_q: 3.947407
 61372/100000: episode: 921, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 74.315, mean reward: 2.252 [1.670, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.429, 10.345], loss: 0.109698, mae: 0.331393, mean_q: 3.913287
 61463/100000: episode: 922, duration: 0.501s, episode steps: 91, steps per second: 182, episode reward: 173.107, mean reward: 1.902 [1.485, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.696, 10.210], loss: 0.106010, mae: 0.321800, mean_q: 3.970757
 61501/100000: episode: 923, duration: 0.228s, episode steps: 38, steps per second: 167, episode reward: 75.923, mean reward: 1.998 [1.579, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.122, 10.273], loss: 0.109213, mae: 0.325206, mean_q: 3.989349
 61551/100000: episode: 924, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 124.644, mean reward: 2.493 [1.719, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.544, 10.507], loss: 0.116033, mae: 0.326136, mean_q: 3.974991
 61642/100000: episode: 925, duration: 0.511s, episode steps: 91, steps per second: 178, episode reward: 181.984, mean reward: 2.000 [1.463, 5.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-1.874, 10.100], loss: 0.131011, mae: 0.342948, mean_q: 4.003431
 61660/100000: episode: 926, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 50.979, mean reward: 2.832 [2.006, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.872, 10.100], loss: 0.081776, mae: 0.286475, mean_q: 3.871657
 61698/100000: episode: 927, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 79.987, mean reward: 2.105 [1.712, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.660, 10.314], loss: 0.120405, mae: 0.333686, mean_q: 3.982836
 61789/100000: episode: 928, duration: 0.488s, episode steps: 91, steps per second: 186, episode reward: 175.500, mean reward: 1.929 [1.491, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.943, 10.100], loss: 0.116229, mae: 0.334675, mean_q: 3.986758
 61802/100000: episode: 929, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 35.414, mean reward: 2.724 [2.121, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.391, 10.100], loss: 0.179274, mae: 0.364994, mean_q: 4.058773
 61815/100000: episode: 930, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 25.982, mean reward: 1.999 [1.860, 2.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.221, 10.100], loss: 0.115583, mae: 0.321184, mean_q: 3.948477
 61848/100000: episode: 931, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 81.160, mean reward: 2.459 [1.603, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.275, 10.244], loss: 0.097297, mae: 0.320661, mean_q: 3.970630
 61864/100000: episode: 932, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 33.830, mean reward: 2.114 [1.686, 2.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.151, 10.100], loss: 0.131623, mae: 0.359869, mean_q: 4.024617
 61874/100000: episode: 933, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 21.527, mean reward: 2.153 [1.810, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.074, 10.100], loss: 0.129954, mae: 0.354883, mean_q: 3.970753
 61895/100000: episode: 934, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 78.365, mean reward: 3.732 [2.686, 7.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.390, 10.100], loss: 0.118130, mae: 0.332891, mean_q: 4.004973
 61911/100000: episode: 935, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 38.582, mean reward: 2.411 [2.059, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.746, 10.100], loss: 0.115089, mae: 0.329340, mean_q: 4.021935
 61961/100000: episode: 936, duration: 0.257s, episode steps: 50, steps per second: 194, episode reward: 139.650, mean reward: 2.793 [1.595, 5.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.331, 10.593], loss: 0.123503, mae: 0.336541, mean_q: 3.996783
 62052/100000: episode: 937, duration: 0.486s, episode steps: 91, steps per second: 187, episode reward: 193.229, mean reward: 2.123 [1.451, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-2.250, 10.100], loss: 0.125820, mae: 0.334179, mean_q: 3.996565
 62065/100000: episode: 938, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 29.964, mean reward: 2.305 [2.002, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.387, 10.100], loss: 0.119061, mae: 0.368706, mean_q: 4.062088
 62083/100000: episode: 939, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 58.680, mean reward: 3.260 [1.829, 6.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.436, 10.100], loss: 0.126796, mae: 0.333903, mean_q: 3.990529
 62133/100000: episode: 940, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 113.701, mean reward: 2.274 [1.662, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.336, 10.188], loss: 0.129816, mae: 0.367474, mean_q: 4.074307
 62183/100000: episode: 941, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 153.465, mean reward: 3.069 [2.052, 6.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.866, 10.317], loss: 0.134832, mae: 0.348326, mean_q: 4.070332
 62199/100000: episode: 942, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 38.727, mean reward: 2.420 [2.040, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.451, 10.100], loss: 0.104875, mae: 0.333400, mean_q: 4.065212
 62236/100000: episode: 943, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 84.907, mean reward: 2.295 [1.497, 7.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.254, 10.106], loss: 0.121834, mae: 0.337071, mean_q: 4.064244
 62249/100000: episode: 944, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 31.092, mean reward: 2.392 [2.157, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.305, 10.100], loss: 0.155603, mae: 0.367073, mean_q: 4.107663
 62287/100000: episode: 945, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 85.459, mean reward: 2.249 [1.659, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.356, 10.215], loss: 0.136066, mae: 0.340076, mean_q: 4.083066
 62325/100000: episode: 946, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 77.474, mean reward: 2.039 [1.598, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.239, 10.100], loss: 0.137610, mae: 0.351349, mean_q: 4.112643
 62343/100000: episode: 947, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 41.762, mean reward: 2.320 [2.023, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.260, 10.100], loss: 0.142201, mae: 0.362740, mean_q: 4.100653
 62434/100000: episode: 948, duration: 0.475s, episode steps: 91, steps per second: 191, episode reward: 180.776, mean reward: 1.987 [1.514, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.780, 10.100], loss: 0.119929, mae: 0.341786, mean_q: 4.127152
 62484/100000: episode: 949, duration: 0.283s, episode steps: 50, steps per second: 176, episode reward: 129.744, mean reward: 2.595 [1.668, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.674, 10.331], loss: 0.134281, mae: 0.351587, mean_q: 4.100174
 62505/100000: episode: 950, duration: 0.122s, episode steps: 21, steps per second: 173, episode reward: 45.998, mean reward: 2.190 [1.747, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.455, 10.100], loss: 0.122143, mae: 0.332809, mean_q: 4.012967
 62596/100000: episode: 951, duration: 0.493s, episode steps: 91, steps per second: 185, episode reward: 173.823, mean reward: 1.910 [1.441, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.733, 10.181], loss: 0.148466, mae: 0.362694, mean_q: 4.115522
 62609/100000: episode: 952, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 28.431, mean reward: 2.187 [1.776, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.343, 10.100], loss: 0.178325, mae: 0.368375, mean_q: 4.121249
 62622/100000: episode: 953, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 32.631, mean reward: 2.510 [1.720, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.776, 10.100], loss: 0.124864, mae: 0.352196, mean_q: 4.160686
 62640/100000: episode: 954, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 38.378, mean reward: 2.132 [1.639, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.234, 10.100], loss: 0.111851, mae: 0.342332, mean_q: 4.122147
 62656/100000: episode: 955, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 36.573, mean reward: 2.286 [1.597, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.155, 10.100], loss: 0.137321, mae: 0.347396, mean_q: 4.085874
 62674/100000: episode: 956, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 47.021, mean reward: 2.612 [1.830, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.454, 10.100], loss: 0.135265, mae: 0.366901, mean_q: 4.077604
 62690/100000: episode: 957, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 36.831, mean reward: 2.302 [1.978, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.100], loss: 0.209063, mae: 0.388627, mean_q: 4.139907
 62728/100000: episode: 958, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 117.144, mean reward: 3.083 [1.934, 5.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.811, 10.391], loss: 0.105837, mae: 0.318913, mean_q: 4.083647
 62761/100000: episode: 959, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 66.067, mean reward: 2.002 [1.472, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.254, 10.214], loss: 0.127476, mae: 0.350774, mean_q: 4.043641
 62774/100000: episode: 960, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 29.647, mean reward: 2.281 [1.782, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.671, 10.100], loss: 0.152447, mae: 0.366200, mean_q: 4.129292
 62784/100000: episode: 961, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 23.298, mean reward: 2.330 [2.162, 2.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.252, 10.100], loss: 0.267937, mae: 0.398099, mean_q: 4.126664
 62797/100000: episode: 962, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 37.394, mean reward: 2.876 [1.860, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.584, 10.100], loss: 0.135231, mae: 0.381891, mean_q: 4.082201
 62847/100000: episode: 963, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 97.752, mean reward: 1.955 [1.533, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.924, 10.101], loss: 0.149941, mae: 0.358786, mean_q: 4.128500
 62860/100000: episode: 964, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 32.033, mean reward: 2.464 [1.873, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.794, 10.100], loss: 0.116895, mae: 0.361982, mean_q: 4.123568
 62893/100000: episode: 965, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 78.254, mean reward: 2.371 [1.866, 3.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.915, 10.442], loss: 0.156750, mae: 0.356706, mean_q: 4.107350
 62930/100000: episode: 966, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 89.776, mean reward: 2.426 [1.784, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.423, 10.337], loss: 0.137396, mae: 0.350888, mean_q: 4.080567
 62968/100000: episode: 967, duration: 0.211s, episode steps: 38, steps per second: 181, episode reward: 106.470, mean reward: 2.802 [1.785, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.337, 10.271], loss: 0.156274, mae: 0.374698, mean_q: 4.149399
 62981/100000: episode: 968, duration: 0.089s, episode steps: 13, steps per second: 145, episode reward: 26.221, mean reward: 2.017 [1.623, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.589, 10.100], loss: 0.145297, mae: 0.352286, mean_q: 4.166360
 63014/100000: episode: 969, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 88.508, mean reward: 2.682 [2.097, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.112, 10.294], loss: 0.122064, mae: 0.335589, mean_q: 4.149050
 63024/100000: episode: 970, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 24.937, mean reward: 2.494 [1.985, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.475, 10.100], loss: 0.124915, mae: 0.336588, mean_q: 4.105336
 63057/100000: episode: 971, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 72.425, mean reward: 2.195 [1.743, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.668, 10.239], loss: 0.126336, mae: 0.351504, mean_q: 4.180806
 63094/100000: episode: 972, duration: 0.216s, episode steps: 37, steps per second: 171, episode reward: 72.107, mean reward: 1.949 [1.514, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.743, 10.182], loss: 0.112902, mae: 0.333372, mean_q: 4.153963
 63127/100000: episode: 973, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 77.461, mean reward: 2.347 [1.835, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.218, 10.307], loss: 0.134667, mae: 0.344642, mean_q: 4.160121
 63137/100000: episode: 974, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 21.208, mean reward: 2.121 [1.840, 2.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.194, 10.100], loss: 0.129098, mae: 0.327996, mean_q: 4.115262
 63147/100000: episode: 975, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 20.766, mean reward: 2.077 [1.929, 2.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.179, 10.100], loss: 0.129438, mae: 0.355863, mean_q: 4.171140
 63163/100000: episode: 976, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 38.665, mean reward: 2.417 [2.004, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.255, 10.100], loss: 0.157623, mae: 0.355358, mean_q: 4.180915
 63254/100000: episode: 977, duration: 0.502s, episode steps: 91, steps per second: 181, episode reward: 174.339, mean reward: 1.916 [1.459, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-0.731, 10.200], loss: 0.141576, mae: 0.350093, mean_q: 4.174063
 63291/100000: episode: 978, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 96.740, mean reward: 2.615 [2.019, 6.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.160, 10.393], loss: 0.114161, mae: 0.332420, mean_q: 4.176340
 63301/100000: episode: 979, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 19.070, mean reward: 1.907 [1.542, 2.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.072, 10.100], loss: 0.174234, mae: 0.380537, mean_q: 4.144031
 63351/100000: episode: 980, duration: 0.254s, episode steps: 50, steps per second: 197, episode reward: 96.984, mean reward: 1.940 [1.472, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.340, 10.100], loss: 0.153556, mae: 0.364548, mean_q: 4.180035
 63369/100000: episode: 981, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 66.543, mean reward: 3.697 [2.222, 7.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.859, 10.100], loss: 0.156042, mae: 0.383266, mean_q: 4.199980
 63382/100000: episode: 982, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 26.326, mean reward: 2.025 [1.823, 2.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.254, 10.100], loss: 0.120146, mae: 0.335697, mean_q: 4.129611
 63395/100000: episode: 983, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 33.004, mean reward: 2.539 [1.854, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.402, 10.100], loss: 0.128513, mae: 0.355006, mean_q: 4.212498
 63432/100000: episode: 984, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 78.322, mean reward: 2.117 [1.491, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.318, 10.271], loss: 0.179225, mae: 0.392750, mean_q: 4.209510
 63442/100000: episode: 985, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 20.391, mean reward: 2.039 [1.816, 2.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.329, 10.100], loss: 0.117107, mae: 0.345132, mean_q: 4.168738
 63455/100000: episode: 986, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 33.221, mean reward: 2.555 [1.918, 5.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.654, 10.100], loss: 0.142446, mae: 0.346040, mean_q: 4.191406
 63468/100000: episode: 987, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 30.862, mean reward: 2.374 [1.924, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.245, 10.100], loss: 0.144744, mae: 0.367411, mean_q: 4.206933
 63505/100000: episode: 988, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 80.182, mean reward: 2.167 [1.775, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.475, 10.279], loss: 0.151000, mae: 0.367235, mean_q: 4.205623
 63521/100000: episode: 989, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 46.975, mean reward: 2.936 [1.983, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.346, 10.100], loss: 0.131270, mae: 0.353275, mean_q: 4.217951
 63542/100000: episode: 990, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 59.446, mean reward: 2.831 [2.301, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.282, 10.100], loss: 0.127630, mae: 0.340152, mean_q: 4.180979
 63563/100000: episode: 991, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 69.249, mean reward: 3.298 [2.575, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.032, 10.100], loss: 0.135963, mae: 0.362709, mean_q: 4.215050
[Info] 2-TH LEVEL FOUND: 6.841739654541016, Considering 10/90 traces
 63581/100000: episode: 992, duration: 4.208s, episode steps: 18, steps per second: 4, episode reward: 56.475, mean reward: 3.138 [1.902, 5.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.851, 10.100], loss: 0.127958, mae: 0.345517, mean_q: 4.195335
 63609/100000: episode: 993, duration: 0.176s, episode steps: 28, steps per second: 159, episode reward: 72.211, mean reward: 2.579 [2.014, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.306], loss: 0.241218, mae: 0.394868, mean_q: 4.196653
 63628/100000: episode: 994, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 65.025, mean reward: 3.422 [2.692, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.311, 10.451], loss: 0.165014, mae: 0.404660, mean_q: 4.263152
 63638/100000: episode: 995, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 35.882, mean reward: 3.588 [2.915, 5.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.356, 10.100], loss: 0.223201, mae: 0.432360, mean_q: 4.239468
 63682/100000: episode: 996, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 111.120, mean reward: 2.525 [1.500, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.920, 10.247], loss: 0.178535, mae: 0.393739, mean_q: 4.242389
 63692/100000: episode: 997, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 34.442, mean reward: 3.444 [2.822, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.483, 10.100], loss: 0.106626, mae: 0.358960, mean_q: 4.342145
 63720/100000: episode: 998, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 71.946, mean reward: 2.569 [1.688, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.218], loss: 0.141766, mae: 0.380040, mean_q: 4.181687
 63736/100000: episode: 999, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 55.460, mean reward: 3.466 [2.662, 4.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.445], loss: 0.129355, mae: 0.365485, mean_q: 4.247772
 63744/100000: episode: 1000, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 28.202, mean reward: 3.525 [2.815, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.413, 10.100], loss: 0.175047, mae: 0.415397, mean_q: 4.349211
 63772/100000: episode: 1001, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 77.826, mean reward: 2.780 [1.879, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.256], loss: 0.162000, mae: 0.388084, mean_q: 4.324855
 63816/100000: episode: 1002, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 170.999, mean reward: 3.886 [2.329, 9.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.819, 10.339], loss: 0.175476, mae: 0.393900, mean_q: 4.330811
 63860/100000: episode: 1003, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 122.104, mean reward: 2.775 [2.117, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.657, 10.355], loss: 0.155419, mae: 0.370005, mean_q: 4.300910
 63870/100000: episode: 1004, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 46.258, mean reward: 4.626 [3.004, 7.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.533, 10.100], loss: 0.129888, mae: 0.341865, mean_q: 4.340863
 63914/100000: episode: 1005, duration: 0.251s, episode steps: 44, steps per second: 175, episode reward: 161.940, mean reward: 3.680 [2.278, 8.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.268, 10.537], loss: 0.167047, mae: 0.390112, mean_q: 4.298485
 63944/100000: episode: 1006, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 100.381, mean reward: 3.346 [2.215, 7.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.139, 10.556], loss: 0.171732, mae: 0.393085, mean_q: 4.393227
 63960/100000: episode: 1007, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 49.011, mean reward: 3.063 [2.542, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.049, 10.502], loss: 0.195939, mae: 0.386905, mean_q: 4.366437
 63990/100000: episode: 1008, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 71.727, mean reward: 2.391 [1.896, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.047, 10.385], loss: 0.209621, mae: 0.375376, mean_q: 4.329753
 64020/100000: episode: 1009, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 78.359, mean reward: 2.612 [1.722, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.123, 10.324], loss: 0.168137, mae: 0.391975, mean_q: 4.367869
 64030/100000: episode: 1010, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 40.005, mean reward: 4.001 [3.398, 4.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.444, 10.100], loss: 0.192736, mae: 0.396900, mean_q: 4.334138
 64060/100000: episode: 1011, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 93.583, mean reward: 3.119 [1.988, 7.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.532, 10.298], loss: 0.207183, mae: 0.405427, mean_q: 4.449572
 64068/100000: episode: 1012, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 32.638, mean reward: 4.080 [3.392, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.349, 10.100], loss: 0.150722, mae: 0.361838, mean_q: 4.381913
 64078/100000: episode: 1013, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 50.196, mean reward: 5.020 [3.752, 6.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.523, 10.100], loss: 0.136733, mae: 0.373491, mean_q: 4.436864
 64088/100000: episode: 1014, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 32.810, mean reward: 3.281 [2.665, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.699, 10.100], loss: 0.119400, mae: 0.367619, mean_q: 4.397855
 64107/100000: episode: 1015, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 46.109, mean reward: 2.427 [1.784, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.284, 10.352], loss: 0.184126, mae: 0.395477, mean_q: 4.432638
 64115/100000: episode: 1016, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 29.446, mean reward: 3.681 [2.780, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.551, 10.100], loss: 0.187083, mae: 0.401300, mean_q: 4.474531
 64131/100000: episode: 1017, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 52.645, mean reward: 3.290 [2.282, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.413], loss: 0.197596, mae: 0.405721, mean_q: 4.521616
 64150/100000: episode: 1018, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 54.553, mean reward: 2.871 [2.174, 4.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.728, 10.363], loss: 0.234876, mae: 0.449804, mean_q: 4.466206
 64158/100000: episode: 1019, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 23.210, mean reward: 2.901 [2.355, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.047, 10.100], loss: 0.130144, mae: 0.366720, mean_q: 4.520712
 64202/100000: episode: 1020, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 108.793, mean reward: 2.473 [1.724, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.261, 10.366], loss: 0.200588, mae: 0.411207, mean_q: 4.573536
 64212/100000: episode: 1021, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 49.441, mean reward: 4.944 [3.141, 8.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.557, 10.100], loss: 0.194331, mae: 0.412398, mean_q: 4.564597
 64242/100000: episode: 1022, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 67.983, mean reward: 2.266 [1.561, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.396, 10.148], loss: 0.177279, mae: 0.393460, mean_q: 4.457674
 64261/100000: episode: 1023, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 52.238, mean reward: 2.749 [1.912, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.398], loss: 0.192968, mae: 0.424894, mean_q: 4.479647
 64291/100000: episode: 1024, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 95.692, mean reward: 3.190 [2.305, 4.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.860, 10.525], loss: 0.209871, mae: 0.406475, mean_q: 4.450678
 64319/100000: episode: 1025, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 74.703, mean reward: 2.668 [2.270, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.348, 10.451], loss: 0.202344, mae: 0.427817, mean_q: 4.496189
 64335/100000: episode: 1026, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 36.840, mean reward: 2.303 [1.468, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.488, 10.259], loss: 0.149911, mae: 0.402641, mean_q: 4.521406
 64343/100000: episode: 1027, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 22.796, mean reward: 2.850 [2.617, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.436, 10.100], loss: 0.159375, mae: 0.361517, mean_q: 4.403797
 64387/100000: episode: 1028, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 113.348, mean reward: 2.576 [1.844, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.266, 10.319], loss: 0.152094, mae: 0.395138, mean_q: 4.567701
 64403/100000: episode: 1029, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 46.394, mean reward: 2.900 [2.155, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.411], loss: 0.185322, mae: 0.428568, mean_q: 4.542458
 64447/100000: episode: 1030, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 101.434, mean reward: 2.305 [1.568, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.931, 10.172], loss: 0.224674, mae: 0.449374, mean_q: 4.558590
 64457/100000: episode: 1031, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 51.144, mean reward: 5.114 [2.934, 7.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.496, 10.100], loss: 0.215923, mae: 0.440201, mean_q: 4.585793
 64487/100000: episode: 1032, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 112.436, mean reward: 3.748 [2.770, 6.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.601, 10.433], loss: 0.186657, mae: 0.422876, mean_q: 4.592086
 64531/100000: episode: 1033, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 123.620, mean reward: 2.810 [2.005, 6.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.574, 10.415], loss: 0.243747, mae: 0.440089, mean_q: 4.592642
 64569/100000: episode: 1034, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 90.702, mean reward: 2.387 [1.815, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.417, 10.267], loss: 0.182556, mae: 0.414469, mean_q: 4.576466
 64588/100000: episode: 1035, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 52.136, mean reward: 2.744 [2.027, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.369], loss: 0.246504, mae: 0.430689, mean_q: 4.676625
 64607/100000: episode: 1036, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 53.464, mean reward: 2.814 [2.245, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.483, 10.377], loss: 0.164063, mae: 0.401255, mean_q: 4.599315
 64617/100000: episode: 1037, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 45.931, mean reward: 4.593 [3.309, 6.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.381, 10.100], loss: 0.225523, mae: 0.433511, mean_q: 4.734376
 64625/100000: episode: 1038, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 30.379, mean reward: 3.797 [2.970, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.422, 10.100], loss: 0.215668, mae: 0.438685, mean_q: 4.432930
 64633/100000: episode: 1039, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 51.254, mean reward: 6.407 [3.821, 9.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.429, 10.100], loss: 0.131671, mae: 0.406531, mean_q: 4.732796
 64661/100000: episode: 1040, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 92.654, mean reward: 3.309 [2.151, 5.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.451, 10.321], loss: 0.284731, mae: 0.466913, mean_q: 4.727162
 64669/100000: episode: 1041, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 22.882, mean reward: 2.860 [2.599, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.353, 10.100], loss: 0.148524, mae: 0.405684, mean_q: 4.532806
 64679/100000: episode: 1042, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 39.169, mean reward: 3.917 [2.941, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.521, 10.100], loss: 0.167050, mae: 0.404337, mean_q: 4.703807
 64717/100000: episode: 1043, duration: 0.194s, episode steps: 38, steps per second: 195, episode reward: 99.843, mean reward: 2.627 [1.642, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.611, 10.277], loss: 0.185099, mae: 0.421703, mean_q: 4.662221
 64747/100000: episode: 1044, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 85.918, mean reward: 2.864 [1.990, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.333], loss: 0.240921, mae: 0.441866, mean_q: 4.731975
 64777/100000: episode: 1045, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 90.903, mean reward: 3.030 [2.038, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.491, 10.378], loss: 0.224583, mae: 0.446066, mean_q: 4.717140
 64785/100000: episode: 1046, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 42.802, mean reward: 5.350 [3.622, 9.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.531, 10.100], loss: 0.266990, mae: 0.493732, mean_q: 4.872613
 64813/100000: episode: 1047, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 75.386, mean reward: 2.692 [2.088, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.373], loss: 0.200889, mae: 0.431422, mean_q: 4.655722
 64823/100000: episode: 1048, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 40.209, mean reward: 4.021 [3.318, 5.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.606, 10.100], loss: 0.236884, mae: 0.490589, mean_q: 4.796140
 64867/100000: episode: 1049, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 110.434, mean reward: 2.510 [1.506, 4.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.475, 10.224], loss: 0.239163, mae: 0.453468, mean_q: 4.739798
 64875/100000: episode: 1050, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 27.546, mean reward: 3.443 [2.670, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.331, 10.100], loss: 0.177945, mae: 0.402356, mean_q: 4.746699
 64894/100000: episode: 1051, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 57.684, mean reward: 3.036 [2.316, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.599, 10.457], loss: 0.196266, mae: 0.406249, mean_q: 4.678817
 64910/100000: episode: 1052, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 55.172, mean reward: 3.448 [2.493, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.245, 10.432], loss: 0.252542, mae: 0.467458, mean_q: 4.828837
 64918/100000: episode: 1053, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 25.983, mean reward: 3.248 [2.634, 4.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.262, 10.100], loss: 0.226301, mae: 0.439982, mean_q: 4.792572
 64926/100000: episode: 1054, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 26.725, mean reward: 3.341 [2.871, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.361, 10.100], loss: 0.236960, mae: 0.434599, mean_q: 4.809336
 64936/100000: episode: 1055, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 32.276, mean reward: 3.228 [2.840, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.457, 10.100], loss: 0.277194, mae: 0.462204, mean_q: 4.744069
 64966/100000: episode: 1056, duration: 0.163s, episode steps: 30, steps per second: 185, episode reward: 98.212, mean reward: 3.274 [2.357, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.363, 10.449], loss: 0.200222, mae: 0.429113, mean_q: 4.717888
 64974/100000: episode: 1057, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 28.303, mean reward: 3.538 [2.666, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.337, 10.100], loss: 0.247226, mae: 0.500412, mean_q: 4.921179
 64982/100000: episode: 1058, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 35.621, mean reward: 4.453 [3.857, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.425, 10.100], loss: 0.234187, mae: 0.420181, mean_q: 4.779912
 64992/100000: episode: 1059, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 38.151, mean reward: 3.815 [3.116, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.829, 10.100], loss: 0.246879, mae: 0.424538, mean_q: 4.640514
 65030/100000: episode: 1060, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 260.929, mean reward: 6.867 [2.406, 42.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.967, 10.549], loss: 0.210587, mae: 0.437056, mean_q: 4.780132
 65068/100000: episode: 1061, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 97.849, mean reward: 2.575 [1.614, 7.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.951, 10.245], loss: 0.219347, mae: 0.444913, mean_q: 4.874762
 65076/100000: episode: 1062, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 22.347, mean reward: 2.793 [2.284, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.372, 10.100], loss: 3.184703, mae: 0.763052, mean_q: 5.013453
 65084/100000: episode: 1063, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 31.245, mean reward: 3.906 [3.280, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.388, 10.100], loss: 0.321330, mae: 0.525449, mean_q: 4.472890
 65103/100000: episode: 1064, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 45.374, mean reward: 2.388 [1.952, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.390], loss: 0.316458, mae: 0.526199, mean_q: 4.834601
 65122/100000: episode: 1065, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 63.221, mean reward: 3.327 [2.572, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.086, 10.451], loss: 0.241257, mae: 0.462839, mean_q: 4.865678
 65150/100000: episode: 1066, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 100.449, mean reward: 3.587 [2.618, 5.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.477], loss: 0.237117, mae: 0.465453, mean_q: 4.907109
 65158/100000: episode: 1067, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 51.838, mean reward: 6.480 [4.203, 11.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.433, 10.100], loss: 0.305451, mae: 0.503222, mean_q: 4.865160
 65177/100000: episode: 1068, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 58.674, mean reward: 3.088 [2.316, 7.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.197, 10.637], loss: 1.441844, mae: 0.586808, mean_q: 4.992105
 65187/100000: episode: 1069, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 33.540, mean reward: 3.354 [2.892, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.441, 10.100], loss: 0.869636, mae: 0.704679, mean_q: 5.146198
 65215/100000: episode: 1070, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 63.172, mean reward: 2.256 [1.598, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.625, 10.365], loss: 0.280173, mae: 0.515446, mean_q: 4.908187
 65253/100000: episode: 1071, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 122.416, mean reward: 3.221 [1.634, 6.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.120, 10.312], loss: 0.367266, mae: 0.479036, mean_q: 4.872363
 65272/100000: episode: 1072, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 58.819, mean reward: 3.096 [1.680, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.237], loss: 0.269891, mae: 0.471247, mean_q: 4.918420
 65282/100000: episode: 1073, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 36.239, mean reward: 3.624 [2.933, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.861, 10.100], loss: 0.176255, mae: 0.396772, mean_q: 4.762434
 65310/100000: episode: 1074, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 79.004, mean reward: 2.822 [2.082, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.111, 10.321], loss: 1.083471, mae: 0.548134, mean_q: 4.990499
 65329/100000: episode: 1075, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 64.441, mean reward: 3.392 [2.153, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.404], loss: 0.237432, mae: 0.484971, mean_q: 4.832578
 65357/100000: episode: 1076, duration: 0.159s, episode steps: 28, steps per second: 177, episode reward: 79.075, mean reward: 2.824 [2.228, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.614, 10.401], loss: 0.440461, mae: 0.509105, mean_q: 4.963254
 65401/100000: episode: 1077, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 114.333, mean reward: 2.598 [1.650, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.585, 10.319], loss: 0.392515, mae: 0.506320, mean_q: 5.008377
 65439/100000: episode: 1078, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 128.183, mean reward: 3.373 [2.071, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.833, 10.381], loss: 0.247610, mae: 0.477881, mean_q: 4.904363
 65455/100000: episode: 1079, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 51.150, mean reward: 3.197 [2.080, 5.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.423, 10.447], loss: 0.258839, mae: 0.447478, mean_q: 4.939332
 65485/100000: episode: 1080, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 69.156, mean reward: 2.305 [1.775, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.263, 10.260], loss: 0.416213, mae: 0.479181, mean_q: 5.068944
 65493/100000: episode: 1081, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 26.040, mean reward: 3.255 [2.433, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.884, 10.100], loss: 0.217654, mae: 0.482178, mean_q: 4.894967
[Info] 3-TH LEVEL FOUND: 8.738369941711426, Considering 10/90 traces
 65503/100000: episode: 1082, duration: 4.128s, episode steps: 10, steps per second: 2, episode reward: 37.880, mean reward: 3.788 [3.227, 4.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.943, 10.100], loss: 0.232034, mae: 0.448934, mean_q: 4.905465
 65513/100000: episode: 1083, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 42.164, mean reward: 4.216 [3.297, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.544], loss: 0.285653, mae: 0.468277, mean_q: 5.061330
 65546/100000: episode: 1084, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 129.063, mean reward: 3.911 [2.898, 6.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.035, 10.471], loss: 1.691259, mae: 0.621920, mean_q: 5.104029
 65567/100000: episode: 1085, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 114.588, mean reward: 5.457 [3.876, 10.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.770, 10.576], loss: 0.268231, mae: 0.509562, mean_q: 5.159512
 65583/100000: episode: 1086, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 62.045, mean reward: 3.878 [2.563, 6.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.481], loss: 0.205064, mae: 0.435555, mean_q: 4.940243
 65602/100000: episode: 1087, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 55.041, mean reward: 2.897 [2.583, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.456], loss: 0.238086, mae: 0.466511, mean_q: 5.060225
 65623/100000: episode: 1088, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 76.879, mean reward: 3.661 [2.723, 6.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.509], loss: 0.220759, mae: 0.453416, mean_q: 4.957314
 65648/100000: episode: 1089, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 75.427, mean reward: 3.017 [1.800, 5.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.865, 10.305], loss: 1.269904, mae: 0.688834, mean_q: 5.260454
 65663/100000: episode: 1090, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 58.039, mean reward: 3.869 [3.288, 5.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-1.328, 10.430], loss: 0.333559, mae: 0.534029, mean_q: 5.112929
 65673/100000: episode: 1091, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 40.809, mean reward: 4.081 [2.706, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.416], loss: 0.354683, mae: 0.493109, mean_q: 4.964694
 65697/100000: episode: 1092, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 102.160, mean reward: 4.257 [2.793, 14.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.896, 10.448], loss: 0.284582, mae: 0.497068, mean_q: 5.098685
 65718/100000: episode: 1093, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 81.260, mean reward: 3.870 [2.739, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.419, 10.466], loss: 0.287720, mae: 0.511528, mean_q: 5.101930
 65748/100000: episode: 1094, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 101.354, mean reward: 3.378 [2.109, 5.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.094, 10.381], loss: 1.104246, mae: 0.612316, mean_q: 5.106999
 65781/100000: episode: 1095, duration: 0.203s, episode steps: 33, steps per second: 162, episode reward: 143.455, mean reward: 4.347 [3.013, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-1.130, 10.508], loss: 0.412838, mae: 0.543256, mean_q: 5.189198
 65806/100000: episode: 1096, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 106.166, mean reward: 4.247 [2.476, 6.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.049, 10.446], loss: 0.268398, mae: 0.515086, mean_q: 5.203196
 65830/100000: episode: 1097, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 94.962, mean reward: 3.957 [2.608, 7.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.341, 10.450], loss: 0.273572, mae: 0.497573, mean_q: 5.234997
 65845/100000: episode: 1098, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 64.113, mean reward: 4.274 [2.852, 6.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.222, 10.496], loss: 0.215902, mae: 0.460878, mean_q: 5.090209
 65861/100000: episode: 1099, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 49.852, mean reward: 3.116 [2.131, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.515], loss: 0.324189, mae: 0.527226, mean_q: 5.207849
 65877/100000: episode: 1100, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 58.181, mean reward: 3.636 [2.882, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.195, 10.521], loss: 0.287444, mae: 0.483900, mean_q: 5.140252
 65892/100000: episode: 1101, duration: 0.080s, episode steps: 15, steps per second: 189, episode reward: 108.453, mean reward: 7.230 [2.914, 27.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.185, 10.650], loss: 0.317145, mae: 0.532771, mean_q: 5.273787
 65909/100000: episode: 1102, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 54.383, mean reward: 3.199 [2.650, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.410], loss: 0.221659, mae: 0.461784, mean_q: 5.267452
 65942/100000: episode: 1103, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 117.196, mean reward: 3.551 [2.251, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.734, 10.469], loss: 0.589870, mae: 0.590186, mean_q: 5.303421
 65952/100000: episode: 1104, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 36.018, mean reward: 3.602 [3.033, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.534], loss: 1.210377, mae: 0.647810, mean_q: 5.291481
 65969/100000: episode: 1105, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 55.283, mean reward: 3.252 [2.789, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.524], loss: 0.757218, mae: 0.698077, mean_q: 5.332516
 65984/100000: episode: 1106, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 65.130, mean reward: 4.342 [3.399, 5.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.245, 10.532], loss: 0.316772, mae: 0.529055, mean_q: 5.148620
 66000/100000: episode: 1107, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 79.283, mean reward: 4.955 [3.621, 14.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.200, 10.527], loss: 0.241478, mae: 0.496767, mean_q: 5.270543
 66021/100000: episode: 1108, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 84.564, mean reward: 4.027 [2.967, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.548], loss: 0.435070, mae: 0.541167, mean_q: 5.202587
 66036/100000: episode: 1109, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 71.978, mean reward: 4.799 [3.173, 7.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.399, 10.621], loss: 0.964898, mae: 0.627321, mean_q: 5.428067
 66069/100000: episode: 1110, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 120.883, mean reward: 3.663 [2.505, 7.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.431], loss: 0.467230, mae: 0.555355, mean_q: 5.378753
 66079/100000: episode: 1111, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 48.678, mean reward: 4.868 [3.890, 10.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.597], loss: 0.381354, mae: 0.526741, mean_q: 5.311970
 66096/100000: episode: 1112, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 63.368, mean reward: 3.728 [3.220, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.554], loss: 0.367987, mae: 0.515061, mean_q: 5.273304
 66117/100000: episode: 1113, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 80.129, mean reward: 3.816 [2.000, 7.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.382], loss: 0.767224, mae: 0.577025, mean_q: 5.400871
 66136/100000: episode: 1114, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 128.422, mean reward: 6.759 [3.487, 13.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.259, 10.581], loss: 0.488612, mae: 0.657671, mean_q: 5.535028
 66161/100000: episode: 1115, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 94.754, mean reward: 3.790 [1.949, 8.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.102, 10.338], loss: 0.680348, mae: 0.594324, mean_q: 5.347693
 66177/100000: episode: 1116, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 68.403, mean reward: 4.275 [3.166, 6.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.245, 10.457], loss: 0.293347, mae: 0.512340, mean_q: 5.309360
 66207/100000: episode: 1117, duration: 0.183s, episode steps: 30, steps per second: 164, episode reward: 91.883, mean reward: 3.063 [2.571, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.478], loss: 0.355940, mae: 0.551141, mean_q: 5.460234
 66232/100000: episode: 1118, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 75.583, mean reward: 3.023 [2.022, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.184, 10.334], loss: 1.392959, mae: 0.657008, mean_q: 5.481627
 66257/100000: episode: 1119, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 99.820, mean reward: 3.993 [2.542, 6.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.653], loss: 0.724926, mae: 0.619940, mean_q: 5.591462
 66272/100000: episode: 1120, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 36.504, mean reward: 2.434 [2.036, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.068, 10.310], loss: 0.450427, mae: 0.615220, mean_q: 5.530521
 66296/100000: episode: 1121, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 70.260, mean reward: 2.928 [2.194, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.478], loss: 0.681890, mae: 0.568400, mean_q: 5.395862
 66313/100000: episode: 1122, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 75.073, mean reward: 4.416 [3.400, 7.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.493], loss: 0.480455, mae: 0.645213, mean_q: 5.576982
 66343/100000: episode: 1123, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 102.758, mean reward: 3.425 [2.452, 4.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.683, 10.391], loss: 1.040140, mae: 0.584663, mean_q: 5.541146
 66362/100000: episode: 1124, duration: 0.111s, episode steps: 19, steps per second: 170, episode reward: 72.287, mean reward: 3.805 [2.877, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.557], loss: 0.373952, mae: 0.597103, mean_q: 5.440227
 66372/100000: episode: 1125, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 120.275, mean reward: 12.027 [4.432, 22.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.342, 10.697], loss: 0.638978, mae: 0.652405, mean_q: 5.516999
 66389/100000: episode: 1126, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 105.194, mean reward: 6.188 [3.148, 35.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.623], loss: 0.431601, mae: 0.642963, mean_q: 5.641962
 66413/100000: episode: 1127, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 106.731, mean reward: 4.447 [3.394, 7.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.655, 10.474], loss: 0.572240, mae: 0.648208, mean_q: 5.749320
 66434/100000: episode: 1128, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 82.429, mean reward: 3.925 [2.504, 7.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.710, 10.484], loss: 0.440672, mae: 0.611404, mean_q: 5.645162
 66451/100000: episode: 1129, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 56.142, mean reward: 3.302 [2.640, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.622, 10.485], loss: 0.574956, mae: 0.660700, mean_q: 5.674695
 66470/100000: episode: 1130, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 71.502, mean reward: 3.763 [2.730, 6.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.464, 10.644], loss: 0.484723, mae: 0.603429, mean_q: 5.590253
 66503/100000: episode: 1131, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 91.431, mean reward: 2.771 [1.906, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.300, 10.378], loss: 1.973818, mae: 0.796198, mean_q: 5.719971
 66524/100000: episode: 1132, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 89.149, mean reward: 4.245 [2.941, 6.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.083, 10.448], loss: 0.854539, mae: 0.713873, mean_q: 5.638554
 66540/100000: episode: 1133, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 58.750, mean reward: 3.672 [2.665, 5.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.093, 10.547], loss: 0.637231, mae: 0.718777, mean_q: 5.820472
 66557/100000: episode: 1134, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 71.787, mean reward: 4.223 [2.852, 5.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.493], loss: 0.946543, mae: 0.656444, mean_q: 5.763091
 66581/100000: episode: 1135, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 74.281, mean reward: 3.095 [2.344, 4.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.895, 10.385], loss: 0.382442, mae: 0.565532, mean_q: 5.797147
 66611/100000: episode: 1136, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 88.408, mean reward: 2.947 [1.509, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.088, 10.117], loss: 0.463934, mae: 0.583420, mean_q: 5.575329
 66632/100000: episode: 1137, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 83.008, mean reward: 3.953 [2.177, 8.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.455, 10.305], loss: 0.496676, mae: 0.592420, mean_q: 5.709878
 66651/100000: episode: 1138, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 88.843, mean reward: 4.676 [3.122, 6.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.548, 10.593], loss: 0.627798, mae: 0.675810, mean_q: 5.753798
 66681/100000: episode: 1139, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 86.511, mean reward: 2.884 [1.852, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.303, 10.230], loss: 0.449403, mae: 0.582208, mean_q: 5.706464
 66691/100000: episode: 1140, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 35.515, mean reward: 3.552 [2.961, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.513], loss: 0.500619, mae: 0.615636, mean_q: 5.958570
 66716/100000: episode: 1141, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 75.953, mean reward: 3.038 [2.013, 6.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.365], loss: 0.565663, mae: 0.640872, mean_q: 5.805568
 66732/100000: episode: 1142, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 45.784, mean reward: 2.861 [2.470, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.538, 10.448], loss: 0.709082, mae: 0.677391, mean_q: 5.850991
 66749/100000: episode: 1143, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 82.182, mean reward: 4.834 [3.734, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.435, 10.573], loss: 1.212821, mae: 0.773109, mean_q: 6.018973
 66768/100000: episode: 1144, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 54.089, mean reward: 2.847 [2.075, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.299, 10.397], loss: 0.611138, mae: 0.658392, mean_q: 5.600653
 66784/100000: episode: 1145, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 68.100, mean reward: 4.256 [3.019, 8.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.507], loss: 0.520514, mae: 0.645663, mean_q: 5.811110
 66801/100000: episode: 1146, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 50.968, mean reward: 2.998 [2.382, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.718, 10.429], loss: 1.929361, mae: 0.772287, mean_q: 5.979153
 66818/100000: episode: 1147, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 55.138, mean reward: 3.243 [2.411, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.070, 10.470], loss: 0.817919, mae: 0.744431, mean_q: 5.667011
 66835/100000: episode: 1148, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 66.603, mean reward: 3.918 [2.762, 6.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.627, 10.366], loss: 1.395426, mae: 0.792798, mean_q: 5.897590
 66865/100000: episode: 1149, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 94.357, mean reward: 3.145 [2.302, 5.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.654, 10.480], loss: 0.718688, mae: 0.639837, mean_q: 5.951181
[Info] FALSIFICATION!
[Info] Levels: [5.011613, 6.8417397, 8.73837, 10.364237]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.17]
[Info] Error Prob: 0.00017000000000000004

 66867/100000: episode: 1150, duration: 4.415s, episode steps: 2, steps per second: 0, episode reward: 111.313, mean reward: 55.656 [11.313, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.026, 8.899], loss: 0.496253, mae: 0.834191, mean_q: 6.509213
 66967/100000: episode: 1151, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 184.984, mean reward: 1.850 [1.508, 2.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.540, 10.098], loss: 1.295472, mae: 0.716432, mean_q: 5.855628
 67067/100000: episode: 1152, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 177.521, mean reward: 1.775 [1.441, 2.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.426, 10.190], loss: 2.058469, mae: 0.677714, mean_q: 5.775809
 67167/100000: episode: 1153, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 182.872, mean reward: 1.829 [1.456, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.441, 10.213], loss: 2.392146, mae: 0.808180, mean_q: 5.818569
 67267/100000: episode: 1154, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 203.066, mean reward: 2.031 [1.464, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.531, 10.098], loss: 3.560528, mae: 0.849884, mean_q: 5.802770
 67367/100000: episode: 1155, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 195.080, mean reward: 1.951 [1.519, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.086, 10.098], loss: 2.094533, mae: 0.694480, mean_q: 5.839182
 67467/100000: episode: 1156, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.143, mean reward: 1.861 [1.449, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.125], loss: 0.910229, mae: 0.682997, mean_q: 5.820192
 67567/100000: episode: 1157, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 193.029, mean reward: 1.930 [1.483, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.150], loss: 1.564077, mae: 0.726382, mean_q: 5.768027
 67667/100000: episode: 1158, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.854, mean reward: 1.999 [1.505, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.416, 10.311], loss: 2.496479, mae: 0.776398, mean_q: 5.790453
 67767/100000: episode: 1159, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 228.742, mean reward: 2.287 [1.456, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.098], loss: 5.098146, mae: 0.880228, mean_q: 5.887265
 67867/100000: episode: 1160, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 202.904, mean reward: 2.029 [1.450, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.874, 10.098], loss: 2.128337, mae: 0.741163, mean_q: 5.762580
 67967/100000: episode: 1161, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 202.743, mean reward: 2.027 [1.489, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.998, 10.098], loss: 0.738914, mae: 0.652187, mean_q: 5.779800
 68067/100000: episode: 1162, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 187.749, mean reward: 1.877 [1.497, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.681, 10.201], loss: 2.496993, mae: 0.753404, mean_q: 5.734874
 68167/100000: episode: 1163, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 179.435, mean reward: 1.794 [1.439, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.476, 10.214], loss: 0.606989, mae: 0.626634, mean_q: 5.713468
 68267/100000: episode: 1164, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 185.722, mean reward: 1.857 [1.459, 2.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.776, 10.098], loss: 2.061291, mae: 0.703228, mean_q: 5.707598
 68367/100000: episode: 1165, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 185.649, mean reward: 1.856 [1.446, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.925, 10.361], loss: 0.981929, mae: 0.646673, mean_q: 5.634057
 68467/100000: episode: 1166, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.332, mean reward: 1.863 [1.456, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.511, 10.130], loss: 0.713782, mae: 0.615774, mean_q: 5.690765
 68567/100000: episode: 1167, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.507, mean reward: 1.905 [1.470, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.329, 10.238], loss: 0.730528, mae: 0.617990, mean_q: 5.612796
 68667/100000: episode: 1168, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 225.496, mean reward: 2.255 [1.512, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.712, 10.098], loss: 2.149463, mae: 0.705803, mean_q: 5.581683
 68767/100000: episode: 1169, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.858, mean reward: 1.939 [1.437, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.099, 10.411], loss: 0.925533, mae: 0.626838, mean_q: 5.510766
 68867/100000: episode: 1170, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 185.606, mean reward: 1.856 [1.460, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.428, 10.255], loss: 2.133776, mae: 0.656258, mean_q: 5.476393
 68967/100000: episode: 1171, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 180.917, mean reward: 1.809 [1.458, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.196, 10.237], loss: 0.807149, mae: 0.612880, mean_q: 5.452250
 69067/100000: episode: 1172, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 182.991, mean reward: 1.830 [1.473, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.502, 10.098], loss: 2.065469, mae: 0.670283, mean_q: 5.359629
 69167/100000: episode: 1173, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 183.024, mean reward: 1.830 [1.448, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.611, 10.140], loss: 0.546043, mae: 0.585914, mean_q: 5.345834
 69267/100000: episode: 1174, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.056, mean reward: 1.981 [1.488, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.831, 10.135], loss: 3.568329, mae: 0.735264, mean_q: 5.427597
 69367/100000: episode: 1175, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 182.377, mean reward: 1.824 [1.482, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.008, 10.202], loss: 2.572830, mae: 0.697890, mean_q: 5.304560
 69467/100000: episode: 1176, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 180.007, mean reward: 1.800 [1.474, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.987, 10.098], loss: 0.512817, mae: 0.513927, mean_q: 5.111274
 69567/100000: episode: 1177, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 199.254, mean reward: 1.993 [1.485, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.069, 10.348], loss: 2.049298, mae: 0.636157, mean_q: 5.166740
 69667/100000: episode: 1178, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.950, mean reward: 1.849 [1.456, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.148], loss: 2.005561, mae: 0.611450, mean_q: 5.093872
 69767/100000: episode: 1179, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 199.230, mean reward: 1.992 [1.480, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.798, 10.098], loss: 3.571648, mae: 0.734437, mean_q: 5.188399
 69867/100000: episode: 1180, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.468, mean reward: 1.915 [1.471, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.552, 10.098], loss: 2.023138, mae: 0.602168, mean_q: 5.130341
 69967/100000: episode: 1181, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 188.176, mean reward: 1.882 [1.453, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.476, 10.303], loss: 1.677169, mae: 0.558459, mean_q: 4.983034
 70067/100000: episode: 1182, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 196.198, mean reward: 1.962 [1.495, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.504, 10.098], loss: 3.164415, mae: 0.599935, mean_q: 4.977159
 70167/100000: episode: 1183, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.432, mean reward: 1.844 [1.459, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.173], loss: 0.682772, mae: 0.531499, mean_q: 4.939538
 70267/100000: episode: 1184, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 178.572, mean reward: 1.786 [1.439, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.752, 10.098], loss: 0.385164, mae: 0.475402, mean_q: 4.819053
 70367/100000: episode: 1185, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 180.368, mean reward: 1.804 [1.464, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.223], loss: 1.899437, mae: 0.554542, mean_q: 4.804419
 70467/100000: episode: 1186, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 182.877, mean reward: 1.829 [1.456, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.263], loss: 0.350220, mae: 0.465707, mean_q: 4.709846
 70567/100000: episode: 1187, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 194.569, mean reward: 1.946 [1.462, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.599, 10.265], loss: 0.557334, mae: 0.489256, mean_q: 4.698438
 70667/100000: episode: 1188, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 204.537, mean reward: 2.045 [1.463, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.868, 10.354], loss: 2.854327, mae: 0.568036, mean_q: 4.674949
 70767/100000: episode: 1189, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.457, mean reward: 1.945 [1.439, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.418, 10.111], loss: 2.041656, mae: 0.538170, mean_q: 4.560400
 70867/100000: episode: 1190, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 192.552, mean reward: 1.926 [1.465, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.227, 10.199], loss: 1.468474, mae: 0.424911, mean_q: 4.440962
 70967/100000: episode: 1191, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 213.143, mean reward: 2.131 [1.441, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.900, 10.244], loss: 1.679581, mae: 0.491770, mean_q: 4.455848
 71067/100000: episode: 1192, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.642, mean reward: 1.876 [1.469, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.655, 10.098], loss: 0.232294, mae: 0.398372, mean_q: 4.333804
 71167/100000: episode: 1193, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 178.194, mean reward: 1.782 [1.500, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.456, 10.180], loss: 0.464140, mae: 0.369004, mean_q: 4.207057
 71267/100000: episode: 1194, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 190.036, mean reward: 1.900 [1.453, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.098], loss: 0.383061, mae: 0.384293, mean_q: 4.179230
 71367/100000: episode: 1195, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 218.179, mean reward: 2.182 [1.482, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.151, 10.404], loss: 1.440090, mae: 0.399482, mean_q: 4.120203
 71467/100000: episode: 1196, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 194.608, mean reward: 1.946 [1.458, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.835, 10.327], loss: 0.105431, mae: 0.302714, mean_q: 4.010720
 71567/100000: episode: 1197, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 184.010, mean reward: 1.840 [1.447, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.701, 10.166], loss: 1.385163, mae: 0.357384, mean_q: 3.987257
 71667/100000: episode: 1198, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 184.151, mean reward: 1.842 [1.451, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.979, 10.098], loss: 0.101579, mae: 0.296563, mean_q: 3.896749
 71767/100000: episode: 1199, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 177.790, mean reward: 1.778 [1.450, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.430, 10.125], loss: 0.079542, mae: 0.282878, mean_q: 3.842008
 71867/100000: episode: 1200, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 182.981, mean reward: 1.830 [1.443, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.138], loss: 1.334106, mae: 0.326668, mean_q: 3.805731
 71967/100000: episode: 1201, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.552, mean reward: 1.906 [1.449, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.013, 10.098], loss: 0.071752, mae: 0.273299, mean_q: 3.784958
 72067/100000: episode: 1202, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.258, mean reward: 1.953 [1.490, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.397, 10.311], loss: 0.072299, mae: 0.263977, mean_q: 3.794147
 72167/100000: episode: 1203, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 222.824, mean reward: 2.228 [1.552, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.327, 10.098], loss: 0.069360, mae: 0.273620, mean_q: 3.801193
 72267/100000: episode: 1204, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 191.975, mean reward: 1.920 [1.462, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.862, 10.098], loss: 0.074517, mae: 0.276137, mean_q: 3.806765
 72367/100000: episode: 1205, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 186.295, mean reward: 1.863 [1.485, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.167], loss: 0.076193, mae: 0.275080, mean_q: 3.795987
 72467/100000: episode: 1206, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.394, mean reward: 2.004 [1.543, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.982, 10.098], loss: 0.078104, mae: 0.274381, mean_q: 3.821251
 72567/100000: episode: 1207, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 187.208, mean reward: 1.872 [1.500, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.284, 10.098], loss: 0.065656, mae: 0.263982, mean_q: 3.794550
 72667/100000: episode: 1208, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 182.804, mean reward: 1.828 [1.454, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.968, 10.135], loss: 0.075122, mae: 0.272165, mean_q: 3.800848
 72767/100000: episode: 1209, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 198.211, mean reward: 1.982 [1.462, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.495, 10.162], loss: 0.064062, mae: 0.263640, mean_q: 3.787251
 72867/100000: episode: 1210, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 196.594, mean reward: 1.966 [1.456, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.459, 10.259], loss: 0.069255, mae: 0.265320, mean_q: 3.784105
 72967/100000: episode: 1211, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 188.687, mean reward: 1.887 [1.491, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.379, 10.123], loss: 0.067894, mae: 0.264480, mean_q: 3.765717
 73067/100000: episode: 1212, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.606, mean reward: 1.836 [1.471, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.359, 10.285], loss: 0.076411, mae: 0.276902, mean_q: 3.797330
 73167/100000: episode: 1213, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.812, mean reward: 1.838 [1.448, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.558, 10.113], loss: 0.072479, mae: 0.270519, mean_q: 3.788132
 73267/100000: episode: 1214, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 172.347, mean reward: 1.723 [1.451, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.755, 10.138], loss: 0.069289, mae: 0.268876, mean_q: 3.773249
 73367/100000: episode: 1215, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.858, mean reward: 1.939 [1.489, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.168, 10.098], loss: 0.065454, mae: 0.265143, mean_q: 3.774950
 73467/100000: episode: 1216, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.127, mean reward: 1.951 [1.450, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.654, 10.255], loss: 0.065148, mae: 0.261494, mean_q: 3.777808
 73567/100000: episode: 1217, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 207.748, mean reward: 2.077 [1.442, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.424], loss: 0.074334, mae: 0.271300, mean_q: 3.786850
 73667/100000: episode: 1218, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.498, mean reward: 1.995 [1.462, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.148], loss: 0.067856, mae: 0.261586, mean_q: 3.772557
 73767/100000: episode: 1219, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.351, mean reward: 1.884 [1.459, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.189], loss: 0.069282, mae: 0.264566, mean_q: 3.766363
 73867/100000: episode: 1220, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 193.229, mean reward: 1.932 [1.441, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.684, 10.140], loss: 0.074933, mae: 0.266803, mean_q: 3.760062
 73967/100000: episode: 1221, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 192.318, mean reward: 1.923 [1.472, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.895, 10.098], loss: 0.077846, mae: 0.277954, mean_q: 3.787062
 74067/100000: episode: 1222, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 188.177, mean reward: 1.882 [1.443, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.172, 10.098], loss: 0.071226, mae: 0.269489, mean_q: 3.774334
 74167/100000: episode: 1223, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 198.388, mean reward: 1.984 [1.457, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.301], loss: 0.069676, mae: 0.263071, mean_q: 3.773679
 74267/100000: episode: 1224, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 202.841, mean reward: 2.028 [1.495, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.454, 10.397], loss: 0.065632, mae: 0.260535, mean_q: 3.778134
 74367/100000: episode: 1225, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 188.069, mean reward: 1.881 [1.441, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.270, 10.149], loss: 0.071148, mae: 0.270955, mean_q: 3.796948
 74467/100000: episode: 1226, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 182.319, mean reward: 1.823 [1.434, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.911, 10.098], loss: 0.066841, mae: 0.263960, mean_q: 3.793302
 74567/100000: episode: 1227, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 198.818, mean reward: 1.988 [1.463, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.957, 10.257], loss: 0.070165, mae: 0.271526, mean_q: 3.793390
 74667/100000: episode: 1228, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 195.055, mean reward: 1.951 [1.450, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.098], loss: 0.072920, mae: 0.270784, mean_q: 3.784513
 74767/100000: episode: 1229, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.315, mean reward: 1.913 [1.452, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.142], loss: 0.071853, mae: 0.276831, mean_q: 3.793267
 74867/100000: episode: 1230, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 196.136, mean reward: 1.961 [1.474, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.573, 10.175], loss: 0.068362, mae: 0.269205, mean_q: 3.782268
 74967/100000: episode: 1231, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 200.640, mean reward: 2.006 [1.510, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.098], loss: 0.074649, mae: 0.279175, mean_q: 3.798988
 75067/100000: episode: 1232, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.115, mean reward: 1.911 [1.450, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.801, 10.098], loss: 0.075853, mae: 0.282035, mean_q: 3.812923
 75167/100000: episode: 1233, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 193.635, mean reward: 1.936 [1.441, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.098], loss: 0.077706, mae: 0.277599, mean_q: 3.794220
 75267/100000: episode: 1234, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 206.135, mean reward: 2.061 [1.490, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.850, 10.265], loss: 0.078698, mae: 0.279442, mean_q: 3.793528
 75367/100000: episode: 1235, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 214.267, mean reward: 2.143 [1.462, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.669, 10.098], loss: 0.081259, mae: 0.285053, mean_q: 3.810009
 75467/100000: episode: 1236, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 181.077, mean reward: 1.811 [1.445, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.256], loss: 0.075318, mae: 0.277912, mean_q: 3.811032
 75567/100000: episode: 1237, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 194.714, mean reward: 1.947 [1.504, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.705, 10.098], loss: 0.073971, mae: 0.278310, mean_q: 3.837696
 75667/100000: episode: 1238, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 184.673, mean reward: 1.847 [1.457, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.506, 10.166], loss: 0.082616, mae: 0.291198, mean_q: 3.853415
 75767/100000: episode: 1239, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 180.005, mean reward: 1.800 [1.453, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.567, 10.157], loss: 0.074092, mae: 0.275330, mean_q: 3.803961
 75867/100000: episode: 1240, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.688, mean reward: 1.887 [1.434, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.263, 10.348], loss: 0.071652, mae: 0.277755, mean_q: 3.826746
 75967/100000: episode: 1241, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.299, mean reward: 1.913 [1.491, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.822, 10.135], loss: 0.075294, mae: 0.274615, mean_q: 3.806551
 76067/100000: episode: 1242, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.367, mean reward: 1.994 [1.476, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.132, 10.221], loss: 0.070728, mae: 0.273313, mean_q: 3.798984
 76167/100000: episode: 1243, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.219, mean reward: 1.892 [1.437, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.599, 10.114], loss: 0.076662, mae: 0.272544, mean_q: 3.793869
 76267/100000: episode: 1244, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 180.841, mean reward: 1.808 [1.447, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.298, 10.098], loss: 0.082394, mae: 0.285919, mean_q: 3.805587
 76367/100000: episode: 1245, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 203.234, mean reward: 2.032 [1.484, 7.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.169, 10.098], loss: 0.075427, mae: 0.279292, mean_q: 3.808751
 76467/100000: episode: 1246, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.620, mean reward: 1.926 [1.438, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.538, 10.098], loss: 0.080434, mae: 0.281241, mean_q: 3.809495
 76567/100000: episode: 1247, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 186.780, mean reward: 1.868 [1.495, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.958, 10.098], loss: 0.073858, mae: 0.274845, mean_q: 3.799787
 76667/100000: episode: 1248, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 212.952, mean reward: 2.130 [1.491, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.198, 10.098], loss: 0.083021, mae: 0.289713, mean_q: 3.818040
 76767/100000: episode: 1249, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.767, mean reward: 1.858 [1.453, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.815, 10.098], loss: 0.086069, mae: 0.290797, mean_q: 3.829752
[Info] 1-TH LEVEL FOUND: 5.022235870361328, Considering 10/90 traces
 76867/100000: episode: 1250, duration: 4.570s, episode steps: 100, steps per second: 22, episode reward: 188.214, mean reward: 1.882 [1.435, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.584, 10.098], loss: 0.083348, mae: 0.285918, mean_q: 3.822457
 76875/100000: episode: 1251, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 22.606, mean reward: 2.826 [2.174, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.440, 10.434], loss: 0.069073, mae: 0.284672, mean_q: 3.850663
 76894/100000: episode: 1252, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 85.486, mean reward: 4.499 [2.819, 7.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.626, 10.100], loss: 0.073506, mae: 0.274419, mean_q: 3.801845
 76993/100000: episode: 1253, duration: 0.532s, episode steps: 99, steps per second: 186, episode reward: 209.427, mean reward: 2.115 [1.500, 4.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.758, 10.100], loss: 0.089475, mae: 0.287418, mean_q: 3.830904
 77008/100000: episode: 1254, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 40.044, mean reward: 2.670 [2.304, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.309, 10.100], loss: 0.067769, mae: 0.273944, mean_q: 3.834637
 77026/100000: episode: 1255, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 59.679, mean reward: 3.315 [2.141, 6.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.329, 10.100], loss: 0.071874, mae: 0.279113, mean_q: 3.849395
 77125/100000: episode: 1256, duration: 0.539s, episode steps: 99, steps per second: 184, episode reward: 214.890, mean reward: 2.171 [1.496, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.119, 10.406], loss: 0.089207, mae: 0.286722, mean_q: 3.860978
 77137/100000: episode: 1257, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 24.403, mean reward: 2.034 [1.914, 2.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.283, 10.100], loss: 0.076608, mae: 0.285628, mean_q: 3.793108
 77145/100000: episode: 1258, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 30.866, mean reward: 3.858 [2.657, 5.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.275, 10.609], loss: 0.068813, mae: 0.270079, mean_q: 3.832580
 77152/100000: episode: 1259, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 18.271, mean reward: 2.610 [1.828, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.403, 10.100], loss: 0.094673, mae: 0.300719, mean_q: 3.783571
 77163/100000: episode: 1260, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 25.433, mean reward: 2.312 [1.808, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.632, 10.100], loss: 0.100498, mae: 0.306811, mean_q: 3.855862
 77262/100000: episode: 1261, duration: 0.555s, episode steps: 99, steps per second: 179, episode reward: 231.288, mean reward: 2.336 [1.494, 18.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.500, 10.100], loss: 0.085761, mae: 0.293530, mean_q: 3.862135
 77270/100000: episode: 1262, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 20.670, mean reward: 2.584 [2.219, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.340], loss: 0.076927, mae: 0.283320, mean_q: 3.817508
 77281/100000: episode: 1263, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 22.068, mean reward: 2.006 [1.715, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.258, 10.100], loss: 0.101208, mae: 0.318315, mean_q: 3.950792
 77299/100000: episode: 1264, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 48.660, mean reward: 2.703 [2.317, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.153, 10.100], loss: 0.280550, mae: 0.339195, mean_q: 3.896186
 77306/100000: episode: 1265, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 13.971, mean reward: 1.996 [1.821, 2.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.291, 10.100], loss: 0.313110, mae: 0.442109, mean_q: 3.931095
 77405/100000: episode: 1266, duration: 0.556s, episode steps: 99, steps per second: 178, episode reward: 189.143, mean reward: 1.911 [1.447, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.666, 10.106], loss: 0.095966, mae: 0.301876, mean_q: 3.876390
 77416/100000: episode: 1267, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 26.566, mean reward: 2.415 [1.754, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.318, 10.100], loss: 0.085993, mae: 0.281086, mean_q: 3.852445
 77424/100000: episode: 1268, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 27.541, mean reward: 3.443 [2.550, 5.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.152, 10.599], loss: 0.086878, mae: 0.279599, mean_q: 3.846768
 77442/100000: episode: 1269, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 48.087, mean reward: 2.672 [2.063, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.795, 10.100], loss: 0.092871, mae: 0.301693, mean_q: 3.911374
 77454/100000: episode: 1270, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 34.050, mean reward: 2.837 [2.651, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.378, 10.100], loss: 0.069832, mae: 0.270947, mean_q: 3.884969
 77466/100000: episode: 1271, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 28.288, mean reward: 2.357 [1.935, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.988, 10.100], loss: 0.083706, mae: 0.298976, mean_q: 3.926354
 77477/100000: episode: 1272, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 22.272, mean reward: 2.025 [1.643, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.414, 10.100], loss: 0.110204, mae: 0.316852, mean_q: 3.934365
 77496/100000: episode: 1273, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 56.930, mean reward: 2.996 [2.436, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.074, 10.100], loss: 0.097892, mae: 0.284466, mean_q: 3.850245
 77515/100000: episode: 1274, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 70.106, mean reward: 3.690 [2.716, 5.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.250, 10.100], loss: 0.091180, mae: 0.314612, mean_q: 3.874741
 77522/100000: episode: 1275, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 15.536, mean reward: 2.219 [1.859, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.291, 10.100], loss: 0.113709, mae: 0.327848, mean_q: 3.952870
 77541/100000: episode: 1276, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 56.239, mean reward: 2.960 [2.375, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.775, 10.100], loss: 0.098199, mae: 0.310055, mean_q: 3.871530
 77640/100000: episode: 1277, duration: 0.538s, episode steps: 99, steps per second: 184, episode reward: 185.365, mean reward: 1.872 [1.464, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.370, 10.247], loss: 0.150428, mae: 0.330634, mean_q: 3.910864
 77659/100000: episode: 1278, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 64.218, mean reward: 3.380 [2.517, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.811, 10.100], loss: 0.161259, mae: 0.351252, mean_q: 3.979953
 77666/100000: episode: 1279, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 15.364, mean reward: 2.195 [1.937, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.510, 10.100], loss: 0.089674, mae: 0.298997, mean_q: 3.862912
 77676/100000: episode: 1280, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 19.388, mean reward: 1.939 [1.592, 2.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.231, 10.100], loss: 0.100270, mae: 0.330383, mean_q: 3.981321
 77687/100000: episode: 1281, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 21.528, mean reward: 1.957 [1.777, 2.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.222, 10.100], loss: 0.088595, mae: 0.311326, mean_q: 3.943638
 77699/100000: episode: 1282, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 31.696, mean reward: 2.641 [2.138, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.539, 10.100], loss: 0.081515, mae: 0.289966, mean_q: 3.885485
 77718/100000: episode: 1283, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 56.163, mean reward: 2.956 [1.736, 6.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.170, 10.100], loss: 0.111711, mae: 0.304189, mean_q: 3.922950
 77730/100000: episode: 1284, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 29.393, mean reward: 2.449 [1.961, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.321, 10.100], loss: 0.111567, mae: 0.326532, mean_q: 3.891083
 77742/100000: episode: 1285, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 27.865, mean reward: 2.322 [1.716, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.120, 10.100], loss: 0.079260, mae: 0.293014, mean_q: 3.991749
 77753/100000: episode: 1286, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 33.388, mean reward: 3.035 [2.000, 4.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.327, 10.100], loss: 0.083985, mae: 0.281031, mean_q: 3.906448
 77765/100000: episode: 1287, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 32.723, mean reward: 2.727 [2.319, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.493, 10.100], loss: 0.134637, mae: 0.322980, mean_q: 3.935576
 77780/100000: episode: 1288, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 40.954, mean reward: 2.730 [2.227, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.776, 10.100], loss: 0.097791, mae: 0.324795, mean_q: 3.990178
 77799/100000: episode: 1289, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 55.654, mean reward: 2.929 [2.220, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.440, 10.100], loss: 0.493729, mae: 0.393529, mean_q: 3.904538
 77898/100000: episode: 1290, duration: 0.514s, episode steps: 99, steps per second: 192, episode reward: 221.969, mean reward: 2.242 [1.487, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.063, 10.100], loss: 0.110358, mae: 0.326596, mean_q: 3.981532
 77905/100000: episode: 1291, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 15.237, mean reward: 2.177 [1.867, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.185, 10.100], loss: 0.124041, mae: 0.329078, mean_q: 4.007867
 77924/100000: episode: 1292, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 45.769, mean reward: 2.409 [1.594, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.391, 10.100], loss: 0.200685, mae: 0.412486, mean_q: 3.958878
 77932/100000: episode: 1293, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 25.559, mean reward: 3.195 [2.460, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.550], loss: 0.199997, mae: 0.440089, mean_q: 4.119241
 77951/100000: episode: 1294, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 49.873, mean reward: 2.625 [1.828, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.323, 10.100], loss: 0.160317, mae: 0.370443, mean_q: 3.907849
 77969/100000: episode: 1295, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 35.912, mean reward: 1.995 [1.499, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.192, 10.123], loss: 0.135104, mae: 0.340080, mean_q: 4.015708
 77981/100000: episode: 1296, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 26.634, mean reward: 2.219 [1.935, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.764, 10.100], loss: 0.100742, mae: 0.322067, mean_q: 3.934540
 77993/100000: episode: 1297, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 26.983, mean reward: 2.249 [1.995, 2.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.254, 10.100], loss: 0.109087, mae: 0.343556, mean_q: 4.027915
 78005/100000: episode: 1298, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 28.946, mean reward: 2.412 [1.924, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.174, 10.100], loss: 0.098469, mae: 0.324321, mean_q: 3.984795
 78017/100000: episode: 1299, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 37.005, mean reward: 3.084 [2.555, 3.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.410, 10.100], loss: 0.110907, mae: 0.315510, mean_q: 3.977763
 78027/100000: episode: 1300, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 24.118, mean reward: 2.412 [1.921, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.294, 10.100], loss: 0.392156, mae: 0.364295, mean_q: 4.001635
 78046/100000: episode: 1301, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 61.299, mean reward: 3.226 [1.770, 4.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.145, 10.100], loss: 0.305008, mae: 0.383708, mean_q: 4.017030
 78058/100000: episode: 1302, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 43.905, mean reward: 3.659 [2.596, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.262, 10.100], loss: 0.092700, mae: 0.303832, mean_q: 3.922840
 78076/100000: episode: 1303, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 77.834, mean reward: 4.324 [2.205, 7.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.670, 10.100], loss: 0.119091, mae: 0.330213, mean_q: 4.062820
 78083/100000: episode: 1304, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 19.658, mean reward: 2.808 [2.302, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.256, 10.100], loss: 0.124140, mae: 0.334559, mean_q: 4.126242
 78182/100000: episode: 1305, duration: 0.540s, episode steps: 99, steps per second: 183, episode reward: 197.914, mean reward: 1.999 [1.518, 6.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-1.251, 10.100], loss: 0.147404, mae: 0.341779, mean_q: 4.000139
 78192/100000: episode: 1306, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 28.230, mean reward: 2.823 [2.548, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.655, 10.100], loss: 0.130817, mae: 0.356090, mean_q: 4.142357
 78200/100000: episode: 1307, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 22.398, mean reward: 2.800 [2.549, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.129, 10.485], loss: 0.131167, mae: 0.339528, mean_q: 4.018496
 78211/100000: episode: 1308, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 25.928, mean reward: 2.357 [1.699, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.581, 10.100], loss: 0.087427, mae: 0.302657, mean_q: 4.046161
 78219/100000: episode: 1309, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 26.079, mean reward: 3.260 [2.548, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.138, 10.556], loss: 0.123350, mae: 0.347736, mean_q: 4.010000
 78226/100000: episode: 1310, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 21.323, mean reward: 3.046 [1.794, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.380, 10.100], loss: 0.142287, mae: 0.356602, mean_q: 4.133311
 78234/100000: episode: 1311, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 21.802, mean reward: 2.725 [2.229, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.524, 10.530], loss: 0.107753, mae: 0.325775, mean_q: 4.052237
 78246/100000: episode: 1312, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 31.068, mean reward: 2.589 [2.273, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.308, 10.100], loss: 0.111404, mae: 0.322200, mean_q: 4.095220
 78254/100000: episode: 1313, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 29.841, mean reward: 3.730 [2.722, 5.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.550], loss: 0.521518, mae: 0.439439, mean_q: 4.142793
 78269/100000: episode: 1314, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 39.847, mean reward: 2.656 [2.099, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.114, 10.100], loss: 0.103761, mae: 0.340993, mean_q: 3.989382
 78279/100000: episode: 1315, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 22.749, mean reward: 2.275 [2.071, 2.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.331, 10.100], loss: 0.123303, mae: 0.352868, mean_q: 4.054865
 78287/100000: episode: 1316, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 24.325, mean reward: 3.041 [2.399, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.523], loss: 0.114980, mae: 0.336377, mean_q: 4.023007
 78386/100000: episode: 1317, duration: 0.540s, episode steps: 99, steps per second: 183, episode reward: 187.551, mean reward: 1.894 [1.476, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.086, 10.251], loss: 0.162238, mae: 0.353740, mean_q: 4.060250
 78405/100000: episode: 1318, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 60.421, mean reward: 3.180 [2.347, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.204, 10.100], loss: 0.146632, mae: 0.365771, mean_q: 4.107152
 78417/100000: episode: 1319, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 26.064, mean reward: 2.172 [1.642, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.189, 10.100], loss: 0.125160, mae: 0.338834, mean_q: 4.065014
 78429/100000: episode: 1320, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 32.716, mean reward: 2.726 [2.312, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.338, 10.100], loss: 0.116645, mae: 0.342923, mean_q: 4.107363
 78444/100000: episode: 1321, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 41.312, mean reward: 2.754 [2.351, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.219, 10.100], loss: 0.107697, mae: 0.332496, mean_q: 4.101152
 78456/100000: episode: 1322, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 43.504, mean reward: 3.625 [2.772, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.241, 10.100], loss: 0.134312, mae: 0.349718, mean_q: 4.129005
 78464/100000: episode: 1323, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 24.158, mean reward: 3.020 [2.431, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.397], loss: 0.146722, mae: 0.370999, mean_q: 4.078406
 78471/100000: episode: 1324, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 14.496, mean reward: 2.071 [1.774, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.256, 10.100], loss: 0.114032, mae: 0.368107, mean_q: 4.171416
 78479/100000: episode: 1325, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 22.975, mean reward: 2.872 [2.453, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.696, 10.473], loss: 0.097477, mae: 0.295748, mean_q: 3.969789
 78498/100000: episode: 1326, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 60.180, mean reward: 3.167 [2.617, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.264, 10.100], loss: 0.106205, mae: 0.332740, mean_q: 4.107054
 78513/100000: episode: 1327, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 43.295, mean reward: 2.886 [2.391, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.342, 10.100], loss: 0.123523, mae: 0.327915, mean_q: 4.188679
 78532/100000: episode: 1328, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 69.644, mean reward: 3.665 [2.761, 5.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.357, 10.100], loss: 0.251129, mae: 0.327792, mean_q: 4.098562
 78547/100000: episode: 1329, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 37.151, mean reward: 2.477 [1.802, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.429, 10.100], loss: 0.154960, mae: 0.369545, mean_q: 4.102242
 78559/100000: episode: 1330, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 32.309, mean reward: 2.692 [2.334, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.547, 10.100], loss: 0.178749, mae: 0.350610, mean_q: 4.093958
 78577/100000: episode: 1331, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 47.380, mean reward: 2.632 [2.224, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.393, 10.100], loss: 0.108006, mae: 0.316656, mean_q: 4.119005
 78595/100000: episode: 1332, duration: 0.124s, episode steps: 18, steps per second: 146, episode reward: 58.937, mean reward: 3.274 [2.252, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.722, 10.100], loss: 0.129728, mae: 0.352181, mean_q: 4.175528
 78602/100000: episode: 1333, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 13.946, mean reward: 1.992 [1.854, 2.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.103, 10.100], loss: 0.124569, mae: 0.331182, mean_q: 4.039000
 78610/100000: episode: 1334, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 20.563, mean reward: 2.570 [2.068, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.351], loss: 0.480043, mae: 0.414611, mean_q: 4.154223
 78628/100000: episode: 1335, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 40.507, mean reward: 2.250 [1.899, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.416, 10.100], loss: 0.158778, mae: 0.332515, mean_q: 4.086415
 78639/100000: episode: 1336, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 23.975, mean reward: 2.180 [1.711, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.256, 10.100], loss: 0.152439, mae: 0.365943, mean_q: 4.174546
 78738/100000: episode: 1337, duration: 0.550s, episode steps: 99, steps per second: 180, episode reward: 187.458, mean reward: 1.894 [1.474, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.511, 10.100], loss: 0.131457, mae: 0.348817, mean_q: 4.153518
 78749/100000: episode: 1338, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 26.167, mean reward: 2.379 [1.837, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.343, 10.100], loss: 0.103699, mae: 0.322783, mean_q: 4.200948
 78756/100000: episode: 1339, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 13.693, mean reward: 1.956 [1.591, 2.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.248, 10.100], loss: 0.138284, mae: 0.335753, mean_q: 4.005188
[Info] 2-TH LEVEL FOUND: 6.943150997161865, Considering 10/90 traces
 78763/100000: episode: 1340, duration: 4.104s, episode steps: 7, steps per second: 2, episode reward: 13.760, mean reward: 1.966 [1.773, 2.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.262, 10.100], loss: 0.123608, mae: 0.357878, mean_q: 4.200070
 78770/100000: episode: 1341, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 20.218, mean reward: 2.888 [2.594, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.241, 10.100], loss: 0.096322, mae: 0.317978, mean_q: 4.105492
 78778/100000: episode: 1342, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 27.932, mean reward: 3.492 [2.509, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.578, 10.100], loss: 0.103586, mae: 0.347947, mean_q: 4.046211
 78783/100000: episode: 1343, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 21.146, mean reward: 4.229 [3.487, 5.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.481, 10.100], loss: 0.078475, mae: 0.289278, mean_q: 4.025302
 78794/100000: episode: 1344, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 40.751, mean reward: 3.705 [2.368, 5.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.381, 10.100], loss: 0.163236, mae: 0.397337, mean_q: 4.188240
 78802/100000: episode: 1345, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 30.399, mean reward: 3.800 [3.009, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.387, 10.100], loss: 0.152828, mae: 0.363175, mean_q: 4.170536
 78807/100000: episode: 1346, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 19.115, mean reward: 3.823 [2.715, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.104, 10.486], loss: 0.142568, mae: 0.367262, mean_q: 4.225309
 78820/100000: episode: 1347, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 90.510, mean reward: 6.962 [3.994, 18.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.649, 10.100], loss: 0.147051, mae: 0.333893, mean_q: 4.147462
 78831/100000: episode: 1348, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 45.855, mean reward: 4.169 [2.561, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.948, 10.100], loss: 0.169092, mae: 0.379701, mean_q: 4.193737
 78844/100000: episode: 1349, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 41.290, mean reward: 3.176 [2.351, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.473, 10.100], loss: 0.086481, mae: 0.309846, mean_q: 4.163594
 78857/100000: episode: 1350, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 61.258, mean reward: 4.712 [3.952, 6.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.007, 10.100], loss: 0.156674, mae: 0.375787, mean_q: 4.176577
 78864/100000: episode: 1351, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 20.003, mean reward: 2.858 [2.442, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.127, 10.100], loss: 0.144121, mae: 0.367993, mean_q: 4.248389
 78877/100000: episode: 1352, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 61.690, mean reward: 4.745 [3.535, 6.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.420, 10.100], loss: 0.146347, mae: 0.367810, mean_q: 4.228632
 78889/100000: episode: 1353, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 35.254, mean reward: 2.938 [2.521, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.148, 10.100], loss: 0.134258, mae: 0.348639, mean_q: 4.075085
 78897/100000: episode: 1354, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 25.601, mean reward: 3.200 [2.574, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.616, 10.100], loss: 0.136499, mae: 0.337387, mean_q: 4.264525
 78907/100000: episode: 1355, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 32.372, mean reward: 3.237 [2.768, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.363, 10.100], loss: 0.108887, mae: 0.341699, mean_q: 4.369351
 78914/100000: episode: 1356, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 25.078, mean reward: 3.583 [2.737, 5.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.780, 10.100], loss: 0.147533, mae: 0.375392, mean_q: 4.158310
 78919/100000: episode: 1357, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 25.087, mean reward: 5.017 [4.021, 6.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.677, 10.100], loss: 0.107604, mae: 0.344982, mean_q: 4.140529
 78924/100000: episode: 1358, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 15.639, mean reward: 3.128 [2.629, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.464], loss: 0.101967, mae: 0.322985, mean_q: 4.108099
 78937/100000: episode: 1359, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 59.418, mean reward: 4.571 [3.132, 9.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.266, 10.100], loss: 0.360479, mae: 0.422425, mean_q: 4.271289
 78947/100000: episode: 1360, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 32.775, mean reward: 3.277 [2.606, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.389, 10.100], loss: 0.275776, mae: 0.440636, mean_q: 4.121026
 78958/100000: episode: 1361, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 39.230, mean reward: 3.566 [2.390, 6.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.378, 10.100], loss: 0.257523, mae: 0.478665, mean_q: 4.328467
 78969/100000: episode: 1362, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 49.262, mean reward: 4.478 [2.934, 11.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.498, 10.100], loss: 0.139228, mae: 0.396001, mean_q: 4.280910
 78974/100000: episode: 1363, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 17.746, mean reward: 3.549 [3.422, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.355, 10.100], loss: 0.109715, mae: 0.337013, mean_q: 4.250196
 78986/100000: episode: 1364, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 54.597, mean reward: 4.550 [3.546, 6.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.348, 10.100], loss: 0.241478, mae: 0.370443, mean_q: 4.217314
 78997/100000: episode: 1365, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 46.841, mean reward: 4.258 [2.958, 6.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.802, 10.100], loss: 0.157170, mae: 0.384456, mean_q: 4.343060
 79010/100000: episode: 1366, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 59.396, mean reward: 4.569 [3.509, 7.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.412, 10.100], loss: 0.153686, mae: 0.358736, mean_q: 4.232009
 79015/100000: episode: 1367, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 28.048, mean reward: 5.610 [3.088, 11.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.457, 10.100], loss: 0.156985, mae: 0.375591, mean_q: 4.311832
 79020/100000: episode: 1368, duration: 0.038s, episode steps: 5, steps per second: 133, episode reward: 18.101, mean reward: 3.620 [3.199, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.477, 10.100], loss: 0.288142, mae: 0.416996, mean_q: 4.301051
 79031/100000: episode: 1369, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 44.009, mean reward: 4.001 [3.035, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.444, 10.100], loss: 0.423645, mae: 0.471277, mean_q: 4.399074
 79038/100000: episode: 1370, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 20.568, mean reward: 2.938 [2.666, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.370, 10.100], loss: 0.166124, mae: 0.411805, mean_q: 4.191536
 79048/100000: episode: 1371, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 34.137, mean reward: 3.414 [2.496, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.448, 10.100], loss: 0.174282, mae: 0.393323, mean_q: 4.396084
 79053/100000: episode: 1372, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 15.356, mean reward: 3.071 [2.704, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.463], loss: 0.156866, mae: 0.391509, mean_q: 4.441161
 79061/100000: episode: 1373, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 23.994, mean reward: 2.999 [2.795, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.337, 10.100], loss: 0.161558, mae: 0.375218, mean_q: 4.339995
 79066/100000: episode: 1374, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 13.444, mean reward: 2.689 [2.377, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.437, 10.100], loss: 0.077750, mae: 0.293049, mean_q: 4.131232
 79078/100000: episode: 1375, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 43.961, mean reward: 3.663 [3.181, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.286, 10.100], loss: 0.208585, mae: 0.413921, mean_q: 4.342143
 79083/100000: episode: 1376, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 19.403, mean reward: 3.881 [2.840, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.498, 10.100], loss: 0.164913, mae: 0.396992, mean_q: 4.343552
 79093/100000: episode: 1377, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 38.260, mean reward: 3.826 [2.768, 5.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.425, 10.100], loss: 0.150189, mae: 0.356856, mean_q: 4.185274
 79100/100000: episode: 1378, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 19.654, mean reward: 2.808 [2.360, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.312, 10.100], loss: 0.197040, mae: 0.401573, mean_q: 4.417777
 79107/100000: episode: 1379, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 20.379, mean reward: 2.911 [2.586, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.270, 10.100], loss: 0.137350, mae: 0.369204, mean_q: 4.465253
 79118/100000: episode: 1380, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 34.267, mean reward: 3.115 [2.730, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.372, 10.100], loss: 0.151288, mae: 0.389991, mean_q: 4.398617
 79123/100000: episode: 1381, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 19.034, mean reward: 3.807 [3.331, 4.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.485, 10.100], loss: 0.108409, mae: 0.379754, mean_q: 4.274604
 79128/100000: episode: 1382, duration: 0.044s, episode steps: 5, steps per second: 113, episode reward: 24.985, mean reward: 4.997 [3.965, 6.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.428, 10.100], loss: 0.322380, mae: 0.471157, mean_q: 4.467262
 79139/100000: episode: 1383, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 43.403, mean reward: 3.946 [3.179, 5.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.342, 10.100], loss: 0.177798, mae: 0.386302, mean_q: 4.389675
 79150/100000: episode: 1384, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 72.048, mean reward: 6.550 [4.514, 10.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.477, 10.100], loss: 0.256844, mae: 0.416042, mean_q: 4.379215
 79162/100000: episode: 1385, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 47.375, mean reward: 3.948 [3.076, 5.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.304, 10.100], loss: 0.181013, mae: 0.407659, mean_q: 4.440944
 79167/100000: episode: 1386, duration: 0.038s, episode steps: 5, steps per second: 130, episode reward: 22.935, mean reward: 4.587 [3.955, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.547, 10.100], loss: 0.180988, mae: 0.414373, mean_q: 4.451155
 79174/100000: episode: 1387, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 20.815, mean reward: 2.974 [2.304, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.325, 10.100], loss: 0.197547, mae: 0.384435, mean_q: 4.098733
 79181/100000: episode: 1388, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 21.391, mean reward: 3.056 [2.668, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.257, 10.100], loss: 0.116109, mae: 0.359052, mean_q: 4.317506
 79191/100000: episode: 1389, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 43.530, mean reward: 4.353 [3.482, 5.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.389, 10.100], loss: 0.148856, mae: 0.375032, mean_q: 4.364088
 79203/100000: episode: 1390, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 41.736, mean reward: 3.478 [2.927, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.372, 10.100], loss: 0.200225, mae: 0.407705, mean_q: 4.390088
 79213/100000: episode: 1391, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 39.451, mean reward: 3.945 [3.308, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.202, 10.100], loss: 0.170706, mae: 0.395301, mean_q: 4.452776
 79224/100000: episode: 1392, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 35.789, mean reward: 3.254 [2.757, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.386, 10.100], loss: 0.205826, mae: 0.437610, mean_q: 4.482892
 79234/100000: episode: 1393, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 30.856, mean reward: 3.086 [2.495, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.457, 10.100], loss: 0.219933, mae: 0.419917, mean_q: 4.350150
 79245/100000: episode: 1394, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 42.278, mean reward: 3.843 [2.953, 7.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.530, 10.100], loss: 0.202537, mae: 0.433407, mean_q: 4.513945
 79252/100000: episode: 1395, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 21.719, mean reward: 3.103 [2.307, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.416, 10.100], loss: 0.248134, mae: 0.428040, mean_q: 4.560070
 79262/100000: episode: 1396, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 70.260, mean reward: 7.026 [2.693, 20.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.493, 10.100], loss: 0.157617, mae: 0.379014, mean_q: 4.453980
 79272/100000: episode: 1397, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 38.123, mean reward: 3.812 [3.240, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.667, 10.100], loss: 0.544383, mae: 0.489216, mean_q: 4.508405
 79277/100000: episode: 1398, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 15.819, mean reward: 3.164 [2.904, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-1.298, 10.420], loss: 0.184926, mae: 0.392097, mean_q: 4.097913
 79282/100000: episode: 1399, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 21.247, mean reward: 4.249 [3.283, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.479, 10.100], loss: 0.925390, mae: 0.608014, mean_q: 4.707142
 79289/100000: episode: 1400, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 22.763, mean reward: 3.252 [2.141, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.403, 10.100], loss: 0.213991, mae: 0.477867, mean_q: 4.624554
 79294/100000: episode: 1401, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 11.870, mean reward: 2.374 [2.295, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.354], loss: 0.549231, mae: 0.592532, mean_q: 4.561191
 79302/100000: episode: 1402, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 23.319, mean reward: 2.915 [2.701, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.309, 10.100], loss: 0.191351, mae: 0.402459, mean_q: 4.451235
 79310/100000: episode: 1403, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 26.858, mean reward: 3.357 [2.899, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.390, 10.100], loss: 0.160600, mae: 0.403549, mean_q: 4.492426
 79323/100000: episode: 1404, duration: 0.093s, episode steps: 13, steps per second: 140, episode reward: 50.295, mean reward: 3.869 [3.090, 4.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.724, 10.100], loss: 0.311155, mae: 0.475041, mean_q: 4.456169
 79330/100000: episode: 1405, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 18.909, mean reward: 2.701 [2.365, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.471, 10.100], loss: 0.306418, mae: 0.493708, mean_q: 4.713376
 79341/100000: episode: 1406, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 39.853, mean reward: 3.623 [2.951, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.380, 10.100], loss: 0.207345, mae: 0.387941, mean_q: 4.488799
 79346/100000: episode: 1407, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 13.839, mean reward: 2.768 [2.362, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.452], loss: 0.164915, mae: 0.397711, mean_q: 4.569806
 79353/100000: episode: 1408, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 22.455, mean reward: 3.208 [2.721, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.266, 10.100], loss: 0.147401, mae: 0.391311, mean_q: 4.498047
 79361/100000: episode: 1409, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 29.928, mean reward: 3.741 [2.938, 4.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.383, 10.100], loss: 0.175901, mae: 0.385436, mean_q: 4.463720
 79368/100000: episode: 1410, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 19.050, mean reward: 2.721 [2.148, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.461, 10.100], loss: 0.142347, mae: 0.402927, mean_q: 4.726089
 79373/100000: episode: 1411, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 16.406, mean reward: 3.281 [3.035, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.401, 10.100], loss: 0.153185, mae: 0.394662, mean_q: 4.530381
 79378/100000: episode: 1412, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 19.045, mean reward: 3.809 [2.986, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.386, 10.100], loss: 0.210303, mae: 0.419295, mean_q: 4.419057
 79385/100000: episode: 1413, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 24.770, mean reward: 3.539 [2.847, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.567, 10.100], loss: 0.150357, mae: 0.392908, mean_q: 4.475100
 79398/100000: episode: 1414, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 50.703, mean reward: 3.900 [3.052, 5.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.955, 10.100], loss: 0.283696, mae: 0.453559, mean_q: 4.578481
 79411/100000: episode: 1415, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 57.490, mean reward: 4.422 [3.707, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.455, 10.100], loss: 0.149887, mae: 0.391883, mean_q: 4.422101
 79419/100000: episode: 1416, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 23.453, mean reward: 2.932 [2.440, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.350, 10.100], loss: 0.910007, mae: 0.563938, mean_q: 4.770950
 79426/100000: episode: 1417, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 28.880, mean reward: 4.126 [3.040, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.763, 10.100], loss: 0.387836, mae: 0.508487, mean_q: 4.410738
 79434/100000: episode: 1418, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 22.729, mean reward: 2.841 [2.461, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.341, 10.100], loss: 0.266295, mae: 0.448120, mean_q: 4.508885
 79445/100000: episode: 1419, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 42.477, mean reward: 3.862 [2.825, 5.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.361, 10.100], loss: 0.466528, mae: 0.468538, mean_q: 4.537899
 79452/100000: episode: 1420, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 19.059, mean reward: 2.723 [2.505, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.409, 10.100], loss: 0.596869, mae: 0.463044, mean_q: 4.618515
 79460/100000: episode: 1421, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 25.262, mean reward: 3.158 [2.629, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.255, 10.100], loss: 0.192825, mae: 0.435216, mean_q: 4.535079
 79471/100000: episode: 1422, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 45.299, mean reward: 4.118 [3.615, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.380, 10.100], loss: 0.196307, mae: 0.412777, mean_q: 4.491821
 79478/100000: episode: 1423, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 20.961, mean reward: 2.994 [2.415, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.509, 10.100], loss: 0.172522, mae: 0.437058, mean_q: 4.749173
 79483/100000: episode: 1424, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 20.758, mean reward: 4.152 [3.450, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.551, 10.100], loss: 0.365943, mae: 0.484809, mean_q: 4.252348
 79496/100000: episode: 1425, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 69.607, mean reward: 5.354 [3.496, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.527, 10.100], loss: 0.253503, mae: 0.471680, mean_q: 4.631557
 79508/100000: episode: 1426, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 49.618, mean reward: 4.135 [3.044, 6.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.087, 10.100], loss: 0.428119, mae: 0.472868, mean_q: 4.612552
 79516/100000: episode: 1427, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 24.785, mean reward: 3.098 [2.294, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.375, 10.100], loss: 0.218883, mae: 0.384137, mean_q: 4.483478
 79523/100000: episode: 1428, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 15.967, mean reward: 2.281 [1.966, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.436, 10.100], loss: 0.238968, mae: 0.448959, mean_q: 4.575695
 79533/100000: episode: 1429, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 31.677, mean reward: 3.168 [2.737, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.415, 10.100], loss: 0.262994, mae: 0.437591, mean_q: 4.544948
[Info] 3-TH LEVEL FOUND: 7.934845447540283, Considering 10/90 traces
 79540/100000: episode: 1430, duration: 4.174s, episode steps: 7, steps per second: 2, episode reward: 19.684, mean reward: 2.812 [2.343, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.421, 10.100], loss: 0.257357, mae: 0.484530, mean_q: 4.648765
 79547/100000: episode: 1431, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 38.664, mean reward: 5.523 [3.671, 7.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.428, 10.100], loss: 0.365913, mae: 0.527077, mean_q: 4.605717
 79554/100000: episode: 1432, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 37.894, mean reward: 5.413 [4.217, 6.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.348, 10.100], loss: 0.260740, mae: 0.512485, mean_q: 4.836163
 79560/100000: episode: 1433, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 23.739, mean reward: 3.956 [2.784, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.520, 10.100], loss: 0.304277, mae: 0.482647, mean_q: 4.427460
 79568/100000: episode: 1434, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 40.314, mean reward: 5.039 [4.004, 6.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.482, 10.100], loss: 0.190295, mae: 0.441038, mean_q: 4.823496
 79575/100000: episode: 1435, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 29.595, mean reward: 4.228 [3.982, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.386, 10.100], loss: 0.254482, mae: 0.454740, mean_q: 4.637313
 79583/100000: episode: 1436, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 41.379, mean reward: 5.172 [4.515, 5.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.414, 10.100], loss: 0.274403, mae: 0.496260, mean_q: 4.734002
 79593/100000: episode: 1437, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 71.891, mean reward: 7.189 [5.352, 12.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.504, 10.100], loss: 0.200909, mae: 0.420964, mean_q: 4.605394
 79603/100000: episode: 1438, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 45.410, mean reward: 4.541 [3.255, 8.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.351, 10.100], loss: 0.344244, mae: 0.493314, mean_q: 4.550250
 79613/100000: episode: 1439, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 38.287, mean reward: 3.829 [2.449, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.268, 10.100], loss: 0.336218, mae: 0.529302, mean_q: 4.760035
 79620/100000: episode: 1440, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 31.746, mean reward: 4.535 [3.818, 5.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.502, 10.100], loss: 0.262805, mae: 0.432171, mean_q: 4.681878
 79628/100000: episode: 1441, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 30.816, mean reward: 3.852 [3.285, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.376, 10.100], loss: 0.201377, mae: 0.429908, mean_q: 4.740437
 79636/100000: episode: 1442, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 41.970, mean reward: 5.246 [4.550, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.398, 10.100], loss: 0.764982, mae: 0.571120, mean_q: 4.877749
 79645/100000: episode: 1443, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 61.190, mean reward: 6.799 [5.917, 7.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.539, 10.100], loss: 0.237944, mae: 0.446727, mean_q: 4.627268
 79654/100000: episode: 1444, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 53.428, mean reward: 5.936 [4.237, 8.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.513, 10.100], loss: 0.206356, mae: 0.438391, mean_q: 4.712684
 79661/100000: episode: 1445, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 26.378, mean reward: 3.768 [3.429, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.494, 10.100], loss: 0.685845, mae: 0.518115, mean_q: 4.586030
 79669/100000: episode: 1446, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 66.333, mean reward: 8.292 [4.408, 20.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.533, 10.100], loss: 0.259225, mae: 0.506483, mean_q: 4.951171
 79677/100000: episode: 1447, duration: 0.062s, episode steps: 8, steps per second: 130, episode reward: 41.864, mean reward: 5.233 [3.950, 8.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.432, 10.100], loss: 0.226674, mae: 0.407033, mean_q: 4.654866
 79684/100000: episode: 1448, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 43.064, mean reward: 6.152 [3.695, 9.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.417, 10.100], loss: 0.286445, mae: 0.449952, mean_q: 4.730471
 79691/100000: episode: 1449, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 31.420, mean reward: 4.489 [3.436, 5.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.486, 10.100], loss: 0.207704, mae: 0.436332, mean_q: 4.578434
 79699/100000: episode: 1450, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 54.482, mean reward: 6.810 [3.974, 20.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.940, 10.100], loss: 0.377435, mae: 0.513534, mean_q: 4.968179
 79706/100000: episode: 1451, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 25.405, mean reward: 3.629 [2.632, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.589, 10.100], loss: 0.311424, mae: 0.501831, mean_q: 4.879692
 79713/100000: episode: 1452, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 27.361, mean reward: 3.909 [2.895, 5.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.353, 10.100], loss: 0.204864, mae: 0.463287, mean_q: 4.693511
 79722/100000: episode: 1453, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 36.439, mean reward: 4.049 [2.992, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.535, 10.100], loss: 0.231373, mae: 0.462800, mean_q: 4.828256
 79730/100000: episode: 1454, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 45.874, mean reward: 5.734 [4.617, 7.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.541, 10.100], loss: 0.258631, mae: 0.469058, mean_q: 4.868829
 79740/100000: episode: 1455, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 70.613, mean reward: 7.061 [3.020, 19.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.360, 10.100], loss: 0.325087, mae: 0.476569, mean_q: 4.800751
 79750/100000: episode: 1456, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 67.009, mean reward: 6.701 [4.463, 15.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.480, 10.100], loss: 0.323245, mae: 0.476813, mean_q: 4.758551
 79758/100000: episode: 1457, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 26.655, mean reward: 3.332 [2.376, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.272, 10.100], loss: 0.613418, mae: 0.559093, mean_q: 4.900564
 79766/100000: episode: 1458, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 52.276, mean reward: 6.534 [4.446, 9.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.546, 10.100], loss: 0.263370, mae: 0.459262, mean_q: 4.637161
 79773/100000: episode: 1459, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 24.483, mean reward: 3.498 [3.151, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.608, 10.100], loss: 0.965096, mae: 0.616743, mean_q: 4.738524
 79779/100000: episode: 1460, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 29.974, mean reward: 4.996 [4.019, 5.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.562, 10.100], loss: 0.297198, mae: 0.498349, mean_q: 4.900270
 79787/100000: episode: 1461, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 56.689, mean reward: 7.086 [4.687, 17.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.399, 10.100], loss: 0.638510, mae: 0.507546, mean_q: 4.736824
 79795/100000: episode: 1462, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 57.398, mean reward: 7.175 [4.267, 9.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.450, 10.100], loss: 0.302055, mae: 0.523782, mean_q: 4.857358
 79805/100000: episode: 1463, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 51.028, mean reward: 5.103 [3.711, 6.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.303, 10.100], loss: 0.221642, mae: 0.500862, mean_q: 4.988904
 79812/100000: episode: 1464, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 31.559, mean reward: 4.508 [3.772, 5.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.954, 10.100], loss: 0.447274, mae: 0.544286, mean_q: 4.937396
 79822/100000: episode: 1465, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 29.366, mean reward: 2.937 [2.561, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.347, 10.100], loss: 0.413575, mae: 0.575553, mean_q: 5.066018
 79830/100000: episode: 1466, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 44.615, mean reward: 5.577 [4.118, 8.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.192, 10.100], loss: 0.454101, mae: 0.573893, mean_q: 5.154492
 79837/100000: episode: 1467, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 29.317, mean reward: 4.188 [3.557, 5.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.417, 10.100], loss: 0.421118, mae: 0.504595, mean_q: 4.535194
 79847/100000: episode: 1468, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 54.535, mean reward: 5.453 [4.395, 6.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.526, 10.100], loss: 0.589769, mae: 0.493011, mean_q: 4.894666
 79854/100000: episode: 1469, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 33.665, mean reward: 4.809 [3.947, 5.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.425, 10.100], loss: 1.324201, mae: 0.641141, mean_q: 5.064484
 79862/100000: episode: 1470, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 39.814, mean reward: 4.977 [4.175, 6.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.381, 10.100], loss: 1.806139, mae: 0.790728, mean_q: 4.924748
 79871/100000: episode: 1471, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 188.931, mean reward: 20.992 [6.300, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.981, 10.100], loss: 16.998375, mae: 1.101434, mean_q: 5.134405
 79880/100000: episode: 1472, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 53.592, mean reward: 5.955 [3.662, 10.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.390, 10.100], loss: 16.828186, mae: 1.549886, mean_q: 5.876575
 79887/100000: episode: 1473, duration: 0.053s, episode steps: 7, steps per second: 132, episode reward: 25.647, mean reward: 3.664 [2.802, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.556, 10.100], loss: 1.143361, mae: 0.849497, mean_q: 4.376995
 79895/100000: episode: 1474, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 78.970, mean reward: 9.871 [4.242, 34.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.448, 10.100], loss: 1.059363, mae: 0.746263, mean_q: 5.246502
 79902/100000: episode: 1475, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 30.371, mean reward: 4.339 [3.644, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.991, 10.100], loss: 0.803482, mae: 0.579924, mean_q: 5.013900
 79912/100000: episode: 1476, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 82.209, mean reward: 8.221 [3.632, 15.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.473, 10.100], loss: 1.076835, mae: 0.623720, mean_q: 5.007644
 79920/100000: episode: 1477, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 35.599, mean reward: 4.450 [3.727, 6.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.913, 10.100], loss: 0.647133, mae: 0.587688, mean_q: 5.119602
 79928/100000: episode: 1478, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 60.981, mean reward: 7.623 [5.253, 9.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.513, 10.100], loss: 0.731196, mae: 0.642283, mean_q: 5.095451
 79934/100000: episode: 1479, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 28.757, mean reward: 4.793 [3.625, 7.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.498, 10.100], loss: 0.626376, mae: 0.566235, mean_q: 5.179674
 79941/100000: episode: 1480, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 29.386, mean reward: 4.198 [2.984, 5.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.443, 10.100], loss: 0.497484, mae: 0.562847, mean_q: 4.860493
 79948/100000: episode: 1481, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 30.706, mean reward: 4.387 [3.924, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.462, 10.100], loss: 0.285990, mae: 0.519681, mean_q: 5.274662
 79956/100000: episode: 1482, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 42.583, mean reward: 5.323 [4.572, 5.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.507, 10.100], loss: 1.461936, mae: 0.649093, mean_q: 4.890053
 79966/100000: episode: 1483, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 52.853, mean reward: 5.285 [4.239, 8.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.502, 10.100], loss: 0.499846, mae: 0.585100, mean_q: 5.174273
 79973/100000: episode: 1484, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 27.664, mean reward: 3.952 [3.380, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.552, 10.100], loss: 0.397132, mae: 0.529325, mean_q: 5.065043
 79983/100000: episode: 1485, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 110.081, mean reward: 11.008 [4.567, 31.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.433, 10.100], loss: 0.341837, mae: 0.520197, mean_q: 5.239092
 79991/100000: episode: 1486, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 48.283, mean reward: 6.035 [4.665, 7.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.059, 10.100], loss: 1.107545, mae: 0.670482, mean_q: 5.341928
 79998/100000: episode: 1487, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 35.364, mean reward: 5.052 [4.062, 5.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.437, 10.100], loss: 0.786204, mae: 0.607637, mean_q: 5.235301
 80006/100000: episode: 1488, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 33.930, mean reward: 4.241 [3.326, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.377, 10.100], loss: 0.858529, mae: 0.599294, mean_q: 5.073600
 80014/100000: episode: 1489, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 28.961, mean reward: 3.620 [3.315, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.365, 10.100], loss: 0.280595, mae: 0.464781, mean_q: 4.883208
 80021/100000: episode: 1490, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 40.390, mean reward: 5.770 [3.255, 7.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.624, 10.100], loss: 20.455973, mae: 0.997197, mean_q: 5.397283
 80029/100000: episode: 1491, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 35.495, mean reward: 4.437 [3.396, 5.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.317, 10.100], loss: 17.950775, mae: 1.303431, mean_q: 6.041603
 80038/100000: episode: 1492, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 49.874, mean reward: 5.542 [4.107, 6.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.399, 10.100], loss: 1.381915, mae: 0.743754, mean_q: 4.971433
 80046/100000: episode: 1493, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 39.683, mean reward: 4.960 [3.908, 6.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.386, 10.100], loss: 1.378908, mae: 0.787434, mean_q: 5.386440
 80055/100000: episode: 1494, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 36.522, mean reward: 4.058 [2.727, 6.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.460, 10.100], loss: 15.771171, mae: 0.880459, mean_q: 5.236277
 80065/100000: episode: 1495, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 49.866, mean reward: 4.987 [3.198, 9.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.310, 10.100], loss: 0.643787, mae: 0.756177, mean_q: 5.579092
 80074/100000: episode: 1496, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 46.837, mean reward: 5.204 [3.747, 8.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.341, 10.100], loss: 1.059064, mae: 0.747969, mean_q: 5.053146
 80083/100000: episode: 1497, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 38.358, mean reward: 4.262 [3.216, 6.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.427, 10.100], loss: 0.959556, mae: 0.687573, mean_q: 5.417337
 80090/100000: episode: 1498, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 24.370, mean reward: 3.481 [2.870, 5.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.480, 10.100], loss: 0.708431, mae: 0.592780, mean_q: 5.345033
 80100/100000: episode: 1499, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 52.842, mean reward: 5.284 [3.486, 8.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.651, 10.100], loss: 0.683173, mae: 0.650753, mean_q: 5.209534
 80106/100000: episode: 1500, duration: 0.051s, episode steps: 6, steps per second: 118, episode reward: 26.361, mean reward: 4.393 [3.477, 5.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.503, 10.100], loss: 0.353778, mae: 0.572693, mean_q: 5.409100
 80115/100000: episode: 1501, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 69.860, mean reward: 7.762 [5.573, 9.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.602, 10.100], loss: 0.818928, mae: 0.669261, mean_q: 5.538883
 80125/100000: episode: 1502, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 40.032, mean reward: 4.003 [3.141, 5.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.316, 10.100], loss: 0.438269, mae: 0.594030, mean_q: 5.439364
 80134/100000: episode: 1503, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 57.816, mean reward: 6.424 [4.820, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.518, 10.100], loss: 0.690224, mae: 0.594933, mean_q: 5.361954
 80142/100000: episode: 1504, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 40.531, mean reward: 5.066 [3.585, 6.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.437, 10.100], loss: 0.795362, mae: 0.564475, mean_q: 5.341843
 80148/100000: episode: 1505, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 25.224, mean reward: 4.204 [3.508, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.497, 10.100], loss: 1.422049, mae: 0.839826, mean_q: 5.554906
 80156/100000: episode: 1506, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 32.123, mean reward: 4.015 [3.204, 5.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.377, 10.100], loss: 0.356496, mae: 0.547399, mean_q: 5.488814
 80164/100000: episode: 1507, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 39.065, mean reward: 4.883 [3.622, 9.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.619, 10.100], loss: 0.872858, mae: 0.592871, mean_q: 5.296036
 80172/100000: episode: 1508, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 76.929, mean reward: 9.616 [4.348, 33.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.413, 10.100], loss: 2.429378, mae: 0.717023, mean_q: 5.191222
 80180/100000: episode: 1509, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 71.933, mean reward: 8.992 [5.238, 13.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.585, 10.100], loss: 3.419446, mae: 0.840195, mean_q: 5.499218
 80189/100000: episode: 1510, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 47.838, mean reward: 5.315 [4.224, 7.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.371, 10.100], loss: 0.543354, mae: 0.680088, mean_q: 5.632075
 80195/100000: episode: 1511, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 33.514, mean reward: 5.586 [4.267, 9.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.519, 10.100], loss: 0.751115, mae: 0.676722, mean_q: 5.285698
 80204/100000: episode: 1512, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 53.251, mean reward: 5.917 [4.115, 7.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.336, 10.100], loss: 0.529891, mae: 0.568292, mean_q: 5.304285
 80212/100000: episode: 1513, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 37.170, mean reward: 4.646 [3.811, 6.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.436, 10.100], loss: 0.584775, mae: 0.660536, mean_q: 5.530639
 80220/100000: episode: 1514, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 44.490, mean reward: 5.561 [4.963, 6.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.374, 10.100], loss: 0.508869, mae: 0.595685, mean_q: 5.270497
 80230/100000: episode: 1515, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 38.527, mean reward: 3.853 [3.207, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.357, 10.100], loss: 2.007038, mae: 0.714200, mean_q: 5.306636
 80237/100000: episode: 1516, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 41.023, mean reward: 5.860 [4.617, 7.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.361, 10.100], loss: 0.484852, mae: 0.724675, mean_q: 5.745222
 80245/100000: episode: 1517, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 39.431, mean reward: 4.929 [3.884, 6.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.587, 10.100], loss: 0.623274, mae: 0.611750, mean_q: 5.316347
 80253/100000: episode: 1518, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 30.668, mean reward: 3.833 [2.726, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.435, 10.100], loss: 0.576349, mae: 0.571092, mean_q: 5.388118
 80259/100000: episode: 1519, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 30.843, mean reward: 5.140 [3.946, 7.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.673, 10.100], loss: 0.190680, mae: 0.420421, mean_q: 5.200356
[Info] 4-TH LEVEL FOUND: 10.756108283996582, Considering 10/90 traces
 80269/100000: episode: 1520, duration: 4.135s, episode steps: 10, steps per second: 2, episode reward: 51.057, mean reward: 5.106 [3.745, 7.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.520, 10.100], loss: 0.521923, mae: 0.624350, mean_q: 5.503479
 80274/100000: episode: 1521, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 28.857, mean reward: 5.771 [4.947, 6.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.558, 10.100], loss: 0.603755, mae: 0.620681, mean_q: 5.836009
 80279/100000: episode: 1522, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 37.259, mean reward: 7.452 [5.458, 11.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.586, 10.100], loss: 3.042714, mae: 0.799954, mean_q: 5.509711
 80287/100000: episode: 1523, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 166.828, mean reward: 20.853 [5.959, 78.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.556, 10.100], loss: 0.421834, mae: 0.573135, mean_q: 5.260183
 80292/100000: episode: 1524, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 32.694, mean reward: 6.539 [5.499, 9.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.519, 10.100], loss: 0.321938, mae: 0.515251, mean_q: 5.409224
 80300/100000: episode: 1525, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 30.356, mean reward: 3.795 [3.126, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.489, 10.100], loss: 0.277110, mae: 0.490352, mean_q: 5.269105
 80308/100000: episode: 1526, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 31.355, mean reward: 3.919 [3.633, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.016, 10.100], loss: 0.521689, mae: 0.680566, mean_q: 6.016364
 80314/100000: episode: 1527, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 36.317, mean reward: 6.053 [4.828, 7.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.545, 10.100], loss: 2.334912, mae: 0.812007, mean_q: 5.998186
 80322/100000: episode: 1528, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 42.119, mean reward: 5.265 [4.010, 7.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.510, 10.100], loss: 0.689382, mae: 0.621285, mean_q: 5.485009
 80330/100000: episode: 1529, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 49.257, mean reward: 6.157 [4.639, 8.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.535, 10.100], loss: 0.838960, mae: 0.691691, mean_q: 5.620489
 80338/100000: episode: 1530, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 49.220, mean reward: 6.152 [4.265, 8.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.551, 10.100], loss: 0.431664, mae: 0.497548, mean_q: 5.218197
 80346/100000: episode: 1531, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 38.226, mean reward: 4.778 [3.373, 6.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.412, 10.100], loss: 0.370410, mae: 0.546062, mean_q: 5.406596
 80354/100000: episode: 1532, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 43.981, mean reward: 5.498 [4.414, 6.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.489, 10.100], loss: 2.711558, mae: 0.911391, mean_q: 5.980251
 80362/100000: episode: 1533, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 50.413, mean reward: 6.302 [4.958, 8.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.490, 10.100], loss: 0.727952, mae: 0.688317, mean_q: 5.352513
 80370/100000: episode: 1534, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 57.551, mean reward: 7.194 [5.965, 9.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.553, 10.100], loss: 0.698420, mae: 0.669318, mean_q: 5.706852
 80378/100000: episode: 1535, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 44.806, mean reward: 5.601 [4.379, 6.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.421, 10.100], loss: 11.121145, mae: 1.107222, mean_q: 6.198280
 80386/100000: episode: 1536, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 77.783, mean reward: 9.723 [5.756, 21.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.705, 10.100], loss: 2.992485, mae: 0.883090, mean_q: 5.627328
 80394/100000: episode: 1537, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 31.473, mean reward: 3.934 [3.332, 6.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.489, 10.100], loss: 0.623537, mae: 0.641394, mean_q: 5.835194
 80400/100000: episode: 1538, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 39.675, mean reward: 6.613 [5.183, 7.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.426, 10.100], loss: 5.047247, mae: 1.142859, mean_q: 6.295375
 80405/100000: episode: 1539, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 30.910, mean reward: 6.182 [4.203, 7.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.940, 10.100], loss: 3.617535, mae: 1.116957, mean_q: 6.544631
 80413/100000: episode: 1540, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 42.923, mean reward: 5.365 [4.056, 7.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.373, 10.100], loss: 1.151706, mae: 0.745026, mean_q: 5.405392
 80419/100000: episode: 1541, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 90.186, mean reward: 15.031 [6.549, 23.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.572, 10.100], loss: 0.544796, mae: 0.610244, mean_q: 5.918640
 80427/100000: episode: 1542, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 59.340, mean reward: 7.418 [5.629, 10.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.691, 10.100], loss: 0.927739, mae: 0.643174, mean_q: 5.617287
 80435/100000: episode: 1543, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 57.813, mean reward: 7.227 [4.471, 11.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.547, 10.100], loss: 0.677231, mae: 0.744110, mean_q: 5.848491
 80440/100000: episode: 1544, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 20.712, mean reward: 4.142 [3.544, 5.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.583, 10.100], loss: 29.017115, mae: 1.227471, mean_q: 5.643332
 80448/100000: episode: 1545, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 54.873, mean reward: 6.859 [5.046, 8.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.475, 10.100], loss: 1.708855, mae: 1.122286, mean_q: 6.404350
 80454/100000: episode: 1546, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 43.253, mean reward: 7.209 [6.144, 8.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.456, 10.100], loss: 0.430379, mae: 0.617491, mean_q: 5.501035
 80459/100000: episode: 1547, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 26.435, mean reward: 5.287 [4.425, 5.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.563, 10.100], loss: 0.363920, mae: 0.551435, mean_q: 5.726447
 80467/100000: episode: 1548, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 40.965, mean reward: 5.121 [4.469, 6.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.458, 10.100], loss: 1.211417, mae: 0.931071, mean_q: 6.363496
 80472/100000: episode: 1549, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 21.630, mean reward: 4.326 [3.708, 5.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.603, 10.100], loss: 0.506943, mae: 0.686090, mean_q: 5.575716
 80480/100000: episode: 1550, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 35.659, mean reward: 4.457 [3.601, 5.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.479, 10.100], loss: 2.446533, mae: 0.855929, mean_q: 5.957005
 80488/100000: episode: 1551, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 30.100, mean reward: 3.763 [3.154, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.332, 10.100], loss: 10.672482, mae: 1.058206, mean_q: 6.369546
 80493/100000: episode: 1552, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 34.803, mean reward: 6.961 [5.691, 8.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.541, 10.100], loss: 17.039356, mae: 1.095452, mean_q: 5.845538
 80499/100000: episode: 1553, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 33.790, mean reward: 5.632 [4.472, 7.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.502, 10.100], loss: 1.112504, mae: 0.917216, mean_q: 6.299416
 80507/100000: episode: 1554, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 48.501, mean reward: 6.063 [4.301, 6.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.399, 10.100], loss: 1.510368, mae: 0.769936, mean_q: 5.907498
 80515/100000: episode: 1555, duration: 0.050s, episode steps: 8, steps per second: 158, episode reward: 40.063, mean reward: 5.008 [4.358, 6.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.674, 10.100], loss: 0.672426, mae: 0.597625, mean_q: 5.582179
 80520/100000: episode: 1556, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 25.222, mean reward: 5.044 [3.876, 7.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.620, 10.100], loss: 0.443823, mae: 0.605562, mean_q: 5.565110
 80525/100000: episode: 1557, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 25.156, mean reward: 5.031 [4.708, 5.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.497, 10.100], loss: 3.922268, mae: 1.030168, mean_q: 6.528763
 80533/100000: episode: 1558, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 52.544, mean reward: 6.568 [3.283, 10.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.503, 10.100], loss: 3.063867, mae: 1.137984, mean_q: 6.616340
 80538/100000: episode: 1559, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 34.588, mean reward: 6.918 [5.888, 8.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.609, 10.100], loss: 0.457083, mae: 0.577802, mean_q: 5.429451
 80543/100000: episode: 1560, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 30.229, mean reward: 6.046 [5.171, 6.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.575, 10.100], loss: 28.622396, mae: 1.301312, mean_q: 6.399714
 80548/100000: episode: 1561, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 27.985, mean reward: 5.597 [4.552, 7.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.645, 10.100], loss: 1.644018, mae: 1.248764, mean_q: 7.141353
 80553/100000: episode: 1562, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 27.898, mean reward: 5.580 [4.762, 7.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.441, 10.100], loss: 3.310286, mae: 0.939317, mean_q: 5.771332
 80561/100000: episode: 1563, duration: 0.059s, episode steps: 8, steps per second: 135, episode reward: 81.271, mean reward: 10.159 [4.945, 29.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.401, 10.100], loss: 0.589249, mae: 0.649240, mean_q: 5.780280
 80566/100000: episode: 1564, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 35.982, mean reward: 7.196 [5.118, 10.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.559, 10.100], loss: 0.490935, mae: 0.657403, mean_q: 5.825418
 80574/100000: episode: 1565, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 38.723, mean reward: 4.840 [3.537, 6.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.503, 10.100], loss: 0.950288, mae: 0.680479, mean_q: 5.809725
 80580/100000: episode: 1566, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 35.968, mean reward: 5.995 [4.765, 7.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.565, 10.100], loss: 14.754825, mae: 1.387827, mean_q: 6.663871
 80588/100000: episode: 1567, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 33.572, mean reward: 4.196 [3.460, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.408, 10.100], loss: 0.627506, mae: 0.700572, mean_q: 5.389654
 80596/100000: episode: 1568, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 36.872, mean reward: 4.609 [3.584, 5.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.517, 10.100], loss: 2.613692, mae: 0.940378, mean_q: 6.103080
 80604/100000: episode: 1569, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 40.602, mean reward: 5.075 [4.352, 6.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.455, 10.100], loss: 1.527648, mae: 0.915268, mean_q: 6.177505
 80609/100000: episode: 1570, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 25.095, mean reward: 5.019 [4.056, 6.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.647, 10.100], loss: 0.486485, mae: 0.623362, mean_q: 5.383576
 80617/100000: episode: 1571, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 62.229, mean reward: 7.779 [4.438, 22.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.598, 10.100], loss: 0.583668, mae: 0.671258, mean_q: 6.244422
 80622/100000: episode: 1572, duration: 0.043s, episode steps: 5, steps per second: 116, episode reward: 21.793, mean reward: 4.359 [3.613, 4.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.451, 10.100], loss: 0.863649, mae: 0.737528, mean_q: 6.023403
 80627/100000: episode: 1573, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 17.859, mean reward: 3.572 [3.371, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.591, 10.100], loss: 0.838589, mae: 0.791905, mean_q: 6.109571
 80635/100000: episode: 1574, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 47.821, mean reward: 5.978 [4.244, 8.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.492, 10.100], loss: 0.500371, mae: 0.641468, mean_q: 5.728041
 80641/100000: episode: 1575, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 55.756, mean reward: 9.293 [6.903, 13.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.543, 10.100], loss: 0.554722, mae: 0.627686, mean_q: 5.771816
 80649/100000: episode: 1576, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 71.733, mean reward: 8.967 [5.863, 17.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.585, 10.100], loss: 1.607357, mae: 0.811817, mean_q: 6.002192
 80657/100000: episode: 1577, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 66.830, mean reward: 8.354 [6.213, 14.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.488, 10.100], loss: 1.808544, mae: 0.897762, mean_q: 6.127457
 80662/100000: episode: 1578, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 60.113, mean reward: 12.023 [9.000, 16.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.565, 10.100], loss: 1.463275, mae: 0.786636, mean_q: 5.723367
 80670/100000: episode: 1579, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 50.363, mean reward: 6.295 [5.650, 7.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.609, 10.100], loss: 1.602550, mae: 0.891897, mean_q: 6.015843
 80675/100000: episode: 1580, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 31.173, mean reward: 6.235 [3.904, 9.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.646, 10.100], loss: 0.938993, mae: 0.768921, mean_q: 5.917487
 80680/100000: episode: 1581, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 22.723, mean reward: 4.545 [3.983, 4.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.964, 10.100], loss: 1.074370, mae: 0.716998, mean_q: 6.268182
 80686/100000: episode: 1582, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 31.785, mean reward: 5.297 [4.390, 6.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.430, 10.100], loss: 0.555646, mae: 0.733198, mean_q: 6.457920
[Info] FALSIFICATION!
[Info] Levels: [5.022236, 6.943151, 7.9348454, 10.756108, 11.479588]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.67]
[Info] Error Prob: 6.700000000000003e-05

 80694/100000: episode: 1583, duration: 4.590s, episode steps: 8, steps per second: 2, episode reward: 207.839, mean reward: 25.980 [6.069, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.453, 10.100], loss: 2.853441, mae: 0.897636, mean_q: 5.920332
 80794/100000: episode: 1584, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.469, mean reward: 1.865 [1.444, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.723, 10.098], loss: 1.538035, mae: 0.836433, mean_q: 6.301313
 80894/100000: episode: 1585, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 198.895, mean reward: 1.989 [1.456, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.098], loss: 3.855245, mae: 0.946396, mean_q: 6.355310
 80994/100000: episode: 1586, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 197.927, mean reward: 1.979 [1.451, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.190, 10.098], loss: 2.773371, mae: 0.900495, mean_q: 6.281995
 81094/100000: episode: 1587, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 207.151, mean reward: 2.072 [1.454, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.937, 10.098], loss: 4.392913, mae: 0.981371, mean_q: 6.336486
 81194/100000: episode: 1588, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 187.226, mean reward: 1.872 [1.481, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.639, 10.098], loss: 2.895494, mae: 0.864258, mean_q: 6.269639
 81294/100000: episode: 1589, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 204.255, mean reward: 2.043 [1.430, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.827, 10.505], loss: 3.874599, mae: 0.915105, mean_q: 6.316801
 81394/100000: episode: 1590, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.847, mean reward: 1.978 [1.470, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.677, 10.134], loss: 4.650045, mae: 0.987750, mean_q: 6.429994
 81494/100000: episode: 1591, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 182.753, mean reward: 1.828 [1.455, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.184], loss: 5.031730, mae: 0.987167, mean_q: 6.376288
 81594/100000: episode: 1592, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 174.873, mean reward: 1.749 [1.469, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.319, 10.168], loss: 9.913458, mae: 1.218975, mean_q: 6.427464
 81694/100000: episode: 1593, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.211, mean reward: 1.882 [1.476, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.713, 10.365], loss: 3.812693, mae: 0.962944, mean_q: 6.335374
 81794/100000: episode: 1594, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 209.223, mean reward: 2.092 [1.505, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.252, 10.159], loss: 3.739378, mae: 0.887812, mean_q: 6.238881
 81894/100000: episode: 1595, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.756, mean reward: 1.938 [1.478, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.791, 10.241], loss: 1.889695, mae: 0.876663, mean_q: 6.249824
 81994/100000: episode: 1596, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.541, mean reward: 2.035 [1.440, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.453, 10.098], loss: 5.196146, mae: 0.993464, mean_q: 6.211657
 82094/100000: episode: 1597, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 218.698, mean reward: 2.187 [1.479, 14.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.254, 10.239], loss: 1.591406, mae: 0.813234, mean_q: 6.205872
 82194/100000: episode: 1598, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 196.993, mean reward: 1.970 [1.506, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.761, 10.253], loss: 3.495244, mae: 0.996022, mean_q: 6.326526
 82294/100000: episode: 1599, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 186.180, mean reward: 1.862 [1.444, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.736, 10.098], loss: 1.236113, mae: 0.782617, mean_q: 6.147890
 82394/100000: episode: 1600, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 199.854, mean reward: 1.999 [1.474, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.599, 10.098], loss: 4.146080, mae: 0.881399, mean_q: 6.175962
 82494/100000: episode: 1601, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 194.401, mean reward: 1.944 [1.460, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.541, 10.413], loss: 4.500831, mae: 0.919221, mean_q: 6.094773
 82594/100000: episode: 1602, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 202.064, mean reward: 2.021 [1.532, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.793, 10.098], loss: 2.394033, mae: 0.845128, mean_q: 6.123575
 82694/100000: episode: 1603, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.222, mean reward: 1.972 [1.480, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.817, 10.185], loss: 7.018486, mae: 0.965440, mean_q: 6.168545
 82794/100000: episode: 1604, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.453, mean reward: 1.865 [1.451, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.478, 10.098], loss: 1.423697, mae: 0.784059, mean_q: 6.012649
 82894/100000: episode: 1605, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 190.571, mean reward: 1.906 [1.454, 5.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.475, 10.125], loss: 3.150197, mae: 0.896673, mean_q: 6.165935
 82994/100000: episode: 1606, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.401, mean reward: 1.884 [1.474, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.761, 10.098], loss: 4.044780, mae: 0.823798, mean_q: 6.015302
 83094/100000: episode: 1607, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 204.548, mean reward: 2.045 [1.465, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.562, 10.359], loss: 2.803780, mae: 0.778363, mean_q: 5.982335
 83194/100000: episode: 1608, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 184.692, mean reward: 1.847 [1.453, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.038, 10.106], loss: 5.363759, mae: 0.947019, mean_q: 5.954354
 83294/100000: episode: 1609, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 202.386, mean reward: 2.024 [1.483, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.208, 10.098], loss: 1.974809, mae: 0.775691, mean_q: 5.857666
 83394/100000: episode: 1610, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 188.735, mean reward: 1.887 [1.459, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.820, 10.098], loss: 2.880941, mae: 0.816483, mean_q: 5.900028
 83494/100000: episode: 1611, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 199.779, mean reward: 1.998 [1.493, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.882, 10.328], loss: 2.545102, mae: 0.751010, mean_q: 5.884427
 83594/100000: episode: 1612, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.998, mean reward: 1.860 [1.443, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.692, 10.098], loss: 3.679868, mae: 0.774591, mean_q: 5.890407
 83694/100000: episode: 1613, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.307, mean reward: 1.863 [1.445, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.292, 10.218], loss: 3.536146, mae: 0.833634, mean_q: 5.901663
 83794/100000: episode: 1614, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 184.117, mean reward: 1.841 [1.456, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.186], loss: 2.429042, mae: 0.730009, mean_q: 5.757060
 83894/100000: episode: 1615, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 200.264, mean reward: 2.003 [1.466, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.946, 10.241], loss: 4.223460, mae: 0.819547, mean_q: 5.682514
 83994/100000: episode: 1616, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 208.006, mean reward: 2.080 [1.520, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.714, 10.098], loss: 1.180754, mae: 0.708875, mean_q: 5.644932
 84094/100000: episode: 1617, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 187.082, mean reward: 1.871 [1.449, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.575, 10.098], loss: 2.424176, mae: 0.694078, mean_q: 5.466224
 84194/100000: episode: 1618, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 196.618, mean reward: 1.966 [1.524, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.356, 10.098], loss: 0.959267, mae: 0.609003, mean_q: 5.351915
 84294/100000: episode: 1619, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 197.390, mean reward: 1.974 [1.453, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.187], loss: 1.382828, mae: 0.678044, mean_q: 5.411242
 84394/100000: episode: 1620, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 201.935, mean reward: 2.019 [1.439, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.159], loss: 4.258028, mae: 0.775720, mean_q: 5.518956
 84494/100000: episode: 1621, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.454, mean reward: 1.935 [1.495, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.514, 10.230], loss: 2.551758, mae: 0.705108, mean_q: 5.432384
 84594/100000: episode: 1622, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 217.032, mean reward: 2.170 [1.484, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.005, 10.098], loss: 4.865748, mae: 0.782425, mean_q: 5.290945
 84694/100000: episode: 1623, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 186.279, mean reward: 1.863 [1.461, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.341, 10.098], loss: 4.154310, mae: 0.738731, mean_q: 5.161442
 84794/100000: episode: 1624, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.122, mean reward: 1.911 [1.444, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.559, 10.346], loss: 3.040749, mae: 0.615494, mean_q: 4.988667
 84894/100000: episode: 1625, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 197.575, mean reward: 1.976 [1.434, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.618, 10.098], loss: 2.324432, mae: 0.586182, mean_q: 4.932680
 84994/100000: episode: 1626, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 178.847, mean reward: 1.788 [1.464, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.262, 10.193], loss: 2.101543, mae: 0.548005, mean_q: 4.732520
 85094/100000: episode: 1627, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.790, mean reward: 1.948 [1.483, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.667, 10.123], loss: 4.652849, mae: 0.615330, mean_q: 4.598468
 85194/100000: episode: 1628, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.804, mean reward: 1.908 [1.471, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.395, 10.123], loss: 0.541722, mae: 0.473606, mean_q: 4.571642
 85294/100000: episode: 1629, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 186.995, mean reward: 1.870 [1.441, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.655, 10.098], loss: 0.406088, mae: 0.418088, mean_q: 4.375626
 85394/100000: episode: 1630, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.482, mean reward: 1.835 [1.464, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.098], loss: 1.894522, mae: 0.471211, mean_q: 4.307423
 85494/100000: episode: 1631, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 209.651, mean reward: 2.097 [1.444, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.332, 10.114], loss: 0.467252, mae: 0.397321, mean_q: 4.121002
 85594/100000: episode: 1632, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.800, mean reward: 1.958 [1.472, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.535, 10.098], loss: 0.349862, mae: 0.340124, mean_q: 4.049967
 85694/100000: episode: 1633, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 205.823, mean reward: 2.058 [1.456, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.151, 10.098], loss: 0.280704, mae: 0.323939, mean_q: 3.893591
 85794/100000: episode: 1634, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 208.705, mean reward: 2.087 [1.470, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.098], loss: 0.142880, mae: 0.316190, mean_q: 3.877975
 85894/100000: episode: 1635, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 219.566, mean reward: 2.196 [1.509, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.581, 10.434], loss: 0.146261, mae: 0.315108, mean_q: 3.880835
 85994/100000: episode: 1636, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 184.128, mean reward: 1.841 [1.467, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.809, 10.210], loss: 0.114915, mae: 0.302682, mean_q: 3.873637
 86094/100000: episode: 1637, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 197.545, mean reward: 1.975 [1.457, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.710, 10.098], loss: 0.113267, mae: 0.302809, mean_q: 3.872419
 86194/100000: episode: 1638, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.414, mean reward: 1.874 [1.488, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.986, 10.321], loss: 0.119172, mae: 0.310718, mean_q: 3.882216
 86294/100000: episode: 1639, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.750, mean reward: 1.917 [1.489, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.091, 10.098], loss: 0.109882, mae: 0.302962, mean_q: 3.846684
 86394/100000: episode: 1640, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 187.949, mean reward: 1.879 [1.430, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.906, 10.186], loss: 0.076397, mae: 0.282972, mean_q: 3.837809
 86494/100000: episode: 1641, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.768, mean reward: 1.888 [1.454, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.995, 10.098], loss: 0.081313, mae: 0.286922, mean_q: 3.840506
 86594/100000: episode: 1642, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 207.235, mean reward: 2.072 [1.475, 5.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.535, 10.098], loss: 0.101909, mae: 0.289563, mean_q: 3.854091
 86694/100000: episode: 1643, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 253.005, mean reward: 2.530 [1.441, 5.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.312], loss: 0.104085, mae: 0.295945, mean_q: 3.871255
 86794/100000: episode: 1644, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.541, mean reward: 1.865 [1.464, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.266, 10.098], loss: 0.137973, mae: 0.313259, mean_q: 3.907311
 86894/100000: episode: 1645, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.423, mean reward: 1.834 [1.476, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.966, 10.098], loss: 0.116424, mae: 0.312319, mean_q: 3.894950
 86994/100000: episode: 1646, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 215.521, mean reward: 2.155 [1.452, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.592, 10.098], loss: 0.110847, mae: 0.296533, mean_q: 3.877720
 87094/100000: episode: 1647, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 188.737, mean reward: 1.887 [1.452, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.210], loss: 0.085836, mae: 0.298520, mean_q: 3.871985
 87194/100000: episode: 1648, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 181.479, mean reward: 1.815 [1.452, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.174], loss: 0.091291, mae: 0.300433, mean_q: 3.882087
 87294/100000: episode: 1649, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.060, mean reward: 1.931 [1.548, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.220], loss: 0.102792, mae: 0.321573, mean_q: 3.872083
 87394/100000: episode: 1650, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 191.541, mean reward: 1.915 [1.439, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.505, 10.098], loss: 0.090400, mae: 0.297597, mean_q: 3.870015
 87494/100000: episode: 1651, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.209, mean reward: 1.932 [1.446, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.547, 10.198], loss: 0.100936, mae: 0.311617, mean_q: 3.890300
 87594/100000: episode: 1652, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 192.287, mean reward: 1.923 [1.500, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.794, 10.286], loss: 0.090447, mae: 0.299897, mean_q: 3.871857
 87694/100000: episode: 1653, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 191.884, mean reward: 1.919 [1.472, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.536, 10.331], loss: 0.083039, mae: 0.294755, mean_q: 3.847304
 87794/100000: episode: 1654, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 185.995, mean reward: 1.860 [1.443, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.575, 10.098], loss: 0.090679, mae: 0.304189, mean_q: 3.861566
 87894/100000: episode: 1655, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.660, mean reward: 1.877 [1.494, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.098], loss: 0.088738, mae: 0.303131, mean_q: 3.874919
 87994/100000: episode: 1656, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 200.107, mean reward: 2.001 [1.457, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.648, 10.222], loss: 0.081140, mae: 0.288933, mean_q: 3.859356
 88094/100000: episode: 1657, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.906, mean reward: 1.879 [1.444, 6.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.212], loss: 0.081737, mae: 0.282488, mean_q: 3.845982
 88194/100000: episode: 1658, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 211.358, mean reward: 2.114 [1.457, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.580, 10.147], loss: 0.093531, mae: 0.307357, mean_q: 3.874797
 88294/100000: episode: 1659, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 195.253, mean reward: 1.953 [1.473, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.376, 10.269], loss: 0.095743, mae: 0.300758, mean_q: 3.860170
 88394/100000: episode: 1660, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 228.895, mean reward: 2.289 [1.503, 8.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.138, 10.394], loss: 0.086489, mae: 0.288777, mean_q: 3.864449
 88494/100000: episode: 1661, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 198.605, mean reward: 1.986 [1.457, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.132, 10.252], loss: 0.097583, mae: 0.311752, mean_q: 3.873222
 88594/100000: episode: 1662, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 190.916, mean reward: 1.909 [1.444, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.098], loss: 0.085719, mae: 0.297931, mean_q: 3.875889
 88694/100000: episode: 1663, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 189.429, mean reward: 1.894 [1.435, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.479, 10.098], loss: 0.096290, mae: 0.300403, mean_q: 3.872150
 88794/100000: episode: 1664, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.575, mean reward: 1.996 [1.464, 4.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.229, 10.149], loss: 0.099980, mae: 0.303053, mean_q: 3.885287
 88894/100000: episode: 1665, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 196.701, mean reward: 1.967 [1.477, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.288, 10.109], loss: 0.109463, mae: 0.312701, mean_q: 3.895175
 88994/100000: episode: 1666, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 198.332, mean reward: 1.983 [1.481, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.659, 10.098], loss: 0.092421, mae: 0.302072, mean_q: 3.883808
 89094/100000: episode: 1667, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 183.803, mean reward: 1.838 [1.486, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.186, 10.102], loss: 0.096574, mae: 0.305889, mean_q: 3.892420
 89194/100000: episode: 1668, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 185.877, mean reward: 1.859 [1.462, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.972, 10.137], loss: 0.101760, mae: 0.310814, mean_q: 3.898901
 89294/100000: episode: 1669, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 193.613, mean reward: 1.936 [1.544, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.155, 10.184], loss: 0.099303, mae: 0.302138, mean_q: 3.882169
 89394/100000: episode: 1670, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.712, mean reward: 2.017 [1.488, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.669, 10.237], loss: 0.103375, mae: 0.308336, mean_q: 3.899829
 89494/100000: episode: 1671, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 205.270, mean reward: 2.053 [1.456, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.806, 10.098], loss: 0.104107, mae: 0.305374, mean_q: 3.883222
 89594/100000: episode: 1672, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 191.496, mean reward: 1.915 [1.477, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.098], loss: 0.110537, mae: 0.312171, mean_q: 3.894797
 89694/100000: episode: 1673, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 183.495, mean reward: 1.835 [1.451, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.098], loss: 0.100459, mae: 0.309187, mean_q: 3.892207
 89794/100000: episode: 1674, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 201.430, mean reward: 2.014 [1.472, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.765, 10.098], loss: 0.090404, mae: 0.284301, mean_q: 3.859958
 89894/100000: episode: 1675, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 201.177, mean reward: 2.012 [1.451, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.858, 10.237], loss: 0.086116, mae: 0.289612, mean_q: 3.876495
 89994/100000: episode: 1676, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 177.791, mean reward: 1.778 [1.470, 2.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.584, 10.151], loss: 0.089368, mae: 0.287209, mean_q: 3.859290
 90094/100000: episode: 1677, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.074, mean reward: 1.851 [1.441, 5.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.451, 10.209], loss: 0.088481, mae: 0.297707, mean_q: 3.865292
 90194/100000: episode: 1678, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 191.393, mean reward: 1.914 [1.437, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.123, 10.098], loss: 0.100413, mae: 0.296800, mean_q: 3.855087
 90294/100000: episode: 1679, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.717, mean reward: 2.017 [1.453, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.901, 10.098], loss: 0.092346, mae: 0.291874, mean_q: 3.869356
 90394/100000: episode: 1680, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 198.098, mean reward: 1.981 [1.465, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.328], loss: 0.110919, mae: 0.307298, mean_q: 3.874235
 90494/100000: episode: 1681, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 178.407, mean reward: 1.784 [1.495, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.065, 10.244], loss: 0.099292, mae: 0.303677, mean_q: 3.868350
 90594/100000: episode: 1682, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 204.003, mean reward: 2.040 [1.451, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.128, 10.098], loss: 0.078731, mae: 0.277574, mean_q: 3.859893
[Info] 1-TH LEVEL FOUND: 5.191329002380371, Considering 10/90 traces
 90694/100000: episode: 1683, duration: 4.687s, episode steps: 100, steps per second: 21, episode reward: 225.260, mean reward: 2.253 [1.436, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.474, 10.098], loss: 0.096713, mae: 0.292282, mean_q: 3.889447
 90745/100000: episode: 1684, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 117.475, mean reward: 2.303 [1.772, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.262, 10.320], loss: 0.087680, mae: 0.283779, mean_q: 3.884572
 90765/100000: episode: 1685, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 51.546, mean reward: 2.577 [2.251, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.262, 10.100], loss: 0.087042, mae: 0.292624, mean_q: 3.892609
 90802/100000: episode: 1686, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 81.661, mean reward: 2.207 [1.451, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.410, 10.128], loss: 0.090369, mae: 0.294668, mean_q: 3.899142
 90824/100000: episode: 1687, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 66.040, mean reward: 3.002 [2.103, 5.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.420], loss: 0.097331, mae: 0.308143, mean_q: 3.894593
 90875/100000: episode: 1688, duration: 0.284s, episode steps: 51, steps per second: 179, episode reward: 108.943, mean reward: 2.136 [1.592, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.900, 10.230], loss: 0.102263, mae: 0.307289, mean_q: 3.905819
 90926/100000: episode: 1689, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 118.992, mean reward: 2.333 [1.648, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.369, 10.100], loss: 0.081226, mae: 0.279746, mean_q: 3.877332
 91025/100000: episode: 1690, duration: 0.546s, episode steps: 99, steps per second: 181, episode reward: 191.120, mean reward: 1.931 [1.442, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.138, 10.243], loss: 0.103454, mae: 0.304311, mean_q: 3.923650
 91047/100000: episode: 1691, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 45.922, mean reward: 2.087 [1.594, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.047, 10.172], loss: 0.101826, mae: 0.316571, mean_q: 3.934810
 91067/100000: episode: 1692, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 54.059, mean reward: 2.703 [2.262, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.482], loss: 0.074538, mae: 0.272271, mean_q: 3.910793
 91100/100000: episode: 1693, duration: 0.201s, episode steps: 33, steps per second: 165, episode reward: 74.080, mean reward: 2.245 [1.558, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.405, 10.186], loss: 0.092264, mae: 0.292860, mean_q: 3.896722
 91151/100000: episode: 1694, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 99.958, mean reward: 1.960 [1.471, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.082, 10.316], loss: 0.092497, mae: 0.294100, mean_q: 3.910136
 91202/100000: episode: 1695, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 98.459, mean reward: 1.931 [1.462, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.224, 10.183], loss: 0.080070, mae: 0.283091, mean_q: 3.859859
 91301/100000: episode: 1696, duration: 0.568s, episode steps: 99, steps per second: 174, episode reward: 198.944, mean reward: 2.010 [1.462, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.364, 10.369], loss: 0.086849, mae: 0.291303, mean_q: 3.886620
 91327/100000: episode: 1697, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 55.753, mean reward: 2.144 [1.674, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.271], loss: 0.099850, mae: 0.320114, mean_q: 3.933876
 91378/100000: episode: 1698, duration: 0.281s, episode steps: 51, steps per second: 181, episode reward: 99.346, mean reward: 1.948 [1.504, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.265, 10.351], loss: 0.108271, mae: 0.316966, mean_q: 3.913809
 91429/100000: episode: 1699, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 103.111, mean reward: 2.022 [1.478, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.295, 10.100], loss: 0.110898, mae: 0.315527, mean_q: 3.884672
 91480/100000: episode: 1700, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 109.056, mean reward: 2.138 [1.472, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.606, 10.100], loss: 0.088666, mae: 0.299163, mean_q: 3.917525
 91494/100000: episode: 1701, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 47.954, mean reward: 3.425 [1.995, 6.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.672, 10.100], loss: 0.097478, mae: 0.306211, mean_q: 3.962304
 91520/100000: episode: 1702, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 54.292, mean reward: 2.088 [1.506, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.254, 10.100], loss: 0.082908, mae: 0.281823, mean_q: 3.891574
 91540/100000: episode: 1703, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 56.060, mean reward: 2.803 [1.921, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.497, 10.100], loss: 0.103523, mae: 0.294888, mean_q: 3.937267
 91573/100000: episode: 1704, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 69.125, mean reward: 2.095 [1.661, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.094, 10.286], loss: 0.080224, mae: 0.286110, mean_q: 3.929724
 91599/100000: episode: 1705, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 58.178, mean reward: 2.238 [1.701, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.224, 10.408], loss: 0.072135, mae: 0.282160, mean_q: 3.899570
 91625/100000: episode: 1706, duration: 0.161s, episode steps: 26, steps per second: 162, episode reward: 59.056, mean reward: 2.271 [1.559, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.743, 10.235], loss: 0.082421, mae: 0.276467, mean_q: 3.857052
 91662/100000: episode: 1707, duration: 0.232s, episode steps: 37, steps per second: 160, episode reward: 105.845, mean reward: 2.861 [1.757, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.278, 10.278], loss: 0.090658, mae: 0.303026, mean_q: 3.909327
 91682/100000: episode: 1708, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 48.577, mean reward: 2.429 [1.832, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.439], loss: 0.089905, mae: 0.296395, mean_q: 3.914945
 91702/100000: episode: 1709, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 60.515, mean reward: 3.026 [2.003, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.384, 10.100], loss: 0.102027, mae: 0.307531, mean_q: 3.930376
 91724/100000: episode: 1710, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 53.326, mean reward: 2.424 [1.817, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.059, 10.383], loss: 0.079273, mae: 0.294159, mean_q: 3.874030
 91775/100000: episode: 1711, duration: 0.289s, episode steps: 51, steps per second: 176, episode reward: 94.786, mean reward: 1.859 [1.487, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.125, 10.100], loss: 0.085445, mae: 0.293465, mean_q: 3.950402
 91797/100000: episode: 1712, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 51.582, mean reward: 2.345 [1.827, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.481, 10.365], loss: 0.084323, mae: 0.302156, mean_q: 3.929690
 91811/100000: episode: 1713, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 40.065, mean reward: 2.862 [1.947, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.640, 10.100], loss: 0.096419, mae: 0.296396, mean_q: 3.942436
 91910/100000: episode: 1714, duration: 0.531s, episode steps: 99, steps per second: 187, episode reward: 190.291, mean reward: 1.922 [1.460, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.608, 10.393], loss: 0.099044, mae: 0.306705, mean_q: 3.960681
 91930/100000: episode: 1715, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 51.007, mean reward: 2.550 [1.903, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.364, 10.100], loss: 0.080042, mae: 0.296322, mean_q: 3.919898
 91981/100000: episode: 1716, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 111.615, mean reward: 2.189 [1.607, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.481, 10.334], loss: 0.084588, mae: 0.287936, mean_q: 3.936705
 92007/100000: episode: 1717, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 49.927, mean reward: 1.920 [1.632, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.079, 10.239], loss: 0.115759, mae: 0.313963, mean_q: 3.959102
 92040/100000: episode: 1718, duration: 0.201s, episode steps: 33, steps per second: 164, episode reward: 78.590, mean reward: 2.382 [1.866, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.933, 10.302], loss: 0.099361, mae: 0.305299, mean_q: 3.942078
 92073/100000: episode: 1719, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 64.504, mean reward: 1.955 [1.483, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.318, 10.112], loss: 0.097259, mae: 0.306673, mean_q: 3.981451
 92093/100000: episode: 1720, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 51.248, mean reward: 2.562 [2.126, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.249, 10.395], loss: 0.082794, mae: 0.271802, mean_q: 3.947135
 92113/100000: episode: 1721, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 61.819, mean reward: 3.091 [2.253, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.507], loss: 0.090300, mae: 0.298096, mean_q: 3.979324
 92135/100000: episode: 1722, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 49.882, mean reward: 2.267 [1.544, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.210, 10.147], loss: 0.088690, mae: 0.301368, mean_q: 3.979136
 92157/100000: episode: 1723, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 63.870, mean reward: 2.903 [1.965, 6.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.127, 10.286], loss: 0.076903, mae: 0.283509, mean_q: 3.949425
 92177/100000: episode: 1724, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 56.261, mean reward: 2.813 [1.970, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.384, 10.397], loss: 0.112666, mae: 0.311085, mean_q: 3.971092
 92276/100000: episode: 1725, duration: 0.505s, episode steps: 99, steps per second: 196, episode reward: 185.204, mean reward: 1.871 [1.443, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.418, 10.100], loss: 0.111432, mae: 0.320108, mean_q: 3.985044
 92302/100000: episode: 1726, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 53.493, mean reward: 2.057 [1.696, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.658, 10.202], loss: 0.087234, mae: 0.293828, mean_q: 3.971645
 92353/100000: episode: 1727, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 108.624, mean reward: 2.130 [1.522, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.056, 10.100], loss: 0.095549, mae: 0.308768, mean_q: 4.020943
 92386/100000: episode: 1728, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 71.618, mean reward: 2.170 [1.518, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.023, 10.100], loss: 0.097986, mae: 0.309365, mean_q: 3.976710
 92408/100000: episode: 1729, duration: 0.136s, episode steps: 22, steps per second: 161, episode reward: 48.102, mean reward: 2.186 [1.674, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.298], loss: 0.076666, mae: 0.287551, mean_q: 4.000137
 92430/100000: episode: 1730, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 48.658, mean reward: 2.212 [1.793, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.842, 10.291], loss: 0.105688, mae: 0.312330, mean_q: 3.973776
 92481/100000: episode: 1731, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 92.975, mean reward: 1.823 [1.497, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.448, 10.220], loss: 0.103644, mae: 0.320840, mean_q: 3.991438
 92501/100000: episode: 1732, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 53.906, mean reward: 2.695 [2.093, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.423, 10.474], loss: 0.099966, mae: 0.292579, mean_q: 3.992586
 92515/100000: episode: 1733, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 42.095, mean reward: 3.007 [2.036, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.444, 10.100], loss: 0.086800, mae: 0.304823, mean_q: 4.028472
 92529/100000: episode: 1734, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 40.723, mean reward: 2.909 [2.160, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.689, 10.100], loss: 0.087662, mae: 0.287007, mean_q: 4.008064
 92543/100000: episode: 1735, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 41.311, mean reward: 2.951 [2.311, 5.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.350, 10.100], loss: 0.088082, mae: 0.298625, mean_q: 3.980783
 92557/100000: episode: 1736, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 38.038, mean reward: 2.717 [2.177, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.355, 10.100], loss: 0.111632, mae: 0.328567, mean_q: 4.035458
 92608/100000: episode: 1737, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 104.927, mean reward: 2.057 [1.494, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.449, 10.100], loss: 0.108599, mae: 0.305100, mean_q: 3.993180
 92659/100000: episode: 1738, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 112.631, mean reward: 2.208 [1.637, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.796, 10.363], loss: 0.117671, mae: 0.327583, mean_q: 4.011609
 92692/100000: episode: 1739, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 74.967, mean reward: 2.272 [1.676, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.219, 10.218], loss: 0.108338, mae: 0.306446, mean_q: 4.045610
 92714/100000: episode: 1740, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 51.176, mean reward: 2.326 [1.810, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.470, 10.237], loss: 0.096872, mae: 0.302642, mean_q: 4.038392
 92728/100000: episode: 1741, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 35.102, mean reward: 2.507 [1.957, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.240, 10.100], loss: 0.079138, mae: 0.287504, mean_q: 4.007347
 92754/100000: episode: 1742, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 62.266, mean reward: 2.395 [1.673, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.664, 10.190], loss: 0.087156, mae: 0.304351, mean_q: 4.049766
 92853/100000: episode: 1743, duration: 0.521s, episode steps: 99, steps per second: 190, episode reward: 182.871, mean reward: 1.847 [1.449, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.396, 10.102], loss: 0.110733, mae: 0.322180, mean_q: 4.044219
 92886/100000: episode: 1744, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 85.293, mean reward: 2.585 [1.715, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.423, 10.261], loss: 0.110946, mae: 0.329362, mean_q: 4.080600
 92985/100000: episode: 1745, duration: 0.539s, episode steps: 99, steps per second: 184, episode reward: 182.275, mean reward: 1.841 [1.491, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-1.083, 10.309], loss: 0.094987, mae: 0.300309, mean_q: 4.023189
 93022/100000: episode: 1746, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 130.843, mean reward: 3.536 [2.158, 6.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.474, 10.568], loss: 0.112076, mae: 0.327106, mean_q: 4.058504
 93048/100000: episode: 1747, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 54.213, mean reward: 2.085 [1.621, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.350, 10.255], loss: 0.096466, mae: 0.306953, mean_q: 4.005368
 93099/100000: episode: 1748, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 104.512, mean reward: 2.049 [1.500, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.586, 10.283], loss: 0.108541, mae: 0.326052, mean_q: 4.085736
 93132/100000: episode: 1749, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 85.407, mean reward: 2.588 [2.129, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.062, 10.380], loss: 0.082374, mae: 0.278968, mean_q: 4.030236
 93158/100000: episode: 1750, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 50.303, mean reward: 1.935 [1.522, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.045, 10.315], loss: 0.095181, mae: 0.296925, mean_q: 4.084797
 93184/100000: episode: 1751, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 48.418, mean reward: 1.862 [1.604, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.244], loss: 0.098441, mae: 0.309308, mean_q: 4.124000
 93210/100000: episode: 1752, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 52.263, mean reward: 2.010 [1.601, 2.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.283], loss: 0.096152, mae: 0.312525, mean_q: 4.064827
 93232/100000: episode: 1753, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 57.781, mean reward: 2.626 [1.632, 4.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.749, 10.463], loss: 0.089804, mae: 0.302096, mean_q: 4.087154
 93258/100000: episode: 1754, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 61.369, mean reward: 2.360 [1.813, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.361, 10.386], loss: 0.114825, mae: 0.323629, mean_q: 4.089792
 93357/100000: episode: 1755, duration: 0.561s, episode steps: 99, steps per second: 176, episode reward: 208.894, mean reward: 2.110 [1.517, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.330, 10.100], loss: 0.084798, mae: 0.295496, mean_q: 4.053669
 93456/100000: episode: 1756, duration: 0.570s, episode steps: 99, steps per second: 174, episode reward: 196.599, mean reward: 1.986 [1.466, 4.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.296, 10.100], loss: 0.096533, mae: 0.305873, mean_q: 4.066290
 93478/100000: episode: 1757, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 49.465, mean reward: 2.248 [1.907, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.359, 10.317], loss: 0.103783, mae: 0.306739, mean_q: 4.056118
 93515/100000: episode: 1758, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 97.576, mean reward: 2.637 [2.006, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.421, 10.325], loss: 0.099548, mae: 0.309543, mean_q: 4.084060
 93548/100000: episode: 1759, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 72.159, mean reward: 2.187 [1.658, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.874, 10.335], loss: 0.101907, mae: 0.315793, mean_q: 4.065887
 93585/100000: episode: 1760, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 102.033, mean reward: 2.758 [1.878, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.642, 10.429], loss: 0.108436, mae: 0.331256, mean_q: 4.101485
 93636/100000: episode: 1761, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 101.488, mean reward: 1.990 [1.500, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.372, 10.100], loss: 0.118152, mae: 0.321201, mean_q: 4.111075
 93650/100000: episode: 1762, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 50.412, mean reward: 3.601 [2.215, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.270, 10.100], loss: 0.127120, mae: 0.321881, mean_q: 4.148611
 93749/100000: episode: 1763, duration: 0.531s, episode steps: 99, steps per second: 187, episode reward: 217.828, mean reward: 2.200 [1.433, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.797, 10.100], loss: 0.124130, mae: 0.337906, mean_q: 4.133852
 93763/100000: episode: 1764, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 30.770, mean reward: 2.198 [1.912, 2.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.306, 10.100], loss: 0.096040, mae: 0.308550, mean_q: 4.137404
 93862/100000: episode: 1765, duration: 0.533s, episode steps: 99, steps per second: 186, episode reward: 189.397, mean reward: 1.913 [1.464, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.523, 10.122], loss: 0.093594, mae: 0.307900, mean_q: 4.131833
 93895/100000: episode: 1766, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 83.706, mean reward: 2.537 [1.900, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.739, 10.362], loss: 0.085709, mae: 0.301794, mean_q: 4.085057
 93917/100000: episode: 1767, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 38.407, mean reward: 1.746 [1.515, 2.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.076, 10.100], loss: 0.102689, mae: 0.310624, mean_q: 4.121710
 93937/100000: episode: 1768, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 49.492, mean reward: 2.475 [2.031, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.295], loss: 0.108362, mae: 0.316129, mean_q: 4.124080
 93988/100000: episode: 1769, duration: 0.272s, episode steps: 51, steps per second: 188, episode reward: 109.038, mean reward: 2.138 [1.559, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.492, 10.190], loss: 0.116338, mae: 0.323898, mean_q: 4.126209
 94039/100000: episode: 1770, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 125.004, mean reward: 2.451 [1.766, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.816, 10.266], loss: 0.091972, mae: 0.313627, mean_q: 4.149703
 94059/100000: episode: 1771, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 50.847, mean reward: 2.542 [2.018, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.486, 10.425], loss: 0.117631, mae: 0.320901, mean_q: 4.140012
 94110/100000: episode: 1772, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 133.504, mean reward: 2.618 [1.759, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.440, 10.503], loss: 0.097659, mae: 0.318118, mean_q: 4.182644
[Info] 2-TH LEVEL FOUND: 6.288913249969482, Considering 10/90 traces
 94161/100000: episode: 1773, duration: 4.348s, episode steps: 51, steps per second: 12, episode reward: 105.244, mean reward: 2.064 [1.479, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.531, 10.228], loss: 0.107403, mae: 0.328939, mean_q: 4.208734
 94187/100000: episode: 1774, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 90.920, mean reward: 3.497 [2.450, 5.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.814, 10.612], loss: 0.093548, mae: 0.302779, mean_q: 4.172217
 94196/100000: episode: 1775, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 27.618, mean reward: 3.069 [2.426, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.536, 10.100], loss: 0.101013, mae: 0.317635, mean_q: 4.189766
 94205/100000: episode: 1776, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 35.370, mean reward: 3.930 [2.249, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.602, 10.100], loss: 0.102978, mae: 0.303692, mean_q: 4.153397
 94217/100000: episode: 1777, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 42.079, mean reward: 3.507 [2.360, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.453, 10.100], loss: 0.098002, mae: 0.316233, mean_q: 4.247623
 94223/100000: episode: 1778, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 24.032, mean reward: 4.005 [3.261, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.359, 10.100], loss: 0.151587, mae: 0.348998, mean_q: 4.287512
 94229/100000: episode: 1779, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 22.523, mean reward: 3.754 [3.026, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.304, 10.100], loss: 0.134996, mae: 0.374634, mean_q: 4.344708
 94235/100000: episode: 1780, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 24.066, mean reward: 4.011 [3.434, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.238, 10.100], loss: 0.141978, mae: 0.353013, mean_q: 4.145190
 94241/100000: episode: 1781, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 21.921, mean reward: 3.653 [2.811, 4.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.622, 10.100], loss: 0.079291, mae: 0.318731, mean_q: 4.223709
 94256/100000: episode: 1782, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 45.083, mean reward: 3.006 [2.460, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.515], loss: 0.109108, mae: 0.337967, mean_q: 4.200426
 94271/100000: episode: 1783, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 55.265, mean reward: 3.684 [2.779, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.724, 10.583], loss: 0.099512, mae: 0.332850, mean_q: 4.200166
 94283/100000: episode: 1784, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 48.983, mean reward: 4.082 [3.070, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.471, 10.100], loss: 0.084879, mae: 0.301842, mean_q: 4.117282
 94295/100000: episode: 1785, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 46.052, mean reward: 3.838 [2.195, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.087, 10.510], loss: 0.114847, mae: 0.346405, mean_q: 4.229597
 94320/100000: episode: 1786, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 75.331, mean reward: 3.013 [2.094, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.428, 10.367], loss: 0.105750, mae: 0.334110, mean_q: 4.224612
 94332/100000: episode: 1787, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 43.537, mean reward: 3.628 [2.828, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.390, 10.100], loss: 0.096975, mae: 0.326199, mean_q: 4.232830
 94339/100000: episode: 1788, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 19.866, mean reward: 2.838 [2.456, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.408, 10.100], loss: 0.106510, mae: 0.327270, mean_q: 4.144490
 94345/100000: episode: 1789, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 65.437, mean reward: 10.906 [6.412, 24.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.334, 10.100], loss: 0.102718, mae: 0.314925, mean_q: 4.165471
 94357/100000: episode: 1790, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 37.516, mean reward: 3.126 [2.420, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.383, 10.100], loss: 0.120537, mae: 0.350791, mean_q: 4.256715
 94367/100000: episode: 1791, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 27.041, mean reward: 2.704 [2.246, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.390, 10.100], loss: 0.098004, mae: 0.327272, mean_q: 4.134822
 94393/100000: episode: 1792, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 71.574, mean reward: 2.753 [2.136, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.897, 10.436], loss: 0.115310, mae: 0.340286, mean_q: 4.189542
 94405/100000: episode: 1793, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 37.737, mean reward: 3.145 [2.525, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.250, 10.100], loss: 0.104693, mae: 0.337768, mean_q: 4.170261
 94431/100000: episode: 1794, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 72.704, mean reward: 2.796 [2.178, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.381], loss: 0.428851, mae: 0.384945, mean_q: 4.275755
 94440/100000: episode: 1795, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 33.852, mean reward: 3.761 [2.480, 7.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.491, 10.100], loss: 0.319862, mae: 0.537857, mean_q: 4.354297
 94446/100000: episode: 1796, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 24.295, mean reward: 4.049 [2.963, 5.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.300, 10.100], loss: 0.216470, mae: 0.481290, mean_q: 4.197659
 94455/100000: episode: 1797, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 31.926, mean reward: 3.547 [2.526, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.578, 10.100], loss: 0.176344, mae: 0.428947, mean_q: 4.166509
 94467/100000: episode: 1798, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 63.095, mean reward: 5.258 [3.323, 6.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.513, 10.100], loss: 0.170812, mae: 0.427012, mean_q: 4.331020
 94492/100000: episode: 1799, duration: 0.161s, episode steps: 25, steps per second: 155, episode reward: 77.244, mean reward: 3.090 [1.483, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.495, 10.174], loss: 0.134680, mae: 0.370402, mean_q: 4.322920
 94501/100000: episode: 1800, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 25.653, mean reward: 2.850 [2.430, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.462, 10.100], loss: 0.173257, mae: 0.381940, mean_q: 4.395217
 94516/100000: episode: 1801, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 47.015, mean reward: 3.134 [2.135, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.299, 10.320], loss: 0.139298, mae: 0.379645, mean_q: 4.277428
 94525/100000: episode: 1802, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 23.511, mean reward: 2.612 [2.246, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.278, 10.100], loss: 0.102730, mae: 0.330978, mean_q: 4.260164
 94537/100000: episode: 1803, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 48.191, mean reward: 4.016 [2.768, 5.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.496], loss: 0.108898, mae: 0.332475, mean_q: 4.297394
 94562/100000: episode: 1804, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 85.795, mean reward: 3.432 [2.177, 7.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.370], loss: 0.161730, mae: 0.374749, mean_q: 4.378395
 94572/100000: episode: 1805, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 27.695, mean reward: 2.770 [2.346, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.337, 10.100], loss: 0.115464, mae: 0.313259, mean_q: 4.372878
 94584/100000: episode: 1806, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 71.227, mean reward: 5.936 [2.869, 14.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.286, 10.100], loss: 0.160078, mae: 0.405498, mean_q: 4.403618
 94609/100000: episode: 1807, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 134.606, mean reward: 5.384 [2.988, 12.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.785, 10.440], loss: 0.237157, mae: 0.383692, mean_q: 4.366660
 94621/100000: episode: 1808, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 37.154, mean reward: 3.096 [2.572, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.728, 10.418], loss: 0.133772, mae: 0.359089, mean_q: 4.340801
 94628/100000: episode: 1809, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 17.288, mean reward: 2.470 [2.243, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.368, 10.100], loss: 0.150740, mae: 0.371453, mean_q: 4.310364
 94640/100000: episode: 1810, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 32.284, mean reward: 2.690 [1.919, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.459], loss: 0.171853, mae: 0.388435, mean_q: 4.393907
 94652/100000: episode: 1811, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 35.093, mean reward: 2.924 [2.160, 5.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.278, 10.100], loss: 0.206610, mae: 0.410785, mean_q: 4.486960
 94664/100000: episode: 1812, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 50.740, mean reward: 4.228 [2.713, 6.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.503, 10.100], loss: 0.189293, mae: 0.418011, mean_q: 4.497883
 94676/100000: episode: 1813, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 45.527, mean reward: 3.794 [2.715, 7.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.275, 10.100], loss: 0.153942, mae: 0.388489, mean_q: 4.429151
 94685/100000: episode: 1814, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 26.472, mean reward: 2.941 [2.624, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.876, 10.100], loss: 0.175012, mae: 0.384639, mean_q: 4.399964
 94694/100000: episode: 1815, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 23.750, mean reward: 2.639 [2.212, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.356, 10.100], loss: 0.109820, mae: 0.344151, mean_q: 4.480364
 94720/100000: episode: 1816, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 68.211, mean reward: 2.624 [1.638, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.331, 10.127], loss: 0.456388, mae: 0.444414, mean_q: 4.485807
 94732/100000: episode: 1817, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 33.460, mean reward: 2.788 [2.413, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.721, 10.100], loss: 0.226003, mae: 0.422689, mean_q: 4.428656
 94741/100000: episode: 1818, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 31.625, mean reward: 3.514 [2.337, 6.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.369, 10.100], loss: 0.170288, mae: 0.421764, mean_q: 4.516715
 94747/100000: episode: 1819, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 19.728, mean reward: 3.288 [2.819, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.361, 10.100], loss: 0.180897, mae: 0.410478, mean_q: 4.445829
 94759/100000: episode: 1820, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 34.156, mean reward: 2.846 [2.080, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.393], loss: 0.210442, mae: 0.423738, mean_q: 4.454183
 94771/100000: episode: 1821, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 37.113, mean reward: 3.093 [2.865, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.549, 10.459], loss: 0.185799, mae: 0.422334, mean_q: 4.529938
 94781/100000: episode: 1822, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 31.240, mean reward: 3.124 [2.329, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.288, 10.100], loss: 0.171170, mae: 0.432957, mean_q: 4.527039
 94806/100000: episode: 1823, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 73.420, mean reward: 2.937 [2.387, 5.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.562, 10.437], loss: 0.220490, mae: 0.381233, mean_q: 4.444854
 94812/100000: episode: 1824, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 25.656, mean reward: 4.276 [3.217, 6.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.232, 10.100], loss: 0.169867, mae: 0.391516, mean_q: 4.490417
 94837/100000: episode: 1825, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 56.959, mean reward: 2.278 [1.778, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.141, 10.281], loss: 0.187845, mae: 0.405663, mean_q: 4.534698
 94847/100000: episode: 1826, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 36.520, mean reward: 3.652 [2.463, 6.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.525, 10.100], loss: 0.269587, mae: 0.398901, mean_q: 4.498198
 94859/100000: episode: 1827, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 37.557, mean reward: 3.130 [2.673, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.325, 10.100], loss: 0.173045, mae: 0.398937, mean_q: 4.447969
 94869/100000: episode: 1828, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 32.705, mean reward: 3.271 [2.546, 5.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.333, 10.100], loss: 0.296594, mae: 0.433783, mean_q: 4.555194
 94879/100000: episode: 1829, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 29.522, mean reward: 2.952 [2.663, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.424, 10.100], loss: 0.175044, mae: 0.390193, mean_q: 4.358763
 94891/100000: episode: 1830, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 37.178, mean reward: 3.098 [2.413, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.542, 10.100], loss: 0.309865, mae: 0.441591, mean_q: 4.640736
 94897/100000: episode: 1831, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 23.280, mean reward: 3.880 [3.193, 4.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.313, 10.100], loss: 0.158928, mae: 0.403208, mean_q: 4.584754
 94909/100000: episode: 1832, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 37.006, mean reward: 3.084 [2.412, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.269, 10.100], loss: 0.477473, mae: 0.455953, mean_q: 4.507354
 94934/100000: episode: 1833, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 90.623, mean reward: 3.625 [2.843, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.020, 10.527], loss: 0.151155, mae: 0.397617, mean_q: 4.547305
 94946/100000: episode: 1834, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 35.160, mean reward: 2.930 [2.636, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.426, 10.100], loss: 0.196487, mae: 0.384079, mean_q: 4.600523
 94958/100000: episode: 1835, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 48.303, mean reward: 4.025 [2.619, 6.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.524, 10.100], loss: 0.274778, mae: 0.430743, mean_q: 4.555074
 94983/100000: episode: 1836, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 86.407, mean reward: 3.456 [2.716, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.462, 10.524], loss: 0.298475, mae: 0.426851, mean_q: 4.558295
 94993/100000: episode: 1837, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 25.229, mean reward: 2.523 [1.958, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.366, 10.100], loss: 0.362726, mae: 0.455994, mean_q: 4.442019
 95000/100000: episode: 1838, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 25.300, mean reward: 3.614 [2.552, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.407, 10.100], loss: 0.107556, mae: 0.350639, mean_q: 4.513147
 95026/100000: episode: 1839, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 78.138, mean reward: 3.005 [2.187, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.463], loss: 0.203459, mae: 0.430492, mean_q: 4.535277
 95035/100000: episode: 1840, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 34.759, mean reward: 3.862 [2.999, 6.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.486, 10.100], loss: 0.176494, mae: 0.425837, mean_q: 4.626432
 95061/100000: episode: 1841, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 71.982, mean reward: 2.769 [1.560, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.187, 10.172], loss: 0.203888, mae: 0.406240, mean_q: 4.590283
 95073/100000: episode: 1842, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 32.341, mean reward: 2.695 [2.241, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.587, 10.456], loss: 0.137473, mae: 0.378908, mean_q: 4.479378
 95080/100000: episode: 1843, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 22.349, mean reward: 3.193 [2.297, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.714, 10.100], loss: 0.121739, mae: 0.338436, mean_q: 4.439282
 95105/100000: episode: 1844, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: 72.491, mean reward: 2.900 [1.508, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.495, 10.251], loss: 0.240465, mae: 0.425995, mean_q: 4.553398
 95130/100000: episode: 1845, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 62.769, mean reward: 2.511 [1.910, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.370], loss: 0.142356, mae: 0.379612, mean_q: 4.557599
 95142/100000: episode: 1846, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 37.116, mean reward: 3.093 [2.547, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.311, 10.100], loss: 0.189337, mae: 0.422193, mean_q: 4.643581
 95149/100000: episode: 1847, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 24.182, mean reward: 3.455 [2.709, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.678, 10.100], loss: 0.154414, mae: 0.359147, mean_q: 4.491951
 95158/100000: episode: 1848, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 39.589, mean reward: 4.399 [2.564, 9.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.421, 10.100], loss: 0.393429, mae: 0.424860, mean_q: 4.578227
 95170/100000: episode: 1849, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 42.972, mean reward: 3.581 [2.448, 6.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.394, 10.100], loss: 0.185598, mae: 0.412865, mean_q: 4.695740
 95185/100000: episode: 1850, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 45.629, mean reward: 3.042 [1.943, 8.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.116, 10.351], loss: 0.303144, mae: 0.481120, mean_q: 4.739511
 95195/100000: episode: 1851, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 29.439, mean reward: 2.944 [2.481, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.273, 10.100], loss: 0.259360, mae: 0.434502, mean_q: 4.502107
 95201/100000: episode: 1852, duration: 0.045s, episode steps: 6, steps per second: 132, episode reward: 35.611, mean reward: 5.935 [4.622, 7.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.355, 10.100], loss: 0.171808, mae: 0.420378, mean_q: 4.812129
 95211/100000: episode: 1853, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 29.473, mean reward: 2.947 [2.542, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.738, 10.100], loss: 0.193535, mae: 0.398562, mean_q: 4.699277
 95223/100000: episode: 1854, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 31.801, mean reward: 2.650 [2.204, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.953, 10.100], loss: 0.214528, mae: 0.450430, mean_q: 4.836146
 95233/100000: episode: 1855, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 27.223, mean reward: 2.722 [2.175, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.286, 10.100], loss: 0.240381, mae: 0.455669, mean_q: 4.638856
 95245/100000: episode: 1856, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 39.700, mean reward: 3.308 [2.758, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.033, 10.100], loss: 0.170958, mae: 0.405424, mean_q: 4.603510
 95270/100000: episode: 1857, duration: 0.142s, episode steps: 25, steps per second: 175, episode reward: 91.401, mean reward: 3.656 [2.954, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.662, 10.528], loss: 0.300790, mae: 0.456406, mean_q: 4.783725
 95285/100000: episode: 1858, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 38.359, mean reward: 2.557 [1.989, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.320], loss: 0.230455, mae: 0.446863, mean_q: 4.738287
 95294/100000: episode: 1859, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 29.261, mean reward: 3.251 [2.818, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.298, 10.100], loss: 0.436606, mae: 0.512118, mean_q: 4.801911
 95306/100000: episode: 1860, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 43.936, mean reward: 3.661 [2.333, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.526, 10.100], loss: 0.283779, mae: 0.493569, mean_q: 4.740401
 95318/100000: episode: 1861, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 33.597, mean reward: 2.800 [2.289, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.418], loss: 0.354355, mae: 0.460026, mean_q: 4.603403
 95344/100000: episode: 1862, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 84.141, mean reward: 3.236 [2.404, 7.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.279, 10.639], loss: 0.274853, mae: 0.430221, mean_q: 4.710167
[Info] 3-TH LEVEL FOUND: 7.613006591796875, Considering 10/90 traces
 95353/100000: episode: 1863, duration: 4.101s, episode steps: 9, steps per second: 2, episode reward: 25.557, mean reward: 2.840 [2.324, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.448, 10.100], loss: 0.203722, mae: 0.435394, mean_q: 4.847323
 95377/100000: episode: 1864, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 70.678, mean reward: 2.945 [2.256, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.202, 10.354], loss: 0.253222, mae: 0.466761, mean_q: 4.662468
 95401/100000: episode: 1865, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 85.364, mean reward: 3.557 [1.881, 6.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.341, 10.340], loss: 0.214889, mae: 0.457884, mean_q: 4.730860
 95404/100000: episode: 1866, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 11.643, mean reward: 3.881 [3.292, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.585, 10.100], loss: 0.204734, mae: 0.480638, mean_q: 4.901420
 95428/100000: episode: 1867, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 68.927, mean reward: 2.872 [2.120, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.043, 10.449], loss: 0.513790, mae: 0.463389, mean_q: 4.826088
 95433/100000: episode: 1868, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 18.512, mean reward: 3.702 [3.134, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.496, 10.100], loss: 0.138665, mae: 0.378614, mean_q: 4.650473
 95437/100000: episode: 1869, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 17.578, mean reward: 4.395 [3.983, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.541, 10.100], loss: 0.106766, mae: 0.341717, mean_q: 4.750794
 95461/100000: episode: 1870, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 72.139, mean reward: 3.006 [2.107, 6.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.394], loss: 0.566593, mae: 0.522760, mean_q: 4.866496
 95482/100000: episode: 1871, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 72.254, mean reward: 3.441 [1.965, 7.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.197, 10.285], loss: 0.280945, mae: 0.447761, mean_q: 4.768843
 95506/100000: episode: 1872, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 69.960, mean reward: 2.915 [1.633, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.997, 10.249], loss: 0.181104, mae: 0.395439, mean_q: 4.715456
 95530/100000: episode: 1873, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 92.395, mean reward: 3.850 [2.723, 6.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.342, 10.546], loss: 0.309650, mae: 0.447556, mean_q: 4.818297
 95554/100000: episode: 1874, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 97.032, mean reward: 4.043 [2.990, 6.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.186, 10.576], loss: 0.334931, mae: 0.473536, mean_q: 4.905447
 95558/100000: episode: 1875, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 14.535, mean reward: 3.634 [2.846, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.535, 10.100], loss: 0.406618, mae: 0.497136, mean_q: 4.605035
 95562/100000: episode: 1876, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 15.734, mean reward: 3.934 [3.723, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.547, 10.100], loss: 0.139082, mae: 0.396149, mean_q: 4.627598
 95586/100000: episode: 1877, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 74.955, mean reward: 3.123 [2.119, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.670, 10.465], loss: 0.268155, mae: 0.472773, mean_q: 4.887930
 95590/100000: episode: 1878, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 16.116, mean reward: 4.029 [3.863, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.462, 10.100], loss: 0.392499, mae: 0.591217, mean_q: 5.176972
 95614/100000: episode: 1879, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 121.424, mean reward: 5.059 [3.518, 11.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.428, 10.561], loss: 0.203887, mae: 0.415332, mean_q: 4.913453
 95618/100000: episode: 1880, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 15.909, mean reward: 3.977 [3.599, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.537, 10.100], loss: 0.233278, mae: 0.467079, mean_q: 4.638394
 95642/100000: episode: 1881, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 80.397, mean reward: 3.350 [2.442, 4.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.074, 10.396], loss: 0.577130, mae: 0.506815, mean_q: 5.016837
 95666/100000: episode: 1882, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 78.605, mean reward: 3.275 [2.213, 6.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.509, 10.463], loss: 0.934098, mae: 0.623721, mean_q: 5.006747
 95671/100000: episode: 1883, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 17.714, mean reward: 3.543 [3.250, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.395, 10.100], loss: 0.427353, mae: 0.592815, mean_q: 4.812295
 95694/100000: episode: 1884, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 62.342, mean reward: 2.711 [1.712, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.117, 10.275], loss: 0.267876, mae: 0.452760, mean_q: 4.890831
 95715/100000: episode: 1885, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 88.494, mean reward: 4.214 [2.514, 7.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.445], loss: 0.299327, mae: 0.466033, mean_q: 4.928697
 95736/100000: episode: 1886, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 106.990, mean reward: 5.095 [3.566, 7.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.767, 10.513], loss: 0.291630, mae: 0.471345, mean_q: 5.013646
 95740/100000: episode: 1887, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 11.396, mean reward: 2.849 [2.558, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.502, 10.100], loss: 0.236573, mae: 0.500489, mean_q: 4.955638
 95743/100000: episode: 1888, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.800, mean reward: 2.933 [2.775, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.478, 10.100], loss: 0.509010, mae: 0.499906, mean_q: 4.704175
 95765/100000: episode: 1889, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 108.704, mean reward: 4.941 [3.358, 7.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.614], loss: 0.248680, mae: 0.462434, mean_q: 4.933271
 95789/100000: episode: 1890, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 74.630, mean reward: 3.110 [2.186, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.091, 10.434], loss: 0.237225, mae: 0.471640, mean_q: 4.991627
 95792/100000: episode: 1891, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 65.950, mean reward: 21.983 [3.212, 57.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.512, 10.100], loss: 0.479843, mae: 0.626325, mean_q: 5.103135
 95815/100000: episode: 1892, duration: 0.136s, episode steps: 23, steps per second: 170, episode reward: 98.319, mean reward: 4.275 [3.198, 7.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.483, 10.499], loss: 0.242330, mae: 0.444587, mean_q: 4.972906
 95839/100000: episode: 1893, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 62.757, mean reward: 2.615 [1.646, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.302], loss: 0.257550, mae: 0.465037, mean_q: 4.980988
 95843/100000: episode: 1894, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 13.158, mean reward: 3.290 [2.901, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.473, 10.100], loss: 0.197107, mae: 0.431242, mean_q: 4.903333
 95867/100000: episode: 1895, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 71.239, mean reward: 2.968 [1.991, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.223, 10.324], loss: 0.401751, mae: 0.520813, mean_q: 5.001286
 95891/100000: episode: 1896, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 70.049, mean reward: 2.919 [1.870, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.053, 10.330], loss: 0.262485, mae: 0.462091, mean_q: 4.950414
 95915/100000: episode: 1897, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 72.459, mean reward: 3.019 [2.308, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.347, 10.326], loss: 0.248248, mae: 0.478045, mean_q: 5.040286
 95937/100000: episode: 1898, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 80.120, mean reward: 3.642 [2.731, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.542, 10.528], loss: 0.371433, mae: 0.507573, mean_q: 5.017094
 95942/100000: episode: 1899, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 20.566, mean reward: 4.113 [3.243, 6.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.289, 10.100], loss: 0.333790, mae: 0.467268, mean_q: 4.866435
 95964/100000: episode: 1900, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 70.625, mean reward: 3.210 [2.099, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.711, 10.426], loss: 0.284317, mae: 0.479088, mean_q: 5.042104
 95988/100000: episode: 1901, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 91.792, mean reward: 3.825 [1.971, 6.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.062, 10.301], loss: 3.656554, mae: 0.617358, mean_q: 5.217295
 95992/100000: episode: 1902, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 14.331, mean reward: 3.583 [3.004, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.515, 10.100], loss: 1.174842, mae: 1.017800, mean_q: 5.527081
 96016/100000: episode: 1903, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 56.411, mean reward: 2.350 [1.899, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.274, 10.385], loss: 0.480113, mae: 0.634768, mean_q: 5.092235
 96038/100000: episode: 1904, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 64.629, mean reward: 2.938 [1.820, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.428, 10.321], loss: 2.205353, mae: 0.634532, mean_q: 5.206571
 96042/100000: episode: 1905, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 14.203, mean reward: 3.551 [3.438, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.473, 10.100], loss: 0.441851, mae: 0.581671, mean_q: 4.829906
 96047/100000: episode: 1906, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 17.858, mean reward: 3.572 [2.934, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.539, 10.100], loss: 0.382555, mae: 0.526960, mean_q: 5.092763
 96068/100000: episode: 1907, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 72.583, mean reward: 3.456 [2.479, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.270, 10.419], loss: 0.350609, mae: 0.556060, mean_q: 5.098734
 96089/100000: episode: 1908, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 78.446, mean reward: 3.736 [2.613, 6.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.132, 10.547], loss: 0.487613, mae: 0.593718, mean_q: 5.132070
 96093/100000: episode: 1909, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 21.492, mean reward: 5.373 [3.396, 9.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.461, 10.100], loss: 0.266084, mae: 0.492401, mean_q: 5.481380
 96096/100000: episode: 1910, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 8.612, mean reward: 2.871 [2.748, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.479, 10.100], loss: 0.248499, mae: 0.479528, mean_q: 5.224228
 96120/100000: episode: 1911, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 104.348, mean reward: 4.348 [2.928, 10.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.534], loss: 0.443067, mae: 0.574300, mean_q: 5.097996
 96144/100000: episode: 1912, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 73.904, mean reward: 3.079 [1.928, 5.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.117, 10.359], loss: 2.174376, mae: 0.670762, mean_q: 5.171236
 96166/100000: episode: 1913, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 70.824, mean reward: 3.219 [2.629, 4.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.148, 10.481], loss: 0.824241, mae: 0.660632, mean_q: 5.219655
 96171/100000: episode: 1914, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 22.529, mean reward: 4.506 [3.926, 5.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.468, 10.100], loss: 0.439993, mae: 0.576244, mean_q: 4.933871
 96176/100000: episode: 1915, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 15.968, mean reward: 3.194 [2.867, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.491, 10.100], loss: 0.514112, mae: 0.665210, mean_q: 5.082961
 96181/100000: episode: 1916, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 21.381, mean reward: 4.276 [3.920, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.359, 10.100], loss: 0.314059, mae: 0.548326, mean_q: 5.158015
 96205/100000: episode: 1917, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 69.554, mean reward: 2.898 [1.733, 8.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.071, 10.236], loss: 0.500163, mae: 0.597874, mean_q: 5.257919
[Info] FALSIFICATION!
[Info] Levels: [5.191329, 6.2889132, 7.6130066, 8.933628]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.05]
[Info] Error Prob: 5.0000000000000016e-05

 96220/100000: episode: 1918, duration: 4.319s, episode steps: 15, steps per second: 3, episode reward: 190.924, mean reward: 12.728 [2.938, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.467, 10.123], loss: 0.367806, mae: 0.566030, mean_q: 5.271722
 96320/100000: episode: 1919, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 195.112, mean reward: 1.951 [1.456, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.704, 10.098], loss: 0.966027, mae: 0.596125, mean_q: 5.195942
 96420/100000: episode: 1920, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 191.484, mean reward: 1.915 [1.454, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.098], loss: 0.470637, mae: 0.587639, mean_q: 5.117849
 96520/100000: episode: 1921, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.554, mean reward: 1.886 [1.434, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.128, 10.098], loss: 0.437099, mae: 0.567346, mean_q: 5.162212
 96620/100000: episode: 1922, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 191.761, mean reward: 1.918 [1.479, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.149], loss: 0.964839, mae: 0.612423, mean_q: 5.129910
 96720/100000: episode: 1923, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 204.325, mean reward: 2.043 [1.507, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.209, 10.098], loss: 0.543990, mae: 0.572981, mean_q: 5.152599
 96820/100000: episode: 1924, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 205.572, mean reward: 2.056 [1.458, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.400, 10.167], loss: 0.822971, mae: 0.577395, mean_q: 5.127391
 96920/100000: episode: 1925, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 207.979, mean reward: 2.080 [1.483, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.126, 10.282], loss: 0.779140, mae: 0.570418, mean_q: 5.137880
 97020/100000: episode: 1926, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.660, mean reward: 1.847 [1.450, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.363, 10.102], loss: 2.223726, mae: 0.669490, mean_q: 5.090774
 97120/100000: episode: 1927, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.768, mean reward: 1.898 [1.479, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.438, 10.232], loss: 2.263422, mae: 0.655248, mean_q: 5.169684
 97220/100000: episode: 1928, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.767, mean reward: 1.828 [1.463, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.922, 10.261], loss: 3.205754, mae: 0.680317, mean_q: 5.110506
 97320/100000: episode: 1929, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 197.610, mean reward: 1.976 [1.473, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.062, 10.098], loss: 3.607636, mae: 0.768125, mean_q: 5.162977
 97420/100000: episode: 1930, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.010, mean reward: 1.920 [1.494, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.886, 10.268], loss: 1.077117, mae: 0.617911, mean_q: 5.141055
 97520/100000: episode: 1931, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 215.629, mean reward: 2.156 [1.435, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.963, 10.164], loss: 2.501005, mae: 0.638539, mean_q: 5.067301
 97620/100000: episode: 1932, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.388, mean reward: 1.974 [1.496, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.322, 10.098], loss: 0.432611, mae: 0.555080, mean_q: 5.102339
 97720/100000: episode: 1933, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 207.530, mean reward: 2.075 [1.471, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.799, 10.315], loss: 2.130600, mae: 0.613040, mean_q: 5.073602
 97820/100000: episode: 1934, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.186, mean reward: 1.922 [1.467, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.098], loss: 2.087177, mae: 0.614803, mean_q: 5.058377
 97920/100000: episode: 1935, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 180.206, mean reward: 1.802 [1.437, 2.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.746, 10.098], loss: 0.489315, mae: 0.551174, mean_q: 5.036253
 98020/100000: episode: 1936, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 182.930, mean reward: 1.829 [1.449, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.510, 10.285], loss: 0.384586, mae: 0.528428, mean_q: 5.030838
 98120/100000: episode: 1937, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 203.635, mean reward: 2.036 [1.440, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.377, 10.101], loss: 0.796264, mae: 0.534129, mean_q: 5.060346
 98220/100000: episode: 1938, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 200.080, mean reward: 2.001 [1.458, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.555, 10.161], loss: 1.717731, mae: 0.603723, mean_q: 4.990895
 98320/100000: episode: 1939, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.208, mean reward: 1.892 [1.470, 2.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.812, 10.333], loss: 1.709367, mae: 0.581629, mean_q: 5.051051
 98420/100000: episode: 1940, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.213, mean reward: 1.872 [1.467, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.137, 10.098], loss: 1.617698, mae: 0.553836, mean_q: 5.045115
 98520/100000: episode: 1941, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 194.206, mean reward: 1.942 [1.452, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.512, 10.098], loss: 0.815320, mae: 0.549838, mean_q: 4.975921
 98620/100000: episode: 1942, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 216.730, mean reward: 2.167 [1.525, 6.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.394, 10.098], loss: 0.851785, mae: 0.542865, mean_q: 5.024365
 98720/100000: episode: 1943, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 186.953, mean reward: 1.870 [1.451, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.760, 10.098], loss: 2.238554, mae: 0.611856, mean_q: 5.065576
 98820/100000: episode: 1944, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 181.303, mean reward: 1.813 [1.454, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.479, 10.098], loss: 2.155285, mae: 0.610088, mean_q: 5.044429
 98920/100000: episode: 1945, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 227.655, mean reward: 2.277 [1.491, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.762, 10.098], loss: 4.385982, mae: 0.746742, mean_q: 5.065377
 99020/100000: episode: 1946, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 181.636, mean reward: 1.816 [1.438, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.487, 10.229], loss: 0.861819, mae: 0.539550, mean_q: 4.939973
 99120/100000: episode: 1947, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.458, mean reward: 1.885 [1.498, 2.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.071, 10.098], loss: 1.606441, mae: 0.536688, mean_q: 4.855169
 99220/100000: episode: 1948, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.093, mean reward: 1.951 [1.497, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.364, 10.098], loss: 0.381136, mae: 0.498569, mean_q: 4.840392
 99320/100000: episode: 1949, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.210, mean reward: 1.972 [1.465, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.098], loss: 3.234560, mae: 0.618882, mean_q: 4.818503
 99420/100000: episode: 1950, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 195.840, mean reward: 1.958 [1.456, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.818, 10.098], loss: 0.683997, mae: 0.463696, mean_q: 4.728287
 99520/100000: episode: 1951, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 181.040, mean reward: 1.810 [1.446, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.732, 10.131], loss: 0.325418, mae: 0.457908, mean_q: 4.713756
 99620/100000: episode: 1952, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.815, mean reward: 1.968 [1.454, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.250, 10.156], loss: 0.265549, mae: 0.438994, mean_q: 4.647332
 99720/100000: episode: 1953, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 209.810, mean reward: 2.098 [1.440, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.786, 10.292], loss: 2.393811, mae: 0.552192, mean_q: 4.629177
 99820/100000: episode: 1954, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 183.723, mean reward: 1.837 [1.453, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.691, 10.098], loss: 1.523669, mae: 0.488062, mean_q: 4.592203
 99920/100000: episode: 1955, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 215.559, mean reward: 2.156 [1.441, 10.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.631, 10.098], loss: 0.239047, mae: 0.410412, mean_q: 4.529435
done, took 616.916 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
