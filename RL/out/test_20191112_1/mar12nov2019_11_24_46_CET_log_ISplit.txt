Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.172s, episode steps: 100, steps per second: 583, episode reward: 190.242, mean reward: 1.902 [1.466, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.769, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1592, episode reward: 195.376, mean reward: 1.954 [1.465, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.313, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 194.310, mean reward: 1.943 [1.513, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.559, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 185.616, mean reward: 1.856 [1.465, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.280, 10.195], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.068s, episode steps: 100, steps per second: 1475, episode reward: 187.825, mean reward: 1.878 [1.448, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.332], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.067s, episode steps: 100, steps per second: 1502, episode reward: 196.071, mean reward: 1.961 [1.436, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.575, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 191.888, mean reward: 1.919 [1.451, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.069s, episode steps: 100, steps per second: 1442, episode reward: 184.302, mean reward: 1.843 [1.478, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.337, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 189.557, mean reward: 1.896 [1.474, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.030, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 194.833, mean reward: 1.948 [1.433, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.865, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.068s, episode steps: 100, steps per second: 1471, episode reward: 183.875, mean reward: 1.839 [1.493, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.259, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 212.710, mean reward: 2.127 [1.466, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.212, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 179.990, mean reward: 1.800 [1.448, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.148], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.069s, episode steps: 100, steps per second: 1452, episode reward: 187.996, mean reward: 1.880 [1.469, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.261, 10.377], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 191.018, mean reward: 1.910 [1.506, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.163], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.073s, episode steps: 100, steps per second: 1371, episode reward: 190.614, mean reward: 1.906 [1.448, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.082, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.071s, episode steps: 100, steps per second: 1402, episode reward: 189.350, mean reward: 1.894 [1.461, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.949, 10.321], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.069s, episode steps: 100, steps per second: 1453, episode reward: 185.293, mean reward: 1.853 [1.454, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.338, 10.233], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 182.368, mean reward: 1.824 [1.462, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.804, 10.180], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 209.248, mean reward: 2.092 [1.480, 7.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.453, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 202.843, mean reward: 2.028 [1.450, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.120], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 197.727, mean reward: 1.977 [1.429, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.295], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.066s, episode steps: 100, steps per second: 1521, episode reward: 185.899, mean reward: 1.859 [1.441, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.968, 10.100], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 191.720, mean reward: 1.917 [1.460, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.031, 10.295], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 221.147, mean reward: 2.211 [1.459, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.600, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.073s, episode steps: 100, steps per second: 1376, episode reward: 200.102, mean reward: 2.001 [1.496, 5.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.697, 10.153], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 195.043, mean reward: 1.950 [1.441, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.266, 10.106], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 185.271, mean reward: 1.853 [1.461, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.061, 10.144], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 192.587, mean reward: 1.926 [1.497, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.176], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 187.820, mean reward: 1.878 [1.439, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.495, 10.135], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 199.880, mean reward: 1.999 [1.443, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.646, 10.108], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 212.281, mean reward: 2.123 [1.471, 10.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.567, 10.489], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 196.233, mean reward: 1.962 [1.505, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 187.911, mean reward: 1.879 [1.469, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.509, 10.193], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 184.681, mean reward: 1.847 [1.481, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.575, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 185.983, mean reward: 1.860 [1.464, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.371, 10.144], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 193.246, mean reward: 1.932 [1.444, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.134, 10.150], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 223.425, mean reward: 2.234 [1.451, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.311], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 193.785, mean reward: 1.938 [1.482, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.616, 10.267], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: 189.145, mean reward: 1.891 [1.442, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.740, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 197.950, mean reward: 1.979 [1.520, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.012, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 198.077, mean reward: 1.981 [1.472, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.872, 10.311], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.083s, episode steps: 100, steps per second: 1212, episode reward: 204.124, mean reward: 2.041 [1.486, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.069s, episode steps: 100, steps per second: 1441, episode reward: 190.530, mean reward: 1.905 [1.449, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.086, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 182.888, mean reward: 1.829 [1.485, 2.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.990, 10.166], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 181.270, mean reward: 1.813 [1.446, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.425, 10.117], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 197.323, mean reward: 1.973 [1.500, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.968, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.075s, episode steps: 100, steps per second: 1333, episode reward: 196.976, mean reward: 1.970 [1.441, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.274, 10.135], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.071s, episode steps: 100, steps per second: 1405, episode reward: 190.502, mean reward: 1.905 [1.472, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.843, 10.204], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 182.516, mean reward: 1.825 [1.434, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.296, 10.138], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 217.351, mean reward: 2.174 [1.454, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.437, 10.098], loss: 0.165169, mae: 0.393981, mean_q: 2.804608
  5200/100000: episode: 52, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 197.254, mean reward: 1.973 [1.445, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.532, 10.420], loss: 0.099977, mae: 0.311150, mean_q: 3.234363
  5300/100000: episode: 53, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.984, mean reward: 1.840 [1.457, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.685, 10.098], loss: 0.132034, mae: 0.327557, mean_q: 3.464697
  5400/100000: episode: 54, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 193.419, mean reward: 1.934 [1.448, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.130], loss: 0.129457, mae: 0.328922, mean_q: 3.605335
  5500/100000: episode: 55, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.278, mean reward: 1.903 [1.475, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.626, 10.261], loss: 0.133023, mae: 0.327689, mean_q: 3.695989
  5600/100000: episode: 56, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 206.449, mean reward: 2.064 [1.519, 5.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.574, 10.098], loss: 0.129385, mae: 0.327922, mean_q: 3.746071
  5700/100000: episode: 57, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 259.503, mean reward: 2.595 [1.477, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.598, 10.484], loss: 0.104558, mae: 0.307500, mean_q: 3.773159
  5800/100000: episode: 58, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 184.961, mean reward: 1.850 [1.488, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.098], loss: 0.154798, mae: 0.342781, mean_q: 3.809124
  5900/100000: episode: 59, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 192.736, mean reward: 1.927 [1.506, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.420, 10.273], loss: 0.125639, mae: 0.323127, mean_q: 3.837932
  6000/100000: episode: 60, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 190.962, mean reward: 1.910 [1.443, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.479, 10.098], loss: 0.126913, mae: 0.330213, mean_q: 3.864196
  6100/100000: episode: 61, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.926, mean reward: 1.849 [1.467, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.559, 10.144], loss: 0.152290, mae: 0.348409, mean_q: 3.879190
  6200/100000: episode: 62, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 200.236, mean reward: 2.002 [1.440, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.176, 10.164], loss: 0.124576, mae: 0.332673, mean_q: 3.872014
  6300/100000: episode: 63, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 200.645, mean reward: 2.006 [1.455, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.107, 10.098], loss: 0.146053, mae: 0.344449, mean_q: 3.882048
  6400/100000: episode: 64, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 213.583, mean reward: 2.136 [1.467, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.126, 10.098], loss: 0.115060, mae: 0.324006, mean_q: 3.865894
  6500/100000: episode: 65, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.306, mean reward: 1.863 [1.459, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.820, 10.098], loss: 0.110465, mae: 0.310225, mean_q: 3.860925
  6600/100000: episode: 66, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 198.715, mean reward: 1.987 [1.463, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.176, 10.098], loss: 0.118336, mae: 0.330517, mean_q: 3.870362
  6700/100000: episode: 67, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 185.213, mean reward: 1.852 [1.449, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.659, 10.190], loss: 0.131099, mae: 0.340059, mean_q: 3.892423
  6800/100000: episode: 68, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.260, mean reward: 1.963 [1.494, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.459, 10.098], loss: 0.125093, mae: 0.336208, mean_q: 3.881311
  6900/100000: episode: 69, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.134, mean reward: 1.951 [1.475, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.303], loss: 0.135812, mae: 0.341066, mean_q: 3.902711
  7000/100000: episode: 70, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.873, mean reward: 1.889 [1.497, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.695, 10.275], loss: 0.132488, mae: 0.343281, mean_q: 3.905910
  7100/100000: episode: 71, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 190.816, mean reward: 1.908 [1.435, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.255, 10.098], loss: 0.118602, mae: 0.320382, mean_q: 3.874668
  7200/100000: episode: 72, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 202.070, mean reward: 2.021 [1.517, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.257, 10.480], loss: 0.130448, mae: 0.334560, mean_q: 3.891482
  7300/100000: episode: 73, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 183.853, mean reward: 1.839 [1.430, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.355, 10.199], loss: 0.139643, mae: 0.337027, mean_q: 3.880633
  7400/100000: episode: 74, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 177.740, mean reward: 1.777 [1.484, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.673, 10.098], loss: 0.122581, mae: 0.334928, mean_q: 3.882047
  7500/100000: episode: 75, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.149, mean reward: 1.841 [1.439, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.414, 10.162], loss: 0.120951, mae: 0.323450, mean_q: 3.868157
  7600/100000: episode: 76, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.120, mean reward: 1.901 [1.467, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.522, 10.098], loss: 0.128276, mae: 0.332185, mean_q: 3.861728
  7700/100000: episode: 77, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 207.143, mean reward: 2.071 [1.466, 5.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.997, 10.098], loss: 0.135676, mae: 0.331059, mean_q: 3.868273
  7800/100000: episode: 78, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 203.618, mean reward: 2.036 [1.456, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.098], loss: 0.121713, mae: 0.326115, mean_q: 3.871495
  7900/100000: episode: 79, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 181.909, mean reward: 1.819 [1.471, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.121828, mae: 0.336842, mean_q: 3.878474
  8000/100000: episode: 80, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 187.846, mean reward: 1.878 [1.454, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.402, 10.098], loss: 0.108654, mae: 0.307253, mean_q: 3.852424
  8100/100000: episode: 81, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.427, mean reward: 1.884 [1.441, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.444, 10.098], loss: 0.128898, mae: 0.328087, mean_q: 3.865092
  8200/100000: episode: 82, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.814, mean reward: 1.958 [1.510, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.436], loss: 0.106999, mae: 0.309529, mean_q: 3.845078
  8300/100000: episode: 83, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 216.707, mean reward: 2.167 [1.587, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.387], loss: 0.123720, mae: 0.322650, mean_q: 3.867765
  8400/100000: episode: 84, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 223.936, mean reward: 2.239 [1.445, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.631, 10.123], loss: 0.114798, mae: 0.319416, mean_q: 3.853974
  8500/100000: episode: 85, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 188.378, mean reward: 1.884 [1.459, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.170, 10.098], loss: 0.133515, mae: 0.336166, mean_q: 3.875494
  8600/100000: episode: 86, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 181.825, mean reward: 1.818 [1.445, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.986, 10.098], loss: 0.114771, mae: 0.325512, mean_q: 3.861620
  8700/100000: episode: 87, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 201.419, mean reward: 2.014 [1.446, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.474, 10.345], loss: 0.133640, mae: 0.343152, mean_q: 3.891333
  8800/100000: episode: 88, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.442, mean reward: 1.864 [1.441, 2.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.837, 10.344], loss: 0.105973, mae: 0.314241, mean_q: 3.851772
  8900/100000: episode: 89, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 210.647, mean reward: 2.106 [1.470, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.232, 10.335], loss: 0.103135, mae: 0.304935, mean_q: 3.835555
  9000/100000: episode: 90, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 220.698, mean reward: 2.207 [1.450, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.374, 10.386], loss: 0.119577, mae: 0.334807, mean_q: 3.871188
  9100/100000: episode: 91, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 199.647, mean reward: 1.996 [1.531, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.996, 10.098], loss: 0.114439, mae: 0.328194, mean_q: 3.878256
  9200/100000: episode: 92, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 198.238, mean reward: 1.982 [1.485, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.828, 10.098], loss: 0.123679, mae: 0.334602, mean_q: 3.881516
  9300/100000: episode: 93, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.806, mean reward: 1.888 [1.484, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.360, 10.339], loss: 0.110222, mae: 0.315147, mean_q: 3.866621
  9400/100000: episode: 94, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 199.785, mean reward: 1.998 [1.477, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.463, 10.187], loss: 0.107773, mae: 0.318875, mean_q: 3.857968
  9500/100000: episode: 95, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 180.842, mean reward: 1.808 [1.433, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.293, 10.098], loss: 0.128027, mae: 0.337429, mean_q: 3.883783
  9600/100000: episode: 96, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 190.670, mean reward: 1.907 [1.469, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.639, 10.098], loss: 0.106669, mae: 0.311832, mean_q: 3.878355
  9700/100000: episode: 97, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 177.766, mean reward: 1.778 [1.470, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.262], loss: 0.105496, mae: 0.314759, mean_q: 3.854071
  9800/100000: episode: 98, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.550, mean reward: 1.916 [1.444, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.347], loss: 0.108114, mae: 0.323552, mean_q: 3.866694
  9900/100000: episode: 99, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.778, mean reward: 1.908 [1.458, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.007, 10.142], loss: 0.110518, mae: 0.315869, mean_q: 3.851099
 10000/100000: episode: 100, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 211.744, mean reward: 2.117 [1.566, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.515, 10.392], loss: 0.106431, mae: 0.312309, mean_q: 3.871811
 10100/100000: episode: 101, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 188.050, mean reward: 1.881 [1.463, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.405, 10.098], loss: 0.099775, mae: 0.309950, mean_q: 3.853318
 10200/100000: episode: 102, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 199.014, mean reward: 1.990 [1.495, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.968, 10.098], loss: 0.111300, mae: 0.320388, mean_q: 3.880314
 10300/100000: episode: 103, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.508, mean reward: 1.925 [1.503, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.015, 10.098], loss: 0.092942, mae: 0.304103, mean_q: 3.868189
 10400/100000: episode: 104, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 182.918, mean reward: 1.829 [1.451, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.448, 10.170], loss: 0.113323, mae: 0.315814, mean_q: 3.863230
 10500/100000: episode: 105, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 183.140, mean reward: 1.831 [1.455, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.098], loss: 0.096409, mae: 0.314358, mean_q: 3.882523
 10600/100000: episode: 106, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 227.411, mean reward: 2.274 [1.507, 6.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.590, 10.098], loss: 0.112660, mae: 0.321221, mean_q: 3.884563
 10700/100000: episode: 107, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.724, mean reward: 1.887 [1.467, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.681, 10.098], loss: 0.083344, mae: 0.287511, mean_q: 3.857202
 10800/100000: episode: 108, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 212.378, mean reward: 2.124 [1.462, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.749, 10.325], loss: 0.107937, mae: 0.312905, mean_q: 3.847677
 10900/100000: episode: 109, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.852, mean reward: 1.949 [1.474, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.701, 10.098], loss: 0.104998, mae: 0.309402, mean_q: 3.872379
 11000/100000: episode: 110, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 179.897, mean reward: 1.799 [1.436, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.390, 10.159], loss: 0.114173, mae: 0.325805, mean_q: 3.894984
 11100/100000: episode: 111, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 204.464, mean reward: 2.045 [1.504, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.448, 10.098], loss: 0.104499, mae: 0.305685, mean_q: 3.857234
 11200/100000: episode: 112, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 199.990, mean reward: 2.000 [1.477, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.061, 10.390], loss: 0.105847, mae: 0.317136, mean_q: 3.885002
 11300/100000: episode: 113, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.743, mean reward: 1.917 [1.528, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.214, 10.183], loss: 0.105313, mae: 0.316949, mean_q: 3.880759
 11400/100000: episode: 114, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 172.663, mean reward: 1.727 [1.452, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.061, 10.225], loss: 0.096274, mae: 0.302483, mean_q: 3.854002
 11500/100000: episode: 115, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 205.366, mean reward: 2.054 [1.442, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.609, 10.315], loss: 0.103439, mae: 0.313334, mean_q: 3.860871
 11600/100000: episode: 116, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 185.912, mean reward: 1.859 [1.478, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.429, 10.098], loss: 0.094378, mae: 0.302151, mean_q: 3.846574
 11700/100000: episode: 117, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.925, mean reward: 1.859 [1.448, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.732, 10.159], loss: 0.091923, mae: 0.307328, mean_q: 3.857976
 11800/100000: episode: 118, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.155, mean reward: 1.962 [1.505, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.858, 10.248], loss: 0.091270, mae: 0.302514, mean_q: 3.833726
 11900/100000: episode: 119, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 195.056, mean reward: 1.951 [1.446, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.045, 10.344], loss: 0.090824, mae: 0.292252, mean_q: 3.846152
 12000/100000: episode: 120, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 204.567, mean reward: 2.046 [1.498, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.351, 10.327], loss: 0.104903, mae: 0.305536, mean_q: 3.865555
 12100/100000: episode: 121, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.234, mean reward: 1.832 [1.505, 2.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.991, 10.254], loss: 0.096437, mae: 0.307342, mean_q: 3.850295
 12200/100000: episode: 122, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 189.262, mean reward: 1.893 [1.469, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.883, 10.098], loss: 0.093329, mae: 0.302578, mean_q: 3.832527
 12300/100000: episode: 123, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.743, mean reward: 1.807 [1.450, 2.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.229, 10.111], loss: 0.094201, mae: 0.300293, mean_q: 3.843670
 12400/100000: episode: 124, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.587, mean reward: 1.876 [1.433, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.530, 10.098], loss: 0.089939, mae: 0.298218, mean_q: 3.832837
 12500/100000: episode: 125, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 177.640, mean reward: 1.776 [1.443, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.691, 10.180], loss: 0.101450, mae: 0.309580, mean_q: 3.850775
 12600/100000: episode: 126, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 199.387, mean reward: 1.994 [1.456, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.684, 10.098], loss: 0.094549, mae: 0.303475, mean_q: 3.843887
 12700/100000: episode: 127, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 193.750, mean reward: 1.938 [1.464, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.231, 10.098], loss: 0.093063, mae: 0.303578, mean_q: 3.850666
 12800/100000: episode: 128, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.490, mean reward: 1.915 [1.447, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.492, 10.098], loss: 0.095251, mae: 0.301004, mean_q: 3.858567
 12900/100000: episode: 129, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.179, mean reward: 1.992 [1.450, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.195, 10.098], loss: 0.092596, mae: 0.294907, mean_q: 3.842840
 13000/100000: episode: 130, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 175.560, mean reward: 1.756 [1.469, 2.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.868, 10.098], loss: 0.100633, mae: 0.313539, mean_q: 3.864421
 13100/100000: episode: 131, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 206.540, mean reward: 2.065 [1.447, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.508, 10.305], loss: 0.093728, mae: 0.305857, mean_q: 3.849659
 13200/100000: episode: 132, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.370, mean reward: 1.974 [1.446, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.098], loss: 0.088251, mae: 0.296495, mean_q: 3.845176
 13300/100000: episode: 133, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 181.825, mean reward: 1.818 [1.458, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.098], loss: 0.098577, mae: 0.307886, mean_q: 3.860413
 13400/100000: episode: 134, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.004, mean reward: 1.910 [1.501, 4.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.596, 10.322], loss: 0.081801, mae: 0.282790, mean_q: 3.829820
 13500/100000: episode: 135, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.661, mean reward: 1.957 [1.451, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.348, 10.098], loss: 0.086709, mae: 0.291548, mean_q: 3.815415
 13600/100000: episode: 136, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 192.804, mean reward: 1.928 [1.456, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.692, 10.098], loss: 0.093273, mae: 0.298327, mean_q: 3.842354
 13700/100000: episode: 137, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 178.744, mean reward: 1.787 [1.473, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.140], loss: 0.080371, mae: 0.285051, mean_q: 3.820820
 13800/100000: episode: 138, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 211.634, mean reward: 2.116 [1.468, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.194], loss: 0.080018, mae: 0.283415, mean_q: 3.815017
 13900/100000: episode: 139, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 194.393, mean reward: 1.944 [1.535, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.290], loss: 0.085620, mae: 0.289335, mean_q: 3.821818
 14000/100000: episode: 140, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 192.226, mean reward: 1.922 [1.447, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.968, 10.098], loss: 0.082911, mae: 0.287213, mean_q: 3.819100
 14100/100000: episode: 141, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.085, mean reward: 1.891 [1.493, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.491, 10.214], loss: 0.085368, mae: 0.290239, mean_q: 3.833472
 14200/100000: episode: 142, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.304, mean reward: 1.913 [1.463, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.552, 10.128], loss: 0.074334, mae: 0.271798, mean_q: 3.789872
 14300/100000: episode: 143, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 189.365, mean reward: 1.894 [1.437, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.074, 10.229], loss: 0.080988, mae: 0.280915, mean_q: 3.803970
 14400/100000: episode: 144, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.959, mean reward: 1.940 [1.465, 7.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.948, 10.098], loss: 0.092598, mae: 0.297458, mean_q: 3.816002
 14500/100000: episode: 145, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 190.660, mean reward: 1.907 [1.495, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.841, 10.286], loss: 0.095778, mae: 0.293154, mean_q: 3.811166
 14600/100000: episode: 146, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 195.381, mean reward: 1.954 [1.484, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.462, 10.185], loss: 0.088015, mae: 0.283475, mean_q: 3.814258
 14700/100000: episode: 147, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 198.520, mean reward: 1.985 [1.501, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.982, 10.098], loss: 0.088169, mae: 0.287558, mean_q: 3.822545
 14800/100000: episode: 148, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 187.604, mean reward: 1.876 [1.459, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.159], loss: 0.079515, mae: 0.284792, mean_q: 3.813053
 14900/100000: episode: 149, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 198.236, mean reward: 1.982 [1.469, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.877, 10.098], loss: 0.086145, mae: 0.282414, mean_q: 3.805553
[Info] 1-TH LEVEL FOUND: 4.13876485824585, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.119s, episode steps: 100, steps per second: 20, episode reward: 186.436, mean reward: 1.864 [1.452, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.695, 10.146], loss: 0.083851, mae: 0.279828, mean_q: 3.810238
 15018/100000: episode: 151, duration: 0.128s, episode steps: 18, steps per second: 140, episode reward: 48.597, mean reward: 2.700 [2.023, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.718, 10.100], loss: 0.079290, mae: 0.290927, mean_q: 3.831651
 15035/100000: episode: 152, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 37.515, mean reward: 2.207 [1.444, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.275, 10.100], loss: 0.115936, mae: 0.298959, mean_q: 3.804755
 15055/100000: episode: 153, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 40.562, mean reward: 2.028 [1.616, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.478, 10.100], loss: 0.100835, mae: 0.311058, mean_q: 3.851224
 15072/100000: episode: 154, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 33.707, mean reward: 1.983 [1.599, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.289, 10.100], loss: 0.121098, mae: 0.323944, mean_q: 3.860114
 15088/100000: episode: 155, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 29.605, mean reward: 1.850 [1.552, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.156, 10.177], loss: 0.114311, mae: 0.313942, mean_q: 3.847670
 15104/100000: episode: 156, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 30.164, mean reward: 1.885 [1.579, 2.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.899, 10.164], loss: 0.083841, mae: 0.284051, mean_q: 3.793550
 15124/100000: episode: 157, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 38.470, mean reward: 1.923 [1.537, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.269, 10.100], loss: 0.079292, mae: 0.286297, mean_q: 3.831679
 15141/100000: episode: 158, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 33.116, mean reward: 1.948 [1.569, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.233, 10.100], loss: 0.093583, mae: 0.292975, mean_q: 3.812627
 15157/100000: episode: 159, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 28.195, mean reward: 1.762 [1.488, 2.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.489, 10.100], loss: 0.073948, mae: 0.275981, mean_q: 3.796910
 15174/100000: episode: 160, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 38.058, mean reward: 2.239 [1.594, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.273], loss: 0.134907, mae: 0.327380, mean_q: 3.842623
 15191/100000: episode: 161, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 32.016, mean reward: 1.883 [1.452, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.665, 10.141], loss: 0.078017, mae: 0.285856, mean_q: 3.807526
 15212/100000: episode: 162, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 42.413, mean reward: 2.020 [1.576, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.237], loss: 0.100003, mae: 0.313762, mean_q: 3.805812
 15233/100000: episode: 163, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 38.253, mean reward: 1.822 [1.503, 2.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.289], loss: 0.078179, mae: 0.281817, mean_q: 3.791599
 15255/100000: episode: 164, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 50.168, mean reward: 2.280 [1.853, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.929, 10.332], loss: 0.074124, mae: 0.280149, mean_q: 3.788300
 15276/100000: episode: 165, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 38.470, mean reward: 1.832 [1.534, 2.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.812, 10.264], loss: 0.078837, mae: 0.285170, mean_q: 3.785902
 15292/100000: episode: 166, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 29.007, mean reward: 1.813 [1.475, 2.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.324, 10.159], loss: 0.079753, mae: 0.285033, mean_q: 3.813564
 15310/100000: episode: 167, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 49.513, mean reward: 2.751 [2.105, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.264, 10.100], loss: 0.098134, mae: 0.302348, mean_q: 3.806746
 15329/100000: episode: 168, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 35.848, mean reward: 1.887 [1.539, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.109, 10.252], loss: 0.084998, mae: 0.289262, mean_q: 3.816138
 15345/100000: episode: 169, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 38.461, mean reward: 2.404 [1.977, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.830, 10.100], loss: 0.085673, mae: 0.289016, mean_q: 3.827727
 15361/100000: episode: 170, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 33.266, mean reward: 2.079 [1.537, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.255, 10.100], loss: 0.091464, mae: 0.314606, mean_q: 3.841227
 15379/100000: episode: 171, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 48.365, mean reward: 2.687 [2.156, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.179, 10.100], loss: 0.082149, mae: 0.295150, mean_q: 3.844077
 15396/100000: episode: 172, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 35.648, mean reward: 2.097 [1.850, 2.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.323], loss: 0.093746, mae: 0.311138, mean_q: 3.883005
 15413/100000: episode: 173, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 32.055, mean reward: 1.886 [1.560, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.903, 10.238], loss: 0.082577, mae: 0.302721, mean_q: 3.797050
 15435/100000: episode: 174, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 51.069, mean reward: 2.321 [1.735, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.086, 10.411], loss: 0.099750, mae: 0.320497, mean_q: 3.850216
 15456/100000: episode: 175, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 42.885, mean reward: 2.042 [1.517, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.600, 10.216], loss: 0.086054, mae: 0.294868, mean_q: 3.787882
 15473/100000: episode: 176, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 32.857, mean reward: 1.933 [1.506, 2.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.259, 10.100], loss: 0.086909, mae: 0.291996, mean_q: 3.791298
 15491/100000: episode: 177, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 40.190, mean reward: 2.233 [1.882, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.143, 10.100], loss: 0.103596, mae: 0.311842, mean_q: 3.821725
 15512/100000: episode: 178, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 39.341, mean reward: 1.873 [1.530, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.808, 10.234], loss: 0.097333, mae: 0.307635, mean_q: 3.830580
 15528/100000: episode: 179, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 28.522, mean reward: 1.783 [1.554, 2.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.167], loss: 0.085650, mae: 0.298567, mean_q: 3.813811
 15544/100000: episode: 180, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 31.061, mean reward: 1.941 [1.665, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.102, 10.100], loss: 0.065500, mae: 0.275156, mean_q: 3.842886
 15565/100000: episode: 181, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 43.144, mean reward: 2.054 [1.593, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.113, 10.200], loss: 0.092336, mae: 0.309134, mean_q: 3.807901
 15581/100000: episode: 182, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 28.179, mean reward: 1.761 [1.446, 2.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.159], loss: 0.080628, mae: 0.285415, mean_q: 3.843842
 15598/100000: episode: 183, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 36.954, mean reward: 2.174 [1.596, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.184, 10.100], loss: 0.071637, mae: 0.272847, mean_q: 3.800430
 15614/100000: episode: 184, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 35.020, mean reward: 2.189 [1.607, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.257, 10.158], loss: 0.071160, mae: 0.277601, mean_q: 3.816566
 15631/100000: episode: 185, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 38.166, mean reward: 2.245 [1.664, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.507, 10.100], loss: 0.078705, mae: 0.278912, mean_q: 3.778384
 15650/100000: episode: 186, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 32.873, mean reward: 1.730 [1.542, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.657, 10.143], loss: 0.106219, mae: 0.296880, mean_q: 3.866287
 15670/100000: episode: 187, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 35.048, mean reward: 1.752 [1.446, 2.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.102, 10.100], loss: 0.090951, mae: 0.302834, mean_q: 3.856771
 15687/100000: episode: 188, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 30.710, mean reward: 1.806 [1.481, 2.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.493, 10.200], loss: 0.101107, mae: 0.309439, mean_q: 3.828900
 15705/100000: episode: 189, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 46.594, mean reward: 2.589 [2.088, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.259, 10.100], loss: 0.080208, mae: 0.286024, mean_q: 3.827047
 15721/100000: episode: 190, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 29.121, mean reward: 1.820 [1.476, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.115, 10.100], loss: 0.089922, mae: 0.297432, mean_q: 3.791484
 15737/100000: episode: 191, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 30.426, mean reward: 1.902 [1.552, 2.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.315, 10.100], loss: 0.089756, mae: 0.300331, mean_q: 3.822807
 15756/100000: episode: 192, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 39.058, mean reward: 2.056 [1.529, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.335, 10.100], loss: 0.087095, mae: 0.308716, mean_q: 3.796218
 15774/100000: episode: 193, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 53.721, mean reward: 2.985 [2.389, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.466, 10.100], loss: 0.078616, mae: 0.286151, mean_q: 3.807725
 15792/100000: episode: 194, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 50.328, mean reward: 2.796 [2.233, 4.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.092, 10.100], loss: 0.081931, mae: 0.283779, mean_q: 3.821653
 15811/100000: episode: 195, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 32.527, mean reward: 1.712 [1.509, 1.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.071, 10.170], loss: 0.101825, mae: 0.316529, mean_q: 3.839132
 15828/100000: episode: 196, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 30.787, mean reward: 1.811 [1.480, 2.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.499, 10.100], loss: 0.100977, mae: 0.319133, mean_q: 3.836990
 15850/100000: episode: 197, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 53.538, mean reward: 2.434 [1.816, 6.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.630, 10.278], loss: 0.096181, mae: 0.308334, mean_q: 3.827859
 15868/100000: episode: 198, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 54.105, mean reward: 3.006 [2.310, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.458, 10.100], loss: 0.129775, mae: 0.346241, mean_q: 3.865959
 15884/100000: episode: 199, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 30.239, mean reward: 1.890 [1.638, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.300, 10.100], loss: 0.133003, mae: 0.338621, mean_q: 3.859712
 15900/100000: episode: 200, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 31.744, mean reward: 1.984 [1.456, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.223], loss: 0.099517, mae: 0.320998, mean_q: 3.785859
 15916/100000: episode: 201, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 30.138, mean reward: 1.884 [1.510, 2.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.647, 10.219], loss: 0.096541, mae: 0.322159, mean_q: 3.830968
 15938/100000: episode: 202, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 45.110, mean reward: 2.050 [1.639, 2.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.059, 10.274], loss: 0.104774, mae: 0.325385, mean_q: 3.822038
 15957/100000: episode: 203, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 38.649, mean reward: 2.034 [1.646, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.130, 10.100], loss: 0.093059, mae: 0.308011, mean_q: 3.792185
 15976/100000: episode: 204, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 39.686, mean reward: 2.089 [1.888, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.346, 10.100], loss: 0.079701, mae: 0.288257, mean_q: 3.788006
 15995/100000: episode: 205, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 43.955, mean reward: 2.313 [1.814, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.495, 10.100], loss: 0.102738, mae: 0.312252, mean_q: 3.805321
 16013/100000: episode: 206, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 47.029, mean reward: 2.613 [2.005, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.297, 10.100], loss: 0.108484, mae: 0.337348, mean_q: 3.856534
 16031/100000: episode: 207, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 51.768, mean reward: 2.876 [2.460, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.201, 10.100], loss: 0.092170, mae: 0.315230, mean_q: 3.872190
 16049/100000: episode: 208, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 37.167, mean reward: 2.065 [1.750, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.236, 10.100], loss: 0.106495, mae: 0.317936, mean_q: 3.789937
 16071/100000: episode: 209, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 51.966, mean reward: 2.362 [1.951, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.114, 10.302], loss: 0.083126, mae: 0.303292, mean_q: 3.850580
 16093/100000: episode: 210, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 40.988, mean reward: 1.863 [1.586, 2.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.235], loss: 0.088256, mae: 0.302101, mean_q: 3.839739
 16115/100000: episode: 211, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 45.166, mean reward: 2.053 [1.765, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.041, 10.392], loss: 0.089074, mae: 0.304208, mean_q: 3.842251
 16135/100000: episode: 212, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 34.487, mean reward: 1.724 [1.491, 2.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.739, 10.100], loss: 0.100863, mae: 0.315548, mean_q: 3.826965
 16156/100000: episode: 213, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 51.151, mean reward: 2.436 [1.662, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.409, 10.441], loss: 0.104250, mae: 0.320166, mean_q: 3.798893
 16172/100000: episode: 214, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 35.342, mean reward: 2.209 [1.722, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.449, 10.100], loss: 0.095275, mae: 0.322021, mean_q: 3.838745
 16189/100000: episode: 215, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 39.719, mean reward: 2.336 [1.621, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.443, 10.200], loss: 0.095539, mae: 0.309326, mean_q: 3.856119
 16211/100000: episode: 216, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 40.806, mean reward: 1.855 [1.595, 2.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.299], loss: 0.127843, mae: 0.324828, mean_q: 3.876662
 16227/100000: episode: 217, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 26.458, mean reward: 1.654 [1.501, 1.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.117, 10.120], loss: 0.091017, mae: 0.303555, mean_q: 3.811901
 16247/100000: episode: 218, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 37.591, mean reward: 1.880 [1.457, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.782, 10.100], loss: 0.082113, mae: 0.296991, mean_q: 3.817338
 16269/100000: episode: 219, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 41.911, mean reward: 1.905 [1.604, 2.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.268, 10.175], loss: 0.101586, mae: 0.313866, mean_q: 3.851998
 16289/100000: episode: 220, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 36.994, mean reward: 1.850 [1.638, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.373, 10.100], loss: 0.125974, mae: 0.334150, mean_q: 3.850941
 16306/100000: episode: 221, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 34.819, mean reward: 2.048 [1.450, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.341], loss: 0.151296, mae: 0.342568, mean_q: 3.843429
 16323/100000: episode: 222, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 34.435, mean reward: 2.026 [1.635, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.140, 10.100], loss: 0.094580, mae: 0.313187, mean_q: 3.832971
 16342/100000: episode: 223, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 32.781, mean reward: 1.725 [1.437, 2.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.100], loss: 0.121924, mae: 0.336212, mean_q: 3.866160
 16358/100000: episode: 224, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 29.792, mean reward: 1.862 [1.481, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.216], loss: 0.080482, mae: 0.284252, mean_q: 3.837400
 16376/100000: episode: 225, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 73.517, mean reward: 4.084 [2.475, 6.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.325, 10.100], loss: 0.110152, mae: 0.344511, mean_q: 3.874892
 16394/100000: episode: 226, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 39.713, mean reward: 2.206 [1.815, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.131, 10.100], loss: 0.102490, mae: 0.317981, mean_q: 3.868633
 16415/100000: episode: 227, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 41.056, mean reward: 1.955 [1.529, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.976, 10.209], loss: 0.134197, mae: 0.348563, mean_q: 3.860998
 16434/100000: episode: 228, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 33.206, mean reward: 1.748 [1.487, 2.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.504, 10.208], loss: 0.109450, mae: 0.311270, mean_q: 3.845016
 16450/100000: episode: 229, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 31.037, mean reward: 1.940 [1.536, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.250], loss: 0.123536, mae: 0.336388, mean_q: 3.864075
 16470/100000: episode: 230, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 37.091, mean reward: 1.855 [1.618, 2.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.240, 10.100], loss: 0.109649, mae: 0.314870, mean_q: 3.797309
 16486/100000: episode: 231, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 28.444, mean reward: 1.778 [1.515, 2.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.061, 10.142], loss: 0.101880, mae: 0.316814, mean_q: 3.872473
 16507/100000: episode: 232, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 41.076, mean reward: 1.956 [1.524, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.209], loss: 0.127075, mae: 0.330334, mean_q: 3.849113
 16526/100000: episode: 233, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 32.413, mean reward: 1.706 [1.486, 2.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.101], loss: 0.106774, mae: 0.309277, mean_q: 3.840110
 16542/100000: episode: 234, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 34.300, mean reward: 2.144 [1.659, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.301], loss: 0.114614, mae: 0.315020, mean_q: 3.798607
 16561/100000: episode: 235, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 36.841, mean reward: 1.939 [1.477, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.151, 10.100], loss: 0.107600, mae: 0.325565, mean_q: 3.848134
 16578/100000: episode: 236, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 36.010, mean reward: 2.118 [1.572, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.084, 10.100], loss: 0.149847, mae: 0.352471, mean_q: 3.846862
 16597/100000: episode: 237, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 33.246, mean reward: 1.750 [1.507, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.127, 10.123], loss: 0.131798, mae: 0.349535, mean_q: 3.854023
 16619/100000: episode: 238, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 46.091, mean reward: 2.095 [1.513, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.486, 10.228], loss: 0.095942, mae: 0.310890, mean_q: 3.856940
 16636/100000: episode: 239, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 33.430, mean reward: 1.966 [1.452, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.111, 10.100], loss: 0.103504, mae: 0.311111, mean_q: 3.808513
[Info] 2-TH LEVEL FOUND: 4.384673118591309, Considering 12/88 traces
 16657/100000: episode: 240, duration: 4.213s, episode steps: 21, steps per second: 5, episode reward: 38.382, mean reward: 1.828 [1.559, 2.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.191, 10.275], loss: 0.098251, mae: 0.319696, mean_q: 3.837646
 16674/100000: episode: 241, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 39.343, mean reward: 2.314 [1.959, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.993, 10.100], loss: 0.114228, mae: 0.335625, mean_q: 3.821477
 16691/100000: episode: 242, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 31.695, mean reward: 1.864 [1.599, 2.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.512, 10.100], loss: 0.106721, mae: 0.330254, mean_q: 3.776251
 16703/100000: episode: 243, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 32.116, mean reward: 2.676 [2.054, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.435, 10.100], loss: 0.140414, mae: 0.338059, mean_q: 3.850463
 16720/100000: episode: 244, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 29.857, mean reward: 1.756 [1.464, 2.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.059, 10.104], loss: 0.103814, mae: 0.328569, mean_q: 3.850720
 16737/100000: episode: 245, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 31.845, mean reward: 1.873 [1.574, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.706, 10.100], loss: 0.110716, mae: 0.337524, mean_q: 3.866968
 16754/100000: episode: 246, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 33.390, mean reward: 1.964 [1.465, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.143], loss: 0.102966, mae: 0.314110, mean_q: 3.830378
 16771/100000: episode: 247, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 32.771, mean reward: 1.928 [1.647, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.145, 10.100], loss: 0.115104, mae: 0.346949, mean_q: 3.864634
 16788/100000: episode: 248, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 33.112, mean reward: 1.948 [1.637, 2.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.305, 10.100], loss: 0.114767, mae: 0.350742, mean_q: 3.880340
 16805/100000: episode: 249, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 33.150, mean reward: 1.950 [1.526, 2.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.199, 10.100], loss: 0.124040, mae: 0.356595, mean_q: 3.862880
 16822/100000: episode: 250, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 33.818, mean reward: 1.989 [1.625, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.297, 10.100], loss: 0.135561, mae: 0.367752, mean_q: 3.890552
 16839/100000: episode: 251, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 37.915, mean reward: 2.230 [1.584, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.119, 10.100], loss: 0.114589, mae: 0.325344, mean_q: 3.849778
 16851/100000: episode: 252, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 51.461, mean reward: 4.288 [3.797, 5.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.031, 10.100], loss: 0.132356, mae: 0.334486, mean_q: 3.912448
 16868/100000: episode: 253, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 34.231, mean reward: 2.014 [1.559, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.103, 10.100], loss: 0.132499, mae: 0.344067, mean_q: 3.876870
 16885/100000: episode: 254, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 33.880, mean reward: 1.993 [1.571, 2.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.274, 10.100], loss: 0.120481, mae: 0.324537, mean_q: 3.828725
 16902/100000: episode: 255, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 30.272, mean reward: 1.781 [1.476, 2.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.102, 10.123], loss: 0.137431, mae: 0.336285, mean_q: 3.783388
 16919/100000: episode: 256, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 32.543, mean reward: 1.914 [1.556, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.272, 10.100], loss: 0.148596, mae: 0.372113, mean_q: 3.835491
 16931/100000: episode: 257, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 31.409, mean reward: 2.617 [1.939, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.551, 10.100], loss: 0.142926, mae: 0.367391, mean_q: 3.876469
 16948/100000: episode: 258, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 32.482, mean reward: 1.911 [1.705, 2.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.127, 10.100], loss: 0.125257, mae: 0.342617, mean_q: 3.843952
 16965/100000: episode: 259, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 32.404, mean reward: 1.906 [1.489, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.513, 10.100], loss: 0.113264, mae: 0.345271, mean_q: 3.860533
 16982/100000: episode: 260, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 35.961, mean reward: 2.115 [1.482, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.123], loss: 0.131524, mae: 0.339629, mean_q: 3.808927
 16999/100000: episode: 261, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 32.338, mean reward: 1.902 [1.661, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.111], loss: 0.095849, mae: 0.311288, mean_q: 3.803328
 17016/100000: episode: 262, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 31.742, mean reward: 1.867 [1.503, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.188, 10.100], loss: 0.107111, mae: 0.323644, mean_q: 3.847576
 17027/100000: episode: 263, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 30.818, mean reward: 2.802 [2.420, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.249, 10.100], loss: 0.174191, mae: 0.365894, mean_q: 3.889333
 17044/100000: episode: 264, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 29.729, mean reward: 1.749 [1.489, 2.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.083, 10.128], loss: 0.111990, mae: 0.313358, mean_q: 3.776380
 17055/100000: episode: 265, duration: 0.069s, episode steps: 11, steps per second: 161, episode reward: 25.792, mean reward: 2.345 [1.748, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.089, 10.100], loss: 0.124582, mae: 0.347240, mean_q: 3.898175
 17072/100000: episode: 266, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 36.295, mean reward: 2.135 [1.764, 2.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.232, 10.100], loss: 0.159999, mae: 0.356470, mean_q: 3.857646
 17089/100000: episode: 267, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 28.825, mean reward: 1.696 [1.451, 2.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.527, 10.100], loss: 0.099284, mae: 0.327292, mean_q: 3.863933
 17106/100000: episode: 268, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 35.561, mean reward: 2.092 [1.623, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.230, 10.100], loss: 0.134028, mae: 0.337768, mean_q: 3.833690
 17118/100000: episode: 269, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 46.276, mean reward: 3.856 [2.806, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.270, 10.100], loss: 0.101836, mae: 0.332680, mean_q: 3.867166
 17135/100000: episode: 270, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 30.347, mean reward: 1.785 [1.493, 2.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.070, 10.100], loss: 0.115370, mae: 0.334230, mean_q: 3.839381
 17152/100000: episode: 271, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 31.546, mean reward: 1.856 [1.613, 2.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.890, 10.100], loss: 0.096227, mae: 0.320974, mean_q: 3.830633
 17169/100000: episode: 272, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 34.553, mean reward: 2.033 [1.467, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.490, 10.100], loss: 0.175973, mae: 0.374529, mean_q: 3.904475
 17186/100000: episode: 273, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 34.179, mean reward: 2.011 [1.681, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.218, 10.100], loss: 0.127196, mae: 0.333226, mean_q: 3.853725
 17203/100000: episode: 274, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 32.521, mean reward: 1.913 [1.589, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.127, 10.100], loss: 0.136063, mae: 0.354722, mean_q: 3.893785
 17220/100000: episode: 275, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 36.665, mean reward: 2.157 [1.602, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.288, 10.100], loss: 0.114993, mae: 0.334233, mean_q: 3.878240
 17230/100000: episode: 276, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 24.568, mean reward: 2.457 [2.151, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.226, 10.100], loss: 0.114045, mae: 0.351824, mean_q: 3.869133
 17241/100000: episode: 277, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 29.544, mean reward: 2.686 [2.262, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.266, 10.100], loss: 0.097665, mae: 0.322122, mean_q: 3.896618
 17251/100000: episode: 278, duration: 0.066s, episode steps: 10, steps per second: 150, episode reward: 24.956, mean reward: 2.496 [1.703, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.445, 10.100], loss: 0.117623, mae: 0.336455, mean_q: 3.846946
 17268/100000: episode: 279, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 32.476, mean reward: 1.910 [1.662, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.530, 10.100], loss: 0.126798, mae: 0.340127, mean_q: 3.860297
 17280/100000: episode: 280, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 43.975, mean reward: 3.665 [2.516, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.192, 10.100], loss: 0.117257, mae: 0.338153, mean_q: 3.906860
 17297/100000: episode: 281, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 34.144, mean reward: 2.008 [1.598, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.124, 10.100], loss: 0.123169, mae: 0.354749, mean_q: 3.918191
 17309/100000: episode: 282, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 39.571, mean reward: 3.298 [2.388, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.189, 10.100], loss: 0.108113, mae: 0.325317, mean_q: 3.837608
 17326/100000: episode: 283, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 29.243, mean reward: 1.720 [1.467, 2.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.357, 10.174], loss: 0.097904, mae: 0.318028, mean_q: 3.852712
 17343/100000: episode: 284, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 37.653, mean reward: 2.215 [1.696, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.295, 10.100], loss: 0.157981, mae: 0.358667, mean_q: 3.884943
 17360/100000: episode: 285, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 27.918, mean reward: 1.642 [1.456, 2.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.141, 10.153], loss: 0.131699, mae: 0.346569, mean_q: 3.893945
 17377/100000: episode: 286, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 34.717, mean reward: 2.042 [1.559, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.094, 10.100], loss: 0.192738, mae: 0.385792, mean_q: 3.921667
 17394/100000: episode: 287, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 35.515, mean reward: 2.089 [1.722, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.474, 10.100], loss: 0.118308, mae: 0.340436, mean_q: 3.833441
 17404/100000: episode: 288, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 29.129, mean reward: 2.913 [2.213, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.322, 10.100], loss: 0.174239, mae: 0.366691, mean_q: 3.904446
 17414/100000: episode: 289, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 23.128, mean reward: 2.313 [1.979, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.135, 10.100], loss: 0.131389, mae: 0.366975, mean_q: 3.928180
 17431/100000: episode: 290, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 36.873, mean reward: 2.169 [1.777, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.279, 10.100], loss: 0.107536, mae: 0.345984, mean_q: 3.838154
 17448/100000: episode: 291, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 34.841, mean reward: 2.049 [1.750, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.531, 10.100], loss: 0.133237, mae: 0.349738, mean_q: 3.924753
 17465/100000: episode: 292, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 37.717, mean reward: 2.219 [1.768, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.188, 10.100], loss: 0.126286, mae: 0.347503, mean_q: 3.893342
 17482/100000: episode: 293, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 39.620, mean reward: 2.331 [1.471, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.260], loss: 0.123423, mae: 0.354466, mean_q: 3.881426
 17499/100000: episode: 294, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 31.086, mean reward: 1.829 [1.465, 2.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.046, 10.100], loss: 0.139962, mae: 0.368155, mean_q: 3.875223
 17510/100000: episode: 295, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 27.032, mean reward: 2.457 [1.985, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.290, 10.100], loss: 0.152950, mae: 0.367062, mean_q: 3.922942
 17521/100000: episode: 296, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 40.250, mean reward: 3.659 [2.812, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.196, 10.100], loss: 0.113189, mae: 0.342292, mean_q: 3.949758
 17538/100000: episode: 297, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 37.905, mean reward: 2.230 [1.886, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.326, 10.100], loss: 0.118504, mae: 0.348369, mean_q: 3.859056
 17555/100000: episode: 298, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 32.327, mean reward: 1.902 [1.605, 2.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.277, 10.100], loss: 0.127772, mae: 0.359097, mean_q: 3.899549
 17572/100000: episode: 299, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 35.791, mean reward: 2.105 [1.599, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.135, 10.100], loss: 0.092871, mae: 0.317049, mean_q: 3.847791
 17589/100000: episode: 300, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 34.120, mean reward: 2.007 [1.719, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.074, 10.100], loss: 0.125803, mae: 0.358641, mean_q: 3.920305
 17606/100000: episode: 301, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 38.726, mean reward: 2.278 [1.653, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.064, 10.100], loss: 0.116165, mae: 0.341547, mean_q: 3.865966
 17623/100000: episode: 302, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 34.851, mean reward: 2.050 [1.477, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.742, 10.110], loss: 0.118904, mae: 0.343726, mean_q: 3.946141
 17640/100000: episode: 303, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 30.990, mean reward: 1.823 [1.570, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.089, 10.100], loss: 0.099617, mae: 0.321916, mean_q: 3.835887
 17657/100000: episode: 304, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 36.274, mean reward: 2.134 [1.580, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.335, 10.100], loss: 0.111125, mae: 0.323378, mean_q: 3.877088
 17674/100000: episode: 305, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 40.806, mean reward: 2.400 [1.513, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.195, 10.100], loss: 0.120357, mae: 0.368721, mean_q: 3.914925
 17691/100000: episode: 306, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 34.975, mean reward: 2.057 [1.621, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.261, 10.100], loss: 0.128804, mae: 0.358750, mean_q: 3.846095
 17708/100000: episode: 307, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 34.528, mean reward: 2.031 [1.550, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.423, 10.100], loss: 0.166908, mae: 0.387392, mean_q: 3.884279
 17719/100000: episode: 308, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 25.375, mean reward: 2.307 [1.697, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.117, 10.100], loss: 0.150807, mae: 0.375520, mean_q: 3.912164
 17736/100000: episode: 309, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 33.339, mean reward: 1.961 [1.474, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.203, 10.100], loss: 0.114235, mae: 0.348804, mean_q: 3.870058
 17753/100000: episode: 310, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 29.890, mean reward: 1.758 [1.495, 2.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.280, 10.100], loss: 0.189681, mae: 0.383565, mean_q: 3.915320
 17770/100000: episode: 311, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 35.009, mean reward: 2.059 [1.776, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.930, 10.100], loss: 0.142574, mae: 0.360047, mean_q: 3.867707
 17787/100000: episode: 312, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 35.156, mean reward: 2.068 [1.478, 2.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.130], loss: 0.133265, mae: 0.351059, mean_q: 3.878966
 17804/100000: episode: 313, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 34.513, mean reward: 2.030 [1.811, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.216, 10.100], loss: 0.081118, mae: 0.308535, mean_q: 3.860809
 17821/100000: episode: 314, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 32.069, mean reward: 1.886 [1.521, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.256, 10.100], loss: 0.121033, mae: 0.332900, mean_q: 3.897783
 17833/100000: episode: 315, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 44.747, mean reward: 3.729 [3.130, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.425, 10.100], loss: 0.147427, mae: 0.370818, mean_q: 3.894094
 17850/100000: episode: 316, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 34.154, mean reward: 2.009 [1.468, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.321, 10.100], loss: 0.139252, mae: 0.381720, mean_q: 3.886987
 17867/100000: episode: 317, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 37.066, mean reward: 2.180 [1.792, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.123, 10.100], loss: 0.101013, mae: 0.335880, mean_q: 3.870597
 17884/100000: episode: 318, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 29.780, mean reward: 1.752 [1.534, 2.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.104, 10.100], loss: 0.132341, mae: 0.376525, mean_q: 3.919386
 17901/100000: episode: 319, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 37.537, mean reward: 2.208 [1.620, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.296, 10.100], loss: 0.121692, mae: 0.358788, mean_q: 3.896056
 17918/100000: episode: 320, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 31.649, mean reward: 1.862 [1.465, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.057, 10.291], loss: 0.119895, mae: 0.345063, mean_q: 3.883107
 17929/100000: episode: 321, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 24.716, mean reward: 2.247 [1.652, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.097, 10.100], loss: 0.111993, mae: 0.332888, mean_q: 3.853813
 17946/100000: episode: 322, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 36.480, mean reward: 2.146 [1.624, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.393, 10.100], loss: 0.128206, mae: 0.371024, mean_q: 3.958797
 17956/100000: episode: 323, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 25.795, mean reward: 2.579 [2.052, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.226, 10.100], loss: 0.115117, mae: 0.338763, mean_q: 3.755913
 17973/100000: episode: 324, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 31.312, mean reward: 1.842 [1.620, 2.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.321, 10.100], loss: 0.129992, mae: 0.364616, mean_q: 3.915153
 17985/100000: episode: 325, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 40.936, mean reward: 3.411 [2.419, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.270, 10.100], loss: 0.170068, mae: 0.409513, mean_q: 3.914422
 18002/100000: episode: 326, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 30.833, mean reward: 1.814 [1.498, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.188], loss: 0.108242, mae: 0.342558, mean_q: 3.983333
 18019/100000: episode: 327, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 34.421, mean reward: 2.025 [1.572, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.485, 10.100], loss: 0.119464, mae: 0.345954, mean_q: 3.888479
[Info] 3-TH LEVEL FOUND: 4.737689971923828, Considering 10/90 traces
 18036/100000: episode: 328, duration: 4.211s, episode steps: 17, steps per second: 4, episode reward: 31.638, mean reward: 1.861 [1.609, 2.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.160, 10.100], loss: 0.125948, mae: 0.362316, mean_q: 3.945189
 18044/100000: episode: 329, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 34.233, mean reward: 4.279 [3.257, 5.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.212, 10.100], loss: 0.172897, mae: 0.381785, mean_q: 3.944893
 18052/100000: episode: 330, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 27.653, mean reward: 3.457 [2.820, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.315, 10.100], loss: 0.123608, mae: 0.359186, mean_q: 3.828881
 18061/100000: episode: 331, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.038, mean reward: 2.782 [2.317, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.352, 10.100], loss: 0.101424, mae: 0.339360, mean_q: 3.911426
 18068/100000: episode: 332, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 14.531, mean reward: 2.076 [1.816, 2.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.315, 10.100], loss: 0.158666, mae: 0.396845, mean_q: 3.986113
 18078/100000: episode: 333, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 57.541, mean reward: 5.754 [4.892, 6.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.363, 10.100], loss: 0.150267, mae: 0.393870, mean_q: 3.969912
 18087/100000: episode: 334, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.917, mean reward: 2.880 [2.562, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.586, 10.100], loss: 0.157591, mae: 0.393668, mean_q: 3.875178
 18096/100000: episode: 335, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 28.057, mean reward: 3.117 [2.572, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.156, 10.100], loss: 0.101884, mae: 0.334657, mean_q: 3.955680
 18106/100000: episode: 336, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 34.957, mean reward: 3.496 [2.977, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.136, 10.100], loss: 0.145437, mae: 0.366796, mean_q: 3.938421
 18116/100000: episode: 337, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 39.703, mean reward: 3.970 [3.035, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.283, 10.100], loss: 0.181070, mae: 0.394774, mean_q: 3.914702
 18124/100000: episode: 338, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 26.568, mean reward: 3.321 [2.597, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.305, 10.100], loss: 0.124788, mae: 0.370114, mean_q: 4.053432
 18132/100000: episode: 339, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 27.557, mean reward: 3.445 [3.067, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.586, 10.100], loss: 0.189971, mae: 0.414009, mean_q: 3.961427
 18143/100000: episode: 340, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 49.677, mean reward: 4.516 [3.247, 8.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.437, 10.100], loss: 0.156894, mae: 0.410211, mean_q: 3.962842
 18152/100000: episode: 341, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 22.746, mean reward: 2.527 [2.388, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.253, 10.100], loss: 0.193358, mae: 0.393075, mean_q: 3.855854
 18161/100000: episode: 342, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 32.863, mean reward: 3.651 [2.946, 5.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.431, 10.100], loss: 0.146075, mae: 0.395116, mean_q: 3.976936
 18172/100000: episode: 343, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 40.238, mean reward: 3.658 [2.713, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.638, 10.100], loss: 0.148453, mae: 0.397616, mean_q: 3.938533
 18180/100000: episode: 344, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 27.562, mean reward: 3.445 [2.402, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.250, 10.100], loss: 0.160383, mae: 0.390775, mean_q: 4.021515
 18188/100000: episode: 345, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 16.280, mean reward: 2.035 [1.811, 2.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.366, 10.100], loss: 0.238890, mae: 0.404681, mean_q: 4.001672
 18199/100000: episode: 346, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 53.359, mean reward: 4.851 [3.222, 8.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.562, 10.100], loss: 0.155341, mae: 0.383259, mean_q: 3.936788
 18207/100000: episode: 347, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 27.545, mean reward: 3.443 [2.851, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.233, 10.100], loss: 0.137117, mae: 0.394799, mean_q: 4.109693
 18215/100000: episode: 348, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 26.311, mean reward: 3.289 [2.838, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.246, 10.100], loss: 0.140197, mae: 0.371684, mean_q: 3.910936
 18223/100000: episode: 349, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 20.048, mean reward: 2.506 [2.240, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.224, 10.100], loss: 0.211651, mae: 0.455322, mean_q: 4.137151
 18232/100000: episode: 350, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 25.172, mean reward: 2.797 [2.142, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.222, 10.100], loss: 0.171645, mae: 0.405006, mean_q: 3.960598
 18240/100000: episode: 351, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 24.394, mean reward: 3.049 [2.602, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.377, 10.100], loss: 0.174017, mae: 0.413941, mean_q: 4.010922
 18247/100000: episode: 352, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 15.263, mean reward: 2.180 [1.687, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.318, 10.100], loss: 0.194085, mae: 0.426417, mean_q: 4.044961
 18258/100000: episode: 353, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 29.439, mean reward: 2.676 [1.963, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.197, 10.100], loss: 0.200052, mae: 0.393358, mean_q: 3.984780
 18266/100000: episode: 354, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 23.190, mean reward: 2.899 [2.563, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.287, 10.100], loss: 0.265540, mae: 0.432754, mean_q: 4.006516
 18274/100000: episode: 355, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 37.553, mean reward: 4.694 [3.801, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.337, 10.100], loss: 0.139141, mae: 0.397970, mean_q: 4.062280
 18282/100000: episode: 356, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 28.946, mean reward: 3.618 [2.963, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.444, 10.100], loss: 0.120752, mae: 0.350935, mean_q: 3.873011
 18293/100000: episode: 357, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 33.295, mean reward: 3.027 [2.356, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.341, 10.100], loss: 0.158719, mae: 0.414232, mean_q: 4.053143
 18304/100000: episode: 358, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 32.460, mean reward: 2.951 [2.400, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.166, 10.100], loss: 0.182211, mae: 0.418133, mean_q: 4.083242
 18315/100000: episode: 359, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 33.712, mean reward: 3.065 [2.545, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.167, 10.100], loss: 0.150436, mae: 0.402655, mean_q: 4.049404
 18323/100000: episode: 360, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 19.869, mean reward: 2.484 [2.010, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.281, 10.100], loss: 0.186985, mae: 0.420078, mean_q: 4.235404
 18332/100000: episode: 361, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 24.184, mean reward: 2.687 [2.151, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.221, 10.100], loss: 0.195740, mae: 0.377909, mean_q: 3.966009
 18340/100000: episode: 362, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 17.531, mean reward: 2.191 [1.946, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.227, 10.100], loss: 0.157618, mae: 0.400971, mean_q: 4.077328
 18349/100000: episode: 363, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 23.217, mean reward: 2.580 [2.263, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.319, 10.100], loss: 0.169669, mae: 0.400612, mean_q: 4.110593
 18358/100000: episode: 364, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 31.394, mean reward: 3.488 [2.209, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.257, 10.100], loss: 0.279890, mae: 0.463897, mean_q: 4.048376
 18365/100000: episode: 365, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 14.786, mean reward: 2.112 [1.871, 2.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.461, 10.100], loss: 0.173986, mae: 0.422391, mean_q: 4.003157
 18375/100000: episode: 366, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 45.340, mean reward: 4.534 [2.943, 7.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.189, 10.100], loss: 0.121006, mae: 0.347270, mean_q: 3.973157
 18384/100000: episode: 367, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 49.085, mean reward: 5.454 [3.669, 8.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.263, 10.100], loss: 0.168844, mae: 0.411431, mean_q: 4.021196
 18391/100000: episode: 368, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 14.478, mean reward: 2.068 [1.835, 2.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.355, 10.100], loss: 0.200765, mae: 0.430675, mean_q: 4.198087
 18401/100000: episode: 369, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 57.068, mean reward: 5.707 [4.181, 8.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.388, 10.100], loss: 0.166374, mae: 0.430497, mean_q: 4.126881
 18409/100000: episode: 370, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 26.976, mean reward: 3.372 [2.730, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.240, 10.100], loss: 0.224869, mae: 0.440704, mean_q: 3.975791
 18420/100000: episode: 371, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 38.214, mean reward: 3.474 [3.061, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.280, 10.100], loss: 0.192046, mae: 0.446885, mean_q: 4.054513
 18427/100000: episode: 372, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 15.900, mean reward: 2.271 [2.057, 2.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.268, 10.100], loss: 0.227969, mae: 0.450528, mean_q: 3.965741
 18438/100000: episode: 373, duration: 0.069s, episode steps: 11, steps per second: 161, episode reward: 39.681, mean reward: 3.607 [2.991, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.418, 10.100], loss: 0.175394, mae: 0.419001, mean_q: 4.081078
 18446/100000: episode: 374, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 19.862, mean reward: 2.483 [2.135, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.290, 10.100], loss: 0.205298, mae: 0.497018, mean_q: 4.238029
 18457/100000: episode: 375, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 35.167, mean reward: 3.197 [2.566, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.590, 10.100], loss: 0.172983, mae: 0.426209, mean_q: 3.973121
 18464/100000: episode: 376, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 15.223, mean reward: 2.175 [1.910, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.295, 10.100], loss: 0.180916, mae: 0.414962, mean_q: 4.077754
 18472/100000: episode: 377, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 22.769, mean reward: 2.846 [2.446, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.434, 10.100], loss: 0.175415, mae: 0.425055, mean_q: 4.089504
 18482/100000: episode: 378, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 29.411, mean reward: 2.941 [1.631, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.100], loss: 0.150382, mae: 0.412151, mean_q: 4.030779
 18491/100000: episode: 379, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 37.103, mean reward: 4.123 [2.975, 5.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.277, 10.100], loss: 0.136540, mae: 0.370448, mean_q: 4.085698
 18501/100000: episode: 380, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 64.445, mean reward: 6.444 [4.667, 15.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.241, 10.100], loss: 0.252940, mae: 0.465245, mean_q: 4.058250
 18511/100000: episode: 381, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 35.266, mean reward: 3.527 [3.085, 4.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.335, 10.100], loss: 0.213735, mae: 0.472798, mean_q: 4.234806
 18522/100000: episode: 382, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 47.862, mean reward: 4.351 [3.672, 5.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.384, 10.100], loss: 0.207438, mae: 0.444015, mean_q: 4.121901
 18532/100000: episode: 383, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 36.866, mean reward: 3.687 [3.128, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.281, 10.100], loss: 0.163611, mae: 0.420765, mean_q: 4.062338
 18540/100000: episode: 384, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 41.168, mean reward: 5.146 [3.818, 7.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.115, 10.100], loss: 0.170655, mae: 0.419836, mean_q: 4.040336
 18551/100000: episode: 385, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 42.662, mean reward: 3.878 [2.245, 7.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.301, 10.100], loss: 0.273581, mae: 0.457073, mean_q: 4.157084
 18559/100000: episode: 386, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 21.372, mean reward: 2.671 [2.335, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.276, 10.100], loss: 0.217709, mae: 0.452982, mean_q: 4.100593
 18568/100000: episode: 387, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 58.798, mean reward: 6.533 [3.019, 19.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.257, 10.100], loss: 0.313803, mae: 0.515196, mean_q: 4.364241
 18576/100000: episode: 388, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 31.804, mean reward: 3.975 [3.486, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.476, 10.100], loss: 0.166016, mae: 0.445800, mean_q: 4.234485
 18584/100000: episode: 389, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 18.270, mean reward: 2.284 [1.888, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.302, 10.100], loss: 0.247093, mae: 0.512344, mean_q: 4.246939
 18592/100000: episode: 390, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 27.864, mean reward: 3.483 [1.939, 5.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.428, 10.100], loss: 0.283867, mae: 0.445535, mean_q: 3.922848
 18600/100000: episode: 391, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 34.777, mean reward: 4.347 [3.106, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.361, 10.100], loss: 0.260011, mae: 0.475460, mean_q: 4.156925
 18609/100000: episode: 392, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 27.263, mean reward: 3.029 [2.163, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.240, 10.100], loss: 0.216222, mae: 0.474374, mean_q: 4.260163
 18620/100000: episode: 393, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 35.088, mean reward: 3.190 [2.720, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.251, 10.100], loss: 0.211898, mae: 0.448469, mean_q: 4.137014
 18629/100000: episode: 394, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 22.659, mean reward: 2.518 [2.152, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.139, 10.100], loss: 0.172711, mae: 0.421632, mean_q: 4.143450
 18639/100000: episode: 395, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 62.508, mean reward: 6.251 [4.157, 11.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.757, 10.100], loss: 0.155786, mae: 0.387809, mean_q: 4.032470
 18647/100000: episode: 396, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 34.427, mean reward: 4.303 [3.721, 5.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.247, 10.100], loss: 0.168490, mae: 0.391157, mean_q: 4.179867
 18656/100000: episode: 397, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 38.425, mean reward: 4.269 [3.664, 5.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.433, 10.100], loss: 0.744750, mae: 0.633273, mean_q: 4.394134
 18664/100000: episode: 398, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 133.506, mean reward: 16.688 [4.105, 50.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.420, 10.100], loss: 0.384625, mae: 0.622078, mean_q: 4.307014
 18673/100000: episode: 399, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 29.545, mean reward: 3.283 [2.828, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.208, 10.100], loss: 0.471226, mae: 0.515575, mean_q: 4.221255
 18682/100000: episode: 400, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 24.724, mean reward: 2.747 [2.537, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.395, 10.100], loss: 0.219328, mae: 0.472399, mean_q: 4.268734
 18692/100000: episode: 401, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 27.540, mean reward: 2.754 [2.125, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.055, 10.100], loss: 0.572968, mae: 0.491725, mean_q: 4.315826
 18703/100000: episode: 402, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 31.760, mean reward: 2.887 [2.291, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.802, 10.100], loss: 0.246107, mae: 0.487662, mean_q: 4.372438
 18711/100000: episode: 403, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 29.874, mean reward: 3.734 [2.977, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.212, 10.100], loss: 0.465026, mae: 0.603952, mean_q: 4.450982
 18720/100000: episode: 404, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 36.807, mean reward: 4.090 [3.389, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.265, 10.100], loss: 0.634648, mae: 0.518230, mean_q: 4.167273
 18730/100000: episode: 405, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 29.100, mean reward: 2.910 [2.364, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.874, 10.100], loss: 0.211548, mae: 0.440142, mean_q: 4.319458
 18741/100000: episode: 406, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 50.441, mean reward: 4.586 [3.193, 8.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.633, 10.100], loss: 0.204498, mae: 0.459932, mean_q: 4.220585
 18748/100000: episode: 407, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 15.510, mean reward: 2.216 [1.800, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.327, 10.100], loss: 0.785980, mae: 0.580735, mean_q: 4.477067
 18756/100000: episode: 408, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 20.665, mean reward: 2.583 [2.282, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.247, 10.100], loss: 0.211324, mae: 0.442293, mean_q: 4.147562
 18764/100000: episode: 409, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 18.642, mean reward: 2.330 [1.953, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.780, 10.100], loss: 0.174887, mae: 0.434865, mean_q: 4.307290
 18772/100000: episode: 410, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 26.793, mean reward: 3.349 [2.817, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.208, 10.100], loss: 3.975749, mae: 0.777952, mean_q: 4.495690
 18782/100000: episode: 411, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 40.000, mean reward: 4.000 [3.164, 5.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.350, 10.100], loss: 0.483708, mae: 0.625782, mean_q: 4.046813
 18790/100000: episode: 412, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 31.574, mean reward: 3.947 [3.451, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.355, 10.100], loss: 0.676222, mae: 0.624613, mean_q: 4.408587
 18800/100000: episode: 413, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 37.580, mean reward: 3.758 [3.098, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.370, 10.100], loss: 0.228788, mae: 0.499067, mean_q: 4.181556
 18809/100000: episode: 414, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 30.347, mean reward: 3.372 [2.749, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.262, 10.100], loss: 0.209831, mae: 0.461815, mean_q: 4.295816
 18817/100000: episode: 415, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 34.078, mean reward: 4.260 [3.659, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.470, 10.100], loss: 3.840709, mae: 0.770569, mean_q: 4.624955
 18824/100000: episode: 416, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 13.802, mean reward: 1.972 [1.855, 2.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.274, 10.100], loss: 0.827921, mae: 0.550931, mean_q: 4.122804
 18833/100000: episode: 417, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 26.246, mean reward: 2.916 [2.265, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.220, 10.100], loss: 0.484022, mae: 0.577313, mean_q: 4.650753
[Info] 4-TH LEVEL FOUND: 6.834491729736328, Considering 10/90 traces
 18841/100000: episode: 418, duration: 4.175s, episode steps: 8, steps per second: 2, episode reward: 32.088, mean reward: 4.011 [3.485, 5.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.302, 10.100], loss: 0.157849, mae: 0.406336, mean_q: 4.263887
 18848/100000: episode: 419, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 35.097, mean reward: 5.014 [3.397, 6.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.332, 10.100], loss: 0.239315, mae: 0.489361, mean_q: 4.239315
 18854/100000: episode: 420, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 28.323, mean reward: 4.720 [3.815, 5.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.391, 10.100], loss: 0.322295, mae: 0.515319, mean_q: 4.483529
 18861/100000: episode: 421, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 33.873, mean reward: 4.839 [3.718, 6.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.356, 10.100], loss: 0.173933, mae: 0.421678, mean_q: 4.227443
 18868/100000: episode: 422, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 74.555, mean reward: 10.651 [4.371, 24.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.444, 10.100], loss: 0.311307, mae: 0.543333, mean_q: 4.346088
 18874/100000: episode: 423, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 20.452, mean reward: 3.409 [2.997, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.361, 10.100], loss: 0.284426, mae: 0.492168, mean_q: 4.313324
 18880/100000: episode: 424, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 23.551, mean reward: 3.925 [2.848, 6.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.212, 10.100], loss: 0.332918, mae: 0.494317, mean_q: 4.376149
 18887/100000: episode: 425, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 33.687, mean reward: 4.812 [3.895, 6.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.443, 10.100], loss: 0.225363, mae: 0.474339, mean_q: 4.352796
 18894/100000: episode: 426, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 23.967, mean reward: 3.424 [2.672, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.476, 10.100], loss: 0.210912, mae: 0.455392, mean_q: 4.461903
 18900/100000: episode: 427, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 19.115, mean reward: 3.186 [2.830, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.336, 10.100], loss: 0.176812, mae: 0.405342, mean_q: 4.082655
 18906/100000: episode: 428, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 36.516, mean reward: 6.086 [4.319, 8.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.335, 10.100], loss: 0.870782, mae: 0.591078, mean_q: 4.370327
 18912/100000: episode: 429, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 37.003, mean reward: 6.167 [4.986, 7.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.407, 10.100], loss: 0.392596, mae: 0.508613, mean_q: 4.293387
 18919/100000: episode: 430, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 31.936, mean reward: 4.562 [3.804, 5.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.422, 10.100], loss: 0.245939, mae: 0.445401, mean_q: 4.209836
 18925/100000: episode: 431, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 29.750, mean reward: 4.958 [4.521, 5.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.316, 10.100], loss: 0.271131, mae: 0.522996, mean_q: 4.502682
 18932/100000: episode: 432, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 40.335, mean reward: 5.762 [4.194, 9.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.427, 10.100], loss: 0.455948, mae: 0.572044, mean_q: 4.216033
 18938/100000: episode: 433, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 27.468, mean reward: 4.578 [3.831, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.402, 10.100], loss: 0.225986, mae: 0.506198, mean_q: 4.461212
 18944/100000: episode: 434, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 17.544, mean reward: 2.924 [2.325, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.329, 10.100], loss: 1.420430, mae: 0.640494, mean_q: 4.520258
 18950/100000: episode: 435, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 19.161, mean reward: 3.193 [2.681, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.175, 10.100], loss: 0.359148, mae: 0.542838, mean_q: 4.287315
 18956/100000: episode: 436, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 21.408, mean reward: 3.568 [2.938, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.407, 10.100], loss: 5.983506, mae: 0.772237, mean_q: 4.507562
 18962/100000: episode: 437, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 20.043, mean reward: 3.341 [2.721, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.461, 10.100], loss: 0.431450, mae: 0.659081, mean_q: 4.505588
 18971/100000: episode: 438, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 32.338, mean reward: 3.593 [2.962, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.312, 10.100], loss: 0.370562, mae: 0.611867, mean_q: 4.375124
 18977/100000: episode: 439, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 23.214, mean reward: 3.869 [2.818, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.247, 10.100], loss: 0.266557, mae: 0.516940, mean_q: 4.265390
 18983/100000: episode: 440, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 25.109, mean reward: 4.185 [3.156, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.286, 10.100], loss: 0.295079, mae: 0.532134, mean_q: 4.525857
 18989/100000: episode: 441, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 26.918, mean reward: 4.486 [3.954, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.390, 10.100], loss: 0.273162, mae: 0.470191, mean_q: 4.304627
 18995/100000: episode: 442, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 21.871, mean reward: 3.645 [3.076, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.358, 10.100], loss: 0.306941, mae: 0.502098, mean_q: 4.271714
 19001/100000: episode: 443, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 21.052, mean reward: 3.509 [3.027, 4.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.177, 10.100], loss: 0.367630, mae: 0.537293, mean_q: 4.451172
 19007/100000: episode: 444, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 21.205, mean reward: 3.534 [2.624, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.367, 10.100], loss: 0.270353, mae: 0.485028, mean_q: 4.411920
 19014/100000: episode: 445, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 21.644, mean reward: 3.092 [2.459, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.320, 10.100], loss: 0.279071, mae: 0.509523, mean_q: 4.537819
 19022/100000: episode: 446, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 51.303, mean reward: 6.413 [3.910, 8.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.382, 10.100], loss: 0.327684, mae: 0.546344, mean_q: 4.489303
 19031/100000: episode: 447, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 32.412, mean reward: 3.601 [2.890, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.277, 10.100], loss: 0.423974, mae: 0.540130, mean_q: 4.488827
 19037/100000: episode: 448, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 33.162, mean reward: 5.527 [3.932, 8.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.579, 10.100], loss: 0.512186, mae: 0.682034, mean_q: 4.687641
 19045/100000: episode: 449, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 33.017, mean reward: 4.127 [3.658, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.476, 10.100], loss: 4.727152, mae: 0.863322, mean_q: 4.786306
 19051/100000: episode: 450, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 18.951, mean reward: 3.158 [2.955, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.292, 10.100], loss: 0.406412, mae: 0.602162, mean_q: 4.265898
 19057/100000: episode: 451, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 19.379, mean reward: 3.230 [2.799, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.382, 10.100], loss: 0.308838, mae: 0.534597, mean_q: 4.560299
 19064/100000: episode: 452, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 38.200, mean reward: 5.457 [3.743, 10.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.229, 10.100], loss: 0.300644, mae: 0.528784, mean_q: 4.418399
 19071/100000: episode: 453, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 33.653, mean reward: 4.808 [3.679, 6.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.308, 10.100], loss: 0.177136, mae: 0.450535, mean_q: 4.408265
 19078/100000: episode: 454, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 27.604, mean reward: 3.943 [3.552, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.349, 10.100], loss: 0.230270, mae: 0.472818, mean_q: 4.375767
 19086/100000: episode: 455, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 61.198, mean reward: 7.650 [5.444, 10.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.456, 10.100], loss: 0.252899, mae: 0.492509, mean_q: 4.415748
 19095/100000: episode: 456, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 44.508, mean reward: 4.945 [3.346, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.386, 10.100], loss: 0.255964, mae: 0.494575, mean_q: 4.563601
 19102/100000: episode: 457, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 32.221, mean reward: 4.603 [3.268, 5.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.380, 10.100], loss: 0.220131, mae: 0.459398, mean_q: 4.321862
 19110/100000: episode: 458, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 42.369, mean reward: 5.296 [4.232, 7.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.429, 10.100], loss: 0.313005, mae: 0.522154, mean_q: 4.445497
 19118/100000: episode: 459, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 47.812, mean reward: 5.977 [4.869, 7.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.431, 10.100], loss: 0.349402, mae: 0.563168, mean_q: 4.537391
 19124/100000: episode: 460, duration: 0.044s, episode steps: 6, steps per second: 137, episode reward: 23.826, mean reward: 3.971 [3.533, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.363, 10.100], loss: 0.316732, mae: 0.559158, mean_q: 4.780911
 19131/100000: episode: 461, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 38.903, mean reward: 5.558 [4.170, 8.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.347, 10.100], loss: 0.222725, mae: 0.463560, mean_q: 4.428641
 19137/100000: episode: 462, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 19.323, mean reward: 3.221 [2.739, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.348, 10.100], loss: 1.015773, mae: 0.685275, mean_q: 4.515333
 19143/100000: episode: 463, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 30.745, mean reward: 5.124 [3.900, 6.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.222, 10.100], loss: 0.185564, mae: 0.443307, mean_q: 4.489228
 19149/100000: episode: 464, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 29.345, mean reward: 4.891 [3.506, 8.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.286, 10.100], loss: 0.335483, mae: 0.574783, mean_q: 4.569637
 19155/100000: episode: 465, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 25.689, mean reward: 4.281 [2.991, 6.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.351, 10.100], loss: 0.315789, mae: 0.532248, mean_q: 4.631001
 19161/100000: episode: 466, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 19.655, mean reward: 3.276 [2.967, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.364, 10.100], loss: 0.565779, mae: 0.810097, mean_q: 5.135286
 19170/100000: episode: 467, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 37.122, mean reward: 4.125 [3.534, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.706, 10.100], loss: 0.507763, mae: 0.673005, mean_q: 4.425768
 19176/100000: episode: 468, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 21.134, mean reward: 3.522 [2.850, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.316, 10.100], loss: 0.488554, mae: 0.654545, mean_q: 4.643420
 19182/100000: episode: 469, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 18.720, mean reward: 3.120 [2.779, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.403, 10.100], loss: 0.393850, mae: 0.559206, mean_q: 4.753305
 19191/100000: episode: 470, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 44.690, mean reward: 4.966 [3.597, 5.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.215, 10.100], loss: 1.493599, mae: 0.748149, mean_q: 4.799639
 19199/100000: episode: 471, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 44.455, mean reward: 5.557 [3.707, 9.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.353, 10.100], loss: 0.352550, mae: 0.602250, mean_q: 4.301236
 19208/100000: episode: 472, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 37.254, mean reward: 4.139 [2.163, 6.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.201, 10.100], loss: 0.375768, mae: 0.621311, mean_q: 4.840858
 19214/100000: episode: 473, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 24.495, mean reward: 4.083 [3.614, 4.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.358, 10.100], loss: 0.961878, mae: 0.631924, mean_q: 4.399577
 19220/100000: episode: 474, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 21.214, mean reward: 3.536 [2.821, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.315, 10.100], loss: 6.006643, mae: 0.989435, mean_q: 5.232391
 19226/100000: episode: 475, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 44.840, mean reward: 7.473 [4.991, 11.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.476, 10.100], loss: 0.822112, mae: 0.641184, mean_q: 4.797121
 19235/100000: episode: 476, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 31.052, mean reward: 3.450 [3.021, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.318, 10.100], loss: 0.379036, mae: 0.555853, mean_q: 4.727249
 19243/100000: episode: 477, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 45.596, mean reward: 5.699 [4.331, 6.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.241, 10.100], loss: 1.164941, mae: 0.601932, mean_q: 4.609712
 19250/100000: episode: 478, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 32.611, mean reward: 4.659 [3.854, 5.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.394, 10.100], loss: 0.352840, mae: 0.591633, mean_q: 4.672608
 19256/100000: episode: 479, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 26.501, mean reward: 4.417 [3.686, 5.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.560, 10.100], loss: 0.312490, mae: 0.541027, mean_q: 4.608203
 19265/100000: episode: 480, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 34.955, mean reward: 3.884 [3.318, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.273, 10.100], loss: 0.301199, mae: 0.530746, mean_q: 4.433350
 19274/100000: episode: 481, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 52.965, mean reward: 5.885 [4.054, 11.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.437, 10.100], loss: 1.071839, mae: 0.654796, mean_q: 4.959610
 19283/100000: episode: 482, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 29.884, mean reward: 3.320 [2.472, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.220, 10.100], loss: 0.800251, mae: 0.667646, mean_q: 4.858592
 19289/100000: episode: 483, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 18.621, mean reward: 3.104 [2.437, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.319, 10.100], loss: 0.339349, mae: 0.533697, mean_q: 4.630642
 19295/100000: episode: 484, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 20.144, mean reward: 3.357 [2.737, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.443, 10.100], loss: 5.082621, mae: 0.880314, mean_q: 5.008347
 19302/100000: episode: 485, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 33.989, mean reward: 4.856 [3.597, 7.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.275, 10.100], loss: 0.720112, mae: 0.739960, mean_q: 4.765911
 19309/100000: episode: 486, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 42.258, mean reward: 6.037 [4.425, 12.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.550, 10.100], loss: 4.322090, mae: 0.880033, mean_q: 4.974790
 19317/100000: episode: 487, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 37.806, mean reward: 4.726 [3.659, 6.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.364, 10.100], loss: 0.497628, mae: 0.697975, mean_q: 4.617270
 19324/100000: episode: 488, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 27.048, mean reward: 3.864 [3.246, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.244, 10.100], loss: 0.361035, mae: 0.596164, mean_q: 4.769512
 19333/100000: episode: 489, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 33.744, mean reward: 3.749 [3.100, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.329, 10.100], loss: 0.330027, mae: 0.539894, mean_q: 4.787234
 19341/100000: episode: 490, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 44.234, mean reward: 5.529 [4.212, 6.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.408, 10.100], loss: 0.334968, mae: 0.547613, mean_q: 4.769314
 19348/100000: episode: 491, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 32.070, mean reward: 4.581 [3.385, 5.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.260, 10.100], loss: 0.540751, mae: 0.588910, mean_q: 4.843469
 19355/100000: episode: 492, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 34.248, mean reward: 4.893 [3.429, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.316, 10.100], loss: 1.488899, mae: 0.791448, mean_q: 4.955100
 19361/100000: episode: 493, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 28.990, mean reward: 4.832 [4.118, 5.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.841, 10.100], loss: 0.246047, mae: 0.493061, mean_q: 4.577237
 19370/100000: episode: 494, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 34.543, mean reward: 3.838 [2.357, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.333, 10.100], loss: 0.446075, mae: 0.623162, mean_q: 4.967918
 19378/100000: episode: 495, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 34.763, mean reward: 4.345 [3.207, 7.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.348, 10.100], loss: 0.335808, mae: 0.543286, mean_q: 4.571965
 19386/100000: episode: 496, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 38.225, mean reward: 4.778 [2.873, 10.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.380, 10.100], loss: 0.524907, mae: 0.659217, mean_q: 4.977169
 19393/100000: episode: 497, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 33.627, mean reward: 4.804 [4.120, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.213, 10.100], loss: 0.328085, mae: 0.594052, mean_q: 5.009289
 19400/100000: episode: 498, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 27.292, mean reward: 3.899 [3.303, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.307, 10.100], loss: 0.424285, mae: 0.547844, mean_q: 4.644481
 19407/100000: episode: 499, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 28.148, mean reward: 4.021 [3.338, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.408, 10.100], loss: 0.283331, mae: 0.516385, mean_q: 4.690089
 19416/100000: episode: 500, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 31.894, mean reward: 3.544 [2.980, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.264, 10.100], loss: 3.487719, mae: 0.802662, mean_q: 4.988272
 19422/100000: episode: 501, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 32.511, mean reward: 5.418 [4.716, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.390, 10.100], loss: 0.379781, mae: 0.606232, mean_q: 4.716506
 19430/100000: episode: 502, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 47.543, mean reward: 5.943 [4.028, 7.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.410, 10.100], loss: 1.436287, mae: 0.650415, mean_q: 4.844664
 19437/100000: episode: 503, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 25.797, mean reward: 3.685 [3.325, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.343, 10.100], loss: 0.491146, mae: 0.647365, mean_q: 4.835612
 19443/100000: episode: 504, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 23.571, mean reward: 3.928 [3.378, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.373, 10.100], loss: 0.668243, mae: 0.681513, mean_q: 4.734768
 19449/100000: episode: 505, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 21.961, mean reward: 3.660 [2.853, 5.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.603, 10.100], loss: 0.438559, mae: 0.627825, mean_q: 5.248986
 19456/100000: episode: 506, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 29.345, mean reward: 4.192 [3.480, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.373, 10.100], loss: 0.423469, mae: 0.637761, mean_q: 4.440491
 19463/100000: episode: 507, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 26.066, mean reward: 3.724 [3.135, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.246, 10.100], loss: 0.522653, mae: 0.648299, mean_q: 4.824090
[Info] 5-TH LEVEL FOUND: 8.678908348083496, Considering 10/90 traces
 19471/100000: episode: 508, duration: 4.222s, episode steps: 8, steps per second: 2, episode reward: 34.237, mean reward: 4.280 [3.413, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.471, 10.100], loss: 0.368944, mae: 0.587533, mean_q: 4.945877
 19477/100000: episode: 509, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 28.809, mean reward: 4.801 [4.508, 5.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.368, 10.100], loss: 1.662116, mae: 0.730419, mean_q: 4.620541
 19483/100000: episode: 510, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 46.605, mean reward: 7.768 [6.013, 11.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.440, 10.100], loss: 6.160317, mae: 0.933042, mean_q: 5.231928
 19489/100000: episode: 511, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 29.046, mean reward: 4.841 [4.021, 6.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.466, 10.100], loss: 0.355535, mae: 0.594835, mean_q: 4.995932
 19496/100000: episode: 512, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 36.668, mean reward: 5.238 [3.816, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.256, 10.100], loss: 0.442409, mae: 0.615356, mean_q: 4.710613
 19503/100000: episode: 513, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 27.036, mean reward: 3.862 [3.285, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.356, 10.100], loss: 0.567248, mae: 0.624308, mean_q: 4.856709
 19509/100000: episode: 514, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 20.858, mean reward: 3.476 [2.701, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.329, 10.100], loss: 1.372538, mae: 0.707417, mean_q: 5.039109
 19516/100000: episode: 515, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 27.310, mean reward: 3.901 [3.361, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.398, 10.100], loss: 0.675975, mae: 0.624388, mean_q: 4.742116
 19523/100000: episode: 516, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 29.616, mean reward: 4.231 [3.741, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.358, 10.100], loss: 5.282177, mae: 0.996109, mean_q: 5.307099
 19530/100000: episode: 517, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 26.938, mean reward: 3.848 [3.358, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.395, 10.100], loss: 0.477465, mae: 0.633750, mean_q: 4.439524
 19537/100000: episode: 518, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 27.303, mean reward: 3.900 [3.386, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.402, 10.100], loss: 0.971681, mae: 0.793390, mean_q: 5.298579
 19543/100000: episode: 519, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 29.548, mean reward: 4.925 [3.175, 8.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.503, 10.100], loss: 0.389185, mae: 0.590935, mean_q: 4.719942
 19550/100000: episode: 520, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 31.782, mean reward: 4.540 [3.584, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.340, 10.100], loss: 4.234042, mae: 0.862074, mean_q: 5.360930
 19557/100000: episode: 521, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 28.992, mean reward: 4.142 [3.695, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.339, 10.100], loss: 0.506258, mae: 0.697390, mean_q: 5.234240
 19564/100000: episode: 522, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 38.484, mean reward: 5.498 [4.127, 7.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.373, 10.100], loss: 0.425614, mae: 0.603530, mean_q: 4.802630
 19571/100000: episode: 523, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 32.625, mean reward: 4.661 [3.854, 5.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.342, 10.100], loss: 0.437293, mae: 0.621368, mean_q: 5.056485
 19578/100000: episode: 524, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 45.934, mean reward: 6.562 [4.263, 12.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.427, 10.100], loss: 0.640995, mae: 0.642077, mean_q: 4.869356
 19585/100000: episode: 525, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 34.528, mean reward: 4.933 [4.312, 7.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.454, 10.100], loss: 4.256225, mae: 0.840902, mean_q: 5.231292
 19592/100000: episode: 526, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 36.237, mean reward: 5.177 [4.682, 5.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.397, 10.100], loss: 1.029512, mae: 0.716876, mean_q: 4.757133
 19599/100000: episode: 527, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 38.751, mean reward: 5.536 [2.962, 10.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.655, 10.100], loss: 4.425960, mae: 1.106705, mean_q: 5.773392
 19606/100000: episode: 528, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 37.086, mean reward: 5.298 [3.921, 10.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.524, 10.100], loss: 0.971889, mae: 0.825134, mean_q: 4.344706
 19613/100000: episode: 529, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 53.589, mean reward: 7.656 [4.548, 13.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.466, 10.100], loss: 0.650223, mae: 0.846104, mean_q: 5.662297
 19619/100000: episode: 530, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 42.420, mean reward: 7.070 [4.881, 11.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.307, 10.100], loss: 0.502514, mae: 0.680084, mean_q: 4.272746
 19625/100000: episode: 531, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 27.575, mean reward: 4.596 [3.510, 6.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.333, 10.100], loss: 0.332675, mae: 0.587631, mean_q: 5.306101
 19632/100000: episode: 532, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 31.127, mean reward: 4.447 [3.678, 5.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.372, 10.100], loss: 0.505275, mae: 0.656913, mean_q: 4.822369
 19638/100000: episode: 533, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 39.543, mean reward: 6.590 [5.012, 10.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.365, 10.100], loss: 0.683567, mae: 0.763410, mean_q: 5.185096
 19645/100000: episode: 534, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 45.824, mean reward: 6.546 [3.922, 8.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.366, 10.100], loss: 0.608087, mae: 0.599114, mean_q: 4.723553
 19651/100000: episode: 535, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 27.323, mean reward: 4.554 [3.877, 5.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.405, 10.100], loss: 1.115440, mae: 0.935809, mean_q: 5.503437
 19658/100000: episode: 536, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 37.770, mean reward: 5.396 [3.679, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.391, 10.100], loss: 0.582786, mae: 0.682616, mean_q: 4.326930
 19665/100000: episode: 537, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 34.587, mean reward: 4.941 [4.184, 6.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.651, 10.100], loss: 0.736223, mae: 0.790140, mean_q: 5.074736
 19671/100000: episode: 538, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 43.755, mean reward: 7.292 [5.046, 10.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.928, 10.100], loss: 0.427855, mae: 0.592451, mean_q: 4.805298
 19677/100000: episode: 539, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 29.265, mean reward: 4.878 [3.819, 6.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.282, 10.100], loss: 2.373743, mae: 0.996968, mean_q: 5.305366
 19683/100000: episode: 540, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 28.269, mean reward: 4.711 [3.972, 5.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.378, 10.100], loss: 0.479872, mae: 0.653273, mean_q: 4.972916
 19690/100000: episode: 541, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 31.002, mean reward: 4.429 [3.838, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.436, 10.100], loss: 1.716364, mae: 0.837172, mean_q: 5.269481
 19697/100000: episode: 542, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 43.762, mean reward: 6.252 [4.752, 7.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.468, 10.100], loss: 5.212165, mae: 0.889813, mean_q: 5.554166
 19704/100000: episode: 543, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 31.176, mean reward: 4.454 [3.898, 5.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.403, 10.100], loss: 0.419053, mae: 0.573717, mean_q: 4.787057
 19711/100000: episode: 544, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 39.760, mean reward: 5.680 [4.227, 7.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.367, 10.100], loss: 0.369898, mae: 0.586567, mean_q: 5.074729
 19718/100000: episode: 545, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 62.236, mean reward: 8.891 [7.040, 11.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.412, 10.100], loss: 0.484268, mae: 0.638336, mean_q: 5.196033
 19725/100000: episode: 546, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 28.706, mean reward: 4.101 [3.384, 5.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.381, 10.100], loss: 0.841871, mae: 0.655433, mean_q: 5.219791
 19732/100000: episode: 547, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 41.772, mean reward: 5.967 [5.003, 7.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.899, 10.100], loss: 0.281256, mae: 0.525667, mean_q: 5.009351
 19739/100000: episode: 548, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 32.668, mean reward: 4.667 [3.939, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.268, 10.100], loss: 0.737592, mae: 0.680001, mean_q: 5.191114
 19746/100000: episode: 549, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 52.614, mean reward: 7.516 [5.385, 10.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.395, 10.100], loss: 0.559187, mae: 0.700211, mean_q: 5.241632
 19753/100000: episode: 550, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 50.690, mean reward: 7.241 [6.176, 8.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.474, 10.100], loss: 0.613353, mae: 0.708466, mean_q: 5.121986
 19760/100000: episode: 551, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 46.741, mean reward: 6.677 [4.288, 8.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.365, 10.100], loss: 0.537825, mae: 0.690234, mean_q: 5.098308
 19767/100000: episode: 552, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 38.245, mean reward: 5.464 [4.071, 7.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.456, 10.100], loss: 0.734368, mae: 0.775752, mean_q: 5.551350
 19774/100000: episode: 553, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 50.851, mean reward: 7.264 [5.232, 10.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.397, 10.100], loss: 0.506370, mae: 0.652717, mean_q: 5.163356
 19780/100000: episode: 554, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 29.949, mean reward: 4.991 [4.111, 6.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.399, 10.100], loss: 0.442856, mae: 0.605287, mean_q: 5.271019
 19787/100000: episode: 555, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 29.042, mean reward: 4.149 [3.545, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.400, 10.100], loss: 5.406561, mae: 0.892958, mean_q: 5.248479
 19793/100000: episode: 556, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 30.380, mean reward: 5.063 [3.491, 8.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.349, 10.100], loss: 0.505804, mae: 0.707472, mean_q: 5.732623
 19800/100000: episode: 557, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 36.359, mean reward: 5.194 [4.519, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.372, 10.100], loss: 0.586378, mae: 0.719009, mean_q: 5.388196
 19806/100000: episode: 558, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 26.317, mean reward: 4.386 [3.477, 6.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.295, 10.100], loss: 0.472388, mae: 0.650292, mean_q: 5.422821
 19813/100000: episode: 559, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 57.847, mean reward: 8.264 [5.138, 14.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.344, 10.100], loss: 0.387712, mae: 0.599051, mean_q: 5.066738
 19819/100000: episode: 560, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 26.467, mean reward: 4.411 [3.581, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.376, 10.100], loss: 0.335121, mae: 0.581072, mean_q: 5.140876
 19826/100000: episode: 561, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 35.574, mean reward: 5.082 [4.506, 5.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.578, 10.100], loss: 1.501892, mae: 0.776945, mean_q: 5.460159
 19832/100000: episode: 562, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 25.030, mean reward: 4.172 [3.562, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.379, 10.100], loss: 0.680816, mae: 0.683399, mean_q: 5.272102
 19838/100000: episode: 563, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 26.469, mean reward: 4.411 [3.236, 5.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.617, 10.100], loss: 0.474294, mae: 0.700886, mean_q: 5.717984
 19845/100000: episode: 564, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 38.435, mean reward: 5.491 [3.851, 8.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.568, 10.100], loss: 0.657952, mae: 0.675803, mean_q: 5.366371
 19851/100000: episode: 565, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 35.898, mean reward: 5.983 [4.652, 7.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.323, 10.100], loss: 0.441772, mae: 0.675966, mean_q: 5.644997
 19857/100000: episode: 566, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 28.937, mean reward: 4.823 [3.500, 7.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.352, 10.100], loss: 0.439673, mae: 0.657858, mean_q: 5.271673
 19864/100000: episode: 567, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 36.002, mean reward: 5.143 [3.909, 6.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.400, 10.100], loss: 0.343886, mae: 0.564864, mean_q: 5.443118
 19871/100000: episode: 568, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 35.688, mean reward: 5.098 [4.130, 6.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.875, 10.100], loss: 0.297114, mae: 0.509900, mean_q: 4.960727
 19878/100000: episode: 569, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 26.108, mean reward: 3.730 [3.108, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.335, 10.100], loss: 1.491969, mae: 0.836852, mean_q: 5.748354
 19884/100000: episode: 570, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 38.133, mean reward: 6.355 [4.937, 7.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.388, 10.100], loss: 1.411338, mae: 0.891880, mean_q: 5.122145
 19891/100000: episode: 571, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 33.500, mean reward: 4.786 [4.156, 5.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.430, 10.100], loss: 1.095718, mae: 0.853199, mean_q: 5.687537
 19898/100000: episode: 572, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 30.920, mean reward: 4.417 [3.648, 5.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.432, 10.100], loss: 0.574017, mae: 0.629759, mean_q: 5.098374
 19905/100000: episode: 573, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 33.366, mean reward: 4.767 [3.106, 7.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.327, 10.100], loss: 0.736242, mae: 0.684303, mean_q: 5.501935
 19911/100000: episode: 574, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 24.329, mean reward: 4.055 [3.389, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.347, 10.100], loss: 1.292843, mae: 0.827775, mean_q: 5.347908
 19918/100000: episode: 575, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 27.577, mean reward: 3.940 [3.205, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.367, 10.100], loss: 0.501747, mae: 0.642019, mean_q: 5.314703
 19925/100000: episode: 576, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 43.950, mean reward: 6.279 [4.726, 7.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.359, 10.100], loss: 1.237881, mae: 0.885915, mean_q: 5.754553
 19931/100000: episode: 577, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 27.563, mean reward: 4.594 [3.614, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.412, 10.100], loss: 0.609329, mae: 0.679819, mean_q: 5.107785
 19937/100000: episode: 578, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 54.719, mean reward: 9.120 [3.029, 22.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.362, 10.100], loss: 6.066608, mae: 1.025823, mean_q: 5.334132
 19943/100000: episode: 579, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 25.103, mean reward: 4.184 [3.449, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.499, 10.100], loss: 1.534412, mae: 0.922941, mean_q: 5.571270
 19950/100000: episode: 580, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 28.742, mean reward: 4.106 [3.621, 4.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.465, 10.100], loss: 0.534190, mae: 0.673205, mean_q: 5.300651
 19957/100000: episode: 581, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 28.153, mean reward: 4.022 [3.019, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.291, 10.100], loss: 0.714003, mae: 0.713051, mean_q: 5.149641
 19964/100000: episode: 582, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 32.063, mean reward: 4.580 [3.632, 5.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.429, 10.100], loss: 0.402800, mae: 0.624737, mean_q: 5.539421
 19971/100000: episode: 583, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 44.697, mean reward: 6.385 [4.798, 10.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.548, 10.100], loss: 0.517886, mae: 0.666542, mean_q: 5.702237
 19978/100000: episode: 584, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 34.389, mean reward: 4.913 [3.789, 8.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.305, 10.100], loss: 5.395545, mae: 0.899457, mean_q: 5.625600
 19985/100000: episode: 585, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 29.557, mean reward: 4.222 [3.308, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.315, 10.100], loss: 0.729120, mae: 0.795320, mean_q: 5.598536
 19992/100000: episode: 586, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 37.616, mean reward: 5.374 [3.381, 7.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.380, 10.100], loss: 0.637509, mae: 0.668065, mean_q: 5.432254
 19999/100000: episode: 587, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 60.466, mean reward: 8.638 [5.182, 12.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.458, 10.100], loss: 0.628471, mae: 0.709021, mean_q: 5.259301
 20006/100000: episode: 588, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 38.113, mean reward: 5.445 [4.267, 6.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.343, 10.100], loss: 0.562052, mae: 0.678682, mean_q: 5.650534
 20013/100000: episode: 589, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 37.437, mean reward: 5.348 [4.835, 6.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.329, 10.100], loss: 0.495308, mae: 0.692615, mean_q: 5.306995
 20020/100000: episode: 590, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 40.253, mean reward: 5.750 [4.795, 7.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.346, 10.100], loss: 0.644806, mae: 0.821458, mean_q: 5.707357
 20027/100000: episode: 591, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 34.554, mean reward: 4.936 [3.869, 5.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.399, 10.100], loss: 0.598728, mae: 0.682747, mean_q: 5.108659
 20034/100000: episode: 592, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 33.752, mean reward: 4.822 [4.262, 5.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.433, 10.100], loss: 0.815391, mae: 0.818697, mean_q: 5.898430
 20040/100000: episode: 593, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 31.027, mean reward: 5.171 [4.105, 6.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.304, 10.100], loss: 0.634382, mae: 0.710543, mean_q: 5.129202
 20046/100000: episode: 594, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 23.936, mean reward: 3.989 [3.469, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.362, 10.100], loss: 1.251527, mae: 0.841454, mean_q: 5.962080
 20052/100000: episode: 595, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 27.450, mean reward: 4.575 [3.990, 6.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.550, 10.100], loss: 6.121084, mae: 0.957356, mean_q: 5.425486
 20059/100000: episode: 596, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 43.471, mean reward: 6.210 [3.366, 8.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.208, 10.100], loss: 0.904366, mae: 0.855079, mean_q: 5.736145
 20065/100000: episode: 597, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 28.923, mean reward: 4.821 [3.756, 5.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.393, 10.100], loss: 1.410816, mae: 0.913670, mean_q: 5.562286
[Info] 6-TH LEVEL FOUND: 10.09494686126709, Considering 10/90 traces
 20071/100000: episode: 598, duration: 4.215s, episode steps: 6, steps per second: 1, episode reward: 32.958, mean reward: 5.493 [2.614, 10.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.346, 10.100], loss: 0.531530, mae: 0.721454, mean_q: 5.789214
 20076/100000: episode: 599, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 29.707, mean reward: 5.941 [3.332, 11.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.466, 10.100], loss: 0.476560, mae: 0.634518, mean_q: 5.259164
 20081/100000: episode: 600, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 29.381, mean reward: 5.876 [4.280, 8.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.554, 10.100], loss: 0.804204, mae: 0.877303, mean_q: 5.836466
 20087/100000: episode: 601, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 48.244, mean reward: 8.041 [5.507, 11.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.461, 10.100], loss: 0.382611, mae: 0.616813, mean_q: 5.759640
 20092/100000: episode: 602, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 32.088, mean reward: 6.418 [4.883, 8.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.526, 10.100], loss: 0.397131, mae: 0.616978, mean_q: 5.184984
 20097/100000: episode: 603, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 20.716, mean reward: 4.143 [2.668, 5.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.354, 10.100], loss: 0.836566, mae: 0.821738, mean_q: 6.018962
 20102/100000: episode: 604, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 31.770, mean reward: 6.354 [4.633, 7.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.503, 10.100], loss: 0.742060, mae: 0.714092, mean_q: 5.588706
 20107/100000: episode: 605, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 25.597, mean reward: 5.119 [3.041, 8.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.526, 10.100], loss: 7.302234, mae: 1.220340, mean_q: 5.910584
 20113/100000: episode: 606, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 29.971, mean reward: 4.995 [3.933, 7.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.421, 10.100], loss: 0.846259, mae: 0.776566, mean_q: 5.343439
 20119/100000: episode: 607, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 25.722, mean reward: 4.287 [3.481, 5.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.561, 10.100], loss: 0.647414, mae: 0.790758, mean_q: 6.022831
 20125/100000: episode: 608, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 28.229, mean reward: 4.705 [3.372, 7.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.394, 10.100], loss: 0.663590, mae: 0.747835, mean_q: 5.361931
 20131/100000: episode: 609, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 25.217, mean reward: 4.203 [2.991, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.535, 10.100], loss: 1.193499, mae: 0.915172, mean_q: 5.920860
 20137/100000: episode: 610, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 42.350, mean reward: 7.058 [5.060, 8.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.348, 10.100], loss: 0.870177, mae: 0.786694, mean_q: 5.684676
 20142/100000: episode: 611, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 75.548, mean reward: 15.110 [8.940, 31.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.376, 10.100], loss: 1.311219, mae: 0.753230, mean_q: 5.446179
[Info] FALSIFICATION!
[Info] Levels: [4.138765, 4.384673, 4.73769, 6.8344917, 8.678908, 10.094947, 11.236856]
[Info] Cond. Prob: [0.1, 0.12, 0.1, 0.1, 0.1, 0.1, 0.17]
[Info] Error Prob: 2.0400000000000008e-07

 20143/100000: episode: 612, duration: 4.650s, episode steps: 1, steps per second: 0, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.421, 10.067], loss: 0.371573, mae: 0.644125, mean_q: 6.152680
 20243/100000: episode: 613, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 188.672, mean reward: 1.887 [1.449, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.099, 10.251], loss: 0.831199, mae: 0.738348, mean_q: 5.680155
 20343/100000: episode: 614, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.119, mean reward: 1.851 [1.451, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.583, 10.098], loss: 2.240710, mae: 0.795878, mean_q: 5.598098
 20443/100000: episode: 615, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 194.897, mean reward: 1.949 [1.449, 5.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.749, 10.098], loss: 2.813032, mae: 0.932004, mean_q: 5.743906
 20543/100000: episode: 616, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 182.415, mean reward: 1.824 [1.443, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.177, 10.098], loss: 4.259457, mae: 0.906956, mean_q: 5.817479
 20643/100000: episode: 617, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 203.289, mean reward: 2.033 [1.477, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.089, 10.098], loss: 0.988326, mae: 0.740557, mean_q: 5.572153
 20743/100000: episode: 618, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 181.842, mean reward: 1.818 [1.489, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.496, 10.146], loss: 6.141235, mae: 1.007010, mean_q: 5.811977
 20843/100000: episode: 619, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.672, mean reward: 1.907 [1.456, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.714, 10.161], loss: 1.272413, mae: 0.779054, mean_q: 5.568346
 20943/100000: episode: 620, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 192.504, mean reward: 1.925 [1.466, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.775, 10.098], loss: 2.345074, mae: 0.807506, mean_q: 5.593640
 21043/100000: episode: 621, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 197.001, mean reward: 1.970 [1.445, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.178], loss: 1.492055, mae: 0.764547, mean_q: 5.745130
 21143/100000: episode: 622, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.085, mean reward: 1.891 [1.473, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.835, 10.244], loss: 0.651924, mae: 0.686119, mean_q: 5.577295
 21243/100000: episode: 623, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.848, mean reward: 1.868 [1.502, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.424, 10.098], loss: 1.120363, mae: 0.777235, mean_q: 5.663589
 21343/100000: episode: 624, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 300.303, mean reward: 3.003 [1.485, 9.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.395, 10.098], loss: 2.981976, mae: 0.839228, mean_q: 5.722189
 21443/100000: episode: 625, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.311, mean reward: 1.873 [1.460, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.774, 10.098], loss: 0.868758, mae: 0.723064, mean_q: 5.603982
 21543/100000: episode: 626, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.804, mean reward: 1.928 [1.446, 6.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.359, 10.098], loss: 1.327150, mae: 0.732295, mean_q: 5.582724
 21643/100000: episode: 627, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 187.099, mean reward: 1.871 [1.492, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.340, 10.118], loss: 2.779263, mae: 0.838020, mean_q: 5.692466
 21743/100000: episode: 628, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 185.300, mean reward: 1.853 [1.475, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.109, 10.098], loss: 1.896621, mae: 0.755513, mean_q: 5.692183
 21843/100000: episode: 629, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 193.867, mean reward: 1.939 [1.446, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.105, 10.098], loss: 0.722594, mae: 0.692382, mean_q: 5.624787
 21943/100000: episode: 630, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 173.593, mean reward: 1.736 [1.445, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.025, 10.181], loss: 1.238292, mae: 0.768625, mean_q: 5.575026
 22043/100000: episode: 631, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 198.637, mean reward: 1.986 [1.486, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.198, 10.098], loss: 1.975796, mae: 0.790875, mean_q: 5.621834
 22143/100000: episode: 632, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.990, mean reward: 1.940 [1.448, 4.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.998, 10.098], loss: 1.196800, mae: 0.725394, mean_q: 5.579176
 22243/100000: episode: 633, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 199.784, mean reward: 1.998 [1.440, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.769, 10.231], loss: 3.646833, mae: 0.848192, mean_q: 5.588802
 22343/100000: episode: 634, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 188.894, mean reward: 1.889 [1.491, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.750, 10.389], loss: 1.409332, mae: 0.764099, mean_q: 5.685416
 22443/100000: episode: 635, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 199.524, mean reward: 1.995 [1.462, 6.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.104, 10.098], loss: 2.111606, mae: 0.738408, mean_q: 5.583838
 22543/100000: episode: 636, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 186.721, mean reward: 1.867 [1.459, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.098], loss: 2.243138, mae: 0.752807, mean_q: 5.572174
 22643/100000: episode: 637, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 195.518, mean reward: 1.955 [1.450, 7.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.924, 10.098], loss: 4.706837, mae: 0.851895, mean_q: 5.595128
 22743/100000: episode: 638, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 187.551, mean reward: 1.876 [1.462, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.121], loss: 3.002860, mae: 0.845860, mean_q: 5.592679
 22843/100000: episode: 639, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 197.083, mean reward: 1.971 [1.508, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.659, 10.214], loss: 0.743765, mae: 0.671158, mean_q: 5.546342
 22943/100000: episode: 640, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 196.030, mean reward: 1.960 [1.446, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.597, 10.141], loss: 0.984864, mae: 0.684529, mean_q: 5.437363
 23043/100000: episode: 641, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.422, mean reward: 1.934 [1.443, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.782, 10.115], loss: 1.088368, mae: 0.664164, mean_q: 5.418908
 23143/100000: episode: 642, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 178.774, mean reward: 1.788 [1.490, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.294, 10.098], loss: 1.034813, mae: 0.668033, mean_q: 5.391419
 23243/100000: episode: 643, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 191.721, mean reward: 1.917 [1.447, 2.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.874, 10.228], loss: 1.245211, mae: 0.670786, mean_q: 5.371855
 23343/100000: episode: 644, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 184.699, mean reward: 1.847 [1.449, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.641, 10.098], loss: 0.957733, mae: 0.629307, mean_q: 5.285555
 23443/100000: episode: 645, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 187.687, mean reward: 1.877 [1.496, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.475, 10.098], loss: 0.931060, mae: 0.599393, mean_q: 5.179365
 23543/100000: episode: 646, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 199.936, mean reward: 1.999 [1.486, 4.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.436, 10.098], loss: 0.709195, mae: 0.597118, mean_q: 5.186108
 23643/100000: episode: 647, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 175.147, mean reward: 1.751 [1.439, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.551, 10.132], loss: 1.908795, mae: 0.634875, mean_q: 5.137012
 23743/100000: episode: 648, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 204.475, mean reward: 2.045 [1.474, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.107], loss: 0.724339, mae: 0.586328, mean_q: 5.105648
 23843/100000: episode: 649, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 232.774, mean reward: 2.328 [1.506, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.306, 10.444], loss: 0.580163, mae: 0.546241, mean_q: 5.003421
 23943/100000: episode: 650, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 184.269, mean reward: 1.843 [1.484, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.611, 10.098], loss: 1.691135, mae: 0.585166, mean_q: 4.901409
 24043/100000: episode: 651, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 214.382, mean reward: 2.144 [1.542, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.459, 10.375], loss: 2.961359, mae: 0.645073, mean_q: 4.842106
 24143/100000: episode: 652, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 201.256, mean reward: 2.013 [1.514, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.662, 10.098], loss: 1.601298, mae: 0.508915, mean_q: 4.707210
 24243/100000: episode: 653, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.932, mean reward: 1.969 [1.459, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.719, 10.357], loss: 0.326592, mae: 0.458638, mean_q: 4.651379
 24343/100000: episode: 654, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 187.150, mean reward: 1.871 [1.455, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.499, 10.098], loss: 1.776575, mae: 0.516398, mean_q: 4.639502
 24443/100000: episode: 655, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 194.324, mean reward: 1.943 [1.453, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.274, 10.192], loss: 0.366681, mae: 0.453594, mean_q: 4.478589
 24543/100000: episode: 656, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 201.579, mean reward: 2.016 [1.460, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.444, 10.098], loss: 1.667925, mae: 0.501925, mean_q: 4.488408
 24643/100000: episode: 657, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.254, mean reward: 1.853 [1.442, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.601, 10.098], loss: 0.426778, mae: 0.420548, mean_q: 4.325755
 24743/100000: episode: 658, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 204.104, mean reward: 2.041 [1.503, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.264, 10.098], loss: 1.549786, mae: 0.437789, mean_q: 4.276353
 24843/100000: episode: 659, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.688, mean reward: 1.877 [1.450, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.793, 10.098], loss: 0.250764, mae: 0.353401, mean_q: 4.099452
 24943/100000: episode: 660, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 197.717, mean reward: 1.977 [1.437, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.024, 10.293], loss: 0.277561, mae: 0.362851, mean_q: 4.027248
 25043/100000: episode: 661, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 202.270, mean reward: 2.023 [1.452, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.682, 10.169], loss: 1.420446, mae: 0.404159, mean_q: 3.977096
 25143/100000: episode: 662, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.171, mean reward: 1.892 [1.474, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.354, 10.098], loss: 0.207481, mae: 0.319564, mean_q: 3.862109
 25243/100000: episode: 663, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.961, mean reward: 1.970 [1.433, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.250], loss: 0.100745, mae: 0.298958, mean_q: 3.847409
 25343/100000: episode: 664, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 197.761, mean reward: 1.978 [1.505, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.135, 10.098], loss: 0.090384, mae: 0.294378, mean_q: 3.845877
 25443/100000: episode: 665, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.098, mean reward: 1.861 [1.456, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.983, 10.164], loss: 0.109608, mae: 0.307101, mean_q: 3.869839
 25543/100000: episode: 666, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 202.616, mean reward: 2.026 [1.442, 5.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.675, 10.351], loss: 0.106013, mae: 0.303461, mean_q: 3.870850
 25643/100000: episode: 667, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 201.044, mean reward: 2.010 [1.443, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.765, 10.130], loss: 0.100126, mae: 0.300587, mean_q: 3.872822
 25743/100000: episode: 668, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 196.419, mean reward: 1.964 [1.486, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.460, 10.098], loss: 0.089331, mae: 0.301157, mean_q: 3.875111
 25843/100000: episode: 669, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 222.018, mean reward: 2.220 [1.466, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.627, 10.098], loss: 0.090850, mae: 0.301351, mean_q: 3.882890
 25943/100000: episode: 670, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 201.924, mean reward: 2.019 [1.503, 5.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.514, 10.289], loss: 0.083155, mae: 0.293313, mean_q: 3.863245
 26043/100000: episode: 671, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 203.553, mean reward: 2.036 [1.449, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.086, 10.436], loss: 0.092682, mae: 0.297348, mean_q: 3.876212
 26143/100000: episode: 672, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.991, mean reward: 1.960 [1.497, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.920, 10.098], loss: 0.115026, mae: 0.319664, mean_q: 3.899951
 26243/100000: episode: 673, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 222.007, mean reward: 2.220 [1.455, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.791, 10.098], loss: 0.130666, mae: 0.327345, mean_q: 3.909246
 26343/100000: episode: 674, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 203.308, mean reward: 2.033 [1.445, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.234, 10.127], loss: 0.103412, mae: 0.315605, mean_q: 3.890398
 26443/100000: episode: 675, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 212.911, mean reward: 2.129 [1.435, 5.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.894, 10.192], loss: 0.098217, mae: 0.307792, mean_q: 3.882323
 26543/100000: episode: 676, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 200.348, mean reward: 2.003 [1.460, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.007, 10.323], loss: 0.089444, mae: 0.297706, mean_q: 3.878140
 26643/100000: episode: 677, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 190.628, mean reward: 1.906 [1.471, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.647, 10.114], loss: 0.109787, mae: 0.321992, mean_q: 3.895239
 26743/100000: episode: 678, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 187.516, mean reward: 1.875 [1.484, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.642, 10.098], loss: 0.091630, mae: 0.307411, mean_q: 3.894997
 26843/100000: episode: 679, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 175.378, mean reward: 1.754 [1.451, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.099], loss: 0.107714, mae: 0.319061, mean_q: 3.899536
 26943/100000: episode: 680, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 191.922, mean reward: 1.919 [1.531, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.378, 10.106], loss: 0.091273, mae: 0.302585, mean_q: 3.892044
 27043/100000: episode: 681, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 215.782, mean reward: 2.158 [1.482, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.842, 10.098], loss: 0.097442, mae: 0.306154, mean_q: 3.893383
 27143/100000: episode: 682, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.061, mean reward: 1.971 [1.480, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.474, 10.204], loss: 0.094098, mae: 0.301909, mean_q: 3.899452
 27243/100000: episode: 683, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 182.413, mean reward: 1.824 [1.459, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.857, 10.226], loss: 0.080747, mae: 0.291342, mean_q: 3.889498
 27343/100000: episode: 684, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 196.315, mean reward: 1.963 [1.495, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.283], loss: 0.102189, mae: 0.315850, mean_q: 3.904843
 27443/100000: episode: 685, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 190.432, mean reward: 1.904 [1.431, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.467, 10.236], loss: 0.088402, mae: 0.305142, mean_q: 3.892693
 27543/100000: episode: 686, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.843, mean reward: 1.918 [1.459, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.074, 10.098], loss: 0.091945, mae: 0.300231, mean_q: 3.881193
 27643/100000: episode: 687, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 195.480, mean reward: 1.955 [1.462, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.764, 10.098], loss: 0.080858, mae: 0.289779, mean_q: 3.894157
 27743/100000: episode: 688, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 182.699, mean reward: 1.827 [1.456, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.379, 10.143], loss: 0.079492, mae: 0.283342, mean_q: 3.857539
 27843/100000: episode: 689, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 194.303, mean reward: 1.943 [1.445, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.584, 10.462], loss: 0.092382, mae: 0.298497, mean_q: 3.875523
 27943/100000: episode: 690, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 211.833, mean reward: 2.118 [1.457, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.923, 10.098], loss: 0.090336, mae: 0.299557, mean_q: 3.894209
 28043/100000: episode: 691, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.705, mean reward: 1.957 [1.452, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.098], loss: 0.090544, mae: 0.302984, mean_q: 3.898116
 28143/100000: episode: 692, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 199.698, mean reward: 1.997 [1.490, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.716, 10.280], loss: 0.082949, mae: 0.291636, mean_q: 3.894507
 28243/100000: episode: 693, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.826, mean reward: 1.828 [1.447, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.329, 10.098], loss: 0.096767, mae: 0.311872, mean_q: 3.901737
 28343/100000: episode: 694, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.000, mean reward: 1.910 [1.447, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.695, 10.402], loss: 0.090578, mae: 0.301137, mean_q: 3.903253
 28443/100000: episode: 695, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 199.427, mean reward: 1.994 [1.460, 6.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.173, 10.098], loss: 0.091510, mae: 0.305684, mean_q: 3.893104
 28543/100000: episode: 696, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.616, mean reward: 1.966 [1.487, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.760, 10.098], loss: 0.086347, mae: 0.299781, mean_q: 3.903434
 28643/100000: episode: 697, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.719, mean reward: 1.997 [1.436, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.774, 10.098], loss: 0.083197, mae: 0.291778, mean_q: 3.898711
 28743/100000: episode: 698, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 192.154, mean reward: 1.922 [1.457, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.546, 10.147], loss: 0.084717, mae: 0.296254, mean_q: 3.892127
 28843/100000: episode: 699, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 189.574, mean reward: 1.896 [1.475, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.098], loss: 0.084120, mae: 0.292371, mean_q: 3.893326
 28943/100000: episode: 700, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 201.527, mean reward: 2.015 [1.471, 4.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.575, 10.137], loss: 0.083696, mae: 0.290170, mean_q: 3.881170
 29043/100000: episode: 701, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.796, mean reward: 1.838 [1.483, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.112, 10.233], loss: 0.094367, mae: 0.303781, mean_q: 3.905374
 29143/100000: episode: 702, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.733, mean reward: 1.867 [1.481, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.731, 10.262], loss: 0.092859, mae: 0.301279, mean_q: 3.892100
 29243/100000: episode: 703, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.952, mean reward: 2.040 [1.526, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.162, 10.516], loss: 0.092076, mae: 0.301690, mean_q: 3.896978
 29343/100000: episode: 704, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.786, mean reward: 1.958 [1.493, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.363, 10.151], loss: 0.088695, mae: 0.296861, mean_q: 3.886187
 29443/100000: episode: 705, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.646, mean reward: 1.906 [1.502, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.513, 10.157], loss: 0.098333, mae: 0.301810, mean_q: 3.886386
 29543/100000: episode: 706, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.701, mean reward: 1.887 [1.455, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.055, 10.237], loss: 0.090655, mae: 0.292094, mean_q: 3.871942
 29643/100000: episode: 707, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.383, mean reward: 1.914 [1.462, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.138], loss: 0.087185, mae: 0.293962, mean_q: 3.890772
 29743/100000: episode: 708, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 199.629, mean reward: 1.996 [1.476, 5.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.457, 10.098], loss: 0.095257, mae: 0.305496, mean_q: 3.886453
 29843/100000: episode: 709, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.199, mean reward: 1.852 [1.451, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.180, 10.098], loss: 0.087050, mae: 0.297343, mean_q: 3.870377
 29943/100000: episode: 710, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.288, mean reward: 1.873 [1.439, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.300], loss: 0.087710, mae: 0.293326, mean_q: 3.877184
 30043/100000: episode: 711, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.460, mean reward: 1.915 [1.479, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.241, 10.098], loss: 0.091482, mae: 0.304383, mean_q: 3.874940
[Info] 1-TH LEVEL FOUND: 5.4076337814331055, Considering 10/90 traces
 30143/100000: episode: 712, duration: 4.691s, episode steps: 100, steps per second: 21, episode reward: 208.542, mean reward: 2.085 [1.488, 5.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.577, 10.525], loss: 0.081778, mae: 0.284508, mean_q: 3.841153
 30155/100000: episode: 713, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 28.381, mean reward: 2.365 [1.833, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.202, 10.100], loss: 0.111022, mae: 0.318972, mean_q: 3.943158
 30167/100000: episode: 714, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 37.336, mean reward: 3.111 [2.140, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.372, 10.100], loss: 0.094729, mae: 0.317230, mean_q: 3.860345
 30204/100000: episode: 715, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 104.006, mean reward: 2.811 [2.067, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.986, 10.100], loss: 0.099069, mae: 0.307253, mean_q: 3.874046
 30241/100000: episode: 716, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 153.282, mean reward: 4.143 [2.062, 7.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.040, 10.100], loss: 0.093783, mae: 0.308532, mean_q: 3.892015
 30249/100000: episode: 717, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 21.188, mean reward: 2.648 [2.502, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.363, 10.100], loss: 0.077181, mae: 0.279173, mean_q: 3.909981
 30261/100000: episode: 718, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 37.949, mean reward: 3.162 [2.453, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.146, 10.100], loss: 0.111983, mae: 0.319862, mean_q: 3.938922
 30270/100000: episode: 719, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 18.298, mean reward: 2.033 [1.791, 2.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.181, 10.100], loss: 0.090615, mae: 0.290703, mean_q: 3.922659
 30307/100000: episode: 720, duration: 0.207s, episode steps: 37, steps per second: 179, episode reward: 118.796, mean reward: 3.211 [2.110, 5.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.512, 10.100], loss: 0.125764, mae: 0.329323, mean_q: 3.947053
 30344/100000: episode: 721, duration: 0.206s, episode steps: 37, steps per second: 179, episode reward: 143.394, mean reward: 3.876 [2.469, 7.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.269, 10.100], loss: 0.106680, mae: 0.316081, mean_q: 3.944933
 30356/100000: episode: 722, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 24.508, mean reward: 2.042 [1.709, 2.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.111, 10.100], loss: 0.123792, mae: 0.329004, mean_q: 3.984396
 30365/100000: episode: 723, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 18.622, mean reward: 2.069 [1.924, 2.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.203, 10.100], loss: 0.105333, mae: 0.316976, mean_q: 3.934789
 30402/100000: episode: 724, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 161.892, mean reward: 4.375 [2.312, 26.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.349, 10.100], loss: 0.338906, mae: 0.394159, mean_q: 3.996247
[Info] FALSIFICATION!
[Info] Levels: [5.407634, 10.0901]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

 30418/100000: episode: 725, duration: 4.300s, episode steps: 16, steps per second: 4, episode reward: 203.703, mean reward: 12.731 [2.663, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.822 [-0.324, 9.042], loss: 0.122113, mae: 0.360894, mean_q: 4.006569
 30518/100000: episode: 726, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.797, mean reward: 1.838 [1.447, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.535, 10.166], loss: 0.144970, mae: 0.360665, mean_q: 4.037590
 30618/100000: episode: 727, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.642, mean reward: 1.846 [1.459, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.121, 10.124], loss: 0.301290, mae: 0.361091, mean_q: 4.002172
 30718/100000: episode: 728, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 177.224, mean reward: 1.772 [1.445, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.681, 10.098], loss: 0.355330, mae: 0.436302, mean_q: 4.003810
 30818/100000: episode: 729, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 191.901, mean reward: 1.919 [1.457, 5.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.814, 10.273], loss: 1.485741, mae: 0.429471, mean_q: 4.041057
 30918/100000: episode: 730, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 218.935, mean reward: 2.189 [1.478, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.815, 10.212], loss: 1.636717, mae: 0.497150, mean_q: 4.070019
 31018/100000: episode: 731, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.052, mean reward: 1.881 [1.503, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.098], loss: 0.213200, mae: 0.351878, mean_q: 3.958852
 31118/100000: episode: 732, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 214.546, mean reward: 2.145 [1.512, 7.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.184, 10.098], loss: 1.520108, mae: 0.458133, mean_q: 4.048899
 31218/100000: episode: 733, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 186.030, mean reward: 1.860 [1.451, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.093, 10.226], loss: 0.378812, mae: 0.380326, mean_q: 4.028449
 31318/100000: episode: 734, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 231.387, mean reward: 2.314 [1.518, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.959, 10.337], loss: 0.180613, mae: 0.331179, mean_q: 3.994428
 31418/100000: episode: 735, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.204, mean reward: 1.862 [1.483, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.329, 10.139], loss: 0.186911, mae: 0.344363, mean_q: 4.013495
 31518/100000: episode: 736, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 206.409, mean reward: 2.064 [1.476, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.298, 10.098], loss: 1.468292, mae: 0.421972, mean_q: 4.034128
 31618/100000: episode: 737, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 192.136, mean reward: 1.921 [1.459, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.327, 10.225], loss: 1.308122, mae: 0.394522, mean_q: 4.021187
 31718/100000: episode: 738, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.328, mean reward: 1.923 [1.452, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.936, 10.098], loss: 0.255695, mae: 0.368002, mean_q: 4.008089
 31818/100000: episode: 739, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.183, mean reward: 1.802 [1.458, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.665, 10.151], loss: 0.105685, mae: 0.310464, mean_q: 3.968985
 31918/100000: episode: 740, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.620, mean reward: 1.896 [1.462, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.270], loss: 1.331411, mae: 0.389250, mean_q: 4.044924
 32018/100000: episode: 741, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 189.866, mean reward: 1.899 [1.465, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.756, 10.135], loss: 0.239779, mae: 0.367316, mean_q: 3.973508
 32118/100000: episode: 742, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.183, mean reward: 1.852 [1.435, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.098], loss: 0.111457, mae: 0.327016, mean_q: 3.993654
 32218/100000: episode: 743, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 196.183, mean reward: 1.962 [1.506, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.117, 10.098], loss: 1.388869, mae: 0.406713, mean_q: 4.011071
 32318/100000: episode: 744, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 199.733, mean reward: 1.997 [1.480, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.449, 10.098], loss: 0.407169, mae: 0.377001, mean_q: 4.033792
 32418/100000: episode: 745, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 201.409, mean reward: 2.014 [1.485, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.430, 10.363], loss: 0.196521, mae: 0.337204, mean_q: 4.022866
 32518/100000: episode: 746, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.410, mean reward: 1.944 [1.488, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.125], loss: 1.366795, mae: 0.416620, mean_q: 4.039417
 32618/100000: episode: 747, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 180.878, mean reward: 1.809 [1.482, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.247, 10.215], loss: 0.251232, mae: 0.338441, mean_q: 4.002699
 32718/100000: episode: 748, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.169, mean reward: 2.052 [1.452, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.650, 10.109], loss: 0.102039, mae: 0.314287, mean_q: 4.022046
 32818/100000: episode: 749, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.888, mean reward: 1.829 [1.454, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.734, 10.196], loss: 1.417391, mae: 0.427112, mean_q: 4.047007
 32918/100000: episode: 750, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 191.241, mean reward: 1.912 [1.476, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.098], loss: 0.265824, mae: 0.336828, mean_q: 4.002194
 33018/100000: episode: 751, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 196.168, mean reward: 1.962 [1.473, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.355, 10.152], loss: 1.383152, mae: 0.387496, mean_q: 4.051906
 33118/100000: episode: 752, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.250, mean reward: 1.922 [1.458, 4.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.098], loss: 1.395593, mae: 0.416752, mean_q: 4.047231
 33218/100000: episode: 753, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.845, mean reward: 1.858 [1.432, 4.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.395, 10.098], loss: 2.614290, mae: 0.513354, mean_q: 4.068908
 33318/100000: episode: 754, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 197.780, mean reward: 1.978 [1.458, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.447], loss: 1.620036, mae: 0.466389, mean_q: 4.065313
 33418/100000: episode: 755, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 185.434, mean reward: 1.854 [1.456, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.280, 10.098], loss: 0.343360, mae: 0.375646, mean_q: 4.052790
 33518/100000: episode: 756, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.272, mean reward: 1.853 [1.454, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.108, 10.179], loss: 0.283189, mae: 0.354145, mean_q: 4.016111
 33618/100000: episode: 757, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 196.848, mean reward: 1.968 [1.475, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.826, 10.186], loss: 0.172735, mae: 0.313644, mean_q: 3.974593
 33718/100000: episode: 758, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 187.949, mean reward: 1.879 [1.469, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.527, 10.184], loss: 0.160961, mae: 0.313181, mean_q: 3.968037
 33818/100000: episode: 759, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 220.685, mean reward: 2.207 [1.465, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.905, 10.128], loss: 1.500456, mae: 0.417363, mean_q: 4.049922
 33918/100000: episode: 760, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.410, mean reward: 1.784 [1.465, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.536, 10.098], loss: 3.387102, mae: 0.491509, mean_q: 4.032232
 34018/100000: episode: 761, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 179.040, mean reward: 1.790 [1.451, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.030, 10.098], loss: 1.307872, mae: 0.427606, mean_q: 4.076761
 34118/100000: episode: 762, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.737, mean reward: 1.897 [1.463, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.574, 10.098], loss: 0.344229, mae: 0.390769, mean_q: 4.054971
 34218/100000: episode: 763, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.470, mean reward: 1.965 [1.488, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.490, 10.292], loss: 0.193858, mae: 0.331904, mean_q: 4.023006
 34318/100000: episode: 764, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 203.352, mean reward: 2.034 [1.472, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.492, 10.098], loss: 0.263981, mae: 0.328638, mean_q: 4.008829
 34418/100000: episode: 765, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 185.005, mean reward: 1.850 [1.451, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.925, 10.180], loss: 0.268888, mae: 0.352289, mean_q: 4.039496
 34518/100000: episode: 766, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.644, mean reward: 1.846 [1.451, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.926, 10.223], loss: 0.250713, mae: 0.329729, mean_q: 3.976298
 34618/100000: episode: 767, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 223.627, mean reward: 2.236 [1.465, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.309, 10.098], loss: 2.517554, mae: 0.520180, mean_q: 4.053814
 34718/100000: episode: 768, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.099, mean reward: 1.951 [1.447, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.093, 10.111], loss: 0.271538, mae: 0.352719, mean_q: 4.024627
 34818/100000: episode: 769, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.500, mean reward: 1.895 [1.451, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.004, 10.214], loss: 0.182757, mae: 0.331464, mean_q: 4.008624
 34918/100000: episode: 770, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.007, mean reward: 1.920 [1.459, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.403, 10.098], loss: 0.227038, mae: 0.326368, mean_q: 4.013603
 35018/100000: episode: 771, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.571, mean reward: 1.866 [1.471, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.963, 10.198], loss: 1.303264, mae: 0.440252, mean_q: 4.050355
 35118/100000: episode: 772, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 206.205, mean reward: 2.062 [1.474, 6.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.083, 10.237], loss: 1.345413, mae: 0.446516, mean_q: 4.015896
 35218/100000: episode: 773, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 188.844, mean reward: 1.888 [1.465, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.715, 10.127], loss: 1.209547, mae: 0.353533, mean_q: 3.982792
 35318/100000: episode: 774, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.184, mean reward: 1.942 [1.436, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.843, 10.118], loss: 0.107303, mae: 0.316718, mean_q: 3.884756
 35418/100000: episode: 775, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 204.673, mean reward: 2.047 [1.546, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.357, 10.181], loss: 0.071963, mae: 0.275177, mean_q: 3.855338
 35518/100000: episode: 776, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.945, mean reward: 1.959 [1.474, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.173], loss: 0.071052, mae: 0.272606, mean_q: 3.847735
 35618/100000: episode: 777, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 187.363, mean reward: 1.874 [1.462, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.945, 10.218], loss: 0.083141, mae: 0.288852, mean_q: 3.855239
 35718/100000: episode: 778, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 218.271, mean reward: 2.183 [1.456, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.602, 10.129], loss: 0.074457, mae: 0.276887, mean_q: 3.842822
 35818/100000: episode: 779, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.187, mean reward: 1.932 [1.431, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.456, 10.098], loss: 0.081280, mae: 0.288615, mean_q: 3.884636
 35918/100000: episode: 780, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.805, mean reward: 1.948 [1.503, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.693, 10.098], loss: 0.078117, mae: 0.269508, mean_q: 3.846365
 36018/100000: episode: 781, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.570, mean reward: 2.026 [1.506, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.629, 10.098], loss: 0.070929, mae: 0.271494, mean_q: 3.854097
 36118/100000: episode: 782, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 180.362, mean reward: 1.804 [1.475, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.452, 10.098], loss: 0.082687, mae: 0.287045, mean_q: 3.863682
 36218/100000: episode: 783, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 187.298, mean reward: 1.873 [1.437, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.108], loss: 0.077198, mae: 0.279262, mean_q: 3.839653
 36318/100000: episode: 784, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 237.174, mean reward: 2.372 [1.512, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.368, 10.246], loss: 0.068312, mae: 0.268582, mean_q: 3.836209
 36418/100000: episode: 785, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 206.156, mean reward: 2.062 [1.487, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.912, 10.098], loss: 0.075212, mae: 0.277249, mean_q: 3.847740
 36518/100000: episode: 786, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.346, mean reward: 1.873 [1.484, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.132, 10.143], loss: 0.068433, mae: 0.272299, mean_q: 3.843019
 36618/100000: episode: 787, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.855, mean reward: 1.869 [1.466, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.205], loss: 0.086551, mae: 0.288469, mean_q: 3.858817
 36718/100000: episode: 788, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.970, mean reward: 1.920 [1.436, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.577, 10.165], loss: 0.084885, mae: 0.297477, mean_q: 3.858491
 36818/100000: episode: 789, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 189.517, mean reward: 1.895 [1.480, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.538, 10.098], loss: 0.073009, mae: 0.273923, mean_q: 3.856888
 36918/100000: episode: 790, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 207.666, mean reward: 2.077 [1.445, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.098], loss: 0.084105, mae: 0.289486, mean_q: 3.860744
 37018/100000: episode: 791, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 212.839, mean reward: 2.128 [1.533, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.173, 10.098], loss: 0.083838, mae: 0.285971, mean_q: 3.876156
 37118/100000: episode: 792, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.851, mean reward: 1.949 [1.457, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.725, 10.119], loss: 0.083569, mae: 0.293565, mean_q: 3.857746
 37218/100000: episode: 793, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 208.448, mean reward: 2.084 [1.462, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.277, 10.098], loss: 0.076105, mae: 0.281739, mean_q: 3.864639
 37318/100000: episode: 794, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.348, mean reward: 1.933 [1.515, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.156, 10.279], loss: 0.072361, mae: 0.274896, mean_q: 3.847972
 37418/100000: episode: 795, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 204.440, mean reward: 2.044 [1.484, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.771, 10.163], loss: 0.080347, mae: 0.287482, mean_q: 3.877460
 37518/100000: episode: 796, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.346, mean reward: 1.843 [1.455, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.873, 10.101], loss: 0.078126, mae: 0.282504, mean_q: 3.854754
 37618/100000: episode: 797, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 189.146, mean reward: 1.891 [1.432, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.473, 10.103], loss: 0.083837, mae: 0.286901, mean_q: 3.867522
 37718/100000: episode: 798, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.527, mean reward: 1.815 [1.437, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.406, 10.205], loss: 0.074904, mae: 0.280741, mean_q: 3.855393
 37818/100000: episode: 799, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 206.670, mean reward: 2.067 [1.453, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.030, 10.244], loss: 0.080508, mae: 0.289308, mean_q: 3.871700
 37918/100000: episode: 800, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 218.214, mean reward: 2.182 [1.459, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.741, 10.098], loss: 0.077037, mae: 0.289115, mean_q: 3.876101
 38018/100000: episode: 801, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 194.735, mean reward: 1.947 [1.472, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.383, 10.102], loss: 0.082723, mae: 0.288371, mean_q: 3.884333
 38118/100000: episode: 802, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.023, mean reward: 1.910 [1.448, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.073, 10.145], loss: 0.085177, mae: 0.292826, mean_q: 3.878018
 38218/100000: episode: 803, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 200.869, mean reward: 2.009 [1.488, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.098], loss: 0.083649, mae: 0.294240, mean_q: 3.902642
 38318/100000: episode: 804, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.605, mean reward: 1.906 [1.499, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.393, 10.188], loss: 0.079518, mae: 0.289169, mean_q: 3.881914
 38418/100000: episode: 805, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.829, mean reward: 1.968 [1.464, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.919, 10.312], loss: 0.077430, mae: 0.281366, mean_q: 3.871350
 38518/100000: episode: 806, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.230, mean reward: 1.882 [1.478, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.628, 10.098], loss: 0.072693, mae: 0.279424, mean_q: 3.868146
 38618/100000: episode: 807, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 201.073, mean reward: 2.011 [1.493, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.328], loss: 0.077207, mae: 0.288105, mean_q: 3.872786
 38718/100000: episode: 808, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 212.792, mean reward: 2.128 [1.466, 5.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.713, 10.098], loss: 0.077697, mae: 0.286880, mean_q: 3.888397
 38818/100000: episode: 809, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.338, mean reward: 1.953 [1.454, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.249, 10.376], loss: 0.081580, mae: 0.287790, mean_q: 3.889194
 38918/100000: episode: 810, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 195.692, mean reward: 1.957 [1.485, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.673, 10.098], loss: 0.083708, mae: 0.294383, mean_q: 3.886163
 39018/100000: episode: 811, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 203.815, mean reward: 2.038 [1.489, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.152, 10.098], loss: 0.086683, mae: 0.296686, mean_q: 3.899385
 39118/100000: episode: 812, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 185.334, mean reward: 1.853 [1.439, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.349, 10.268], loss: 0.075545, mae: 0.274442, mean_q: 3.882392
 39218/100000: episode: 813, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 182.928, mean reward: 1.829 [1.465, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.940, 10.098], loss: 0.088263, mae: 0.289275, mean_q: 3.894840
 39318/100000: episode: 814, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.459, mean reward: 1.895 [1.471, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.871, 10.098], loss: 0.081513, mae: 0.285940, mean_q: 3.898016
 39418/100000: episode: 815, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.154, mean reward: 2.012 [1.496, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.555, 10.135], loss: 0.076017, mae: 0.281433, mean_q: 3.874408
 39518/100000: episode: 816, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 204.925, mean reward: 2.049 [1.524, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.944, 10.277], loss: 0.083168, mae: 0.292875, mean_q: 3.897076
 39618/100000: episode: 817, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 230.460, mean reward: 2.305 [1.451, 4.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.098], loss: 0.090262, mae: 0.298331, mean_q: 3.898516
 39718/100000: episode: 818, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 221.988, mean reward: 2.220 [1.445, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.161, 10.098], loss: 0.080691, mae: 0.284731, mean_q: 3.902916
 39818/100000: episode: 819, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 212.517, mean reward: 2.125 [1.451, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.875, 10.098], loss: 0.083825, mae: 0.294548, mean_q: 3.907943
 39918/100000: episode: 820, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 195.025, mean reward: 1.950 [1.462, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.964, 10.098], loss: 0.086461, mae: 0.296739, mean_q: 3.919375
 40018/100000: episode: 821, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 177.635, mean reward: 1.776 [1.444, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.280, 10.136], loss: 0.083758, mae: 0.294979, mean_q: 3.919959
 40118/100000: episode: 822, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 207.635, mean reward: 2.076 [1.481, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.164, 10.427], loss: 0.085427, mae: 0.301373, mean_q: 3.925520
 40218/100000: episode: 823, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 185.653, mean reward: 1.857 [1.438, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.235, 10.358], loss: 0.083371, mae: 0.289876, mean_q: 3.917080
 40318/100000: episode: 824, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.674, mean reward: 1.947 [1.461, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.814, 10.249], loss: 0.086591, mae: 0.294867, mean_q: 3.922930
[Info] 1-TH LEVEL FOUND: 6.029263019561768, Considering 10/90 traces
 40418/100000: episode: 825, duration: 4.760s, episode steps: 100, steps per second: 21, episode reward: 224.091, mean reward: 2.241 [1.487, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.149, 10.174], loss: 0.087383, mae: 0.302653, mean_q: 3.935034
 40450/100000: episode: 826, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 76.178, mean reward: 2.381 [1.478, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.064, 10.198], loss: 0.090133, mae: 0.308162, mean_q: 3.934607
 40458/100000: episode: 827, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 19.005, mean reward: 2.376 [2.089, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.346, 10.100], loss: 0.074923, mae: 0.282689, mean_q: 3.909582
 40495/100000: episode: 828, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 109.299, mean reward: 2.954 [1.743, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.398, 10.196], loss: 0.087571, mae: 0.299265, mean_q: 3.950164
 40522/100000: episode: 829, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 79.988, mean reward: 2.963 [2.294, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.217, 10.100], loss: 0.082826, mae: 0.283939, mean_q: 3.977900
 40547/100000: episode: 830, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 84.065, mean reward: 3.363 [2.381, 4.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.248, 10.100], loss: 0.087162, mae: 0.297802, mean_q: 3.964999
 40588/100000: episode: 831, duration: 0.198s, episode steps: 41, steps per second: 207, episode reward: 108.553, mean reward: 2.648 [1.609, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.532, 10.216], loss: 0.103178, mae: 0.317216, mean_q: 3.973831
 40615/100000: episode: 832, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 65.096, mean reward: 2.411 [1.648, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.517, 10.100], loss: 0.098513, mae: 0.315782, mean_q: 3.984940
 40642/100000: episode: 833, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 80.753, mean reward: 2.991 [2.318, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.578, 10.100], loss: 0.102620, mae: 0.309678, mean_q: 3.972809
 40650/100000: episode: 834, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 26.471, mean reward: 3.309 [2.196, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.335, 10.100], loss: 0.108293, mae: 0.324021, mean_q: 3.923182
 40682/100000: episode: 835, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 100.151, mean reward: 3.130 [2.372, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.372, 10.100], loss: 0.102246, mae: 0.317241, mean_q: 4.003543
 40711/100000: episode: 836, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 85.484, mean reward: 2.948 [1.860, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.658, 10.100], loss: 0.093454, mae: 0.308063, mean_q: 3.985291
 40748/100000: episode: 837, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 87.644, mean reward: 2.369 [1.478, 13.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.347, 10.100], loss: 0.143576, mae: 0.320624, mean_q: 3.966995
 40780/100000: episode: 838, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 82.484, mean reward: 2.578 [2.124, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.717, 10.100], loss: 0.100980, mae: 0.331242, mean_q: 4.010707
 40788/100000: episode: 839, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 19.832, mean reward: 2.479 [2.046, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.381, 10.100], loss: 0.107323, mae: 0.342790, mean_q: 4.115470
 40825/100000: episode: 840, duration: 0.212s, episode steps: 37, steps per second: 175, episode reward: 84.126, mean reward: 2.274 [1.626, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.033, 10.208], loss: 0.098495, mae: 0.322924, mean_q: 4.019626
 40850/100000: episode: 841, duration: 0.158s, episode steps: 25, steps per second: 158, episode reward: 65.743, mean reward: 2.630 [1.742, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.932, 10.100], loss: 0.103000, mae: 0.327106, mean_q: 4.025386
 40888/100000: episode: 842, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 140.248, mean reward: 3.691 [2.369, 6.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.153, 10.498], loss: 0.111971, mae: 0.337472, mean_q: 4.087239
 40926/100000: episode: 843, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 126.455, mean reward: 3.328 [2.313, 6.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.178, 10.430], loss: 0.115415, mae: 0.335595, mean_q: 4.093937
 40934/100000: episode: 844, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 19.175, mean reward: 2.397 [2.063, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.370, 10.100], loss: 0.097438, mae: 0.327607, mean_q: 4.042723
 40971/100000: episode: 845, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 86.026, mean reward: 2.325 [1.604, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.964, 10.320], loss: 0.109904, mae: 0.338243, mean_q: 4.071069
 40979/100000: episode: 846, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 26.959, mean reward: 3.370 [2.171, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.411, 10.100], loss: 0.122424, mae: 0.364528, mean_q: 4.222164
 41011/100000: episode: 847, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 113.580, mean reward: 3.549 [2.476, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.180, 10.100], loss: 0.174303, mae: 0.352234, mean_q: 4.121508
 41048/100000: episode: 848, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 81.401, mean reward: 2.200 [1.475, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.950, 10.176], loss: 0.108559, mae: 0.328007, mean_q: 4.124700
 41085/100000: episode: 849, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 76.299, mean reward: 2.062 [1.441, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.139, 10.100], loss: 0.100095, mae: 0.320670, mean_q: 4.084859
 41123/100000: episode: 850, duration: 0.223s, episode steps: 38, steps per second: 171, episode reward: 99.964, mean reward: 2.631 [1.909, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.579, 10.333], loss: 0.113261, mae: 0.337331, mean_q: 4.122466
 41148/100000: episode: 851, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 62.882, mean reward: 2.515 [2.045, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.204, 10.100], loss: 0.131432, mae: 0.344937, mean_q: 4.127069
 41186/100000: episode: 852, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 130.004, mean reward: 3.421 [2.638, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.743, 10.473], loss: 0.114156, mae: 0.346309, mean_q: 4.143323
 41215/100000: episode: 853, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 87.082, mean reward: 3.003 [2.294, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.871, 10.100], loss: 0.254259, mae: 0.383340, mean_q: 4.156011
 41253/100000: episode: 854, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 93.137, mean reward: 2.451 [2.020, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.467, 10.358], loss: 0.103436, mae: 0.324251, mean_q: 4.116442
 41285/100000: episode: 855, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 94.999, mean reward: 2.969 [2.069, 6.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.786, 10.100], loss: 0.103963, mae: 0.327949, mean_q: 4.158883
 41317/100000: episode: 856, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 182.348, mean reward: 5.698 [2.011, 24.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.999, 10.100], loss: 0.139024, mae: 0.343162, mean_q: 4.130598
 41349/100000: episode: 857, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 116.978, mean reward: 3.656 [2.347, 9.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.589, 10.100], loss: 0.158560, mae: 0.391398, mean_q: 4.255139
 41374/100000: episode: 858, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 62.548, mean reward: 2.502 [1.802, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.318, 10.100], loss: 0.242484, mae: 0.397635, mean_q: 4.177236
 41398/100000: episode: 859, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 56.380, mean reward: 2.349 [1.703, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.205, 10.100], loss: 0.147771, mae: 0.381956, mean_q: 4.216438
 41427/100000: episode: 860, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 96.838, mean reward: 3.339 [2.297, 6.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.702, 10.100], loss: 0.126984, mae: 0.344146, mean_q: 4.258160
 41465/100000: episode: 861, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 85.374, mean reward: 2.247 [1.530, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.305, 10.302], loss: 0.280058, mae: 0.423203, mean_q: 4.282380
 41473/100000: episode: 862, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 19.501, mean reward: 2.438 [2.113, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.205, 10.100], loss: 0.101273, mae: 0.324929, mean_q: 4.074547
 41497/100000: episode: 863, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 59.716, mean reward: 2.488 [1.691, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.171, 10.100], loss: 0.197682, mae: 0.386575, mean_q: 4.335220
 41535/100000: episode: 864, duration: 0.184s, episode steps: 38, steps per second: 206, episode reward: 105.660, mean reward: 2.781 [1.620, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.355, 10.246], loss: 0.130426, mae: 0.355963, mean_q: 4.303112
 41562/100000: episode: 865, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 91.214, mean reward: 3.378 [2.432, 4.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.292, 10.100], loss: 0.113141, mae: 0.329321, mean_q: 4.275175
 41600/100000: episode: 866, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 96.344, mean reward: 2.535 [1.596, 5.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.067, 10.200], loss: 0.395260, mae: 0.412921, mean_q: 4.294988
 41638/100000: episode: 867, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 102.062, mean reward: 2.686 [1.621, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.174, 10.251], loss: 0.166258, mae: 0.387386, mean_q: 4.293952
 41662/100000: episode: 868, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 65.603, mean reward: 2.733 [2.167, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.294, 10.100], loss: 0.229236, mae: 0.400124, mean_q: 4.250232
 41694/100000: episode: 869, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 88.358, mean reward: 2.761 [2.290, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.218, 10.100], loss: 0.143505, mae: 0.371126, mean_q: 4.315406
 41723/100000: episode: 870, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 97.126, mean reward: 3.349 [2.259, 6.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.579, 10.100], loss: 0.198650, mae: 0.390215, mean_q: 4.292298
 41761/100000: episode: 871, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 88.387, mean reward: 2.326 [1.902, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.186, 10.341], loss: 0.165478, mae: 0.398287, mean_q: 4.417672
 41788/100000: episode: 872, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 73.702, mean reward: 2.730 [1.620, 5.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.134, 10.100], loss: 0.128593, mae: 0.341895, mean_q: 4.323623
 41813/100000: episode: 873, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 66.850, mean reward: 2.674 [1.827, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.585, 10.100], loss: 0.191084, mae: 0.353024, mean_q: 4.344941
 41854/100000: episode: 874, duration: 0.208s, episode steps: 41, steps per second: 198, episode reward: 92.275, mean reward: 2.251 [1.758, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.738, 10.263], loss: 0.140659, mae: 0.368010, mean_q: 4.377461
 41878/100000: episode: 875, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 58.765, mean reward: 2.449 [2.010, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.229, 10.100], loss: 0.157760, mae: 0.352576, mean_q: 4.337796
 41905/100000: episode: 876, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 96.237, mean reward: 3.564 [2.456, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.279, 10.100], loss: 0.293568, mae: 0.401906, mean_q: 4.412709
 41942/100000: episode: 877, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 176.064, mean reward: 4.758 [2.743, 17.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.210, 10.508], loss: 0.256760, mae: 0.401052, mean_q: 4.486110
 41974/100000: episode: 878, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 72.747, mean reward: 2.273 [1.516, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.460, 10.119], loss: 0.208323, mae: 0.406327, mean_q: 4.448553
 42006/100000: episode: 879, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 97.122, mean reward: 3.035 [2.140, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.513, 10.100], loss: 0.633508, mae: 0.474652, mean_q: 4.526010
 42030/100000: episode: 880, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 95.397, mean reward: 3.975 [2.506, 6.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.323, 10.100], loss: 0.183838, mae: 0.427821, mean_q: 4.396586
 42038/100000: episode: 881, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 23.622, mean reward: 2.953 [2.288, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.362, 10.100], loss: 0.158618, mae: 0.381885, mean_q: 4.473303
 42065/100000: episode: 882, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 90.942, mean reward: 3.368 [2.073, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.687, 10.100], loss: 0.587292, mae: 0.513548, mean_q: 4.506183
 42106/100000: episode: 883, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 114.324, mean reward: 2.788 [1.658, 9.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.497, 10.297], loss: 0.177851, mae: 0.412641, mean_q: 4.482478
 42133/100000: episode: 884, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 90.111, mean reward: 3.337 [2.202, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.258, 10.100], loss: 0.186000, mae: 0.403935, mean_q: 4.574889
 42171/100000: episode: 885, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 90.987, mean reward: 2.394 [1.764, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.602, 10.265], loss: 0.425365, mae: 0.469827, mean_q: 4.578233
 42179/100000: episode: 886, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 22.744, mean reward: 2.843 [2.197, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.404, 10.100], loss: 0.931376, mae: 0.484388, mean_q: 4.390919
 42208/100000: episode: 887, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 74.897, mean reward: 2.583 [1.648, 6.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.832, 10.100], loss: 0.212562, mae: 0.449898, mean_q: 4.577815
 42232/100000: episode: 888, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 108.483, mean reward: 4.520 [2.647, 6.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.781, 10.100], loss: 0.270411, mae: 0.409353, mean_q: 4.522753
 42270/100000: episode: 889, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 89.379, mean reward: 2.352 [1.611, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.035, 10.162], loss: 0.355869, mae: 0.446720, mean_q: 4.615023
 42299/100000: episode: 890, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 70.348, mean reward: 2.426 [1.531, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.042, 10.125], loss: 0.467264, mae: 0.445148, mean_q: 4.545407
 42336/100000: episode: 891, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 83.145, mean reward: 2.247 [1.603, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.575, 10.237], loss: 0.358934, mae: 0.439403, mean_q: 4.495739
 42368/100000: episode: 892, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 119.935, mean reward: 3.748 [2.245, 8.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.547, 10.100], loss: 0.243099, mae: 0.416747, mean_q: 4.595716
 42376/100000: episode: 893, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 20.657, mean reward: 2.582 [2.119, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.393, 10.100], loss: 0.184157, mae: 0.415306, mean_q: 4.573254
 42384/100000: episode: 894, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 23.530, mean reward: 2.941 [2.646, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.251, 10.100], loss: 0.137945, mae: 0.385663, mean_q: 4.514844
 42408/100000: episode: 895, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 72.991, mean reward: 3.041 [2.328, 5.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.347, 10.100], loss: 0.166178, mae: 0.395004, mean_q: 4.600444
 42446/100000: episode: 896, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 91.579, mean reward: 2.410 [2.058, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.650, 10.381], loss: 0.195902, mae: 0.423864, mean_q: 4.599167
 42483/100000: episode: 897, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 85.726, mean reward: 2.317 [1.819, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.281, 10.353], loss: 0.210500, mae: 0.414031, mean_q: 4.638309
 42491/100000: episode: 898, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 23.420, mean reward: 2.928 [2.443, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.399, 10.100], loss: 0.119060, mae: 0.359279, mean_q: 4.473652
 42515/100000: episode: 899, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 58.971, mean reward: 2.457 [2.002, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.294, 10.100], loss: 0.488706, mae: 0.455079, mean_q: 4.584488
 42547/100000: episode: 900, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 90.574, mean reward: 2.830 [1.825, 6.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.336, 10.100], loss: 0.211494, mae: 0.407689, mean_q: 4.566187
 42584/100000: episode: 901, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 85.473, mean reward: 2.310 [1.626, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.144, 10.205], loss: 0.215003, mae: 0.411175, mean_q: 4.681363
 42608/100000: episode: 902, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 55.483, mean reward: 2.312 [1.621, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.586, 10.100], loss: 0.170603, mae: 0.390212, mean_q: 4.580055
 42633/100000: episode: 903, duration: 0.118s, episode steps: 25, steps per second: 212, episode reward: 66.735, mean reward: 2.669 [2.016, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.015, 10.100], loss: 0.276729, mae: 0.450593, mean_q: 4.725513
 42658/100000: episode: 904, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 91.405, mean reward: 3.656 [2.668, 5.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.348, 10.100], loss: 0.473481, mae: 0.462022, mean_q: 4.660257
 42699/100000: episode: 905, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 135.747, mean reward: 3.311 [1.596, 6.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.573, 10.184], loss: 0.235467, mae: 0.424760, mean_q: 4.620550
 42737/100000: episode: 906, duration: 0.194s, episode steps: 38, steps per second: 195, episode reward: 87.604, mean reward: 2.305 [1.606, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.450, 10.270], loss: 0.183639, mae: 0.421018, mean_q: 4.598461
 42761/100000: episode: 907, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 68.336, mean reward: 2.847 [2.071, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.311, 10.100], loss: 0.283600, mae: 0.470628, mean_q: 4.668834
 42799/100000: episode: 908, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 100.603, mean reward: 2.647 [1.777, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.472, 10.336], loss: 0.374013, mae: 0.478372, mean_q: 4.750216
 42824/100000: episode: 909, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 60.592, mean reward: 2.424 [1.983, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.325, 10.100], loss: 0.173713, mae: 0.396217, mean_q: 4.626931
 42865/100000: episode: 910, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 102.222, mean reward: 2.493 [1.781, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.611, 10.335], loss: 0.190912, mae: 0.404378, mean_q: 4.708172
 42889/100000: episode: 911, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 62.147, mean reward: 2.589 [1.899, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.042, 10.100], loss: 0.196064, mae: 0.447006, mean_q: 4.726870
 42913/100000: episode: 912, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 58.606, mean reward: 2.442 [1.922, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.469, 10.100], loss: 0.266968, mae: 0.421107, mean_q: 4.758604
 42940/100000: episode: 913, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 78.219, mean reward: 2.897 [2.250, 5.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.263, 10.100], loss: 0.251630, mae: 0.463643, mean_q: 4.735481
 42969/100000: episode: 914, duration: 0.174s, episode steps: 29, steps per second: 166, episode reward: 79.438, mean reward: 2.739 [1.671, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.292, 10.100], loss: 0.279194, mae: 0.464408, mean_q: 4.790449
[Info] 2-TH LEVEL FOUND: 7.322019577026367, Considering 10/90 traces
 43010/100000: episode: 915, duration: 4.416s, episode steps: 41, steps per second: 9, episode reward: 109.853, mean reward: 2.679 [1.821, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.277, 10.373], loss: 0.389299, mae: 0.462380, mean_q: 4.734740
 43034/100000: episode: 916, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 107.267, mean reward: 4.469 [3.143, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.407, 10.100], loss: 0.301672, mae: 0.440200, mean_q: 4.704418
 43060/100000: episode: 917, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 138.487, mean reward: 5.326 [3.132, 10.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.331, 10.100], loss: 0.284429, mae: 0.418807, mean_q: 4.808012
 43090/100000: episode: 918, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 128.042, mean reward: 4.268 [2.451, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.793, 10.393], loss: 0.423955, mae: 0.463504, mean_q: 4.777575
 43114/100000: episode: 919, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 83.928, mean reward: 3.497 [2.665, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.443, 10.100], loss: 0.433659, mae: 0.449202, mean_q: 4.727905
 43129/100000: episode: 920, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 64.395, mean reward: 4.293 [3.293, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.396, 10.100], loss: 0.434875, mae: 0.478019, mean_q: 4.805532
 43159/100000: episode: 921, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 123.186, mean reward: 4.106 [2.862, 6.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.922, 10.580], loss: 0.286528, mae: 0.467884, mean_q: 4.783661
 43178/100000: episode: 922, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 56.034, mean reward: 2.949 [1.951, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.372, 10.100], loss: 0.244075, mae: 0.452675, mean_q: 4.813661
 43192/100000: episode: 923, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 58.613, mean reward: 4.187 [3.000, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.612, 10.100], loss: 0.353306, mae: 0.503570, mean_q: 4.935436
 43218/100000: episode: 924, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 132.744, mean reward: 5.106 [3.125, 8.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.270, 10.100], loss: 0.272744, mae: 0.461194, mean_q: 4.813312
 43232/100000: episode: 925, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 58.592, mean reward: 4.185 [3.228, 5.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.706, 10.100], loss: 0.412516, mae: 0.542673, mean_q: 4.922681
 43251/100000: episode: 926, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 77.074, mean reward: 4.057 [2.286, 8.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.305, 10.100], loss: 0.444588, mae: 0.496007, mean_q: 4.889975
 43277/100000: episode: 927, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 129.998, mean reward: 5.000 [3.357, 7.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.326, 10.100], loss: 0.250855, mae: 0.447401, mean_q: 4.927323
 43291/100000: episode: 928, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 65.917, mean reward: 4.708 [2.965, 7.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.343, 10.100], loss: 0.811394, mae: 0.583154, mean_q: 5.168361
 43310/100000: episode: 929, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 79.435, mean reward: 4.181 [3.306, 5.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.384, 10.100], loss: 0.594774, mae: 0.539163, mean_q: 5.100012
 43336/100000: episode: 930, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 78.959, mean reward: 3.037 [1.784, 4.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.125, 10.100], loss: 0.233545, mae: 0.458778, mean_q: 5.031312
 43362/100000: episode: 931, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 99.732, mean reward: 3.836 [2.605, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.361, 10.100], loss: 0.459889, mae: 0.475675, mean_q: 4.881942
 43395/100000: episode: 932, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 87.219, mean reward: 2.643 [1.489, 5.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.009, 10.100], loss: 0.239012, mae: 0.469536, mean_q: 4.898084
 43425/100000: episode: 933, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 90.468, mean reward: 3.016 [1.928, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.994, 10.268], loss: 0.501391, mae: 0.534019, mean_q: 5.117461
 43444/100000: episode: 934, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 52.150, mean reward: 2.745 [2.339, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.389, 10.100], loss: 0.259869, mae: 0.455587, mean_q: 5.013283
 43468/100000: episode: 935, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 83.975, mean reward: 3.499 [2.239, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.356, 10.100], loss: 0.201457, mae: 0.428548, mean_q: 5.034600
 43492/100000: episode: 936, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 74.163, mean reward: 3.090 [2.275, 5.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.761, 10.100], loss: 0.314759, mae: 0.471244, mean_q: 5.058346
 43518/100000: episode: 937, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 89.766, mean reward: 3.453 [2.486, 7.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.528, 10.100], loss: 0.237283, mae: 0.480898, mean_q: 5.009118
 43544/100000: episode: 938, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 129.728, mean reward: 4.990 [3.256, 8.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.396, 10.100], loss: 0.530198, mae: 0.544799, mean_q: 5.188371
 43568/100000: episode: 939, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 91.808, mean reward: 3.825 [2.831, 6.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.300, 10.100], loss: 0.505786, mae: 0.578775, mean_q: 5.192013
 43582/100000: episode: 940, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 57.797, mean reward: 4.128 [2.634, 7.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.507, 10.100], loss: 0.265579, mae: 0.467778, mean_q: 5.062057
 43608/100000: episode: 941, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 124.649, mean reward: 4.794 [3.667, 8.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.473, 10.100], loss: 0.336625, mae: 0.530541, mean_q: 5.340960
 43623/100000: episode: 942, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 51.000, mean reward: 3.400 [2.675, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.437, 10.100], loss: 0.252541, mae: 0.479232, mean_q: 5.200282
 43649/100000: episode: 943, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 123.759, mean reward: 4.760 [2.606, 7.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.264, 10.100], loss: 0.380088, mae: 0.536220, mean_q: 5.220782
 43682/100000: episode: 944, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 91.948, mean reward: 2.786 [1.960, 5.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.382], loss: 0.260503, mae: 0.494231, mean_q: 5.241414
 43708/100000: episode: 945, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 275.505, mean reward: 10.596 [3.660, 22.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.726, 10.100], loss: 0.619613, mae: 0.590115, mean_q: 5.289299
[Info] FALSIFICATION!
[Info] Levels: [6.029263, 7.3220196, 8.448883]
[Info] Cond. Prob: [0.1, 0.1, 0.28]
[Info] Error Prob: 0.002800000000000001

 43718/100000: episode: 946, duration: 4.509s, episode steps: 10, steps per second: 2, episode reward: 142.243, mean reward: 14.224 [3.372, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.022, 9.529], loss: 0.307641, mae: 0.434516, mean_q: 4.986753
 43818/100000: episode: 947, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.695, mean reward: 1.847 [1.436, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.268], loss: 0.552099, mae: 0.571363, mean_q: 5.283171
 43918/100000: episode: 948, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.918, mean reward: 1.909 [1.434, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.339, 10.098], loss: 0.621961, mae: 0.590893, mean_q: 5.327164
 44018/100000: episode: 949, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.348, mean reward: 1.833 [1.470, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.676, 10.098], loss: 0.493176, mae: 0.571632, mean_q: 5.321536
 44118/100000: episode: 950, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 180.207, mean reward: 1.802 [1.450, 2.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.803, 10.236], loss: 1.784667, mae: 0.590747, mean_q: 5.306692
 44218/100000: episode: 951, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 196.829, mean reward: 1.968 [1.472, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.099], loss: 1.806156, mae: 0.667057, mean_q: 5.230180
 44318/100000: episode: 952, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 185.933, mean reward: 1.859 [1.440, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.692, 10.130], loss: 0.449840, mae: 0.546984, mean_q: 5.297859
 44418/100000: episode: 953, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 238.447, mean reward: 2.384 [1.558, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.310, 10.098], loss: 0.595635, mae: 0.594756, mean_q: 5.337753
 44518/100000: episode: 954, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.406, mean reward: 1.874 [1.486, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.741, 10.098], loss: 1.997872, mae: 0.742879, mean_q: 5.289972
 44618/100000: episode: 955, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.952, mean reward: 1.990 [1.487, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.180, 10.098], loss: 0.596662, mae: 0.575282, mean_q: 5.248802
 44718/100000: episode: 956, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 214.972, mean reward: 2.150 [1.511, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.658, 10.098], loss: 0.391361, mae: 0.538192, mean_q: 5.257676
 44818/100000: episode: 957, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 187.091, mean reward: 1.871 [1.461, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.958, 10.159], loss: 0.392246, mae: 0.528238, mean_q: 5.259485
 44918/100000: episode: 958, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 185.156, mean reward: 1.852 [1.461, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.272, 10.143], loss: 0.622555, mae: 0.622654, mean_q: 5.363733
 45018/100000: episode: 959, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 181.477, mean reward: 1.815 [1.458, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.386, 10.098], loss: 1.811673, mae: 0.626116, mean_q: 5.289302
 45118/100000: episode: 960, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 218.918, mean reward: 2.189 [1.531, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.098], loss: 4.570910, mae: 0.848529, mean_q: 5.359918
 45218/100000: episode: 961, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.320, mean reward: 1.843 [1.459, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.302, 10.098], loss: 1.905028, mae: 0.655145, mean_q: 5.337049
 45318/100000: episode: 962, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.987, mean reward: 1.900 [1.457, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.554, 10.098], loss: 0.558310, mae: 0.606074, mean_q: 5.350248
 45418/100000: episode: 963, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 195.938, mean reward: 1.959 [1.442, 5.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.320, 10.098], loss: 1.827559, mae: 0.641933, mean_q: 5.365160
 45518/100000: episode: 964, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 206.169, mean reward: 2.062 [1.492, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.997, 10.325], loss: 1.734012, mae: 0.608414, mean_q: 5.291678
 45618/100000: episode: 965, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 205.186, mean reward: 2.052 [1.450, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.859, 10.482], loss: 0.415126, mae: 0.543432, mean_q: 5.180930
 45718/100000: episode: 966, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 208.199, mean reward: 2.082 [1.448, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.098], loss: 0.520637, mae: 0.581692, mean_q: 5.259187
 45818/100000: episode: 967, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 209.720, mean reward: 2.097 [1.486, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.360, 10.098], loss: 1.660733, mae: 0.540615, mean_q: 5.179358
 45918/100000: episode: 968, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 181.921, mean reward: 1.819 [1.439, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.190, 10.098], loss: 1.667988, mae: 0.596419, mean_q: 5.174986
 46018/100000: episode: 969, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 193.594, mean reward: 1.936 [1.439, 4.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.098], loss: 1.702740, mae: 0.560481, mean_q: 5.144266
 46118/100000: episode: 970, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 201.773, mean reward: 2.018 [1.468, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.426, 10.200], loss: 0.475088, mae: 0.548779, mean_q: 5.066497
 46218/100000: episode: 971, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.494, mean reward: 1.905 [1.436, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.661, 10.282], loss: 0.438402, mae: 0.543251, mean_q: 5.002246
 46318/100000: episode: 972, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 199.456, mean reward: 1.995 [1.454, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.622, 10.299], loss: 1.723326, mae: 0.568551, mean_q: 4.988295
 46418/100000: episode: 973, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 214.287, mean reward: 2.143 [1.475, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.346], loss: 0.426210, mae: 0.525954, mean_q: 4.996841
 46518/100000: episode: 974, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 195.698, mean reward: 1.957 [1.477, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.371, 10.098], loss: 0.436333, mae: 0.516455, mean_q: 4.941698
 46618/100000: episode: 975, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 200.944, mean reward: 2.009 [1.460, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.898, 10.339], loss: 0.311047, mae: 0.461611, mean_q: 4.888972
 46718/100000: episode: 976, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 197.178, mean reward: 1.972 [1.471, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.838, 10.325], loss: 1.681609, mae: 0.552880, mean_q: 4.940094
 46818/100000: episode: 977, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.463, mean reward: 1.835 [1.446, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.573, 10.098], loss: 0.345365, mae: 0.496248, mean_q: 4.794408
 46918/100000: episode: 978, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 178.737, mean reward: 1.787 [1.459, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.529, 10.100], loss: 0.316134, mae: 0.474424, mean_q: 4.791564
 47018/100000: episode: 979, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 178.721, mean reward: 1.787 [1.478, 2.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.845, 10.165], loss: 1.662976, mae: 0.524155, mean_q: 4.679231
 47118/100000: episode: 980, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 185.274, mean reward: 1.853 [1.448, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.139], loss: 0.299255, mae: 0.452751, mean_q: 4.657150
 47218/100000: episode: 981, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.832, mean reward: 1.898 [1.480, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.744, 10.098], loss: 4.348333, mae: 0.724833, mean_q: 4.725370
 47318/100000: episode: 982, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.798, mean reward: 1.958 [1.452, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.536, 10.098], loss: 1.725703, mae: 0.548177, mean_q: 4.732697
 47418/100000: episode: 983, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.850, mean reward: 1.968 [1.446, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.092, 10.098], loss: 2.974290, mae: 0.603044, mean_q: 4.709569
 47518/100000: episode: 984, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 204.995, mean reward: 2.050 [1.446, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.852, 10.098], loss: 1.588156, mae: 0.540993, mean_q: 4.645335
 47618/100000: episode: 985, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 216.633, mean reward: 2.166 [1.467, 8.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.815, 10.098], loss: 1.551124, mae: 0.480566, mean_q: 4.580971
 47718/100000: episode: 986, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.768, mean reward: 1.878 [1.499, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.481, 10.299], loss: 0.256569, mae: 0.442019, mean_q: 4.552586
 47818/100000: episode: 987, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.187, mean reward: 1.892 [1.488, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.617, 10.098], loss: 0.267592, mae: 0.416097, mean_q: 4.497516
 47918/100000: episode: 988, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.798, mean reward: 1.878 [1.468, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.954, 10.098], loss: 0.377989, mae: 0.445188, mean_q: 4.503375
 48018/100000: episode: 989, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.847, mean reward: 2.028 [1.540, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.122, 10.098], loss: 0.306126, mae: 0.430770, mean_q: 4.410315
 48118/100000: episode: 990, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 183.514, mean reward: 1.835 [1.443, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.788, 10.098], loss: 0.286907, mae: 0.398907, mean_q: 4.326221
 48218/100000: episode: 991, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.029, mean reward: 1.850 [1.443, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.086, 10.098], loss: 2.700757, mae: 0.500845, mean_q: 4.286130
 48318/100000: episode: 992, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 206.282, mean reward: 2.063 [1.460, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.605, 10.098], loss: 0.219181, mae: 0.366824, mean_q: 4.167233
 48418/100000: episode: 993, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.659, mean reward: 1.887 [1.449, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.336], loss: 0.267533, mae: 0.377122, mean_q: 4.127583
 48518/100000: episode: 994, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.567, mean reward: 1.896 [1.449, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.200], loss: 0.210953, mae: 0.344553, mean_q: 4.046137
 48618/100000: episode: 995, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 194.324, mean reward: 1.943 [1.468, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.914, 10.233], loss: 1.419421, mae: 0.399901, mean_q: 4.029791
 48718/100000: episode: 996, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.001, mean reward: 2.000 [1.459, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.264, 10.192], loss: 1.369160, mae: 0.379904, mean_q: 3.910211
 48818/100000: episode: 997, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.620, mean reward: 1.976 [1.476, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.072, 10.208], loss: 0.090721, mae: 0.293547, mean_q: 3.860801
 48918/100000: episode: 998, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.798, mean reward: 1.908 [1.495, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.371], loss: 0.086333, mae: 0.287641, mean_q: 3.852561
 49018/100000: episode: 999, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.141, mean reward: 1.951 [1.443, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.098], loss: 0.078901, mae: 0.283208, mean_q: 3.857661
 49118/100000: episode: 1000, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.913, mean reward: 1.839 [1.494, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.011, 10.177], loss: 0.081083, mae: 0.288638, mean_q: 3.859994
 49218/100000: episode: 1001, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 200.011, mean reward: 2.000 [1.480, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.408, 10.404], loss: 0.080121, mae: 0.279792, mean_q: 3.857259
 49318/100000: episode: 1002, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 181.225, mean reward: 1.812 [1.473, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.098, 10.098], loss: 0.088830, mae: 0.293584, mean_q: 3.877234
 49418/100000: episode: 1003, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 199.373, mean reward: 1.994 [1.491, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.401, 10.098], loss: 0.082401, mae: 0.277569, mean_q: 3.866164
 49518/100000: episode: 1004, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 240.481, mean reward: 2.405 [1.467, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.481], loss: 0.088601, mae: 0.292509, mean_q: 3.870183
 49618/100000: episode: 1005, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 194.116, mean reward: 1.941 [1.447, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.016, 10.098], loss: 0.099764, mae: 0.290710, mean_q: 3.874476
 49718/100000: episode: 1006, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.737, mean reward: 1.797 [1.484, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.261], loss: 0.079231, mae: 0.276630, mean_q: 3.854904
 49818/100000: episode: 1007, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 202.510, mean reward: 2.025 [1.443, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.841, 10.098], loss: 0.082814, mae: 0.282335, mean_q: 3.850370
 49918/100000: episode: 1008, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 184.912, mean reward: 1.849 [1.456, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.148], loss: 0.091116, mae: 0.297457, mean_q: 3.877529
 50018/100000: episode: 1009, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.828, mean reward: 1.928 [1.462, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.317, 10.098], loss: 0.075585, mae: 0.277275, mean_q: 3.856269
 50118/100000: episode: 1010, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.476, mean reward: 1.855 [1.459, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.387, 10.239], loss: 0.082450, mae: 0.282354, mean_q: 3.866696
 50218/100000: episode: 1011, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 207.476, mean reward: 2.075 [1.483, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.780, 10.098], loss: 0.077052, mae: 0.283981, mean_q: 3.860417
 50318/100000: episode: 1012, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 198.140, mean reward: 1.981 [1.537, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.933, 10.098], loss: 0.077292, mae: 0.276651, mean_q: 3.853385
 50418/100000: episode: 1013, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.242, mean reward: 1.912 [1.458, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.290, 10.098], loss: 0.074907, mae: 0.275079, mean_q: 3.855592
 50518/100000: episode: 1014, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.124, mean reward: 1.981 [1.442, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.883, 10.297], loss: 0.075049, mae: 0.268698, mean_q: 3.879394
 50618/100000: episode: 1015, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.399, mean reward: 1.914 [1.487, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.776, 10.098], loss: 0.086363, mae: 0.280503, mean_q: 3.869905
 50718/100000: episode: 1016, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.630, mean reward: 1.926 [1.449, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.897, 10.469], loss: 0.081356, mae: 0.272398, mean_q: 3.843838
 50818/100000: episode: 1017, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 202.760, mean reward: 2.028 [1.443, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.013, 10.098], loss: 0.076200, mae: 0.274726, mean_q: 3.845664
 50918/100000: episode: 1018, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 181.448, mean reward: 1.814 [1.460, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.523, 10.098], loss: 0.084621, mae: 0.283080, mean_q: 3.857733
 51018/100000: episode: 1019, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 195.377, mean reward: 1.954 [1.459, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.803, 10.378], loss: 0.087933, mae: 0.278877, mean_q: 3.846519
 51118/100000: episode: 1020, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.722, mean reward: 1.887 [1.492, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.988, 10.098], loss: 0.095486, mae: 0.287267, mean_q: 3.859956
 51218/100000: episode: 1021, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 196.321, mean reward: 1.963 [1.485, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.399, 10.098], loss: 0.089660, mae: 0.284434, mean_q: 3.853948
 51318/100000: episode: 1022, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.069, mean reward: 1.981 [1.443, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.901, 10.098], loss: 0.079395, mae: 0.273240, mean_q: 3.851786
 51418/100000: episode: 1023, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 213.863, mean reward: 2.139 [1.440, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.284, 10.098], loss: 0.094749, mae: 0.284838, mean_q: 3.847703
 51518/100000: episode: 1024, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 179.725, mean reward: 1.797 [1.464, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.483, 10.098], loss: 0.075213, mae: 0.271861, mean_q: 3.855065
 51618/100000: episode: 1025, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 186.653, mean reward: 1.867 [1.486, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.168, 10.274], loss: 0.077664, mae: 0.273614, mean_q: 3.851378
 51718/100000: episode: 1026, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 192.041, mean reward: 1.920 [1.450, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.098], loss: 0.082596, mae: 0.268045, mean_q: 3.839274
 51818/100000: episode: 1027, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.544, mean reward: 1.875 [1.490, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.340, 10.186], loss: 0.077231, mae: 0.269344, mean_q: 3.829970
 51918/100000: episode: 1028, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 190.475, mean reward: 1.905 [1.455, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.060, 10.231], loss: 0.083558, mae: 0.276299, mean_q: 3.861385
 52018/100000: episode: 1029, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.554, mean reward: 1.876 [1.456, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.305], loss: 0.079573, mae: 0.279778, mean_q: 3.843178
 52118/100000: episode: 1030, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 220.199, mean reward: 2.202 [1.506, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.721, 10.098], loss: 0.075505, mae: 0.269902, mean_q: 3.844431
 52218/100000: episode: 1031, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 186.817, mean reward: 1.868 [1.447, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.142, 10.098], loss: 0.086778, mae: 0.284913, mean_q: 3.866821
 52318/100000: episode: 1032, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 192.370, mean reward: 1.924 [1.436, 7.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.922, 10.098], loss: 0.092623, mae: 0.286451, mean_q: 3.849741
 52418/100000: episode: 1033, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 183.382, mean reward: 1.834 [1.497, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.718, 10.213], loss: 0.070938, mae: 0.267920, mean_q: 3.844434
 52518/100000: episode: 1034, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.495, mean reward: 1.925 [1.463, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.956, 10.098], loss: 0.087020, mae: 0.285631, mean_q: 3.849868
 52618/100000: episode: 1035, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.768, mean reward: 1.878 [1.481, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.308, 10.105], loss: 0.088649, mae: 0.273504, mean_q: 3.846711
 52718/100000: episode: 1036, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 186.975, mean reward: 1.870 [1.473, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.466, 10.098], loss: 0.071618, mae: 0.269431, mean_q: 3.838843
 52818/100000: episode: 1037, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 206.823, mean reward: 2.068 [1.452, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.710, 10.371], loss: 0.070431, mae: 0.264802, mean_q: 3.835219
 52918/100000: episode: 1038, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 201.018, mean reward: 2.010 [1.436, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.107], loss: 0.067860, mae: 0.264129, mean_q: 3.828410
 53018/100000: episode: 1039, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 184.847, mean reward: 1.848 [1.435, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.576, 10.186], loss: 0.067086, mae: 0.263780, mean_q: 3.832281
 53118/100000: episode: 1040, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.259, mean reward: 1.893 [1.477, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.975, 10.210], loss: 0.076786, mae: 0.273332, mean_q: 3.843793
 53218/100000: episode: 1041, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 223.518, mean reward: 2.235 [1.528, 9.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.540, 10.098], loss: 0.070413, mae: 0.268440, mean_q: 3.822291
 53318/100000: episode: 1042, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 207.281, mean reward: 2.073 [1.513, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.994, 10.098], loss: 0.090085, mae: 0.278338, mean_q: 3.852762
 53418/100000: episode: 1043, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 203.470, mean reward: 2.035 [1.506, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.904, 10.098], loss: 0.101328, mae: 0.284513, mean_q: 3.840113
 53518/100000: episode: 1044, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 231.033, mean reward: 2.310 [1.493, 6.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.729, 10.098], loss: 0.082288, mae: 0.286045, mean_q: 3.843606
 53618/100000: episode: 1045, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 213.685, mean reward: 2.137 [1.457, 4.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.098], loss: 0.080892, mae: 0.282224, mean_q: 3.851436
[Info] 1-TH LEVEL FOUND: 5.535589694976807, Considering 10/90 traces
 53718/100000: episode: 1046, duration: 4.661s, episode steps: 100, steps per second: 21, episode reward: 187.171, mean reward: 1.872 [1.451, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.554, 10.264], loss: 0.088658, mae: 0.277761, mean_q: 3.881934
 53724/100000: episode: 1047, duration: 0.047s, episode steps: 6, steps per second: 127, episode reward: 18.654, mean reward: 3.109 [2.481, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.458, 10.466], loss: 0.078830, mae: 0.288164, mean_q: 3.861656
 53764/100000: episode: 1048, duration: 0.219s, episode steps: 40, steps per second: 182, episode reward: 97.876, mean reward: 2.447 [1.754, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.458, 10.325], loss: 0.071641, mae: 0.271083, mean_q: 3.868428
 53777/100000: episode: 1049, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 28.699, mean reward: 2.208 [1.601, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.293, 10.316], loss: 0.064455, mae: 0.266601, mean_q: 3.886388
 53809/100000: episode: 1050, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 92.174, mean reward: 2.880 [1.804, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.213, 10.100], loss: 0.081226, mae: 0.268048, mean_q: 3.865115
 53829/100000: episode: 1051, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 39.707, mean reward: 1.985 [1.603, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.823, 10.177], loss: 0.073461, mae: 0.271233, mean_q: 3.878621
 53861/100000: episode: 1052, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 102.813, mean reward: 3.213 [1.933, 6.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.321, 10.100], loss: 0.083872, mae: 0.294518, mean_q: 3.920385
 53901/100000: episode: 1053, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 94.243, mean reward: 2.356 [1.788, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.887, 10.260], loss: 0.089962, mae: 0.296729, mean_q: 3.907102
 53934/100000: episode: 1054, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 126.356, mean reward: 3.829 [2.480, 5.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.753, 10.100], loss: 0.100204, mae: 0.305423, mean_q: 3.908530
 53947/100000: episode: 1055, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 33.107, mean reward: 2.547 [1.622, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.239, 10.374], loss: 0.138619, mae: 0.300158, mean_q: 3.975138
 53975/100000: episode: 1056, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 94.455, mean reward: 3.373 [2.735, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.445, 10.100], loss: 0.132845, mae: 0.321506, mean_q: 3.960131
 53989/100000: episode: 1057, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 30.373, mean reward: 2.170 [1.517, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.183, 10.100], loss: 0.112701, mae: 0.293640, mean_q: 3.909513
 54012/100000: episode: 1058, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 79.157, mean reward: 3.442 [2.214, 5.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.354, 10.100], loss: 0.089575, mae: 0.315965, mean_q: 3.985705
 54052/100000: episode: 1059, duration: 0.213s, episode steps: 40, steps per second: 187, episode reward: 101.398, mean reward: 2.535 [2.023, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.932, 10.365], loss: 0.085077, mae: 0.292091, mean_q: 3.980659
 54072/100000: episode: 1060, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 75.185, mean reward: 3.759 [2.092, 10.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.159, 10.100], loss: 0.090969, mae: 0.288106, mean_q: 3.910402
 54092/100000: episode: 1061, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 55.468, mean reward: 2.773 [2.483, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.282, 10.100], loss: 0.100855, mae: 0.321325, mean_q: 4.012568
 54125/100000: episode: 1062, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 87.902, mean reward: 2.664 [2.163, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.646, 10.100], loss: 0.090626, mae: 0.303231, mean_q: 3.978656
 54131/100000: episode: 1063, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 16.708, mean reward: 2.785 [2.542, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.428], loss: 0.119421, mae: 0.314226, mean_q: 3.843496
 54151/100000: episode: 1064, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 42.954, mean reward: 2.148 [1.760, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.293, 10.244], loss: 0.109341, mae: 0.339448, mean_q: 4.039530
 54171/100000: episode: 1065, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 70.616, mean reward: 3.531 [2.424, 5.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.575, 10.100], loss: 0.097827, mae: 0.311908, mean_q: 4.015965
 54203/100000: episode: 1066, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 75.232, mean reward: 2.351 [1.709, 8.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.100], loss: 0.092219, mae: 0.296254, mean_q: 4.042037
 54235/100000: episode: 1067, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 86.330, mean reward: 2.698 [2.203, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.538, 10.100], loss: 0.096022, mae: 0.305990, mean_q: 4.029090
 54241/100000: episode: 1068, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 18.436, mean reward: 3.073 [2.666, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.469], loss: 0.094777, mae: 0.297207, mean_q: 3.924315
 54281/100000: episode: 1069, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 89.705, mean reward: 2.243 [1.740, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.213, 10.273], loss: 0.093273, mae: 0.310232, mean_q: 4.037190
 54309/100000: episode: 1070, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 73.098, mean reward: 2.611 [2.031, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.818, 10.100], loss: 0.101943, mae: 0.297276, mean_q: 4.027604
 54323/100000: episode: 1071, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 28.554, mean reward: 2.040 [1.848, 2.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.318], loss: 0.079895, mae: 0.292215, mean_q: 4.037970
 54355/100000: episode: 1072, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 84.535, mean reward: 2.642 [2.099, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.444, 10.100], loss: 0.105823, mae: 0.306443, mean_q: 4.019823
[Info] FALSIFICATION!
[Info] Levels: [5.5355897, 12.549186]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 54375/100000: episode: 1073, duration: 4.487s, episode steps: 20, steps per second: 4, episode reward: 197.404, mean reward: 9.870 [2.384, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.693, 9.756], loss: 0.090090, mae: 0.306112, mean_q: 4.068952
 54475/100000: episode: 1074, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 202.535, mean reward: 2.025 [1.503, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.571, 10.098], loss: 1.547791, mae: 0.458526, mean_q: 4.093349
 54575/100000: episode: 1075, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 184.872, mean reward: 1.849 [1.492, 2.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.098], loss: 1.511022, mae: 0.433618, mean_q: 4.061324
 54675/100000: episode: 1076, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 250.606, mean reward: 2.506 [1.586, 6.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.504, 10.098], loss: 1.451476, mae: 0.407134, mean_q: 4.046173
 54775/100000: episode: 1077, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 216.728, mean reward: 2.167 [1.471, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.489, 10.373], loss: 2.720251, mae: 0.477227, mean_q: 4.150404
 54875/100000: episode: 1078, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.870, mean reward: 1.879 [1.478, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.776, 10.123], loss: 1.409666, mae: 0.411017, mean_q: 4.094959
 54975/100000: episode: 1079, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 195.832, mean reward: 1.958 [1.464, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.098], loss: 1.393482, mae: 0.391183, mean_q: 4.105615
 55075/100000: episode: 1080, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.541, mean reward: 1.975 [1.487, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.545, 10.098], loss: 2.647755, mae: 0.481495, mean_q: 4.181149
 55175/100000: episode: 1081, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 196.410, mean reward: 1.964 [1.466, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.880, 10.204], loss: 0.124828, mae: 0.336912, mean_q: 4.109750
 55275/100000: episode: 1082, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 203.719, mean reward: 2.037 [1.447, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.521, 10.098], loss: 2.664008, mae: 0.482627, mean_q: 4.164874
 55375/100000: episode: 1083, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.752, mean reward: 1.948 [1.468, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.090, 10.098], loss: 3.772019, mae: 0.581774, mean_q: 4.163026
 55475/100000: episode: 1084, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.261, mean reward: 1.963 [1.458, 7.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.247], loss: 0.206189, mae: 0.407166, mean_q: 4.117460
 55575/100000: episode: 1085, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.878, mean reward: 1.979 [1.543, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.723, 10.235], loss: 1.391387, mae: 0.455503, mean_q: 4.163987
 55675/100000: episode: 1086, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.285, mean reward: 1.873 [1.483, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.219, 10.098], loss: 1.322478, mae: 0.475785, mean_q: 4.093946
 55775/100000: episode: 1087, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 206.495, mean reward: 2.065 [1.450, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.745, 10.205], loss: 0.167097, mae: 0.360647, mean_q: 4.112680
 55875/100000: episode: 1088, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.892, mean reward: 1.919 [1.454, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.707, 10.104], loss: 0.107818, mae: 0.330873, mean_q: 4.101809
 55975/100000: episode: 1089, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 261.632, mean reward: 2.616 [1.565, 12.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.512, 10.098], loss: 1.348271, mae: 0.435296, mean_q: 4.139526
 56075/100000: episode: 1090, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 177.018, mean reward: 1.770 [1.444, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.289, 10.098], loss: 1.299168, mae: 0.446434, mean_q: 4.162147
 56175/100000: episode: 1091, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.197, mean reward: 1.852 [1.477, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.769, 10.098], loss: 1.361858, mae: 0.436535, mean_q: 4.174959
 56275/100000: episode: 1092, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.303, mean reward: 1.903 [1.513, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.638, 10.345], loss: 0.210288, mae: 0.380922, mean_q: 4.108850
 56375/100000: episode: 1093, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 195.652, mean reward: 1.957 [1.435, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.219], loss: 1.374303, mae: 0.445856, mean_q: 4.170896
 56475/100000: episode: 1094, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 181.012, mean reward: 1.810 [1.456, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.337, 10.098], loss: 0.217703, mae: 0.390146, mean_q: 4.143297
 56575/100000: episode: 1095, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 201.663, mean reward: 2.017 [1.442, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.822, 10.205], loss: 1.299252, mae: 0.435254, mean_q: 4.123536
 56675/100000: episode: 1096, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 231.827, mean reward: 2.318 [1.493, 5.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.562, 10.098], loss: 0.146014, mae: 0.341133, mean_q: 4.095005
 56775/100000: episode: 1097, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 210.053, mean reward: 2.101 [1.484, 6.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.720, 10.098], loss: 0.189939, mae: 0.362194, mean_q: 4.154744
 56875/100000: episode: 1098, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.274, mean reward: 1.853 [1.449, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.455, 10.157], loss: 1.486323, mae: 0.486338, mean_q: 4.237522
 56975/100000: episode: 1099, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 180.402, mean reward: 1.804 [1.439, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.569, 10.236], loss: 0.167392, mae: 0.360160, mean_q: 4.147140
 57075/100000: episode: 1100, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.954, mean reward: 1.920 [1.473, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.121], loss: 0.158754, mae: 0.344035, mean_q: 4.141868
 57175/100000: episode: 1101, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.091, mean reward: 2.001 [1.530, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.854, 10.125], loss: 0.168854, mae: 0.347841, mean_q: 4.165953
 57275/100000: episode: 1102, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.019, mean reward: 1.880 [1.456, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.388, 10.209], loss: 1.451752, mae: 0.474043, mean_q: 4.152634
 57375/100000: episode: 1103, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 186.430, mean reward: 1.864 [1.485, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.342, 10.346], loss: 0.151809, mae: 0.340421, mean_q: 4.153561
 57475/100000: episode: 1104, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.814, mean reward: 1.858 [1.456, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.551, 10.098], loss: 2.481884, mae: 0.469187, mean_q: 4.230442
 57575/100000: episode: 1105, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 211.945, mean reward: 2.119 [1.455, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.838, 10.098], loss: 1.293426, mae: 0.420508, mean_q: 4.174762
 57675/100000: episode: 1106, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 210.778, mean reward: 2.108 [1.508, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.396, 10.098], loss: 0.158054, mae: 0.345477, mean_q: 4.151474
 57775/100000: episode: 1107, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 193.955, mean reward: 1.940 [1.508, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.380, 10.149], loss: 2.474854, mae: 0.493619, mean_q: 4.211823
 57875/100000: episode: 1108, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.470, mean reward: 1.885 [1.449, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.894, 10.214], loss: 1.332565, mae: 0.461645, mean_q: 4.199550
 57975/100000: episode: 1109, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.709, mean reward: 2.027 [1.467, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.170, 10.287], loss: 0.151460, mae: 0.345661, mean_q: 4.143192
 58075/100000: episode: 1110, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 190.724, mean reward: 1.907 [1.462, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.098], loss: 0.151188, mae: 0.346140, mean_q: 4.198754
 58175/100000: episode: 1111, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 180.301, mean reward: 1.803 [1.464, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.192], loss: 2.474328, mae: 0.485413, mean_q: 4.211543
 58275/100000: episode: 1112, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.375, mean reward: 1.814 [1.457, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.584, 10.098], loss: 1.294251, mae: 0.406992, mean_q: 4.182175
 58375/100000: episode: 1113, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 202.618, mean reward: 2.026 [1.500, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.671, 10.098], loss: 1.218578, mae: 0.389840, mean_q: 4.186998
 58475/100000: episode: 1114, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.742, mean reward: 1.837 [1.470, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.746, 10.344], loss: 0.159303, mae: 0.352243, mean_q: 4.080328
 58575/100000: episode: 1115, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.504, mean reward: 1.915 [1.464, 6.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.269, 10.098], loss: 0.119625, mae: 0.325884, mean_q: 4.099614
 58675/100000: episode: 1116, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 185.344, mean reward: 1.853 [1.497, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.415, 10.098], loss: 1.333985, mae: 0.407937, mean_q: 4.092421
 58775/100000: episode: 1117, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.440, mean reward: 1.884 [1.456, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.216, 10.134], loss: 0.122639, mae: 0.330525, mean_q: 4.058869
 58875/100000: episode: 1118, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 214.094, mean reward: 2.141 [1.477, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.278], loss: 0.124263, mae: 0.321773, mean_q: 4.052067
 58975/100000: episode: 1119, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.750, mean reward: 1.867 [1.465, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.921, 10.241], loss: 0.108278, mae: 0.305671, mean_q: 4.003651
 59075/100000: episode: 1120, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.698, mean reward: 1.857 [1.440, 2.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.926, 10.231], loss: 1.281805, mae: 0.412817, mean_q: 3.968167
 59175/100000: episode: 1121, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 207.723, mean reward: 2.077 [1.505, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.354, 10.347], loss: 0.119946, mae: 0.296609, mean_q: 3.946570
 59275/100000: episode: 1122, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 192.504, mean reward: 1.925 [1.440, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.056, 10.098], loss: 0.087728, mae: 0.283700, mean_q: 3.912272
 59375/100000: episode: 1123, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.391, mean reward: 1.814 [1.451, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.518, 10.224], loss: 0.083595, mae: 0.277537, mean_q: 3.881093
 59475/100000: episode: 1124, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.398, mean reward: 1.934 [1.462, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.791, 10.271], loss: 0.099234, mae: 0.290925, mean_q: 3.910188
 59575/100000: episode: 1125, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.265, mean reward: 1.933 [1.474, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.710, 10.211], loss: 0.095871, mae: 0.290456, mean_q: 3.907788
 59675/100000: episode: 1126, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 217.774, mean reward: 2.178 [1.441, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.340, 10.296], loss: 0.071819, mae: 0.266524, mean_q: 3.871948
 59775/100000: episode: 1127, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.457, mean reward: 1.925 [1.441, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.120], loss: 0.076613, mae: 0.273029, mean_q: 3.863975
 59875/100000: episode: 1128, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.946, mean reward: 1.989 [1.470, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.028, 10.128], loss: 0.078067, mae: 0.270897, mean_q: 3.863598
 59975/100000: episode: 1129, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.626, mean reward: 1.856 [1.458, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.138, 10.098], loss: 0.073251, mae: 0.265939, mean_q: 3.861204
 60075/100000: episode: 1130, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 193.450, mean reward: 1.934 [1.433, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.983, 10.098], loss: 0.089892, mae: 0.279331, mean_q: 3.864731
 60175/100000: episode: 1131, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 179.351, mean reward: 1.794 [1.458, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.159], loss: 0.075369, mae: 0.267087, mean_q: 3.887317
 60275/100000: episode: 1132, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 188.523, mean reward: 1.885 [1.454, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.098], loss: 0.085880, mae: 0.277043, mean_q: 3.860577
 60375/100000: episode: 1133, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 223.754, mean reward: 2.238 [1.503, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.477, 10.098], loss: 0.066087, mae: 0.265084, mean_q: 3.830899
 60475/100000: episode: 1134, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 206.667, mean reward: 2.067 [1.450, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.120], loss: 0.073995, mae: 0.262535, mean_q: 3.862862
 60575/100000: episode: 1135, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 219.789, mean reward: 2.198 [1.459, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.259, 10.098], loss: 0.066477, mae: 0.268821, mean_q: 3.863230
 60675/100000: episode: 1136, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 180.360, mean reward: 1.804 [1.430, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.493, 10.156], loss: 0.073077, mae: 0.268902, mean_q: 3.890239
 60775/100000: episode: 1137, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.879, mean reward: 1.929 [1.466, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.887, 10.098], loss: 0.069793, mae: 0.264012, mean_q: 3.873137
 60875/100000: episode: 1138, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 205.834, mean reward: 2.058 [1.446, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.538, 10.098], loss: 0.078999, mae: 0.272819, mean_q: 3.874367
 60975/100000: episode: 1139, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.576, mean reward: 1.886 [1.489, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.297], loss: 0.076286, mae: 0.276405, mean_q: 3.851465
 61075/100000: episode: 1140, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.984, mean reward: 1.940 [1.452, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.355, 10.098], loss: 0.072105, mae: 0.266517, mean_q: 3.859012
 61175/100000: episode: 1141, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.133, mean reward: 1.831 [1.478, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.998, 10.158], loss: 0.066334, mae: 0.263908, mean_q: 3.845852
 61275/100000: episode: 1142, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.925, mean reward: 2.009 [1.454, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.589, 10.098], loss: 0.078323, mae: 0.279071, mean_q: 3.868515
 61375/100000: episode: 1143, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 196.848, mean reward: 1.968 [1.480, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.098], loss: 0.065966, mae: 0.257552, mean_q: 3.865874
 61475/100000: episode: 1144, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.861, mean reward: 1.929 [1.515, 6.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.098], loss: 0.079502, mae: 0.280231, mean_q: 3.874907
 61575/100000: episode: 1145, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.459, mean reward: 1.915 [1.452, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.734, 10.098], loss: 0.082031, mae: 0.283154, mean_q: 3.876818
 61675/100000: episode: 1146, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 205.837, mean reward: 2.058 [1.441, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.251], loss: 0.079883, mae: 0.279244, mean_q: 3.868781
 61775/100000: episode: 1147, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 200.981, mean reward: 2.010 [1.466, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.823, 10.301], loss: 0.067667, mae: 0.263781, mean_q: 3.847860
 61875/100000: episode: 1148, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 186.698, mean reward: 1.867 [1.448, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.098], loss: 0.068699, mae: 0.268191, mean_q: 3.863163
 61975/100000: episode: 1149, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 197.275, mean reward: 1.973 [1.443, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.800, 10.098], loss: 0.069365, mae: 0.270080, mean_q: 3.858494
 62075/100000: episode: 1150, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.219, mean reward: 1.932 [1.465, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.385, 10.139], loss: 0.075726, mae: 0.268648, mean_q: 3.858375
 62175/100000: episode: 1151, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.310, mean reward: 1.953 [1.465, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.967, 10.153], loss: 0.071870, mae: 0.268510, mean_q: 3.857851
 62275/100000: episode: 1152, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.674, mean reward: 1.857 [1.486, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.228, 10.168], loss: 0.066195, mae: 0.263264, mean_q: 3.859518
 62375/100000: episode: 1153, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.161, mean reward: 1.902 [1.437, 7.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.286, 10.098], loss: 0.062506, mae: 0.255880, mean_q: 3.835084
 62475/100000: episode: 1154, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 189.825, mean reward: 1.898 [1.463, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.689, 10.285], loss: 0.069430, mae: 0.270154, mean_q: 3.849739
 62575/100000: episode: 1155, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.627, mean reward: 1.896 [1.463, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.146], loss: 0.070700, mae: 0.268610, mean_q: 3.854617
 62675/100000: episode: 1156, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.928, mean reward: 1.919 [1.463, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.180], loss: 0.062654, mae: 0.254651, mean_q: 3.840396
 62775/100000: episode: 1157, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.362, mean reward: 1.904 [1.448, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.764, 10.144], loss: 0.075281, mae: 0.270516, mean_q: 3.841666
 62875/100000: episode: 1158, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 199.821, mean reward: 1.998 [1.454, 5.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.646, 10.098], loss: 0.070183, mae: 0.261586, mean_q: 3.832816
 62975/100000: episode: 1159, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 238.072, mean reward: 2.381 [1.487, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.961, 10.098], loss: 0.066520, mae: 0.264422, mean_q: 3.821986
 63075/100000: episode: 1160, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.950, mean reward: 1.899 [1.453, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.098], loss: 0.077980, mae: 0.277010, mean_q: 3.852704
 63175/100000: episode: 1161, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 233.759, mean reward: 2.338 [1.471, 7.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.409 [-0.927, 10.098], loss: 0.079941, mae: 0.275314, mean_q: 3.856431
 63275/100000: episode: 1162, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 185.573, mean reward: 1.856 [1.464, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.307, 10.166], loss: 0.076919, mae: 0.278620, mean_q: 3.884106
 63375/100000: episode: 1163, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.650, mean reward: 1.856 [1.439, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.240, 10.098], loss: 0.076044, mae: 0.268715, mean_q: 3.869825
 63475/100000: episode: 1164, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.936, mean reward: 1.889 [1.456, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.255], loss: 0.077508, mae: 0.272572, mean_q: 3.866342
 63575/100000: episode: 1165, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.522, mean reward: 1.845 [1.443, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.187, 10.098], loss: 0.080603, mae: 0.272027, mean_q: 3.883326
 63675/100000: episode: 1166, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.292, mean reward: 1.893 [1.467, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.790, 10.242], loss: 0.075454, mae: 0.275271, mean_q: 3.868517
 63775/100000: episode: 1167, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 198.019, mean reward: 1.980 [1.488, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.383, 10.098], loss: 0.079317, mae: 0.279318, mean_q: 3.873854
 63875/100000: episode: 1168, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 196.783, mean reward: 1.968 [1.485, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.915, 10.109], loss: 0.073827, mae: 0.274679, mean_q: 3.849785
 63975/100000: episode: 1169, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.701, mean reward: 1.917 [1.472, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.276, 10.322], loss: 0.080240, mae: 0.277550, mean_q: 3.854455
 64075/100000: episode: 1170, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 175.714, mean reward: 1.757 [1.447, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.613, 10.098], loss: 0.066545, mae: 0.260252, mean_q: 3.863655
 64175/100000: episode: 1171, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.226, mean reward: 1.862 [1.455, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.098], loss: 0.074021, mae: 0.280303, mean_q: 3.847622
 64275/100000: episode: 1172, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 180.300, mean reward: 1.803 [1.442, 2.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.493, 10.098], loss: 0.084700, mae: 0.278993, mean_q: 3.851887
[Info] 1-TH LEVEL FOUND: 5.715445041656494, Considering 10/90 traces
 64375/100000: episode: 1173, duration: 4.680s, episode steps: 100, steps per second: 21, episode reward: 188.605, mean reward: 1.886 [1.469, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.132, 10.214], loss: 0.076220, mae: 0.277294, mean_q: 3.846349
 64397/100000: episode: 1174, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 43.767, mean reward: 1.989 [1.593, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.860, 10.100], loss: 0.068908, mae: 0.263390, mean_q: 3.848727
 64411/100000: episode: 1175, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 35.691, mean reward: 2.549 [2.346, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.240, 10.100], loss: 0.077519, mae: 0.289255, mean_q: 3.893923
 64425/100000: episode: 1176, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 35.862, mean reward: 2.562 [2.027, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.089, 10.100], loss: 0.117880, mae: 0.318501, mean_q: 3.862829
 64444/100000: episode: 1177, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 49.280, mean reward: 2.594 [1.821, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.235, 10.100], loss: 0.078016, mae: 0.289170, mean_q: 3.852684
 64458/100000: episode: 1178, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 38.976, mean reward: 2.784 [2.225, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.186, 10.100], loss: 0.095459, mae: 0.295964, mean_q: 3.921552
 64498/100000: episode: 1179, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 93.540, mean reward: 2.339 [1.631, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.518, 10.100], loss: 0.100403, mae: 0.297206, mean_q: 3.860889
 64513/100000: episode: 1180, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 56.057, mean reward: 3.737 [2.188, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.255, 10.100], loss: 0.068386, mae: 0.269440, mean_q: 3.837982
 64553/100000: episode: 1181, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 141.985, mean reward: 3.550 [2.187, 7.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.157, 10.100], loss: 0.108541, mae: 0.313849, mean_q: 3.937589
 64567/100000: episode: 1182, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 37.718, mean reward: 2.694 [2.147, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.533, 10.100], loss: 0.074751, mae: 0.273784, mean_q: 3.879526
 64586/100000: episode: 1183, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 85.784, mean reward: 4.515 [2.814, 12.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.167, 10.100], loss: 0.096583, mae: 0.292242, mean_q: 3.898858
 64614/100000: episode: 1184, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 102.994, mean reward: 3.678 [2.137, 6.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.426, 10.100], loss: 0.089795, mae: 0.290268, mean_q: 3.916985
 64639/100000: episode: 1185, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 83.564, mean reward: 3.343 [2.611, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.400, 10.100], loss: 0.101869, mae: 0.306111, mean_q: 3.964231
 64667/100000: episode: 1186, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 93.542, mean reward: 3.341 [2.532, 6.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.350, 10.100], loss: 0.117852, mae: 0.324647, mean_q: 3.975919
 64692/100000: episode: 1187, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 61.986, mean reward: 2.479 [2.037, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.633, 10.100], loss: 0.083002, mae: 0.294693, mean_q: 3.915632
 64717/100000: episode: 1188, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 60.210, mean reward: 2.408 [1.688, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.191, 10.100], loss: 0.194828, mae: 0.351634, mean_q: 4.011534
 64731/100000: episode: 1189, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 32.660, mean reward: 2.333 [1.884, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.107, 10.100], loss: 0.122158, mae: 0.326854, mean_q: 3.951245
 64753/100000: episode: 1190, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 42.497, mean reward: 1.932 [1.455, 2.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.085, 10.105], loss: 0.102688, mae: 0.287911, mean_q: 3.893477
 64778/100000: episode: 1191, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 75.956, mean reward: 3.038 [2.422, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.838, 10.100], loss: 0.087269, mae: 0.296145, mean_q: 3.975320
 64797/100000: episode: 1192, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 64.435, mean reward: 3.391 [2.731, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.483, 10.100], loss: 0.099227, mae: 0.317095, mean_q: 3.977591
 64837/100000: episode: 1193, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 110.836, mean reward: 2.771 [1.652, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.344, 10.100], loss: 0.170662, mae: 0.343876, mean_q: 4.031248
 64851/100000: episode: 1194, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 42.091, mean reward: 3.006 [2.468, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.760, 10.100], loss: 0.076522, mae: 0.273516, mean_q: 3.862615
 64865/100000: episode: 1195, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 51.420, mean reward: 3.673 [2.706, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.670, 10.100], loss: 0.174255, mae: 0.358799, mean_q: 4.124906
 64887/100000: episode: 1196, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 44.065, mean reward: 2.003 [1.661, 2.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.198, 10.100], loss: 0.117994, mae: 0.317092, mean_q: 3.993982
 64901/100000: episode: 1197, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 40.704, mean reward: 2.907 [2.409, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.355, 10.100], loss: 0.110380, mae: 0.322315, mean_q: 3.994592
 64987/100000: episode: 1198, duration: 0.474s, episode steps: 86, steps per second: 181, episode reward: 201.224, mean reward: 2.340 [1.456, 5.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.583 [-0.776, 10.168], loss: 0.104010, mae: 0.309012, mean_q: 3.996493
 65001/100000: episode: 1199, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 43.126, mean reward: 3.080 [2.426, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.353, 10.100], loss: 0.203109, mae: 0.336017, mean_q: 4.055683
 65029/100000: episode: 1200, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 73.141, mean reward: 2.612 [2.083, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.139, 10.100], loss: 0.114110, mae: 0.324899, mean_q: 3.963569
 65048/100000: episode: 1201, duration: 0.098s, episode steps: 19, steps per second: 195, episode reward: 43.067, mean reward: 2.267 [1.521, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.100], loss: 0.140920, mae: 0.344969, mean_q: 4.052977
 65062/100000: episode: 1202, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 43.674, mean reward: 3.120 [2.421, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.414, 10.100], loss: 0.124759, mae: 0.327161, mean_q: 4.095031
 65087/100000: episode: 1203, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 62.112, mean reward: 2.484 [1.877, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.704, 10.100], loss: 0.112238, mae: 0.317416, mean_q: 3.987018
 65101/100000: episode: 1204, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 34.080, mean reward: 2.434 [1.955, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.378, 10.100], loss: 0.094292, mae: 0.313701, mean_q: 4.045570
 65141/100000: episode: 1205, duration: 0.208s, episode steps: 40, steps per second: 193, episode reward: 115.910, mean reward: 2.898 [2.170, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.102, 10.100], loss: 0.140203, mae: 0.341904, mean_q: 4.033234
 65163/100000: episode: 1206, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 69.583, mean reward: 3.163 [2.222, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.399, 10.100], loss: 0.113023, mae: 0.330800, mean_q: 4.137191
 65188/100000: episode: 1207, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 72.116, mean reward: 2.885 [2.195, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.931, 10.100], loss: 0.097083, mae: 0.306725, mean_q: 4.060962
 65207/100000: episode: 1208, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 51.509, mean reward: 2.711 [1.882, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.292, 10.100], loss: 0.185590, mae: 0.344767, mean_q: 4.036777
 65221/100000: episode: 1209, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 38.057, mean reward: 2.718 [2.349, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.623, 10.100], loss: 0.109796, mae: 0.348185, mean_q: 4.138423
 65235/100000: episode: 1210, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 40.014, mean reward: 2.858 [2.510, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.938, 10.100], loss: 0.242545, mae: 0.387461, mean_q: 4.177840
 65263/100000: episode: 1211, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 80.356, mean reward: 2.870 [2.219, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.050, 10.100], loss: 0.133334, mae: 0.344250, mean_q: 4.045104
 65303/100000: episode: 1212, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 130.282, mean reward: 3.257 [1.605, 8.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.241, 10.100], loss: 0.147575, mae: 0.326718, mean_q: 4.103553
 65318/100000: episode: 1213, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 34.420, mean reward: 2.295 [1.698, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.196, 10.100], loss: 0.099623, mae: 0.297508, mean_q: 4.117671
 65337/100000: episode: 1214, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 39.312, mean reward: 2.069 [1.477, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.227, 10.121], loss: 0.135409, mae: 0.325619, mean_q: 4.113376
 65351/100000: episode: 1215, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 41.404, mean reward: 2.957 [2.347, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.396, 10.100], loss: 0.136017, mae: 0.324902, mean_q: 4.088730
 65373/100000: episode: 1216, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 75.340, mean reward: 3.425 [2.041, 5.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.370, 10.100], loss: 0.127224, mae: 0.329543, mean_q: 4.125202
 65459/100000: episode: 1217, duration: 0.433s, episode steps: 86, steps per second: 199, episode reward: 167.248, mean reward: 1.945 [1.480, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.585 [-1.214, 10.100], loss: 0.149207, mae: 0.348882, mean_q: 4.157042
 65478/100000: episode: 1218, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 52.459, mean reward: 2.761 [2.375, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.332, 10.100], loss: 0.195079, mae: 0.363017, mean_q: 4.205772
 65492/100000: episode: 1219, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 34.202, mean reward: 2.443 [1.992, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.707, 10.100], loss: 0.128758, mae: 0.329478, mean_q: 4.123638
 65517/100000: episode: 1220, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 92.440, mean reward: 3.698 [2.059, 6.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.187, 10.100], loss: 0.133829, mae: 0.336547, mean_q: 4.134723
 65536/100000: episode: 1221, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 57.077, mean reward: 3.004 [2.440, 4.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.168, 10.100], loss: 0.117245, mae: 0.318754, mean_q: 4.162775
 65555/100000: episode: 1222, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 54.594, mean reward: 2.873 [1.939, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.731, 10.100], loss: 0.096222, mae: 0.321703, mean_q: 4.136026
 65574/100000: episode: 1223, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 53.010, mean reward: 2.790 [1.900, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.240, 10.100], loss: 0.145924, mae: 0.358335, mean_q: 4.213924
 65602/100000: episode: 1224, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 73.891, mean reward: 2.639 [1.581, 14.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.676, 10.100], loss: 0.232741, mae: 0.385108, mean_q: 4.220673
 65624/100000: episode: 1225, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 52.977, mean reward: 2.408 [1.636, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.273, 10.100], loss: 0.156833, mae: 0.388200, mean_q: 4.250252
 65664/100000: episode: 1226, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 140.441, mean reward: 3.511 [2.124, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.508, 10.100], loss: 0.183045, mae: 0.361949, mean_q: 4.260994
 65678/100000: episode: 1227, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 41.688, mean reward: 2.978 [2.249, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.521, 10.100], loss: 0.130259, mae: 0.330315, mean_q: 4.245805
 65692/100000: episode: 1228, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 33.781, mean reward: 2.413 [2.145, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.311, 10.100], loss: 0.132924, mae: 0.350311, mean_q: 4.300765
 65717/100000: episode: 1229, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 68.238, mean reward: 2.730 [2.044, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.378, 10.100], loss: 0.142076, mae: 0.350400, mean_q: 4.211313
 65736/100000: episode: 1230, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 45.719, mean reward: 2.406 [1.687, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.561, 10.100], loss: 0.218007, mae: 0.363592, mean_q: 4.229640
 65761/100000: episode: 1231, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 90.856, mean reward: 3.634 [2.645, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.311, 10.100], loss: 0.116120, mae: 0.338974, mean_q: 4.199175
 65789/100000: episode: 1232, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 76.902, mean reward: 2.747 [2.208, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.275, 10.100], loss: 0.135749, mae: 0.356979, mean_q: 4.293868
 65817/100000: episode: 1233, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 78.248, mean reward: 2.795 [2.267, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.246, 10.100], loss: 0.170955, mae: 0.382861, mean_q: 4.310235
 65831/100000: episode: 1234, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 39.433, mean reward: 2.817 [2.331, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.380, 10.100], loss: 0.110957, mae: 0.341663, mean_q: 4.212915
 65859/100000: episode: 1235, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 78.028, mean reward: 2.787 [1.952, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.542, 10.100], loss: 0.154803, mae: 0.353791, mean_q: 4.270153
 65878/100000: episode: 1236, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 44.451, mean reward: 2.340 [1.945, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.116, 10.100], loss: 0.120631, mae: 0.334566, mean_q: 4.274185
 65903/100000: episode: 1237, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 78.076, mean reward: 3.123 [2.057, 4.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.254, 10.100], loss: 0.242334, mae: 0.389734, mean_q: 4.297861
 65917/100000: episode: 1238, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 37.059, mean reward: 2.647 [2.132, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.333, 10.100], loss: 0.142165, mae: 0.346481, mean_q: 4.207420
 65931/100000: episode: 1239, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 37.824, mean reward: 2.702 [1.843, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.963, 10.100], loss: 0.138562, mae: 0.344653, mean_q: 4.355580
 65959/100000: episode: 1240, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 68.933, mean reward: 2.462 [1.803, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.178, 10.100], loss: 0.189727, mae: 0.382379, mean_q: 4.383776
 65978/100000: episode: 1241, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 59.755, mean reward: 3.145 [2.126, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.177, 10.100], loss: 0.192298, mae: 0.382657, mean_q: 4.362520
 65993/100000: episode: 1242, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 40.042, mean reward: 2.669 [1.925, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.470, 10.100], loss: 0.254601, mae: 0.409043, mean_q: 4.437622
 66012/100000: episode: 1243, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 69.648, mean reward: 3.666 [2.662, 5.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.178, 10.100], loss: 0.146025, mae: 0.363193, mean_q: 4.291958
 66040/100000: episode: 1244, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 89.411, mean reward: 3.193 [2.187, 4.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.429, 10.100], loss: 0.120934, mae: 0.345233, mean_q: 4.339446
 66080/100000: episode: 1245, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 103.345, mean reward: 2.584 [1.770, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.357, 10.100], loss: 0.158219, mae: 0.370734, mean_q: 4.398005
 66120/100000: episode: 1246, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 101.323, mean reward: 2.533 [1.941, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.324, 10.100], loss: 0.161695, mae: 0.360597, mean_q: 4.329991
 66148/100000: episode: 1247, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 89.353, mean reward: 3.191 [2.155, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.902, 10.100], loss: 0.146489, mae: 0.350895, mean_q: 4.378480
 66162/100000: episode: 1248, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 43.732, mean reward: 3.124 [2.213, 5.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.360, 10.100], loss: 0.126866, mae: 0.344883, mean_q: 4.347776
 66181/100000: episode: 1249, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 53.886, mean reward: 2.836 [2.357, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.386, 10.100], loss: 0.234711, mae: 0.381086, mean_q: 4.419711
 66203/100000: episode: 1250, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 55.982, mean reward: 2.545 [1.951, 4.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.272, 10.100], loss: 0.188656, mae: 0.357969, mean_q: 4.374856
 66243/100000: episode: 1251, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 134.500, mean reward: 3.363 [2.091, 9.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.322, 10.100], loss: 0.147914, mae: 0.360677, mean_q: 4.414538
 66265/100000: episode: 1252, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 49.708, mean reward: 2.259 [1.521, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.079, 10.100], loss: 0.229347, mae: 0.381507, mean_q: 4.370824
 66290/100000: episode: 1253, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 59.575, mean reward: 2.383 [1.918, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.143, 10.100], loss: 0.156964, mae: 0.373920, mean_q: 4.384535
 66312/100000: episode: 1254, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 49.607, mean reward: 2.255 [1.626, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.302, 10.100], loss: 0.210834, mae: 0.391305, mean_q: 4.501131
 66326/100000: episode: 1255, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 37.382, mean reward: 2.670 [1.767, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.145, 10.100], loss: 0.179183, mae: 0.406698, mean_q: 4.465317
 66345/100000: episode: 1256, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 61.866, mean reward: 3.256 [2.444, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.528, 10.100], loss: 0.134570, mae: 0.359514, mean_q: 4.401456
 66359/100000: episode: 1257, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 40.967, mean reward: 2.926 [2.254, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.187, 10.100], loss: 0.192954, mae: 0.400115, mean_q: 4.428413
 66378/100000: episode: 1258, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 41.631, mean reward: 2.191 [1.616, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.158, 10.100], loss: 0.205687, mae: 0.375905, mean_q: 4.440565
 66397/100000: episode: 1259, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 58.575, mean reward: 3.083 [1.517, 6.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.199, 10.100], loss: 0.159906, mae: 0.386224, mean_q: 4.436799
 66425/100000: episode: 1260, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 72.023, mean reward: 2.572 [2.084, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.181, 10.100], loss: 0.175692, mae: 0.394543, mean_q: 4.441010
 66440/100000: episode: 1261, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 31.104, mean reward: 2.074 [1.785, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.390, 10.100], loss: 0.151037, mae: 0.380147, mean_q: 4.470564
 66526/100000: episode: 1262, duration: 0.426s, episode steps: 86, steps per second: 202, episode reward: 180.503, mean reward: 2.099 [1.548, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.591 [-0.848, 10.100], loss: 0.178907, mae: 0.386234, mean_q: 4.484878
[Info] 2-TH LEVEL FOUND: 7.470165252685547, Considering 10/90 traces
 66540/100000: episode: 1263, duration: 4.341s, episode steps: 14, steps per second: 3, episode reward: 52.971, mean reward: 3.784 [2.599, 7.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.491, 10.100], loss: 0.290687, mae: 0.424665, mean_q: 4.439528
 66575/100000: episode: 1264, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 145.261, mean reward: 4.150 [1.977, 14.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.008, 10.100], loss: 0.186278, mae: 0.404624, mean_q: 4.461226
 66599/100000: episode: 1265, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 82.831, mean reward: 3.451 [2.849, 5.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.352, 10.100], loss: 0.267216, mae: 0.419353, mean_q: 4.509121
 66623/100000: episode: 1266, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 62.157, mean reward: 2.590 [1.757, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.076, 10.100], loss: 0.201254, mae: 0.412355, mean_q: 4.555759
 66639/100000: episode: 1267, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 115.226, mean reward: 7.202 [3.782, 35.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.084, 10.100], loss: 0.131166, mae: 0.346580, mean_q: 4.394051
 66657/100000: episode: 1268, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 53.306, mean reward: 2.961 [2.476, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.982, 10.100], loss: 0.194550, mae: 0.416202, mean_q: 4.592788
 66673/100000: episode: 1269, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 57.581, mean reward: 3.599 [3.031, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.353, 10.100], loss: 0.320178, mae: 0.463490, mean_q: 4.569921
 66689/100000: episode: 1270, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 48.418, mean reward: 3.026 [2.220, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.335, 10.100], loss: 0.163852, mae: 0.395779, mean_q: 4.589104
 66705/100000: episode: 1271, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 66.605, mean reward: 4.163 [3.352, 6.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.504, 10.100], loss: 0.348115, mae: 0.475305, mean_q: 4.577296
 66722/100000: episode: 1272, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 57.271, mean reward: 3.369 [2.750, 4.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.332, 10.100], loss: 0.198274, mae: 0.418577, mean_q: 4.591861
 66738/100000: episode: 1273, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 54.577, mean reward: 3.411 [2.762, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.388, 10.100], loss: 0.311061, mae: 0.435109, mean_q: 4.660335
 66754/100000: episode: 1274, duration: 0.077s, episode steps: 16, steps per second: 206, episode reward: 64.522, mean reward: 4.033 [3.358, 6.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.800, 10.100], loss: 0.144828, mae: 0.377563, mean_q: 4.608939
 66789/100000: episode: 1275, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 201.985, mean reward: 5.771 [3.467, 11.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.567, 10.100], loss: 0.212576, mae: 0.398234, mean_q: 4.630942
 66813/100000: episode: 1276, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 174.528, mean reward: 7.272 [3.033, 31.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.707, 10.100], loss: 0.178379, mae: 0.392074, mean_q: 4.522023
 66831/100000: episode: 1277, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 71.979, mean reward: 3.999 [2.319, 6.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.814, 10.100], loss: 0.237189, mae: 0.446262, mean_q: 4.658221
 66848/100000: episode: 1278, duration: 0.084s, episode steps: 17, steps per second: 201, episode reward: 55.213, mean reward: 3.248 [2.782, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.395, 10.100], loss: 0.229336, mae: 0.445151, mean_q: 4.667543
 66864/100000: episode: 1279, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 47.952, mean reward: 2.997 [2.023, 3.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.309, 10.100], loss: 1.388228, mae: 0.597967, mean_q: 4.741365
 66880/100000: episode: 1280, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 47.008, mean reward: 2.938 [2.403, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.811, 10.100], loss: 0.481393, mae: 0.597905, mean_q: 4.650156
 66904/100000: episode: 1281, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 168.000, mean reward: 7.000 [3.035, 19.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.647, 10.100], loss: 0.976580, mae: 0.554691, mean_q: 4.727430
 66920/100000: episode: 1282, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 54.320, mean reward: 3.395 [2.749, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.311, 10.100], loss: 0.510461, mae: 0.493734, mean_q: 4.713816
 66937/100000: episode: 1283, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 50.488, mean reward: 2.970 [2.392, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.264, 10.100], loss: 0.385499, mae: 0.527329, mean_q: 4.806597
 66961/100000: episode: 1284, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 105.764, mean reward: 4.407 [2.748, 11.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.979, 10.100], loss: 0.266836, mae: 0.466554, mean_q: 4.825795
 66985/100000: episode: 1285, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 82.108, mean reward: 3.421 [2.480, 5.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.251, 10.100], loss: 0.335149, mae: 0.470578, mean_q: 4.800912
 67002/100000: episode: 1286, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 51.209, mean reward: 3.012 [2.511, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.726, 10.100], loss: 1.150032, mae: 0.498924, mean_q: 4.844451
 67018/100000: episode: 1287, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 53.051, mean reward: 3.316 [2.679, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.276, 10.100], loss: 0.257148, mae: 0.444392, mean_q: 4.718446
 67034/100000: episode: 1288, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 48.992, mean reward: 3.062 [2.613, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.314, 10.100], loss: 0.242635, mae: 0.466667, mean_q: 4.808561
 67069/100000: episode: 1289, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 123.794, mean reward: 3.537 [1.967, 6.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.154, 10.100], loss: 0.392634, mae: 0.475802, mean_q: 4.769005
 67104/100000: episode: 1290, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 201.054, mean reward: 5.744 [3.209, 14.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.756, 10.100], loss: 0.251058, mae: 0.454753, mean_q: 4.877032
 67128/100000: episode: 1291, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 74.806, mean reward: 3.117 [2.428, 4.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.326, 10.100], loss: 0.612265, mae: 0.537613, mean_q: 4.942281
 67152/100000: episode: 1292, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 90.943, mean reward: 3.789 [2.647, 7.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.497, 10.100], loss: 1.361073, mae: 0.680524, mean_q: 4.947329
 67176/100000: episode: 1293, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 119.042, mean reward: 4.960 [2.521, 9.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.363, 10.100], loss: 0.405497, mae: 0.550947, mean_q: 5.021356
 67200/100000: episode: 1294, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 94.301, mean reward: 3.929 [2.616, 6.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.616, 10.100], loss: 0.489658, mae: 0.544066, mean_q: 4.954216
 67235/100000: episode: 1295, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 146.269, mean reward: 4.179 [1.919, 9.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.275, 10.100], loss: 0.338487, mae: 0.488463, mean_q: 4.940667
 67259/100000: episode: 1296, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 132.232, mean reward: 5.510 [3.377, 7.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.506, 10.100], loss: 0.282908, mae: 0.485707, mean_q: 4.888947
 67283/100000: episode: 1297, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 80.823, mean reward: 3.368 [2.274, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.401, 10.100], loss: 0.498732, mae: 0.531098, mean_q: 5.043672
 67318/100000: episode: 1298, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 115.024, mean reward: 3.286 [2.004, 7.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.619, 10.100], loss: 1.321742, mae: 0.600140, mean_q: 5.038164
 67342/100000: episode: 1299, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 104.391, mean reward: 4.350 [3.066, 9.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.363, 10.100], loss: 0.338729, mae: 0.495428, mean_q: 5.021676
 67366/100000: episode: 1300, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 98.128, mean reward: 4.089 [2.897, 5.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.370, 10.100], loss: 0.334052, mae: 0.545436, mean_q: 5.176605
 67390/100000: episode: 1301, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 226.360, mean reward: 9.432 [3.042, 32.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.463, 10.100], loss: 0.942124, mae: 0.629589, mean_q: 5.272408
 67406/100000: episode: 1302, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 62.690, mean reward: 3.918 [3.172, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.350, 10.100], loss: 1.164071, mae: 0.640361, mean_q: 5.066838
 67430/100000: episode: 1303, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 108.240, mean reward: 4.510 [2.511, 7.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.528, 10.100], loss: 0.507920, mae: 0.624375, mean_q: 5.231787
 67465/100000: episode: 1304, duration: 0.194s, episode steps: 35, steps per second: 181, episode reward: 192.700, mean reward: 5.506 [2.648, 12.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.595, 10.100], loss: 0.590320, mae: 0.575241, mean_q: 5.282118
 67489/100000: episode: 1305, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 107.620, mean reward: 4.484 [2.415, 14.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.317, 10.100], loss: 0.614810, mae: 0.613290, mean_q: 5.268893
 67505/100000: episode: 1306, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 80.979, mean reward: 5.061 [3.875, 8.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.271, 10.100], loss: 0.383394, mae: 0.568918, mean_q: 5.429973
 67521/100000: episode: 1307, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 54.700, mean reward: 3.419 [2.512, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.249, 10.100], loss: 0.782111, mae: 0.649479, mean_q: 5.540268
 67545/100000: episode: 1308, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 69.676, mean reward: 2.903 [1.734, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.584, 10.100], loss: 0.381626, mae: 0.529394, mean_q: 5.260106
 67580/100000: episode: 1309, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 158.067, mean reward: 4.516 [2.419, 15.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.069, 10.100], loss: 0.701026, mae: 0.662424, mean_q: 5.394485
 67604/100000: episode: 1310, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 165.382, mean reward: 6.891 [2.703, 12.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.406, 10.100], loss: 0.659169, mae: 0.620648, mean_q: 5.374120
 67620/100000: episode: 1311, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 47.711, mean reward: 2.982 [2.276, 4.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.303, 10.100], loss: 0.492411, mae: 0.589196, mean_q: 5.488652
 67636/100000: episode: 1312, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 67.650, mean reward: 4.228 [3.174, 5.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.325, 10.100], loss: 0.350820, mae: 0.536506, mean_q: 5.263296
 67660/100000: episode: 1313, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 172.913, mean reward: 7.205 [3.117, 26.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.627, 10.100], loss: 0.410182, mae: 0.568891, mean_q: 5.336957
 67684/100000: episode: 1314, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 85.967, mean reward: 3.582 [2.536, 6.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.359, 10.100], loss: 0.998587, mae: 0.630054, mean_q: 5.363555
 67708/100000: episode: 1315, duration: 0.129s, episode steps: 24, steps per second: 185, episode reward: 85.922, mean reward: 3.580 [2.647, 4.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.824, 10.100], loss: 0.733250, mae: 0.661158, mean_q: 5.538128
 67732/100000: episode: 1316, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 69.710, mean reward: 2.905 [1.970, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.369, 10.100], loss: 1.203483, mae: 0.694432, mean_q: 5.572903
 67756/100000: episode: 1317, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 64.946, mean reward: 2.706 [1.989, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.776, 10.100], loss: 0.569447, mae: 0.608414, mean_q: 5.434183
 67773/100000: episode: 1318, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 260.267, mean reward: 15.310 [3.358, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.369, 10.100], loss: 0.456292, mae: 0.622537, mean_q: 5.581623
 67797/100000: episode: 1319, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 108.938, mean reward: 4.539 [3.105, 7.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.753, 10.100], loss: 1.174853, mae: 0.666392, mean_q: 5.520998
 67814/100000: episode: 1320, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 66.592, mean reward: 3.917 [2.422, 12.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.978, 10.100], loss: 4.462933, mae: 0.904437, mean_q: 5.794430
 67830/100000: episode: 1321, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 52.388, mean reward: 3.274 [2.531, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.352, 10.100], loss: 0.586411, mae: 0.585997, mean_q: 5.217043
 67846/100000: episode: 1322, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 74.636, mean reward: 4.665 [2.658, 7.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.622, 10.100], loss: 10.039287, mae: 1.030298, mean_q: 5.821361
 67870/100000: episode: 1323, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 153.556, mean reward: 6.398 [4.039, 12.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.568, 10.100], loss: 1.570538, mae: 0.697866, mean_q: 5.481224
 67905/100000: episode: 1324, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 179.158, mean reward: 5.119 [2.960, 7.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.473, 10.100], loss: 2.930698, mae: 0.832269, mean_q: 5.711139
 67929/100000: episode: 1325, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 69.543, mean reward: 2.898 [2.126, 5.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.115, 10.100], loss: 0.711528, mae: 0.712188, mean_q: 5.853570
 67945/100000: episode: 1326, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 69.904, mean reward: 4.369 [2.353, 9.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.324, 10.100], loss: 0.642162, mae: 0.643084, mean_q: 5.643651
 67980/100000: episode: 1327, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 113.451, mean reward: 3.241 [2.023, 4.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.344, 10.100], loss: 1.021185, mae: 0.738756, mean_q: 5.731264
 67996/100000: episode: 1328, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 62.505, mean reward: 3.907 [3.034, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.365, 10.100], loss: 6.433755, mae: 1.152157, mean_q: 6.204486
 68012/100000: episode: 1329, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 68.741, mean reward: 4.296 [2.829, 6.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.384, 10.100], loss: 0.860267, mae: 0.747823, mean_q: 5.779393
 68028/100000: episode: 1330, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 59.076, mean reward: 3.692 [2.436, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.305, 10.100], loss: 0.865026, mae: 0.728164, mean_q: 5.663400
 68063/100000: episode: 1331, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 122.150, mean reward: 3.490 [2.015, 7.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.124, 10.100], loss: 5.086189, mae: 0.950261, mean_q: 5.849747
 68087/100000: episode: 1332, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 97.029, mean reward: 4.043 [3.146, 6.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.326, 10.100], loss: 1.426931, mae: 0.712574, mean_q: 5.770203
 68122/100000: episode: 1333, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 311.250, mean reward: 8.893 [3.525, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.342, 10.100], loss: 0.958899, mae: 0.728347, mean_q: 5.740732
 68138/100000: episode: 1334, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 68.100, mean reward: 4.256 [2.831, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.501, 10.100], loss: 1.233944, mae: 0.805405, mean_q: 6.088182
 68154/100000: episode: 1335, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 82.101, mean reward: 5.131 [3.472, 10.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.517, 10.100], loss: 5.279334, mae: 0.993387, mean_q: 6.116344
 68170/100000: episode: 1336, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 63.948, mean reward: 3.997 [2.918, 7.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.867, 10.100], loss: 4.809896, mae: 0.914270, mean_q: 5.760879
 68205/100000: episode: 1337, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 145.201, mean reward: 4.149 [2.340, 10.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.287, 10.100], loss: 2.696111, mae: 0.807909, mean_q: 5.948570
 68240/100000: episode: 1338, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 105.490, mean reward: 3.014 [2.005, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.061, 10.100], loss: 1.361732, mae: 0.807539, mean_q: 6.044511
 68258/100000: episode: 1339, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 56.115, mean reward: 3.118 [1.925, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.218, 10.100], loss: 0.592762, mae: 0.693767, mean_q: 5.957831
 68282/100000: episode: 1340, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 97.866, mean reward: 4.078 [2.188, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.324, 10.100], loss: 1.094863, mae: 0.754486, mean_q: 5.919621
 68306/100000: episode: 1341, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 105.330, mean reward: 4.389 [2.420, 16.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.955, 10.100], loss: 7.420048, mae: 0.984802, mean_q: 6.266725
 68330/100000: episode: 1342, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 95.196, mean reward: 3.967 [2.414, 6.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.510, 10.100], loss: 0.847814, mae: 0.902614, mean_q: 5.937281
 68347/100000: episode: 1343, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 52.747, mean reward: 3.103 [2.810, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-2.086, 10.100], loss: 1.719841, mae: 0.867795, mean_q: 6.077806
 68371/100000: episode: 1344, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 109.233, mean reward: 4.551 [2.110, 12.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.315, 10.100], loss: 6.940845, mae: 1.110714, mean_q: 6.243444
 68395/100000: episode: 1345, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 92.774, mean reward: 3.866 [2.693, 5.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.524, 10.100], loss: 3.590539, mae: 0.921698, mean_q: 6.082437
[Info] FALSIFICATION!
[Info] Levels: [5.715445, 7.4701653, 15.196842]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 68415/100000: episode: 1346, duration: 4.657s, episode steps: 20, steps per second: 4, episode reward: 244.324, mean reward: 12.216 [3.076, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.519, 10.067], loss: 8.425762, mae: 1.098728, mean_q: 6.155751
 68515/100000: episode: 1347, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.377, mean reward: 1.904 [1.451, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.139, 10.293], loss: 6.657050, mae: 1.108752, mean_q: 6.305594
 68615/100000: episode: 1348, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 206.150, mean reward: 2.062 [1.525, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.522, 10.098], loss: 5.700069, mae: 0.983518, mean_q: 6.264827
 68715/100000: episode: 1349, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.214, mean reward: 1.912 [1.501, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.289, 10.145], loss: 5.735356, mae: 0.959024, mean_q: 6.339002
 68815/100000: episode: 1350, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 186.022, mean reward: 1.860 [1.467, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.450, 10.098], loss: 4.390242, mae: 0.893568, mean_q: 6.313006
 68915/100000: episode: 1351, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.901, mean reward: 1.929 [1.481, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.504, 10.140], loss: 2.537231, mae: 0.859012, mean_q: 6.380940
 69015/100000: episode: 1352, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.657, mean reward: 1.967 [1.474, 7.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.917, 10.158], loss: 4.726225, mae: 0.959676, mean_q: 6.337832
 69115/100000: episode: 1353, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 190.592, mean reward: 1.906 [1.495, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.113], loss: 2.682933, mae: 0.864784, mean_q: 6.269970
 69215/100000: episode: 1354, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 193.366, mean reward: 1.934 [1.471, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.505, 10.233], loss: 4.005541, mae: 0.893599, mean_q: 6.343352
 69315/100000: episode: 1355, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.990, mean reward: 1.860 [1.449, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.221], loss: 2.515588, mae: 0.834920, mean_q: 6.194770
 69415/100000: episode: 1356, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.514, mean reward: 1.885 [1.437, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.318, 10.098], loss: 2.559267, mae: 0.832664, mean_q: 6.127234
 69515/100000: episode: 1357, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.987, mean reward: 1.910 [1.496, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.166, 10.242], loss: 4.121044, mae: 0.893781, mean_q: 6.191746
 69615/100000: episode: 1358, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 183.011, mean reward: 1.830 [1.442, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.042, 10.098], loss: 3.925823, mae: 0.855846, mean_q: 6.137713
 69715/100000: episode: 1359, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 206.978, mean reward: 2.070 [1.454, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.269, 10.098], loss: 7.459689, mae: 1.081555, mean_q: 6.131559
 69815/100000: episode: 1360, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 196.110, mean reward: 1.961 [1.486, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.362, 10.257], loss: 2.208145, mae: 0.822098, mean_q: 6.108383
 69915/100000: episode: 1361, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.552, mean reward: 1.826 [1.448, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.280, 10.119], loss: 6.558647, mae: 0.968425, mean_q: 6.231315
 70015/100000: episode: 1362, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 211.832, mean reward: 2.118 [1.463, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.796, 10.098], loss: 4.712897, mae: 0.882062, mean_q: 6.059311
 70115/100000: episode: 1363, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.453, mean reward: 1.845 [1.461, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.620, 10.142], loss: 5.857982, mae: 0.984581, mean_q: 6.098709
 70215/100000: episode: 1364, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 202.221, mean reward: 2.022 [1.522, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.183, 10.161], loss: 3.624685, mae: 0.875993, mean_q: 5.998660
 70315/100000: episode: 1365, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.344, mean reward: 1.913 [1.458, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.215], loss: 7.820758, mae: 1.012273, mean_q: 6.077086
 70415/100000: episode: 1366, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.870, mean reward: 2.009 [1.469, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.815, 10.215], loss: 2.703551, mae: 0.819890, mean_q: 5.899007
 70515/100000: episode: 1367, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 202.483, mean reward: 2.025 [1.460, 11.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.733, 10.098], loss: 2.390140, mae: 0.826841, mean_q: 5.913413
 70615/100000: episode: 1368, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 193.854, mean reward: 1.939 [1.473, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.497, 10.297], loss: 4.368272, mae: 0.902752, mean_q: 5.980916
 70715/100000: episode: 1369, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.768, mean reward: 1.958 [1.464, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.046, 10.098], loss: 2.932866, mae: 0.789185, mean_q: 5.778702
 70815/100000: episode: 1370, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.443, mean reward: 1.794 [1.478, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.421, 10.098], loss: 2.220683, mae: 0.736505, mean_q: 5.700665
 70915/100000: episode: 1371, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 204.497, mean reward: 2.045 [1.455, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.695, 10.098], loss: 2.925635, mae: 0.794355, mean_q: 5.723598
 71015/100000: episode: 1372, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.300, mean reward: 1.973 [1.469, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.501, 10.098], loss: 7.550234, mae: 0.948932, mean_q: 5.808056
 71115/100000: episode: 1373, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.771, mean reward: 1.898 [1.454, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.844, 10.209], loss: 6.945801, mae: 0.949354, mean_q: 5.800183
 71215/100000: episode: 1374, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 202.686, mean reward: 2.027 [1.473, 5.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.677, 10.188], loss: 3.812485, mae: 0.800405, mean_q: 5.739316
 71315/100000: episode: 1375, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.107, mean reward: 1.881 [1.448, 2.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.892, 10.227], loss: 3.693894, mae: 0.749058, mean_q: 5.623965
 71415/100000: episode: 1376, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.661, mean reward: 1.917 [1.444, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.027, 10.241], loss: 8.062658, mae: 0.990204, mean_q: 5.914228
 71515/100000: episode: 1377, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 243.157, mean reward: 2.432 [1.515, 17.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.152], loss: 3.989644, mae: 0.784517, mean_q: 5.666945
 71615/100000: episode: 1378, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 192.485, mean reward: 1.925 [1.471, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.373], loss: 4.442353, mae: 0.829743, mean_q: 5.713762
 71715/100000: episode: 1379, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 201.625, mean reward: 2.016 [1.504, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.654, 10.098], loss: 4.578493, mae: 0.795630, mean_q: 5.443069
 71815/100000: episode: 1380, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.132, mean reward: 1.891 [1.449, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.317, 10.387], loss: 3.907706, mae: 0.737930, mean_q: 5.321099
 71915/100000: episode: 1381, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.761, mean reward: 1.978 [1.433, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.216], loss: 4.630250, mae: 0.803337, mean_q: 5.356390
 72015/100000: episode: 1382, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 181.626, mean reward: 1.816 [1.461, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.409, 10.268], loss: 1.343183, mae: 0.622784, mean_q: 5.156444
 72115/100000: episode: 1383, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 187.246, mean reward: 1.872 [1.470, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.090, 10.098], loss: 1.157887, mae: 0.566713, mean_q: 4.990725
 72215/100000: episode: 1384, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 185.734, mean reward: 1.857 [1.453, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.751, 10.199], loss: 4.653419, mae: 0.712321, mean_q: 5.010293
 72315/100000: episode: 1385, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 189.128, mean reward: 1.891 [1.455, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.214], loss: 4.540098, mae: 0.763196, mean_q: 4.964484
 72415/100000: episode: 1386, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 217.433, mean reward: 2.174 [1.483, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.997, 10.108], loss: 3.130648, mae: 0.624919, mean_q: 4.813258
 72515/100000: episode: 1387, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.108, mean reward: 1.911 [1.462, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.236, 10.268], loss: 2.458009, mae: 0.555320, mean_q: 4.697128
 72615/100000: episode: 1388, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.867, mean reward: 1.859 [1.518, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.064, 10.098], loss: 1.931764, mae: 0.512200, mean_q: 4.551601
 72715/100000: episode: 1389, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.927, mean reward: 1.909 [1.445, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.291, 10.099], loss: 2.401305, mae: 0.523358, mean_q: 4.507442
 72815/100000: episode: 1390, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.913, mean reward: 1.819 [1.443, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.782, 10.156], loss: 1.398207, mae: 0.468338, mean_q: 4.405519
 72915/100000: episode: 1391, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.200, mean reward: 1.882 [1.476, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.098], loss: 3.137775, mae: 0.488559, mean_q: 4.335529
 73015/100000: episode: 1392, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 231.041, mean reward: 2.310 [1.460, 5.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.430, 10.098], loss: 3.761268, mae: 0.521747, mean_q: 4.295118
 73115/100000: episode: 1393, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.854, mean reward: 1.899 [1.451, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.523, 10.164], loss: 0.244838, mae: 0.366889, mean_q: 4.197669
 73215/100000: episode: 1394, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 197.564, mean reward: 1.976 [1.502, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.231], loss: 2.182911, mae: 0.414383, mean_q: 4.103085
 73315/100000: episode: 1395, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 173.959, mean reward: 1.740 [1.443, 2.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.990, 10.199], loss: 2.204281, mae: 0.411693, mean_q: 4.010469
 73415/100000: episode: 1396, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 196.127, mean reward: 1.961 [1.454, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.666, 10.098], loss: 0.230232, mae: 0.323312, mean_q: 3.862736
 73515/100000: episode: 1397, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 226.921, mean reward: 2.269 [1.466, 5.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.066, 10.178], loss: 0.133365, mae: 0.310163, mean_q: 3.876686
 73615/100000: episode: 1398, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.063, mean reward: 1.931 [1.472, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.068, 10.098], loss: 0.128403, mae: 0.314791, mean_q: 3.863623
 73715/100000: episode: 1399, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 216.257, mean reward: 2.163 [1.470, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.947, 10.098], loss: 0.139441, mae: 0.314041, mean_q: 3.865184
 73815/100000: episode: 1400, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 204.004, mean reward: 2.040 [1.457, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.269, 10.442], loss: 0.123251, mae: 0.306084, mean_q: 3.885243
 73915/100000: episode: 1401, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 215.828, mean reward: 2.158 [1.454, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.589, 10.098], loss: 0.086219, mae: 0.288272, mean_q: 3.888737
 74015/100000: episode: 1402, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.404, mean reward: 1.924 [1.476, 4.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.373, 10.232], loss: 0.090468, mae: 0.296566, mean_q: 3.873969
 74115/100000: episode: 1403, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.246, mean reward: 1.922 [1.468, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.256, 10.133], loss: 0.117311, mae: 0.299080, mean_q: 3.897849
 74215/100000: episode: 1404, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.283, mean reward: 1.893 [1.447, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.186], loss: 0.080828, mae: 0.293539, mean_q: 3.876498
 74315/100000: episode: 1405, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.225, mean reward: 1.932 [1.477, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.920, 10.167], loss: 0.113036, mae: 0.290093, mean_q: 3.886808
 74415/100000: episode: 1406, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.445, mean reward: 1.914 [1.447, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.218, 10.098], loss: 0.078895, mae: 0.282163, mean_q: 3.876492
 74515/100000: episode: 1407, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.202, mean reward: 1.952 [1.472, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.461, 10.098], loss: 0.124390, mae: 0.299533, mean_q: 3.902212
 74615/100000: episode: 1408, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 208.103, mean reward: 2.081 [1.442, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.429, 10.098], loss: 0.148808, mae: 0.304898, mean_q: 3.914196
 74715/100000: episode: 1409, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 194.170, mean reward: 1.942 [1.491, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.084, 10.098], loss: 0.119752, mae: 0.300958, mean_q: 3.917903
 74815/100000: episode: 1410, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.438, mean reward: 1.934 [1.437, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.182, 10.098], loss: 0.116714, mae: 0.289972, mean_q: 3.898674
 74915/100000: episode: 1411, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 203.601, mean reward: 2.036 [1.437, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.683, 10.318], loss: 0.119554, mae: 0.298443, mean_q: 3.900609
 75015/100000: episode: 1412, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 197.406, mean reward: 1.974 [1.467, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.479, 10.290], loss: 0.111552, mae: 0.283249, mean_q: 3.889926
 75115/100000: episode: 1413, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 184.103, mean reward: 1.841 [1.472, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.364, 10.146], loss: 0.117908, mae: 0.301116, mean_q: 3.901832
 75215/100000: episode: 1414, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 188.677, mean reward: 1.887 [1.454, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.781, 10.098], loss: 0.081063, mae: 0.287533, mean_q: 3.886997
 75315/100000: episode: 1415, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 180.750, mean reward: 1.808 [1.454, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.098], loss: 0.080162, mae: 0.282853, mean_q: 3.898297
 75415/100000: episode: 1416, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.478, mean reward: 1.885 [1.456, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.239, 10.258], loss: 0.111256, mae: 0.285365, mean_q: 3.879171
 75515/100000: episode: 1417, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 215.252, mean reward: 2.153 [1.592, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.734, 10.098], loss: 0.080941, mae: 0.279146, mean_q: 3.895762
 75615/100000: episode: 1418, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.663, mean reward: 1.887 [1.482, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.982, 10.110], loss: 0.076337, mae: 0.286053, mean_q: 3.888755
 75715/100000: episode: 1419, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 234.267, mean reward: 2.343 [1.498, 7.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.098], loss: 0.068352, mae: 0.270303, mean_q: 3.857120
 75815/100000: episode: 1420, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 214.037, mean reward: 2.140 [1.478, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.098], loss: 0.119714, mae: 0.293755, mean_q: 3.901649
 75915/100000: episode: 1421, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 196.550, mean reward: 1.966 [1.460, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.739, 10.260], loss: 0.114323, mae: 0.297814, mean_q: 3.919886
 76015/100000: episode: 1422, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 190.642, mean reward: 1.906 [1.477, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.156], loss: 0.075980, mae: 0.281480, mean_q: 3.906862
 76115/100000: episode: 1423, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.286, mean reward: 1.853 [1.468, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.866, 10.172], loss: 0.084402, mae: 0.292372, mean_q: 3.904493
 76215/100000: episode: 1424, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.535, mean reward: 1.945 [1.498, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.392, 10.098], loss: 0.117835, mae: 0.289356, mean_q: 3.900577
 76315/100000: episode: 1425, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.029, mean reward: 1.850 [1.461, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.671, 10.153], loss: 0.084064, mae: 0.285986, mean_q: 3.887488
 76415/100000: episode: 1426, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 183.187, mean reward: 1.832 [1.430, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.445, 10.110], loss: 0.110683, mae: 0.284868, mean_q: 3.879249
 76515/100000: episode: 1427, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 204.465, mean reward: 2.045 [1.453, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.153], loss: 0.074597, mae: 0.276517, mean_q: 3.878782
 76615/100000: episode: 1428, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.011, mean reward: 1.930 [1.447, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.881, 10.176], loss: 0.076029, mae: 0.275103, mean_q: 3.879894
 76715/100000: episode: 1429, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 214.189, mean reward: 2.142 [1.455, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.172], loss: 0.076189, mae: 0.278557, mean_q: 3.889527
 76815/100000: episode: 1430, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.645, mean reward: 1.976 [1.463, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.445, 10.233], loss: 0.072249, mae: 0.275449, mean_q: 3.883776
 76915/100000: episode: 1431, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 205.548, mean reward: 2.055 [1.547, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.632, 10.098], loss: 0.075363, mae: 0.277714, mean_q: 3.887094
 77015/100000: episode: 1432, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 241.759, mean reward: 2.418 [1.463, 5.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.228, 10.318], loss: 0.072238, mae: 0.271783, mean_q: 3.887091
 77115/100000: episode: 1433, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.183, mean reward: 1.942 [1.438, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.824, 10.098], loss: 0.083749, mae: 0.288546, mean_q: 3.916235
 77215/100000: episode: 1434, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 203.100, mean reward: 2.031 [1.487, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.599, 10.253], loss: 0.092895, mae: 0.293324, mean_q: 3.931787
 77315/100000: episode: 1435, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 200.146, mean reward: 2.001 [1.498, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.433, 10.250], loss: 0.081836, mae: 0.285694, mean_q: 3.927138
 77415/100000: episode: 1436, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.824, mean reward: 1.878 [1.435, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.238, 10.117], loss: 0.091298, mae: 0.299929, mean_q: 3.927296
 77515/100000: episode: 1437, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.545, mean reward: 1.955 [1.440, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.512, 10.247], loss: 0.084368, mae: 0.288363, mean_q: 3.929443
 77615/100000: episode: 1438, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.210, mean reward: 1.882 [1.463, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.143], loss: 0.083964, mae: 0.287998, mean_q: 3.936907
 77715/100000: episode: 1439, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.914, mean reward: 1.859 [1.458, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.767, 10.098], loss: 0.090336, mae: 0.289725, mean_q: 3.909356
 77815/100000: episode: 1440, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.686, mean reward: 1.927 [1.473, 7.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.571, 10.208], loss: 0.092449, mae: 0.288894, mean_q: 3.918904
 77915/100000: episode: 1441, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 205.376, mean reward: 2.054 [1.461, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.560, 10.098], loss: 0.090348, mae: 0.291893, mean_q: 3.926863
 78015/100000: episode: 1442, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 204.252, mean reward: 2.043 [1.488, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.089, 10.298], loss: 0.093410, mae: 0.295146, mean_q: 3.925086
 78115/100000: episode: 1443, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 215.432, mean reward: 2.154 [1.454, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.487, 10.098], loss: 0.090037, mae: 0.295737, mean_q: 3.906869
 78215/100000: episode: 1444, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.062, mean reward: 1.991 [1.452, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.212], loss: 0.095920, mae: 0.299462, mean_q: 3.929900
 78315/100000: episode: 1445, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 215.046, mean reward: 2.150 [1.492, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.338, 10.098], loss: 0.087391, mae: 0.288738, mean_q: 3.925214
[Info] 1-TH LEVEL FOUND: 5.917520046234131, Considering 10/90 traces
 78415/100000: episode: 1446, duration: 4.705s, episode steps: 100, steps per second: 21, episode reward: 213.128, mean reward: 2.131 [1.487, 4.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.132], loss: 0.093749, mae: 0.300061, mean_q: 3.932372
 78431/100000: episode: 1447, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 42.671, mean reward: 2.667 [1.952, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.266, 10.100], loss: 0.083947, mae: 0.290713, mean_q: 3.959943
 78474/100000: episode: 1448, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 91.803, mean reward: 2.135 [1.488, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.108, 10.100], loss: 0.096927, mae: 0.297541, mean_q: 3.955970
 78524/100000: episode: 1449, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 122.000, mean reward: 2.440 [1.598, 6.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.382, 10.281], loss: 0.099837, mae: 0.296672, mean_q: 3.941436
 78570/100000: episode: 1450, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 113.295, mean reward: 2.463 [1.902, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.304, 10.436], loss: 0.089022, mae: 0.299125, mean_q: 3.955267
 78616/100000: episode: 1451, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 128.400, mean reward: 2.791 [1.792, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.693, 10.421], loss: 0.081036, mae: 0.278883, mean_q: 3.992355
 78632/100000: episode: 1452, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 45.422, mean reward: 2.839 [2.476, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.974, 10.100], loss: 0.086305, mae: 0.296147, mean_q: 3.967012
 78682/100000: episode: 1453, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 127.166, mean reward: 2.543 [1.633, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.078, 10.393], loss: 0.101313, mae: 0.299434, mean_q: 3.965206
 78708/100000: episode: 1454, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 66.209, mean reward: 2.547 [1.866, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.240, 10.294], loss: 0.089235, mae: 0.303033, mean_q: 3.972325
 78723/100000: episode: 1455, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 39.344, mean reward: 2.623 [2.197, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.185, 10.100], loss: 0.109854, mae: 0.331862, mean_q: 4.046254
 78773/100000: episode: 1456, duration: 0.253s, episode steps: 50, steps per second: 198, episode reward: 143.326, mean reward: 2.867 [2.051, 7.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.415, 10.407], loss: 0.089483, mae: 0.287505, mean_q: 3.986157
 78846/100000: episode: 1457, duration: 0.371s, episode steps: 73, steps per second: 197, episode reward: 141.656, mean reward: 1.940 [1.443, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.221, 10.177], loss: 0.112154, mae: 0.302883, mean_q: 4.013304
 78851/100000: episode: 1458, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 14.389, mean reward: 2.878 [2.717, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.342, 10.100], loss: 0.058528, mae: 0.252670, mean_q: 3.916626
 78924/100000: episode: 1459, duration: 0.374s, episode steps: 73, steps per second: 195, episode reward: 190.929, mean reward: 2.615 [1.924, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.698 [-0.904, 10.100], loss: 0.112933, mae: 0.313767, mean_q: 4.014186
 78967/100000: episode: 1460, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 93.160, mean reward: 2.167 [1.507, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.417, 10.147], loss: 0.094933, mae: 0.309176, mean_q: 4.026898
 78993/100000: episode: 1461, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 70.438, mean reward: 2.709 [2.046, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.706, 10.429], loss: 0.130551, mae: 0.331955, mean_q: 4.067931
 79024/100000: episode: 1462, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 83.527, mean reward: 2.694 [1.967, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.082, 10.100], loss: 0.113504, mae: 0.307214, mean_q: 4.027102
 79050/100000: episode: 1463, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 79.349, mean reward: 3.052 [2.229, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.677, 10.375], loss: 0.084909, mae: 0.290310, mean_q: 4.028725
 79123/100000: episode: 1464, duration: 0.386s, episode steps: 73, steps per second: 189, episode reward: 144.318, mean reward: 1.977 [1.491, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.664, 10.197], loss: 0.110648, mae: 0.316248, mean_q: 4.064052
 79149/100000: episode: 1465, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 90.530, mean reward: 3.482 [2.616, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.541], loss: 0.084503, mae: 0.301920, mean_q: 4.053740
 79164/100000: episode: 1466, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 41.303, mean reward: 2.754 [2.429, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.330, 10.100], loss: 0.087899, mae: 0.312323, mean_q: 4.063105
 79179/100000: episode: 1467, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 48.424, mean reward: 3.228 [2.625, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.399, 10.100], loss: 0.129217, mae: 0.323031, mean_q: 4.045688
 79205/100000: episode: 1468, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 76.287, mean reward: 2.934 [1.816, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.318], loss: 0.094156, mae: 0.312633, mean_q: 4.052763
 79248/100000: episode: 1469, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 91.215, mean reward: 2.121 [1.531, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.163, 10.237], loss: 0.097073, mae: 0.309353, mean_q: 4.066344
 79278/100000: episode: 1470, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 83.810, mean reward: 2.794 [2.287, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.416, 10.100], loss: 0.106403, mae: 0.303135, mean_q: 4.114879
 79294/100000: episode: 1471, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 49.439, mean reward: 3.090 [2.375, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.740, 10.100], loss: 0.137225, mae: 0.321989, mean_q: 4.020191
 79309/100000: episode: 1472, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 53.239, mean reward: 3.549 [2.784, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.474, 10.100], loss: 0.118562, mae: 0.324251, mean_q: 4.180164
 79382/100000: episode: 1473, duration: 0.366s, episode steps: 73, steps per second: 199, episode reward: 169.367, mean reward: 2.320 [1.709, 5.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-1.611, 10.100], loss: 0.096816, mae: 0.304325, mean_q: 4.097834
 79425/100000: episode: 1474, duration: 0.222s, episode steps: 43, steps per second: 194, episode reward: 92.965, mean reward: 2.162 [1.634, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.146, 10.198], loss: 0.106629, mae: 0.328355, mean_q: 4.106818
 79455/100000: episode: 1475, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 83.967, mean reward: 2.799 [2.078, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.212, 10.100], loss: 0.102879, mae: 0.317370, mean_q: 4.111792
 79460/100000: episode: 1476, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 15.191, mean reward: 3.038 [2.510, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.350, 10.100], loss: 0.162603, mae: 0.298811, mean_q: 4.124025
 79491/100000: episode: 1477, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 74.224, mean reward: 2.394 [1.639, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.928, 10.100], loss: 0.119238, mae: 0.328356, mean_q: 4.161489
 79517/100000: episode: 1478, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 117.271, mean reward: 4.510 [3.144, 7.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.583], loss: 0.092583, mae: 0.306378, mean_q: 4.177751
 79548/100000: episode: 1479, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 85.663, mean reward: 2.763 [1.933, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.442, 10.100], loss: 0.149592, mae: 0.337990, mean_q: 4.137207
 79553/100000: episode: 1480, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 15.540, mean reward: 3.108 [2.182, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.394, 10.100], loss: 0.097038, mae: 0.289488, mean_q: 4.071964
 79579/100000: episode: 1481, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 73.298, mean reward: 2.819 [1.641, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.073, 10.307], loss: 0.121020, mae: 0.332687, mean_q: 4.219415
 79652/100000: episode: 1482, duration: 0.397s, episode steps: 73, steps per second: 184, episode reward: 170.458, mean reward: 2.335 [1.512, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.731, 10.103], loss: 0.123681, mae: 0.338164, mean_q: 4.173955
 79725/100000: episode: 1483, duration: 0.383s, episode steps: 73, steps per second: 191, episode reward: 169.355, mean reward: 2.320 [1.663, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.283, 10.100], loss: 0.121470, mae: 0.329567, mean_q: 4.189062
 79771/100000: episode: 1484, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 130.839, mean reward: 2.844 [2.096, 5.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.687, 10.367], loss: 0.136638, mae: 0.349508, mean_q: 4.263725
 79797/100000: episode: 1485, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 75.564, mean reward: 2.906 [2.169, 4.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.659, 10.457], loss: 0.123199, mae: 0.343282, mean_q: 4.211115
 79843/100000: episode: 1486, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 102.763, mean reward: 2.234 [1.584, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.732, 10.166], loss: 0.115905, mae: 0.335954, mean_q: 4.275766
 79848/100000: episode: 1487, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 15.629, mean reward: 3.126 [2.586, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.497, 10.100], loss: 0.112564, mae: 0.311136, mean_q: 4.281964
 79879/100000: episode: 1488, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 100.352, mean reward: 3.237 [2.508, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.462, 10.100], loss: 0.113433, mae: 0.312114, mean_q: 4.221967
 79925/100000: episode: 1489, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 98.290, mean reward: 2.137 [1.581, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.183, 10.241], loss: 0.130377, mae: 0.354530, mean_q: 4.246390
 79956/100000: episode: 1490, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 125.272, mean reward: 4.041 [2.387, 6.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.247, 10.100], loss: 0.137096, mae: 0.365564, mean_q: 4.291476
 80029/100000: episode: 1491, duration: 0.388s, episode steps: 73, steps per second: 188, episode reward: 139.304, mean reward: 1.908 [1.462, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-1.440, 10.271], loss: 0.146876, mae: 0.368168, mean_q: 4.284571
 80034/100000: episode: 1492, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 13.477, mean reward: 2.695 [2.265, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.406, 10.100], loss: 0.174230, mae: 0.414509, mean_q: 4.249394
 80084/100000: episode: 1493, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 108.650, mean reward: 2.173 [1.455, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.491, 10.100], loss: 0.110419, mae: 0.330510, mean_q: 4.295280
 80100/100000: episode: 1494, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 54.718, mean reward: 3.420 [2.794, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.445, 10.100], loss: 0.124807, mae: 0.355445, mean_q: 4.318118
 80116/100000: episode: 1495, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 52.411, mean reward: 3.276 [2.620, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.410, 10.100], loss: 0.159424, mae: 0.351779, mean_q: 4.233659
 80166/100000: episode: 1496, duration: 0.258s, episode steps: 50, steps per second: 193, episode reward: 157.779, mean reward: 3.156 [1.613, 9.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.386, 10.246], loss: 0.145908, mae: 0.359322, mean_q: 4.304956
 80212/100000: episode: 1497, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 138.839, mean reward: 3.018 [2.173, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.022, 10.499], loss: 0.141537, mae: 0.356430, mean_q: 4.303353
 80243/100000: episode: 1498, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 67.829, mean reward: 2.188 [1.512, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.212], loss: 0.134185, mae: 0.361323, mean_q: 4.322646
 80293/100000: episode: 1499, duration: 0.264s, episode steps: 50, steps per second: 190, episode reward: 134.362, mean reward: 2.687 [1.864, 5.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.439, 10.545], loss: 0.149600, mae: 0.361971, mean_q: 4.340504
 80339/100000: episode: 1500, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 125.468, mean reward: 2.728 [1.853, 4.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.194, 10.260], loss: 0.143234, mae: 0.367116, mean_q: 4.334449
 80355/100000: episode: 1501, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 45.604, mean reward: 2.850 [2.246, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.965, 10.100], loss: 0.113712, mae: 0.335176, mean_q: 4.367258
 80385/100000: episode: 1502, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 79.971, mean reward: 2.666 [2.185, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.361, 10.100], loss: 0.138212, mae: 0.352095, mean_q: 4.376843
 80401/100000: episode: 1503, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 42.168, mean reward: 2.635 [2.258, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.238, 10.100], loss: 0.199404, mae: 0.399235, mean_q: 4.422650
 80432/100000: episode: 1504, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 85.974, mean reward: 2.773 [1.833, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.775, 10.100], loss: 0.111472, mae: 0.333709, mean_q: 4.369277
 80463/100000: episode: 1505, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 63.702, mean reward: 2.055 [1.480, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.123, 10.127], loss: 0.151384, mae: 0.370378, mean_q: 4.464634
 80513/100000: episode: 1506, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 178.024, mean reward: 3.560 [1.520, 8.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.401, 10.162], loss: 0.141196, mae: 0.354876, mean_q: 4.403065
 80539/100000: episode: 1507, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 61.024, mean reward: 2.347 [1.636, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.080, 10.284], loss: 0.161034, mae: 0.367197, mean_q: 4.461372
 80569/100000: episode: 1508, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 71.113, mean reward: 2.370 [1.806, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.357, 10.100], loss: 0.157327, mae: 0.372141, mean_q: 4.402623
 80600/100000: episode: 1509, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 96.602, mean reward: 3.116 [2.028, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.670, 10.100], loss: 0.142582, mae: 0.363618, mean_q: 4.432917
 80615/100000: episode: 1510, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 47.258, mean reward: 3.151 [2.314, 4.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.287, 10.100], loss: 0.157567, mae: 0.364016, mean_q: 4.351199
 80658/100000: episode: 1511, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 93.344, mean reward: 2.171 [1.540, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.152, 10.127], loss: 0.139803, mae: 0.365970, mean_q: 4.432353
 80708/100000: episode: 1512, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 148.383, mean reward: 2.968 [2.166, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.239, 10.529], loss: 0.154686, mae: 0.372782, mean_q: 4.473316
 80713/100000: episode: 1513, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 14.552, mean reward: 2.910 [2.587, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.405, 10.100], loss: 0.154429, mae: 0.378409, mean_q: 4.297598
 80718/100000: episode: 1514, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 12.408, mean reward: 2.482 [2.374, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.452, 10.100], loss: 0.259589, mae: 0.451895, mean_q: 4.467986
 80764/100000: episode: 1515, duration: 0.243s, episode steps: 46, steps per second: 190, episode reward: 109.900, mean reward: 2.389 [1.737, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.306, 10.292], loss: 0.151176, mae: 0.372330, mean_q: 4.482371
 80810/100000: episode: 1516, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 115.662, mean reward: 2.514 [1.895, 5.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.643, 10.341], loss: 0.156278, mae: 0.387369, mean_q: 4.465516
 80815/100000: episode: 1517, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 13.303, mean reward: 2.661 [2.350, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.420, 10.100], loss: 0.207834, mae: 0.425449, mean_q: 4.386580
 80861/100000: episode: 1518, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 132.711, mean reward: 2.885 [1.926, 7.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.298, 10.484], loss: 0.160493, mae: 0.389514, mean_q: 4.507577
 80866/100000: episode: 1519, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 14.239, mean reward: 2.848 [2.486, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.779, 10.100], loss: 0.131192, mae: 0.367176, mean_q: 4.327034
 80912/100000: episode: 1520, duration: 0.245s, episode steps: 46, steps per second: 188, episode reward: 99.541, mean reward: 2.164 [1.503, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.683, 10.298], loss: 0.162649, mae: 0.376901, mean_q: 4.431677
 80927/100000: episode: 1521, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 36.591, mean reward: 2.439 [2.090, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.174, 10.100], loss: 0.162280, mae: 0.391014, mean_q: 4.530677
 80970/100000: episode: 1522, duration: 0.237s, episode steps: 43, steps per second: 181, episode reward: 107.172, mean reward: 2.492 [1.776, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.544, 10.322], loss: 0.178291, mae: 0.404673, mean_q: 4.507124
 81043/100000: episode: 1523, duration: 0.365s, episode steps: 73, steps per second: 200, episode reward: 143.133, mean reward: 1.961 [1.513, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.169, 10.407], loss: 0.149839, mae: 0.383657, mean_q: 4.552979
 81073/100000: episode: 1524, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 72.459, mean reward: 2.415 [1.556, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.404, 10.100], loss: 0.131250, mae: 0.362057, mean_q: 4.427471
 81123/100000: episode: 1525, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 117.042, mean reward: 2.341 [1.456, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.374, 10.130], loss: 0.202535, mae: 0.405345, mean_q: 4.581744
 81128/100000: episode: 1526, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 14.532, mean reward: 2.906 [2.535, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.410, 10.100], loss: 0.157186, mae: 0.396982, mean_q: 4.617093
 81201/100000: episode: 1527, duration: 0.371s, episode steps: 73, steps per second: 197, episode reward: 160.808, mean reward: 2.203 [1.596, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.121, 10.100], loss: 0.159464, mae: 0.381632, mean_q: 4.487990
 81244/100000: episode: 1528, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 116.456, mean reward: 2.708 [1.827, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.304, 10.448], loss: 0.150334, mae: 0.376598, mean_q: 4.521906
 81294/100000: episode: 1529, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 142.528, mean reward: 2.851 [1.620, 5.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.673, 10.275], loss: 0.145476, mae: 0.371546, mean_q: 4.569773
 81337/100000: episode: 1530, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 112.316, mean reward: 2.612 [2.056, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.413, 10.342], loss: 0.159947, mae: 0.387686, mean_q: 4.569349
 81367/100000: episode: 1531, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 61.637, mean reward: 2.055 [1.614, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.077, 10.100], loss: 0.169758, mae: 0.399504, mean_q: 4.581571
 81417/100000: episode: 1532, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 120.756, mean reward: 2.415 [1.632, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.389, 10.280], loss: 0.222412, mae: 0.438268, mean_q: 4.663209
 81467/100000: episode: 1533, duration: 0.244s, episode steps: 50, steps per second: 205, episode reward: 124.121, mean reward: 2.482 [1.896, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.384, 10.376], loss: 0.167599, mae: 0.392904, mean_q: 4.599877
 81498/100000: episode: 1534, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 140.311, mean reward: 4.526 [2.775, 8.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.597, 10.100], loss: 0.196382, mae: 0.401338, mean_q: 4.598513
 81529/100000: episode: 1535, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 72.264, mean reward: 2.331 [1.840, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.139, 10.100], loss: 0.193352, mae: 0.422870, mean_q: 4.654112
[Info] 2-TH LEVEL FOUND: 7.364880084991455, Considering 10/90 traces
 81534/100000: episode: 1536, duration: 4.228s, episode steps: 5, steps per second: 1, episode reward: 18.994, mean reward: 3.799 [2.463, 6.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.528, 10.100], loss: 0.185823, mae: 0.429345, mean_q: 4.861259
 81560/100000: episode: 1537, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 589.309, mean reward: 22.666 [2.603, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.392, 10.100], loss: 0.202142, mae: 0.423948, mean_q: 4.647522
 81584/100000: episode: 1538, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 60.415, mean reward: 2.517 [1.886, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.519, 10.317], loss: 0.168736, mae: 0.410104, mean_q: 4.654555
 81609/100000: episode: 1539, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 87.470, mean reward: 3.499 [2.143, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.490], loss: 1.385912, mae: 0.578146, mean_q: 4.744966
 81614/100000: episode: 1540, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 21.484, mean reward: 4.297 [3.657, 5.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.609], loss: 29.129385, mae: 1.146873, mean_q: 4.398843
 81619/100000: episode: 1541, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 21.453, mean reward: 4.291 [3.906, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.519], loss: 0.545500, mae: 0.792901, mean_q: 5.267896
 81643/100000: episode: 1542, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 63.521, mean reward: 2.647 [1.784, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.331, 10.371], loss: 13.230285, mae: 1.235830, mean_q: 5.098430
 81667/100000: episode: 1543, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 59.157, mean reward: 2.465 [1.685, 5.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.259], loss: 1.954784, mae: 0.977615, mean_q: 4.587470
 81691/100000: episode: 1544, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 67.055, mean reward: 2.794 [1.769, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.345], loss: 1.314292, mae: 0.636273, mean_q: 4.848614
 81715/100000: episode: 1545, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 56.572, mean reward: 2.357 [1.880, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.563, 10.288], loss: 0.523999, mae: 0.497852, mean_q: 4.729273
 81740/100000: episode: 1546, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 151.675, mean reward: 6.067 [3.756, 8.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.583], loss: 6.078951, mae: 0.669845, mean_q: 4.820095
 81766/100000: episode: 1547, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 78.809, mean reward: 3.031 [2.286, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.616, 10.100], loss: 0.416942, mae: 0.562660, mean_q: 4.844856
 81790/100000: episode: 1548, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 71.750, mean reward: 2.990 [2.386, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.405], loss: 7.346382, mae: 0.675710, mean_q: 4.806149
 81816/100000: episode: 1549, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 190.623, mean reward: 7.332 [2.738, 68.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.315, 10.100], loss: 8.838842, mae: 1.053946, mean_q: 5.156401
 81821/100000: episode: 1550, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 18.688, mean reward: 3.738 [3.142, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.510], loss: 0.413119, mae: 0.574891, mean_q: 4.738422
 81847/100000: episode: 1551, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 89.981, mean reward: 3.461 [2.784, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.349, 10.100], loss: 6.634841, mae: 0.779728, mean_q: 4.927347
 81883/100000: episode: 1552, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 146.292, mean reward: 4.064 [1.983, 7.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.310, 10.371], loss: 5.069119, mae: 0.766675, mean_q: 5.038563
 81907/100000: episode: 1553, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 63.238, mean reward: 2.635 [1.934, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.057, 10.313], loss: 6.557824, mae: 0.768777, mean_q: 5.157946
 81932/100000: episode: 1554, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 71.592, mean reward: 2.864 [1.988, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.074, 10.288], loss: 17.585598, mae: 1.212295, mean_q: 5.197148
 81968/100000: episode: 1555, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 117.460, mean reward: 3.263 [2.164, 5.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.371, 10.485], loss: 0.453180, mae: 0.583609, mean_q: 4.901101
 81993/100000: episode: 1556, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 80.605, mean reward: 3.224 [2.038, 5.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.523], loss: 1.743584, mae: 0.651972, mean_q: 5.065063
 82017/100000: episode: 1557, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 78.615, mean reward: 3.276 [2.716, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.393], loss: 4.543521, mae: 0.835163, mean_q: 5.090884
 82041/100000: episode: 1558, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 62.269, mean reward: 2.595 [1.880, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.658, 10.337], loss: 0.468572, mae: 0.565518, mean_q: 5.141677
 82067/100000: episode: 1559, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 88.596, mean reward: 3.408 [2.567, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.169, 10.100], loss: 7.106553, mae: 0.897385, mean_q: 5.245221
 82092/100000: episode: 1560, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 82.645, mean reward: 3.306 [2.538, 4.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.494], loss: 0.502555, mae: 0.520619, mean_q: 4.933278
 82118/100000: episode: 1561, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 73.997, mean reward: 2.846 [2.031, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.288, 10.100], loss: 0.444258, mae: 0.535266, mean_q: 5.046019
 82143/100000: episode: 1562, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 87.172, mean reward: 3.487 [2.403, 5.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.505], loss: 0.396096, mae: 0.539412, mean_q: 5.095440
 82168/100000: episode: 1563, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 100.684, mean reward: 4.027 [2.757, 6.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.484], loss: 0.484790, mae: 0.487738, mean_q: 5.026526
 82192/100000: episode: 1564, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 66.741, mean reward: 2.781 [1.828, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.046, 10.300], loss: 0.303743, mae: 0.478316, mean_q: 5.071278
 82216/100000: episode: 1565, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 72.914, mean reward: 3.038 [2.328, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.130, 10.424], loss: 0.409911, mae: 0.521811, mean_q: 4.974807
 82240/100000: episode: 1566, duration: 0.122s, episode steps: 24, steps per second: 198, episode reward: 64.861, mean reward: 2.703 [1.527, 6.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.732, 10.176], loss: 0.292305, mae: 0.485851, mean_q: 5.079403
 82245/100000: episode: 1567, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 23.543, mean reward: 4.709 [3.824, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.669, 10.621], loss: 0.434735, mae: 0.563571, mean_q: 4.967041
 82281/100000: episode: 1568, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 209.287, mean reward: 5.814 [2.404, 18.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.258, 10.446], loss: 1.270995, mae: 0.627837, mean_q: 5.231781
 82307/100000: episode: 1569, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 100.702, mean reward: 3.873 [2.795, 5.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.603, 10.100], loss: 6.632317, mae: 0.684008, mean_q: 5.073112
 82333/100000: episode: 1570, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 72.238, mean reward: 2.778 [2.076, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.418, 10.100], loss: 0.751523, mae: 0.681603, mean_q: 5.381390
 82359/100000: episode: 1571, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 76.135, mean reward: 2.928 [2.004, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.201, 10.100], loss: 7.065545, mae: 0.793713, mean_q: 5.253372
 82380/100000: episode: 1572, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 85.669, mean reward: 4.079 [2.466, 15.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.165, 10.375], loss: 0.409853, mae: 0.560042, mean_q: 5.067917
 82401/100000: episode: 1573, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 72.105, mean reward: 3.434 [2.154, 4.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.194, 10.367], loss: 12.672483, mae: 1.040979, mean_q: 5.283930
 82427/100000: episode: 1574, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 103.572, mean reward: 3.984 [2.450, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.267, 10.100], loss: 0.899388, mae: 0.826673, mean_q: 5.421755
 82463/100000: episode: 1575, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 96.590, mean reward: 2.683 [1.499, 6.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.062, 10.112], loss: 1.329679, mae: 0.649831, mean_q: 5.302551
 82468/100000: episode: 1576, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 19.279, mean reward: 3.856 [3.280, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.525], loss: 3.542479, mae: 0.744557, mean_q: 5.327080
 82504/100000: episode: 1577, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 124.237, mean reward: 3.451 [1.658, 6.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.362, 10.300], loss: 0.446256, mae: 0.588426, mean_q: 5.272597
 82525/100000: episode: 1578, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 68.235, mean reward: 3.249 [1.897, 6.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.268], loss: 7.401250, mae: 0.811534, mean_q: 5.378064
 82561/100000: episode: 1579, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 165.073, mean reward: 4.585 [2.453, 8.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.689, 10.482], loss: 5.264614, mae: 0.811766, mean_q: 5.419708
 82585/100000: episode: 1580, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 75.275, mean reward: 3.136 [2.506, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.065, 10.470], loss: 0.576317, mae: 0.628543, mean_q: 5.363288
 82609/100000: episode: 1581, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 104.468, mean reward: 4.353 [2.813, 6.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.318, 10.504], loss: 0.397187, mae: 0.553859, mean_q: 5.170389
 82633/100000: episode: 1582, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 67.706, mean reward: 2.821 [2.209, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.421], loss: 8.428504, mae: 0.858762, mean_q: 5.504579
 82638/100000: episode: 1583, duration: 0.038s, episode steps: 5, steps per second: 130, episode reward: 18.111, mean reward: 3.622 [3.291, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.554], loss: 28.987818, mae: 1.451156, mean_q: 5.252197
 82663/100000: episode: 1584, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 72.573, mean reward: 2.903 [2.428, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.447, 10.508], loss: 12.075352, mae: 1.073396, mean_q: 5.747366
[Info] FALSIFICATION!
[Info] Levels: [5.91752, 7.36488, 9.15654]
[Info] Cond. Prob: [0.1, 0.1, 0.18]
[Info] Error Prob: 0.0018000000000000004

 82665/100000: episode: 1585, duration: 4.436s, episode steps: 2, steps per second: 0, episode reward: 110.372, mean reward: 55.186 [10.372, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.751 [-0.075, 7.956], loss: 1.353511, mae: 0.859321, mean_q: 5.390038
 82765/100000: episode: 1586, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.855, mean reward: 1.879 [1.454, 2.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.476, 10.098], loss: 0.714939, mae: 0.595658, mean_q: 5.420833
 82865/100000: episode: 1587, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 185.261, mean reward: 1.853 [1.452, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.307, 10.098], loss: 7.885626, mae: 0.961722, mean_q: 5.513482
 82965/100000: episode: 1588, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.676, mean reward: 1.797 [1.457, 2.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.507, 10.098], loss: 7.687987, mae: 0.882906, mean_q: 5.514117
 83065/100000: episode: 1589, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 191.054, mean reward: 1.911 [1.514, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.098], loss: 6.265201, mae: 0.806150, mean_q: 5.524725
 83165/100000: episode: 1590, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.471, mean reward: 1.915 [1.459, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.561, 10.098], loss: 4.493956, mae: 0.806379, mean_q: 5.542200
 83265/100000: episode: 1591, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 193.259, mean reward: 1.933 [1.471, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.771, 10.177], loss: 6.330762, mae: 0.811019, mean_q: 5.401611
 83365/100000: episode: 1592, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 223.176, mean reward: 2.232 [1.559, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.330, 10.221], loss: 3.482461, mae: 0.689815, mean_q: 5.415105
 83465/100000: episode: 1593, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.066, mean reward: 1.971 [1.468, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.776, 10.098], loss: 2.941875, mae: 0.734112, mean_q: 5.434920
 83565/100000: episode: 1594, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.131, mean reward: 1.831 [1.485, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.259, 10.098], loss: 4.576037, mae: 0.780813, mean_q: 5.385175
 83665/100000: episode: 1595, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.980, mean reward: 1.980 [1.456, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.391, 10.237], loss: 2.022398, mae: 0.657280, mean_q: 5.312653
 83765/100000: episode: 1596, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.650, mean reward: 1.866 [1.453, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.703, 10.308], loss: 7.494672, mae: 0.826896, mean_q: 5.278882
 83865/100000: episode: 1597, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.778, mean reward: 2.088 [1.468, 5.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.021, 10.098], loss: 4.346421, mae: 0.754625, mean_q: 5.338141
 83965/100000: episode: 1598, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.883, mean reward: 1.789 [1.442, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.010, 10.098], loss: 1.964774, mae: 0.630059, mean_q: 5.256212
 84065/100000: episode: 1599, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.962, mean reward: 1.880 [1.448, 2.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.277, 10.267], loss: 3.354498, mae: 0.674126, mean_q: 5.286533
 84165/100000: episode: 1600, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 196.543, mean reward: 1.965 [1.454, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.697, 10.300], loss: 4.051006, mae: 0.709137, mean_q: 5.194085
 84265/100000: episode: 1601, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 198.302, mean reward: 1.983 [1.458, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.483, 10.098], loss: 2.725933, mae: 0.654124, mean_q: 5.125257
 84365/100000: episode: 1602, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.726, mean reward: 1.897 [1.453, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.266], loss: 4.360471, mae: 0.774586, mean_q: 5.151618
 84465/100000: episode: 1603, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.292, mean reward: 1.883 [1.464, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.654, 10.098], loss: 2.728096, mae: 0.632351, mean_q: 5.127975
 84565/100000: episode: 1604, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 195.770, mean reward: 1.958 [1.447, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.724, 10.098], loss: 2.144279, mae: 0.604234, mean_q: 5.059011
 84665/100000: episode: 1605, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 216.594, mean reward: 2.166 [1.472, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.299], loss: 3.798399, mae: 0.679920, mean_q: 5.133673
 84765/100000: episode: 1606, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.097, mean reward: 1.921 [1.491, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.327, 10.251], loss: 1.695520, mae: 0.592543, mean_q: 5.065764
 84865/100000: episode: 1607, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.588, mean reward: 1.966 [1.460, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.194], loss: 2.138931, mae: 0.599563, mean_q: 5.021135
 84965/100000: episode: 1608, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.970, mean reward: 1.850 [1.469, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.899, 10.098], loss: 3.028687, mae: 0.614398, mean_q: 5.013196
 85065/100000: episode: 1609, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.447, mean reward: 1.854 [1.438, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.113, 10.172], loss: 4.060761, mae: 0.641962, mean_q: 4.944381
 85165/100000: episode: 1610, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 216.029, mean reward: 2.160 [1.452, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.310, 10.098], loss: 4.641636, mae: 0.773719, mean_q: 5.017536
 85265/100000: episode: 1611, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.334, mean reward: 1.943 [1.490, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.560, 10.208], loss: 5.695570, mae: 0.676202, mean_q: 4.940561
 85365/100000: episode: 1612, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.705, mean reward: 1.897 [1.455, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.681, 10.098], loss: 6.211051, mae: 0.746386, mean_q: 5.028173
 85465/100000: episode: 1613, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 204.457, mean reward: 2.045 [1.457, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.955, 10.098], loss: 1.159507, mae: 0.582802, mean_q: 4.921895
 85565/100000: episode: 1614, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 189.630, mean reward: 1.896 [1.437, 2.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.244], loss: 3.903140, mae: 0.640052, mean_q: 4.894087
 85665/100000: episode: 1615, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.323, mean reward: 1.853 [1.446, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.667, 10.157], loss: 3.851634, mae: 0.644031, mean_q: 4.861620
 85765/100000: episode: 1616, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 184.494, mean reward: 1.845 [1.447, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.326], loss: 4.719679, mae: 0.650459, mean_q: 4.802649
 85865/100000: episode: 1617, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.292, mean reward: 1.823 [1.458, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.431, 10.175], loss: 8.593300, mae: 0.865181, mean_q: 4.904117
 85965/100000: episode: 1618, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.038, mean reward: 1.950 [1.450, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.721, 10.287], loss: 5.352177, mae: 0.718015, mean_q: 4.903641
 86065/100000: episode: 1619, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.422, mean reward: 1.874 [1.443, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.401, 10.232], loss: 3.030541, mae: 0.606238, mean_q: 4.750835
 86165/100000: episode: 1620, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.809, mean reward: 1.798 [1.478, 2.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.455, 10.098], loss: 0.825857, mae: 0.498848, mean_q: 4.701566
 86265/100000: episode: 1621, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 187.628, mean reward: 1.876 [1.441, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.098], loss: 5.971332, mae: 0.663566, mean_q: 4.744194
 86365/100000: episode: 1622, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 215.183, mean reward: 2.152 [1.494, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.079, 10.098], loss: 3.158980, mae: 0.583814, mean_q: 4.703945
 86465/100000: episode: 1623, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 215.199, mean reward: 2.152 [1.523, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.359, 10.213], loss: 4.600639, mae: 0.627878, mean_q: 4.663496
 86565/100000: episode: 1624, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 199.196, mean reward: 1.992 [1.465, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.967, 10.098], loss: 2.707866, mae: 0.594517, mean_q: 4.605187
 86665/100000: episode: 1625, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 209.265, mean reward: 2.093 [1.474, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.150, 10.098], loss: 2.138905, mae: 0.527393, mean_q: 4.486395
 86765/100000: episode: 1626, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.075, mean reward: 1.901 [1.458, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.039, 10.137], loss: 0.915762, mae: 0.455103, mean_q: 4.396227
 86865/100000: episode: 1627, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.940, mean reward: 1.849 [1.455, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.643, 10.098], loss: 0.192947, mae: 0.365035, mean_q: 4.259619
 86965/100000: episode: 1628, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 190.998, mean reward: 1.910 [1.474, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.753, 10.109], loss: 1.408273, mae: 0.431727, mean_q: 4.252664
 87065/100000: episode: 1629, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 281.979, mean reward: 2.820 [1.477, 10.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.573, 10.098], loss: 3.583109, mae: 0.525767, mean_q: 4.219897
 87165/100000: episode: 1630, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 207.269, mean reward: 2.073 [1.479, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.673, 10.308], loss: 1.389218, mae: 0.452052, mean_q: 4.264333
 87265/100000: episode: 1631, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 190.705, mean reward: 1.907 [1.481, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.203, 10.098], loss: 1.338373, mae: 0.426715, mean_q: 4.189466
 87365/100000: episode: 1632, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 209.802, mean reward: 2.098 [1.472, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.399, 10.098], loss: 0.134576, mae: 0.334754, mean_q: 4.075672
 87465/100000: episode: 1633, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 186.908, mean reward: 1.869 [1.471, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.358, 10.098], loss: 0.120388, mae: 0.323977, mean_q: 4.022485
 87565/100000: episode: 1634, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.608, mean reward: 1.936 [1.449, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.323, 10.165], loss: 0.116392, mae: 0.322833, mean_q: 3.951690
 87665/100000: episode: 1635, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.227, mean reward: 1.832 [1.462, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.726, 10.098], loss: 1.277065, mae: 0.360183, mean_q: 3.900065
 87765/100000: episode: 1636, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.186, mean reward: 1.932 [1.469, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.358, 10.238], loss: 0.102351, mae: 0.300181, mean_q: 3.873566
 87865/100000: episode: 1637, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 179.387, mean reward: 1.794 [1.480, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.098], loss: 0.087720, mae: 0.288950, mean_q: 3.890286
 87965/100000: episode: 1638, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.772, mean reward: 1.848 [1.496, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.164], loss: 0.102439, mae: 0.302517, mean_q: 3.884924
 88065/100000: episode: 1639, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.230, mean reward: 2.002 [1.473, 6.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.896, 10.098], loss: 0.100553, mae: 0.307334, mean_q: 3.885677
 88165/100000: episode: 1640, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 176.825, mean reward: 1.768 [1.452, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.165, 10.110], loss: 0.094317, mae: 0.291757, mean_q: 3.874632
 88265/100000: episode: 1641, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.806, mean reward: 1.858 [1.450, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.749, 10.294], loss: 0.094746, mae: 0.299190, mean_q: 3.864536
 88365/100000: episode: 1642, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 181.816, mean reward: 1.818 [1.454, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.208, 10.230], loss: 0.087428, mae: 0.291566, mean_q: 3.861795
 88465/100000: episode: 1643, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 183.042, mean reward: 1.830 [1.454, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.124], loss: 0.097327, mae: 0.296270, mean_q: 3.871532
 88565/100000: episode: 1644, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.250, mean reward: 1.872 [1.449, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.098], loss: 0.085350, mae: 0.284682, mean_q: 3.850123
 88665/100000: episode: 1645, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 178.422, mean reward: 1.784 [1.453, 2.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.098], loss: 0.083676, mae: 0.272568, mean_q: 3.860818
 88765/100000: episode: 1646, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.935, mean reward: 1.879 [1.455, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.260, 10.098], loss: 0.093337, mae: 0.287160, mean_q: 3.847351
 88865/100000: episode: 1647, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.570, mean reward: 1.906 [1.439, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.098], loss: 0.088837, mae: 0.280991, mean_q: 3.852169
 88965/100000: episode: 1648, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.965, mean reward: 1.990 [1.473, 5.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.076711, mae: 0.270522, mean_q: 3.830973
 89065/100000: episode: 1649, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.867, mean reward: 1.879 [1.464, 5.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.167, 10.326], loss: 0.089397, mae: 0.278440, mean_q: 3.819919
 89165/100000: episode: 1650, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.668, mean reward: 1.987 [1.498, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.382], loss: 0.090310, mae: 0.280002, mean_q: 3.825679
 89265/100000: episode: 1651, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.339, mean reward: 1.893 [1.477, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.357, 10.293], loss: 0.083444, mae: 0.277231, mean_q: 3.821874
 89365/100000: episode: 1652, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 179.566, mean reward: 1.796 [1.446, 2.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.098], loss: 0.098080, mae: 0.287152, mean_q: 3.836555
 89465/100000: episode: 1653, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.805, mean reward: 1.908 [1.506, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.067, 10.098], loss: 0.093208, mae: 0.288443, mean_q: 3.857430
 89565/100000: episode: 1654, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.241, mean reward: 1.812 [1.457, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.251, 10.150], loss: 0.088854, mae: 0.278629, mean_q: 3.832442
 89665/100000: episode: 1655, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.192, mean reward: 1.842 [1.453, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.237, 10.098], loss: 0.083761, mae: 0.280569, mean_q: 3.842517
 89765/100000: episode: 1656, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.272, mean reward: 1.843 [1.457, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.624, 10.203], loss: 0.091013, mae: 0.277743, mean_q: 3.820939
 89865/100000: episode: 1657, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 214.668, mean reward: 2.147 [1.453, 13.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.105, 10.180], loss: 0.085928, mae: 0.284319, mean_q: 3.824121
 89965/100000: episode: 1658, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.377, mean reward: 1.934 [1.486, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.098], loss: 0.108872, mae: 0.278342, mean_q: 3.797481
 90065/100000: episode: 1659, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 212.433, mean reward: 2.124 [1.634, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.363, 10.098], loss: 0.180841, mae: 0.305829, mean_q: 3.853435
 90165/100000: episode: 1660, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.894, mean reward: 1.859 [1.432, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.206, 10.137], loss: 0.089430, mae: 0.276446, mean_q: 3.837391
 90265/100000: episode: 1661, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 214.410, mean reward: 2.144 [1.453, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.098], loss: 0.080522, mae: 0.270958, mean_q: 3.808791
 90365/100000: episode: 1662, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 202.135, mean reward: 2.021 [1.494, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.651, 10.098], loss: 0.139419, mae: 0.292663, mean_q: 3.831008
 90465/100000: episode: 1663, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 184.059, mean reward: 1.841 [1.439, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.110], loss: 0.133581, mae: 0.296215, mean_q: 3.863758
 90565/100000: episode: 1664, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 180.576, mean reward: 1.806 [1.458, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.952, 10.294], loss: 0.099753, mae: 0.286648, mean_q: 3.833294
 90665/100000: episode: 1665, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 211.884, mean reward: 2.119 [1.441, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.582, 10.098], loss: 0.086071, mae: 0.272647, mean_q: 3.810517
 90765/100000: episode: 1666, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.253, mean reward: 1.873 [1.441, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.127], loss: 0.112114, mae: 0.289461, mean_q: 3.870412
 90865/100000: episode: 1667, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 210.086, mean reward: 2.101 [1.509, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.231], loss: 0.089195, mae: 0.286542, mean_q: 3.819459
 90965/100000: episode: 1668, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 204.617, mean reward: 2.046 [1.486, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.766, 10.482], loss: 0.114255, mae: 0.293158, mean_q: 3.877592
 91065/100000: episode: 1669, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.928, mean reward: 1.869 [1.513, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.090, 10.098], loss: 0.115505, mae: 0.294100, mean_q: 3.857870
 91165/100000: episode: 1670, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.346, mean reward: 1.833 [1.455, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.440, 10.098], loss: 0.103946, mae: 0.299556, mean_q: 3.849336
 91265/100000: episode: 1671, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 208.228, mean reward: 2.082 [1.460, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.470, 10.298], loss: 0.109665, mae: 0.302171, mean_q: 3.861215
 91365/100000: episode: 1672, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 186.343, mean reward: 1.863 [1.468, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.688, 10.176], loss: 0.113552, mae: 0.294366, mean_q: 3.865407
 91465/100000: episode: 1673, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.932, mean reward: 1.849 [1.442, 5.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.390], loss: 0.081505, mae: 0.274480, mean_q: 3.849810
 91565/100000: episode: 1674, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.325, mean reward: 1.873 [1.466, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.389, 10.098], loss: 0.111037, mae: 0.294463, mean_q: 3.818883
 91665/100000: episode: 1675, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 182.731, mean reward: 1.827 [1.473, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.725, 10.098], loss: 0.106567, mae: 0.300241, mean_q: 3.819869
 91765/100000: episode: 1676, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.966, mean reward: 2.040 [1.540, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.890, 10.098], loss: 0.094962, mae: 0.290108, mean_q: 3.821591
 91865/100000: episode: 1677, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 204.368, mean reward: 2.044 [1.490, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.327, 10.214], loss: 0.080040, mae: 0.269180, mean_q: 3.791620
 91965/100000: episode: 1678, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 199.165, mean reward: 1.992 [1.494, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.149], loss: 0.117808, mae: 0.293646, mean_q: 3.840108
 92065/100000: episode: 1679, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 201.845, mean reward: 2.018 [1.454, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.035, 10.098], loss: 0.143942, mae: 0.298811, mean_q: 3.823014
 92165/100000: episode: 1680, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 212.438, mean reward: 2.124 [1.553, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.704, 10.308], loss: 0.092189, mae: 0.288069, mean_q: 3.809125
 92265/100000: episode: 1681, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.544, mean reward: 1.885 [1.472, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.572, 10.214], loss: 0.112129, mae: 0.285859, mean_q: 3.796288
 92365/100000: episode: 1682, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 211.395, mean reward: 2.114 [1.461, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.762, 10.098], loss: 0.081262, mae: 0.274431, mean_q: 3.793859
 92465/100000: episode: 1683, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.373, mean reward: 1.914 [1.464, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.083, 10.098], loss: 0.114416, mae: 0.293979, mean_q: 3.828714
 92565/100000: episode: 1684, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.301, mean reward: 1.953 [1.500, 10.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.184, 10.242], loss: 0.088337, mae: 0.289140, mean_q: 3.812125
[Info] 1-TH LEVEL FOUND: 4.945985794067383, Considering 10/90 traces
 92665/100000: episode: 1685, duration: 4.668s, episode steps: 100, steps per second: 21, episode reward: 203.154, mean reward: 2.032 [1.477, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.892, 10.098], loss: 0.138984, mae: 0.290757, mean_q: 3.814237
 92760/100000: episode: 1686, duration: 0.483s, episode steps: 95, steps per second: 197, episode reward: 177.793, mean reward: 1.872 [1.453, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.650, 10.100], loss: 0.103723, mae: 0.287164, mean_q: 3.818972
 92781/100000: episode: 1687, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 96.380, mean reward: 4.590 [3.189, 6.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.100, 10.100], loss: 0.146509, mae: 0.300677, mean_q: 3.823597
 92876/100000: episode: 1688, duration: 0.478s, episode steps: 95, steps per second: 199, episode reward: 210.989, mean reward: 2.221 [1.468, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.751, 10.100], loss: 0.106845, mae: 0.303606, mean_q: 3.851374
 92904/100000: episode: 1689, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 84.936, mean reward: 3.033 [2.140, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.438, 10.100], loss: 0.092177, mae: 0.286190, mean_q: 3.833851
 92999/100000: episode: 1690, duration: 0.513s, episode steps: 95, steps per second: 185, episode reward: 182.319, mean reward: 1.919 [1.434, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.293, 10.100], loss: 0.149844, mae: 0.306736, mean_q: 3.875319
 93045/100000: episode: 1691, duration: 0.226s, episode steps: 46, steps per second: 204, episode reward: 96.402, mean reward: 2.096 [1.608, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.243, 10.163], loss: 0.108119, mae: 0.310440, mean_q: 3.875126
 93073/100000: episode: 1692, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 65.568, mean reward: 2.342 [1.799, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.120, 10.100], loss: 0.088719, mae: 0.275580, mean_q: 3.818030
 93092/100000: episode: 1693, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 54.123, mean reward: 2.849 [2.370, 5.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.305, 10.100], loss: 0.123627, mae: 0.331652, mean_q: 3.935556
 93111/100000: episode: 1694, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 64.673, mean reward: 3.404 [2.446, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.928, 10.100], loss: 0.098822, mae: 0.305711, mean_q: 3.868642
 93129/100000: episode: 1695, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 46.729, mean reward: 2.596 [2.123, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.269, 10.100], loss: 0.270462, mae: 0.380166, mean_q: 3.974375
 93221/100000: episode: 1696, duration: 0.492s, episode steps: 92, steps per second: 187, episode reward: 171.322, mean reward: 1.862 [1.476, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-1.219, 10.100], loss: 0.141153, mae: 0.333085, mean_q: 3.908752
 93247/100000: episode: 1697, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 70.227, mean reward: 2.701 [2.195, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.577, 10.392], loss: 0.191930, mae: 0.327610, mean_q: 3.902727
 93266/100000: episode: 1698, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 59.870, mean reward: 3.151 [2.351, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.153, 10.100], loss: 0.103807, mae: 0.301380, mean_q: 3.913322
 93312/100000: episode: 1699, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 97.469, mean reward: 2.119 [1.501, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.252, 10.100], loss: 0.094685, mae: 0.295417, mean_q: 3.904405
 93409/100000: episode: 1700, duration: 0.513s, episode steps: 97, steps per second: 189, episode reward: 183.531, mean reward: 1.892 [1.474, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.338, 10.125], loss: 0.144817, mae: 0.317312, mean_q: 3.927884
 93440/100000: episode: 1701, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 89.954, mean reward: 2.902 [2.002, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.980, 10.389], loss: 0.147113, mae: 0.329452, mean_q: 3.956157
 93468/100000: episode: 1702, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 105.105, mean reward: 3.754 [2.522, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.146, 10.100], loss: 0.103963, mae: 0.316266, mean_q: 3.995470
 93496/100000: episode: 1703, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 96.568, mean reward: 3.449 [2.397, 7.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.013, 10.100], loss: 0.192162, mae: 0.340604, mean_q: 3.975698
 93591/100000: episode: 1704, duration: 0.505s, episode steps: 95, steps per second: 188, episode reward: 188.939, mean reward: 1.989 [1.481, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-1.317, 10.155], loss: 0.191203, mae: 0.340083, mean_q: 3.990538
 93619/100000: episode: 1705, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 75.525, mean reward: 2.697 [1.613, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.238, 10.100], loss: 0.108689, mae: 0.315851, mean_q: 3.940131
 93665/100000: episode: 1706, duration: 0.249s, episode steps: 46, steps per second: 184, episode reward: 135.414, mean reward: 2.944 [2.071, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.541, 10.324], loss: 0.139219, mae: 0.331463, mean_q: 4.029088
 93684/100000: episode: 1707, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 47.366, mean reward: 2.493 [1.926, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.499, 10.100], loss: 0.104635, mae: 0.316966, mean_q: 4.000494
 93702/100000: episode: 1708, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 48.215, mean reward: 2.679 [2.125, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.430, 10.100], loss: 0.148751, mae: 0.357777, mean_q: 4.061827
 93799/100000: episode: 1709, duration: 0.529s, episode steps: 97, steps per second: 183, episode reward: 190.161, mean reward: 1.960 [1.480, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.057, 10.100], loss: 0.161490, mae: 0.330899, mean_q: 4.006524
 93896/100000: episode: 1710, duration: 0.504s, episode steps: 97, steps per second: 192, episode reward: 180.297, mean reward: 1.859 [1.442, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-1.286, 10.110], loss: 0.152621, mae: 0.332199, mean_q: 4.033908
 93927/100000: episode: 1711, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 113.265, mean reward: 3.654 [2.830, 7.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.331, 10.476], loss: 0.093563, mae: 0.299603, mean_q: 4.005218
 94019/100000: episode: 1712, duration: 0.486s, episode steps: 92, steps per second: 189, episode reward: 181.102, mean reward: 1.968 [1.479, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.767, 10.252], loss: 0.115885, mae: 0.321193, mean_q: 4.004484
 94038/100000: episode: 1713, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 67.059, mean reward: 3.529 [2.418, 5.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.303, 10.100], loss: 0.121227, mae: 0.327692, mean_q: 3.999714
 94084/100000: episode: 1714, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 98.594, mean reward: 2.143 [1.510, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.458, 10.100], loss: 0.110571, mae: 0.326749, mean_q: 4.057387
 94102/100000: episode: 1715, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 42.006, mean reward: 2.334 [1.821, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.763, 10.100], loss: 0.093451, mae: 0.309058, mean_q: 4.005974
 94133/100000: episode: 1716, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 72.056, mean reward: 2.324 [1.580, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.319, 10.200], loss: 0.183312, mae: 0.331055, mean_q: 4.038279
 94159/100000: episode: 1717, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 61.775, mean reward: 2.376 [1.695, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.395, 10.216], loss: 0.109385, mae: 0.325368, mean_q: 4.087214
 94185/100000: episode: 1718, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 78.460, mean reward: 3.018 [2.128, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.772, 10.545], loss: 0.166101, mae: 0.353128, mean_q: 4.063868
 94203/100000: episode: 1719, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 54.059, mean reward: 3.003 [2.356, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.291, 10.100], loss: 0.141256, mae: 0.361195, mean_q: 4.139872
 94222/100000: episode: 1720, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 48.308, mean reward: 2.543 [2.172, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.191, 10.100], loss: 0.119679, mae: 0.316214, mean_q: 4.054416
 94314/100000: episode: 1721, duration: 0.471s, episode steps: 92, steps per second: 195, episode reward: 170.381, mean reward: 1.852 [1.453, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-0.308, 10.173], loss: 0.153715, mae: 0.349618, mean_q: 4.058914
 94340/100000: episode: 1722, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 60.487, mean reward: 2.326 [1.858, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.424], loss: 0.221154, mae: 0.371676, mean_q: 4.094110
 94435/100000: episode: 1723, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 178.341, mean reward: 1.877 [1.486, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.001, 10.160], loss: 0.142608, mae: 0.351240, mean_q: 4.084051
 94466/100000: episode: 1724, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 72.688, mean reward: 2.345 [1.984, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.139, 10.367], loss: 0.164564, mae: 0.353037, mean_q: 4.108470
 94485/100000: episode: 1725, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 46.506, mean reward: 2.448 [2.130, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.202, 10.100], loss: 0.128704, mae: 0.341806, mean_q: 4.135104
 94582/100000: episode: 1726, duration: 0.499s, episode steps: 97, steps per second: 195, episode reward: 186.629, mean reward: 1.924 [1.434, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-1.260, 10.100], loss: 0.121721, mae: 0.333122, mean_q: 4.073529
 94608/100000: episode: 1727, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 94.706, mean reward: 3.643 [2.320, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.869, 10.483], loss: 0.109603, mae: 0.321815, mean_q: 4.110079
 94700/100000: episode: 1728, duration: 0.449s, episode steps: 92, steps per second: 205, episode reward: 178.202, mean reward: 1.937 [1.531, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-0.442, 10.100], loss: 0.114393, mae: 0.326028, mean_q: 4.104000
 94721/100000: episode: 1729, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 61.475, mean reward: 2.927 [2.357, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.034, 10.100], loss: 0.238552, mae: 0.373105, mean_q: 4.107830
 94742/100000: episode: 1730, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 57.828, mean reward: 2.754 [2.098, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.257, 10.100], loss: 0.102364, mae: 0.316759, mean_q: 4.144451
 94839/100000: episode: 1731, duration: 0.484s, episode steps: 97, steps per second: 200, episode reward: 212.856, mean reward: 2.194 [1.571, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-1.867, 10.100], loss: 0.131199, mae: 0.348443, mean_q: 4.124043
 94865/100000: episode: 1732, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 56.364, mean reward: 2.168 [1.474, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.469, 10.240], loss: 0.165453, mae: 0.337548, mean_q: 4.059020
 94884/100000: episode: 1733, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 63.983, mean reward: 3.368 [2.701, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.790, 10.100], loss: 0.114018, mae: 0.347958, mean_q: 4.120582
 94905/100000: episode: 1734, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 54.268, mean reward: 2.584 [1.954, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.223, 10.100], loss: 0.124776, mae: 0.342736, mean_q: 4.098924
 94926/100000: episode: 1735, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 49.299, mean reward: 2.348 [2.012, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.145, 10.100], loss: 0.119485, mae: 0.330415, mean_q: 4.129856
 94945/100000: episode: 1736, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 46.222, mean reward: 2.433 [1.964, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.229, 10.100], loss: 0.160837, mae: 0.362284, mean_q: 4.172813
 94963/100000: episode: 1737, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 46.213, mean reward: 2.567 [1.813, 6.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.144, 10.100], loss: 0.113953, mae: 0.317137, mean_q: 4.118921
 95058/100000: episode: 1738, duration: 0.495s, episode steps: 95, steps per second: 192, episode reward: 180.798, mean reward: 1.903 [1.436, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.709, 10.100], loss: 0.118803, mae: 0.338877, mean_q: 4.141148
 95084/100000: episode: 1739, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 59.337, mean reward: 2.282 [1.766, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.484, 10.312], loss: 0.167300, mae: 0.343645, mean_q: 4.120401
 95103/100000: episode: 1740, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 105.609, mean reward: 5.558 [2.408, 7.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.395, 10.100], loss: 0.129627, mae: 0.338633, mean_q: 4.166500
 95149/100000: episode: 1741, duration: 0.229s, episode steps: 46, steps per second: 201, episode reward: 94.047, mean reward: 2.045 [1.473, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.239, 10.100], loss: 0.134246, mae: 0.348628, mean_q: 4.119745
 95180/100000: episode: 1742, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 72.219, mean reward: 2.330 [1.495, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.502, 10.100], loss: 0.141621, mae: 0.351092, mean_q: 4.101254
 95211/100000: episode: 1743, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 94.493, mean reward: 3.048 [2.034, 7.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.908, 10.377], loss: 0.206947, mae: 0.398925, mean_q: 4.127472
 95237/100000: episode: 1744, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 61.720, mean reward: 2.374 [1.762, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.747, 10.308], loss: 0.137989, mae: 0.351450, mean_q: 4.163602
 95265/100000: episode: 1745, duration: 0.169s, episode steps: 28, steps per second: 166, episode reward: 74.658, mean reward: 2.666 [1.930, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.412, 10.100], loss: 0.128469, mae: 0.343501, mean_q: 4.164211
 95311/100000: episode: 1746, duration: 0.226s, episode steps: 46, steps per second: 203, episode reward: 99.367, mean reward: 2.160 [1.561, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.252, 10.276], loss: 0.191992, mae: 0.377963, mean_q: 4.218809
 95332/100000: episode: 1747, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 61.389, mean reward: 2.923 [2.335, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.328, 10.100], loss: 0.264354, mae: 0.387501, mean_q: 4.265384
 95378/100000: episode: 1748, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 103.052, mean reward: 2.240 [1.737, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.244, 10.391], loss: 0.233934, mae: 0.415027, mean_q: 4.293237
 95406/100000: episode: 1749, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 69.227, mean reward: 2.472 [1.804, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.131, 10.100], loss: 0.142190, mae: 0.355245, mean_q: 4.208703
 95437/100000: episode: 1750, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 81.730, mean reward: 2.636 [1.907, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.722, 10.489], loss: 0.139546, mae: 0.361216, mean_q: 4.194408
 95483/100000: episode: 1751, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 97.058, mean reward: 2.110 [1.565, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.266, 10.102], loss: 0.132607, mae: 0.348493, mean_q: 4.254228
 95502/100000: episode: 1752, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 49.121, mean reward: 2.585 [1.734, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.116, 10.100], loss: 0.110002, mae: 0.329426, mean_q: 4.204528
 95533/100000: episode: 1753, duration: 0.149s, episode steps: 31, steps per second: 207, episode reward: 75.484, mean reward: 2.435 [1.778, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.117, 10.374], loss: 0.141750, mae: 0.352986, mean_q: 4.270248
 95561/100000: episode: 1754, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 115.699, mean reward: 4.132 [3.017, 5.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.399, 10.100], loss: 0.178694, mae: 0.365484, mean_q: 4.272818
 95587/100000: episode: 1755, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 65.735, mean reward: 2.528 [1.887, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.341], loss: 0.133820, mae: 0.367916, mean_q: 4.269273
 95605/100000: episode: 1756, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 45.188, mean reward: 2.510 [2.052, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.566, 10.100], loss: 0.196306, mae: 0.369584, mean_q: 4.280365
 95697/100000: episode: 1757, duration: 0.472s, episode steps: 92, steps per second: 195, episode reward: 203.472, mean reward: 2.212 [1.555, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.425, 10.100], loss: 0.146142, mae: 0.362741, mean_q: 4.276820
 95725/100000: episode: 1758, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 71.266, mean reward: 2.545 [2.037, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.521, 10.100], loss: 0.137562, mae: 0.356507, mean_q: 4.239415
 95751/100000: episode: 1759, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 61.333, mean reward: 2.359 [1.959, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.170, 10.331], loss: 0.148460, mae: 0.370508, mean_q: 4.304431
 95782/100000: episode: 1760, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 99.813, mean reward: 3.220 [2.055, 5.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.472, 10.541], loss: 0.138863, mae: 0.352905, mean_q: 4.289493
 95828/100000: episode: 1761, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 95.555, mean reward: 2.077 [1.519, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.233, 10.136], loss: 0.125100, mae: 0.351799, mean_q: 4.285171
 95846/100000: episode: 1762, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 44.699, mean reward: 2.483 [1.919, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.580, 10.100], loss: 0.131960, mae: 0.353485, mean_q: 4.210190
 95892/100000: episode: 1763, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 82.840, mean reward: 1.801 [1.450, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.668, 10.232], loss: 0.145873, mae: 0.369231, mean_q: 4.316813
 95910/100000: episode: 1764, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 42.433, mean reward: 2.357 [1.797, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.485, 10.100], loss: 0.119646, mae: 0.344287, mean_q: 4.253130
 96007/100000: episode: 1765, duration: 0.491s, episode steps: 97, steps per second: 198, episode reward: 183.373, mean reward: 1.890 [1.466, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-0.350, 10.100], loss: 0.170876, mae: 0.383379, mean_q: 4.293524
 96025/100000: episode: 1766, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 51.877, mean reward: 2.882 [2.105, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.283, 10.100], loss: 0.106413, mae: 0.331465, mean_q: 4.372798
 96043/100000: episode: 1767, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 52.554, mean reward: 2.920 [2.211, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.584, 10.100], loss: 0.141723, mae: 0.366834, mean_q: 4.388412
 96138/100000: episode: 1768, duration: 0.493s, episode steps: 95, steps per second: 193, episode reward: 181.214, mean reward: 1.908 [1.461, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.238, 10.208], loss: 0.143301, mae: 0.364264, mean_q: 4.334455
 96235/100000: episode: 1769, duration: 0.482s, episode steps: 97, steps per second: 201, episode reward: 186.883, mean reward: 1.927 [1.464, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.829, 10.100], loss: 0.127161, mae: 0.348795, mean_q: 4.277594
 96330/100000: episode: 1770, duration: 0.516s, episode steps: 95, steps per second: 184, episode reward: 185.690, mean reward: 1.955 [1.471, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.884, 10.100], loss: 0.142323, mae: 0.353115, mean_q: 4.315218
 96358/100000: episode: 1771, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 76.677, mean reward: 2.738 [2.230, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.120, 10.100], loss: 0.108806, mae: 0.333269, mean_q: 4.306570
 96376/100000: episode: 1772, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 59.237, mean reward: 3.291 [2.440, 5.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.385, 10.100], loss: 0.152679, mae: 0.376075, mean_q: 4.350824
 96404/100000: episode: 1773, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 83.781, mean reward: 2.992 [2.062, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.829, 10.100], loss: 0.141394, mae: 0.359582, mean_q: 4.248334
 96499/100000: episode: 1774, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 178.820, mean reward: 1.882 [1.468, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.028, 10.285], loss: 0.150278, mae: 0.371153, mean_q: 4.351071
[Info] 2-TH LEVEL FOUND: 7.675559043884277, Considering 10/90 traces
 96520/100000: episode: 1775, duration: 4.234s, episode steps: 21, steps per second: 5, episode reward: 49.853, mean reward: 2.374 [1.944, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.252, 10.100], loss: 0.107049, mae: 0.331069, mean_q: 4.356550
 96531/100000: episode: 1776, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 34.147, mean reward: 3.104 [2.439, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.288, 10.100], loss: 0.088870, mae: 0.316595, mean_q: 4.295179
 96542/100000: episode: 1777, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 34.940, mean reward: 3.176 [2.475, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.513, 10.100], loss: 0.114491, mae: 0.342283, mean_q: 4.408290
 96555/100000: episode: 1778, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 47.609, mean reward: 3.662 [2.982, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.339, 10.100], loss: 0.148215, mae: 0.351744, mean_q: 4.397899
 96573/100000: episode: 1779, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 64.445, mean reward: 3.580 [2.790, 8.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.448, 10.100], loss: 0.210213, mae: 0.427563, mean_q: 4.480093
 96583/100000: episode: 1780, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 30.468, mean reward: 3.047 [2.236, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.553, 10.400], loss: 0.116127, mae: 0.350389, mean_q: 4.407269
 96601/100000: episode: 1781, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 80.372, mean reward: 4.465 [3.082, 6.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.472, 10.100], loss: 0.116745, mae: 0.331381, mean_q: 4.309800
 96612/100000: episode: 1782, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 37.168, mean reward: 3.379 [2.669, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.485, 10.100], loss: 0.107651, mae: 0.335001, mean_q: 4.438496
 96623/100000: episode: 1783, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 56.840, mean reward: 5.167 [3.203, 8.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.356, 10.100], loss: 0.157345, mae: 0.380438, mean_q: 4.455899
 96643/100000: episode: 1784, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 77.579, mean reward: 3.879 [2.989, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.817, 10.100], loss: 0.134709, mae: 0.361283, mean_q: 4.427952
 96655/100000: episode: 1785, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 45.044, mean reward: 3.754 [2.852, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.429, 10.100], loss: 0.167912, mae: 0.391083, mean_q: 4.424871
 96667/100000: episode: 1786, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 35.776, mean reward: 2.981 [2.564, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.414, 10.100], loss: 0.166765, mae: 0.421577, mean_q: 4.510077
 96678/100000: episode: 1787, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 38.571, mean reward: 3.506 [2.875, 4.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.322, 10.100], loss: 0.119318, mae: 0.316182, mean_q: 4.474552
[Info] FALSIFICATION!
[Info] Levels: [4.945986, 7.675559, 11.709857]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 96692/100000: episode: 1788, duration: 4.510s, episode steps: 14, steps per second: 3, episode reward: 220.784, mean reward: 15.770 [4.821, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.275, 10.020], loss: 0.139196, mae: 0.372240, mean_q: 4.435371
 96792/100000: episode: 1789, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 224.597, mean reward: 2.246 [1.525, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.245, 10.098], loss: 2.850532, mae: 0.615970, mean_q: 4.583510
 96892/100000: episode: 1790, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 188.421, mean reward: 1.884 [1.466, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.886, 10.232], loss: 0.238888, mae: 0.412004, mean_q: 4.527584
 96992/100000: episode: 1791, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.097, mean reward: 1.911 [1.454, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.763, 10.098], loss: 1.483264, mae: 0.501539, mean_q: 4.539099
 97092/100000: episode: 1792, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.059, mean reward: 1.991 [1.474, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.699, 10.156], loss: 1.542781, mae: 0.509845, mean_q: 4.544858
 97192/100000: episode: 1793, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 213.008, mean reward: 2.130 [1.466, 6.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.110, 10.098], loss: 0.154477, mae: 0.372595, mean_q: 4.512381
 97292/100000: episode: 1794, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 186.433, mean reward: 1.864 [1.463, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.401, 10.098], loss: 0.180617, mae: 0.375267, mean_q: 4.519046
 97392/100000: episode: 1795, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.680, mean reward: 1.957 [1.457, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.432, 10.111], loss: 0.228547, mae: 0.406483, mean_q: 4.545883
 97492/100000: episode: 1796, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.289, mean reward: 1.903 [1.462, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.399, 10.098], loss: 1.489706, mae: 0.488615, mean_q: 4.535437
 97592/100000: episode: 1797, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.998, mean reward: 1.880 [1.448, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.098], loss: 2.552143, mae: 0.458549, mean_q: 4.556346
 97692/100000: episode: 1798, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.150, mean reward: 1.971 [1.448, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.582, 10.098], loss: 2.743244, mae: 0.629702, mean_q: 4.539881
 97792/100000: episode: 1799, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 222.833, mean reward: 2.228 [1.480, 5.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.755, 10.098], loss: 0.171050, mae: 0.367058, mean_q: 4.447491
 97892/100000: episode: 1800, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.594, mean reward: 1.886 [1.452, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.952, 10.123], loss: 0.164445, mae: 0.368761, mean_q: 4.449342
 97992/100000: episode: 1801, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.169, mean reward: 1.862 [1.447, 2.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.813, 10.118], loss: 0.156538, mae: 0.369134, mean_q: 4.413192
 98092/100000: episode: 1802, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.015, mean reward: 1.970 [1.476, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.801, 10.098], loss: 0.179493, mae: 0.386032, mean_q: 4.455278
 98192/100000: episode: 1803, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 179.306, mean reward: 1.793 [1.443, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.648, 10.357], loss: 0.140684, mae: 0.346196, mean_q: 4.390982
 98292/100000: episode: 1804, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 194.066, mean reward: 1.941 [1.460, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.977, 10.098], loss: 3.845381, mae: 0.567166, mean_q: 4.523390
 98392/100000: episode: 1805, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 198.928, mean reward: 1.989 [1.479, 6.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.848, 10.098], loss: 0.197561, mae: 0.380971, mean_q: 4.394259
 98492/100000: episode: 1806, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 182.164, mean reward: 1.822 [1.468, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.558, 10.165], loss: 1.435623, mae: 0.471933, mean_q: 4.384440
 98592/100000: episode: 1807, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.800, mean reward: 1.958 [1.489, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.233, 10.098], loss: 1.432566, mae: 0.454631, mean_q: 4.362693
 98692/100000: episode: 1808, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.944, mean reward: 1.879 [1.439, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.422, 10.098], loss: 3.868323, mae: 0.593378, mean_q: 4.435441
 98792/100000: episode: 1809, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.833, mean reward: 1.928 [1.480, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.287, 10.217], loss: 0.188236, mae: 0.381889, mean_q: 4.306846
 98892/100000: episode: 1810, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.458, mean reward: 1.905 [1.499, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.128, 10.348], loss: 0.156332, mae: 0.347088, mean_q: 4.305388
 98992/100000: episode: 1811, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.275, mean reward: 1.863 [1.479, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.253, 10.120], loss: 1.397031, mae: 0.421234, mean_q: 4.339136
 99092/100000: episode: 1812, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.041, mean reward: 2.000 [1.487, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.444, 10.144], loss: 1.430437, mae: 0.423635, mean_q: 4.318470
 99192/100000: episode: 1813, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.025, mean reward: 1.810 [1.452, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.288, 10.098], loss: 0.159275, mae: 0.350104, mean_q: 4.234791
 99292/100000: episode: 1814, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 198.188, mean reward: 1.982 [1.456, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.002, 10.389], loss: 0.138434, mae: 0.337476, mean_q: 4.184087
 99392/100000: episode: 1815, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 197.065, mean reward: 1.971 [1.467, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.613, 10.098], loss: 1.411959, mae: 0.425775, mean_q: 4.240562
 99492/100000: episode: 1816, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 183.338, mean reward: 1.833 [1.473, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.126], loss: 0.162453, mae: 0.344313, mean_q: 4.226243
 99592/100000: episode: 1817, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 213.757, mean reward: 2.138 [1.448, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.138, 10.098], loss: 0.137281, mae: 0.331302, mean_q: 4.178352
 99692/100000: episode: 1818, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 184.883, mean reward: 1.849 [1.466, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.689, 10.098], loss: 1.359150, mae: 0.397658, mean_q: 4.172637
 99792/100000: episode: 1819, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 198.074, mean reward: 1.981 [1.473, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.178, 10.267], loss: 1.452859, mae: 0.444888, mean_q: 4.240713
 99892/100000: episode: 1820, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.936, mean reward: 1.939 [1.439, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.787, 10.145], loss: 0.172492, mae: 0.354146, mean_q: 4.261439
 99992/100000: episode: 1821, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 197.557, mean reward: 1.976 [1.432, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.449, 10.098], loss: 2.690809, mae: 0.500912, mean_q: 4.233430
done, took 603.102 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
