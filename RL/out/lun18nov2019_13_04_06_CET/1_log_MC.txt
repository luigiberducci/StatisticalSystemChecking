Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 1000000 steps ...
    100/1000000: episode: 1, duration: 0.213s, episode steps: 100, steps per second: 470, episode reward: 58.685, mean reward: 0.587 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.779, 10.378], loss: --, mae: --, mean_q: --
    200/1000000: episode: 2, duration: 0.085s, episode steps: 100, steps per second: 1171, episode reward: 57.286, mean reward: 0.573 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.632, 10.244], loss: --, mae: --, mean_q: --
    300/1000000: episode: 3, duration: 0.098s, episode steps: 100, steps per second: 1026, episode reward: 59.180, mean reward: 0.592 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.433, 10.346], loss: --, mae: --, mean_q: --
    400/1000000: episode: 4, duration: 0.076s, episode steps: 100, steps per second: 1319, episode reward: 58.447, mean reward: 0.584 [0.506, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.898, 10.098], loss: --, mae: --, mean_q: --
    500/1000000: episode: 5, duration: 0.073s, episode steps: 100, steps per second: 1371, episode reward: 57.664, mean reward: 0.577 [0.497, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.118], loss: --, mae: --, mean_q: --
    600/1000000: episode: 6, duration: 0.089s, episode steps: 100, steps per second: 1118, episode reward: 59.344, mean reward: 0.593 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.059, 10.327], loss: --, mae: --, mean_q: --
    700/1000000: episode: 7, duration: 0.094s, episode steps: 100, steps per second: 1064, episode reward: 62.129, mean reward: 0.621 [0.514, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.908, 10.330], loss: --, mae: --, mean_q: --
    800/1000000: episode: 8, duration: 0.096s, episode steps: 100, steps per second: 1045, episode reward: 58.521, mean reward: 0.585 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.151, 10.098], loss: --, mae: --, mean_q: --
    900/1000000: episode: 9, duration: 0.074s, episode steps: 100, steps per second: 1360, episode reward: 58.817, mean reward: 0.588 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.585, 10.216], loss: --, mae: --, mean_q: --
   1000/1000000: episode: 10, duration: 0.085s, episode steps: 100, steps per second: 1182, episode reward: 58.628, mean reward: 0.586 [0.509, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.680, 10.098], loss: --, mae: --, mean_q: --
   1100/1000000: episode: 11, duration: 0.085s, episode steps: 100, steps per second: 1171, episode reward: 59.873, mean reward: 0.599 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.504, 10.387], loss: --, mae: --, mean_q: --
   1200/1000000: episode: 12, duration: 0.070s, episode steps: 100, steps per second: 1433, episode reward: 57.441, mean reward: 0.574 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.533, 10.098], loss: --, mae: --, mean_q: --
   1300/1000000: episode: 13, duration: 0.087s, episode steps: 100, steps per second: 1147, episode reward: 59.066, mean reward: 0.591 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.573, 10.098], loss: --, mae: --, mean_q: --
   1400/1000000: episode: 14, duration: 0.084s, episode steps: 100, steps per second: 1184, episode reward: 57.624, mean reward: 0.576 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.462, 10.098], loss: --, mae: --, mean_q: --
   1500/1000000: episode: 15, duration: 0.084s, episode steps: 100, steps per second: 1191, episode reward: 60.589, mean reward: 0.606 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.993, 10.360], loss: --, mae: --, mean_q: --
   1600/1000000: episode: 16, duration: 0.092s, episode steps: 100, steps per second: 1090, episode reward: 60.370, mean reward: 0.604 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.816, 10.264], loss: --, mae: --, mean_q: --
   1700/1000000: episode: 17, duration: 0.083s, episode steps: 100, steps per second: 1211, episode reward: 59.856, mean reward: 0.599 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.815, 10.098], loss: --, mae: --, mean_q: --
   1800/1000000: episode: 18, duration: 0.077s, episode steps: 100, steps per second: 1294, episode reward: 57.287, mean reward: 0.573 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.164, 10.098], loss: --, mae: --, mean_q: --
   1900/1000000: episode: 19, duration: 0.073s, episode steps: 100, steps per second: 1372, episode reward: 60.132, mean reward: 0.601 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.098], loss: --, mae: --, mean_q: --
   2000/1000000: episode: 20, duration: 0.096s, episode steps: 100, steps per second: 1040, episode reward: 58.821, mean reward: 0.588 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.833, 10.098], loss: --, mae: --, mean_q: --
   2100/1000000: episode: 21, duration: 0.105s, episode steps: 100, steps per second: 949, episode reward: 63.082, mean reward: 0.631 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.594, 10.410], loss: --, mae: --, mean_q: --
   2200/1000000: episode: 22, duration: 0.108s, episode steps: 100, steps per second: 924, episode reward: 61.460, mean reward: 0.615 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.605, 10.291], loss: --, mae: --, mean_q: --
   2300/1000000: episode: 23, duration: 0.107s, episode steps: 100, steps per second: 935, episode reward: 57.701, mean reward: 0.577 [0.498, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.351, 10.098], loss: --, mae: --, mean_q: --
   2400/1000000: episode: 24, duration: 0.106s, episode steps: 100, steps per second: 946, episode reward: 63.436, mean reward: 0.634 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.548, 10.238], loss: --, mae: --, mean_q: --
   2500/1000000: episode: 25, duration: 0.108s, episode steps: 100, steps per second: 924, episode reward: 58.777, mean reward: 0.588 [0.508, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.571, 10.098], loss: --, mae: --, mean_q: --
   2600/1000000: episode: 26, duration: 0.108s, episode steps: 100, steps per second: 924, episode reward: 60.103, mean reward: 0.601 [0.517, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.513, 10.098], loss: --, mae: --, mean_q: --
   2700/1000000: episode: 27, duration: 0.104s, episode steps: 100, steps per second: 957, episode reward: 58.530, mean reward: 0.585 [0.509, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.586, 10.162], loss: --, mae: --, mean_q: --
   2800/1000000: episode: 28, duration: 0.108s, episode steps: 100, steps per second: 923, episode reward: 58.417, mean reward: 0.584 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.171], loss: --, mae: --, mean_q: --
   2900/1000000: episode: 29, duration: 0.112s, episode steps: 100, steps per second: 890, episode reward: 58.347, mean reward: 0.583 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.522, 10.141], loss: --, mae: --, mean_q: --
   3000/1000000: episode: 30, duration: 0.112s, episode steps: 100, steps per second: 894, episode reward: 60.899, mean reward: 0.609 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.655, 10.098], loss: --, mae: --, mean_q: --
   3100/1000000: episode: 31, duration: 0.110s, episode steps: 100, steps per second: 905, episode reward: 57.923, mean reward: 0.579 [0.519, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.691, 10.307], loss: --, mae: --, mean_q: --
   3200/1000000: episode: 32, duration: 0.104s, episode steps: 100, steps per second: 959, episode reward: 57.704, mean reward: 0.577 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.708, 10.151], loss: --, mae: --, mean_q: --
   3300/1000000: episode: 33, duration: 0.096s, episode steps: 100, steps per second: 1037, episode reward: 58.880, mean reward: 0.589 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.949, 10.098], loss: --, mae: --, mean_q: --
   3400/1000000: episode: 34, duration: 0.108s, episode steps: 100, steps per second: 930, episode reward: 58.892, mean reward: 0.589 [0.513, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.217], loss: --, mae: --, mean_q: --
   3500/1000000: episode: 35, duration: 0.106s, episode steps: 100, steps per second: 940, episode reward: 56.621, mean reward: 0.566 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.043, 10.098], loss: --, mae: --, mean_q: --
   3600/1000000: episode: 36, duration: 0.094s, episode steps: 100, steps per second: 1065, episode reward: 59.429, mean reward: 0.594 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.004, 10.194], loss: --, mae: --, mean_q: --
   3700/1000000: episode: 37, duration: 0.111s, episode steps: 100, steps per second: 897, episode reward: 59.059, mean reward: 0.591 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.576, 10.098], loss: --, mae: --, mean_q: --
   3800/1000000: episode: 38, duration: 0.107s, episode steps: 100, steps per second: 932, episode reward: 57.997, mean reward: 0.580 [0.508, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.250], loss: --, mae: --, mean_q: --
   3900/1000000: episode: 39, duration: 0.109s, episode steps: 100, steps per second: 914, episode reward: 61.626, mean reward: 0.616 [0.513, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.369, 10.100], loss: --, mae: --, mean_q: --
   4000/1000000: episode: 40, duration: 0.109s, episode steps: 100, steps per second: 916, episode reward: 59.722, mean reward: 0.597 [0.519, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.878, 10.256], loss: --, mae: --, mean_q: --
   4100/1000000: episode: 41, duration: 0.110s, episode steps: 100, steps per second: 908, episode reward: 59.084, mean reward: 0.591 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.247, 10.365], loss: --, mae: --, mean_q: --
   4200/1000000: episode: 42, duration: 0.110s, episode steps: 100, steps per second: 910, episode reward: 57.489, mean reward: 0.575 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.098], loss: --, mae: --, mean_q: --
   4300/1000000: episode: 43, duration: 0.090s, episode steps: 100, steps per second: 1111, episode reward: 61.750, mean reward: 0.617 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.127, 10.474], loss: --, mae: --, mean_q: --
   4400/1000000: episode: 44, duration: 0.100s, episode steps: 100, steps per second: 996, episode reward: 58.205, mean reward: 0.582 [0.497, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.733, 10.098], loss: --, mae: --, mean_q: --
   4500/1000000: episode: 45, duration: 0.105s, episode steps: 100, steps per second: 950, episode reward: 58.382, mean reward: 0.584 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.194, 10.170], loss: --, mae: --, mean_q: --
   4600/1000000: episode: 46, duration: 0.106s, episode steps: 100, steps per second: 944, episode reward: 58.242, mean reward: 0.582 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.650, 10.312], loss: --, mae: --, mean_q: --
   4700/1000000: episode: 47, duration: 0.106s, episode steps: 100, steps per second: 941, episode reward: 59.483, mean reward: 0.595 [0.515, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.099, 10.127], loss: --, mae: --, mean_q: --
   4800/1000000: episode: 48, duration: 0.107s, episode steps: 100, steps per second: 933, episode reward: 58.210, mean reward: 0.582 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.127, 10.098], loss: --, mae: --, mean_q: --
   4900/1000000: episode: 49, duration: 0.105s, episode steps: 100, steps per second: 949, episode reward: 58.290, mean reward: 0.583 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.746, 10.230], loss: --, mae: --, mean_q: --
   5000/1000000: episode: 50, duration: 0.103s, episode steps: 100, steps per second: 971, episode reward: 59.440, mean reward: 0.594 [0.498, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.311], loss: --, mae: --, mean_q: --
   5100/1000000: episode: 51, duration: 1.558s, episode steps: 100, steps per second: 64, episode reward: 60.033, mean reward: 0.600 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.098], loss: 0.026058, mae: 0.141368, mean_q: 0.477519
   5200/1000000: episode: 52, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 56.432, mean reward: 0.564 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.985, 10.150], loss: 0.002406, mae: 0.052526, mean_q: 0.808077
   5300/1000000: episode: 53, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 58.375, mean reward: 0.584 [0.497, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.817, 10.140], loss: 0.002376, mae: 0.050489, mean_q: 0.950678
   5400/1000000: episode: 54, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 57.688, mean reward: 0.577 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.271], loss: 0.002532, mae: 0.051939, mean_q: 1.038724
   5500/1000000: episode: 55, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 58.723, mean reward: 0.587 [0.503, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.084, 10.098], loss: 0.002582, mae: 0.050242, mean_q: 1.088733
   5600/1000000: episode: 56, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 58.247, mean reward: 0.582 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.595, 10.124], loss: 0.002660, mae: 0.051488, mean_q: 1.120350
   5700/1000000: episode: 57, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 58.033, mean reward: 0.580 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.521, 10.259], loss: 0.002815, mae: 0.052230, mean_q: 1.140141
   5800/1000000: episode: 58, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 61.071, mean reward: 0.611 [0.516, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.682, 10.098], loss: 0.002708, mae: 0.052415, mean_q: 1.150953
   5900/1000000: episode: 59, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 57.626, mean reward: 0.576 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.391], loss: 0.002615, mae: 0.050748, mean_q: 1.156080
   6000/1000000: episode: 60, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 60.115, mean reward: 0.601 [0.518, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.158, 10.233], loss: 0.002687, mae: 0.050724, mean_q: 1.163131
   6100/1000000: episode: 61, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 58.265, mean reward: 0.583 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.662, 10.160], loss: 0.002982, mae: 0.052946, mean_q: 1.166681
   6200/1000000: episode: 62, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 58.332, mean reward: 0.583 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.957, 10.098], loss: 0.002541, mae: 0.051079, mean_q: 1.166365
   6300/1000000: episode: 63, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 59.165, mean reward: 0.592 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.338, 10.423], loss: 0.002444, mae: 0.050217, mean_q: 1.170654
   6400/1000000: episode: 64, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 57.673, mean reward: 0.577 [0.509, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.098], loss: 0.002674, mae: 0.051176, mean_q: 1.171873
   6500/1000000: episode: 65, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 62.811, mean reward: 0.628 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.359, 10.516], loss: 0.002766, mae: 0.052941, mean_q: 1.169814
   6600/1000000: episode: 66, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 59.506, mean reward: 0.595 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.491, 10.294], loss: 0.002830, mae: 0.051489, mean_q: 1.167711
   6700/1000000: episode: 67, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.644, mean reward: 0.606 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.092, 10.098], loss: 0.003163, mae: 0.056595, mean_q: 1.171159
   6800/1000000: episode: 68, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.941, mean reward: 0.589 [0.502, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.090, 10.228], loss: 0.002998, mae: 0.053617, mean_q: 1.167840
   6900/1000000: episode: 69, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.742, mean reward: 0.587 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.360], loss: 0.002497, mae: 0.050900, mean_q: 1.172127
   7000/1000000: episode: 70, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.687, mean reward: 0.567 [0.498, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.925, 10.182], loss: 0.002681, mae: 0.050833, mean_q: 1.166328
   7100/1000000: episode: 71, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.627, mean reward: 0.596 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.930, 10.126], loss: 0.002671, mae: 0.050992, mean_q: 1.169389
   7200/1000000: episode: 72, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.006, mean reward: 0.580 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.317, 10.299], loss: 0.002879, mae: 0.052133, mean_q: 1.167471
   7300/1000000: episode: 73, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.298, mean reward: 0.583 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.579, 10.157], loss: 0.002677, mae: 0.050046, mean_q: 1.167739
   7400/1000000: episode: 74, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 62.344, mean reward: 0.623 [0.510, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.075, 10.350], loss: 0.002657, mae: 0.050376, mean_q: 1.162881
   7500/1000000: episode: 75, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.452, mean reward: 0.575 [0.507, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.352], loss: 0.002492, mae: 0.049425, mean_q: 1.163369
   7600/1000000: episode: 76, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 56.854, mean reward: 0.569 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.178, 10.098], loss: 0.002564, mae: 0.051255, mean_q: 1.165988
   7700/1000000: episode: 77, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.092, mean reward: 0.581 [0.499, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.506, 10.098], loss: 0.002770, mae: 0.050894, mean_q: 1.163660
   7800/1000000: episode: 78, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.580, mean reward: 0.576 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.789, 10.098], loss: 0.002788, mae: 0.050618, mean_q: 1.165200
   7900/1000000: episode: 79, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.957, mean reward: 0.580 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.103, 10.098], loss: 0.002574, mae: 0.050610, mean_q: 1.164763
   8000/1000000: episode: 80, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.305, mean reward: 0.573 [0.510, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.330], loss: 0.002985, mae: 0.054018, mean_q: 1.161894
   8100/1000000: episode: 81, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.145, mean reward: 0.601 [0.507, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.899, 10.174], loss: 0.002605, mae: 0.050535, mean_q: 1.163703
   8200/1000000: episode: 82, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.315, mean reward: 0.573 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.581, 10.098], loss: 0.002853, mae: 0.052256, mean_q: 1.161650
   8300/1000000: episode: 83, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.675, mean reward: 0.577 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.231, 10.156], loss: 0.002497, mae: 0.049366, mean_q: 1.164081
   8400/1000000: episode: 84, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.436, mean reward: 0.584 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.659, 10.129], loss: 0.002165, mae: 0.047307, mean_q: 1.164820
   8500/1000000: episode: 85, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.918, mean reward: 0.599 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.098], loss: 0.002575, mae: 0.050019, mean_q: 1.162337
   8600/1000000: episode: 86, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.902, mean reward: 0.589 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.527, 10.126], loss: 0.002646, mae: 0.050540, mean_q: 1.162878
   8700/1000000: episode: 87, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.869, mean reward: 0.589 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.351, 10.116], loss: 0.002787, mae: 0.052433, mean_q: 1.162136
   8800/1000000: episode: 88, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.152, mean reward: 0.582 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.442, 10.331], loss: 0.002539, mae: 0.050289, mean_q: 1.163834
   8900/1000000: episode: 89, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 57.645, mean reward: 0.576 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.124, 10.135], loss: 0.002451, mae: 0.049509, mean_q: 1.162945
   9000/1000000: episode: 90, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.165, mean reward: 0.582 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.872, 10.098], loss: 0.002734, mae: 0.052405, mean_q: 1.164200
   9100/1000000: episode: 91, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.784, mean reward: 0.598 [0.501, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.687, 10.098], loss: 0.002177, mae: 0.047421, mean_q: 1.162225
   9200/1000000: episode: 92, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.097, mean reward: 0.601 [0.518, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.530, 10.098], loss: 0.002483, mae: 0.049507, mean_q: 1.163408
   9300/1000000: episode: 93, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.434, mean reward: 0.604 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.350, 10.210], loss: 0.002459, mae: 0.049299, mean_q: 1.163409
   9400/1000000: episode: 94, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.464, mean reward: 0.585 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.241, 10.113], loss: 0.002749, mae: 0.050819, mean_q: 1.159904
   9500/1000000: episode: 95, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.530, mean reward: 0.595 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.861, 10.175], loss: 0.002619, mae: 0.050775, mean_q: 1.160915
   9600/1000000: episode: 96, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.856, mean reward: 0.579 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.557, 10.369], loss: 0.002395, mae: 0.048670, mean_q: 1.164483
   9700/1000000: episode: 97, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.876, mean reward: 0.589 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.098], loss: 0.002440, mae: 0.048688, mean_q: 1.162133
   9800/1000000: episode: 98, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.126, mean reward: 0.591 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.805, 10.098], loss: 0.002524, mae: 0.050888, mean_q: 1.164564
   9900/1000000: episode: 99, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.370, mean reward: 0.604 [0.515, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.916, 10.098], loss: 0.002343, mae: 0.048737, mean_q: 1.162989
  10000/1000000: episode: 100, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.763, mean reward: 0.578 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.098], loss: 0.002298, mae: 0.047832, mean_q: 1.163598
  10100/1000000: episode: 101, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.993, mean reward: 0.570 [0.503, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.037, 10.109], loss: 0.002612, mae: 0.051141, mean_q: 1.160803
  10200/1000000: episode: 102, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.072, mean reward: 0.601 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.098], loss: 0.002046, mae: 0.045960, mean_q: 1.162576
  10300/1000000: episode: 103, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.790, mean reward: 0.598 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.120], loss: 0.002444, mae: 0.051147, mean_q: 1.165422
  10400/1000000: episode: 104, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.909, mean reward: 0.589 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.365], loss: 0.002517, mae: 0.049839, mean_q: 1.164516
  10500/1000000: episode: 105, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.807, mean reward: 0.578 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.708, 10.098], loss: 0.002268, mae: 0.049255, mean_q: 1.165594
  10600/1000000: episode: 106, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.480, mean reward: 0.575 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.143], loss: 0.002344, mae: 0.048218, mean_q: 1.163400
  10700/1000000: episode: 107, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.081, mean reward: 0.591 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.174, 10.138], loss: 0.002414, mae: 0.048695, mean_q: 1.165139
  10800/1000000: episode: 108, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.803, mean reward: 0.578 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.829, 10.098], loss: 0.002206, mae: 0.048104, mean_q: 1.164960
  10900/1000000: episode: 109, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.426, mean reward: 0.614 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.379, 10.098], loss: 0.002388, mae: 0.048837, mean_q: 1.161880
  11000/1000000: episode: 110, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.875, mean reward: 0.579 [0.506, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.309, 10.117], loss: 0.002771, mae: 0.052604, mean_q: 1.162198
  11100/1000000: episode: 111, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.108, mean reward: 0.591 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.098], loss: 0.002060, mae: 0.047384, mean_q: 1.164261
  11200/1000000: episode: 112, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 61.129, mean reward: 0.611 [0.509, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.864, 10.205], loss: 0.002307, mae: 0.048512, mean_q: 1.165337
  11300/1000000: episode: 113, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 63.080, mean reward: 0.631 [0.510, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.098], loss: 0.002289, mae: 0.048855, mean_q: 1.163502
  11400/1000000: episode: 114, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.084, mean reward: 0.601 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.995, 10.098], loss: 0.002035, mae: 0.046462, mean_q: 1.165021
  11500/1000000: episode: 115, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.467, mean reward: 0.585 [0.515, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.208], loss: 0.002073, mae: 0.047189, mean_q: 1.167876
  11600/1000000: episode: 116, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.729, mean reward: 0.607 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.636, 10.098], loss: 0.002396, mae: 0.049553, mean_q: 1.165316
  11700/1000000: episode: 117, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.816, mean reward: 0.568 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.914, 10.098], loss: 0.002118, mae: 0.047404, mean_q: 1.165058
  11800/1000000: episode: 118, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.307, mean reward: 0.593 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.262], loss: 0.002128, mae: 0.047940, mean_q: 1.161718
  11900/1000000: episode: 119, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.477, mean reward: 0.585 [0.501, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.600, 10.243], loss: 0.002040, mae: 0.046959, mean_q: 1.166764
  12000/1000000: episode: 120, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.708, mean reward: 0.587 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.257, 10.125], loss: 0.002240, mae: 0.048344, mean_q: 1.163391
  12100/1000000: episode: 121, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.927, mean reward: 0.599 [0.501, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.511, 10.098], loss: 0.002272, mae: 0.050647, mean_q: 1.167536
  12200/1000000: episode: 122, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.365, mean reward: 0.604 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.679, 10.098], loss: 0.002494, mae: 0.050653, mean_q: 1.161630
  12300/1000000: episode: 123, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.678, mean reward: 0.587 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.010, 10.098], loss: 0.001946, mae: 0.047024, mean_q: 1.170263
  12400/1000000: episode: 124, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.956, mean reward: 0.570 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.559, 10.098], loss: 0.002147, mae: 0.048649, mean_q: 1.168050
  12500/1000000: episode: 125, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.518, mean reward: 0.575 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.224, 10.108], loss: 0.002402, mae: 0.050579, mean_q: 1.163614
  12600/1000000: episode: 126, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.284, mean reward: 0.583 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.334], loss: 0.001908, mae: 0.046862, mean_q: 1.167587
  12700/1000000: episode: 127, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.074, mean reward: 0.581 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.913, 10.098], loss: 0.001922, mae: 0.046493, mean_q: 1.164858
  12800/1000000: episode: 128, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.680, mean reward: 0.587 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.914, 10.139], loss: 0.001919, mae: 0.046185, mean_q: 1.163656
  12900/1000000: episode: 129, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 57.865, mean reward: 0.579 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.681, 10.231], loss: 0.002218, mae: 0.049602, mean_q: 1.164648
  13000/1000000: episode: 130, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.276, mean reward: 0.583 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.788, 10.217], loss: 0.002051, mae: 0.047255, mean_q: 1.165104
  13100/1000000: episode: 131, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 60.732, mean reward: 0.607 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.711, 10.372], loss: 0.001979, mae: 0.046766, mean_q: 1.164508
  13200/1000000: episode: 132, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 56.864, mean reward: 0.569 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.525, 10.265], loss: 0.001816, mae: 0.045527, mean_q: 1.166274
  13300/1000000: episode: 133, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.074, mean reward: 0.571 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.439, 10.098], loss: 0.002034, mae: 0.047042, mean_q: 1.166855
  13400/1000000: episode: 134, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.115, mean reward: 0.591 [0.498, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.063, 10.098], loss: 0.001971, mae: 0.046437, mean_q: 1.165255
  13500/1000000: episode: 135, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.550, mean reward: 0.606 [0.513, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.137], loss: 0.002009, mae: 0.046716, mean_q: 1.163619
  13600/1000000: episode: 136, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.930, mean reward: 0.599 [0.506, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.384, 10.444], loss: 0.001720, mae: 0.044688, mean_q: 1.166200
  13700/1000000: episode: 137, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.258, mean reward: 0.573 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.424, 10.098], loss: 0.002076, mae: 0.047720, mean_q: 1.164612
  13800/1000000: episode: 138, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.091, mean reward: 0.581 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.552, 10.098], loss: 0.002142, mae: 0.048402, mean_q: 1.168571
  13900/1000000: episode: 139, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.107, mean reward: 0.581 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.650, 10.163], loss: 0.001877, mae: 0.046725, mean_q: 1.167513
  14000/1000000: episode: 140, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.381, mean reward: 0.594 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.528, 10.098], loss: 0.002132, mae: 0.048947, mean_q: 1.166024
  14100/1000000: episode: 141, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.829, mean reward: 0.608 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.591, 10.098], loss: 0.002671, mae: 0.050193, mean_q: 1.164031
  14200/1000000: episode: 142, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.296, mean reward: 0.583 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.740, 10.352], loss: 0.002164, mae: 0.048089, mean_q: 1.161728
  14300/1000000: episode: 143, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.628, mean reward: 0.576 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.649, 10.146], loss: 0.001784, mae: 0.044573, mean_q: 1.164719
  14400/1000000: episode: 144, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.130, mean reward: 0.591 [0.512, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.325, 10.146], loss: 0.001988, mae: 0.047500, mean_q: 1.164193
  14500/1000000: episode: 145, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.992, mean reward: 0.580 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.962, 10.118], loss: 0.001863, mae: 0.046337, mean_q: 1.165780
  14600/1000000: episode: 146, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.152, mean reward: 0.602 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.724, 10.098], loss: 0.001906, mae: 0.046644, mean_q: 1.165226
  14700/1000000: episode: 147, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 59.176, mean reward: 0.592 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.425, 10.127], loss: 0.001755, mae: 0.044901, mean_q: 1.166422
  14800/1000000: episode: 148, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.684, mean reward: 0.597 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.252, 10.098], loss: 0.001969, mae: 0.046958, mean_q: 1.165162
  14900/1000000: episode: 149, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.547, mean reward: 0.625 [0.503, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.233, 10.098], loss: 0.001833, mae: 0.045679, mean_q: 1.163240
  15000/1000000: episode: 150, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.874, mean reward: 0.579 [0.505, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.626, 10.098], loss: 0.001917, mae: 0.046036, mean_q: 1.165776
  15100/1000000: episode: 151, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.026, mean reward: 0.580 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.436, 10.098], loss: 0.002346, mae: 0.050846, mean_q: 1.164735
  15200/1000000: episode: 152, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.674, mean reward: 0.577 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.287, 10.098], loss: 0.002067, mae: 0.048337, mean_q: 1.165694
  15300/1000000: episode: 153, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.392, mean reward: 0.574 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.481, 10.105], loss: 0.002007, mae: 0.046923, mean_q: 1.165349
  15400/1000000: episode: 154, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.383, mean reward: 0.574 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.164, 10.098], loss: 0.002054, mae: 0.048441, mean_q: 1.167414
  15500/1000000: episode: 155, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.614, mean reward: 0.576 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.191, 10.098], loss: 0.001807, mae: 0.045836, mean_q: 1.166179
  15600/1000000: episode: 156, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 59.953, mean reward: 0.600 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.860, 10.098], loss: 0.001959, mae: 0.046796, mean_q: 1.162910
  15700/1000000: episode: 157, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 62.259, mean reward: 0.623 [0.500, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.341, 10.098], loss: 0.001754, mae: 0.045175, mean_q: 1.164543
  15800/1000000: episode: 158, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.142, mean reward: 0.581 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.216, 10.098], loss: 0.001813, mae: 0.045551, mean_q: 1.165707
  15900/1000000: episode: 159, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.731, mean reward: 0.597 [0.500, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.218, 10.098], loss: 0.001699, mae: 0.044894, mean_q: 1.165957
  16000/1000000: episode: 160, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.708, mean reward: 0.597 [0.513, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.994, 10.098], loss: 0.001971, mae: 0.047676, mean_q: 1.167893
  16100/1000000: episode: 161, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.533, mean reward: 0.595 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.506, 10.333], loss: 0.002046, mae: 0.047555, mean_q: 1.168235
  16200/1000000: episode: 162, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.744, mean reward: 0.577 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.890, 10.098], loss: 0.002971, mae: 0.051628, mean_q: 1.166450
  16300/1000000: episode: 163, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.473, mean reward: 0.565 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.174], loss: 0.002759, mae: 0.050007, mean_q: 1.161850
  16400/1000000: episode: 164, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.978, mean reward: 0.580 [0.513, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.884, 10.098], loss: 0.002901, mae: 0.051470, mean_q: 1.162366
  16500/1000000: episode: 165, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.329, mean reward: 0.573 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.098], loss: 0.002546, mae: 0.049233, mean_q: 1.158247
  16600/1000000: episode: 166, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.397, mean reward: 0.574 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.281, 10.179], loss: 0.002305, mae: 0.047365, mean_q: 1.159495
  16700/1000000: episode: 167, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.005, mean reward: 0.570 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.062, 10.302], loss: 0.002351, mae: 0.047312, mean_q: 1.161048
  16800/1000000: episode: 168, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.500, mean reward: 0.595 [0.500, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.290], loss: 0.002421, mae: 0.047598, mean_q: 1.158932
  16900/1000000: episode: 169, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.581, mean reward: 0.596 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.212], loss: 0.002287, mae: 0.048166, mean_q: 1.160040
  17000/1000000: episode: 170, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.996, mean reward: 0.590 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.747, 10.368], loss: 0.002298, mae: 0.047106, mean_q: 1.161249
  17100/1000000: episode: 171, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.997, mean reward: 0.610 [0.503, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.569, 10.452], loss: 0.002272, mae: 0.047397, mean_q: 1.161105
  17200/1000000: episode: 172, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 63.618, mean reward: 0.636 [0.510, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.898, 10.098], loss: 0.002370, mae: 0.048395, mean_q: 1.157792
  17300/1000000: episode: 173, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.153, mean reward: 0.592 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.206, 10.277], loss: 0.002220, mae: 0.047494, mean_q: 1.163462
  17400/1000000: episode: 174, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.922, mean reward: 0.599 [0.515, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.798, 10.098], loss: 0.002495, mae: 0.048191, mean_q: 1.162690
  17500/1000000: episode: 175, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.420, mean reward: 0.604 [0.523, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.644, 10.098], loss: 0.002488, mae: 0.048807, mean_q: 1.163946
  17600/1000000: episode: 176, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.568, mean reward: 0.606 [0.514, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.098], loss: 0.002223, mae: 0.047316, mean_q: 1.165736
  17700/1000000: episode: 177, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 62.516, mean reward: 0.625 [0.513, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.791, 10.098], loss: 0.002334, mae: 0.048165, mean_q: 1.164149
  17800/1000000: episode: 178, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 60.534, mean reward: 0.605 [0.506, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.098], loss: 0.002489, mae: 0.049187, mean_q: 1.166577
  17900/1000000: episode: 179, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 61.706, mean reward: 0.617 [0.515, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.073, 10.441], loss: 0.002282, mae: 0.047803, mean_q: 1.167934
  18000/1000000: episode: 180, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.829, mean reward: 0.608 [0.500, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.797, 10.098], loss: 0.002410, mae: 0.048978, mean_q: 1.167894
  18100/1000000: episode: 181, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.646, mean reward: 0.586 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.020, 10.098], loss: 0.002239, mae: 0.048080, mean_q: 1.171562
  18200/1000000: episode: 182, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.000, mean reward: 0.600 [0.516, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.989, 10.098], loss: 0.002269, mae: 0.047840, mean_q: 1.172354
  18300/1000000: episode: 183, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 60.201, mean reward: 0.602 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.244, 10.098], loss: 0.002580, mae: 0.049860, mean_q: 1.168867
  18400/1000000: episode: 184, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.378, mean reward: 0.584 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.825, 10.098], loss: 0.002110, mae: 0.047668, mean_q: 1.170478
  18500/1000000: episode: 185, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 63.169, mean reward: 0.632 [0.507, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.485, 10.098], loss: 0.001905, mae: 0.046594, mean_q: 1.172373
  18600/1000000: episode: 186, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.704, mean reward: 0.587 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.657, 10.098], loss: 0.001864, mae: 0.046322, mean_q: 1.171917
  18700/1000000: episode: 187, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.004, mean reward: 0.580 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.531, 10.098], loss: 0.002064, mae: 0.047031, mean_q: 1.172971
  18800/1000000: episode: 188, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 59.315, mean reward: 0.593 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.893, 10.098], loss: 0.002289, mae: 0.048662, mean_q: 1.172727
  18900/1000000: episode: 189, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 62.193, mean reward: 0.622 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.740, 10.098], loss: 0.002207, mae: 0.048956, mean_q: 1.176337
  19000/1000000: episode: 190, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 60.059, mean reward: 0.601 [0.506, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.467, 10.318], loss: 0.001731, mae: 0.045237, mean_q: 1.174925
  19100/1000000: episode: 191, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 59.101, mean reward: 0.591 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.383, 10.197], loss: 0.001938, mae: 0.046045, mean_q: 1.172220
  19200/1000000: episode: 192, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 62.258, mean reward: 0.623 [0.515, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.098], loss: 0.001948, mae: 0.047045, mean_q: 1.172445
  19300/1000000: episode: 193, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 57.391, mean reward: 0.574 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.635, 10.098], loss: 0.001647, mae: 0.043756, mean_q: 1.175827
  19400/1000000: episode: 194, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.971, mean reward: 0.580 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.098], loss: 0.001641, mae: 0.044071, mean_q: 1.172865
  19500/1000000: episode: 195, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 60.536, mean reward: 0.605 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.721, 10.419], loss: 0.001968, mae: 0.047050, mean_q: 1.174866
  19600/1000000: episode: 196, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 62.288, mean reward: 0.623 [0.502, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.004, 10.098], loss: 0.002083, mae: 0.048432, mean_q: 1.175136
  19700/1000000: episode: 197, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 58.999, mean reward: 0.590 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.356, 10.098], loss: 0.001751, mae: 0.045176, mean_q: 1.176893
  19800/1000000: episode: 198, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 58.755, mean reward: 0.588 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.730, 10.369], loss: 0.001648, mae: 0.044314, mean_q: 1.175499
done, took 94.060 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
