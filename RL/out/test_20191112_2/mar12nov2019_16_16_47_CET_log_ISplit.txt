Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.176s, episode steps: 100, steps per second: 568, episode reward: 188.314, mean reward: 1.883 [1.443, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.070, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 184.077, mean reward: 1.841 [1.474, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.613, 10.169], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 215.718, mean reward: 2.157 [1.549, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.474, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 186.869, mean reward: 1.869 [1.455, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.190, 10.347], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.069s, episode steps: 100, steps per second: 1445, episode reward: 210.066, mean reward: 2.101 [1.445, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.221, 10.416], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 195.749, mean reward: 1.957 [1.474, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.937, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 202.257, mean reward: 2.023 [1.449, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.372, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.065s, episode steps: 100, steps per second: 1550, episode reward: 217.891, mean reward: 2.179 [1.460, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 205.888, mean reward: 2.059 [1.461, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.445, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.082s, episode steps: 100, steps per second: 1221, episode reward: 197.953, mean reward: 1.980 [1.486, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.239, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.092s, episode steps: 100, steps per second: 1083, episode reward: 187.082, mean reward: 1.871 [1.470, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.650, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.080s, episode steps: 100, steps per second: 1249, episode reward: 185.150, mean reward: 1.851 [1.468, 6.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.930, 10.188], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.073s, episode steps: 100, steps per second: 1365, episode reward: 175.711, mean reward: 1.757 [1.447, 2.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.757, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.081s, episode steps: 100, steps per second: 1234, episode reward: 199.210, mean reward: 1.992 [1.491, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.504, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.077s, episode steps: 100, steps per second: 1298, episode reward: 190.240, mean reward: 1.902 [1.464, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.680, 10.149], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 191.459, mean reward: 1.915 [1.436, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.272, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.070s, episode steps: 100, steps per second: 1422, episode reward: 199.191, mean reward: 1.992 [1.490, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.928, 10.218], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.072s, episode steps: 100, steps per second: 1397, episode reward: 181.992, mean reward: 1.820 [1.434, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.715, 10.120], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.066s, episode steps: 100, steps per second: 1514, episode reward: 189.843, mean reward: 1.898 [1.486, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.687, 10.158], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 192.092, mean reward: 1.921 [1.474, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.578, 10.236], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 189.683, mean reward: 1.897 [1.447, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.783, 10.155], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: 181.829, mean reward: 1.818 [1.469, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.713, 10.235], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.092s, episode steps: 100, steps per second: 1086, episode reward: 204.713, mean reward: 2.047 [1.505, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.273, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.098s, episode steps: 100, steps per second: 1020, episode reward: 183.937, mean reward: 1.839 [1.437, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.547, 10.139], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: 203.407, mean reward: 2.034 [1.458, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.300, 10.571], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 193.089, mean reward: 1.931 [1.453, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.985, 10.126], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 195.507, mean reward: 1.955 [1.437, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.023, 10.117], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 194.267, mean reward: 1.943 [1.441, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.731, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 188.942, mean reward: 1.889 [1.468, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.128, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 186.278, mean reward: 1.863 [1.440, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.111], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.068s, episode steps: 100, steps per second: 1474, episode reward: 199.696, mean reward: 1.997 [1.467, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.734, 10.270], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 194.309, mean reward: 1.943 [1.476, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.000, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 202.880, mean reward: 2.029 [1.502, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.343, 10.134], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 212.627, mean reward: 2.126 [1.508, 6.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.071, 10.217], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 191.923, mean reward: 1.919 [1.468, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 175.781, mean reward: 1.758 [1.460, 2.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.192, 10.163], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 201.886, mean reward: 2.019 [1.475, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 185.091, mean reward: 1.851 [1.444, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.537, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 188.851, mean reward: 1.889 [1.457, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.910, 10.166], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 193.763, mean reward: 1.938 [1.500, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.583, 10.111], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.068s, episode steps: 100, steps per second: 1476, episode reward: 181.321, mean reward: 1.813 [1.447, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.300, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.079s, episode steps: 100, steps per second: 1271, episode reward: 200.140, mean reward: 2.001 [1.474, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.408, 10.113], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 208.795, mean reward: 2.088 [1.486, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.541, 10.113], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.063s, episode steps: 100, steps per second: 1583, episode reward: 182.426, mean reward: 1.824 [1.485, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.115, 10.257], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 183.027, mean reward: 1.830 [1.433, 2.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 186.530, mean reward: 1.865 [1.447, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.082, 10.201], loss: --, mae: --, mean_q: --
done, took 3.342 seconds
[Info] End Importance Splitting. Falsification occurred 0 times.
