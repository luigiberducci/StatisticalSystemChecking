Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.164s, episode steps: 100, steps per second: 609, episode reward: 188.430, mean reward: 1.884 [1.460, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 189.381, mean reward: 1.894 [1.466, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.204, 10.161], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 176.225, mean reward: 1.762 [1.450, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 198.678, mean reward: 1.987 [1.460, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.795, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 201.795, mean reward: 2.018 [1.464, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.791, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.062s, episode steps: 100, steps per second: 1616, episode reward: 210.689, mean reward: 2.107 [1.477, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.360, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 187.059, mean reward: 1.871 [1.444, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.436, 10.211], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1637, episode reward: 195.338, mean reward: 1.953 [1.509, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.450, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1640, episode reward: 185.632, mean reward: 1.856 [1.518, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.975, 10.272], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.077s, episode steps: 100, steps per second: 1303, episode reward: 190.278, mean reward: 1.903 [1.454, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.483, 10.167], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.075s, episode steps: 100, steps per second: 1333, episode reward: 191.467, mean reward: 1.915 [1.484, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.998, 10.287], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.065s, episode steps: 100, steps per second: 1533, episode reward: 191.889, mean reward: 1.919 [1.451, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.386, 10.145], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.071s, episode steps: 100, steps per second: 1418, episode reward: 187.983, mean reward: 1.880 [1.487, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.746, 10.196], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.069s, episode steps: 100, steps per second: 1442, episode reward: 218.535, mean reward: 2.185 [1.517, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.870, 10.135], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.076s, episode steps: 100, steps per second: 1317, episode reward: 191.578, mean reward: 1.916 [1.479, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.254, 10.140], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.066s, episode steps: 100, steps per second: 1507, episode reward: 213.125, mean reward: 2.131 [1.477, 7.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.173, 10.271], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 189.786, mean reward: 1.898 [1.466, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.581, 10.168], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.071s, episode steps: 100, steps per second: 1407, episode reward: 189.567, mean reward: 1.896 [1.449, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.514, 10.131], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.082s, episode steps: 100, steps per second: 1217, episode reward: 186.824, mean reward: 1.868 [1.458, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.566, 10.162], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.084s, episode steps: 100, steps per second: 1186, episode reward: 194.086, mean reward: 1.941 [1.467, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.069, 10.259], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.111s, episode steps: 100, steps per second: 903, episode reward: 198.654, mean reward: 1.987 [1.507, 4.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.974, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.072s, episode steps: 100, steps per second: 1398, episode reward: 202.645, mean reward: 2.026 [1.453, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.792, 10.194], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.155s, episode steps: 100, steps per second: 644, episode reward: 184.102, mean reward: 1.841 [1.458, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.128, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.143s, episode steps: 100, steps per second: 700, episode reward: 199.086, mean reward: 1.991 [1.509, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.949, 10.404], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.078s, episode steps: 100, steps per second: 1288, episode reward: 211.865, mean reward: 2.119 [1.479, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.749, 10.345], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 203.805, mean reward: 2.038 [1.516, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.078s, episode steps: 100, steps per second: 1288, episode reward: 188.816, mean reward: 1.888 [1.466, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.128, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.061s, episode steps: 100, steps per second: 1640, episode reward: 205.399, mean reward: 2.054 [1.508, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.714, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.078s, episode steps: 100, steps per second: 1284, episode reward: 191.732, mean reward: 1.917 [1.480, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.032, 10.139], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 215.093, mean reward: 2.151 [1.492, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.284, 10.434], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.119s, episode steps: 100, steps per second: 841, episode reward: 202.292, mean reward: 2.023 [1.458, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.514, 10.234], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.110s, episode steps: 100, steps per second: 908, episode reward: 188.533, mean reward: 1.885 [1.449, 5.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.979, 10.181], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.103s, episode steps: 100, steps per second: 974, episode reward: 186.800, mean reward: 1.868 [1.444, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.196, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.083s, episode steps: 100, steps per second: 1211, episode reward: 196.694, mean reward: 1.967 [1.470, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.644, 10.379], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.072s, episode steps: 100, steps per second: 1396, episode reward: 196.127, mean reward: 1.961 [1.465, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.792, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.059s, episode steps: 100, steps per second: 1682, episode reward: 196.359, mean reward: 1.964 [1.445, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.698, 10.206], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.066s, episode steps: 100, steps per second: 1512, episode reward: 185.288, mean reward: 1.853 [1.437, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.222, 10.311], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.068s, episode steps: 100, steps per second: 1471, episode reward: 197.443, mean reward: 1.974 [1.478, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: 184.098, mean reward: 1.841 [1.473, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.154], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 199.060, mean reward: 1.991 [1.497, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.786, 10.303], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.106s, episode steps: 100, steps per second: 944, episode reward: 197.383, mean reward: 1.974 [1.485, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.294, 10.227], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 197.556, mean reward: 1.976 [1.457, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.227, 10.123], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.075s, episode steps: 100, steps per second: 1329, episode reward: 194.490, mean reward: 1.945 [1.462, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.092, 10.112], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 184.453, mean reward: 1.845 [1.446, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.687, 10.210], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.093s, episode steps: 100, steps per second: 1075, episode reward: 197.185, mean reward: 1.972 [1.495, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.002, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 192.668, mean reward: 1.927 [1.461, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.784, 10.273], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 197.676, mean reward: 1.977 [1.470, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.015, 10.444], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 195.174, mean reward: 1.952 [1.442, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.802, 10.151], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 182.548, mean reward: 1.825 [1.486, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.071s, episode steps: 100, steps per second: 1408, episode reward: 193.977, mean reward: 1.940 [1.455, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.708, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 181.885, mean reward: 1.819 [1.459, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.650, 10.292], loss: 0.818840, mae: 1.081226, mean_q: 1.029658
  5200/100000: episode: 52, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 188.035, mean reward: 1.880 [1.473, 4.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.037, 10.102], loss: 0.112628, mae: 0.336125, mean_q: 2.398662
  5300/100000: episode: 53, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 191.841, mean reward: 1.918 [1.451, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.748, 10.346], loss: 0.094394, mae: 0.308928, mean_q: 2.808286
  5400/100000: episode: 54, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 204.027, mean reward: 2.040 [1.441, 5.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.011, 10.098], loss: 0.104835, mae: 0.319228, mean_q: 3.175005
  5500/100000: episode: 55, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 197.974, mean reward: 1.980 [1.464, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.184, 10.098], loss: 0.093257, mae: 0.304749, mean_q: 3.436945
  5600/100000: episode: 56, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 192.036, mean reward: 1.920 [1.445, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.524, 10.152], loss: 0.102243, mae: 0.312814, mean_q: 3.597160
  5700/100000: episode: 57, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 187.280, mean reward: 1.873 [1.440, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.413, 10.098], loss: 0.096940, mae: 0.306774, mean_q: 3.677450
  5800/100000: episode: 58, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 189.370, mean reward: 1.894 [1.457, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.256, 10.098], loss: 0.106696, mae: 0.323478, mean_q: 3.743188
  5900/100000: episode: 59, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 180.598, mean reward: 1.806 [1.440, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.098], loss: 0.107119, mae: 0.317737, mean_q: 3.781617
  6000/100000: episode: 60, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 180.189, mean reward: 1.802 [1.439, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.017, 10.143], loss: 0.094892, mae: 0.310801, mean_q: 3.802658
  6100/100000: episode: 61, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 193.610, mean reward: 1.936 [1.445, 6.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.382, 10.171], loss: 0.110725, mae: 0.324269, mean_q: 3.842137
  6200/100000: episode: 62, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 199.262, mean reward: 1.993 [1.451, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.239, 10.098], loss: 0.111475, mae: 0.317636, mean_q: 3.848544
  6300/100000: episode: 63, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 200.199, mean reward: 2.002 [1.501, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.452, 10.098], loss: 0.093907, mae: 0.305421, mean_q: 3.837903
  6400/100000: episode: 64, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 198.313, mean reward: 1.983 [1.451, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.614, 10.283], loss: 0.128139, mae: 0.328128, mean_q: 3.856602
  6500/100000: episode: 65, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 221.114, mean reward: 2.211 [1.469, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.748, 10.098], loss: 0.112276, mae: 0.317483, mean_q: 3.849767
  6600/100000: episode: 66, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.873, mean reward: 1.909 [1.447, 5.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.796, 10.137], loss: 0.101676, mae: 0.311043, mean_q: 3.831530
  6700/100000: episode: 67, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 200.851, mean reward: 2.009 [1.496, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.695, 10.404], loss: 0.115617, mae: 0.328548, mean_q: 3.847816
  6800/100000: episode: 68, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 185.472, mean reward: 1.855 [1.461, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.222, 10.098], loss: 0.103225, mae: 0.311943, mean_q: 3.845838
  6900/100000: episode: 69, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.050, mean reward: 1.821 [1.454, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.821, 10.098], loss: 0.110897, mae: 0.319915, mean_q: 3.857496
  7000/100000: episode: 70, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 189.456, mean reward: 1.895 [1.462, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.734, 10.098], loss: 0.096987, mae: 0.305861, mean_q: 3.851331
  7100/100000: episode: 71, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 203.495, mean reward: 2.035 [1.457, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.140, 10.098], loss: 0.103517, mae: 0.321282, mean_q: 3.848320
  7200/100000: episode: 72, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 195.267, mean reward: 1.953 [1.456, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.466, 10.345], loss: 0.102481, mae: 0.316991, mean_q: 3.843172
  7300/100000: episode: 73, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 180.407, mean reward: 1.804 [1.450, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.455, 10.098], loss: 0.104392, mae: 0.316060, mean_q: 3.847471
  7400/100000: episode: 74, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.379, mean reward: 1.924 [1.466, 5.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.938, 10.098], loss: 0.102985, mae: 0.310599, mean_q: 3.857252
  7500/100000: episode: 75, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 213.301, mean reward: 2.133 [1.463, 5.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.555, 10.313], loss: 0.094391, mae: 0.297819, mean_q: 3.841750
  7600/100000: episode: 76, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 182.311, mean reward: 1.823 [1.448, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.106568, mae: 0.313177, mean_q: 3.852840
  7700/100000: episode: 77, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 192.218, mean reward: 1.922 [1.446, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.340, 10.098], loss: 0.111594, mae: 0.317874, mean_q: 3.841504
  7800/100000: episode: 78, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 187.662, mean reward: 1.877 [1.476, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.295, 10.224], loss: 0.106975, mae: 0.317785, mean_q: 3.848507
  7900/100000: episode: 79, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 266.935, mean reward: 2.669 [1.482, 6.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.252, 10.439], loss: 0.096103, mae: 0.304720, mean_q: 3.828259
  8000/100000: episode: 80, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.158, mean reward: 1.992 [1.476, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.323, 10.145], loss: 0.133687, mae: 0.336768, mean_q: 3.871493
  8100/100000: episode: 81, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.314, mean reward: 1.893 [1.471, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.076, 10.098], loss: 0.118786, mae: 0.321346, mean_q: 3.845059
  8200/100000: episode: 82, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.027, mean reward: 1.810 [1.459, 2.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.969, 10.098], loss: 0.101638, mae: 0.302307, mean_q: 3.848101
  8300/100000: episode: 83, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 194.907, mean reward: 1.949 [1.459, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.928, 10.338], loss: 0.115054, mae: 0.321071, mean_q: 3.843540
  8400/100000: episode: 84, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 181.162, mean reward: 1.812 [1.476, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.014, 10.098], loss: 0.108142, mae: 0.307929, mean_q: 3.837629
  8500/100000: episode: 85, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 193.310, mean reward: 1.933 [1.477, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.865, 10.098], loss: 0.121750, mae: 0.317085, mean_q: 3.858286
  8600/100000: episode: 86, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.470, mean reward: 1.875 [1.446, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.264, 10.098], loss: 0.114390, mae: 0.311588, mean_q: 3.843146
  8700/100000: episode: 87, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 194.833, mean reward: 1.948 [1.521, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.837, 10.098], loss: 0.124253, mae: 0.330223, mean_q: 3.846450
  8800/100000: episode: 88, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 177.916, mean reward: 1.779 [1.439, 2.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.736, 10.098], loss: 0.125683, mae: 0.323833, mean_q: 3.833945
  8900/100000: episode: 89, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 186.991, mean reward: 1.870 [1.455, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.850, 10.098], loss: 0.119354, mae: 0.316398, mean_q: 3.837721
  9000/100000: episode: 90, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.714, mean reward: 1.957 [1.485, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.134, 10.098], loss: 0.115946, mae: 0.316957, mean_q: 3.850403
  9100/100000: episode: 91, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 177.380, mean reward: 1.774 [1.443, 2.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.514, 10.163], loss: 0.112916, mae: 0.308800, mean_q: 3.827108
  9200/100000: episode: 92, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.243, mean reward: 1.922 [1.477, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.126, 10.153], loss: 0.116298, mae: 0.310903, mean_q: 3.820178
  9300/100000: episode: 93, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 203.969, mean reward: 2.040 [1.468, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.661, 10.098], loss: 0.108945, mae: 0.304841, mean_q: 3.821483
  9400/100000: episode: 94, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 187.733, mean reward: 1.877 [1.444, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.860, 10.098], loss: 0.116077, mae: 0.314222, mean_q: 3.822268
  9500/100000: episode: 95, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 184.957, mean reward: 1.850 [1.452, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.441, 10.242], loss: 0.105324, mae: 0.307928, mean_q: 3.821005
  9600/100000: episode: 96, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 218.651, mean reward: 2.187 [1.527, 6.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.331, 10.098], loss: 0.115500, mae: 0.316264, mean_q: 3.823384
  9700/100000: episode: 97, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 199.826, mean reward: 1.998 [1.490, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.882, 10.385], loss: 0.106433, mae: 0.309275, mean_q: 3.826995
  9800/100000: episode: 98, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 192.208, mean reward: 1.922 [1.450, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: 0.130908, mae: 0.329343, mean_q: 3.824624
  9900/100000: episode: 99, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 192.008, mean reward: 1.920 [1.501, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.098], loss: 0.112705, mae: 0.313538, mean_q: 3.838856
 10000/100000: episode: 100, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 181.383, mean reward: 1.814 [1.442, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.427, 10.292], loss: 0.114469, mae: 0.314303, mean_q: 3.825368
 10100/100000: episode: 101, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 181.629, mean reward: 1.816 [1.468, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.786, 10.098], loss: 0.109764, mae: 0.313173, mean_q: 3.833702
 10200/100000: episode: 102, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 230.182, mean reward: 2.302 [1.509, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.224, 10.360], loss: 0.114241, mae: 0.312385, mean_q: 3.819974
 10300/100000: episode: 103, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.815, mean reward: 1.858 [1.494, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.201, 10.098], loss: 0.106467, mae: 0.303486, mean_q: 3.828782
 10400/100000: episode: 104, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 208.754, mean reward: 2.088 [1.454, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.044, 10.107], loss: 0.113549, mae: 0.322389, mean_q: 3.846493
 10500/100000: episode: 105, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 180.612, mean reward: 1.806 [1.459, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.381, 10.243], loss: 0.122157, mae: 0.322237, mean_q: 3.847738
 10600/100000: episode: 106, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 189.265, mean reward: 1.893 [1.480, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.098], loss: 0.109978, mae: 0.310028, mean_q: 3.830125
 10700/100000: episode: 107, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 189.255, mean reward: 1.893 [1.477, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.833, 10.098], loss: 0.137128, mae: 0.331341, mean_q: 3.854949
 10800/100000: episode: 108, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.726, mean reward: 1.907 [1.482, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.966, 10.121], loss: 0.117974, mae: 0.319471, mean_q: 3.862298
 10900/100000: episode: 109, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 197.670, mean reward: 1.977 [1.496, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.417, 10.098], loss: 0.111848, mae: 0.320953, mean_q: 3.848470
 11000/100000: episode: 110, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 188.286, mean reward: 1.883 [1.440, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.670, 10.098], loss: 0.109316, mae: 0.317730, mean_q: 3.854697
 11100/100000: episode: 111, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.630, mean reward: 2.016 [1.507, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.900, 10.098], loss: 0.105275, mae: 0.308115, mean_q: 3.854357
 11200/100000: episode: 112, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.276, mean reward: 1.823 [1.446, 2.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.785, 10.185], loss: 0.107484, mae: 0.312231, mean_q: 3.842491
 11300/100000: episode: 113, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 210.650, mean reward: 2.107 [1.463, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.948, 10.098], loss: 0.103604, mae: 0.305156, mean_q: 3.848335
 11400/100000: episode: 114, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 197.349, mean reward: 1.973 [1.484, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.743, 10.098], loss: 0.095524, mae: 0.290199, mean_q: 3.831695
 11500/100000: episode: 115, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 187.324, mean reward: 1.873 [1.467, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.275, 10.098], loss: 0.095557, mae: 0.301379, mean_q: 3.843662
 11600/100000: episode: 116, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.690, mean reward: 1.867 [1.486, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.433, 10.264], loss: 0.096914, mae: 0.303169, mean_q: 3.834662
 11700/100000: episode: 117, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 176.799, mean reward: 1.768 [1.431, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.487, 10.113], loss: 0.099080, mae: 0.297134, mean_q: 3.818875
 11800/100000: episode: 118, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 196.903, mean reward: 1.969 [1.504, 4.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.098], loss: 0.110061, mae: 0.312591, mean_q: 3.833283
 11900/100000: episode: 119, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 183.441, mean reward: 1.834 [1.483, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.286, 10.122], loss: 0.093436, mae: 0.289886, mean_q: 3.823546
 12000/100000: episode: 120, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 189.590, mean reward: 1.896 [1.457, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.111, 10.098], loss: 0.104519, mae: 0.306834, mean_q: 3.837183
 12100/100000: episode: 121, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 185.871, mean reward: 1.859 [1.458, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.010, 10.130], loss: 0.089860, mae: 0.292195, mean_q: 3.825173
 12200/100000: episode: 122, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 196.392, mean reward: 1.964 [1.478, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.816, 10.417], loss: 0.103422, mae: 0.296870, mean_q: 3.829271
 12300/100000: episode: 123, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 187.832, mean reward: 1.878 [1.457, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.085, 10.098], loss: 0.105746, mae: 0.304812, mean_q: 3.818101
 12400/100000: episode: 124, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 205.674, mean reward: 2.057 [1.491, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.301, 10.098], loss: 0.101167, mae: 0.302915, mean_q: 3.827187
 12500/100000: episode: 125, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 197.841, mean reward: 1.978 [1.523, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.399, 10.237], loss: 0.095215, mae: 0.301527, mean_q: 3.815608
 12600/100000: episode: 126, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 184.768, mean reward: 1.848 [1.462, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.567, 10.189], loss: 0.097386, mae: 0.298079, mean_q: 3.842788
 12700/100000: episode: 127, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 195.081, mean reward: 1.951 [1.458, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.162, 10.200], loss: 0.096231, mae: 0.304248, mean_q: 3.828434
 12800/100000: episode: 128, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 216.220, mean reward: 2.162 [1.491, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.831, 10.098], loss: 0.091680, mae: 0.292638, mean_q: 3.810047
 12900/100000: episode: 129, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 181.008, mean reward: 1.810 [1.436, 2.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.098], loss: 0.088197, mae: 0.295988, mean_q: 3.805552
 13000/100000: episode: 130, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 186.802, mean reward: 1.868 [1.472, 5.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.098], loss: 0.083685, mae: 0.285520, mean_q: 3.791329
 13100/100000: episode: 131, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 192.972, mean reward: 1.930 [1.451, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.781, 10.183], loss: 0.083725, mae: 0.290660, mean_q: 3.798029
 13200/100000: episode: 132, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 188.225, mean reward: 1.882 [1.439, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.368, 10.098], loss: 0.077680, mae: 0.278226, mean_q: 3.806320
 13300/100000: episode: 133, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 195.850, mean reward: 1.958 [1.453, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.911, 10.356], loss: 0.072296, mae: 0.277549, mean_q: 3.792763
 13400/100000: episode: 134, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 192.168, mean reward: 1.922 [1.451, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.165, 10.119], loss: 0.080834, mae: 0.289540, mean_q: 3.815792
 13500/100000: episode: 135, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 187.249, mean reward: 1.872 [1.452, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.941, 10.098], loss: 0.074745, mae: 0.282737, mean_q: 3.792418
 13600/100000: episode: 136, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 199.801, mean reward: 1.998 [1.469, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.526, 10.098], loss: 0.079016, mae: 0.290038, mean_q: 3.798767
 13700/100000: episode: 137, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 176.062, mean reward: 1.761 [1.481, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.590, 10.098], loss: 0.083481, mae: 0.289688, mean_q: 3.814748
 13800/100000: episode: 138, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 257.699, mean reward: 2.577 [1.476, 6.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.605, 10.098], loss: 0.091983, mae: 0.297939, mean_q: 3.825509
 13900/100000: episode: 139, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 190.153, mean reward: 1.902 [1.495, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.996, 10.098], loss: 0.085385, mae: 0.286439, mean_q: 3.811770
 14000/100000: episode: 140, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 194.979, mean reward: 1.950 [1.474, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.760, 10.342], loss: 0.102389, mae: 0.306535, mean_q: 3.836077
 14100/100000: episode: 141, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 192.324, mean reward: 1.923 [1.556, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.015, 10.098], loss: 0.106676, mae: 0.306546, mean_q: 3.848841
 14200/100000: episode: 142, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 191.535, mean reward: 1.915 [1.459, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.723, 10.098], loss: 0.099798, mae: 0.306989, mean_q: 3.841649
 14300/100000: episode: 143, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 207.653, mean reward: 2.077 [1.431, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.369, 10.098], loss: 0.101340, mae: 0.305913, mean_q: 3.834873
 14400/100000: episode: 144, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 191.050, mean reward: 1.911 [1.456, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.969, 10.270], loss: 0.086709, mae: 0.287039, mean_q: 3.830900
 14500/100000: episode: 145, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 200.800, mean reward: 2.008 [1.437, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.931, 10.111], loss: 0.099953, mae: 0.305694, mean_q: 3.846347
 14600/100000: episode: 146, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 193.838, mean reward: 1.938 [1.441, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.885, 10.433], loss: 0.091790, mae: 0.296616, mean_q: 3.820626
 14700/100000: episode: 147, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.578, mean reward: 1.936 [1.478, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.602, 10.315], loss: 0.095094, mae: 0.300545, mean_q: 3.839886
 14800/100000: episode: 148, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 182.488, mean reward: 1.825 [1.450, 2.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.033, 10.098], loss: 0.097028, mae: 0.302859, mean_q: 3.830962
 14900/100000: episode: 149, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 184.279, mean reward: 1.843 [1.452, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.455, 10.098], loss: 0.097213, mae: 0.295444, mean_q: 3.834889
[Info] 1-TH LEVEL FOUND: 4.358685493469238, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.921s, episode steps: 100, steps per second: 17, episode reward: 190.857, mean reward: 1.909 [1.465, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.205], loss: 0.119610, mae: 0.320970, mean_q: 3.867303
 15100/100000: episode: 151, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 212.293, mean reward: 2.123 [1.455, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.585, 10.100], loss: 0.089400, mae: 0.292950, mean_q: 3.840257
 15130/100000: episode: 152, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 91.447, mean reward: 3.048 [1.922, 6.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.496, 10.331], loss: 0.120570, mae: 0.322848, mean_q: 3.868012
 15230/100000: episode: 153, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 208.006, mean reward: 2.080 [1.486, 8.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.859, 10.100], loss: 0.098680, mae: 0.306359, mean_q: 3.848361
 15236/100000: episode: 154, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 12.160, mean reward: 2.027 [1.618, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.200, 10.100], loss: 0.173345, mae: 0.315901, mean_q: 3.881920
 15242/100000: episode: 155, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 19.624, mean reward: 3.271 [2.218, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.289, 10.100], loss: 0.100344, mae: 0.311632, mean_q: 3.836076
 15247/100000: episode: 156, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 9.294, mean reward: 1.859 [1.601, 2.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.161, 10.100], loss: 0.122632, mae: 0.360881, mean_q: 3.954779
 15274/100000: episode: 157, duration: 0.246s, episode steps: 27, steps per second: 110, episode reward: 141.448, mean reward: 5.239 [2.453, 9.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.666], loss: 0.121524, mae: 0.326370, mean_q: 3.860691
 15279/100000: episode: 158, duration: 0.057s, episode steps: 5, steps per second: 88, episode reward: 8.051, mean reward: 1.610 [1.461, 1.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.092, 10.105], loss: 0.117231, mae: 0.323211, mean_q: 3.837146
 15282/100000: episode: 159, duration: 0.026s, episode steps: 3, steps per second: 116, episode reward: 5.717, mean reward: 1.906 [1.651, 2.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.193, 10.149], loss: 0.094562, mae: 0.283260, mean_q: 3.843773
 15287/100000: episode: 160, duration: 0.069s, episode steps: 5, steps per second: 73, episode reward: 9.431, mean reward: 1.886 [1.607, 2.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.090, 10.122], loss: 0.064140, mae: 0.271081, mean_q: 3.807069
 15290/100000: episode: 161, duration: 0.031s, episode steps: 3, steps per second: 96, episode reward: 6.281, mean reward: 2.094 [1.856, 2.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.258, 10.198], loss: 0.107268, mae: 0.284940, mean_q: 3.796270
 15296/100000: episode: 162, duration: 0.050s, episode steps: 6, steps per second: 119, episode reward: 14.180, mean reward: 2.363 [2.156, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.333, 10.100], loss: 0.076121, mae: 0.293305, mean_q: 3.874381
 15302/100000: episode: 163, duration: 0.061s, episode steps: 6, steps per second: 99, episode reward: 11.627, mean reward: 1.938 [1.793, 2.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.171, 10.100], loss: 0.088184, mae: 0.296559, mean_q: 3.739218
 15329/100000: episode: 164, duration: 0.324s, episode steps: 27, steps per second: 83, episode reward: 69.758, mean reward: 2.584 [1.989, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.151, 10.326], loss: 0.185894, mae: 0.351769, mean_q: 3.889299
 15334/100000: episode: 165, duration: 0.083s, episode steps: 5, steps per second: 60, episode reward: 7.970, mean reward: 1.594 [1.469, 1.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.061, 10.100], loss: 0.187714, mae: 0.348875, mean_q: 3.952267
 15339/100000: episode: 166, duration: 0.071s, episode steps: 5, steps per second: 70, episode reward: 9.680, mean reward: 1.936 [1.704, 2.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.123, 10.100], loss: 0.298595, mae: 0.403865, mean_q: 3.854146
 15347/100000: episode: 167, duration: 0.108s, episode steps: 8, steps per second: 74, episode reward: 23.894, mean reward: 2.987 [2.374, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.195, 10.100], loss: 0.101805, mae: 0.311264, mean_q: 3.914076
 15353/100000: episode: 168, duration: 0.067s, episode steps: 6, steps per second: 89, episode reward: 12.819, mean reward: 2.136 [1.900, 2.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.335, 10.100], loss: 0.117795, mae: 0.310564, mean_q: 3.826852
 15453/100000: episode: 169, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 192.702, mean reward: 1.927 [1.448, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.706, 10.392], loss: 0.166345, mae: 0.355312, mean_q: 3.892272
 15462/100000: episode: 170, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 20.257, mean reward: 2.251 [1.716, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.140, 10.100], loss: 0.162799, mae: 0.345710, mean_q: 3.766786
 15489/100000: episode: 171, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 69.065, mean reward: 2.558 [2.124, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.294, 10.374], loss: 0.157447, mae: 0.334534, mean_q: 3.906269
 15498/100000: episode: 172, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 39.375, mean reward: 4.375 [3.460, 8.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.476, 10.100], loss: 0.206899, mae: 0.401523, mean_q: 4.018332
 15501/100000: episode: 173, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 5.465, mean reward: 1.822 [1.456, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.152, 10.190], loss: 0.113054, mae: 0.353243, mean_q: 3.879374
 15507/100000: episode: 174, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 13.476, mean reward: 2.246 [2.000, 2.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.248, 10.100], loss: 0.125532, mae: 0.333627, mean_q: 3.809813
 15607/100000: episode: 175, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 197.301, mean reward: 1.973 [1.458, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.757, 10.237], loss: 0.153857, mae: 0.351984, mean_q: 3.919904
 15634/100000: episode: 176, duration: 0.155s, episode steps: 27, steps per second: 175, episode reward: 60.897, mean reward: 2.255 [1.820, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.303], loss: 0.176025, mae: 0.345751, mean_q: 3.889629
 15661/100000: episode: 177, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 83.313, mean reward: 3.086 [2.132, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.281, 10.361], loss: 0.193592, mae: 0.369684, mean_q: 3.928079
 15761/100000: episode: 178, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 175.792, mean reward: 1.758 [1.443, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.641, 10.127], loss: 0.158322, mae: 0.345818, mean_q: 3.922827
 15788/100000: episode: 179, duration: 0.157s, episode steps: 27, steps per second: 171, episode reward: 72.070, mean reward: 2.669 [1.596, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.199, 10.178], loss: 0.167806, mae: 0.358538, mean_q: 3.950547
 15791/100000: episode: 180, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 5.405, mean reward: 1.802 [1.669, 1.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.049, 10.162], loss: 0.110230, mae: 0.356315, mean_q: 3.957204
 15797/100000: episode: 181, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 14.936, mean reward: 2.489 [2.079, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.298, 10.100], loss: 0.166081, mae: 0.328953, mean_q: 3.947430
 15827/100000: episode: 182, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 161.168, mean reward: 5.372 [2.603, 13.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.628, 10.668], loss: 0.204039, mae: 0.367607, mean_q: 3.935897
 15833/100000: episode: 183, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 11.856, mean reward: 1.976 [1.666, 2.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.390, 10.100], loss: 0.285306, mae: 0.437350, mean_q: 3.975073
 15839/100000: episode: 184, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 11.667, mean reward: 1.945 [1.653, 2.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.206, 10.188], loss: 0.143885, mae: 0.354468, mean_q: 3.943449
 15866/100000: episode: 185, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 100.105, mean reward: 3.708 [2.241, 6.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.185, 10.539], loss: 0.200656, mae: 0.396026, mean_q: 3.982920
 15966/100000: episode: 186, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.137, mean reward: 1.881 [1.479, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.448, 10.224], loss: 0.200416, mae: 0.396524, mean_q: 3.989951
 15996/100000: episode: 187, duration: 0.150s, episode steps: 30, steps per second: 199, episode reward: 67.405, mean reward: 2.247 [1.855, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.288, 10.340], loss: 0.215568, mae: 0.387012, mean_q: 3.973711
 16005/100000: episode: 188, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 33.983, mean reward: 3.776 [3.201, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.344, 10.100], loss: 0.408146, mae: 0.444549, mean_q: 4.099279
 16008/100000: episode: 189, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 4.804, mean reward: 1.601 [1.525, 1.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.370, 10.146], loss: 0.224382, mae: 0.440870, mean_q: 4.075739
 16017/100000: episode: 190, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 53.760, mean reward: 5.973 [4.381, 9.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.548, 10.100], loss: 0.211244, mae: 0.415207, mean_q: 4.008724
 16047/100000: episode: 191, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 91.430, mean reward: 3.048 [1.914, 6.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.276, 10.392], loss: 0.218921, mae: 0.403025, mean_q: 3.983027
 16053/100000: episode: 192, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 10.721, mean reward: 1.787 [1.682, 1.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.148, 10.100], loss: 0.208950, mae: 0.407708, mean_q: 4.023395
 16062/100000: episode: 193, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 50.376, mean reward: 5.597 [4.611, 7.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.521, 10.100], loss: 0.136006, mae: 0.364298, mean_q: 3.964860
 16071/100000: episode: 194, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 44.472, mean reward: 4.941 [4.002, 5.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.484, 10.100], loss: 0.170952, mae: 0.382497, mean_q: 4.055917
 16080/100000: episode: 195, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 17.703, mean reward: 1.967 [1.659, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.129, 10.100], loss: 0.199992, mae: 0.364989, mean_q: 3.855572
 16085/100000: episode: 196, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 11.525, mean reward: 2.305 [1.815, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.052, 10.100], loss: 0.154147, mae: 0.371418, mean_q: 4.067441
 16094/100000: episode: 197, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 16.787, mean reward: 1.865 [1.540, 2.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.728, 10.100], loss: 0.243904, mae: 0.462372, mean_q: 4.093332
 16103/100000: episode: 198, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 36.772, mean reward: 4.086 [3.532, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.374, 10.100], loss: 0.206978, mae: 0.364368, mean_q: 4.000514
 16130/100000: episode: 199, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 68.368, mean reward: 2.532 [2.142, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.291, 10.263], loss: 0.327365, mae: 0.434598, mean_q: 4.013228
 16133/100000: episode: 200, duration: 0.027s, episode steps: 3, steps per second: 112, episode reward: 6.478, mean reward: 2.159 [1.629, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.106, 10.176], loss: 0.161188, mae: 0.414176, mean_q: 4.105549
 16139/100000: episode: 201, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 11.363, mean reward: 1.894 [1.747, 2.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.122, 10.100], loss: 0.149708, mae: 0.389448, mean_q: 4.093104
 16145/100000: episode: 202, duration: 0.046s, episode steps: 6, steps per second: 130, episode reward: 14.832, mean reward: 2.472 [2.045, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.749, 10.100], loss: 0.231637, mae: 0.416796, mean_q: 4.019377
 16153/100000: episode: 203, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 22.368, mean reward: 2.796 [1.958, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.199, 10.100], loss: 0.243643, mae: 0.416255, mean_q: 4.005826
 16180/100000: episode: 204, duration: 0.161s, episode steps: 27, steps per second: 167, episode reward: 143.707, mean reward: 5.322 [2.559, 12.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.258, 10.622], loss: 0.192808, mae: 0.389560, mean_q: 4.048979
 16210/100000: episode: 205, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 123.212, mean reward: 4.107 [2.750, 10.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.343, 10.473], loss: 0.190100, mae: 0.401360, mean_q: 4.042809
 16215/100000: episode: 206, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 9.941, mean reward: 1.988 [1.732, 2.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.100], loss: 0.274544, mae: 0.412320, mean_q: 4.146573
 16224/100000: episode: 207, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 21.688, mean reward: 2.410 [1.650, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.207, 10.100], loss: 0.266663, mae: 0.465847, mean_q: 4.125462
 16254/100000: episode: 208, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 86.608, mean reward: 2.887 [2.263, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.187, 10.483], loss: 0.248983, mae: 0.408244, mean_q: 4.070738
 16284/100000: episode: 209, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 106.805, mean reward: 3.560 [2.837, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.306, 10.437], loss: 0.211919, mae: 0.390097, mean_q: 4.065863
 16293/100000: episode: 210, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 16.472, mean reward: 1.830 [1.621, 1.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.660, 10.100], loss: 0.153022, mae: 0.353397, mean_q: 4.079535
 16299/100000: episode: 211, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 12.286, mean reward: 2.048 [1.773, 2.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.211, 10.100], loss: 0.249270, mae: 0.436334, mean_q: 4.120438
 16307/100000: episode: 212, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 15.398, mean reward: 1.925 [1.674, 2.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.122, 10.100], loss: 0.171815, mae: 0.416409, mean_q: 4.155237
 16407/100000: episode: 213, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 197.376, mean reward: 1.974 [1.431, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.394, 10.127], loss: 0.235469, mae: 0.427933, mean_q: 4.100465
 16413/100000: episode: 214, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 13.967, mean reward: 2.328 [2.063, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.328, 10.100], loss: 0.353887, mae: 0.480328, mean_q: 4.210314
 16422/100000: episode: 215, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 46.272, mean reward: 5.141 [3.236, 8.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.374, 10.100], loss: 0.236426, mae: 0.407435, mean_q: 4.059879
 16449/100000: episode: 216, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 92.472, mean reward: 3.425 [2.337, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.171, 10.441], loss: 0.254603, mae: 0.415903, mean_q: 4.169303
 16479/100000: episode: 217, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 80.519, mean reward: 2.684 [1.948, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.740, 10.337], loss: 0.211272, mae: 0.426038, mean_q: 4.180902
 16506/100000: episode: 218, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 61.560, mean reward: 2.280 [1.897, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.399], loss: 0.228956, mae: 0.428539, mean_q: 4.203603
 16515/100000: episode: 219, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 40.974, mean reward: 4.553 [4.092, 5.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.513, 10.100], loss: 0.282282, mae: 0.483509, mean_q: 4.317602
 16521/100000: episode: 220, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 10.873, mean reward: 1.812 [1.662, 2.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.100], loss: 0.287630, mae: 0.440444, mean_q: 4.096756
 16524/100000: episode: 221, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.058, mean reward: 2.686 [2.281, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.058, 10.151], loss: 0.285156, mae: 0.516967, mean_q: 4.313989
 16527/100000: episode: 222, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 5.848, mean reward: 1.949 [1.652, 2.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.135, 10.174], loss: 0.392714, mae: 0.543988, mean_q: 4.277984
 16536/100000: episode: 223, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 17.669, mean reward: 1.963 [1.599, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.095, 10.100], loss: 0.234662, mae: 0.393353, mean_q: 4.070915
 16563/100000: episode: 224, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 78.194, mean reward: 2.896 [1.954, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.851, 10.308], loss: 0.245328, mae: 0.393705, mean_q: 4.188797
 16566/100000: episode: 225, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 7.679, mean reward: 2.560 [2.095, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.149, 10.123], loss: 0.236626, mae: 0.451708, mean_q: 4.394559
 16572/100000: episode: 226, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 13.432, mean reward: 2.239 [1.853, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.617, 10.100], loss: 0.225402, mae: 0.414210, mean_q: 4.120487
 16578/100000: episode: 227, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 12.471, mean reward: 2.079 [1.696, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.300, 10.100], loss: 0.254825, mae: 0.471998, mean_q: 4.411669
 16583/100000: episode: 228, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 8.367, mean reward: 1.673 [1.579, 1.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.100], loss: 0.234009, mae: 0.470172, mean_q: 4.275239
 16610/100000: episode: 229, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 94.159, mean reward: 3.487 [2.680, 5.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.323, 10.477], loss: 0.185578, mae: 0.398074, mean_q: 4.173216
 16615/100000: episode: 230, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 8.966, mean reward: 1.793 [1.560, 2.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.110, 10.140], loss: 0.169609, mae: 0.342383, mean_q: 3.939938
 16645/100000: episode: 231, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 85.297, mean reward: 2.843 [1.964, 5.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.445, 10.460], loss: 0.188610, mae: 0.405446, mean_q: 4.262114
 16672/100000: episode: 232, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 57.789, mean reward: 2.140 [1.722, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.252], loss: 0.240861, mae: 0.419438, mean_q: 4.288016
 16678/100000: episode: 233, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 15.621, mean reward: 2.604 [2.344, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.292, 10.100], loss: 0.250851, mae: 0.460769, mean_q: 4.307358
 16681/100000: episode: 234, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 6.253, mean reward: 2.084 [1.966, 2.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.095, 10.187], loss: 0.214794, mae: 0.418272, mean_q: 4.207822
 16711/100000: episode: 235, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 66.640, mean reward: 2.221 [1.826, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.285], loss: 0.210154, mae: 0.391434, mean_q: 4.234345
 16716/100000: episode: 236, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 9.261, mean reward: 1.852 [1.480, 2.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.169, 10.152], loss: 0.175968, mae: 0.388676, mean_q: 4.117816
 16722/100000: episode: 237, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 11.341, mean reward: 1.890 [1.642, 2.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.154, 10.100], loss: 0.132823, mae: 0.371479, mean_q: 4.235158
 16730/100000: episode: 238, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 19.182, mean reward: 2.398 [2.059, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.275, 10.100], loss: 0.136605, mae: 0.371750, mean_q: 4.172553
 16739/100000: episode: 239, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 41.131, mean reward: 4.570 [3.756, 5.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.348, 10.100], loss: 0.344120, mae: 0.443438, mean_q: 4.188192
[Info] 2-TH LEVEL FOUND: 7.572166442871094, Considering 10/90 traces
 16742/100000: episode: 240, duration: 4.969s, episode steps: 3, steps per second: 1, episode reward: 5.862, mean reward: 1.954 [1.483, 2.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.092, 10.137], loss: 0.160340, mae: 0.385817, mean_q: 4.315030
 16750/100000: episode: 241, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 45.744, mean reward: 5.718 [4.469, 7.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.563, 10.100], loss: 0.233693, mae: 0.441937, mean_q: 4.345731
 16757/100000: episode: 242, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 63.096, mean reward: 9.014 [3.463, 29.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.438, 10.100], loss: 0.136292, mae: 0.367250, mean_q: 4.196312
 16765/100000: episode: 243, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 59.873, mean reward: 7.484 [4.070, 10.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.917, 10.100], loss: 0.263242, mae: 0.434190, mean_q: 4.378875
 16773/100000: episode: 244, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 43.660, mean reward: 5.458 [4.977, 6.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.373, 10.100], loss: 0.207971, mae: 0.416605, mean_q: 4.078430
 16781/100000: episode: 245, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 60.746, mean reward: 7.593 [4.602, 13.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.472, 10.100], loss: 0.207988, mae: 0.421158, mean_q: 4.268661
 16789/100000: episode: 246, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 43.673, mean reward: 5.459 [3.869, 6.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.817, 10.100], loss: 0.202051, mae: 0.444263, mean_q: 4.390806
 16807/100000: episode: 247, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 119.784, mean reward: 6.655 [4.625, 10.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.362, 10.567], loss: 0.199591, mae: 0.401491, mean_q: 4.155484
 16815/100000: episode: 248, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 41.483, mean reward: 5.185 [4.582, 5.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.524, 10.100], loss: 0.165427, mae: 0.397634, mean_q: 4.381831
 16823/100000: episode: 249, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 41.857, mean reward: 5.232 [4.702, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.465, 10.100], loss: 0.303344, mae: 0.499618, mean_q: 4.531233
 16831/100000: episode: 250, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 34.083, mean reward: 4.260 [3.441, 6.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.548, 10.100], loss: 0.302531, mae: 0.482251, mean_q: 4.257269
 16839/100000: episode: 251, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 32.205, mean reward: 4.026 [3.694, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.391, 10.100], loss: 0.175582, mae: 0.461627, mean_q: 4.578656
 16847/100000: episode: 252, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 58.612, mean reward: 7.326 [4.666, 13.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.623, 10.100], loss: 0.292288, mae: 0.494981, mean_q: 4.282493
 16855/100000: episode: 253, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 50.138, mean reward: 6.267 [4.771, 9.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.468, 10.100], loss: 0.183748, mae: 0.399050, mean_q: 4.147216
 16863/100000: episode: 254, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 54.964, mean reward: 6.870 [4.801, 9.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.530, 10.100], loss: 0.351323, mae: 0.549380, mean_q: 4.586351
 16871/100000: episode: 255, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 48.687, mean reward: 6.086 [4.778, 8.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.537, 10.100], loss: 0.452245, mae: 0.524884, mean_q: 4.322617
 16879/100000: episode: 256, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 119.907, mean reward: 14.988 [5.079, 51.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.634, 10.100], loss: 0.396229, mae: 0.518964, mean_q: 4.509472
 16887/100000: episode: 257, duration: 0.063s, episode steps: 8, steps per second: 128, episode reward: 45.807, mean reward: 5.726 [4.737, 7.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.506, 10.100], loss: 3.808395, mae: 0.621035, mean_q: 4.542162
 16895/100000: episode: 258, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 33.349, mean reward: 4.169 [3.257, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.409, 10.100], loss: 0.518208, mae: 0.646796, mean_q: 4.471807
 16903/100000: episode: 259, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 30.703, mean reward: 3.838 [3.379, 4.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.432, 10.100], loss: 0.277281, mae: 0.512352, mean_q: 4.247424
 16911/100000: episode: 260, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 51.126, mean reward: 6.391 [5.366, 8.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.514, 10.100], loss: 0.348937, mae: 0.544264, mean_q: 4.540308
[Info] FALSIFICATION!
 16919/100000: episode: 261, duration: 0.373s, episode steps: 8, steps per second: 21, episode reward: 1049.040, mean reward: 131.130 [5.416, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.547, 10.100], loss: 0.218399, mae: 0.414037, mean_q: 4.347982
 16927/100000: episode: 262, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 52.455, mean reward: 6.557 [4.551, 8.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.517, 10.100], loss: 0.328054, mae: 0.483981, mean_q: 4.501410
 16935/100000: episode: 263, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 38.012, mean reward: 4.752 [3.599, 6.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.381, 10.100], loss: 0.392908, mae: 0.433784, mean_q: 4.379766
 16943/100000: episode: 264, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 35.150, mean reward: 4.394 [3.415, 5.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.351, 10.100], loss: 4.000948, mae: 0.889071, mean_q: 4.868955
 16950/100000: episode: 265, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 33.631, mean reward: 4.804 [3.363, 8.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.579, 10.100], loss: 0.590035, mae: 0.795705, mean_q: 4.080163
 16968/100000: episode: 266, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 83.216, mean reward: 4.623 [3.292, 8.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.570, 10.450], loss: 0.875712, mae: 0.788129, mean_q: 4.713066
 16975/100000: episode: 267, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 23.098, mean reward: 3.300 [2.903, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.336, 10.100], loss: 0.542632, mae: 0.597085, mean_q: 4.760078
 16993/100000: episode: 268, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 97.659, mean reward: 5.426 [3.817, 7.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.190, 10.595], loss: 0.246194, mae: 0.426041, mean_q: 4.300444
 17001/100000: episode: 269, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 52.901, mean reward: 6.613 [4.973, 9.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.476, 10.100], loss: 1.532966, mae: 0.619044, mean_q: 4.871875
[Info] FALSIFICATION!
 17007/100000: episode: 270, duration: 0.191s, episode steps: 6, steps per second: 31, episode reward: 1022.220, mean reward: 170.370 [3.895, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.354, 10.098], loss: 0.198718, mae: 0.427377, mean_q: 4.508263
 17025/100000: episode: 271, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 149.033, mean reward: 8.280 [3.681, 14.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.481, 10.521], loss: 1.860665, mae: 0.613969, mean_q: 4.591829
 17033/100000: episode: 272, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 40.689, mean reward: 5.086 [4.240, 7.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.440, 10.100], loss: 0.449590, mae: 0.523555, mean_q: 4.542017
 17051/100000: episode: 273, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 99.193, mean reward: 5.511 [3.913, 16.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.521], loss: 0.479433, mae: 0.605377, mean_q: 4.665102
 17058/100000: episode: 274, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 27.579, mean reward: 3.940 [3.489, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.399, 10.100], loss: 2192.699707, mae: 5.841573, mean_q: 5.707163
 17066/100000: episode: 275, duration: 0.067s, episode steps: 8, steps per second: 119, episode reward: 72.266, mean reward: 9.033 [3.780, 19.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.440, 10.100], loss: 1900.629395, mae: 7.708679, mean_q: 8.539431
 17073/100000: episode: 276, duration: 0.053s, episode steps: 7, steps per second: 132, episode reward: 28.476, mean reward: 4.068 [3.430, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.352, 10.100], loss: 12.361864, mae: 3.503687, mean_q: 8.039172
 17081/100000: episode: 277, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 77.014, mean reward: 9.627 [5.450, 15.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.599, 10.100], loss: 4.454245, mae: 1.974001, mean_q: 6.320894
 17099/100000: episode: 278, duration: 0.098s, episode steps: 18, steps per second: 185, episode reward: 90.806, mean reward: 5.045 [3.725, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.515], loss: 1.723477, mae: 1.353224, mean_q: 4.580714
 17107/100000: episode: 279, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 38.185, mean reward: 4.773 [3.977, 5.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.776, 10.100], loss: 1.128504, mae: 0.905924, mean_q: 4.947803
 17115/100000: episode: 280, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 29.880, mean reward: 3.735 [3.200, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.662, 10.100], loss: 1.249689, mae: 1.080274, mean_q: 5.412900
 17123/100000: episode: 281, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 37.627, mean reward: 4.703 [4.023, 5.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.198, 10.100], loss: 1.849397, mae: 1.032850, mean_q: 4.850374
 17130/100000: episode: 282, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 59.771, mean reward: 8.539 [4.678, 13.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.560, 10.100], loss: 1.040408, mae: 0.861084, mean_q: 5.186150
 17137/100000: episode: 283, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 28.166, mean reward: 4.024 [3.442, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.436, 10.100], loss: 0.551596, mae: 0.646986, mean_q: 5.116012
 17145/100000: episode: 284, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 37.752, mean reward: 4.719 [3.745, 5.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.493, 10.100], loss: 0.892592, mae: 0.757466, mean_q: 5.044549
 17153/100000: episode: 285, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 40.915, mean reward: 5.114 [3.496, 6.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.431, 10.100], loss: 0.919174, mae: 0.763608, mean_q: 5.073263
 17161/100000: episode: 286, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 59.440, mean reward: 7.430 [4.975, 12.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.532, 10.100], loss: 1.125115, mae: 0.766152, mean_q: 5.066397
 17169/100000: episode: 287, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 39.596, mean reward: 4.950 [3.212, 10.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.561, 10.100], loss: 0.802416, mae: 0.725757, mean_q: 5.206868
 17187/100000: episode: 288, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 77.763, mean reward: 4.320 [3.168, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.094, 10.576], loss: 1.026728, mae: 0.788682, mean_q: 5.186975
 17195/100000: episode: 289, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 54.705, mean reward: 6.838 [4.194, 11.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.414, 10.100], loss: 0.834802, mae: 0.713753, mean_q: 4.956415
 17202/100000: episode: 290, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 40.354, mean reward: 5.765 [4.538, 6.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.381, 10.100], loss: 0.537555, mae: 0.596394, mean_q: 4.838841
 17210/100000: episode: 291, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 72.260, mean reward: 9.033 [5.445, 16.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.421, 10.100], loss: 1.000730, mae: 0.743170, mean_q: 5.163867
 17218/100000: episode: 292, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 75.619, mean reward: 9.452 [5.106, 15.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.532, 10.100], loss: 1914.947388, mae: 4.674478, mean_q: 5.313539
 17226/100000: episode: 293, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 66.528, mean reward: 8.316 [5.227, 11.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.526, 10.100], loss: 2.217677, mae: 1.265466, mean_q: 5.889575
 17233/100000: episode: 294, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 24.736, mean reward: 3.534 [3.097, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.470, 10.100], loss: 2.454620, mae: 1.315937, mean_q: 6.005258
 17241/100000: episode: 295, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 37.635, mean reward: 4.704 [3.664, 6.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.357, 10.100], loss: 2.056750, mae: 1.169583, mean_q: 5.614760
 17249/100000: episode: 296, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 39.405, mean reward: 4.926 [3.588, 5.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.544, 10.100], loss: 1.236364, mae: 0.964169, mean_q: 5.314660
 17257/100000: episode: 297, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 45.499, mean reward: 5.687 [4.217, 8.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.414, 10.100], loss: 1.054025, mae: 0.874742, mean_q: 5.481843
 17275/100000: episode: 298, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 101.196, mean reward: 5.622 [3.557, 10.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.122, 10.503], loss: 1699.117920, mae: 5.146101, mean_q: 6.361500
 17283/100000: episode: 299, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 50.533, mean reward: 6.317 [3.803, 10.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.447, 10.100], loss: 10.935371, mae: 3.111042, mean_q: 8.079915
 17291/100000: episode: 300, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 40.658, mean reward: 5.082 [3.850, 7.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.448, 10.100], loss: 4.819088, mae: 1.955500, mean_q: 6.708528
 17309/100000: episode: 301, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 97.004, mean reward: 5.389 [3.819, 8.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.117, 10.522], loss: 3.192320, mae: 1.268886, mean_q: 5.474988
 17317/100000: episode: 302, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 46.065, mean reward: 5.758 [4.546, 8.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.446, 10.100], loss: 1911.816284, mae: 4.933844, mean_q: 4.980133
 17324/100000: episode: 303, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 33.901, mean reward: 4.843 [3.756, 6.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.536, 10.100], loss: 1.876545, mae: 1.126764, mean_q: 5.838623
 17332/100000: episode: 304, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 44.586, mean reward: 5.573 [3.867, 8.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.464, 10.100], loss: 6.393823, mae: 2.339294, mean_q: 7.435375
 17340/100000: episode: 305, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 50.603, mean reward: 6.325 [4.843, 8.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.596, 10.100], loss: 6.011900, mae: 2.216389, mean_q: 7.285963
 17348/100000: episode: 306, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 31.669, mean reward: 3.959 [3.142, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.411, 10.100], loss: 4.623310, mae: 1.736825, mean_q: 6.810790
 17356/100000: episode: 307, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 77.791, mean reward: 9.724 [4.702, 21.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.463, 10.100], loss: 1.335838, mae: 1.036873, mean_q: 5.701762
 17364/100000: episode: 308, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 31.219, mean reward: 3.902 [3.318, 4.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.456, 10.100], loss: 2.631878, mae: 1.301762, mean_q: 5.974941
 17372/100000: episode: 309, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 34.186, mean reward: 4.273 [3.055, 7.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.806, 10.100], loss: 1.554674, mae: 1.135949, mean_q: 5.586640
 17380/100000: episode: 310, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 35.518, mean reward: 4.440 [3.383, 6.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.456, 10.100], loss: 1.049831, mae: 0.942584, mean_q: 5.867610
 17388/100000: episode: 311, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 50.036, mean reward: 6.255 [3.882, 13.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.447, 10.100], loss: 2.249417, mae: 1.150498, mean_q: 6.028562
 17396/100000: episode: 312, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 66.372, mean reward: 8.296 [6.211, 15.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.002, 10.100], loss: 5.286690, mae: 1.812319, mean_q: 6.732344
 17404/100000: episode: 313, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 33.785, mean reward: 4.223 [3.065, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.353, 10.100], loss: 2.799902, mae: 1.362754, mean_q: 6.389368
 17411/100000: episode: 314, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 32.977, mean reward: 4.711 [3.488, 8.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.474, 10.100], loss: 1.864740, mae: 1.086557, mean_q: 6.037236
 17419/100000: episode: 315, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 45.279, mean reward: 5.660 [3.865, 7.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.358, 10.100], loss: 1901.722168, mae: 4.974094, mean_q: 6.149465
 17427/100000: episode: 316, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 40.450, mean reward: 5.056 [4.267, 6.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.645, 10.100], loss: 4.514467, mae: 1.843479, mean_q: 7.009021
 17434/100000: episode: 317, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 43.414, mean reward: 6.202 [3.873, 14.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.530, 10.100], loss: 2.971699, mae: 1.363983, mean_q: 6.783355
 17442/100000: episode: 318, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 44.623, mean reward: 5.578 [3.757, 7.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.473, 10.100], loss: 1.727736, mae: 0.979956, mean_q: 5.809710
 17450/100000: episode: 319, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 54.761, mean reward: 6.845 [4.805, 10.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.445, 10.100], loss: 1896.125977, mae: 5.006914, mean_q: 6.347846
 17458/100000: episode: 320, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 34.363, mean reward: 4.295 [3.670, 5.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.598, 10.100], loss: 2.958934, mae: 1.441315, mean_q: 6.981240
 17466/100000: episode: 321, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 36.893, mean reward: 4.612 [4.000, 6.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.180, 10.100], loss: 4.671346, mae: 1.383152, mean_q: 6.387084
 17474/100000: episode: 322, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 40.834, mean reward: 5.104 [3.635, 7.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.345, 10.100], loss: 2.210441, mae: 1.192915, mean_q: 6.294679
 17492/100000: episode: 323, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 81.038, mean reward: 4.502 [3.154, 5.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.473], loss: 1.350184, mae: 0.960182, mean_q: 6.013059
 17510/100000: episode: 324, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 80.026, mean reward: 4.446 [3.311, 6.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.587], loss: 1.764129, mae: 0.984884, mean_q: 5.651346
 17518/100000: episode: 325, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 200.301, mean reward: 25.038 [4.748, 59.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.518, 10.100], loss: 1.272844, mae: 0.878341, mean_q: 5.627600
 17526/100000: episode: 326, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 34.714, mean reward: 4.339 [3.441, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.480, 10.100], loss: 2.857454, mae: 1.321039, mean_q: 6.466777
 17534/100000: episode: 327, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 36.600, mean reward: 4.575 [3.419, 7.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.358, 10.100], loss: 3.486768, mae: 1.469034, mean_q: 6.635608
 17552/100000: episode: 328, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 97.907, mean reward: 5.439 [3.941, 8.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.013, 10.577], loss: 1.933445, mae: 1.153674, mean_q: 6.627747
 17560/100000: episode: 329, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 38.183, mean reward: 4.773 [3.651, 6.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.407, 10.100], loss: 3786.042969, mae: 8.808413, mean_q: 6.533902
[Info] Complete ISplit Iteration
[Info] Levels: [4.3586855, 7.5721664, 21.834726]
[Info] Cond. Prob: [0.1, 0.1, 0.33]
[Info] Error Prob: 0.003300000000000001

 17568/100000: episode: 330, duration: 4.769s, episode steps: 8, steps per second: 2, episode reward: 45.123, mean reward: 5.640 [4.363, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.755, 10.100], loss: 7.794672, mae: 2.552908, mean_q: 8.280643
 17668/100000: episode: 331, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 184.478, mean reward: 1.845 [1.461, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.436, 10.117], loss: 306.827972, mae: 2.260839, mean_q: 6.708464
 17768/100000: episode: 332, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 197.051, mean reward: 1.971 [1.500, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.231], loss: 459.961914, mae: 2.718362, mean_q: 7.037423
 17868/100000: episode: 333, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.854, mean reward: 1.979 [1.470, 5.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.238], loss: 305.719116, mae: 1.958107, mean_q: 6.696877
 17968/100000: episode: 334, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 176.693, mean reward: 1.767 [1.472, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.717, 10.187], loss: 153.434952, mae: 1.514422, mean_q: 6.232121
 18068/100000: episode: 335, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.885, mean reward: 1.979 [1.454, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.232], loss: 306.401276, mae: 1.907704, mean_q: 6.123199
 18168/100000: episode: 336, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 185.180, mean reward: 1.852 [1.466, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.848, 10.098], loss: 2.867886, mae: 1.201250, mean_q: 6.087795
 18268/100000: episode: 337, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 198.748, mean reward: 1.987 [1.601, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.651, 10.211], loss: 304.589172, mae: 1.892066, mean_q: 6.269423
 18368/100000: episode: 338, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 188.625, mean reward: 1.886 [1.456, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.143, 10.098], loss: 456.677612, mae: 2.357221, mean_q: 6.562516
 18468/100000: episode: 339, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.420, mean reward: 1.974 [1.489, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.907, 10.098], loss: 456.629852, mae: 2.558870, mean_q: 6.798332
 18568/100000: episode: 340, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 179.401, mean reward: 1.794 [1.474, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.930, 10.199], loss: 304.374512, mae: 2.055749, mean_q: 6.484676
 18668/100000: episode: 341, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 179.928, mean reward: 1.799 [1.441, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.581, 10.098], loss: 2.489346, mae: 1.102464, mean_q: 5.810916
 18768/100000: episode: 342, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 207.761, mean reward: 2.078 [1.517, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.046, 10.098], loss: 2.251296, mae: 1.041902, mean_q: 5.662274
 18868/100000: episode: 343, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 186.939, mean reward: 1.869 [1.454, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.691, 10.191], loss: 2.032971, mae: 0.921206, mean_q: 5.596884
 18968/100000: episode: 344, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 190.032, mean reward: 1.900 [1.465, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.100, 10.098], loss: 305.571869, mae: 2.102038, mean_q: 6.167712
 19068/100000: episode: 345, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.716, mean reward: 1.927 [1.470, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.855, 10.098], loss: 305.633820, mae: 1.928882, mean_q: 6.244130
 19168/100000: episode: 346, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 194.088, mean reward: 1.941 [1.467, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.366, 10.201], loss: 154.201889, mae: 1.525793, mean_q: 6.077551
 19268/100000: episode: 347, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 201.780, mean reward: 2.018 [1.461, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.675, 10.275], loss: 454.322723, mae: 2.353524, mean_q: 6.463779
 19368/100000: episode: 348, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 185.913, mean reward: 1.859 [1.469, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 3.455962, mae: 1.210438, mean_q: 5.998442
 19468/100000: episode: 349, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 188.932, mean reward: 1.889 [1.429, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.855, 10.098], loss: 303.950623, mae: 1.911264, mean_q: 6.071111
 19568/100000: episode: 350, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 213.120, mean reward: 2.131 [1.481, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.625, 10.098], loss: 152.952255, mae: 1.459459, mean_q: 5.876740
 19668/100000: episode: 351, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.472, mean reward: 2.005 [1.440, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.216, 10.098], loss: 304.570709, mae: 1.911329, mean_q: 6.069323
 19768/100000: episode: 352, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 207.838, mean reward: 2.078 [1.463, 4.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.753, 10.207], loss: 304.134186, mae: 2.265569, mean_q: 6.691017
 19868/100000: episode: 353, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 178.013, mean reward: 1.780 [1.452, 2.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.141], loss: 304.760620, mae: 2.272613, mean_q: 6.549946
 19968/100000: episode: 354, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 189.898, mean reward: 1.899 [1.446, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.915, 10.098], loss: 3.042606, mae: 1.120092, mean_q: 5.555026
 20068/100000: episode: 355, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 206.397, mean reward: 2.064 [1.463, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.410], loss: 303.273438, mae: 1.905980, mean_q: 5.803988
 20168/100000: episode: 356, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 208.595, mean reward: 2.086 [1.464, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.003, 10.098], loss: 154.924316, mae: 1.586713, mean_q: 5.915269
 20268/100000: episode: 357, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 175.181, mean reward: 1.752 [1.452, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.098], loss: 451.128906, mae: 2.513260, mean_q: 6.676848
 20368/100000: episode: 358, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 208.155, mean reward: 2.082 [1.452, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.345, 10.098], loss: 153.767487, mae: 1.620651, mean_q: 6.114166
 20468/100000: episode: 359, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 186.628, mean reward: 1.866 [1.487, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.179, 10.098], loss: 2.994817, mae: 1.165060, mean_q: 5.613797
 20568/100000: episode: 360, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 178.633, mean reward: 1.786 [1.446, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.592, 10.134], loss: 153.609802, mae: 1.589633, mean_q: 5.952421
 20668/100000: episode: 361, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.287, mean reward: 1.873 [1.443, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.173], loss: 3.074803, mae: 1.078563, mean_q: 5.417556
 20768/100000: episode: 362, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 190.326, mean reward: 1.903 [1.459, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.098], loss: 607.557373, mae: 3.191149, mean_q: 6.943585
 20868/100000: episode: 363, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 198.523, mean reward: 1.985 [1.445, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.175], loss: 3.747600, mae: 1.120490, mean_q: 5.617955
 20968/100000: episode: 364, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.123, mean reward: 2.001 [1.436, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.440, 10.098], loss: 453.114929, mae: 2.409289, mean_q: 6.407641
 21068/100000: episode: 365, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 190.733, mean reward: 1.907 [1.450, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.604, 10.218], loss: 153.648605, mae: 1.550013, mean_q: 5.869569
 21168/100000: episode: 366, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 224.710, mean reward: 2.247 [1.467, 5.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.651, 10.098], loss: 3.260323, mae: 1.064882, mean_q: 5.487490
 21268/100000: episode: 367, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 201.701, mean reward: 2.017 [1.575, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.726, 10.098], loss: 153.501480, mae: 1.414100, mean_q: 5.599446
 21368/100000: episode: 368, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.090, mean reward: 1.931 [1.480, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.335], loss: 448.956451, mae: 2.619860, mean_q: 6.552059
 21468/100000: episode: 369, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.428, mean reward: 1.844 [1.457, 2.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.635, 10.098], loss: 153.419617, mae: 1.578466, mean_q: 5.616985
 21568/100000: episode: 370, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 184.597, mean reward: 1.846 [1.442, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.098, 10.098], loss: 3.392159, mae: 1.046870, mean_q: 5.323666
 21668/100000: episode: 371, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 190.707, mean reward: 1.907 [1.433, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.098], loss: 3.521902, mae: 1.001243, mean_q: 5.196237
 21768/100000: episode: 372, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 189.015, mean reward: 1.890 [1.506, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.211, 10.098], loss: 151.741058, mae: 1.183255, mean_q: 4.981718
 21868/100000: episode: 373, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 233.572, mean reward: 2.336 [1.488, 6.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.391, 10.288], loss: 593.065063, mae: 2.831171, mean_q: 6.103815
 21968/100000: episode: 374, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 193.450, mean reward: 1.934 [1.469, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.874, 10.168], loss: 4.030700, mae: 1.012897, mean_q: 5.228946
 22068/100000: episode: 375, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 197.991, mean reward: 1.980 [1.471, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.788, 10.123], loss: 2.910164, mae: 0.772707, mean_q: 4.617320
 22168/100000: episode: 376, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 187.798, mean reward: 1.878 [1.481, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.715, 10.098], loss: 1.795389, mae: 0.668986, mean_q: 4.449241
 22268/100000: episode: 377, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 204.560, mean reward: 2.046 [1.466, 5.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.843, 10.231], loss: 1.158312, mae: 0.536388, mean_q: 4.231904
 22368/100000: episode: 378, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 191.264, mean reward: 1.913 [1.451, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.012, 10.098], loss: 1.847847, mae: 0.539828, mean_q: 4.148389
 22468/100000: episode: 379, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 194.878, mean reward: 1.949 [1.488, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.148, 10.148], loss: 1.184644, mae: 0.447626, mean_q: 4.019768
 22568/100000: episode: 380, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.039, mean reward: 2.000 [1.469, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.688, 10.347], loss: 0.152434, mae: 0.354461, mean_q: 3.885982
 22668/100000: episode: 381, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.154, mean reward: 1.982 [1.522, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.312, 10.265], loss: 0.171068, mae: 0.358088, mean_q: 3.889703
 22768/100000: episode: 382, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.231, mean reward: 1.862 [1.480, 2.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.872, 10.283], loss: 0.144156, mae: 0.343039, mean_q: 3.877918
 22868/100000: episode: 383, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 208.937, mean reward: 2.089 [1.495, 7.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.633, 10.234], loss: 0.130683, mae: 0.331143, mean_q: 3.868128
 22968/100000: episode: 384, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 203.880, mean reward: 2.039 [1.465, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.792, 10.098], loss: 0.139301, mae: 0.345705, mean_q: 3.878182
 23068/100000: episode: 385, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 199.556, mean reward: 1.996 [1.477, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.141], loss: 0.163414, mae: 0.353912, mean_q: 3.887033
 23168/100000: episode: 386, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 198.557, mean reward: 1.986 [1.463, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.098], loss: 0.146075, mae: 0.347153, mean_q: 3.881112
 23268/100000: episode: 387, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.381, mean reward: 2.074 [1.501, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.250, 10.098], loss: 0.151882, mae: 0.350587, mean_q: 3.883996
 23368/100000: episode: 388, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 205.357, mean reward: 2.054 [1.538, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.280, 10.311], loss: 0.131795, mae: 0.338335, mean_q: 3.877288
 23468/100000: episode: 389, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 204.499, mean reward: 2.045 [1.458, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.681, 10.451], loss: 0.131231, mae: 0.341024, mean_q: 3.888172
 23568/100000: episode: 390, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 180.328, mean reward: 1.803 [1.445, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.327, 10.098], loss: 0.126814, mae: 0.333255, mean_q: 3.889381
 23668/100000: episode: 391, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.230, mean reward: 1.962 [1.478, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.272, 10.298], loss: 0.127485, mae: 0.336537, mean_q: 3.898920
 23768/100000: episode: 392, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 201.695, mean reward: 2.017 [1.462, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.361, 10.098], loss: 0.118053, mae: 0.323378, mean_q: 3.874925
 23868/100000: episode: 393, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.542, mean reward: 1.935 [1.522, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.298, 10.162], loss: 0.137600, mae: 0.340914, mean_q: 3.900001
 23968/100000: episode: 394, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 242.780, mean reward: 2.428 [1.535, 4.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.098], loss: 0.126783, mae: 0.331867, mean_q: 3.897421
 24068/100000: episode: 395, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 190.209, mean reward: 1.902 [1.456, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.274, 10.223], loss: 0.156712, mae: 0.346304, mean_q: 3.922106
 24168/100000: episode: 396, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 206.617, mean reward: 2.066 [1.487, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.448], loss: 0.126262, mae: 0.334405, mean_q: 3.916584
 24268/100000: episode: 397, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 199.460, mean reward: 1.995 [1.454, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.838, 10.100], loss: 0.123873, mae: 0.332507, mean_q: 3.934919
 24368/100000: episode: 398, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.687, mean reward: 1.977 [1.445, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.905, 10.381], loss: 0.118684, mae: 0.323867, mean_q: 3.916104
 24468/100000: episode: 399, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 187.676, mean reward: 1.877 [1.448, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.232, 10.199], loss: 0.121422, mae: 0.339314, mean_q: 3.940401
 24568/100000: episode: 400, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.802, mean reward: 1.828 [1.451, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.785, 10.098], loss: 0.145370, mae: 0.350804, mean_q: 3.932311
 24668/100000: episode: 401, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 200.526, mean reward: 2.005 [1.472, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.812, 10.098], loss: 0.119046, mae: 0.337414, mean_q: 3.934172
 24768/100000: episode: 402, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 182.542, mean reward: 1.825 [1.434, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.904, 10.201], loss: 0.131227, mae: 0.343721, mean_q: 3.932674
 24868/100000: episode: 403, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 196.358, mean reward: 1.964 [1.449, 6.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.179], loss: 0.123112, mae: 0.325548, mean_q: 3.907766
 24968/100000: episode: 404, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.174, mean reward: 1.942 [1.526, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.996, 10.214], loss: 0.111512, mae: 0.319248, mean_q: 3.895656
 25068/100000: episode: 405, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.460, mean reward: 1.845 [1.473, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.772, 10.111], loss: 0.120272, mae: 0.324813, mean_q: 3.895604
 25168/100000: episode: 406, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 201.923, mean reward: 2.019 [1.438, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.098, 10.098], loss: 0.122210, mae: 0.323372, mean_q: 3.885394
 25268/100000: episode: 407, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 220.657, mean reward: 2.207 [1.480, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.375, 10.098], loss: 0.123856, mae: 0.334556, mean_q: 3.902667
 25368/100000: episode: 408, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 196.818, mean reward: 1.968 [1.496, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.307, 10.185], loss: 0.123868, mae: 0.333254, mean_q: 3.904822
 25468/100000: episode: 409, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 228.869, mean reward: 2.289 [1.455, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.562, 10.098], loss: 0.125015, mae: 0.331256, mean_q: 3.922800
 25568/100000: episode: 410, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 198.511, mean reward: 1.985 [1.472, 6.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.367, 10.098], loss: 0.119298, mae: 0.331491, mean_q: 3.902323
 25668/100000: episode: 411, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.764, mean reward: 1.988 [1.443, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.015, 10.405], loss: 0.123116, mae: 0.339498, mean_q: 3.931007
 25768/100000: episode: 412, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.060, mean reward: 2.031 [1.440, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.948, 10.190], loss: 0.117645, mae: 0.330182, mean_q: 3.925637
 25868/100000: episode: 413, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 191.200, mean reward: 1.912 [1.437, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.545, 10.221], loss: 0.115150, mae: 0.329444, mean_q: 3.929688
 25968/100000: episode: 414, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 213.531, mean reward: 2.135 [1.487, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.599, 10.461], loss: 0.130100, mae: 0.342488, mean_q: 3.947495
 26068/100000: episode: 415, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 183.657, mean reward: 1.837 [1.450, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.047, 10.105], loss: 0.124830, mae: 0.329721, mean_q: 3.936295
 26168/100000: episode: 416, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 187.498, mean reward: 1.875 [1.455, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.459, 10.247], loss: 0.124421, mae: 0.331964, mean_q: 3.937001
 26268/100000: episode: 417, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 194.046, mean reward: 1.940 [1.489, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.782, 10.102], loss: 0.133465, mae: 0.346139, mean_q: 3.953611
 26368/100000: episode: 418, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 178.567, mean reward: 1.786 [1.463, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.051, 10.098], loss: 0.126550, mae: 0.336880, mean_q: 3.934217
 26468/100000: episode: 419, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.779, mean reward: 1.908 [1.490, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.921, 10.182], loss: 0.115941, mae: 0.331654, mean_q: 3.924735
 26568/100000: episode: 420, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 179.393, mean reward: 1.794 [1.432, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.691, 10.098], loss: 0.111546, mae: 0.320478, mean_q: 3.932744
 26668/100000: episode: 421, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 206.871, mean reward: 2.069 [1.513, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.950, 10.365], loss: 0.126562, mae: 0.337362, mean_q: 3.938335
 26768/100000: episode: 422, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 198.810, mean reward: 1.988 [1.450, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.098], loss: 0.121071, mae: 0.329417, mean_q: 3.926987
 26868/100000: episode: 423, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 205.044, mean reward: 2.050 [1.491, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.548, 10.457], loss: 0.106417, mae: 0.322019, mean_q: 3.912261
 26968/100000: episode: 424, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.488, mean reward: 1.935 [1.476, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.538, 10.098], loss: 0.107206, mae: 0.321133, mean_q: 3.918683
 27068/100000: episode: 425, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 193.393, mean reward: 1.934 [1.456, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: 0.114828, mae: 0.326730, mean_q: 3.929976
 27168/100000: episode: 426, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.209, mean reward: 1.862 [1.484, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.357, 10.098], loss: 0.103241, mae: 0.309526, mean_q: 3.883307
 27268/100000: episode: 427, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 190.227, mean reward: 1.902 [1.445, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.215, 10.098], loss: 0.103814, mae: 0.314685, mean_q: 3.907170
 27368/100000: episode: 428, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 201.917, mean reward: 2.019 [1.459, 5.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.056, 10.285], loss: 0.108073, mae: 0.318866, mean_q: 3.910983
 27468/100000: episode: 429, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.352, mean reward: 1.874 [1.455, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.382, 10.182], loss: 0.114223, mae: 0.321123, mean_q: 3.903978
[Info] 1-TH LEVEL FOUND: 4.543605327606201, Considering 10/90 traces
 27568/100000: episode: 430, duration: 5.177s, episode steps: 100, steps per second: 19, episode reward: 197.082, mean reward: 1.971 [1.468, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.715, 10.098], loss: 0.110580, mae: 0.323738, mean_q: 3.909729
 27586/100000: episode: 431, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 35.069, mean reward: 1.948 [1.435, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.334, 10.151], loss: 0.104739, mae: 0.315422, mean_q: 3.911443
 27677/100000: episode: 432, duration: 0.488s, episode steps: 91, steps per second: 187, episode reward: 169.125, mean reward: 1.859 [1.445, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.145, 10.171], loss: 0.099655, mae: 0.305503, mean_q: 3.887861
 27700/100000: episode: 433, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 46.738, mean reward: 2.032 [1.566, 2.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.524, 10.100], loss: 0.117237, mae: 0.329447, mean_q: 3.886642
 27791/100000: episode: 434, duration: 0.462s, episode steps: 91, steps per second: 197, episode reward: 166.608, mean reward: 1.831 [1.449, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-1.320, 10.156], loss: 0.106973, mae: 0.320099, mean_q: 3.893533
 27815/100000: episode: 435, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 52.415, mean reward: 2.184 [1.739, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.867, 10.100], loss: 0.096000, mae: 0.305760, mean_q: 3.896503
 27839/100000: episode: 436, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 42.771, mean reward: 1.782 [1.436, 2.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.129, 10.100], loss: 0.096657, mae: 0.310526, mean_q: 3.915051
 27930/100000: episode: 437, duration: 0.459s, episode steps: 91, steps per second: 198, episode reward: 173.284, mean reward: 1.904 [1.476, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.636, 10.135], loss: 0.112231, mae: 0.322074, mean_q: 3.913075
 27948/100000: episode: 438, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 33.522, mean reward: 1.862 [1.556, 2.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.234, 10.254], loss: 0.084143, mae: 0.290961, mean_q: 3.868801
 27971/100000: episode: 439, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 49.008, mean reward: 2.131 [1.597, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.625, 10.164], loss: 0.089810, mae: 0.320782, mean_q: 3.866968
 27995/100000: episode: 440, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 53.639, mean reward: 2.235 [1.660, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.090, 10.100], loss: 0.116612, mae: 0.328778, mean_q: 3.904325
 28018/100000: episode: 441, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 59.351, mean reward: 2.580 [1.751, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.086, 10.444], loss: 0.117237, mae: 0.336511, mean_q: 3.921335
 28036/100000: episode: 442, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 34.446, mean reward: 1.914 [1.502, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.205, 10.333], loss: 0.103532, mae: 0.320554, mean_q: 3.878016
 28054/100000: episode: 443, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 37.960, mean reward: 2.109 [1.580, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.777, 10.143], loss: 0.091471, mae: 0.302620, mean_q: 3.870197
 28077/100000: episode: 444, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 51.021, mean reward: 2.218 [1.690, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.198, 10.100], loss: 0.096705, mae: 0.296908, mean_q: 3.853574
 28095/100000: episode: 445, duration: 0.116s, episode steps: 18, steps per second: 156, episode reward: 35.141, mean reward: 1.952 [1.729, 2.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.741, 10.295], loss: 0.096415, mae: 0.310877, mean_q: 3.872283
 28118/100000: episode: 446, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 47.308, mean reward: 2.057 [1.650, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.110, 10.100], loss: 0.157922, mae: 0.358048, mean_q: 3.875659
 28138/100000: episode: 447, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 68.720, mean reward: 3.436 [2.815, 5.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.469, 10.100], loss: 0.110673, mae: 0.327634, mean_q: 3.907867
 28164/100000: episode: 448, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 59.846, mean reward: 2.302 [1.658, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.083, 10.224], loss: 0.118990, mae: 0.329498, mean_q: 3.909314
 28169/100000: episode: 449, duration: 0.028s, episode steps: 5, steps per second: 175, episode reward: 12.508, mean reward: 2.502 [2.359, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.316, 10.100], loss: 0.104498, mae: 0.332695, mean_q: 3.946569
 28192/100000: episode: 450, duration: 0.143s, episode steps: 23, steps per second: 161, episode reward: 52.120, mean reward: 2.266 [1.712, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.485, 10.100], loss: 0.109040, mae: 0.324091, mean_q: 3.908323
 28216/100000: episode: 451, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 41.155, mean reward: 1.715 [1.477, 2.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.431, 10.100], loss: 0.104511, mae: 0.309998, mean_q: 3.905144
 28240/100000: episode: 452, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 42.585, mean reward: 1.774 [1.531, 2.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.358, 10.100], loss: 0.102668, mae: 0.320419, mean_q: 3.902652
 28264/100000: episode: 453, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 41.194, mean reward: 1.716 [1.490, 2.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.060, 10.154], loss: 0.104908, mae: 0.313415, mean_q: 3.890050
 28288/100000: episode: 454, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 58.126, mean reward: 2.422 [1.775, 4.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.609, 10.100], loss: 0.117163, mae: 0.320240, mean_q: 3.888959
 28308/100000: episode: 455, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 47.216, mean reward: 2.361 [2.142, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.154, 10.100], loss: 0.088976, mae: 0.284298, mean_q: 3.894161
 28399/100000: episode: 456, duration: 0.499s, episode steps: 91, steps per second: 182, episode reward: 169.983, mean reward: 1.868 [1.500, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-0.791, 10.100], loss: 0.111083, mae: 0.322322, mean_q: 3.901643
 28422/100000: episode: 457, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 46.997, mean reward: 2.043 [1.643, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.165, 10.157], loss: 0.101910, mae: 0.311461, mean_q: 3.907996
 28442/100000: episode: 458, duration: 0.144s, episode steps: 20, steps per second: 139, episode reward: 52.332, mean reward: 2.617 [1.813, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.580, 10.100], loss: 0.074393, mae: 0.280606, mean_q: 3.867732
 28466/100000: episode: 459, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 44.895, mean reward: 1.871 [1.470, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.171, 10.100], loss: 0.108363, mae: 0.320714, mean_q: 3.877657
 28486/100000: episode: 460, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 50.794, mean reward: 2.540 [1.987, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.305, 10.100], loss: 0.103758, mae: 0.319174, mean_q: 3.875571
 28504/100000: episode: 461, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 35.477, mean reward: 1.971 [1.547, 2.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.135, 10.202], loss: 0.115368, mae: 0.321592, mean_q: 3.880921
 28528/100000: episode: 462, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 44.178, mean reward: 1.841 [1.573, 2.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.226, 10.100], loss: 0.086390, mae: 0.307515, mean_q: 3.878371
 28552/100000: episode: 463, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 44.044, mean reward: 1.835 [1.525, 2.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.211, 10.100], loss: 0.112240, mae: 0.322662, mean_q: 3.895412
 28576/100000: episode: 464, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 44.343, mean reward: 1.848 [1.548, 2.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.486, 10.100], loss: 0.109886, mae: 0.318407, mean_q: 3.907243
 28594/100000: episode: 465, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 37.830, mean reward: 2.102 [1.728, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.745, 10.189], loss: 0.107461, mae: 0.333106, mean_q: 3.877492
 28620/100000: episode: 466, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 55.688, mean reward: 2.142 [1.570, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.197], loss: 0.161760, mae: 0.371716, mean_q: 3.963153
 28638/100000: episode: 467, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 46.731, mean reward: 2.596 [2.191, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.509], loss: 0.088699, mae: 0.305310, mean_q: 3.927276
 28662/100000: episode: 468, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 43.620, mean reward: 1.817 [1.443, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.465, 10.100], loss: 0.110176, mae: 0.326906, mean_q: 3.932662
 28685/100000: episode: 469, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 47.787, mean reward: 2.078 [1.524, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.041, 10.100], loss: 0.100170, mae: 0.315685, mean_q: 3.904527
 28709/100000: episode: 470, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 59.976, mean reward: 2.499 [1.918, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.574, 10.100], loss: 0.113700, mae: 0.314050, mean_q: 3.903255
 28714/100000: episode: 471, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 16.342, mean reward: 3.268 [2.880, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.340, 10.100], loss: 0.095538, mae: 0.322200, mean_q: 3.910960
 28732/100000: episode: 472, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 35.163, mean reward: 1.954 [1.508, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.060, 10.203], loss: 0.088906, mae: 0.310109, mean_q: 3.900654
 28823/100000: episode: 473, duration: 0.555s, episode steps: 91, steps per second: 164, episode reward: 168.893, mean reward: 1.856 [1.455, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.932, 10.100], loss: 0.103987, mae: 0.315443, mean_q: 3.904853
 28841/100000: episode: 474, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 41.855, mean reward: 2.325 [1.802, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.231, 10.319], loss: 0.090913, mae: 0.294689, mean_q: 3.858940
 28859/100000: episode: 475, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 35.674, mean reward: 1.982 [1.497, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.131, 10.116], loss: 0.105670, mae: 0.315737, mean_q: 3.868089
 28883/100000: episode: 476, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 51.314, mean reward: 2.138 [1.530, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-2.375, 10.100], loss: 0.089939, mae: 0.294082, mean_q: 3.903440
 28909/100000: episode: 477, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 55.190, mean reward: 2.123 [1.762, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.347, 10.221], loss: 0.128238, mae: 0.344392, mean_q: 3.935017
 29000/100000: episode: 478, duration: 0.590s, episode steps: 91, steps per second: 154, episode reward: 179.648, mean reward: 1.974 [1.506, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.157, 10.111], loss: 0.098111, mae: 0.316892, mean_q: 3.867982
 29023/100000: episode: 479, duration: 0.149s, episode steps: 23, steps per second: 154, episode reward: 50.905, mean reward: 2.213 [1.761, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.640, 10.100], loss: 0.104703, mae: 0.309988, mean_q: 3.846760
 29047/100000: episode: 480, duration: 0.162s, episode steps: 24, steps per second: 149, episode reward: 42.825, mean reward: 1.784 [1.449, 2.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.253, 10.228], loss: 0.086244, mae: 0.289146, mean_q: 3.882334
 29065/100000: episode: 481, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 33.657, mean reward: 1.870 [1.477, 2.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.686, 10.165], loss: 0.099053, mae: 0.298517, mean_q: 3.852069
 29089/100000: episode: 482, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 41.669, mean reward: 1.736 [1.484, 2.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.288, 10.100], loss: 0.107698, mae: 0.317451, mean_q: 3.897683
 29180/100000: episode: 483, duration: 0.502s, episode steps: 91, steps per second: 181, episode reward: 170.794, mean reward: 1.877 [1.462, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.241, 10.260], loss: 0.101298, mae: 0.319244, mean_q: 3.866997
 29198/100000: episode: 484, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 41.105, mean reward: 2.284 [1.789, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.108, 10.264], loss: 0.097982, mae: 0.312455, mean_q: 3.816231
 29289/100000: episode: 485, duration: 0.547s, episode steps: 91, steps per second: 166, episode reward: 221.091, mean reward: 2.430 [1.444, 5.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.729, 10.486], loss: 0.098793, mae: 0.303458, mean_q: 3.865504
 29309/100000: episode: 486, duration: 0.152s, episode steps: 20, steps per second: 132, episode reward: 52.116, mean reward: 2.606 [2.316, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.398, 10.100], loss: 0.108294, mae: 0.310237, mean_q: 3.911321
 29400/100000: episode: 487, duration: 0.513s, episode steps: 91, steps per second: 177, episode reward: 201.859, mean reward: 2.218 [1.513, 6.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-1.210, 10.100], loss: 0.101900, mae: 0.307918, mean_q: 3.878490
 29423/100000: episode: 488, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 57.963, mean reward: 2.520 [1.762, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-2.872, 10.100], loss: 0.108658, mae: 0.328044, mean_q: 3.953462
 29449/100000: episode: 489, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 72.941, mean reward: 2.805 [2.095, 4.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.232, 10.537], loss: 0.098286, mae: 0.317366, mean_q: 3.905782
 29469/100000: episode: 490, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 56.370, mean reward: 2.819 [2.308, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.258, 10.100], loss: 0.106196, mae: 0.314511, mean_q: 3.923534
 29487/100000: episode: 491, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 37.304, mean reward: 2.072 [1.523, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.095, 10.124], loss: 0.101254, mae: 0.326203, mean_q: 3.916942
 29510/100000: episode: 492, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 50.626, mean reward: 2.201 [1.829, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.517, 10.100], loss: 0.108746, mae: 0.313956, mean_q: 3.925659
 29534/100000: episode: 493, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 62.749, mean reward: 2.615 [2.070, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.589, 10.100], loss: 0.089804, mae: 0.309902, mean_q: 3.956376
 29552/100000: episode: 494, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 48.817, mean reward: 2.712 [2.136, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.214, 10.419], loss: 0.156836, mae: 0.348140, mean_q: 3.923635
 29570/100000: episode: 495, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 36.086, mean reward: 2.005 [1.667, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.063, 10.227], loss: 0.118165, mae: 0.332109, mean_q: 3.941989
 29661/100000: episode: 496, duration: 0.446s, episode steps: 91, steps per second: 204, episode reward: 170.137, mean reward: 1.870 [1.487, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.951, 10.161], loss: 0.112855, mae: 0.333747, mean_q: 3.952657
 29679/100000: episode: 497, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 43.969, mean reward: 2.443 [1.995, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.290], loss: 0.103388, mae: 0.312236, mean_q: 3.941775
 29770/100000: episode: 498, duration: 0.548s, episode steps: 91, steps per second: 166, episode reward: 171.090, mean reward: 1.880 [1.478, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.482, 10.301], loss: 0.098235, mae: 0.320496, mean_q: 3.927782
 29788/100000: episode: 499, duration: 0.130s, episode steps: 18, steps per second: 138, episode reward: 34.914, mean reward: 1.940 [1.597, 2.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.424, 10.229], loss: 0.077188, mae: 0.289743, mean_q: 3.880638
 29812/100000: episode: 500, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 44.115, mean reward: 1.838 [1.463, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.714, 10.116], loss: 0.127556, mae: 0.328451, mean_q: 3.947214
 29832/100000: episode: 501, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 73.037, mean reward: 3.652 [2.213, 16.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.550, 10.100], loss: 0.089793, mae: 0.301812, mean_q: 3.912980
 29856/100000: episode: 502, duration: 0.178s, episode steps: 24, steps per second: 134, episode reward: 44.606, mean reward: 1.859 [1.523, 2.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.149, 10.100], loss: 0.110431, mae: 0.330968, mean_q: 3.946633
 29876/100000: episode: 503, duration: 0.153s, episode steps: 20, steps per second: 131, episode reward: 47.624, mean reward: 2.381 [1.555, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.773, 10.100], loss: 0.129186, mae: 0.344835, mean_q: 3.982579
 29894/100000: episode: 504, duration: 0.123s, episode steps: 18, steps per second: 146, episode reward: 36.558, mean reward: 2.031 [1.746, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.339, 10.256], loss: 0.117138, mae: 0.336980, mean_q: 3.921556
 29917/100000: episode: 505, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 56.162, mean reward: 2.442 [1.780, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.693, 10.213], loss: 0.115978, mae: 0.345167, mean_q: 3.946813
 29940/100000: episode: 506, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 47.353, mean reward: 2.059 [1.762, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.341, 10.207], loss: 0.100685, mae: 0.328502, mean_q: 3.999425
 29964/100000: episode: 507, duration: 0.168s, episode steps: 24, steps per second: 143, episode reward: 47.647, mean reward: 1.985 [1.538, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.076, 10.210], loss: 0.244428, mae: 0.339930, mean_q: 3.973590
 30055/100000: episode: 508, duration: 0.598s, episode steps: 91, steps per second: 152, episode reward: 166.167, mean reward: 1.826 [1.455, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.930, 10.100], loss: 0.125782, mae: 0.343482, mean_q: 3.958251
 30146/100000: episode: 509, duration: 0.521s, episode steps: 91, steps per second: 175, episode reward: 192.583, mean reward: 2.116 [1.500, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.762, 10.100], loss: 0.150591, mae: 0.342191, mean_q: 3.948536
 30151/100000: episode: 510, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 15.464, mean reward: 3.093 [2.908, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.322, 10.100], loss: 0.144920, mae: 0.381531, mean_q: 4.097214
 30171/100000: episode: 511, duration: 0.155s, episode steps: 20, steps per second: 129, episode reward: 49.522, mean reward: 2.476 [1.760, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.116, 10.100], loss: 0.119525, mae: 0.330567, mean_q: 3.925142
 30189/100000: episode: 512, duration: 0.138s, episode steps: 18, steps per second: 130, episode reward: 43.531, mean reward: 2.418 [1.601, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.458, 10.312], loss: 0.292461, mae: 0.365636, mean_q: 3.969655
 30194/100000: episode: 513, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 16.645, mean reward: 3.329 [2.871, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.322, 10.100], loss: 0.138257, mae: 0.350768, mean_q: 3.912507
 30212/100000: episode: 514, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 36.972, mean reward: 2.054 [1.601, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.443, 10.368], loss: 0.095348, mae: 0.321985, mean_q: 3.960998
 30230/100000: episode: 515, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 35.717, mean reward: 1.984 [1.522, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.250, 10.138], loss: 0.297790, mae: 0.344718, mean_q: 3.948400
 30235/100000: episode: 516, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 14.405, mean reward: 2.881 [2.414, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.241, 10.100], loss: 0.111224, mae: 0.328387, mean_q: 3.863487
 30253/100000: episode: 517, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 37.613, mean reward: 2.090 [1.626, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.294], loss: 0.098564, mae: 0.311318, mean_q: 3.932675
 30277/100000: episode: 518, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 43.357, mean reward: 1.807 [1.538, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.791, 10.231], loss: 0.111428, mae: 0.316557, mean_q: 3.945535
 30282/100000: episode: 519, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 17.154, mean reward: 3.431 [2.355, 4.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.238, 10.100], loss: 0.124019, mae: 0.335537, mean_q: 3.856574
[Info] 2-TH LEVEL FOUND: 5.415768623352051, Considering 10/90 traces
 30305/100000: episode: 520, duration: 4.278s, episode steps: 23, steps per second: 5, episode reward: 63.486, mean reward: 2.760 [2.054, 4.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.958, 10.100], loss: 0.108152, mae: 0.329192, mean_q: 3.954098
 30319/100000: episode: 521, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 36.127, mean reward: 2.581 [2.075, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.376, 10.100], loss: 0.122808, mae: 0.347327, mean_q: 4.016761
 30332/100000: episode: 522, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 39.513, mean reward: 3.039 [2.636, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.526, 10.100], loss: 0.080981, mae: 0.294135, mean_q: 3.970847
 30346/100000: episode: 523, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 39.533, mean reward: 2.824 [2.280, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.405, 10.100], loss: 0.156718, mae: 0.345046, mean_q: 3.975285
 30355/100000: episode: 524, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 29.110, mean reward: 3.234 [2.145, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.426], loss: 0.106933, mae: 0.341119, mean_q: 3.983238
 30369/100000: episode: 525, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 51.716, mean reward: 3.694 [2.577, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.479, 10.100], loss: 0.096830, mae: 0.308206, mean_q: 3.974949
 30384/100000: episode: 526, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 46.631, mean reward: 3.109 [2.079, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.378], loss: 0.150342, mae: 0.350134, mean_q: 4.013286
 30399/100000: episode: 527, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 55.790, mean reward: 3.719 [3.063, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.472], loss: 0.110035, mae: 0.338040, mean_q: 3.948270
 30413/100000: episode: 528, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 45.077, mean reward: 3.220 [2.089, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.372, 10.100], loss: 0.105549, mae: 0.342060, mean_q: 3.979181
 30430/100000: episode: 529, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 59.098, mean reward: 3.476 [2.805, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.432, 10.100], loss: 0.112271, mae: 0.332752, mean_q: 3.956463
 30447/100000: episode: 530, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 47.532, mean reward: 2.796 [2.429, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.901, 10.100], loss: 0.115052, mae: 0.327083, mean_q: 4.058261
 30462/100000: episode: 531, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 42.852, mean reward: 2.857 [2.193, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.420], loss: 0.114001, mae: 0.330426, mean_q: 3.997279
 30476/100000: episode: 532, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 33.348, mean reward: 2.382 [2.088, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.518, 10.100], loss: 0.116491, mae: 0.326481, mean_q: 3.985289
 30489/100000: episode: 533, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 43.000, mean reward: 3.308 [2.547, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.342, 10.100], loss: 0.109388, mae: 0.337222, mean_q: 4.058482
 30498/100000: episode: 534, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 29.194, mean reward: 3.244 [2.208, 4.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.282, 10.433], loss: 0.089965, mae: 0.296366, mean_q: 3.947319
 30512/100000: episode: 535, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 42.653, mean reward: 3.047 [2.047, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.314, 10.100], loss: 0.112085, mae: 0.337623, mean_q: 4.008302
 30525/100000: episode: 536, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 33.674, mean reward: 2.590 [1.807, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.315, 10.100], loss: 0.117451, mae: 0.318912, mean_q: 4.027708
 30534/100000: episode: 537, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 30.083, mean reward: 3.343 [2.567, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.539, 10.445], loss: 0.137704, mae: 0.346225, mean_q: 3.968903
 30548/100000: episode: 538, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 43.424, mean reward: 3.102 [2.590, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.319, 10.100], loss: 0.569368, mae: 0.436201, mean_q: 4.115469
 30565/100000: episode: 539, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 57.452, mean reward: 3.380 [2.275, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.479, 10.100], loss: 0.116512, mae: 0.344578, mean_q: 3.978575
 30580/100000: episode: 540, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 52.720, mean reward: 3.515 [2.364, 6.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.394], loss: 0.091801, mae: 0.313512, mean_q: 4.045542
 30594/100000: episode: 541, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 30.187, mean reward: 2.156 [1.907, 2.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.124, 10.100], loss: 0.093645, mae: 0.326634, mean_q: 4.069125
 30609/100000: episode: 542, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 46.252, mean reward: 3.083 [2.511, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.409], loss: 0.134348, mae: 0.354422, mean_q: 4.099443
 30626/100000: episode: 543, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 66.643, mean reward: 3.920 [2.953, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.529, 10.100], loss: 0.114512, mae: 0.345147, mean_q: 4.055410
 30639/100000: episode: 544, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 35.960, mean reward: 2.766 [2.119, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.234, 10.500], loss: 0.116508, mae: 0.343340, mean_q: 4.105594
 30648/100000: episode: 545, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 21.628, mean reward: 2.403 [2.154, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.359], loss: 0.124233, mae: 0.345472, mean_q: 4.094471
 30662/100000: episode: 546, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 59.084, mean reward: 4.220 [2.202, 7.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.406, 10.100], loss: 0.123334, mae: 0.345883, mean_q: 4.029470
 30679/100000: episode: 547, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 113.667, mean reward: 6.686 [2.154, 62.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.563, 10.100], loss: 0.128754, mae: 0.358342, mean_q: 4.119121
 30696/100000: episode: 548, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 46.260, mean reward: 2.721 [2.224, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.834, 10.100], loss: 3.399338, mae: 0.481402, mean_q: 4.087409
 30711/100000: episode: 549, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 62.191, mean reward: 4.146 [2.628, 6.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.331, 10.419], loss: 0.375074, mae: 0.611282, mean_q: 4.038700
 30725/100000: episode: 550, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 45.245, mean reward: 3.232 [2.306, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.375, 10.100], loss: 0.438040, mae: 0.644316, mean_q: 4.280568
 30738/100000: episode: 551, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 35.183, mean reward: 2.706 [2.074, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.510], loss: 0.296960, mae: 0.535502, mean_q: 4.173064
 30755/100000: episode: 552, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 47.767, mean reward: 2.810 [1.937, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.118, 10.100], loss: 0.177365, mae: 0.411034, mean_q: 4.062265
 30768/100000: episode: 553, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 41.243, mean reward: 3.173 [2.434, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.363], loss: 0.444503, mae: 0.438322, mean_q: 4.207580
 30785/100000: episode: 554, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 33.037, mean reward: 1.943 [1.608, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.510, 10.100], loss: 3.350423, mae: 0.468200, mean_q: 4.077537
 30799/100000: episode: 555, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 58.225, mean reward: 4.159 [2.649, 5.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.776, 10.100], loss: 0.233164, mae: 0.488041, mean_q: 4.237400
 30813/100000: episode: 556, duration: 0.085s, episode steps: 14, steps per second: 166, episode reward: 35.040, mean reward: 2.503 [1.692, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.191, 10.100], loss: 0.157613, mae: 0.382660, mean_q: 4.084249
 30827/100000: episode: 557, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 41.398, mean reward: 2.957 [2.060, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.173, 10.100], loss: 4.135442, mae: 0.690116, mean_q: 4.384652
 30840/100000: episode: 558, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 44.465, mean reward: 3.420 [2.594, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.405, 10.100], loss: 0.216121, mae: 0.453077, mean_q: 4.050700
 30853/100000: episode: 559, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 58.324, mean reward: 4.486 [1.995, 24.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.360, 10.100], loss: 0.171443, mae: 0.397875, mean_q: 4.149365
 30870/100000: episode: 560, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 40.147, mean reward: 2.362 [1.897, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.199, 10.100], loss: 0.166052, mae: 0.392472, mean_q: 4.129693
 30884/100000: episode: 561, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 36.750, mean reward: 2.625 [2.265, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.586, 10.100], loss: 0.177740, mae: 0.398967, mean_q: 4.200739
 30901/100000: episode: 562, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 46.277, mean reward: 2.722 [1.898, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.286, 10.100], loss: 0.164372, mae: 0.378141, mean_q: 4.182671
 30916/100000: episode: 563, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 43.484, mean reward: 2.899 [2.065, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.810, 10.364], loss: 3.916737, mae: 0.506795, mean_q: 4.188869
 30930/100000: episode: 564, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 36.388, mean reward: 2.599 [2.131, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.884, 10.100], loss: 0.332700, mae: 0.544088, mean_q: 4.322379
 30944/100000: episode: 565, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 43.013, mean reward: 3.072 [2.485, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.437, 10.100], loss: 0.172152, mae: 0.393392, mean_q: 4.190726
 30958/100000: episode: 566, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 36.332, mean reward: 2.595 [1.841, 4.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.227, 10.100], loss: 0.228165, mae: 0.416912, mean_q: 4.141755
 30975/100000: episode: 567, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 64.685, mean reward: 3.805 [2.216, 7.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.376, 10.100], loss: 3.473299, mae: 0.628579, mean_q: 4.297453
 30992/100000: episode: 568, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 60.845, mean reward: 3.579 [2.762, 6.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.310, 10.100], loss: 0.197596, mae: 0.434566, mean_q: 4.222016
 31009/100000: episode: 569, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 38.676, mean reward: 2.275 [1.854, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.154, 10.100], loss: 0.656418, mae: 0.457768, mean_q: 4.221270
 31026/100000: episode: 570, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 63.726, mean reward: 3.749 [2.775, 5.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.420, 10.100], loss: 0.649165, mae: 0.472449, mean_q: 4.197204
 31043/100000: episode: 571, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 48.026, mean reward: 2.825 [2.248, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.257, 10.100], loss: 0.166756, mae: 0.394663, mean_q: 4.224345
 31060/100000: episode: 572, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 43.293, mean reward: 2.547 [1.812, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.507, 10.100], loss: 0.189153, mae: 0.389291, mean_q: 4.191730
 31074/100000: episode: 573, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 37.315, mean reward: 2.665 [1.960, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.284, 10.100], loss: 0.178002, mae: 0.393335, mean_q: 4.249615
 31087/100000: episode: 574, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 35.409, mean reward: 2.724 [2.124, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.335, 10.100], loss: 0.175322, mae: 0.384535, mean_q: 4.200819
 31104/100000: episode: 575, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 38.951, mean reward: 2.291 [2.008, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.370, 10.100], loss: 0.171085, mae: 0.389210, mean_q: 4.235553
 31117/100000: episode: 576, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 39.061, mean reward: 3.005 [2.379, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.147, 10.100], loss: 0.157806, mae: 0.383516, mean_q: 4.170188
 31130/100000: episode: 577, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 46.943, mean reward: 3.611 [2.527, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.450, 10.100], loss: 0.773566, mae: 0.472893, mean_q: 4.320936
 31147/100000: episode: 578, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 55.836, mean reward: 3.284 [2.444, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.317, 10.100], loss: 0.380068, mae: 0.459476, mean_q: 4.370329
 31161/100000: episode: 579, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 37.170, mean reward: 2.655 [2.048, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.436, 10.100], loss: 0.456413, mae: 0.484563, mean_q: 4.397662
 31174/100000: episode: 580, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 30.136, mean reward: 2.318 [2.011, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.303], loss: 0.229171, mae: 0.430479, mean_q: 4.287039
 31191/100000: episode: 581, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 58.147, mean reward: 3.420 [2.500, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.446, 10.100], loss: 0.165031, mae: 0.386871, mean_q: 4.337750
 31204/100000: episode: 582, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 36.369, mean reward: 2.798 [2.342, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.187, 10.382], loss: 0.217998, mae: 0.411103, mean_q: 4.197980
 31221/100000: episode: 583, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 42.858, mean reward: 2.521 [2.183, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.275, 10.100], loss: 0.125771, mae: 0.341924, mean_q: 4.280979
 31235/100000: episode: 584, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 37.696, mean reward: 2.693 [2.201, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.290, 10.100], loss: 0.203674, mae: 0.411171, mean_q: 4.351832
 31249/100000: episode: 585, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 38.722, mean reward: 2.766 [2.284, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.178, 10.100], loss: 0.420818, mae: 0.430996, mean_q: 4.295346
 31266/100000: episode: 586, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 47.924, mean reward: 2.819 [2.317, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.460, 10.100], loss: 0.137576, mae: 0.367594, mean_q: 4.336077
 31279/100000: episode: 587, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 42.545, mean reward: 3.273 [2.637, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.279, 10.100], loss: 0.440115, mae: 0.456509, mean_q: 4.418161
 31294/100000: episode: 588, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 47.199, mean reward: 3.147 [2.198, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.441], loss: 3.934390, mae: 0.619698, mean_q: 4.550478
 31311/100000: episode: 589, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 64.331, mean reward: 3.784 [2.551, 5.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.226, 10.100], loss: 0.209375, mae: 0.450895, mean_q: 4.206455
 31324/100000: episode: 590, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 40.710, mean reward: 3.132 [2.461, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.554], loss: 0.190772, mae: 0.418016, mean_q: 4.287783
 31338/100000: episode: 591, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 32.870, mean reward: 2.348 [2.083, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.265, 10.100], loss: 0.195677, mae: 0.410372, mean_q: 4.325369
 31355/100000: episode: 592, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 45.840, mean reward: 2.696 [2.226, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.817, 10.100], loss: 0.170136, mae: 0.403337, mean_q: 4.305836
 31372/100000: episode: 593, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 59.660, mean reward: 3.509 [2.519, 6.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.248, 10.100], loss: 0.177428, mae: 0.404968, mean_q: 4.360297
 31385/100000: episode: 594, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 41.288, mean reward: 3.176 [2.473, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.176, 10.490], loss: 0.231155, mae: 0.435403, mean_q: 4.315337
 31402/100000: episode: 595, duration: 0.083s, episode steps: 17, steps per second: 206, episode reward: 45.732, mean reward: 2.690 [2.122, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.260, 10.100], loss: 0.186712, mae: 0.411830, mean_q: 4.372074
 31411/100000: episode: 596, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 27.126, mean reward: 3.014 [2.505, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.053, 10.431], loss: 0.183085, mae: 0.406257, mean_q: 4.395175
 31428/100000: episode: 597, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 53.529, mean reward: 3.149 [2.514, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.231, 10.100], loss: 3.455137, mae: 0.663832, mean_q: 4.635101
 31445/100000: episode: 598, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 47.730, mean reward: 2.808 [2.241, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.156, 10.100], loss: 0.230757, mae: 0.458333, mean_q: 4.381891
 31462/100000: episode: 599, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 74.838, mean reward: 4.402 [2.511, 8.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.512, 10.100], loss: 0.276287, mae: 0.465735, mean_q: 4.512486
 31479/100000: episode: 600, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 54.575, mean reward: 3.210 [2.609, 4.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.387, 10.100], loss: 0.256021, mae: 0.481438, mean_q: 4.416411
 31496/100000: episode: 601, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 40.649, mean reward: 2.391 [1.909, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.510, 10.100], loss: 3.475886, mae: 0.590852, mean_q: 4.461719
 31509/100000: episode: 602, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 43.612, mean reward: 3.355 [2.533, 5.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.942, 10.100], loss: 0.306306, mae: 0.526816, mean_q: 4.462559
 31524/100000: episode: 603, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 40.454, mean reward: 2.697 [2.248, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.383], loss: 0.211032, mae: 0.444911, mean_q: 4.507586
 31538/100000: episode: 604, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 43.780, mean reward: 3.127 [2.560, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.385, 10.100], loss: 0.200024, mae: 0.416901, mean_q: 4.412432
 31555/100000: episode: 605, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 49.231, mean reward: 2.896 [2.299, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.654, 10.100], loss: 0.766647, mae: 0.569265, mean_q: 4.622091
 31569/100000: episode: 606, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 38.014, mean reward: 2.715 [2.192, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.328, 10.100], loss: 0.462622, mae: 0.499204, mean_q: 4.525617
 31582/100000: episode: 607, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 28.790, mean reward: 2.215 [1.914, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.322, 10.100], loss: 0.762730, mae: 0.465231, mean_q: 4.490493
 31595/100000: episode: 608, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 34.690, mean reward: 2.668 [2.220, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.372], loss: 0.240355, mae: 0.451553, mean_q: 4.392849
 31612/100000: episode: 609, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 49.140, mean reward: 2.891 [2.349, 6.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.308, 10.100], loss: 0.204920, mae: 0.410661, mean_q: 4.394228
[Info] 3-TH LEVEL FOUND: 6.815001964569092, Considering 10/90 traces
 31629/100000: episode: 610, duration: 4.258s, episode steps: 17, steps per second: 4, episode reward: 50.819, mean reward: 2.989 [2.395, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.238, 10.100], loss: 0.210235, mae: 0.423945, mean_q: 4.490778
 31644/100000: episode: 611, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 75.280, mean reward: 5.019 [3.666, 6.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.959, 10.100], loss: 0.196293, mae: 0.416005, mean_q: 4.413007
 31653/100000: episode: 612, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 47.735, mean reward: 5.304 [2.724, 7.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.115, 10.100], loss: 0.579841, mae: 0.494802, mean_q: 4.529966
 31662/100000: episode: 613, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 25.723, mean reward: 2.858 [2.511, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.295, 10.100], loss: 0.242319, mae: 0.456629, mean_q: 4.509558
 31673/100000: episode: 614, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 52.652, mean reward: 4.787 [3.012, 8.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.525, 10.100], loss: 0.260127, mae: 0.470444, mean_q: 4.470734
 31687/100000: episode: 615, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 39.395, mean reward: 2.814 [2.574, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.306, 10.100], loss: 4.218085, mae: 0.707518, mean_q: 4.738584
 31695/100000: episode: 616, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 21.963, mean reward: 2.745 [2.139, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.387, 10.100], loss: 0.366901, mae: 0.580926, mean_q: 4.312183
 31709/100000: episode: 617, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 43.076, mean reward: 3.077 [2.189, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.198, 10.100], loss: 0.269200, mae: 0.476365, mean_q: 4.604921
 31717/100000: episode: 618, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 23.622, mean reward: 2.953 [2.208, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.413, 10.100], loss: 0.221381, mae: 0.450863, mean_q: 4.432464
 31732/100000: episode: 619, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 41.745, mean reward: 2.783 [2.503, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.299, 10.100], loss: 0.736418, mae: 0.482207, mean_q: 4.483491
 31743/100000: episode: 620, duration: 0.054s, episode steps: 11, steps per second: 204, episode reward: 44.102, mean reward: 4.009 [3.171, 6.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.469, 10.100], loss: 0.356040, mae: 0.511386, mean_q: 4.585257
 31752/100000: episode: 621, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 31.922, mean reward: 3.547 [2.730, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.352, 10.100], loss: 0.270580, mae: 0.503071, mean_q: 4.483387
 31760/100000: episode: 622, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 23.406, mean reward: 2.926 [2.376, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.272, 10.100], loss: 0.300774, mae: 0.497233, mean_q: 4.494570
 31767/100000: episode: 623, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 20.726, mean reward: 2.961 [2.395, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.361, 10.100], loss: 0.366661, mae: 0.528844, mean_q: 4.564353
 31782/100000: episode: 624, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 45.206, mean reward: 3.014 [2.484, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.341, 10.100], loss: 0.698484, mae: 0.487573, mean_q: 4.545996
 31791/100000: episode: 625, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 21.796, mean reward: 2.422 [1.918, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.552, 10.100], loss: 0.551704, mae: 0.455706, mean_q: 4.587473
 31800/100000: episode: 626, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 46.756, mean reward: 5.195 [3.406, 9.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.717, 10.100], loss: 0.223401, mae: 0.456357, mean_q: 4.662785
 31808/100000: episode: 627, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 24.326, mean reward: 3.041 [2.374, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.930, 10.100], loss: 0.193197, mae: 0.413096, mean_q: 4.547343
 31823/100000: episode: 628, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 43.329, mean reward: 2.889 [1.964, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.320, 10.100], loss: 0.219772, mae: 0.433553, mean_q: 4.547753
 31831/100000: episode: 629, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 35.800, mean reward: 4.475 [2.784, 8.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.353, 10.100], loss: 0.245997, mae: 0.457105, mean_q: 4.495383
 31839/100000: episode: 630, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 30.280, mean reward: 3.785 [2.595, 4.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.606, 10.100], loss: 0.201552, mae: 0.422380, mean_q: 4.629582
 31848/100000: episode: 631, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 26.359, mean reward: 2.929 [2.619, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.275, 10.100], loss: 0.241166, mae: 0.461927, mean_q: 4.609175
 31855/100000: episode: 632, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 29.126, mean reward: 4.161 [3.022, 6.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.514, 10.100], loss: 0.275011, mae: 0.433485, mean_q: 4.446021
 31868/100000: episode: 633, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 82.028, mean reward: 6.310 [3.174, 21.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.640, 10.100], loss: 0.217140, mae: 0.443663, mean_q: 4.489724
 31883/100000: episode: 634, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 38.750, mean reward: 2.583 [2.140, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.356, 10.100], loss: 0.748373, mae: 0.517079, mean_q: 4.637630
 31890/100000: episode: 635, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 20.964, mean reward: 2.995 [2.399, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.281, 10.100], loss: 0.275433, mae: 0.489197, mean_q: 4.469571
 31898/100000: episode: 636, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 25.228, mean reward: 3.153 [2.662, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.348, 10.100], loss: 0.236724, mae: 0.472489, mean_q: 4.630875
 31907/100000: episode: 637, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 28.515, mean reward: 3.168 [2.486, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.819, 10.100], loss: 0.219366, mae: 0.460983, mean_q: 4.656960
 31922/100000: episode: 638, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 44.685, mean reward: 2.979 [2.627, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.967, 10.100], loss: 0.212011, mae: 0.431775, mean_q: 4.560735
 31937/100000: episode: 639, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 58.765, mean reward: 3.918 [2.961, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.773, 10.100], loss: 0.766358, mae: 0.574292, mean_q: 4.704116
 31944/100000: episode: 640, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 18.430, mean reward: 2.633 [2.280, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.494, 10.100], loss: 0.206580, mae: 0.441088, mean_q: 4.478848
 31951/100000: episode: 641, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 21.276, mean reward: 3.039 [2.322, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.341, 10.100], loss: 0.276289, mae: 0.502387, mean_q: 4.749575
 31966/100000: episode: 642, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 65.252, mean reward: 4.350 [2.481, 21.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.256, 10.100], loss: 0.216740, mae: 0.451907, mean_q: 4.597647
 31973/100000: episode: 643, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 19.394, mean reward: 2.771 [2.340, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.380, 10.100], loss: 0.769286, mae: 0.572263, mean_q: 4.688010
 31980/100000: episode: 644, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 27.229, mean reward: 3.890 [2.769, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.762, 10.100], loss: 0.274635, mae: 0.448164, mean_q: 4.517081
 31994/100000: episode: 645, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 58.751, mean reward: 4.196 [3.219, 6.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.344, 10.100], loss: 0.204727, mae: 0.433540, mean_q: 4.557729
 32009/100000: episode: 646, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 61.736, mean reward: 4.116 [2.865, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.618, 10.100], loss: 0.470967, mae: 0.521767, mean_q: 4.690291
 32022/100000: episode: 647, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 66.925, mean reward: 5.148 [2.840, 8.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.897, 10.100], loss: 8.626075, mae: 0.883693, mean_q: 4.797630
 32037/100000: episode: 648, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 49.191, mean reward: 3.279 [2.625, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.276, 10.100], loss: 0.781369, mae: 0.793494, mean_q: 4.539344
 32052/100000: episode: 649, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 41.716, mean reward: 2.781 [2.317, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.263, 10.100], loss: 0.428593, mae: 0.584891, mean_q: 4.655739
 32061/100000: episode: 650, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 23.597, mean reward: 2.622 [2.399, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.385, 10.100], loss: 0.212750, mae: 0.474498, mean_q: 4.884600
 32072/100000: episode: 651, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 40.088, mean reward: 3.644 [3.037, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.692, 10.100], loss: 0.315903, mae: 0.489796, mean_q: 4.632325
 32080/100000: episode: 652, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 22.852, mean reward: 2.857 [2.630, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.364, 10.100], loss: 0.294845, mae: 0.500534, mean_q: 4.672767
 32091/100000: episode: 653, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 34.345, mean reward: 3.122 [2.656, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.379, 10.100], loss: 0.383123, mae: 0.532628, mean_q: 4.691158
 32099/100000: episode: 654, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 23.725, mean reward: 2.966 [2.332, 5.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.352, 10.100], loss: 7.098647, mae: 0.727248, mean_q: 4.813544
 32107/100000: episode: 655, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 24.114, mean reward: 3.014 [2.571, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.370, 10.100], loss: 0.347039, mae: 0.573758, mean_q: 4.972520
 32122/100000: episode: 656, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 57.567, mean reward: 3.838 [3.284, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.590, 10.100], loss: 0.303254, mae: 0.507279, mean_q: 4.578736
 32136/100000: episode: 657, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 42.421, mean reward: 3.030 [2.645, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.614, 10.100], loss: 0.263403, mae: 0.493534, mean_q: 4.708398
 32149/100000: episode: 658, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 90.720, mean reward: 6.978 [3.148, 13.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.782, 10.100], loss: 0.301502, mae: 0.493706, mean_q: 4.823465
 32162/100000: episode: 659, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 65.121, mean reward: 5.009 [2.888, 12.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.405, 10.100], loss: 0.328819, mae: 0.540273, mean_q: 4.860044
 32171/100000: episode: 660, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 24.898, mean reward: 2.766 [2.567, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.989, 10.100], loss: 0.303708, mae: 0.467806, mean_q: 4.621617
 32186/100000: episode: 661, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 77.036, mean reward: 5.136 [3.513, 7.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.421, 10.100], loss: 0.291299, mae: 0.502553, mean_q: 4.667137
 32194/100000: episode: 662, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 31.381, mean reward: 3.923 [2.202, 6.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.455, 10.100], loss: 0.883504, mae: 0.651230, mean_q: 5.181812
 32208/100000: episode: 663, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 45.818, mean reward: 3.273 [2.326, 4.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.269, 10.100], loss: 0.321928, mae: 0.523773, mean_q: 4.689528
 32219/100000: episode: 664, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 40.297, mean reward: 3.663 [2.734, 8.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.280, 10.100], loss: 0.284052, mae: 0.496985, mean_q: 4.558250
 32234/100000: episode: 665, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 44.262, mean reward: 2.951 [2.090, 4.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.479, 10.100], loss: 0.722321, mae: 0.631253, mean_q: 5.008170
 32241/100000: episode: 666, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 20.203, mean reward: 2.886 [2.398, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.417, 10.100], loss: 0.342306, mae: 0.528807, mean_q: 4.754903
 32252/100000: episode: 667, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 42.770, mean reward: 3.888 [2.817, 5.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.341, 10.100], loss: 0.712997, mae: 0.537809, mean_q: 4.899604
 32259/100000: episode: 668, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 30.927, mean reward: 4.418 [2.848, 6.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.401, 10.100], loss: 0.344240, mae: 0.500262, mean_q: 4.639302
 32270/100000: episode: 669, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 33.280, mean reward: 3.025 [2.799, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.595, 10.100], loss: 0.679613, mae: 0.522098, mean_q: 4.785055
 32281/100000: episode: 670, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 36.234, mean reward: 3.294 [2.680, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.314, 10.100], loss: 0.285496, mae: 0.508586, mean_q: 4.804288
 32296/100000: episode: 671, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 53.906, mean reward: 3.594 [3.079, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.272, 10.100], loss: 0.340414, mae: 0.563436, mean_q: 4.900640
 32310/100000: episode: 672, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 106.840, mean reward: 7.631 [3.331, 19.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.400, 10.100], loss: 0.362170, mae: 0.539089, mean_q: 4.887128
 32318/100000: episode: 673, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 28.810, mean reward: 3.601 [2.312, 9.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.414, 10.100], loss: 0.477176, mae: 0.575939, mean_q: 4.875523
 32333/100000: episode: 674, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 52.573, mean reward: 3.505 [2.895, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.475, 10.100], loss: 0.318642, mae: 0.536398, mean_q: 4.878464
 32340/100000: episode: 675, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 21.860, mean reward: 3.123 [2.644, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.419, 10.100], loss: 0.763546, mae: 0.588940, mean_q: 5.064967
 32355/100000: episode: 676, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 66.216, mean reward: 4.414 [3.007, 10.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.670, 10.100], loss: 0.562500, mae: 0.586456, mean_q: 4.816113
 32369/100000: episode: 677, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 56.570, mean reward: 4.041 [3.027, 8.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.872, 10.100], loss: 0.599152, mae: 0.589494, mean_q: 4.873879
 32378/100000: episode: 678, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 29.945, mean reward: 3.327 [2.363, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.495, 10.100], loss: 0.395527, mae: 0.550048, mean_q: 4.900553
 32391/100000: episode: 679, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 38.745, mean reward: 2.980 [2.606, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.358, 10.100], loss: 0.267846, mae: 0.473480, mean_q: 4.711207
 32406/100000: episode: 680, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 59.022, mean reward: 3.935 [2.913, 8.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.559, 10.100], loss: 0.808905, mae: 0.610893, mean_q: 5.039799
 32414/100000: episode: 681, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 24.934, mean reward: 3.117 [2.595, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.390, 10.100], loss: 0.764321, mae: 0.622440, mean_q: 5.086869
 32421/100000: episode: 682, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 23.482, mean reward: 3.355 [2.579, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.497, 10.100], loss: 0.309057, mae: 0.477387, mean_q: 4.924558
 32428/100000: episode: 683, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 26.896, mean reward: 3.842 [2.924, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.600, 10.100], loss: 8.010318, mae: 0.765657, mean_q: 4.953322
 32439/100000: episode: 684, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 39.251, mean reward: 3.568 [2.602, 8.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.822, 10.100], loss: 0.452254, mae: 0.584447, mean_q: 5.031946
 32447/100000: episode: 685, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 21.183, mean reward: 2.648 [2.457, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.396, 10.100], loss: 0.280704, mae: 0.519983, mean_q: 4.866960
 32456/100000: episode: 686, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 34.484, mean reward: 3.832 [2.592, 5.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.481, 10.100], loss: 0.914413, mae: 0.632631, mean_q: 4.952779
 32464/100000: episode: 687, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 23.598, mean reward: 2.950 [2.592, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.524, 10.100], loss: 0.283680, mae: 0.531815, mean_q: 4.882500
 32471/100000: episode: 688, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 24.855, mean reward: 3.551 [2.629, 5.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.475, 10.100], loss: 2.331475, mae: 0.887042, mean_q: 5.309886
 32482/100000: episode: 689, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 39.636, mean reward: 3.603 [3.167, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.249, 10.100], loss: 5.757599, mae: 0.841411, mean_q: 5.089573
 32489/100000: episode: 690, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 18.123, mean reward: 2.589 [2.266, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.293, 10.100], loss: 0.383044, mae: 0.599254, mean_q: 5.051304
 32496/100000: episode: 691, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 18.159, mean reward: 2.594 [2.286, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.234, 10.100], loss: 1.390362, mae: 0.773269, mean_q: 5.128053
 32511/100000: episode: 692, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 72.749, mean reward: 4.850 [2.345, 8.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.644, 10.100], loss: 0.359240, mae: 0.554960, mean_q: 4.863460
 32525/100000: episode: 693, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 94.106, mean reward: 6.722 [3.476, 9.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.519, 10.100], loss: 0.525543, mae: 0.562923, mean_q: 4.943876
 32538/100000: episode: 694, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 57.807, mean reward: 4.447 [3.222, 7.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.571, 10.100], loss: 0.882359, mae: 0.649288, mean_q: 5.076015
 32553/100000: episode: 695, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 50.392, mean reward: 3.359 [2.316, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.167, 10.100], loss: 0.633846, mae: 0.597748, mean_q: 5.077504
 32568/100000: episode: 696, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 64.902, mean reward: 4.327 [3.508, 5.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.026, 10.100], loss: 0.551808, mae: 0.611615, mean_q: 4.965174
 32583/100000: episode: 697, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 54.177, mean reward: 3.612 [2.651, 4.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.409, 10.100], loss: 0.411555, mae: 0.591060, mean_q: 5.072290
 32596/100000: episode: 698, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 57.575, mean reward: 4.429 [3.317, 6.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.513, 10.100], loss: 0.298306, mae: 0.521089, mean_q: 4.840756
 32611/100000: episode: 699, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 61.868, mean reward: 4.125 [3.567, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.441, 10.100], loss: 0.467390, mae: 0.612686, mean_q: 5.121360
[Info] 4-TH LEVEL FOUND: 11.068528175354004, Considering 10/90 traces
 32620/100000: episode: 700, duration: 4.228s, episode steps: 9, steps per second: 2, episode reward: 42.477, mean reward: 4.720 [2.844, 6.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.480, 10.100], loss: 0.462081, mae: 0.581303, mean_q: 5.041543
 32621/100000: episode: 701, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 5.906, mean reward: 5.906 [5.906, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.479, 10.200], loss: 0.430205, mae: 0.622527, mean_q: 5.312763
 32622/100000: episode: 702, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 7.196, mean reward: 7.196 [7.196, 7.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.593, 10.200], loss: 0.548414, mae: 0.656295, mean_q: 5.319142
 32624/100000: episode: 703, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 16.029, mean reward: 8.014 [7.473, 8.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.477, 10.100], loss: 0.254758, mae: 0.519276, mean_q: 5.295758
 32631/100000: episode: 704, duration: 0.042s, episode steps: 7, steps per second: 169, episode reward: 39.762, mean reward: 5.680 [4.309, 8.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.665, 10.100], loss: 0.318272, mae: 0.523794, mean_q: 4.936076
 32633/100000: episode: 705, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 11.734, mean reward: 5.867 [5.855, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.507, 10.100], loss: 0.475913, mae: 0.525493, mean_q: 5.122337
 32634/100000: episode: 706, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 4.222, mean reward: 4.222 [4.222, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.540, 10.200], loss: 0.467931, mae: 0.663014, mean_q: 5.006221
 32635/100000: episode: 707, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 5.232, mean reward: 5.232 [5.232, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.483, 10.200], loss: 0.788741, mae: 0.654194, mean_q: 5.501984
 32637/100000: episode: 708, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 20.244, mean reward: 10.122 [9.372, 10.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.581, 10.100], loss: 0.370888, mae: 0.548268, mean_q: 5.279462
 32642/100000: episode: 709, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 60.201, mean reward: 12.040 [9.801, 13.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.634, 10.100], loss: 0.216179, mae: 0.478704, mean_q: 4.937634
 32643/100000: episode: 710, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 14.725, mean reward: 14.725 [14.725, 14.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.542, 10.200], loss: 1.102094, mae: 0.761405, mean_q: 5.035881
 32644/100000: episode: 711, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 7.434, mean reward: 7.434 [7.434, 7.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.557, 10.200], loss: 0.162510, mae: 0.465470, mean_q: 4.996978
 32645/100000: episode: 712, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 8.713, mean reward: 8.713 [8.713, 8.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.506, 10.200], loss: 0.400210, mae: 0.661452, mean_q: 5.577003
 32647/100000: episode: 713, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 20.819, mean reward: 10.410 [8.228, 12.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.541, 10.100], loss: 0.401303, mae: 0.617536, mean_q: 5.173821
 32654/100000: episode: 714, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 33.951, mean reward: 4.850 [3.682, 7.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.500, 10.100], loss: 0.607707, mae: 0.579410, mean_q: 4.938836
 32661/100000: episode: 715, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 35.099, mean reward: 5.014 [4.278, 6.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.510, 10.100], loss: 0.359457, mae: 0.550267, mean_q: 5.145172
 32662/100000: episode: 716, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 19.781, mean reward: 19.781 [19.781, 19.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.529, 10.100], loss: 0.224416, mae: 0.528899, mean_q: 5.023512
 32663/100000: episode: 717, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 3.438, mean reward: 3.438 [3.438, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.524, 10.200], loss: 0.212389, mae: 0.453713, mean_q: 5.133035
 32664/100000: episode: 718, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 10.870, mean reward: 10.870 [10.870, 10.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.568, 10.100], loss: 0.218668, mae: 0.500067, mean_q: 5.586073
 32665/100000: episode: 719, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 15.726, mean reward: 15.726 [15.726, 15.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.642, 10.200], loss: 0.684353, mae: 0.690641, mean_q: 5.416916
[Info] FALSIFICATION!
 32670/100000: episode: 720, duration: 0.277s, episode steps: 5, steps per second: 18, episode reward: 1098.573, mean reward: 219.715 [8.318, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.532, 10.100], loss: 0.359224, mae: 0.533917, mean_q: 5.204659
 32672/100000: episode: 721, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 25.774, mean reward: 12.887 [12.793, 12.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.519, 10.100], loss: 0.309588, mae: 0.530593, mean_q: 5.081306
 32673/100000: episode: 722, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 4.608, mean reward: 4.608 [4.608, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.576, 10.200], loss: 0.375646, mae: 0.538849, mean_q: 5.021084
 32678/100000: episode: 723, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 92.391, mean reward: 18.478 [4.883, 50.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.496, 10.100], loss: 2.513947, mae: 0.903430, mean_q: 5.161836
 32683/100000: episode: 724, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 156.383, mean reward: 31.277 [6.958, 125.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.584, 10.100], loss: 0.510469, mae: 0.650845, mean_q: 5.430961
 32687/100000: episode: 725, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 49.378, mean reward: 12.345 [5.837, 27.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.599, 10.100], loss: 3853.130859, mae: 8.455545, mean_q: 5.315718
 32688/100000: episode: 726, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 15.533, mean reward: 15.533 [15.533, 15.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.513, 10.100], loss: 5.025539, mae: 2.563881, mean_q: 7.405525
[Info] FALSIFICATION!
 32691/100000: episode: 727, duration: 0.299s, episode steps: 3, steps per second: 10, episode reward: 1089.523, mean reward: 363.174 [25.267, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.439, 10.093], loss: 9.907775, mae: 3.880676, mean_q: 9.073813
 32692/100000: episode: 728, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 4.787, mean reward: 4.787 [4.787, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.474, 10.200], loss: 10.171635, mae: 4.102888, mean_q: 9.942269
 32693/100000: episode: 729, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 5.109, mean reward: 5.109 [5.109, 5.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.554, 10.200], loss: 5.429121, mae: 2.521504, mean_q: 7.022415
 32698/100000: episode: 730, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 141.916, mean reward: 28.383 [8.395, 87.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.911, 10.100], loss: 2.057616, mae: 1.585232, mean_q: 5.601684
 32704/100000: episode: 731, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 16.583, mean reward: 2.764 [2.365, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.492, 10.100], loss: 1.781705, mae: 1.478638, mean_q: 4.002929
 32709/100000: episode: 732, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 34.138, mean reward: 6.828 [5.255, 11.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.479, 10.100], loss: 1.072947, mae: 1.190955, mean_q: 4.203069
 32715/100000: episode: 733, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 24.600, mean reward: 4.100 [3.650, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.439, 10.100], loss: 2.483829, mae: 1.156123, mean_q: 5.217250
 32716/100000: episode: 734, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 5.680, mean reward: 5.680 [5.680, 5.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.553, 10.200], loss: 0.355977, mae: 0.641739, mean_q: 5.411510
 32718/100000: episode: 735, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 16.051, mean reward: 8.025 [7.684, 8.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.546, 10.100], loss: 0.834866, mae: 0.882694, mean_q: 5.709783
 32719/100000: episode: 736, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 8.942, mean reward: 8.942 [8.942, 8.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.489, 10.200], loss: 1.081989, mae: 1.027743, mean_q: 6.206393
 32725/100000: episode: 737, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 25.782, mean reward: 4.297 [3.483, 4.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.559, 10.100], loss: 1.881988, mae: 0.983829, mean_q: 5.849924
 32727/100000: episode: 738, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 11.142, mean reward: 5.571 [4.442, 6.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.510, 10.100], loss: 7672.336914, mae: 16.778671, mean_q: 5.781457
 32728/100000: episode: 739, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 5.438, mean reward: 5.438 [5.438, 5.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.620, 10.200], loss: 2.869199, mae: 1.855811, mean_q: 6.332890
 32729/100000: episode: 740, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 5.068, mean reward: 5.068 [5.068, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.517, 10.100], loss: 5.117918, mae: 2.472081, mean_q: 7.183138
 32731/100000: episode: 741, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 11.985, mean reward: 5.993 [5.993, 5.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.521, 10.100], loss: 5.936393, mae: 2.835352, mean_q: 7.693409
 32735/100000: episode: 742, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 20.093, mean reward: 5.023 [3.838, 6.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.477, 10.100], loss: 7.120389, mae: 3.027378, mean_q: 8.267287
 32736/100000: episode: 743, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 14.092, mean reward: 14.092 [14.092, 14.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.523, 10.200], loss: 3.251554, mae: 1.940506, mean_q: 6.612015
 32737/100000: episode: 744, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 5.936, mean reward: 5.936 [5.936, 5.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.479, 10.100], loss: 1.786612, mae: 1.559379, mean_q: 6.340548
 32743/100000: episode: 745, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 23.216, mean reward: 3.869 [3.172, 4.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.628, 10.100], loss: 1.815153, mae: 1.192197, mean_q: 5.201892
 32749/100000: episode: 746, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 26.648, mean reward: 4.441 [3.602, 5.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.552, 10.100], loss: 40.198814, mae: 2.124667, mean_q: 4.433113
 32750/100000: episode: 747, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 912.992, mean reward: 912.992 [912.992, 912.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.605, 10.200], loss: 111.064903, mae: 3.751266, mean_q: 4.696664
 32755/100000: episode: 748, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 49.643, mean reward: 9.929 [7.958, 12.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.654, 10.100], loss: 1.466215, mae: 1.162253, mean_q: 5.557722
 32761/100000: episode: 749, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 18.989, mean reward: 3.165 [2.585, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.400, 10.100], loss: 0.888940, mae: 0.885920, mean_q: 5.851840
 32763/100000: episode: 750, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 10.546, mean reward: 5.273 [5.186, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.494, 10.100], loss: 0.463263, mae: 0.635232, mean_q: 5.477255
 32764/100000: episode: 751, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 4.608, mean reward: 4.608 [4.608, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.565, 10.200], loss: 1.719161, mae: 1.213212, mean_q: 5.331694
 32765/100000: episode: 752, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 7.996, mean reward: 7.996 [7.996, 7.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.563, 10.100], loss: 0.990875, mae: 0.968925, mean_q: 5.571781
 32767/100000: episode: 753, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 23.069, mean reward: 11.535 [9.350, 13.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.550, 10.100], loss: 0.866564, mae: 0.778193, mean_q: 5.329382
 32773/100000: episode: 754, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 20.181, mean reward: 3.363 [2.894, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.458, 10.100], loss: 1.052426, mae: 0.901529, mean_q: 5.342877
 32774/100000: episode: 755, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 7.520, mean reward: 7.520 [7.520, 7.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.593, 10.200], loss: 0.633979, mae: 0.710323, mean_q: 5.640628
 32780/100000: episode: 756, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 26.714, mean reward: 4.452 [3.286, 6.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.527, 10.100], loss: 2.032776, mae: 1.020529, mean_q: 5.498057
 32781/100000: episode: 757, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 6.835, mean reward: 6.835 [6.835, 6.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.546, 10.100], loss: 0.647511, mae: 0.820303, mean_q: 5.647911
 32782/100000: episode: 758, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 4.130, mean reward: 4.130 [4.130, 4.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.558, 10.200], loss: 0.880665, mae: 0.742153, mean_q: 5.013621
 32783/100000: episode: 759, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 6.833, mean reward: 6.833 [6.833, 6.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.564, 10.200], loss: 0.281613, mae: 0.541415, mean_q: 5.147144
 32788/100000: episode: 760, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 44.445, mean reward: 8.889 [6.140, 11.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.554, 10.100], loss: 11.356308, mae: 1.151940, mean_q: 5.540447
 32793/100000: episode: 761, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 55.888, mean reward: 11.178 [9.561, 13.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.454, 10.100], loss: 0.663278, mae: 0.675891, mean_q: 5.448613
 32799/100000: episode: 762, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 24.589, mean reward: 4.098 [3.433, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.483, 10.100], loss: 0.688703, mae: 0.714681, mean_q: 5.690176
 32801/100000: episode: 763, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 18.496, mean reward: 9.248 [6.700, 11.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.570, 10.100], loss: 0.980033, mae: 0.809639, mean_q: 5.440800
 32806/100000: episode: 764, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 44.721, mean reward: 8.944 [6.452, 13.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.479, 10.100], loss: 3065.531006, mae: 6.904302, mean_q: 5.429223
[Info] FALSIFICATION!
 32808/100000: episode: 765, duration: 0.194s, episode steps: 2, steps per second: 10, episode reward: 1174.735, mean reward: 587.367 [174.735, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.515, 10.100], loss: 7648.832520, mae: 17.306944, mean_q: 7.435092
 32809/100000: episode: 766, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 5.610, mean reward: 5.610 [5.610, 5.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.629, 10.200], loss: 6.565804, mae: 3.209875, mean_q: 8.688663
 32814/100000: episode: 767, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 45.416, mean reward: 9.083 [7.068, 12.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.678, 10.100], loss: 13.700294, mae: 4.418723, mean_q: 9.661410
 32818/100000: episode: 768, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 38.375, mean reward: 9.594 [5.161, 16.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.591, 10.100], loss: 8.146117, mae: 3.197006, mean_q: 8.439695
 32823/100000: episode: 769, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 40.830, mean reward: 8.166 [5.585, 9.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.461, 10.100], loss: 7.651269, mae: 3.193648, mean_q: 8.682276
 32824/100000: episode: 770, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 5.673, mean reward: 5.673 [5.673, 5.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.542, 10.100], loss: 49.844608, mae: 4.361903, mean_q: 7.936702
[Info] FALSIFICATION!
 32826/100000: episode: 771, duration: 0.283s, episode steps: 2, steps per second: 7, episode reward: 1101.608, mean reward: 550.804 [101.608, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.516, 10.100], loss: 10.225887, mae: 3.357147, mean_q: 8.181138
 32833/100000: episode: 772, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 44.713, mean reward: 6.388 [5.413, 8.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.584, 10.100], loss: 37.563873, mae: 2.548064, mean_q: 6.073453
 32839/100000: episode: 773, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 27.114, mean reward: 4.519 [3.404, 6.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.558, 10.100], loss: 3.028659, mae: 1.703190, mean_q: 5.577257
 32840/100000: episode: 774, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 7.667, mean reward: 7.667 [7.667, 7.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.615, 10.200], loss: 2.415170, mae: 1.455994, mean_q: 5.482105
 32841/100000: episode: 775, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 4.837, mean reward: 4.837 [4.837, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.523, 10.200], loss: 2.162793, mae: 1.544285, mean_q: 6.256671
 32843/100000: episode: 776, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 14.151, mean reward: 7.075 [6.999, 7.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.578, 10.100], loss: 4.891689, mae: 2.480579, mean_q: 7.088821
 32847/100000: episode: 777, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 19.066, mean reward: 4.767 [4.097, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.550, 10.100], loss: 12.491528, mae: 4.012306, mean_q: 9.862457
 32848/100000: episode: 778, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 6.320, mean reward: 6.320 [6.320, 6.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.443, 10.200], loss: 19.125549, mae: 5.145538, mean_q: 10.795485
 32850/100000: episode: 779, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 13.954, mean reward: 6.977 [5.638, 8.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.543, 10.100], loss: 15.045657, mae: 4.412589, mean_q: 10.074429
 32857/100000: episode: 780, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 37.193, mean reward: 5.313 [3.735, 8.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.474, 10.100], loss: 8.519476, mae: 2.904237, mean_q: 8.288463
 32863/100000: episode: 781, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 17.355, mean reward: 2.892 [2.680, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.444, 10.100], loss: 2.686768, mae: 1.682475, mean_q: 5.342751
 32864/100000: episode: 782, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 4.575, mean reward: 4.575 [4.575, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.523, 10.200], loss: 2.860709, mae: 1.860276, mean_q: 5.211711
 32869/100000: episode: 783, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 69.049, mean reward: 13.810 [8.491, 26.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.580, 10.100], loss: 3.325705, mae: 1.949048, mean_q: 4.315554
 32870/100000: episode: 784, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 6.939, mean reward: 6.939 [6.939, 6.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.541, 10.200], loss: 227.231186, mae: 5.441284, mean_q: 3.904749
 32871/100000: episode: 785, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 5.701, mean reward: 5.701 [5.701, 5.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.454, 10.200], loss: 2.115611, mae: 1.770482, mean_q: 4.572250
 32877/100000: episode: 786, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 27.846, mean reward: 4.641 [3.336, 7.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.611, 10.100], loss: 1.656358, mae: 1.289580, mean_q: 5.266298
 32878/100000: episode: 787, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 7.913, mean reward: 7.913 [7.913, 7.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.439, 10.100], loss: 4.033009, mae: 1.443008, mean_q: 6.670240
 32883/100000: episode: 788, duration: 0.028s, episode steps: 5, steps per second: 182, episode reward: 29.367, mean reward: 5.873 [4.880, 7.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.581, 10.100], loss: 1.250739, mae: 0.948540, mean_q: 6.311746
[Info] FALSIFICATION!
 32884/100000: episode: 789, duration: 0.274s, episode steps: 1, steps per second: 4, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.543, 10.098], loss: 1.983777, mae: 1.251110, mean_q: 5.794329
[Info] Complete ISplit Iteration
[Info] Levels: [4.5436053, 5.4157686, 6.815002, 11.068528, 13.498265]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.36]
[Info] Error Prob: 3.600000000000001e-05

 32885/100000: episode: 790, duration: 4.446s, episode steps: 1, steps per second: 0, episode reward: 5.434, mean reward: 5.434 [5.434, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.552, 10.200], loss: 4.149209, mae: 1.687987, mean_q: 6.986253
 32985/100000: episode: 791, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 211.092, mean reward: 2.111 [1.488, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.098], loss: 613.072510, mae: 3.540474, mean_q: 7.527769
 33085/100000: episode: 792, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.259, mean reward: 1.823 [1.429, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.812, 10.098], loss: 9.391765, mae: 1.313309, mean_q: 6.106483
 33185/100000: episode: 793, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.628, mean reward: 1.916 [1.454, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.791, 10.098], loss: 694.150940, mae: 4.138682, mean_q: 7.264766
 33285/100000: episode: 794, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 251.681, mean reward: 2.517 [1.483, 7.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.377, 10.480], loss: 740.449463, mae: 4.278005, mean_q: 7.852081
 33385/100000: episode: 795, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.776, mean reward: 1.888 [1.435, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.151, 10.098], loss: 459.529877, mae: 2.545868, mean_q: 6.772580
 33485/100000: episode: 796, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.555, mean reward: 1.856 [1.463, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.530, 10.276], loss: 587.561157, mae: 3.914815, mean_q: 7.948860
 33585/100000: episode: 797, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 187.665, mean reward: 1.877 [1.461, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.812, 10.098], loss: 464.079895, mae: 3.112692, mean_q: 7.419326
 33685/100000: episode: 798, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 186.728, mean reward: 1.867 [1.442, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.969, 10.286], loss: 441.880981, mae: 3.084328, mean_q: 7.279971
 33785/100000: episode: 799, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 204.201, mean reward: 2.042 [1.459, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.381, 10.372], loss: 157.765167, mae: 2.157955, mean_q: 6.758944
 33885/100000: episode: 800, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.112, mean reward: 1.831 [1.474, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.175, 10.178], loss: 803.990601, mae: 4.161623, mean_q: 7.317823
 33985/100000: episode: 801, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 192.380, mean reward: 1.924 [1.462, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.609, 10.238], loss: 404.078827, mae: 3.189127, mean_q: 7.277941
 34085/100000: episode: 802, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 205.522, mean reward: 2.055 [1.479, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.472, 10.267], loss: 727.848389, mae: 3.443772, mean_q: 7.081574
 34185/100000: episode: 803, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 185.185, mean reward: 1.852 [1.450, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.966, 10.098], loss: 733.644287, mae: 4.490496, mean_q: 8.280818
 34285/100000: episode: 804, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.133, mean reward: 1.911 [1.514, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.162, 10.216], loss: 608.436646, mae: 3.385187, mean_q: 7.278693
 34385/100000: episode: 805, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 211.708, mean reward: 2.117 [1.519, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.199, 10.339], loss: 449.903412, mae: 3.149784, mean_q: 7.389263
 34485/100000: episode: 806, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 223.021, mean reward: 2.230 [1.486, 6.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.982, 10.098], loss: 729.002441, mae: 3.938160, mean_q: 7.970767
 34585/100000: episode: 807, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 185.846, mean reward: 1.858 [1.455, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.700, 10.098], loss: 1033.320801, mae: 4.829655, mean_q: 8.102296
 34685/100000: episode: 808, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 227.130, mean reward: 2.271 [1.548, 6.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.171], loss: 314.032959, mae: 2.901958, mean_q: 7.163973
 34785/100000: episode: 809, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.458, mean reward: 1.935 [1.444, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.728, 10.098], loss: 433.939026, mae: 3.342287, mean_q: 7.675406
 34885/100000: episode: 810, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.387, mean reward: 1.844 [1.469, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.410, 10.114], loss: 726.952759, mae: 3.966136, mean_q: 7.714044
 34985/100000: episode: 811, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 196.434, mean reward: 1.964 [1.491, 5.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.911, 10.182], loss: 417.346802, mae: 3.307322, mean_q: 7.481768
 35085/100000: episode: 812, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 187.609, mean reward: 1.876 [1.453, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.175], loss: 434.683502, mae: 2.958089, mean_q: 7.374788
 35185/100000: episode: 813, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.007, mean reward: 1.860 [1.481, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.348], loss: 602.496277, mae: 3.514358, mean_q: 7.498207
 35285/100000: episode: 814, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 195.775, mean reward: 1.958 [1.498, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.098], loss: 585.277710, mae: 3.647748, mean_q: 7.603296
 35385/100000: episode: 815, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 206.008, mean reward: 2.060 [1.435, 5.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.887, 10.203], loss: 570.950806, mae: 3.174963, mean_q: 7.059233
 35485/100000: episode: 816, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 179.183, mean reward: 1.792 [1.452, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.543, 10.153], loss: 588.367493, mae: 3.639297, mean_q: 7.654683
 35585/100000: episode: 817, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 189.210, mean reward: 1.892 [1.453, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.154], loss: 434.739716, mae: 3.373461, mean_q: 7.477842
 35685/100000: episode: 818, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 185.623, mean reward: 1.856 [1.446, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.676, 10.276], loss: 579.608948, mae: 3.185786, mean_q: 7.083971
 35785/100000: episode: 819, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 180.838, mean reward: 1.808 [1.458, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.884, 10.205], loss: 838.047485, mae: 4.487645, mean_q: 7.550600
 35885/100000: episode: 820, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.064, mean reward: 1.881 [1.480, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.098], loss: 732.797058, mae: 4.051665, mean_q: 7.521500
 35985/100000: episode: 821, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.778, mean reward: 1.978 [1.462, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.177, 10.144], loss: 160.628784, mae: 2.143589, mean_q: 6.430462
 36085/100000: episode: 822, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 208.715, mean reward: 2.087 [1.457, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.458, 10.167], loss: 890.329163, mae: 4.158069, mean_q: 7.154282
 36185/100000: episode: 823, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 195.635, mean reward: 1.956 [1.464, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.295], loss: 601.310608, mae: 3.351248, mean_q: 7.088810
 36285/100000: episode: 824, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.098, mean reward: 1.901 [1.493, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.947, 10.169], loss: 744.026123, mae: 3.997540, mean_q: 7.051200
 36385/100000: episode: 825, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.984, mean reward: 1.840 [1.471, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.165, 10.246], loss: 577.048523, mae: 3.706341, mean_q: 7.240132
 36485/100000: episode: 826, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.699, mean reward: 1.847 [1.524, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.129, 10.098], loss: 580.256714, mae: 3.332216, mean_q: 6.977011
 36585/100000: episode: 827, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 210.996, mean reward: 2.110 [1.469, 5.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.139], loss: 1020.125244, mae: 4.540403, mean_q: 7.373430
 36685/100000: episode: 828, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.656, mean reward: 1.827 [1.478, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.399, 10.098], loss: 307.127319, mae: 2.710081, mean_q: 6.772962
 36785/100000: episode: 829, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.870, mean reward: 1.949 [1.476, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.466, 10.098], loss: 1005.639709, mae: 4.439396, mean_q: 6.874976
 36885/100000: episode: 830, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 198.994, mean reward: 1.990 [1.475, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.188, 10.098], loss: 886.239502, mae: 4.557332, mean_q: 7.265950
 36985/100000: episode: 831, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 194.183, mean reward: 1.942 [1.500, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.564, 10.119], loss: 179.789978, mae: 2.783142, mean_q: 6.644742
 37085/100000: episode: 832, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 202.060, mean reward: 2.021 [1.475, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.385, 10.307], loss: 591.546021, mae: 2.879368, mean_q: 6.275217
 37185/100000: episode: 833, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.474, mean reward: 1.925 [1.494, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.106, 10.299], loss: 459.267731, mae: 2.835339, mean_q: 6.158495
 37285/100000: episode: 834, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 201.210, mean reward: 2.012 [1.503, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.136, 10.307], loss: 880.598877, mae: 3.651004, mean_q: 6.208368
 37385/100000: episode: 835, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 189.073, mean reward: 1.891 [1.458, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.556, 10.243], loss: 313.522644, mae: 2.659827, mean_q: 6.134570
 37485/100000: episode: 836, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 207.424, mean reward: 2.074 [1.511, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.839, 10.230], loss: 280.525970, mae: 1.963082, mean_q: 5.533288
 37585/100000: episode: 837, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.753, mean reward: 1.848 [1.462, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.138, 10.098], loss: 696.320312, mae: 3.413136, mean_q: 6.091125
 37685/100000: episode: 838, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 174.196, mean reward: 1.742 [1.461, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.203], loss: 270.634674, mae: 1.551447, mean_q: 4.877762
 37785/100000: episode: 839, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 181.354, mean reward: 1.814 [1.466, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.550, 10.098], loss: 141.865173, mae: 0.931660, mean_q: 4.369596
 37885/100000: episode: 840, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.446, mean reward: 1.854 [1.469, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.726, 10.098], loss: 0.949312, mae: 0.379277, mean_q: 3.952035
 37985/100000: episode: 841, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.796, mean reward: 1.958 [1.467, 6.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.358, 10.121], loss: 0.157046, mae: 0.339587, mean_q: 3.864844
 38085/100000: episode: 842, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 179.004, mean reward: 1.790 [1.440, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.216], loss: 0.153072, mae: 0.340660, mean_q: 3.865232
 38185/100000: episode: 843, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 182.200, mean reward: 1.822 [1.439, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.774, 10.124], loss: 0.127815, mae: 0.323310, mean_q: 3.847764
 38285/100000: episode: 844, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.658, mean reward: 1.937 [1.467, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.804, 10.354], loss: 0.122814, mae: 0.315499, mean_q: 3.842496
 38385/100000: episode: 845, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 207.841, mean reward: 2.078 [1.474, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.771, 10.387], loss: 0.109084, mae: 0.308958, mean_q: 3.826810
 38485/100000: episode: 846, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 212.686, mean reward: 2.127 [1.560, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.061, 10.098], loss: 0.123190, mae: 0.320364, mean_q: 3.841702
 38585/100000: episode: 847, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 202.343, mean reward: 2.023 [1.446, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.441, 10.219], loss: 0.113642, mae: 0.319141, mean_q: 3.843842
 38685/100000: episode: 848, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: 187.681, mean reward: 1.877 [1.437, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.598, 10.301], loss: 0.121116, mae: 0.319045, mean_q: 3.840200
 38785/100000: episode: 849, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 200.033, mean reward: 2.000 [1.475, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.785, 10.242], loss: 0.113720, mae: 0.316148, mean_q: 3.850604
 38885/100000: episode: 850, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 179.967, mean reward: 1.800 [1.439, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.573, 10.098], loss: 0.119850, mae: 0.321534, mean_q: 3.855277
 38985/100000: episode: 851, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 183.893, mean reward: 1.839 [1.441, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.895, 10.286], loss: 0.117853, mae: 0.314055, mean_q: 3.842013
 39085/100000: episode: 852, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 201.633, mean reward: 2.016 [1.462, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.339, 10.098], loss: 0.117581, mae: 0.314802, mean_q: 3.844424
 39185/100000: episode: 853, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 190.581, mean reward: 1.906 [1.434, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.425, 10.098], loss: 0.110312, mae: 0.312871, mean_q: 3.840803
 39285/100000: episode: 854, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 224.155, mean reward: 2.242 [1.479, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.939, 10.411], loss: 0.124644, mae: 0.322625, mean_q: 3.850128
 39385/100000: episode: 855, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.728, mean reward: 1.977 [1.451, 5.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.898, 10.127], loss: 0.114596, mae: 0.319764, mean_q: 3.868311
 39485/100000: episode: 856, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.077, mean reward: 1.871 [1.456, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.121, 10.237], loss: 0.110750, mae: 0.311105, mean_q: 3.840451
 39585/100000: episode: 857, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.497, mean reward: 1.885 [1.443, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.572, 10.171], loss: 0.113510, mae: 0.314183, mean_q: 3.838217
 39685/100000: episode: 858, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 203.936, mean reward: 2.039 [1.436, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.480, 10.166], loss: 0.113885, mae: 0.314015, mean_q: 3.842971
 39785/100000: episode: 859, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 182.111, mean reward: 1.821 [1.443, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.834, 10.098], loss: 0.094807, mae: 0.299347, mean_q: 3.825843
 39885/100000: episode: 860, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 222.799, mean reward: 2.228 [1.494, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.917, 10.098], loss: 0.111628, mae: 0.310544, mean_q: 3.823584
 39985/100000: episode: 861, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 183.234, mean reward: 1.832 [1.459, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.423, 10.296], loss: 0.094588, mae: 0.304334, mean_q: 3.831509
 40085/100000: episode: 862, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.661, mean reward: 1.837 [1.470, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.450, 10.098], loss: 0.104797, mae: 0.312961, mean_q: 3.838683
 40185/100000: episode: 863, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 182.085, mean reward: 1.821 [1.441, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.589, 10.222], loss: 0.096224, mae: 0.307755, mean_q: 3.844433
 40285/100000: episode: 864, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 214.942, mean reward: 2.149 [1.489, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.355, 10.098], loss: 0.105738, mae: 0.311772, mean_q: 3.835428
 40385/100000: episode: 865, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 203.981, mean reward: 2.040 [1.461, 7.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.102, 10.098], loss: 0.098930, mae: 0.304661, mean_q: 3.819317
 40485/100000: episode: 866, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 227.862, mean reward: 2.279 [1.492, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.088360, mae: 0.290582, mean_q: 3.816922
 40585/100000: episode: 867, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 213.160, mean reward: 2.132 [1.440, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.340, 10.228], loss: 0.109959, mae: 0.322694, mean_q: 3.843097
 40685/100000: episode: 868, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.299, mean reward: 1.933 [1.436, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.288, 10.324], loss: 0.114450, mae: 0.320813, mean_q: 3.858310
 40785/100000: episode: 869, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 199.718, mean reward: 1.997 [1.441, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.191, 10.098], loss: 0.104494, mae: 0.316296, mean_q: 3.866487
 40885/100000: episode: 870, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 198.239, mean reward: 1.982 [1.520, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.262, 10.098], loss: 0.109812, mae: 0.315224, mean_q: 3.870683
 40985/100000: episode: 871, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 197.685, mean reward: 1.977 [1.484, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.075, 10.386], loss: 0.109292, mae: 0.322194, mean_q: 3.881877
 41085/100000: episode: 872, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 199.869, mean reward: 1.999 [1.438, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.098], loss: 0.112515, mae: 0.317579, mean_q: 3.881326
 41185/100000: episode: 873, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 201.791, mean reward: 2.018 [1.458, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.098], loss: 0.104681, mae: 0.308674, mean_q: 3.874681
 41285/100000: episode: 874, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.546, mean reward: 1.905 [1.452, 5.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.615, 10.098], loss: 0.111312, mae: 0.309847, mean_q: 3.864328
 41385/100000: episode: 875, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 184.666, mean reward: 1.847 [1.470, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.623, 10.248], loss: 0.106886, mae: 0.312063, mean_q: 3.867318
 41485/100000: episode: 876, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.274, mean reward: 1.883 [1.440, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.102], loss: 0.102381, mae: 0.312309, mean_q: 3.863521
 41585/100000: episode: 877, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.514, mean reward: 1.905 [1.451, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.447, 10.144], loss: 0.113313, mae: 0.325449, mean_q: 3.887572
 41685/100000: episode: 878, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 199.982, mean reward: 2.000 [1.470, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.788, 10.183], loss: 0.107827, mae: 0.317519, mean_q: 3.864246
 41785/100000: episode: 879, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 176.140, mean reward: 1.761 [1.491, 2.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.130], loss: 0.116604, mae: 0.325828, mean_q: 3.880442
 41885/100000: episode: 880, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 209.043, mean reward: 2.090 [1.443, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.261, 10.131], loss: 0.097934, mae: 0.308762, mean_q: 3.858761
 41985/100000: episode: 881, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 203.070, mean reward: 2.031 [1.499, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.583, 10.098], loss: 0.111330, mae: 0.321360, mean_q: 3.873776
 42085/100000: episode: 882, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 199.416, mean reward: 1.994 [1.505, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.098], loss: 0.107145, mae: 0.320039, mean_q: 3.848886
 42185/100000: episode: 883, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 186.592, mean reward: 1.866 [1.455, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.237, 10.098], loss: 0.133280, mae: 0.331706, mean_q: 3.863177
 42285/100000: episode: 884, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 214.196, mean reward: 2.142 [1.459, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.449, 10.098], loss: 0.114869, mae: 0.316671, mean_q: 3.865823
 42385/100000: episode: 885, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.286, mean reward: 1.973 [1.439, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.703, 10.098], loss: 0.108533, mae: 0.321204, mean_q: 3.882549
 42485/100000: episode: 886, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.176, mean reward: 1.832 [1.504, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.098], loss: 0.106649, mae: 0.312741, mean_q: 3.871125
 42585/100000: episode: 887, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.043, mean reward: 1.870 [1.459, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.080, 10.141], loss: 0.131566, mae: 0.333271, mean_q: 3.888692
 42685/100000: episode: 888, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 182.863, mean reward: 1.829 [1.462, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.882, 10.206], loss: 0.112190, mae: 0.325277, mean_q: 3.887163
 42785/100000: episode: 889, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.103, mean reward: 1.871 [1.445, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.083, 10.214], loss: 0.103852, mae: 0.316673, mean_q: 3.890158
[Info] 1-TH LEVEL FOUND: 4.379993438720703, Considering 10/90 traces
 42885/100000: episode: 890, duration: 5.019s, episode steps: 100, steps per second: 20, episode reward: 185.271, mean reward: 1.853 [1.484, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.349, 10.100], loss: 0.102880, mae: 0.318213, mean_q: 3.868661
 42889/100000: episode: 891, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 8.447, mean reward: 2.112 [2.013, 2.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.264, 10.100], loss: 0.088684, mae: 0.289225, mean_q: 3.790278
 42899/100000: episode: 892, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 26.310, mean reward: 2.631 [1.966, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.428, 10.100], loss: 0.101687, mae: 0.319718, mean_q: 3.890654
 42903/100000: episode: 893, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 9.589, mean reward: 2.397 [2.149, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.389, 10.100], loss: 0.206096, mae: 0.387255, mean_q: 3.866986
 42995/100000: episode: 894, duration: 0.453s, episode steps: 92, steps per second: 203, episode reward: 165.424, mean reward: 1.798 [1.451, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-1.253, 10.100], loss: 0.133114, mae: 0.315423, mean_q: 3.872544
 42996/100000: episode: 895, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 2.803, mean reward: 2.803 [2.803, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.281, 10.100], loss: 0.093361, mae: 0.275391, mean_q: 3.732237
 43090/100000: episode: 896, duration: 0.475s, episode steps: 94, steps per second: 198, episode reward: 191.066, mean reward: 2.033 [1.522, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.605, 10.100], loss: 0.112584, mae: 0.302569, mean_q: 3.854076
 43182/100000: episode: 897, duration: 0.478s, episode steps: 92, steps per second: 192, episode reward: 171.108, mean reward: 1.860 [1.454, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.674, 10.151], loss: 0.100185, mae: 0.322044, mean_q: 3.875940
 43186/100000: episode: 898, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 8.392, mean reward: 2.098 [1.830, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.334, 10.100], loss: 0.119203, mae: 0.350445, mean_q: 3.946807
 43187/100000: episode: 899, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.795, mean reward: 2.795 [2.795, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.259, 10.100], loss: 0.070418, mae: 0.307263, mean_q: 3.906137
 43279/100000: episode: 900, duration: 0.458s, episode steps: 92, steps per second: 201, episode reward: 178.606, mean reward: 1.941 [1.506, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-1.481, 10.100], loss: 0.103272, mae: 0.312995, mean_q: 3.872029
 43289/100000: episode: 901, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 30.057, mean reward: 3.006 [2.552, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.177, 10.100], loss: 0.090552, mae: 0.319919, mean_q: 3.872250
 43383/100000: episode: 902, duration: 0.574s, episode steps: 94, steps per second: 164, episode reward: 185.241, mean reward: 1.971 [1.457, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.138, 10.100], loss: 0.109207, mae: 0.318931, mean_q: 3.895743
 43393/100000: episode: 903, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 37.388, mean reward: 3.739 [2.808, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.317, 10.100], loss: 0.138680, mae: 0.311533, mean_q: 3.857634
 43403/100000: episode: 904, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 27.506, mean reward: 2.751 [2.413, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.333, 10.100], loss: 0.110989, mae: 0.330346, mean_q: 3.913048
 43413/100000: episode: 905, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 23.364, mean reward: 2.336 [1.964, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.289, 10.100], loss: 0.101486, mae: 0.319140, mean_q: 3.890594
 43423/100000: episode: 906, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 22.133, mean reward: 2.213 [1.986, 2.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.343, 10.100], loss: 0.094930, mae: 0.312736, mean_q: 3.853788
 43472/100000: episode: 907, duration: 0.308s, episode steps: 49, steps per second: 159, episode reward: 106.022, mean reward: 2.164 [1.495, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.511, 10.180], loss: 0.099372, mae: 0.311795, mean_q: 3.880278
 43514/100000: episode: 908, duration: 0.245s, episode steps: 42, steps per second: 171, episode reward: 78.739, mean reward: 1.875 [1.510, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.470, 10.100], loss: 0.115324, mae: 0.321716, mean_q: 3.890053
 43609/100000: episode: 909, duration: 0.501s, episode steps: 95, steps per second: 190, episode reward: 198.588, mean reward: 2.090 [1.447, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.715, 10.100], loss: 0.112602, mae: 0.316429, mean_q: 3.883605
 43613/100000: episode: 910, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 14.135, mean reward: 3.534 [2.812, 5.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.307, 10.100], loss: 0.059895, mae: 0.269820, mean_q: 3.952119
 43614/100000: episode: 911, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 2.135, mean reward: 2.135 [2.135, 2.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.310, 10.100], loss: 0.052013, mae: 0.236689, mean_q: 3.821637
 43656/100000: episode: 912, duration: 0.229s, episode steps: 42, steps per second: 184, episode reward: 84.356, mean reward: 2.008 [1.489, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.679, 10.100], loss: 0.157671, mae: 0.346809, mean_q: 3.910929
 43698/100000: episode: 913, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 79.787, mean reward: 1.900 [1.486, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.581, 10.142], loss: 0.112661, mae: 0.314097, mean_q: 3.872061
 43792/100000: episode: 914, duration: 0.501s, episode steps: 94, steps per second: 187, episode reward: 169.592, mean reward: 1.804 [1.464, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.514, 10.268], loss: 0.106880, mae: 0.313699, mean_q: 3.871032
 43793/100000: episode: 915, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 2.438, mean reward: 2.438 [2.438, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.289, 10.100], loss: 0.057560, mae: 0.260163, mean_q: 3.952781
 43797/100000: episode: 916, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 9.726, mean reward: 2.432 [2.035, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.346, 10.100], loss: 0.160316, mae: 0.347972, mean_q: 3.933419
 43801/100000: episode: 917, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 10.350, mean reward: 2.587 [2.292, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.295, 10.100], loss: 0.076903, mae: 0.251440, mean_q: 3.731302
 43893/100000: episode: 918, duration: 0.512s, episode steps: 92, steps per second: 180, episode reward: 179.754, mean reward: 1.954 [1.463, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.666, 10.112], loss: 0.104438, mae: 0.312268, mean_q: 3.874359
 43943/100000: episode: 919, duration: 0.264s, episode steps: 50, steps per second: 190, episode reward: 102.812, mean reward: 2.056 [1.508, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.216, 10.467], loss: 0.101859, mae: 0.326896, mean_q: 3.885833
 43985/100000: episode: 920, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 87.687, mean reward: 2.088 [1.584, 4.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.810, 10.100], loss: 0.110687, mae: 0.325203, mean_q: 3.884757
 44079/100000: episode: 921, duration: 0.556s, episode steps: 94, steps per second: 169, episode reward: 173.180, mean reward: 1.842 [1.488, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.260, 10.210], loss: 0.107365, mae: 0.327053, mean_q: 3.901005
 44128/100000: episode: 922, duration: 0.254s, episode steps: 49, steps per second: 193, episode reward: 94.651, mean reward: 1.932 [1.496, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.268, 10.100], loss: 0.101643, mae: 0.314398, mean_q: 3.881340
 44178/100000: episode: 923, duration: 0.248s, episode steps: 50, steps per second: 201, episode reward: 90.939, mean reward: 1.819 [1.475, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.707, 10.188], loss: 0.098025, mae: 0.313345, mean_q: 3.903920
 44270/100000: episode: 924, duration: 0.465s, episode steps: 92, steps per second: 198, episode reward: 167.085, mean reward: 1.816 [1.450, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-1.360, 10.100], loss: 0.108626, mae: 0.321154, mean_q: 3.889348
 44280/100000: episode: 925, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 29.888, mean reward: 2.989 [2.382, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.242, 10.100], loss: 0.095227, mae: 0.299598, mean_q: 3.846280
 44284/100000: episode: 926, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 8.806, mean reward: 2.202 [1.862, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.290, 10.100], loss: 0.078685, mae: 0.289301, mean_q: 3.896897
 44294/100000: episode: 927, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 25.227, mean reward: 2.523 [2.118, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.561, 10.100], loss: 0.088794, mae: 0.313824, mean_q: 3.908228
 44343/100000: episode: 928, duration: 0.254s, episode steps: 49, steps per second: 193, episode reward: 111.975, mean reward: 2.285 [1.543, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.789, 10.300], loss: 0.093679, mae: 0.311729, mean_q: 3.886151
 44344/100000: episode: 929, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 2.624, mean reward: 2.624 [2.624, 2.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.319, 10.100], loss: 0.172413, mae: 0.433046, mean_q: 3.924465
 44386/100000: episode: 930, duration: 0.322s, episode steps: 42, steps per second: 130, episode reward: 81.564, mean reward: 1.942 [1.537, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.317, 10.184], loss: 0.101836, mae: 0.316649, mean_q: 3.889657
 44435/100000: episode: 931, duration: 0.311s, episode steps: 49, steps per second: 158, episode reward: 93.254, mean reward: 1.903 [1.470, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.312, 10.352], loss: 0.108800, mae: 0.319480, mean_q: 3.895432
 44484/100000: episode: 932, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 96.277, mean reward: 1.965 [1.494, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.966, 10.100], loss: 0.097807, mae: 0.314135, mean_q: 3.862063
 44576/100000: episode: 933, duration: 0.490s, episode steps: 92, steps per second: 188, episode reward: 184.929, mean reward: 2.010 [1.463, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.847, 10.167], loss: 0.106623, mae: 0.311050, mean_q: 3.892998
 44618/100000: episode: 934, duration: 0.223s, episode steps: 42, steps per second: 189, episode reward: 90.346, mean reward: 2.151 [1.602, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.679, 10.100], loss: 0.106203, mae: 0.310207, mean_q: 3.874858
 44712/100000: episode: 935, duration: 0.497s, episode steps: 94, steps per second: 189, episode reward: 193.166, mean reward: 2.055 [1.476, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-1.243, 10.173], loss: 0.100070, mae: 0.314950, mean_q: 3.902808
 44806/100000: episode: 936, duration: 0.497s, episode steps: 94, steps per second: 189, episode reward: 182.081, mean reward: 1.937 [1.494, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.749, 10.100], loss: 0.099840, mae: 0.312119, mean_q: 3.874559
 44855/100000: episode: 937, duration: 0.295s, episode steps: 49, steps per second: 166, episode reward: 92.270, mean reward: 1.883 [1.500, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-1.072, 10.132], loss: 0.097058, mae: 0.312815, mean_q: 3.878277
 44897/100000: episode: 938, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 80.856, mean reward: 1.925 [1.502, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-1.247, 10.124], loss: 0.095297, mae: 0.310460, mean_q: 3.878870
 44901/100000: episode: 939, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 9.697, mean reward: 2.424 [1.696, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.334, 10.100], loss: 0.112478, mae: 0.384487, mean_q: 4.072502
 44950/100000: episode: 940, duration: 0.253s, episode steps: 49, steps per second: 193, episode reward: 107.646, mean reward: 2.197 [1.476, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.144, 10.284], loss: 0.130978, mae: 0.334727, mean_q: 3.875228
 44999/100000: episode: 941, duration: 0.244s, episode steps: 49, steps per second: 201, episode reward: 103.604, mean reward: 2.114 [1.615, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.150, 10.320], loss: 0.098555, mae: 0.317303, mean_q: 3.887430
 45041/100000: episode: 942, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 79.632, mean reward: 1.896 [1.478, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.180, 10.157], loss: 0.091560, mae: 0.299615, mean_q: 3.886991
 45083/100000: episode: 943, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 78.619, mean reward: 1.872 [1.431, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.607, 10.189], loss: 0.109625, mae: 0.321616, mean_q: 3.908972
 45177/100000: episode: 944, duration: 0.484s, episode steps: 94, steps per second: 194, episode reward: 183.018, mean reward: 1.947 [1.468, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.679, 10.259], loss: 0.105424, mae: 0.326733, mean_q: 3.892256
 45187/100000: episode: 945, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 30.366, mean reward: 3.037 [2.514, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.222, 10.100], loss: 0.105086, mae: 0.321393, mean_q: 3.829301
 45237/100000: episode: 946, duration: 0.247s, episode steps: 50, steps per second: 202, episode reward: 98.517, mean reward: 1.970 [1.523, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.777, 10.113], loss: 0.104626, mae: 0.318772, mean_q: 3.882527
 45332/100000: episode: 947, duration: 0.475s, episode steps: 95, steps per second: 200, episode reward: 186.278, mean reward: 1.961 [1.453, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.808, 10.100], loss: 0.096281, mae: 0.311998, mean_q: 3.879417
 45427/100000: episode: 948, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 192.933, mean reward: 2.031 [1.445, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.257, 10.100], loss: 0.102308, mae: 0.310537, mean_q: 3.868947
 45521/100000: episode: 949, duration: 0.762s, episode steps: 94, steps per second: 123, episode reward: 178.856, mean reward: 1.903 [1.460, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.432, 10.100], loss: 0.099705, mae: 0.308598, mean_q: 3.866164
 45613/100000: episode: 950, duration: 0.571s, episode steps: 92, steps per second: 161, episode reward: 171.965, mean reward: 1.869 [1.465, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.295, 10.100], loss: 0.099992, mae: 0.310740, mean_q: 3.885265
 45662/100000: episode: 951, duration: 0.292s, episode steps: 49, steps per second: 168, episode reward: 91.995, mean reward: 1.877 [1.502, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.466, 10.100], loss: 0.097758, mae: 0.310483, mean_q: 3.855409
 45663/100000: episode: 952, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 2.168, mean reward: 2.168 [2.168, 2.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.295, 10.100], loss: 0.071445, mae: 0.302709, mean_q: 3.881846
 45667/100000: episode: 953, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 9.079, mean reward: 2.270 [1.957, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.784, 10.100], loss: 0.076518, mae: 0.303506, mean_q: 3.893522
 45709/100000: episode: 954, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 75.284, mean reward: 1.792 [1.443, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.035, 10.100], loss: 0.101404, mae: 0.311915, mean_q: 3.853171
 45801/100000: episode: 955, duration: 0.480s, episode steps: 92, steps per second: 192, episode reward: 192.026, mean reward: 2.087 [1.522, 6.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.813, 10.378], loss: 0.093820, mae: 0.308453, mean_q: 3.855683
 45895/100000: episode: 956, duration: 0.505s, episode steps: 94, steps per second: 186, episode reward: 183.230, mean reward: 1.949 [1.465, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.211, 10.100], loss: 0.098562, mae: 0.311835, mean_q: 3.855683
 45905/100000: episode: 957, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 25.358, mean reward: 2.536 [2.193, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.150, 10.100], loss: 0.093747, mae: 0.293511, mean_q: 3.781097
 45954/100000: episode: 958, duration: 0.267s, episode steps: 49, steps per second: 183, episode reward: 84.815, mean reward: 1.731 [1.462, 2.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.181, 10.205], loss: 0.099410, mae: 0.315339, mean_q: 3.851032
 45958/100000: episode: 959, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 8.246, mean reward: 2.062 [1.759, 2.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.325, 10.100], loss: 0.086582, mae: 0.318696, mean_q: 3.871923
 46052/100000: episode: 960, duration: 0.498s, episode steps: 94, steps per second: 189, episode reward: 190.361, mean reward: 2.025 [1.452, 7.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.697, 10.255], loss: 0.097548, mae: 0.308984, mean_q: 3.833987
 46102/100000: episode: 961, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 126.358, mean reward: 2.527 [1.588, 5.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.265, 10.229], loss: 0.104975, mae: 0.323028, mean_q: 3.867405
 46197/100000: episode: 962, duration: 0.505s, episode steps: 95, steps per second: 188, episode reward: 183.767, mean reward: 1.934 [1.449, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.234, 10.362], loss: 0.091889, mae: 0.304816, mean_q: 3.845354
 46291/100000: episode: 963, duration: 0.494s, episode steps: 94, steps per second: 190, episode reward: 175.221, mean reward: 1.864 [1.498, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.976, 10.100], loss: 0.119168, mae: 0.334806, mean_q: 3.847957
 46295/100000: episode: 964, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 8.885, mean reward: 2.221 [1.889, 2.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.984, 10.100], loss: 0.097910, mae: 0.273957, mean_q: 3.857884
 46389/100000: episode: 965, duration: 0.524s, episode steps: 94, steps per second: 179, episode reward: 184.318, mean reward: 1.961 [1.469, 4.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-1.058, 10.219], loss: 0.103651, mae: 0.317562, mean_q: 3.853254
 46390/100000: episode: 966, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.558, mean reward: 2.558 [2.558, 2.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.270, 10.100], loss: 0.082286, mae: 0.333901, mean_q: 3.871195
 46400/100000: episode: 967, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 26.269, mean reward: 2.627 [2.260, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.293, 10.100], loss: 0.085919, mae: 0.303972, mean_q: 3.837879
 46492/100000: episode: 968, duration: 0.496s, episode steps: 92, steps per second: 185, episode reward: 185.413, mean reward: 2.015 [1.467, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-1.194, 10.181], loss: 0.104235, mae: 0.310600, mean_q: 3.864029
 46587/100000: episode: 969, duration: 0.592s, episode steps: 95, steps per second: 160, episode reward: 209.753, mean reward: 2.208 [1.530, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.943, 10.313], loss: 0.107301, mae: 0.317247, mean_q: 3.866373
 46637/100000: episode: 970, duration: 0.289s, episode steps: 50, steps per second: 173, episode reward: 100.197, mean reward: 2.004 [1.533, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.752, 10.328], loss: 0.104723, mae: 0.312070, mean_q: 3.891721
 46731/100000: episode: 971, duration: 0.522s, episode steps: 94, steps per second: 180, episode reward: 175.330, mean reward: 1.865 [1.448, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.550, 10.100], loss: 0.094465, mae: 0.306307, mean_q: 3.863359
 46825/100000: episode: 972, duration: 0.514s, episode steps: 94, steps per second: 183, episode reward: 183.277, mean reward: 1.950 [1.443, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-1.139, 10.100], loss: 0.124662, mae: 0.326305, mean_q: 3.872123
 46829/100000: episode: 973, duration: 0.038s, episode steps: 4, steps per second: 104, episode reward: 8.925, mean reward: 2.231 [2.201, 2.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.415, 10.100], loss: 0.162807, mae: 0.359435, mean_q: 3.929298
 46923/100000: episode: 974, duration: 0.475s, episode steps: 94, steps per second: 198, episode reward: 179.047, mean reward: 1.905 [1.451, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.845, 10.214], loss: 0.105514, mae: 0.309825, mean_q: 3.870690
 47017/100000: episode: 975, duration: 0.460s, episode steps: 94, steps per second: 205, episode reward: 186.972, mean reward: 1.989 [1.498, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-1.289, 10.122], loss: 0.109518, mae: 0.311288, mean_q: 3.864708
 47021/100000: episode: 976, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 9.766, mean reward: 2.441 [2.170, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.340, 10.100], loss: 0.138912, mae: 0.313566, mean_q: 3.882689
 47070/100000: episode: 977, duration: 0.342s, episode steps: 49, steps per second: 143, episode reward: 111.176, mean reward: 2.269 [1.524, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.602, 10.404], loss: 0.096003, mae: 0.313995, mean_q: 3.883505
 47164/100000: episode: 978, duration: 0.605s, episode steps: 94, steps per second: 155, episode reward: 187.129, mean reward: 1.991 [1.469, 4.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.633, 10.206], loss: 0.115440, mae: 0.323180, mean_q: 3.875697
 47174/100000: episode: 979, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 24.382, mean reward: 2.438 [2.252, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.247, 10.100], loss: 0.119747, mae: 0.327399, mean_q: 3.896042
[Info] 2-TH LEVEL FOUND: 5.370377540588379, Considering 10/90 traces
 47175/100000: episode: 980, duration: 4.362s, episode steps: 1, steps per second: 0, episode reward: 2.271, mean reward: 2.271 [2.271, 2.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.466, 10.100], loss: 0.030163, mae: 0.218216, mean_q: 3.894908
 47184/100000: episode: 981, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 22.143, mean reward: 2.460 [2.088, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.366, 10.100], loss: 0.139933, mae: 0.305321, mean_q: 3.864300
 47193/100000: episode: 982, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 26.303, mean reward: 2.923 [2.459, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.379, 10.100], loss: 0.085342, mae: 0.309537, mean_q: 3.899974
 47202/100000: episode: 983, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 27.863, mean reward: 3.096 [2.575, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.332, 10.100], loss: 0.099929, mae: 0.308670, mean_q: 3.854228
 47211/100000: episode: 984, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 23.854, mean reward: 2.650 [2.360, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.387, 10.100], loss: 0.074280, mae: 0.284180, mean_q: 3.816246
 47220/100000: episode: 985, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 26.987, mean reward: 2.999 [2.303, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.339, 10.100], loss: 0.103281, mae: 0.315715, mean_q: 3.934241
 47265/100000: episode: 986, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 105.477, mean reward: 2.344 [1.649, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.508, 10.320], loss: 0.093963, mae: 0.306421, mean_q: 3.884759
 47274/100000: episode: 987, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 26.086, mean reward: 2.898 [2.373, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.264, 10.100], loss: 0.101228, mae: 0.297314, mean_q: 3.938897
 47312/100000: episode: 988, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 85.728, mean reward: 2.256 [1.578, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.550, 10.271], loss: 0.095121, mae: 0.309732, mean_q: 3.900480
 47321/100000: episode: 989, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 25.382, mean reward: 2.820 [2.373, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.342, 10.100], loss: 0.114688, mae: 0.341160, mean_q: 3.936924
 47330/100000: episode: 990, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 26.917, mean reward: 2.991 [2.546, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.325, 10.100], loss: 0.078264, mae: 0.286785, mean_q: 3.898158
 47375/100000: episode: 991, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 96.037, mean reward: 2.134 [1.693, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.322, 10.206], loss: 0.101830, mae: 0.312891, mean_q: 3.898804
 47384/100000: episode: 992, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 29.871, mean reward: 3.319 [2.367, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.367, 10.100], loss: 0.165503, mae: 0.343334, mean_q: 3.871518
 47393/100000: episode: 993, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 38.244, mean reward: 4.249 [3.287, 6.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.848, 10.100], loss: 0.096093, mae: 0.317630, mean_q: 3.819951
 47402/100000: episode: 994, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 30.108, mean reward: 3.345 [2.945, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.472, 10.100], loss: 0.125543, mae: 0.369870, mean_q: 4.024379
 47411/100000: episode: 995, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 30.136, mean reward: 3.348 [2.864, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.407, 10.100], loss: 0.070221, mae: 0.282007, mean_q: 3.778333
 47420/100000: episode: 996, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 33.522, mean reward: 3.725 [3.146, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.345, 10.100], loss: 0.106788, mae: 0.302829, mean_q: 3.841375
 47429/100000: episode: 997, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 24.225, mean reward: 2.692 [1.932, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.417, 10.100], loss: 0.154263, mae: 0.379749, mean_q: 4.051731
 47474/100000: episode: 998, duration: 0.256s, episode steps: 45, steps per second: 176, episode reward: 89.371, mean reward: 1.986 [1.498, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.151, 10.100], loss: 0.120293, mae: 0.326727, mean_q: 3.915144
 47483/100000: episode: 999, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.545, mean reward: 2.838 [2.220, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.410, 10.100], loss: 0.090851, mae: 0.309066, mean_q: 3.921756
 47492/100000: episode: 1000, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 22.617, mean reward: 2.513 [2.052, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.719, 10.100], loss: 0.167486, mae: 0.386363, mean_q: 3.871572
 47501/100000: episode: 1001, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 22.724, mean reward: 2.525 [2.125, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.267, 10.100], loss: 0.099626, mae: 0.316875, mean_q: 3.844154
 47510/100000: episode: 1002, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 24.679, mean reward: 2.742 [2.196, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.241, 10.100], loss: 0.104610, mae: 0.338734, mean_q: 3.911664
 47519/100000: episode: 1003, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 27.713, mean reward: 3.079 [2.261, 4.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.291, 10.100], loss: 0.118451, mae: 0.333425, mean_q: 4.018545
 47528/100000: episode: 1004, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 24.169, mean reward: 2.685 [2.378, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.223, 10.100], loss: 0.089874, mae: 0.308686, mean_q: 3.844450
 47537/100000: episode: 1005, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 28.126, mean reward: 3.125 [2.586, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.339, 10.100], loss: 0.148898, mae: 0.374822, mean_q: 4.021480
 47546/100000: episode: 1006, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 19.247, mean reward: 2.139 [1.952, 2.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.578, 10.100], loss: 0.140510, mae: 0.361466, mean_q: 3.993325
 47555/100000: episode: 1007, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 23.968, mean reward: 2.663 [2.320, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.284, 10.100], loss: 0.100496, mae: 0.320640, mean_q: 3.952577
 47600/100000: episode: 1008, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 97.296, mean reward: 2.162 [1.571, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.940, 10.302], loss: 0.122560, mae: 0.336461, mean_q: 3.932809
 47645/100000: episode: 1009, duration: 0.337s, episode steps: 45, steps per second: 134, episode reward: 86.184, mean reward: 1.915 [1.496, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.083, 10.186], loss: 0.117988, mae: 0.340547, mean_q: 3.937511
 47654/100000: episode: 1010, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 24.176, mean reward: 2.686 [2.322, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.241, 10.100], loss: 0.105119, mae: 0.336104, mean_q: 4.007135
 47663/100000: episode: 1011, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 23.777, mean reward: 2.642 [2.301, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.521, 10.100], loss: 0.094573, mae: 0.326585, mean_q: 3.988203
 47672/100000: episode: 1012, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 27.992, mean reward: 3.110 [2.685, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.497, 10.100], loss: 0.107713, mae: 0.315407, mean_q: 3.961793
 47681/100000: episode: 1013, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 26.637, mean reward: 2.960 [2.017, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.352, 10.100], loss: 0.148977, mae: 0.343436, mean_q: 3.896626
 47690/100000: episode: 1014, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 26.173, mean reward: 2.908 [2.657, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.246, 10.100], loss: 0.146009, mae: 0.353938, mean_q: 4.051478
 47735/100000: episode: 1015, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 115.824, mean reward: 2.574 [1.935, 8.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.061, 10.364], loss: 0.149804, mae: 0.358265, mean_q: 4.024213
 47744/100000: episode: 1016, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.437, mean reward: 2.826 [2.501, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.719, 10.100], loss: 0.085723, mae: 0.302170, mean_q: 3.954623
 47753/100000: episode: 1017, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 20.596, mean reward: 2.288 [1.997, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.183, 10.100], loss: 0.187044, mae: 0.378232, mean_q: 4.037329
 47762/100000: episode: 1018, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 26.709, mean reward: 2.968 [2.170, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.083, 10.100], loss: 0.085826, mae: 0.310949, mean_q: 4.013595
 47771/100000: episode: 1019, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 29.011, mean reward: 3.223 [2.235, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.363, 10.100], loss: 0.095191, mae: 0.299878, mean_q: 3.889822
 47780/100000: episode: 1020, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 30.885, mean reward: 3.432 [2.963, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.407, 10.100], loss: 0.153451, mae: 0.358287, mean_q: 4.053148
 47789/100000: episode: 1021, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 25.536, mean reward: 2.837 [2.011, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.855, 10.100], loss: 0.103023, mae: 0.298650, mean_q: 3.907793
 47798/100000: episode: 1022, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 22.026, mean reward: 2.447 [1.857, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.440, 10.100], loss: 0.105021, mae: 0.320708, mean_q: 3.980186
 47807/100000: episode: 1023, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 29.648, mean reward: 3.294 [2.634, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.278, 10.100], loss: 0.096663, mae: 0.311508, mean_q: 3.986797
 47816/100000: episode: 1024, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 20.591, mean reward: 2.288 [1.979, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.583, 10.100], loss: 0.102653, mae: 0.331899, mean_q: 4.054931
 47825/100000: episode: 1025, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 36.031, mean reward: 4.003 [2.987, 5.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.409, 10.100], loss: 0.110672, mae: 0.327643, mean_q: 4.015520
 47834/100000: episode: 1026, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 24.390, mean reward: 2.710 [2.428, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.355, 10.100], loss: 0.134289, mae: 0.342325, mean_q: 3.955099
 47843/100000: episode: 1027, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 35.016, mean reward: 3.891 [2.626, 6.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.410, 10.100], loss: 0.139810, mae: 0.397027, mean_q: 4.047033
 47881/100000: episode: 1028, duration: 0.227s, episode steps: 38, steps per second: 168, episode reward: 101.290, mean reward: 2.666 [1.791, 4.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.670, 10.339], loss: 0.144346, mae: 0.357218, mean_q: 4.036769
 47890/100000: episode: 1029, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 23.527, mean reward: 2.614 [2.252, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.616, 10.100], loss: 0.117802, mae: 0.333797, mean_q: 4.016770
 47899/100000: episode: 1030, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 23.406, mean reward: 2.601 [2.234, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.006, 10.100], loss: 0.103454, mae: 0.325946, mean_q: 4.029509
 47908/100000: episode: 1031, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 24.578, mean reward: 2.731 [2.485, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.457, 10.100], loss: 0.107653, mae: 0.340049, mean_q: 3.982377
 47917/100000: episode: 1032, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 22.824, mean reward: 2.536 [2.269, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.263, 10.100], loss: 0.147216, mae: 0.372737, mean_q: 4.048409
 47955/100000: episode: 1033, duration: 0.186s, episode steps: 38, steps per second: 205, episode reward: 82.120, mean reward: 2.161 [1.728, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.499, 10.240], loss: 0.136768, mae: 0.358511, mean_q: 4.057681
 47964/100000: episode: 1034, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 23.893, mean reward: 2.655 [2.205, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.334, 10.100], loss: 0.128348, mae: 0.352911, mean_q: 4.029396
 47973/100000: episode: 1035, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 26.332, mean reward: 2.926 [2.492, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.273, 10.100], loss: 0.132430, mae: 0.348175, mean_q: 4.008277
 47982/100000: episode: 1036, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 23.580, mean reward: 2.620 [2.332, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.320, 10.100], loss: 0.108179, mae: 0.327556, mean_q: 4.024872
 47991/100000: episode: 1037, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 25.063, mean reward: 2.785 [2.511, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.597, 10.100], loss: 0.165808, mae: 0.356980, mean_q: 4.096335
 48000/100000: episode: 1038, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 30.170, mean reward: 3.352 [2.572, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.362, 10.100], loss: 0.127299, mae: 0.361052, mean_q: 3.973273
 48009/100000: episode: 1039, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 23.401, mean reward: 2.600 [2.193, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.340, 10.100], loss: 0.168262, mae: 0.379036, mean_q: 4.045643
 48018/100000: episode: 1040, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 23.808, mean reward: 2.645 [2.396, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.409, 10.100], loss: 0.133736, mae: 0.379924, mean_q: 4.139389
 48027/100000: episode: 1041, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 33.973, mean reward: 3.775 [3.265, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.423, 10.100], loss: 0.149235, mae: 0.377770, mean_q: 4.095107
 48036/100000: episode: 1042, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 21.607, mean reward: 2.401 [2.078, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.325, 10.100], loss: 0.261441, mae: 0.448045, mean_q: 4.046599
 48045/100000: episode: 1043, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 25.363, mean reward: 2.818 [2.303, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.456, 10.100], loss: 0.130701, mae: 0.357309, mean_q: 3.979349
 48054/100000: episode: 1044, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 30.913, mean reward: 3.435 [2.926, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.323, 10.100], loss: 0.103289, mae: 0.335427, mean_q: 4.113733
 48063/100000: episode: 1045, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 22.604, mean reward: 2.512 [2.138, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.221, 10.100], loss: 0.171596, mae: 0.389252, mean_q: 4.167856
 48072/100000: episode: 1046, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 24.127, mean reward: 2.681 [2.224, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.345, 10.100], loss: 0.134351, mae: 0.347423, mean_q: 3.986661
 48081/100000: episode: 1047, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 21.847, mean reward: 2.427 [2.146, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.210, 10.100], loss: 0.150048, mae: 0.395558, mean_q: 4.176311
 48090/100000: episode: 1048, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 28.369, mean reward: 3.152 [2.562, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.487, 10.100], loss: 0.134847, mae: 0.349874, mean_q: 4.017113
 48099/100000: episode: 1049, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 26.557, mean reward: 2.951 [2.558, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.408, 10.100], loss: 0.142673, mae: 0.372492, mean_q: 4.100562
 48108/100000: episode: 1050, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 25.901, mean reward: 2.878 [2.326, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.563, 10.100], loss: 0.133353, mae: 0.356431, mean_q: 4.126574
 48153/100000: episode: 1051, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 100.931, mean reward: 2.243 [1.596, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.995, 10.313], loss: 0.130388, mae: 0.345922, mean_q: 4.075238
 48162/100000: episode: 1052, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 23.354, mean reward: 2.595 [2.079, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.286, 10.100], loss: 0.144752, mae: 0.341187, mean_q: 4.065596
 48171/100000: episode: 1053, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 26.431, mean reward: 2.937 [2.665, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.505, 10.100], loss: 0.127435, mae: 0.357721, mean_q: 4.038052
 48180/100000: episode: 1054, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 23.035, mean reward: 2.559 [2.242, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.257, 10.100], loss: 0.111933, mae: 0.345231, mean_q: 4.136988
 48189/100000: episode: 1055, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 25.893, mean reward: 2.877 [2.601, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.397, 10.100], loss: 0.147967, mae: 0.374788, mean_q: 4.117866
 48234/100000: episode: 1056, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 98.582, mean reward: 2.191 [1.625, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.467, 10.365], loss: 0.144961, mae: 0.380891, mean_q: 4.118306
 48243/100000: episode: 1057, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 21.794, mean reward: 2.422 [2.132, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.379, 10.100], loss: 0.121377, mae: 0.361316, mean_q: 4.182028
 48252/100000: episode: 1058, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 28.956, mean reward: 3.217 [2.598, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.500, 10.100], loss: 0.141578, mae: 0.352530, mean_q: 4.071047
 48261/100000: episode: 1059, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 25.046, mean reward: 2.783 [2.422, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.312, 10.100], loss: 0.146385, mae: 0.376169, mean_q: 4.041139
 48270/100000: episode: 1060, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 26.085, mean reward: 2.898 [2.366, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.468, 10.100], loss: 0.111591, mae: 0.357999, mean_q: 4.189940
 48279/100000: episode: 1061, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 20.818, mean reward: 2.313 [2.051, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.892, 10.100], loss: 0.214310, mae: 0.362953, mean_q: 4.051268
 48317/100000: episode: 1062, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 83.099, mean reward: 2.187 [1.694, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.290, 10.272], loss: 0.135238, mae: 0.354297, mean_q: 4.092877
 48326/100000: episode: 1063, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 25.310, mean reward: 2.812 [2.229, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.259, 10.100], loss: 0.097723, mae: 0.312014, mean_q: 4.022481
 48364/100000: episode: 1064, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 82.512, mean reward: 2.171 [1.482, 5.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.419, 10.156], loss: 0.162912, mae: 0.371540, mean_q: 4.124362
 48373/100000: episode: 1065, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 24.513, mean reward: 2.724 [2.279, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.348, 10.100], loss: 0.158397, mae: 0.353066, mean_q: 4.133339
 48382/100000: episode: 1066, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 26.658, mean reward: 2.962 [2.575, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.331, 10.100], loss: 0.163864, mae: 0.377865, mean_q: 4.155520
 48391/100000: episode: 1067, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 23.713, mean reward: 2.635 [2.232, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.459, 10.100], loss: 0.123609, mae: 0.343256, mean_q: 3.998015
 48400/100000: episode: 1068, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 23.481, mean reward: 2.609 [2.389, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.302, 10.100], loss: 0.103061, mae: 0.359487, mean_q: 4.091778
 48409/100000: episode: 1069, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 25.826, mean reward: 2.870 [2.304, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.263, 10.100], loss: 0.118821, mae: 0.352669, mean_q: 4.198858
[Info] 3-TH LEVEL FOUND: 5.9378790855407715, Considering 10/90 traces
 48418/100000: episode: 1070, duration: 4.542s, episode steps: 9, steps per second: 2, episode reward: 22.704, mean reward: 2.523 [2.228, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.476, 10.100], loss: 0.145119, mae: 0.363663, mean_q: 4.142744
 48425/100000: episode: 1071, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 21.505, mean reward: 3.072 [2.865, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.615, 10.100], loss: 0.115437, mae: 0.340497, mean_q: 4.145244
 48432/100000: episode: 1072, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 19.800, mean reward: 2.829 [2.416, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.506, 10.100], loss: 0.083027, mae: 0.298932, mean_q: 4.172768
 48440/100000: episode: 1073, duration: 0.047s, episode steps: 8, steps per second: 168, episode reward: 26.717, mean reward: 3.340 [3.139, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.466, 10.100], loss: 0.124513, mae: 0.361900, mean_q: 4.040948
 48448/100000: episode: 1074, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 26.334, mean reward: 3.292 [2.927, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.426, 10.100], loss: 0.181926, mae: 0.378678, mean_q: 4.201179
 48455/100000: episode: 1075, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 22.880, mean reward: 3.269 [2.791, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.726, 10.100], loss: 0.104406, mae: 0.342395, mean_q: 4.111129
 48463/100000: episode: 1076, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 25.635, mean reward: 3.204 [2.957, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.400, 10.100], loss: 0.219848, mae: 0.456527, mean_q: 4.216908
 48471/100000: episode: 1077, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 26.434, mean reward: 3.304 [3.119, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.323, 10.100], loss: 0.209472, mae: 0.375243, mean_q: 3.988366
 48478/100000: episode: 1078, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 25.327, mean reward: 3.618 [3.122, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.401, 10.100], loss: 0.126141, mae: 0.386344, mean_q: 4.206501
 48486/100000: episode: 1079, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 25.027, mean reward: 3.128 [2.667, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.454, 10.100], loss: 0.122119, mae: 0.361193, mean_q: 4.095620
 48493/100000: episode: 1080, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 34.731, mean reward: 4.962 [3.356, 6.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.874, 10.100], loss: 0.104271, mae: 0.316694, mean_q: 4.046059
 48501/100000: episode: 1081, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 38.612, mean reward: 4.827 [3.180, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.358, 10.100], loss: 0.138121, mae: 0.407093, mean_q: 4.233552
 48509/100000: episode: 1082, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 25.984, mean reward: 3.248 [2.788, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.559, 10.100], loss: 0.120181, mae: 0.345132, mean_q: 4.079844
 48517/100000: episode: 1083, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 23.698, mean reward: 2.962 [2.254, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.457, 10.100], loss: 0.099663, mae: 0.326486, mean_q: 4.166651
 48525/100000: episode: 1084, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 29.608, mean reward: 3.701 [2.965, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.314, 10.100], loss: 0.146285, mae: 0.379180, mean_q: 4.189038
 48533/100000: episode: 1085, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 32.157, mean reward: 4.020 [2.929, 5.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.439, 10.100], loss: 0.195626, mae: 0.410224, mean_q: 4.181617
 48540/100000: episode: 1086, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 20.471, mean reward: 2.924 [2.736, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.508, 10.100], loss: 0.185215, mae: 0.383412, mean_q: 4.120053
 48547/100000: episode: 1087, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 20.993, mean reward: 2.999 [2.491, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.417, 10.100], loss: 0.152672, mae: 0.381622, mean_q: 4.210771
 48554/100000: episode: 1088, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 24.492, mean reward: 3.499 [3.082, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.592, 10.100], loss: 0.092586, mae: 0.302889, mean_q: 4.071334
 48562/100000: episode: 1089, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 34.795, mean reward: 4.349 [3.637, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.336, 10.100], loss: 0.168351, mae: 0.384842, mean_q: 4.201672
 48570/100000: episode: 1090, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 29.598, mean reward: 3.700 [2.770, 5.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.362, 10.100], loss: 0.222351, mae: 0.381845, mean_q: 4.141546
 48578/100000: episode: 1091, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 26.742, mean reward: 3.343 [2.705, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.459, 10.100], loss: 0.121391, mae: 0.333381, mean_q: 4.148192
 48586/100000: episode: 1092, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 40.301, mean reward: 5.038 [2.948, 6.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.343, 10.100], loss: 0.126434, mae: 0.359174, mean_q: 4.114787
 48593/100000: episode: 1093, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 23.693, mean reward: 3.385 [2.989, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.431, 10.100], loss: 0.226891, mae: 0.425589, mean_q: 4.214110
 48600/100000: episode: 1094, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 26.376, mean reward: 3.768 [3.364, 4.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.375, 10.100], loss: 0.140821, mae: 0.342897, mean_q: 4.101962
 48608/100000: episode: 1095, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 33.214, mean reward: 4.152 [2.836, 6.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.750, 10.100], loss: 0.169839, mae: 0.371631, mean_q: 4.183983
 48616/100000: episode: 1096, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 69.517, mean reward: 8.690 [3.923, 14.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.555, 10.100], loss: 0.196813, mae: 0.387967, mean_q: 4.151616
 48623/100000: episode: 1097, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 23.698, mean reward: 3.385 [2.754, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.419, 10.100], loss: 0.123123, mae: 0.368285, mean_q: 4.175864
 48630/100000: episode: 1098, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 25.742, mean reward: 3.677 [2.707, 5.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.471, 10.100], loss: 0.138840, mae: 0.364938, mean_q: 4.200281
 48637/100000: episode: 1099, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 19.172, mean reward: 2.739 [2.141, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.324, 10.100], loss: 0.138534, mae: 0.384765, mean_q: 4.192454
 48644/100000: episode: 1100, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 23.792, mean reward: 3.399 [2.898, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.443, 10.100], loss: 0.176141, mae: 0.406510, mean_q: 4.153768
 48652/100000: episode: 1101, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 27.775, mean reward: 3.472 [3.022, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.534, 10.100], loss: 0.148707, mae: 0.370355, mean_q: 4.159637
 48660/100000: episode: 1102, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 23.002, mean reward: 2.875 [2.530, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.373, 10.100], loss: 0.134571, mae: 0.351927, mean_q: 4.134679
 48667/100000: episode: 1103, duration: 0.035s, episode steps: 7, steps per second: 198, episode reward: 26.969, mean reward: 3.853 [2.786, 5.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.385, 10.100], loss: 0.405341, mae: 0.463553, mean_q: 4.428936
 48674/100000: episode: 1104, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 27.124, mean reward: 3.875 [2.970, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.426, 10.100], loss: 0.395929, mae: 0.448054, mean_q: 4.181122
 48681/100000: episode: 1105, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 25.710, mean reward: 3.673 [2.400, 5.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.440, 10.100], loss: 0.159679, mae: 0.386316, mean_q: 4.166908
 48689/100000: episode: 1106, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 30.500, mean reward: 3.813 [3.261, 4.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.442, 10.100], loss: 0.145823, mae: 0.427557, mean_q: 4.466979
 48696/100000: episode: 1107, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 20.141, mean reward: 2.877 [2.551, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.525, 10.100], loss: 0.119068, mae: 0.350087, mean_q: 4.114696
 48703/100000: episode: 1108, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 20.600, mean reward: 2.943 [2.533, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.743, 10.100], loss: 0.117418, mae: 0.366808, mean_q: 4.305104
 48710/100000: episode: 1109, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 31.568, mean reward: 4.510 [3.530, 7.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.502, 10.100], loss: 0.491489, mae: 0.435159, mean_q: 4.305784
 48717/100000: episode: 1110, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 19.782, mean reward: 2.826 [2.620, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.222, 10.100], loss: 0.233274, mae: 0.387347, mean_q: 4.178899
 48725/100000: episode: 1111, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 31.556, mean reward: 3.944 [2.765, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.391, 10.100], loss: 0.174355, mae: 0.403533, mean_q: 4.261210
 48733/100000: episode: 1112, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 35.245, mean reward: 4.406 [3.255, 5.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.520, 10.100], loss: 0.124071, mae: 0.359201, mean_q: 4.221004
 48741/100000: episode: 1113, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 59.952, mean reward: 7.494 [5.124, 22.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.553, 10.100], loss: 0.251899, mae: 0.457311, mean_q: 4.410193
 48749/100000: episode: 1114, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 31.290, mean reward: 3.911 [2.619, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.534, 10.100], loss: 0.212978, mae: 0.491498, mean_q: 4.313750
 48757/100000: episode: 1115, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 26.584, mean reward: 3.323 [2.610, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.971, 10.100], loss: 0.151791, mae: 0.377478, mean_q: 4.205861
 48765/100000: episode: 1116, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 35.772, mean reward: 4.471 [3.549, 5.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.360, 10.100], loss: 0.111218, mae: 0.339679, mean_q: 4.206131
 48773/100000: episode: 1117, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 25.482, mean reward: 3.185 [2.341, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.918, 10.100], loss: 0.226471, mae: 0.439451, mean_q: 4.356739
 48780/100000: episode: 1118, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 16.185, mean reward: 2.312 [1.941, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.352, 10.100], loss: 0.241730, mae: 0.430089, mean_q: 4.227566
 48788/100000: episode: 1119, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 22.761, mean reward: 2.845 [2.684, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.289, 10.100], loss: 0.217237, mae: 0.444026, mean_q: 4.334898
 48795/100000: episode: 1120, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 26.588, mean reward: 3.798 [3.406, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.341, 10.100], loss: 0.429091, mae: 0.449120, mean_q: 4.316907
 48802/100000: episode: 1121, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 26.295, mean reward: 3.756 [2.995, 5.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.427, 10.100], loss: 0.158184, mae: 0.410353, mean_q: 4.347505
 48810/100000: episode: 1122, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 25.052, mean reward: 3.132 [2.872, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.498, 10.100], loss: 0.166528, mae: 0.424260, mean_q: 4.337100
 48818/100000: episode: 1123, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 26.759, mean reward: 3.345 [2.733, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.349, 10.100], loss: 0.899478, mae: 0.542370, mean_q: 4.456409
 48825/100000: episode: 1124, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 30.081, mean reward: 4.297 [3.591, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.387, 10.100], loss: 0.226216, mae: 0.457665, mean_q: 4.071035
 48832/100000: episode: 1125, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 19.234, mean reward: 2.748 [2.398, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.652, 10.100], loss: 0.257311, mae: 0.517259, mean_q: 4.547246
 48839/100000: episode: 1126, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 20.186, mean reward: 2.884 [2.648, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.493, 10.100], loss: 0.229168, mae: 0.427642, mean_q: 4.163368
 48847/100000: episode: 1127, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 27.229, mean reward: 3.404 [3.048, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.477, 10.100], loss: 0.354216, mae: 0.429104, mean_q: 4.439580
 48855/100000: episode: 1128, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 29.510, mean reward: 3.689 [2.375, 6.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.488, 10.100], loss: 0.226383, mae: 0.461564, mean_q: 4.286796
 48862/100000: episode: 1129, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 22.793, mean reward: 3.256 [2.965, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.424, 10.100], loss: 0.180880, mae: 0.406909, mean_q: 4.331519
 48869/100000: episode: 1130, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 25.914, mean reward: 3.702 [3.035, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.423, 10.100], loss: 0.150350, mae: 0.387789, mean_q: 4.399159
 48876/100000: episode: 1131, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 33.843, mean reward: 4.835 [3.098, 6.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.362, 10.100], loss: 0.266500, mae: 0.485548, mean_q: 4.443477
 48883/100000: episode: 1132, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 27.075, mean reward: 3.868 [3.279, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.389, 10.100], loss: 0.157948, mae: 0.397948, mean_q: 4.330374
 48890/100000: episode: 1133, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 20.654, mean reward: 2.951 [2.786, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.191, 10.100], loss: 0.159081, mae: 0.418921, mean_q: 4.479409
 48898/100000: episode: 1134, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 21.754, mean reward: 2.719 [2.277, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.300, 10.100], loss: 0.174633, mae: 0.421304, mean_q: 4.336253
 48906/100000: episode: 1135, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 20.955, mean reward: 2.619 [2.222, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.295, 10.100], loss: 0.147485, mae: 0.382778, mean_q: 4.300777
 48914/100000: episode: 1136, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 38.375, mean reward: 4.797 [3.243, 6.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.447, 10.100], loss: 0.201845, mae: 0.420032, mean_q: 4.479285
 48922/100000: episode: 1137, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 41.246, mean reward: 5.156 [3.383, 9.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.372, 10.100], loss: 0.148926, mae: 0.386110, mean_q: 4.400557
 48930/100000: episode: 1138, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 21.980, mean reward: 2.747 [2.309, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.364, 10.100], loss: 0.201689, mae: 0.471708, mean_q: 4.510455
 48938/100000: episode: 1139, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 27.863, mean reward: 3.483 [2.834, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.413, 10.100], loss: 0.156359, mae: 0.394443, mean_q: 4.324578
 48946/100000: episode: 1140, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 22.185, mean reward: 2.773 [2.316, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.264, 10.100], loss: 0.142871, mae: 0.371476, mean_q: 4.473980
 48953/100000: episode: 1141, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 24.868, mean reward: 3.553 [2.552, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.268, 10.100], loss: 0.445086, mae: 0.462277, mean_q: 4.213487
 48961/100000: episode: 1142, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 23.432, mean reward: 2.929 [2.691, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.315, 10.100], loss: 0.299075, mae: 0.537694, mean_q: 4.521131
 48968/100000: episode: 1143, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 25.908, mean reward: 3.701 [3.073, 4.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.399, 10.100], loss: 0.274799, mae: 0.481182, mean_q: 4.433257
 48976/100000: episode: 1144, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 26.231, mean reward: 3.279 [2.676, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.388, 10.100], loss: 0.258494, mae: 0.480953, mean_q: 4.345065
 48983/100000: episode: 1145, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 27.432, mean reward: 3.919 [3.349, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.409, 10.100], loss: 0.166523, mae: 0.426806, mean_q: 4.521138
 48990/100000: episode: 1146, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 26.179, mean reward: 3.740 [3.373, 4.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.519, 10.100], loss: 0.280331, mae: 0.429606, mean_q: 4.274773
 48997/100000: episode: 1147, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 21.100, mean reward: 3.014 [2.359, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.284, 10.100], loss: 0.265152, mae: 0.462047, mean_q: 4.480558
 49005/100000: episode: 1148, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 26.699, mean reward: 3.337 [2.699, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.446, 10.100], loss: 0.124095, mae: 0.358555, mean_q: 4.419606
 49013/100000: episode: 1149, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 35.165, mean reward: 4.396 [2.984, 6.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.524, 10.100], loss: 0.221255, mae: 0.406324, mean_q: 4.345838
 49021/100000: episode: 1150, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 32.118, mean reward: 4.015 [2.592, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.436, 10.100], loss: 0.211381, mae: 0.450881, mean_q: 4.653492
 49028/100000: episode: 1151, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 22.516, mean reward: 3.217 [2.832, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.357, 10.100], loss: 0.173900, mae: 0.410312, mean_q: 4.375771
 49035/100000: episode: 1152, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 23.841, mean reward: 3.406 [2.742, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.426, 10.100], loss: 0.484913, mae: 0.501569, mean_q: 4.509447
 49042/100000: episode: 1153, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 25.688, mean reward: 3.670 [2.647, 5.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.377, 10.100], loss: 0.175632, mae: 0.411500, mean_q: 4.409560
 49050/100000: episode: 1154, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 28.103, mean reward: 3.513 [2.817, 5.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.273, 10.100], loss: 0.233221, mae: 0.442408, mean_q: 4.571159
 49058/100000: episode: 1155, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 30.546, mean reward: 3.818 [3.199, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.496, 10.100], loss: 0.174084, mae: 0.414440, mean_q: 4.545662
 49066/100000: episode: 1156, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 37.839, mean reward: 4.730 [2.938, 7.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.301, 10.100], loss: 0.295815, mae: 0.443162, mean_q: 4.380897
 49073/100000: episode: 1157, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 18.781, mean reward: 2.683 [2.196, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.341, 10.100], loss: 0.257016, mae: 0.476347, mean_q: 4.421187
 49080/100000: episode: 1158, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 21.250, mean reward: 3.036 [2.699, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.369, 10.100], loss: 0.158381, mae: 0.432766, mean_q: 4.512023
 49087/100000: episode: 1159, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 30.587, mean reward: 4.370 [3.011, 5.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.475, 10.100], loss: 0.492919, mae: 0.496340, mean_q: 4.221252
[Info] 4-TH LEVEL FOUND: 7.0035247802734375, Considering 10/90 traces
 49095/100000: episode: 1160, duration: 4.174s, episode steps: 8, steps per second: 2, episode reward: 47.796, mean reward: 5.974 [3.859, 7.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.260, 10.100], loss: 0.191364, mae: 0.418947, mean_q: 4.378022
 49100/100000: episode: 1161, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 29.654, mean reward: 5.931 [4.452, 8.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.380, 10.100], loss: 0.287102, mae: 0.483246, mean_q: 4.446452
 49106/100000: episode: 1162, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 35.120, mean reward: 5.853 [4.144, 11.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.478, 10.100], loss: 0.209131, mae: 0.470443, mean_q: 4.625379
 49111/100000: episode: 1163, duration: 0.027s, episode steps: 5, steps per second: 188, episode reward: 30.267, mean reward: 6.053 [5.392, 6.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.348, 10.100], loss: 0.137345, mae: 0.384319, mean_q: 4.452271
 49116/100000: episode: 1164, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 21.721, mean reward: 4.344 [3.725, 5.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.501, 10.100], loss: 0.184037, mae: 0.439460, mean_q: 4.450402
 49121/100000: episode: 1165, duration: 0.038s, episode steps: 5, steps per second: 133, episode reward: 23.507, mean reward: 4.701 [3.788, 5.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.547, 10.100], loss: 0.205724, mae: 0.468000, mean_q: 4.659622
 49126/100000: episode: 1166, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 30.830, mean reward: 6.166 [4.156, 8.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.347, 10.100], loss: 0.333872, mae: 0.561179, mean_q: 4.703108
 49132/100000: episode: 1167, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 54.957, mean reward: 9.160 [5.198, 14.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.423, 10.100], loss: 0.183949, mae: 0.399199, mean_q: 4.308420
 49138/100000: episode: 1168, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 31.470, mean reward: 5.245 [4.204, 6.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.561, 10.100], loss: 0.213096, mae: 0.459803, mean_q: 4.477190
 49144/100000: episode: 1169, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 27.122, mean reward: 4.520 [3.666, 6.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.446, 10.100], loss: 0.295428, mae: 0.503523, mean_q: 4.553658
[Info] FALSIFICATION!
 49146/100000: episode: 1170, duration: 0.283s, episode steps: 2, steps per second: 7, episode reward: 1015.131, mean reward: 507.566 [15.131, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.344, 10.067], loss: 0.224971, mae: 0.443706, mean_q: 4.465611
 49151/100000: episode: 1171, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 16.931, mean reward: 3.386 [3.091, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.486, 10.100], loss: 0.171125, mae: 0.402741, mean_q: 4.397826
 49157/100000: episode: 1172, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 19.176, mean reward: 3.196 [2.673, 4.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.533, 10.100], loss: 0.314073, mae: 0.519068, mean_q: 4.520614
 49163/100000: episode: 1173, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 25.641, mean reward: 4.274 [3.203, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.628, 10.100], loss: 0.199709, mae: 0.451416, mean_q: 4.500476
 49169/100000: episode: 1174, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 17.280, mean reward: 2.880 [2.658, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.530, 10.100], loss: 0.214205, mae: 0.457033, mean_q: 4.693266
 49175/100000: episode: 1175, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 28.988, mean reward: 4.831 [3.580, 5.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.460, 10.100], loss: 2568.076904, mae: 5.724396, mean_q: 4.836211
 49181/100000: episode: 1176, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 29.616, mean reward: 4.936 [3.931, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.499, 10.100], loss: 8.290923, mae: 3.360398, mean_q: 8.113377
 49186/100000: episode: 1177, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 22.231, mean reward: 4.446 [3.678, 5.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.442, 10.100], loss: 1.175955, mae: 1.140727, mean_q: 5.127408
 49191/100000: episode: 1178, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 19.329, mean reward: 3.866 [3.291, 5.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.397, 10.100], loss: 3.193496, mae: 1.836667, mean_q: 3.190816
 49196/100000: episode: 1179, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 21.071, mean reward: 4.214 [3.146, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.502, 10.100], loss: 4.912987, mae: 2.253381, mean_q: 6.507777
 49201/100000: episode: 1180, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 20.326, mean reward: 4.065 [3.599, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.470, 10.100], loss: 12.303060, mae: 3.886004, mean_q: 8.715286
 49207/100000: episode: 1181, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 31.949, mean reward: 5.325 [3.099, 8.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.981, 10.100], loss: 2.429120, mae: 1.579185, mean_q: 5.092879
 49213/100000: episode: 1182, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 33.507, mean reward: 5.584 [4.271, 6.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.318, 10.100], loss: 2.634968, mae: 2.004200, mean_q: 3.029721
 49219/100000: episode: 1183, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 22.703, mean reward: 3.784 [2.833, 4.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.538, 10.100], loss: 2568.633545, mae: 6.567081, mean_q: 4.840453
 49225/100000: episode: 1184, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 78.049, mean reward: 13.008 [5.554, 31.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.499, 10.100], loss: 32.002834, mae: 6.466349, mean_q: 11.605957
 49231/100000: episode: 1185, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 35.875, mean reward: 5.979 [3.754, 7.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.725, 10.100], loss: 19.834227, mae: 4.879853, mean_q: 10.095088
 49237/100000: episode: 1186, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 18.412, mean reward: 3.069 [2.715, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.576, 10.100], loss: 1.433948, mae: 1.084612, mean_q: 5.644585
 49243/100000: episode: 1187, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 37.719, mean reward: 6.287 [4.015, 10.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.387, 10.100], loss: 2.425582, mae: 1.912944, mean_q: 3.377476
 49248/100000: episode: 1188, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 15.633, mean reward: 3.127 [2.788, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.483, 10.100], loss: 2.646568, mae: 1.965248, mean_q: 3.314550
 49254/100000: episode: 1189, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 22.845, mean reward: 3.808 [3.328, 4.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.444, 10.100], loss: 1.238126, mae: 1.214358, mean_q: 4.130134
 49260/100000: episode: 1190, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 18.929, mean reward: 3.155 [2.762, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.758, 10.100], loss: 2559.006836, mae: 5.931564, mean_q: 5.245074
 49266/100000: episode: 1191, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 19.001, mean reward: 3.167 [2.791, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.433, 10.100], loss: 5.107589, mae: 2.346560, mean_q: 7.067894
 49272/100000: episode: 1192, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 23.405, mean reward: 3.901 [3.106, 5.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.443, 10.100], loss: 7.934853, mae: 3.095972, mean_q: 8.338998
 49277/100000: episode: 1193, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 93.508, mean reward: 18.702 [5.786, 67.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.360, 10.100], loss: 2.171284, mae: 1.449104, mean_q: 6.595708
 49282/100000: episode: 1194, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 30.143, mean reward: 6.029 [5.105, 6.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.423, 10.100], loss: 0.798250, mae: 0.829520, mean_q: 5.308472
 49288/100000: episode: 1195, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 44.153, mean reward: 7.359 [3.599, 11.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.417, 10.100], loss: 0.742081, mae: 0.875794, mean_q: 4.806519
 49294/100000: episode: 1196, duration: 0.033s, episode steps: 6, steps per second: 179, episode reward: 20.099, mean reward: 3.350 [3.026, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.499, 10.100], loss: 0.771758, mae: 0.997367, mean_q: 4.616810
 49299/100000: episode: 1197, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 35.100, mean reward: 7.020 [5.246, 9.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.482, 10.100], loss: 0.794428, mae: 0.938987, mean_q: 4.702352
 49305/100000: episode: 1198, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 46.787, mean reward: 7.798 [4.279, 12.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.456, 10.100], loss: 0.503992, mae: 0.594179, mean_q: 4.935840
 49311/100000: episode: 1199, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 39.047, mean reward: 6.508 [5.542, 8.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.521, 10.100], loss: 10.301808, mae: 0.876499, mean_q: 4.999387
 49317/100000: episode: 1200, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 22.208, mean reward: 3.701 [3.370, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.472, 10.100], loss: 0.625177, mae: 0.699378, mean_q: 5.450281
 49322/100000: episode: 1201, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 25.653, mean reward: 5.131 [4.600, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.516, 10.100], loss: 0.717492, mae: 0.737976, mean_q: 5.603489
 49327/100000: episode: 1202, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 21.428, mean reward: 4.286 [3.866, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.506, 10.100], loss: 0.589409, mae: 0.666206, mean_q: 5.343808
 49332/100000: episode: 1203, duration: 0.039s, episode steps: 5, steps per second: 130, episode reward: 18.192, mean reward: 3.638 [3.094, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.448, 10.100], loss: 0.419828, mae: 0.577144, mean_q: 5.392401
 49337/100000: episode: 1204, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 32.120, mean reward: 6.424 [3.305, 8.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.579, 10.100], loss: 0.620244, mae: 0.670164, mean_q: 5.185833
 49343/100000: episode: 1205, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 19.197, mean reward: 3.199 [2.633, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.323, 10.100], loss: 0.471866, mae: 0.544241, mean_q: 4.749830
 49349/100000: episode: 1206, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 24.686, mean reward: 4.114 [3.331, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.426, 10.100], loss: 0.790148, mae: 0.729371, mean_q: 5.007728
 49355/100000: episode: 1207, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 22.422, mean reward: 3.737 [2.887, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.454, 10.100], loss: 0.761160, mae: 0.759614, mean_q: 5.231461
 49361/100000: episode: 1208, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 50.211, mean reward: 8.368 [5.109, 11.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.449, 10.100], loss: 0.568777, mae: 0.623935, mean_q: 5.136548
 49367/100000: episode: 1209, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 25.801, mean reward: 4.300 [3.593, 5.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.406, 10.100], loss: 0.695924, mae: 0.682089, mean_q: 5.450232
 49373/100000: episode: 1210, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 23.799, mean reward: 3.967 [3.382, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.489, 10.100], loss: 0.580265, mae: 0.683132, mean_q: 5.216418
 49378/100000: episode: 1211, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 21.300, mean reward: 4.260 [3.604, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.545, 10.100], loss: 0.682419, mae: 0.648702, mean_q: 5.070270
 49383/100000: episode: 1212, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 20.160, mean reward: 4.032 [3.490, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.359, 10.100], loss: 0.758953, mae: 0.614975, mean_q: 4.904782
 49389/100000: episode: 1213, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 23.812, mean reward: 3.969 [3.607, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.975, 10.100], loss: 0.360660, mae: 0.531797, mean_q: 4.807370
 49395/100000: episode: 1214, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 34.227, mean reward: 5.705 [4.275, 7.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.554, 10.100], loss: 0.603749, mae: 0.548263, mean_q: 5.055878
 49400/100000: episode: 1215, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 17.514, mean reward: 3.503 [3.142, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.514, 10.100], loss: 0.498908, mae: 0.632034, mean_q: 4.970679
 49406/100000: episode: 1216, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 28.276, mean reward: 4.713 [4.081, 5.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.406, 10.100], loss: 0.783007, mae: 0.724503, mean_q: 5.028214
 49411/100000: episode: 1217, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 24.729, mean reward: 4.946 [4.029, 6.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.583, 10.100], loss: 0.649128, mae: 0.686671, mean_q: 4.981095
 49417/100000: episode: 1218, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 21.849, mean reward: 3.642 [3.153, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.455, 10.100], loss: 0.603170, mae: 0.662416, mean_q: 5.138660
 49422/100000: episode: 1219, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 24.958, mean reward: 4.992 [4.006, 6.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.513, 10.100], loss: 0.535296, mae: 0.645803, mean_q: 5.023253
 49428/100000: episode: 1220, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 26.882, mean reward: 4.480 [3.735, 6.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.556, 10.100], loss: 0.382741, mae: 0.546684, mean_q: 5.114426
 49434/100000: episode: 1221, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 16.722, mean reward: 2.787 [2.514, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.405, 10.100], loss: 0.608381, mae: 0.690169, mean_q: 5.144639
 49440/100000: episode: 1222, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 23.132, mean reward: 3.855 [2.854, 6.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.507, 10.100], loss: 0.427253, mae: 0.578650, mean_q: 5.198369
 49446/100000: episode: 1223, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 26.488, mean reward: 4.415 [3.478, 6.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.454, 10.100], loss: 0.352498, mae: 0.525899, mean_q: 5.091861
 49452/100000: episode: 1224, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 88.133, mean reward: 14.689 [7.419, 32.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.669, 10.100], loss: 0.423371, mae: 0.597037, mean_q: 5.228292
 49457/100000: episode: 1225, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 27.015, mean reward: 5.403 [4.101, 6.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.435, 10.100], loss: 0.454795, mae: 0.589261, mean_q: 5.168027
 49463/100000: episode: 1226, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 26.131, mean reward: 4.355 [3.386, 5.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.500, 10.100], loss: 0.554795, mae: 0.698789, mean_q: 5.161617
 49468/100000: episode: 1227, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 23.830, mean reward: 4.766 [4.547, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.418, 10.100], loss: 0.454145, mae: 0.569271, mean_q: 4.842151
 49474/100000: episode: 1228, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 20.892, mean reward: 3.482 [3.196, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.484, 10.100], loss: 0.596304, mae: 0.686783, mean_q: 5.087361
 49479/100000: episode: 1229, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 28.844, mean reward: 5.769 [4.669, 6.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.456, 10.100], loss: 0.565073, mae: 0.641589, mean_q: 4.759637
 49485/100000: episode: 1230, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 21.415, mean reward: 3.569 [2.685, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.370, 10.100], loss: 0.438240, mae: 0.617183, mean_q: 5.194931
 49490/100000: episode: 1231, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 18.986, mean reward: 3.797 [3.301, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.449, 10.100], loss: 1.001235, mae: 0.647503, mean_q: 5.186897
 49495/100000: episode: 1232, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 21.229, mean reward: 4.246 [3.944, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.569, 10.100], loss: 0.690152, mae: 0.687151, mean_q: 5.109885
 49501/100000: episode: 1233, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 52.785, mean reward: 8.798 [5.418, 17.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.465, 10.100], loss: 0.384115, mae: 0.596106, mean_q: 5.030637
 49506/100000: episode: 1234, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 20.331, mean reward: 4.066 [3.681, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.567, 10.100], loss: 0.739328, mae: 0.674315, mean_q: 5.079053
 49512/100000: episode: 1235, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 37.925, mean reward: 6.321 [5.680, 6.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.472, 10.100], loss: 10.548207, mae: 0.972310, mean_q: 5.124533
 49517/100000: episode: 1236, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 32.341, mean reward: 6.468 [4.049, 9.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.434, 10.100], loss: 0.402255, mae: 0.644313, mean_q: 5.526782
 49523/100000: episode: 1237, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 56.086, mean reward: 9.348 [6.003, 16.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.474, 10.100], loss: 10.430905, mae: 0.975052, mean_q: 5.380910
 49528/100000: episode: 1238, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 46.279, mean reward: 9.256 [5.471, 16.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.632, 10.100], loss: 0.537133, mae: 0.664047, mean_q: 5.082288
 49534/100000: episode: 1239, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 22.974, mean reward: 3.829 [3.230, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.386, 10.100], loss: 0.577175, mae: 0.653839, mean_q: 4.816048
 49540/100000: episode: 1240, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 25.888, mean reward: 4.315 [3.177, 7.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.519, 10.100], loss: 0.388422, mae: 0.544021, mean_q: 4.828179
 49545/100000: episode: 1241, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 23.800, mean reward: 4.760 [3.914, 6.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.537, 10.100], loss: 0.636961, mae: 0.653427, mean_q: 5.036243
 49550/100000: episode: 1242, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 21.151, mean reward: 4.230 [3.296, 5.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.454, 10.100], loss: 0.292037, mae: 0.479068, mean_q: 5.145517
 49556/100000: episode: 1243, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 22.518, mean reward: 3.753 [3.426, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.493, 10.100], loss: 0.365041, mae: 0.570156, mean_q: 4.951796
 49562/100000: episode: 1244, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 27.717, mean reward: 4.619 [4.146, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.443, 10.100], loss: 0.765826, mae: 0.718655, mean_q: 5.109361
 49567/100000: episode: 1245, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 23.129, mean reward: 4.626 [3.819, 5.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.473, 10.100], loss: 0.473495, mae: 0.672346, mean_q: 4.959937
 49573/100000: episode: 1246, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 24.959, mean reward: 4.160 [3.790, 4.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.471, 10.100], loss: 0.496845, mae: 0.605964, mean_q: 4.926257
 49578/100000: episode: 1247, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 19.128, mean reward: 3.826 [3.273, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.388, 10.100], loss: 0.725618, mae: 0.655891, mean_q: 5.173638
 49584/100000: episode: 1248, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 36.752, mean reward: 6.125 [5.232, 9.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.386, 10.100], loss: 0.674199, mae: 0.711337, mean_q: 5.043441
 49590/100000: episode: 1249, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 31.120, mean reward: 5.187 [4.763, 5.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.491, 10.100], loss: 0.558665, mae: 0.653655, mean_q: 5.038554
[Info] Complete ISplit Iteration
[Info] Levels: [4.3799934, 5.3703775, 5.937879, 7.003525, 8.014394]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.18]
[Info] Error Prob: 1.8000000000000004e-05

 49595/100000: episode: 1250, duration: 4.317s, episode steps: 5, steps per second: 1, episode reward: 18.866, mean reward: 3.773 [3.271, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.597, 10.100], loss: 0.533369, mae: 0.636507, mean_q: 5.114812
 49695/100000: episode: 1251, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 202.123, mean reward: 2.021 [1.477, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: 155.309769, mae: 1.366146, mean_q: 5.374393
 49795/100000: episode: 1252, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.757, mean reward: 1.858 [1.471, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.136, 10.098], loss: 0.782889, mae: 0.685779, mean_q: 5.098639
 49895/100000: episode: 1253, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 217.566, mean reward: 2.176 [1.441, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.425, 10.098], loss: 155.304962, mae: 1.306240, mean_q: 5.416256
 49995/100000: episode: 1254, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.682, mean reward: 1.837 [1.436, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.380, 10.295], loss: 463.144623, mae: 2.276253, mean_q: 6.099132
 50095/100000: episode: 1255, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.940, mean reward: 1.849 [1.494, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.671, 10.306], loss: 155.712341, mae: 1.340959, mean_q: 5.579963
 50195/100000: episode: 1256, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.148, mean reward: 1.921 [1.461, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.098], loss: 154.955627, mae: 1.306785, mean_q: 5.614779
 50295/100000: episode: 1257, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 187.832, mean reward: 1.878 [1.457, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.404, 10.109], loss: 2.237573, mae: 0.758620, mean_q: 5.282237
 50395/100000: episode: 1258, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.162, mean reward: 1.912 [1.452, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.671, 10.154], loss: 155.814255, mae: 1.311531, mean_q: 5.545661
 50495/100000: episode: 1259, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 278.168, mean reward: 2.782 [1.433, 22.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.154, 10.723], loss: 154.919067, mae: 1.255545, mean_q: 5.557931
 50595/100000: episode: 1260, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 178.664, mean reward: 1.787 [1.431, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.740, 10.143], loss: 308.522247, mae: 1.788680, mean_q: 5.850543
 50695/100000: episode: 1261, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 185.034, mean reward: 1.850 [1.480, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.042, 10.098], loss: 2.335414, mae: 0.780875, mean_q: 5.344878
 50795/100000: episode: 1262, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 193.877, mean reward: 1.939 [1.436, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.222, 10.098], loss: 1.568051, mae: 0.694480, mean_q: 5.213045
 50895/100000: episode: 1263, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.945, mean reward: 1.889 [1.453, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.954, 10.151], loss: 155.525360, mae: 1.257674, mean_q: 5.499628
 50995/100000: episode: 1264, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 208.491, mean reward: 2.085 [1.470, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.287, 10.098], loss: 154.943268, mae: 1.196438, mean_q: 5.387255
 51095/100000: episode: 1265, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 196.524, mean reward: 1.965 [1.439, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.841, 10.110], loss: 156.396301, mae: 1.408928, mean_q: 5.633159
 51195/100000: episode: 1266, duration: 0.656s, episode steps: 100, steps per second: 153, episode reward: 186.704, mean reward: 1.867 [1.474, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.496, 10.116], loss: 1.115000, mae: 0.665217, mean_q: 5.217357
 51295/100000: episode: 1267, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 188.898, mean reward: 1.889 [1.527, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.283, 10.191], loss: 1.209161, mae: 0.680304, mean_q: 5.137542
 51395/100000: episode: 1268, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.847, mean reward: 1.898 [1.455, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.098], loss: 1.034375, mae: 0.656284, mean_q: 5.032473
 51495/100000: episode: 1269, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 190.617, mean reward: 1.906 [1.468, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.286], loss: 0.773202, mae: 0.605186, mean_q: 4.992833
 51595/100000: episode: 1270, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 193.386, mean reward: 1.934 [1.448, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.129], loss: 155.158035, mae: 1.254811, mean_q: 5.395858
 51695/100000: episode: 1271, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 226.360, mean reward: 2.264 [1.484, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.764, 10.098], loss: 154.905930, mae: 1.175478, mean_q: 5.399658
 51795/100000: episode: 1272, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 195.102, mean reward: 1.951 [1.502, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.254, 10.284], loss: 1.403612, mae: 0.666224, mean_q: 5.153225
 51895/100000: episode: 1273, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 187.586, mean reward: 1.876 [1.465, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.533, 10.113], loss: 0.678843, mae: 0.639928, mean_q: 5.065519
 51995/100000: episode: 1274, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 185.557, mean reward: 1.856 [1.457, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.293, 10.098], loss: 155.088730, mae: 1.214424, mean_q: 5.434686
 52095/100000: episode: 1275, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 183.439, mean reward: 1.834 [1.463, 2.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.588497, mae: 0.600373, mean_q: 4.964425
 52195/100000: episode: 1276, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 187.843, mean reward: 1.878 [1.482, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.373, 10.230], loss: 1.583257, mae: 0.662591, mean_q: 4.991049
 52295/100000: episode: 1277, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 202.609, mean reward: 2.026 [1.448, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.419, 10.193], loss: 1.470262, mae: 0.647411, mean_q: 5.011571
 52395/100000: episode: 1278, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 189.684, mean reward: 1.897 [1.460, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.572, 10.098], loss: 0.740136, mae: 0.616243, mean_q: 4.972161
 52495/100000: episode: 1279, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 178.398, mean reward: 1.784 [1.435, 2.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.212, 10.098], loss: 1.985865, mae: 0.643850, mean_q: 4.969782
 52595/100000: episode: 1280, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 204.296, mean reward: 2.043 [1.441, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.285, 10.098], loss: 308.687683, mae: 1.679242, mean_q: 5.612525
 52695/100000: episode: 1281, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 186.992, mean reward: 1.870 [1.487, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.614, 10.130], loss: 462.177460, mae: 2.030688, mean_q: 5.691940
 52795/100000: episode: 1282, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 208.218, mean reward: 2.082 [1.506, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.412, 10.098], loss: 155.735123, mae: 1.388427, mean_q: 5.625013
 52895/100000: episode: 1283, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 182.096, mean reward: 1.821 [1.468, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.208, 10.161], loss: 156.529907, mae: 1.313824, mean_q: 5.494822
 52995/100000: episode: 1284, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 183.288, mean reward: 1.833 [1.461, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.098], loss: 154.983948, mae: 1.177693, mean_q: 5.379884
 53095/100000: episode: 1285, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 199.216, mean reward: 1.992 [1.449, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.511, 10.098], loss: 155.175995, mae: 1.170252, mean_q: 5.375822
 53195/100000: episode: 1286, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 218.698, mean reward: 2.187 [1.498, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.693, 10.281], loss: 155.028687, mae: 1.164649, mean_q: 5.301569
 53295/100000: episode: 1287, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.476, mean reward: 1.945 [1.462, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.332, 10.114], loss: 0.712649, mae: 0.580137, mean_q: 4.812847
 53395/100000: episode: 1288, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 204.727, mean reward: 2.047 [1.451, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.355, 10.349], loss: 155.982697, mae: 0.988454, mean_q: 4.855230
 53495/100000: episode: 1289, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 196.208, mean reward: 1.962 [1.445, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.362, 10.098], loss: 308.034851, mae: 1.818523, mean_q: 5.590627
 53595/100000: episode: 1290, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 201.356, mean reward: 2.014 [1.477, 10.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.117, 10.098], loss: 153.994919, mae: 1.264654, mean_q: 5.239915
 53695/100000: episode: 1291, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.723, mean reward: 1.917 [1.448, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.098], loss: 154.404770, mae: 1.100607, mean_q: 5.093699
 53795/100000: episode: 1292, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 192.219, mean reward: 1.922 [1.473, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.585, 10.098], loss: 1.246886, mae: 0.576949, mean_q: 4.636169
 53895/100000: episode: 1293, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 197.150, mean reward: 1.972 [1.454, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.087, 10.219], loss: 0.624921, mae: 0.491985, mean_q: 4.509513
 53995/100000: episode: 1294, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 216.527, mean reward: 2.165 [1.486, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.810, 10.127], loss: 1.159460, mae: 0.525867, mean_q: 4.378423
 54095/100000: episode: 1295, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.360, mean reward: 1.934 [1.477, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.950, 10.163], loss: 0.738297, mae: 0.484185, mean_q: 4.339059
 54195/100000: episode: 1296, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 221.747, mean reward: 2.217 [1.442, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.338], loss: 0.315057, mae: 0.401804, mean_q: 4.179192
 54295/100000: episode: 1297, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 179.931, mean reward: 1.799 [1.456, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.968, 10.208], loss: 0.279200, mae: 0.380313, mean_q: 4.101117
 54395/100000: episode: 1298, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.371, mean reward: 1.834 [1.437, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.856, 10.360], loss: 0.552254, mae: 0.401373, mean_q: 4.070622
 54495/100000: episode: 1299, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 186.474, mean reward: 1.865 [1.508, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.201, 10.138], loss: 0.184603, mae: 0.330625, mean_q: 3.969857
 54595/100000: episode: 1300, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 180.926, mean reward: 1.809 [1.449, 2.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.098], loss: 0.141114, mae: 0.311956, mean_q: 3.902024
 54695/100000: episode: 1301, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 190.119, mean reward: 1.901 [1.462, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.898, 10.188], loss: 0.171921, mae: 0.314378, mean_q: 3.884602
 54795/100000: episode: 1302, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 184.765, mean reward: 1.848 [1.467, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.106], loss: 0.090823, mae: 0.300669, mean_q: 3.867653
 54895/100000: episode: 1303, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 208.485, mean reward: 2.085 [1.484, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.425], loss: 0.153975, mae: 0.313279, mean_q: 3.853991
 54995/100000: episode: 1304, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 189.783, mean reward: 1.898 [1.454, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.459, 10.129], loss: 0.250787, mae: 0.328354, mean_q: 3.888487
 55095/100000: episode: 1305, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 184.262, mean reward: 1.843 [1.468, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.352, 10.156], loss: 0.089591, mae: 0.297974, mean_q: 3.866230
 55195/100000: episode: 1306, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 193.630, mean reward: 1.936 [1.482, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.288, 10.345], loss: 0.194065, mae: 0.313961, mean_q: 3.873401
 55295/100000: episode: 1307, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 202.011, mean reward: 2.020 [1.444, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.338, 10.259], loss: 0.148787, mae: 0.307774, mean_q: 3.877416
 55395/100000: episode: 1308, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.665, mean reward: 1.927 [1.441, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.834, 10.162], loss: 0.140108, mae: 0.297116, mean_q: 3.853474
 55495/100000: episode: 1309, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 219.553, mean reward: 2.196 [1.468, 5.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.039, 10.098], loss: 0.117400, mae: 0.308274, mean_q: 3.869084
 55595/100000: episode: 1310, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 207.503, mean reward: 2.075 [1.439, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.241], loss: 0.080985, mae: 0.285582, mean_q: 3.840422
 55695/100000: episode: 1311, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 197.744, mean reward: 1.977 [1.459, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.897, 10.098], loss: 0.094168, mae: 0.290870, mean_q: 3.844395
 55795/100000: episode: 1312, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 183.575, mean reward: 1.836 [1.458, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.778, 10.185], loss: 0.085916, mae: 0.300265, mean_q: 3.871665
 55895/100000: episode: 1313, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 182.413, mean reward: 1.824 [1.464, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.232, 10.177], loss: 0.091360, mae: 0.289220, mean_q: 3.859464
 55995/100000: episode: 1314, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 198.476, mean reward: 1.985 [1.482, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.593, 10.098], loss: 0.092278, mae: 0.301183, mean_q: 3.864759
 56095/100000: episode: 1315, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.057, mean reward: 1.971 [1.461, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.284], loss: 0.082267, mae: 0.291511, mean_q: 3.860410
 56195/100000: episode: 1316, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.222, mean reward: 1.942 [1.494, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.022, 10.098], loss: 0.079659, mae: 0.284387, mean_q: 3.846539
 56295/100000: episode: 1317, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 184.927, mean reward: 1.849 [1.466, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.321, 10.238], loss: 0.098319, mae: 0.299793, mean_q: 3.858861
 56395/100000: episode: 1318, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 180.132, mean reward: 1.801 [1.473, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.191], loss: 0.090559, mae: 0.287110, mean_q: 3.857633
 56495/100000: episode: 1319, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 196.668, mean reward: 1.967 [1.485, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.203, 10.098], loss: 0.086437, mae: 0.288345, mean_q: 3.863334
 56595/100000: episode: 1320, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 234.240, mean reward: 2.342 [1.456, 4.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.540, 10.508], loss: 0.079159, mae: 0.283884, mean_q: 3.855908
 56695/100000: episode: 1321, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 190.277, mean reward: 1.903 [1.458, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.267, 10.098], loss: 0.085061, mae: 0.294151, mean_q: 3.859447
 56795/100000: episode: 1322, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 187.420, mean reward: 1.874 [1.438, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.510, 10.115], loss: 0.085393, mae: 0.283358, mean_q: 3.879625
 56895/100000: episode: 1323, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 211.852, mean reward: 2.119 [1.486, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.079, 10.143], loss: 0.082377, mae: 0.273803, mean_q: 3.848931
 56995/100000: episode: 1324, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 197.196, mean reward: 1.972 [1.477, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.782, 10.098], loss: 0.076259, mae: 0.276202, mean_q: 3.846608
 57095/100000: episode: 1325, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 195.306, mean reward: 1.953 [1.435, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.427, 10.098], loss: 0.087938, mae: 0.295842, mean_q: 3.867656
 57195/100000: episode: 1326, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 183.620, mean reward: 1.836 [1.474, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.902, 10.125], loss: 0.096640, mae: 0.285651, mean_q: 3.868062
 57295/100000: episode: 1327, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 179.881, mean reward: 1.799 [1.438, 2.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.968, 10.098], loss: 0.080544, mae: 0.272137, mean_q: 3.842450
 57395/100000: episode: 1328, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 201.479, mean reward: 2.015 [1.465, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.525, 10.276], loss: 0.091136, mae: 0.284777, mean_q: 3.852811
 57495/100000: episode: 1329, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 192.763, mean reward: 1.928 [1.469, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.857, 10.224], loss: 0.082815, mae: 0.287637, mean_q: 3.878757
 57595/100000: episode: 1330, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 195.200, mean reward: 1.952 [1.477, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.985, 10.227], loss: 0.075906, mae: 0.279841, mean_q: 3.866732
 57695/100000: episode: 1331, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.000, mean reward: 1.870 [1.482, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.910, 10.098], loss: 0.138394, mae: 0.302166, mean_q: 3.889775
 57795/100000: episode: 1332, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.784, mean reward: 1.918 [1.474, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.853, 10.098], loss: 0.074865, mae: 0.280084, mean_q: 3.844950
 57895/100000: episode: 1333, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 201.842, mean reward: 2.018 [1.446, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.295, 10.098], loss: 0.095261, mae: 0.293778, mean_q: 3.870476
 57995/100000: episode: 1334, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 215.370, mean reward: 2.154 [1.450, 4.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.415, 10.098], loss: 0.091497, mae: 0.292798, mean_q: 3.877810
 58095/100000: episode: 1335, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.917, mean reward: 1.879 [1.493, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.596, 10.098], loss: 0.091316, mae: 0.297711, mean_q: 3.884090
 58195/100000: episode: 1336, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.578, mean reward: 1.826 [1.499, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.895, 10.098], loss: 0.087260, mae: 0.288665, mean_q: 3.858314
 58295/100000: episode: 1337, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 184.401, mean reward: 1.844 [1.466, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.765, 10.098], loss: 0.075055, mae: 0.278903, mean_q: 3.855895
 58395/100000: episode: 1338, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 199.688, mean reward: 1.997 [1.483, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.281, 10.194], loss: 0.083907, mae: 0.292009, mean_q: 3.844308
 58495/100000: episode: 1339, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.126, mean reward: 1.991 [1.488, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.292, 10.098], loss: 0.090268, mae: 0.286738, mean_q: 3.858636
 58595/100000: episode: 1340, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 208.722, mean reward: 2.087 [1.461, 6.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.993, 10.098], loss: 0.073689, mae: 0.276654, mean_q: 3.836962
 58695/100000: episode: 1341, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.047, mean reward: 1.830 [1.452, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.020, 10.098], loss: 0.074915, mae: 0.286419, mean_q: 3.856323
 58795/100000: episode: 1342, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 188.393, mean reward: 1.884 [1.440, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.098], loss: 0.084127, mae: 0.289308, mean_q: 3.851272
 58895/100000: episode: 1343, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.507, mean reward: 1.935 [1.446, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.892, 10.098], loss: 0.083537, mae: 0.286273, mean_q: 3.853631
 58995/100000: episode: 1344, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 176.992, mean reward: 1.770 [1.446, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.340, 10.098], loss: 0.080416, mae: 0.281513, mean_q: 3.835017
 59095/100000: episode: 1345, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 205.349, mean reward: 2.053 [1.473, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.920, 10.098], loss: 0.075812, mae: 0.277884, mean_q: 3.834634
 59195/100000: episode: 1346, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.487, mean reward: 1.855 [1.434, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.098], loss: 0.073657, mae: 0.277750, mean_q: 3.839146
 59295/100000: episode: 1347, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 221.952, mean reward: 2.220 [1.468, 7.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.296, 10.098], loss: 0.083602, mae: 0.279675, mean_q: 3.847208
 59395/100000: episode: 1348, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.736, mean reward: 1.907 [1.474, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.492, 10.124], loss: 0.078032, mae: 0.282128, mean_q: 3.861884
 59495/100000: episode: 1349, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 183.921, mean reward: 1.839 [1.501, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.255], loss: 0.087575, mae: 0.289627, mean_q: 3.856248
[Info] 1-TH LEVEL FOUND: 5.583993434906006, Considering 10/90 traces
 59595/100000: episode: 1350, duration: 5.686s, episode steps: 100, steps per second: 18, episode reward: 192.364, mean reward: 1.924 [1.490, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.027, 10.098], loss: 0.087993, mae: 0.290602, mean_q: 3.840498
 59625/100000: episode: 1351, duration: 0.311s, episode steps: 30, steps per second: 96, episode reward: 102.117, mean reward: 3.404 [2.028, 7.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.366, 10.458], loss: 0.073655, mae: 0.279941, mean_q: 3.873070
 59646/100000: episode: 1352, duration: 0.234s, episode steps: 21, steps per second: 90, episode reward: 56.170, mean reward: 2.675 [1.888, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.340], loss: 0.116018, mae: 0.313017, mean_q: 3.884344
 59676/100000: episode: 1353, duration: 0.306s, episode steps: 30, steps per second: 98, episode reward: 101.383, mean reward: 3.379 [2.561, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.269, 10.484], loss: 0.080318, mae: 0.287602, mean_q: 3.841915
 59697/100000: episode: 1354, duration: 0.202s, episode steps: 21, steps per second: 104, episode reward: 68.215, mean reward: 3.248 [2.449, 5.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.355, 10.399], loss: 0.074345, mae: 0.279636, mean_q: 3.869784
 59727/100000: episode: 1355, duration: 0.190s, episode steps: 30, steps per second: 158, episode reward: 86.445, mean reward: 2.882 [2.261, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.094, 10.493], loss: 0.077621, mae: 0.278865, mean_q: 3.858177
 59738/100000: episode: 1356, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 42.827, mean reward: 3.893 [2.787, 6.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.995, 10.100], loss: 0.129472, mae: 0.343528, mean_q: 3.907877
 59746/100000: episode: 1357, duration: 0.104s, episode steps: 8, steps per second: 77, episode reward: 20.858, mean reward: 2.607 [2.396, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.380, 10.100], loss: 0.095546, mae: 0.307236, mean_q: 3.901225
 59772/100000: episode: 1358, duration: 0.183s, episode steps: 26, steps per second: 142, episode reward: 56.075, mean reward: 2.157 [1.766, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.196, 10.203], loss: 0.089824, mae: 0.287045, mean_q: 3.887223
 59785/100000: episode: 1359, duration: 0.099s, episode steps: 13, steps per second: 132, episode reward: 45.374, mean reward: 3.490 [2.530, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.354, 10.100], loss: 0.103960, mae: 0.309766, mean_q: 3.917880
 59806/100000: episode: 1360, duration: 0.210s, episode steps: 21, steps per second: 100, episode reward: 62.251, mean reward: 2.964 [2.355, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.335, 10.439], loss: 0.110646, mae: 0.312497, mean_q: 3.903746
 59819/100000: episode: 1361, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 37.217, mean reward: 2.863 [2.421, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.611, 10.100], loss: 0.109800, mae: 0.341537, mean_q: 3.969659
 59849/100000: episode: 1362, duration: 0.231s, episode steps: 30, steps per second: 130, episode reward: 135.642, mean reward: 4.521 [2.704, 9.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.135, 10.566], loss: 0.109337, mae: 0.319187, mean_q: 3.967071
 59858/100000: episode: 1363, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 18.930, mean reward: 2.103 [1.796, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.103, 10.100], loss: 0.110681, mae: 0.324724, mean_q: 3.890376
 59879/100000: episode: 1364, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 48.044, mean reward: 2.288 [1.848, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.790, 10.297], loss: 0.103054, mae: 0.314863, mean_q: 3.976966
 59909/100000: episode: 1365, duration: 0.199s, episode steps: 30, steps per second: 151, episode reward: 139.317, mean reward: 4.644 [2.086, 12.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.386], loss: 0.141361, mae: 0.329697, mean_q: 3.968384
 59925/100000: episode: 1366, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 49.996, mean reward: 3.125 [2.044, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.051, 10.328], loss: 0.178567, mae: 0.353589, mean_q: 3.957900
 59933/100000: episode: 1367, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 22.435, mean reward: 2.804 [2.504, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.244, 10.100], loss: 0.164401, mae: 0.377711, mean_q: 4.049002
 59944/100000: episode: 1368, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 28.666, mean reward: 2.606 [1.947, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.157, 10.100], loss: 0.103473, mae: 0.320323, mean_q: 4.048851
 59965/100000: episode: 1369, duration: 0.154s, episode steps: 21, steps per second: 137, episode reward: 52.115, mean reward: 2.482 [1.782, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.389, 10.253], loss: 0.112739, mae: 0.332010, mean_q: 3.956976
 59975/100000: episode: 1370, duration: 0.085s, episode steps: 10, steps per second: 117, episode reward: 23.281, mean reward: 2.328 [2.018, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.530, 10.100], loss: 0.156859, mae: 0.344849, mean_q: 4.074306
 59983/100000: episode: 1371, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 25.843, mean reward: 3.230 [2.682, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.287, 10.100], loss: 0.137774, mae: 0.327263, mean_q: 4.026102
 60008/100000: episode: 1372, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 69.146, mean reward: 2.766 [2.330, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.360, 10.442], loss: 0.112927, mae: 0.322126, mean_q: 3.973782
 60033/100000: episode: 1373, duration: 0.175s, episode steps: 25, steps per second: 143, episode reward: 95.754, mean reward: 3.830 [2.247, 6.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.463, 10.617], loss: 0.159472, mae: 0.359446, mean_q: 4.036883
 60058/100000: episode: 1374, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 54.671, mean reward: 2.187 [1.564, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.495, 10.307], loss: 0.159282, mae: 0.358602, mean_q: 4.079180
 60067/100000: episode: 1375, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 20.791, mean reward: 2.310 [1.816, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.386, 10.100], loss: 0.121110, mae: 0.327853, mean_q: 4.063869
 60092/100000: episode: 1376, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 61.965, mean reward: 2.479 [1.559, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.891, 10.169], loss: 0.110909, mae: 0.321696, mean_q: 4.011594
 60100/100000: episode: 1377, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 25.586, mean reward: 3.198 [2.532, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.467, 10.100], loss: 0.116401, mae: 0.342639, mean_q: 4.095133
 60121/100000: episode: 1378, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 57.727, mean reward: 2.749 [2.343, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.132, 10.405], loss: 0.120192, mae: 0.339816, mean_q: 4.041972
 60131/100000: episode: 1379, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 28.750, mean reward: 2.875 [2.321, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.277, 10.100], loss: 0.103898, mae: 0.314863, mean_q: 4.088328
 60139/100000: episode: 1380, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 19.778, mean reward: 2.472 [2.055, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.272, 10.100], loss: 0.098863, mae: 0.299434, mean_q: 3.937444
 60148/100000: episode: 1381, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 28.380, mean reward: 3.153 [2.675, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.337, 10.100], loss: 0.081166, mae: 0.292842, mean_q: 4.040667
 60174/100000: episode: 1382, duration: 0.124s, episode steps: 26, steps per second: 209, episode reward: 61.731, mean reward: 2.374 [1.687, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.277], loss: 0.171349, mae: 0.346560, mean_q: 4.051930
 60190/100000: episode: 1383, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 40.677, mean reward: 2.542 [2.183, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.342], loss: 0.108503, mae: 0.325756, mean_q: 4.090039
 60220/100000: episode: 1384, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 73.097, mean reward: 2.437 [1.525, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.436, 10.282], loss: 0.146197, mae: 0.353317, mean_q: 4.092100
 60231/100000: episode: 1385, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 47.817, mean reward: 4.347 [2.653, 6.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.335, 10.100], loss: 0.179928, mae: 0.380278, mean_q: 4.169805
 60252/100000: episode: 1386, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 47.732, mean reward: 2.273 [1.969, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.275, 10.354], loss: 0.088134, mae: 0.310545, mean_q: 4.012324
 60265/100000: episode: 1387, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 45.429, mean reward: 3.495 [2.650, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.335, 10.100], loss: 0.113880, mae: 0.330911, mean_q: 4.022120
 60286/100000: episode: 1388, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 51.747, mean reward: 2.464 [1.956, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.268, 10.400], loss: 0.100992, mae: 0.312796, mean_q: 4.079859
 60299/100000: episode: 1389, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 39.919, mean reward: 3.071 [2.446, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.250, 10.100], loss: 0.100417, mae: 0.313043, mean_q: 4.074384
 60325/100000: episode: 1390, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 51.667, mean reward: 1.987 [1.655, 2.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.294, 10.294], loss: 0.109952, mae: 0.327973, mean_q: 4.094470
 60334/100000: episode: 1391, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 19.120, mean reward: 2.124 [1.908, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.358, 10.100], loss: 0.164537, mae: 0.353033, mean_q: 3.999830
 60342/100000: episode: 1392, duration: 0.057s, episode steps: 8, steps per second: 142, episode reward: 29.920, mean reward: 3.740 [2.665, 6.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.294, 10.100], loss: 0.113467, mae: 0.361082, mean_q: 4.168207
 60355/100000: episode: 1393, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 55.842, mean reward: 4.296 [2.974, 6.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.349, 10.100], loss: 0.144728, mae: 0.373059, mean_q: 4.178102
 60363/100000: episode: 1394, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 41.190, mean reward: 5.149 [2.904, 8.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.315, 10.100], loss: 0.147838, mae: 0.365485, mean_q: 4.107593
 60388/100000: episode: 1395, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 89.448, mean reward: 3.578 [2.381, 5.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.111, 10.325], loss: 0.139575, mae: 0.379054, mean_q: 4.155227
 60398/100000: episode: 1396, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 25.833, mean reward: 2.583 [2.313, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.447, 10.100], loss: 0.100375, mae: 0.322407, mean_q: 4.032896
 60411/100000: episode: 1397, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 40.499, mean reward: 3.115 [2.430, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.318, 10.100], loss: 0.142203, mae: 0.369064, mean_q: 4.197833
 60436/100000: episode: 1398, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 71.534, mean reward: 2.861 [2.461, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.532, 10.457], loss: 0.146511, mae: 0.353407, mean_q: 4.129146
 60461/100000: episode: 1399, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 77.705, mean reward: 3.108 [2.135, 7.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.346, 10.479], loss: 0.127875, mae: 0.332987, mean_q: 4.128097
 60486/100000: episode: 1400, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 64.884, mean reward: 2.595 [1.795, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.261], loss: 0.139915, mae: 0.362820, mean_q: 4.217806
 60496/100000: episode: 1401, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 26.421, mean reward: 2.642 [2.264, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.653, 10.100], loss: 0.128734, mae: 0.340644, mean_q: 4.103780
 60512/100000: episode: 1402, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 42.857, mean reward: 2.679 [2.054, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.370], loss: 0.147718, mae: 0.359270, mean_q: 4.195574
 60521/100000: episode: 1403, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 24.415, mean reward: 2.713 [2.244, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.165, 10.100], loss: 0.163883, mae: 0.373328, mean_q: 4.159938
 60551/100000: episode: 1404, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 95.223, mean reward: 3.174 [1.775, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.558, 10.290], loss: 0.195887, mae: 0.379102, mean_q: 4.162699
 60572/100000: episode: 1405, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 49.244, mean reward: 2.345 [1.922, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.327], loss: 0.212717, mae: 0.383747, mean_q: 4.183560
 60598/100000: episode: 1406, duration: 0.122s, episode steps: 26, steps per second: 214, episode reward: 55.738, mean reward: 2.144 [1.737, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.303, 10.338], loss: 0.164439, mae: 0.369400, mean_q: 4.197996
 60611/100000: episode: 1407, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 45.609, mean reward: 3.508 [2.519, 4.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.427, 10.100], loss: 0.120157, mae: 0.332983, mean_q: 4.230768
 60632/100000: episode: 1408, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 47.835, mean reward: 2.278 [1.658, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.258], loss: 0.170877, mae: 0.383392, mean_q: 4.232796
 60642/100000: episode: 1409, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 26.044, mean reward: 2.604 [1.994, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.639, 10.100], loss: 0.201849, mae: 0.388769, mean_q: 4.155433
 60668/100000: episode: 1410, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 61.986, mean reward: 2.384 [1.950, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.308, 10.347], loss: 0.179845, mae: 0.366808, mean_q: 4.207178
 60698/100000: episode: 1411, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 93.913, mean reward: 3.130 [2.278, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.416, 10.571], loss: 0.125355, mae: 0.349394, mean_q: 4.174031
 60723/100000: episode: 1412, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 54.452, mean reward: 2.178 [1.851, 2.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.363], loss: 0.158763, mae: 0.377489, mean_q: 4.229703
 60736/100000: episode: 1413, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 51.279, mean reward: 3.945 [3.072, 7.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.272, 10.100], loss: 0.183510, mae: 0.397969, mean_q: 4.284590
 60747/100000: episode: 1414, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 33.401, mean reward: 3.036 [1.778, 8.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.277, 10.100], loss: 0.157679, mae: 0.380441, mean_q: 4.283800
 60768/100000: episode: 1415, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 50.354, mean reward: 2.398 [1.794, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.879, 10.336], loss: 0.127609, mae: 0.342593, mean_q: 4.256520
 60781/100000: episode: 1416, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 35.621, mean reward: 2.740 [2.061, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.417, 10.100], loss: 0.185698, mae: 0.327331, mean_q: 4.219266
 60789/100000: episode: 1417, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 21.759, mean reward: 2.720 [2.455, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.270, 10.100], loss: 0.186889, mae: 0.370136, mean_q: 4.119758
 60805/100000: episode: 1418, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 40.367, mean reward: 2.523 [1.977, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.131, 10.367], loss: 0.166308, mae: 0.392594, mean_q: 4.249347
 60813/100000: episode: 1419, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 23.406, mean reward: 2.926 [2.303, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.352, 10.100], loss: 0.189308, mae: 0.382316, mean_q: 4.213658
 60824/100000: episode: 1420, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 40.932, mean reward: 3.721 [2.704, 4.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.422, 10.100], loss: 0.170350, mae: 0.412120, mean_q: 4.284225
 60840/100000: episode: 1421, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 48.454, mean reward: 3.028 [2.292, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.475], loss: 0.203141, mae: 0.409112, mean_q: 4.296484
 60865/100000: episode: 1422, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 59.384, mean reward: 2.375 [1.721, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.271], loss: 0.158649, mae: 0.373215, mean_q: 4.274041
 60873/100000: episode: 1423, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 20.997, mean reward: 2.625 [2.142, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.262, 10.100], loss: 0.201037, mae: 0.431814, mean_q: 4.405424
 60903/100000: episode: 1424, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 94.422, mean reward: 3.147 [2.205, 7.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.443, 10.385], loss: 0.161100, mae: 0.395910, mean_q: 4.306727
 60933/100000: episode: 1425, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 78.477, mean reward: 2.616 [2.017, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.374], loss: 0.190937, mae: 0.392540, mean_q: 4.303903
 60943/100000: episode: 1426, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 21.605, mean reward: 2.160 [1.790, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.154, 10.100], loss: 0.192966, mae: 0.431977, mean_q: 4.452734
 60953/100000: episode: 1427, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 26.846, mean reward: 2.685 [2.168, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.197, 10.100], loss: 0.153710, mae: 0.387729, mean_q: 4.225718
 60979/100000: episode: 1428, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 76.455, mean reward: 2.941 [2.093, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.312, 10.493], loss: 0.194024, mae: 0.397787, mean_q: 4.321919
 61005/100000: episode: 1429, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 61.483, mean reward: 2.365 [1.525, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.318, 10.188], loss: 0.184675, mae: 0.396095, mean_q: 4.335330
 61030/100000: episode: 1430, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 65.865, mean reward: 2.635 [1.975, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.908, 10.293], loss: 0.159570, mae: 0.383398, mean_q: 4.393854
 61040/100000: episode: 1431, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 26.911, mean reward: 2.691 [2.055, 4.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.959, 10.100], loss: 0.194832, mae: 0.383466, mean_q: 4.360339
 61065/100000: episode: 1432, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 61.396, mean reward: 2.456 [1.960, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.819, 10.363], loss: 0.156641, mae: 0.381192, mean_q: 4.306075
 61086/100000: episode: 1433, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 56.348, mean reward: 2.683 [2.129, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.203, 10.373], loss: 0.117847, mae: 0.344754, mean_q: 4.290524
 61097/100000: episode: 1434, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 30.084, mean reward: 2.735 [2.248, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.407, 10.100], loss: 0.134951, mae: 0.358007, mean_q: 4.322264
 61122/100000: episode: 1435, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 73.980, mean reward: 2.959 [1.871, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.847, 10.304], loss: 0.137387, mae: 0.360313, mean_q: 4.247367
 61135/100000: episode: 1436, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 37.617, mean reward: 2.894 [2.604, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.499, 10.100], loss: 0.120022, mae: 0.365912, mean_q: 4.317096
 61161/100000: episode: 1437, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 83.551, mean reward: 3.213 [1.912, 5.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.550], loss: 0.166569, mae: 0.393912, mean_q: 4.316327
 61191/100000: episode: 1438, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 131.294, mean reward: 4.376 [2.396, 13.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.677], loss: 0.160618, mae: 0.390677, mean_q: 4.386468
 61217/100000: episode: 1439, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 66.427, mean reward: 2.555 [1.797, 5.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.389, 10.205], loss: 0.197387, mae: 0.410531, mean_q: 4.366114
[Info] 2-TH LEVEL FOUND: 8.147449493408203, Considering 10/90 traces
 61233/100000: episode: 1440, duration: 4.340s, episode steps: 16, steps per second: 4, episode reward: 37.238, mean reward: 2.327 [1.600, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.352, 10.263], loss: 0.126751, mae: 0.348703, mean_q: 4.300771
 61247/100000: episode: 1441, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 52.632, mean reward: 3.759 [2.976, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.367, 10.353], loss: 0.186498, mae: 0.405271, mean_q: 4.317413
 61255/100000: episode: 1442, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 32.305, mean reward: 4.038 [3.369, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.391, 10.100], loss: 0.263491, mae: 0.443044, mean_q: 4.374865
 61264/100000: episode: 1443, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 44.711, mean reward: 4.968 [3.674, 8.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.585, 10.100], loss: 0.168156, mae: 0.406839, mean_q: 4.349799
 61278/100000: episode: 1444, duration: 0.068s, episode steps: 14, steps per second: 204, episode reward: 180.781, mean reward: 12.913 [3.467, 49.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.769], loss: 0.128145, mae: 0.359425, mean_q: 4.438345
 61288/100000: episode: 1445, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 41.176, mean reward: 4.118 [3.496, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.113, 10.505], loss: 1.417615, mae: 0.669930, mean_q: 4.565299
 61296/100000: episode: 1446, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 51.833, mean reward: 6.479 [4.191, 8.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.425, 10.100], loss: 0.420570, mae: 0.511739, mean_q: 4.298245
 61306/100000: episode: 1447, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 92.210, mean reward: 9.221 [4.334, 19.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.808, 10.648], loss: 0.220322, mae: 0.450644, mean_q: 4.355491
 61320/100000: episode: 1448, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 64.557, mean reward: 4.611 [3.253, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.531], loss: 0.230001, mae: 0.422139, mean_q: 4.432273
 61327/100000: episode: 1449, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 36.672, mean reward: 5.239 [4.252, 7.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.343, 10.100], loss: 0.257972, mae: 0.506215, mean_q: 4.556726
 61335/100000: episode: 1450, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 29.206, mean reward: 3.651 [3.119, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.451, 10.100], loss: 0.182725, mae: 0.398954, mean_q: 4.362941
 61342/100000: episode: 1451, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 33.303, mean reward: 4.758 [3.591, 6.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.395, 10.100], loss: 0.138182, mae: 0.403940, mean_q: 4.402990
 61356/100000: episode: 1452, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 59.235, mean reward: 4.231 [3.372, 5.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.602], loss: 0.229890, mae: 0.428627, mean_q: 4.521191
 61359/100000: episode: 1453, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 13.152, mean reward: 4.384 [3.524, 5.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.035, 10.603], loss: 0.153035, mae: 0.433140, mean_q: 4.555778
 61366/100000: episode: 1454, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 48.475, mean reward: 6.925 [4.079, 14.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.731], loss: 0.300034, mae: 0.442847, mean_q: 4.509857
 61375/100000: episode: 1455, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 34.062, mean reward: 3.785 [2.728, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.402, 10.100], loss: 0.237065, mae: 0.437873, mean_q: 4.664728
 61378/100000: episode: 1456, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 9.667, mean reward: 3.222 [3.129, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.462], loss: 0.164008, mae: 0.354958, mean_q: 4.546286
 61387/100000: episode: 1457, duration: 0.067s, episode steps: 9, steps per second: 133, episode reward: 39.441, mean reward: 4.382 [3.250, 5.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.682, 10.100], loss: 0.301981, mae: 0.480462, mean_q: 4.400898
 61401/100000: episode: 1458, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 78.866, mean reward: 5.633 [4.055, 7.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.132, 10.597], loss: 0.197570, mae: 0.434612, mean_q: 4.541594
 61408/100000: episode: 1459, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 36.920, mean reward: 5.274 [4.237, 6.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.444], loss: 0.424090, mae: 0.490629, mean_q: 4.657208
 61416/100000: episode: 1460, duration: 0.083s, episode steps: 8, steps per second: 96, episode reward: 24.860, mean reward: 3.107 [2.499, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.976, 10.100], loss: 0.244905, mae: 0.396128, mean_q: 4.444988
 61424/100000: episode: 1461, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 29.423, mean reward: 3.678 [3.031, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.482, 10.100], loss: 0.442791, mae: 0.509745, mean_q: 4.553738
 61434/100000: episode: 1462, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 38.571, mean reward: 3.857 [2.862, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.545], loss: 0.327021, mae: 0.445069, mean_q: 4.453510
 61444/100000: episode: 1463, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 47.844, mean reward: 4.784 [3.482, 6.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.442, 10.600], loss: 0.193604, mae: 0.406679, mean_q: 4.637593
 61453/100000: episode: 1464, duration: 0.086s, episode steps: 9, steps per second: 104, episode reward: 33.068, mean reward: 3.674 [3.019, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.458, 10.100], loss: 0.177038, mae: 0.392877, mean_q: 4.667303
 61459/100000: episode: 1465, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 20.485, mean reward: 3.414 [2.829, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.370, 10.100], loss: 0.153099, mae: 0.365747, mean_q: 4.571208
 61469/100000: episode: 1466, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 51.948, mean reward: 5.195 [4.066, 7.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.627], loss: 0.185867, mae: 0.384025, mean_q: 4.461594
 61476/100000: episode: 1467, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 35.911, mean reward: 5.130 [4.338, 6.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.521], loss: 4.659207, mae: 0.883487, mean_q: 5.053829
 61483/100000: episode: 1468, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 36.336, mean reward: 5.191 [4.113, 6.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.632, 10.617], loss: 0.427660, mae: 0.579322, mean_q: 4.280593
 61490/100000: episode: 1469, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 24.959, mean reward: 3.566 [2.813, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.489, 10.100], loss: 0.430236, mae: 0.601451, mean_q: 4.426814
 61499/100000: episode: 1470, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 35.591, mean reward: 3.955 [3.459, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.152, 10.100], loss: 0.390128, mae: 0.540363, mean_q: 4.745963
 61505/100000: episode: 1471, duration: 0.046s, episode steps: 6, steps per second: 132, episode reward: 20.741, mean reward: 3.457 [3.166, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.433, 10.100], loss: 0.471147, mae: 0.670929, mean_q: 4.820296
 61512/100000: episode: 1472, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 26.660, mean reward: 3.809 [3.202, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.526], loss: 0.237569, mae: 0.448255, mean_q: 4.416097
 61520/100000: episode: 1473, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 25.292, mean reward: 3.161 [2.345, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.350, 10.100], loss: 0.224839, mae: 0.437771, mean_q: 4.514440
 61527/100000: episode: 1474, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 31.053, mean reward: 4.436 [3.599, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.481, 10.100], loss: 0.403678, mae: 0.516843, mean_q: 4.599614
 61535/100000: episode: 1475, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 70.436, mean reward: 8.804 [3.772, 22.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.465, 10.100], loss: 0.621389, mae: 0.624230, mean_q: 4.908561
 61549/100000: episode: 1476, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 77.303, mean reward: 5.522 [3.758, 7.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.624], loss: 0.441042, mae: 0.513813, mean_q: 4.643025
 61558/100000: episode: 1477, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 32.507, mean reward: 3.612 [3.028, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.429, 10.100], loss: 0.274666, mae: 0.497938, mean_q: 4.726318
 61561/100000: episode: 1478, duration: 0.029s, episode steps: 3, steps per second: 105, episode reward: 15.529, mean reward: 5.176 [4.550, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.570], loss: 0.155442, mae: 0.418431, mean_q: 4.593862
 61567/100000: episode: 1479, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 30.019, mean reward: 5.003 [4.499, 5.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.493, 10.100], loss: 0.264434, mae: 0.427074, mean_q: 4.715176
 61581/100000: episode: 1480, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 67.917, mean reward: 4.851 [3.053, 7.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.546], loss: 0.311539, mae: 0.465385, mean_q: 4.711079
 61589/100000: episode: 1481, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 37.329, mean reward: 4.666 [3.714, 5.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.333, 10.100], loss: 0.311981, mae: 0.486307, mean_q: 4.847671
 61597/100000: episode: 1482, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 22.708, mean reward: 2.839 [2.751, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.362, 10.100], loss: 0.192664, mae: 0.421217, mean_q: 4.586233
 61605/100000: episode: 1483, duration: 0.085s, episode steps: 8, steps per second: 94, episode reward: 25.953, mean reward: 3.244 [2.391, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.559, 10.100], loss: 0.577147, mae: 0.525693, mean_q: 5.057832
 61619/100000: episode: 1484, duration: 0.117s, episode steps: 14, steps per second: 120, episode reward: 59.454, mean reward: 4.247 [2.541, 8.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.671], loss: 0.952236, mae: 0.544430, mean_q: 4.745279
 61628/100000: episode: 1485, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 34.761, mean reward: 3.862 [3.083, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.631, 10.100], loss: 0.449681, mae: 0.511378, mean_q: 4.766665
 61638/100000: episode: 1486, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 60.982, mean reward: 6.098 [4.830, 10.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.702], loss: 0.339041, mae: 0.535068, mean_q: 4.596597
[Info] FALSIFICATION!
 61643/100000: episode: 1487, duration: 0.310s, episode steps: 5, steps per second: 16, episode reward: 1053.248, mean reward: 210.650 [6.980, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.012, 10.786], loss: 0.279210, mae: 0.490277, mean_q: 4.593876
 61657/100000: episode: 1488, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 74.348, mean reward: 5.311 [3.609, 7.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.036, 10.528], loss: 0.336566, mae: 0.537590, mean_q: 4.850298
 61666/100000: episode: 1489, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 34.909, mean reward: 3.879 [3.183, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.332, 10.100], loss: 3.938680, mae: 0.946501, mean_q: 5.282244
 61675/100000: episode: 1490, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 30.550, mean reward: 3.394 [2.697, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.496, 10.100], loss: 0.419086, mae: 0.594789, mean_q: 4.624445
 61683/100000: episode: 1491, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 30.321, mean reward: 3.790 [2.541, 6.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.458, 10.100], loss: 0.236461, mae: 0.480037, mean_q: 4.948339
[Info] FALSIFICATION!
 61696/100000: episode: 1492, duration: 0.359s, episode steps: 13, steps per second: 36, episode reward: 1089.379, mean reward: 83.798 [3.964, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.012, 10.832], loss: 0.284854, mae: 0.508074, mean_q: 4.773514
 61706/100000: episode: 1493, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 60.321, mean reward: 6.032 [3.883, 8.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.473, 10.562], loss: 0.748800, mae: 0.592603, mean_q: 4.839430
 61714/100000: episode: 1494, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 32.797, mean reward: 4.100 [3.378, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.472, 10.100], loss: 0.290890, mae: 0.503848, mean_q: 4.991482
 61721/100000: episode: 1495, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 33.916, mean reward: 4.845 [2.426, 6.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.331], loss: 0.414357, mae: 0.482460, mean_q: 4.866072
 61735/100000: episode: 1496, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 62.589, mean reward: 4.471 [2.789, 7.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.584], loss: 0.451979, mae: 0.530299, mean_q: 4.896286
 61742/100000: episode: 1497, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 35.367, mean reward: 5.052 [4.207, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.600], loss: 0.344934, mae: 0.496869, mean_q: 4.884046
 61751/100000: episode: 1498, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 33.347, mean reward: 3.705 [2.938, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.467, 10.100], loss: 0.436592, mae: 0.496927, mean_q: 5.002828
 61765/100000: episode: 1499, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 56.103, mean reward: 4.007 [2.751, 6.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.694, 10.474], loss: 0.560488, mae: 0.574472, mean_q: 4.884285
 61773/100000: episode: 1500, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 35.020, mean reward: 4.377 [3.706, 5.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.427, 10.100], loss: 1.138033, mae: 0.618828, mean_q: 5.110731
 61783/100000: episode: 1501, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 45.546, mean reward: 4.555 [3.612, 6.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.145, 10.498], loss: 1.359813, mae: 0.625102, mean_q: 4.965142
 61791/100000: episode: 1502, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 27.996, mean reward: 3.499 [2.805, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.509, 10.100], loss: 0.689448, mae: 0.608318, mean_q: 4.773591
 61799/100000: episode: 1503, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 40.417, mean reward: 5.052 [2.843, 9.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.346, 10.100], loss: 0.263968, mae: 0.520851, mean_q: 5.069950
 61807/100000: episode: 1504, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 29.008, mean reward: 3.626 [2.961, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.502, 10.100], loss: 0.310822, mae: 0.482373, mean_q: 4.912058
 61815/100000: episode: 1505, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 22.983, mean reward: 2.873 [2.380, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.515, 10.100], loss: 0.236936, mae: 0.440800, mean_q: 4.855469
 61823/100000: episode: 1506, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 32.417, mean reward: 4.052 [3.267, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.501, 10.100], loss: 1913.182617, mae: 4.634030, mean_q: 5.386341
 61833/100000: episode: 1507, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 45.146, mean reward: 4.515 [3.751, 5.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.476], loss: 12.074754, mae: 3.741239, mean_q: 8.453501
 61847/100000: episode: 1508, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 57.761, mean reward: 4.126 [2.978, 5.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.492], loss: 1.876063, mae: 1.406708, mean_q: 4.931737
 61855/100000: episode: 1509, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 61.644, mean reward: 7.706 [4.374, 13.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.542, 10.100], loss: 1.206397, mae: 0.968469, mean_q: 4.954288
 61863/100000: episode: 1510, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 26.507, mean reward: 3.313 [3.002, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.394, 10.100], loss: 0.969288, mae: 0.865823, mean_q: 5.472938
 61871/100000: episode: 1511, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 60.158, mean reward: 7.520 [3.813, 24.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.501, 10.100], loss: 0.569454, mae: 0.720404, mean_q: 4.951698
 61879/100000: episode: 1512, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 41.818, mean reward: 5.227 [3.831, 6.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.450, 10.100], loss: 0.547637, mae: 0.626897, mean_q: 5.112532
 61886/100000: episode: 1513, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 47.859, mean reward: 6.837 [4.136, 10.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.562, 10.536], loss: 0.386713, mae: 0.598081, mean_q: 5.416402
 61894/100000: episode: 1514, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 34.391, mean reward: 4.299 [3.655, 6.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.648, 10.100], loss: 0.457464, mae: 0.573924, mean_q: 5.017301
 61904/100000: episode: 1515, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 68.292, mean reward: 6.829 [5.471, 9.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.449, 10.533], loss: 0.444052, mae: 0.597696, mean_q: 5.122282
 61907/100000: episode: 1516, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 12.254, mean reward: 4.085 [3.224, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.368, 10.548], loss: 0.468251, mae: 0.610270, mean_q: 5.255768
 61913/100000: episode: 1517, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 18.300, mean reward: 3.050 [2.757, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.418, 10.100], loss: 1.631683, mae: 0.745473, mean_q: 5.153755
 61922/100000: episode: 1518, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 31.911, mean reward: 3.546 [2.483, 5.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.390, 10.100], loss: 0.758453, mae: 0.677504, mean_q: 5.344366
 61936/100000: episode: 1519, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 58.776, mean reward: 4.198 [3.229, 6.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.423], loss: 1097.318237, mae: 4.455524, mean_q: 7.203171
 61943/100000: episode: 1520, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 38.226, mean reward: 5.461 [3.335, 10.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.429], loss: 4.159470, mae: 1.691972, mean_q: 6.365073
 61950/100000: episode: 1521, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 44.649, mean reward: 6.378 [4.894, 8.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.598], loss: 0.916692, mae: 0.968646, mean_q: 5.392396
 61958/100000: episode: 1522, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 24.939, mean reward: 3.117 [2.621, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.476, 10.100], loss: 1923.687012, mae: 4.857273, mean_q: 5.203822
 61972/100000: episode: 1523, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 58.248, mean reward: 4.161 [3.050, 6.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.475], loss: 1091.161743, mae: 4.217635, mean_q: 7.002272
 61975/100000: episode: 1524, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 16.737, mean reward: 5.579 [4.390, 6.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.603], loss: 5.558165, mae: 2.630387, mean_q: 7.568152
 61981/100000: episode: 1525, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 24.606, mean reward: 4.101 [3.510, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.449, 10.100], loss: 8.970489, mae: 2.722390, mean_q: 7.677881
 61989/100000: episode: 1526, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 26.126, mean reward: 3.266 [2.227, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.459, 10.100], loss: 1904.857178, mae: 5.650359, mean_q: 7.124972
 61998/100000: episode: 1527, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 38.777, mean reward: 4.309 [3.827, 5.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.664, 10.100], loss: 3.648714, mae: 1.940269, mean_q: 7.144801
 62004/100000: episode: 1528, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 24.669, mean reward: 4.111 [3.515, 5.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.444, 10.100], loss: 3.382838, mae: 1.440586, mean_q: 6.935928
 62014/100000: episode: 1529, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 91.607, mean reward: 9.161 [4.738, 16.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.572], loss: 0.903740, mae: 0.835625, mean_q: 6.197148
[Info] Complete ISplit Iteration
[Info] Levels: [5.5839934, 8.1474495, 13.462286]
[Info] Cond. Prob: [0.1, 0.1, 0.05]
[Info] Error Prob: 0.0005000000000000001

 62021/100000: episode: 1530, duration: 4.654s, episode steps: 7, steps per second: 2, episode reward: 41.221, mean reward: 5.889 [3.738, 14.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.402, 10.100], loss: 2181.335693, mae: 5.192996, mean_q: 5.897438
 62121/100000: episode: 1531, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 188.617, mean reward: 1.886 [1.464, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.098], loss: 153.712646, mae: 1.328140, mean_q: 6.249955
 62221/100000: episode: 1532, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 199.586, mean reward: 1.996 [1.443, 7.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.907, 10.185], loss: 1.409692, mae: 0.744040, mean_q: 5.756589
 62321/100000: episode: 1533, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.863, mean reward: 1.849 [1.450, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.260], loss: 153.986526, mae: 1.292115, mean_q: 5.855566
 62421/100000: episode: 1534, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.258, mean reward: 1.913 [1.444, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.866, 10.098], loss: 153.559509, mae: 1.051092, mean_q: 5.628085
 62521/100000: episode: 1535, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 239.754, mean reward: 2.398 [1.461, 5.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.884, 10.098], loss: 154.846832, mae: 1.566771, mean_q: 6.084065
 62621/100000: episode: 1536, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 201.793, mean reward: 2.018 [1.442, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.894, 10.201], loss: 305.785370, mae: 1.536943, mean_q: 5.726142
 62721/100000: episode: 1537, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.655, mean reward: 1.897 [1.461, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.484, 10.105], loss: 458.493744, mae: 2.650955, mean_q: 6.729592
 62821/100000: episode: 1538, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 189.565, mean reward: 1.896 [1.502, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.805, 10.135], loss: 1.493920, mae: 0.830072, mean_q: 5.542801
 62921/100000: episode: 1539, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 180.935, mean reward: 1.809 [1.483, 2.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.101, 10.098], loss: 1.387234, mae: 0.969176, mean_q: 5.818787
 63021/100000: episode: 1540, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 177.249, mean reward: 1.772 [1.449, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.722, 10.159], loss: 153.494080, mae: 1.293828, mean_q: 5.889887
 63121/100000: episode: 1541, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.827, mean reward: 1.918 [1.471, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.419, 10.234], loss: 1.426637, mae: 0.759476, mean_q: 5.462142
 63221/100000: episode: 1542, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 192.982, mean reward: 1.930 [1.486, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.248, 10.098], loss: 0.703719, mae: 0.639503, mean_q: 5.312696
 63321/100000: episode: 1543, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 215.736, mean reward: 2.157 [1.460, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.673, 10.098], loss: 0.650527, mae: 0.618806, mean_q: 5.231774
 63421/100000: episode: 1544, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 181.632, mean reward: 1.816 [1.439, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.157], loss: 154.726562, mae: 1.415039, mean_q: 5.834909
 63521/100000: episode: 1545, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 179.408, mean reward: 1.794 [1.449, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.962, 10.113], loss: 153.772858, mae: 1.297966, mean_q: 5.681037
 63621/100000: episode: 1546, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 192.754, mean reward: 1.928 [1.468, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.476, 10.227], loss: 1.402590, mae: 0.751032, mean_q: 5.430246
 63721/100000: episode: 1547, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 174.867, mean reward: 1.749 [1.448, 2.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.917, 10.098], loss: 153.909332, mae: 1.329489, mean_q: 5.714092
 63821/100000: episode: 1548, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 186.129, mean reward: 1.861 [1.461, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.243, 10.121], loss: 1.146063, mae: 0.724050, mean_q: 5.388144
 63921/100000: episode: 1549, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 186.602, mean reward: 1.866 [1.471, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.458, 10.315], loss: 456.792999, mae: 2.200315, mean_q: 6.208474
 64021/100000: episode: 1550, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 196.933, mean reward: 1.969 [1.469, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.789, 10.098], loss: 153.033768, mae: 1.308449, mean_q: 5.696973
 64121/100000: episode: 1551, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 203.000, mean reward: 2.030 [1.448, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.967, 10.098], loss: 153.331894, mae: 1.358555, mean_q: 5.866174
 64221/100000: episode: 1552, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 211.242, mean reward: 2.112 [1.510, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.098], loss: 1.147463, mae: 0.699820, mean_q: 5.479983
 64321/100000: episode: 1553, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.497, mean reward: 1.855 [1.479, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.905, 10.275], loss: 0.861630, mae: 0.642578, mean_q: 5.273381
 64421/100000: episode: 1554, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 183.785, mean reward: 1.838 [1.455, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.887, 10.337], loss: 154.344925, mae: 1.359194, mean_q: 5.675930
 64521/100000: episode: 1555, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.806, mean reward: 1.898 [1.463, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.560, 10.263], loss: 153.765259, mae: 1.326488, mean_q: 5.751646
 64621/100000: episode: 1556, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.344, mean reward: 1.953 [1.465, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.014, 10.149], loss: 1.131210, mae: 0.699709, mean_q: 5.213519
 64721/100000: episode: 1557, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.699, mean reward: 1.987 [1.482, 5.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.339, 10.205], loss: 153.318649, mae: 1.269005, mean_q: 5.652495
 64821/100000: episode: 1558, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 178.481, mean reward: 1.785 [1.470, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.629, 10.272], loss: 455.772644, mae: 1.916920, mean_q: 5.825438
 64921/100000: episode: 1559, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.324, mean reward: 2.033 [1.493, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.289], loss: 154.080124, mae: 1.513106, mean_q: 5.994831
 65021/100000: episode: 1560, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.021, mean reward: 1.860 [1.475, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.679, 10.325], loss: 0.771712, mae: 0.650942, mean_q: 5.137556
 65121/100000: episode: 1561, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.796, mean reward: 1.918 [1.479, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.130, 10.098], loss: 153.707031, mae: 1.213964, mean_q: 5.488628
 65221/100000: episode: 1562, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 191.189, mean reward: 1.912 [1.456, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.073, 10.098], loss: 1.485014, mae: 0.700967, mean_q: 5.117311
 65321/100000: episode: 1563, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.014, mean reward: 1.930 [1.469, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.464, 10.238], loss: 152.737534, mae: 0.941262, mean_q: 4.989345
 65421/100000: episode: 1564, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 198.652, mean reward: 1.987 [1.431, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.720, 10.098], loss: 153.057281, mae: 1.226423, mean_q: 5.443815
 65521/100000: episode: 1565, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 203.136, mean reward: 2.031 [1.441, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.603, 10.098], loss: 1.370508, mae: 0.668818, mean_q: 5.043208
 65621/100000: episode: 1566, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 199.679, mean reward: 1.997 [1.502, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.970, 10.279], loss: 1.152123, mae: 0.593668, mean_q: 4.931193
 65721/100000: episode: 1567, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 207.456, mean reward: 2.075 [1.485, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.044, 10.119], loss: 304.439911, mae: 1.749471, mean_q: 5.594660
 65821/100000: episode: 1568, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 196.415, mean reward: 1.964 [1.463, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.767, 10.098], loss: 153.552673, mae: 1.171488, mean_q: 5.352232
 65921/100000: episode: 1569, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 196.600, mean reward: 1.966 [1.528, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.285, 10.248], loss: 453.826599, mae: 2.051829, mean_q: 5.511892
 66021/100000: episode: 1570, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 186.943, mean reward: 1.869 [1.446, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.991, 10.194], loss: 303.323761, mae: 2.002404, mean_q: 5.870289
 66121/100000: episode: 1571, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 190.368, mean reward: 1.904 [1.483, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.659, 10.209], loss: 151.829330, mae: 0.958120, mean_q: 5.137549
 66221/100000: episode: 1572, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 185.867, mean reward: 1.859 [1.474, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.098], loss: 151.605682, mae: 1.359285, mean_q: 5.464404
 66321/100000: episode: 1573, duration: 1.564s, episode steps: 100, steps per second: 64, episode reward: 198.313, mean reward: 1.983 [1.466, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.285, 10.327], loss: 302.436249, mae: 1.628735, mean_q: 5.217695
 66421/100000: episode: 1574, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 199.424, mean reward: 1.994 [1.461, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.296, 10.098], loss: 151.858154, mae: 1.115222, mean_q: 4.996633
 66521/100000: episode: 1575, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 192.917, mean reward: 1.929 [1.474, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.868, 10.098], loss: 301.480621, mae: 1.411862, mean_q: 4.895784
 66621/100000: episode: 1576, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 190.781, mean reward: 1.908 [1.463, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.583, 10.098], loss: 1.226560, mae: 0.613297, mean_q: 4.584205
 66721/100000: episode: 1577, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 204.646, mean reward: 2.046 [1.464, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.656, 10.098], loss: 0.261759, mae: 0.381263, mean_q: 4.079995
 66821/100000: episode: 1578, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 185.521, mean reward: 1.855 [1.449, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.891, 10.098], loss: 0.197639, mae: 0.364633, mean_q: 3.998789
 66921/100000: episode: 1579, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 183.151, mean reward: 1.832 [1.471, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.489, 10.284], loss: 0.184450, mae: 0.343491, mean_q: 3.916554
 67021/100000: episode: 1580, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 194.746, mean reward: 1.947 [1.494, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.617, 10.098], loss: 0.122087, mae: 0.316356, mean_q: 3.835531
 67121/100000: episode: 1581, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 194.564, mean reward: 1.946 [1.436, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.249, 10.272], loss: 0.093474, mae: 0.304464, mean_q: 3.839288
 67221/100000: episode: 1582, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 179.488, mean reward: 1.795 [1.441, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.318, 10.098], loss: 0.089329, mae: 0.294433, mean_q: 3.824588
 67321/100000: episode: 1583, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 190.018, mean reward: 1.900 [1.490, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.255, 10.098], loss: 0.082224, mae: 0.287104, mean_q: 3.814414
 67421/100000: episode: 1584, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 200.151, mean reward: 2.002 [1.458, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.266], loss: 0.080758, mae: 0.284967, mean_q: 3.801297
 67521/100000: episode: 1585, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 208.574, mean reward: 2.086 [1.449, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.996, 10.098], loss: 0.083336, mae: 0.287468, mean_q: 3.799654
 67621/100000: episode: 1586, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.134, mean reward: 1.911 [1.463, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.531, 10.098], loss: 0.081460, mae: 0.284456, mean_q: 3.799586
 67721/100000: episode: 1587, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 199.683, mean reward: 1.997 [1.458, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.699, 10.276], loss: 0.087343, mae: 0.293652, mean_q: 3.806484
 67821/100000: episode: 1588, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 190.633, mean reward: 1.906 [1.464, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.927, 10.098], loss: 0.086151, mae: 0.285821, mean_q: 3.807837
 67921/100000: episode: 1589, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 207.189, mean reward: 2.072 [1.470, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.719, 10.098], loss: 0.076763, mae: 0.284302, mean_q: 3.803985
 68021/100000: episode: 1590, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 203.575, mean reward: 2.036 [1.472, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.438, 10.098], loss: 0.084748, mae: 0.290457, mean_q: 3.827852
 68121/100000: episode: 1591, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 183.556, mean reward: 1.836 [1.456, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.398, 10.098], loss: 0.082293, mae: 0.285093, mean_q: 3.831585
 68221/100000: episode: 1592, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 198.524, mean reward: 1.985 [1.486, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.844, 10.098], loss: 0.084200, mae: 0.287566, mean_q: 3.825665
 68321/100000: episode: 1593, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.241, mean reward: 1.862 [1.487, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.753, 10.228], loss: 0.087628, mae: 0.293529, mean_q: 3.829991
 68421/100000: episode: 1594, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 200.240, mean reward: 2.002 [1.460, 4.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.986, 10.131], loss: 0.085120, mae: 0.292243, mean_q: 3.837242
 68521/100000: episode: 1595, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 178.032, mean reward: 1.780 [1.444, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.966, 10.164], loss: 0.079787, mae: 0.282847, mean_q: 3.817382
 68621/100000: episode: 1596, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.018, mean reward: 1.950 [1.453, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.194, 10.098], loss: 0.085097, mae: 0.288503, mean_q: 3.834747
 68721/100000: episode: 1597, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 198.023, mean reward: 1.980 [1.462, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.802, 10.098], loss: 0.081916, mae: 0.281380, mean_q: 3.838517
 68821/100000: episode: 1598, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 194.042, mean reward: 1.940 [1.448, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.324], loss: 0.086888, mae: 0.293201, mean_q: 3.840857
 68921/100000: episode: 1599, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.581, mean reward: 1.916 [1.453, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.253, 10.098], loss: 0.079761, mae: 0.282744, mean_q: 3.835969
 69021/100000: episode: 1600, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.684, mean reward: 1.927 [1.494, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.135], loss: 0.075860, mae: 0.277497, mean_q: 3.817669
 69121/100000: episode: 1601, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.594, mean reward: 1.916 [1.480, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.243], loss: 0.078052, mae: 0.280745, mean_q: 3.828374
 69221/100000: episode: 1602, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.404, mean reward: 2.034 [1.500, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.292, 10.316], loss: 0.082288, mae: 0.277965, mean_q: 3.830752
 69321/100000: episode: 1603, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 180.224, mean reward: 1.802 [1.447, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.985, 10.116], loss: 0.075008, mae: 0.280036, mean_q: 3.820679
 69421/100000: episode: 1604, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 211.945, mean reward: 2.119 [1.508, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.172, 10.098], loss: 0.082494, mae: 0.284412, mean_q: 3.846120
 69521/100000: episode: 1605, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 200.180, mean reward: 2.002 [1.445, 6.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.524, 10.114], loss: 0.081610, mae: 0.289259, mean_q: 3.871045
 69621/100000: episode: 1606, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 186.956, mean reward: 1.870 [1.442, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.893, 10.108], loss: 0.079871, mae: 0.279526, mean_q: 3.837754
 69721/100000: episode: 1607, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 211.903, mean reward: 2.119 [1.475, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.057, 10.156], loss: 0.081749, mae: 0.281977, mean_q: 3.846178
 69821/100000: episode: 1608, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 213.664, mean reward: 2.137 [1.495, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.493, 10.098], loss: 0.082355, mae: 0.285203, mean_q: 3.840851
 69921/100000: episode: 1609, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 221.992, mean reward: 2.220 [1.490, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.639, 10.187], loss: 0.081426, mae: 0.285630, mean_q: 3.872759
 70021/100000: episode: 1610, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 181.529, mean reward: 1.815 [1.446, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.889, 10.098], loss: 0.083513, mae: 0.285377, mean_q: 3.854243
 70121/100000: episode: 1611, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 197.298, mean reward: 1.973 [1.508, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.668, 10.098], loss: 0.094932, mae: 0.298394, mean_q: 3.878419
 70221/100000: episode: 1612, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 189.862, mean reward: 1.899 [1.518, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.890, 10.098], loss: 0.083477, mae: 0.291578, mean_q: 3.864848
 70321/100000: episode: 1613, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 193.060, mean reward: 1.931 [1.443, 5.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.210, 10.098], loss: 0.080393, mae: 0.289811, mean_q: 3.885456
 70421/100000: episode: 1614, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 210.787, mean reward: 2.108 [1.503, 6.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.582, 10.098], loss: 0.080104, mae: 0.285003, mean_q: 3.866037
 70521/100000: episode: 1615, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 192.339, mean reward: 1.923 [1.511, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.301, 10.315], loss: 0.085298, mae: 0.287822, mean_q: 3.875355
 70621/100000: episode: 1616, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 180.090, mean reward: 1.801 [1.454, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.024, 10.178], loss: 0.081100, mae: 0.280495, mean_q: 3.880459
 70721/100000: episode: 1617, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 215.434, mean reward: 2.154 [1.457, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.520, 10.109], loss: 0.094275, mae: 0.290772, mean_q: 3.872733
 70821/100000: episode: 1618, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.635, mean reward: 1.976 [1.448, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.443, 10.294], loss: 0.071928, mae: 0.271663, mean_q: 3.872733
 70921/100000: episode: 1619, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 182.997, mean reward: 1.830 [1.458, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.953, 10.168], loss: 0.084039, mae: 0.287773, mean_q: 3.876090
 71021/100000: episode: 1620, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.561, mean reward: 1.976 [1.443, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.022, 10.245], loss: 0.081489, mae: 0.279188, mean_q: 3.868313
 71121/100000: episode: 1621, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.824, mean reward: 1.888 [1.466, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.949, 10.218], loss: 0.080238, mae: 0.281219, mean_q: 3.868875
 71221/100000: episode: 1622, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 204.032, mean reward: 2.040 [1.444, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.537, 10.480], loss: 0.075960, mae: 0.278492, mean_q: 3.867853
 71321/100000: episode: 1623, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 184.638, mean reward: 1.846 [1.436, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.245], loss: 0.082498, mae: 0.286930, mean_q: 3.864280
 71421/100000: episode: 1624, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 185.884, mean reward: 1.859 [1.484, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.213], loss: 0.075228, mae: 0.271470, mean_q: 3.871723
 71521/100000: episode: 1625, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 214.576, mean reward: 2.146 [1.473, 5.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.982, 10.338], loss: 0.077349, mae: 0.283191, mean_q: 3.874072
 71621/100000: episode: 1626, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 196.173, mean reward: 1.962 [1.470, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.470, 10.318], loss: 0.088245, mae: 0.288768, mean_q: 3.868112
 71721/100000: episode: 1627, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 207.623, mean reward: 2.076 [1.456, 6.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.957, 10.310], loss: 0.080202, mae: 0.290092, mean_q: 3.883035
 71821/100000: episode: 1628, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 180.784, mean reward: 1.808 [1.471, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.924, 10.098], loss: 0.094220, mae: 0.287205, mean_q: 3.882575
 71921/100000: episode: 1629, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 187.694, mean reward: 1.877 [1.459, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.248, 10.125], loss: 0.079750, mae: 0.284114, mean_q: 3.850385
[Info] 1-TH LEVEL FOUND: 5.014353275299072, Considering 10/90 traces
 72021/100000: episode: 1630, duration: 5.309s, episode steps: 100, steps per second: 19, episode reward: 189.683, mean reward: 1.897 [1.464, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.374, 10.098], loss: 0.090945, mae: 0.296185, mean_q: 3.861849
 72032/100000: episode: 1631, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 30.544, mean reward: 2.777 [2.290, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.370, 10.100], loss: 0.093452, mae: 0.313475, mean_q: 3.885789
 72052/100000: episode: 1632, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 74.638, mean reward: 3.732 [2.725, 5.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.390, 10.100], loss: 0.078278, mae: 0.290523, mean_q: 3.873385
 72063/100000: episode: 1633, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 21.291, mean reward: 1.936 [1.680, 2.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.239, 10.100], loss: 0.108315, mae: 0.294150, mean_q: 3.873500
 72079/100000: episode: 1634, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 45.320, mean reward: 2.832 [2.044, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.006, 10.100], loss: 0.082905, mae: 0.299662, mean_q: 3.901724
 72090/100000: episode: 1635, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 29.266, mean reward: 2.661 [2.002, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.271, 10.100], loss: 0.133497, mae: 0.322746, mean_q: 3.919588
 72101/100000: episode: 1636, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 23.412, mean reward: 2.128 [1.824, 2.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.335, 10.100], loss: 0.131202, mae: 0.322623, mean_q: 3.903877
 72113/100000: episode: 1637, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 24.375, mean reward: 2.031 [1.817, 2.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.323, 10.100], loss: 0.065923, mae: 0.282010, mean_q: 3.940723
 72124/100000: episode: 1638, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 21.291, mean reward: 1.936 [1.744, 2.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.246, 10.100], loss: 0.094431, mae: 0.298661, mean_q: 3.881444
 72142/100000: episode: 1639, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 50.863, mean reward: 2.826 [2.157, 4.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.662, 10.100], loss: 0.103881, mae: 0.288478, mean_q: 3.868880
 72160/100000: episode: 1640, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 59.696, mean reward: 3.316 [2.387, 6.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.692, 10.100], loss: 0.075204, mae: 0.283585, mean_q: 3.876447
 72180/100000: episode: 1641, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 60.051, mean reward: 3.003 [2.054, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.343, 10.100], loss: 0.083949, mae: 0.294537, mean_q: 3.900808
 72190/100000: episode: 1642, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 25.738, mean reward: 2.574 [1.810, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.454, 10.100], loss: 0.094245, mae: 0.293120, mean_q: 3.925046
 72206/100000: episode: 1643, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 29.618, mean reward: 1.851 [1.444, 2.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.100], loss: 0.116542, mae: 0.313945, mean_q: 3.921311
 72224/100000: episode: 1644, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 48.017, mean reward: 2.668 [2.210, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.382, 10.100], loss: 0.091916, mae: 0.301369, mean_q: 3.892142
 72235/100000: episode: 1645, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 26.782, mean reward: 2.435 [1.879, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.449, 10.100], loss: 0.096366, mae: 0.314590, mean_q: 3.915215
 72246/100000: episode: 1646, duration: 0.057s, episode steps: 11, steps per second: 191, episode reward: 23.028, mean reward: 2.093 [1.765, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.108, 10.100], loss: 0.136051, mae: 0.336793, mean_q: 3.915199
 72266/100000: episode: 1647, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 76.125, mean reward: 3.806 [2.519, 8.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.192, 10.100], loss: 0.105215, mae: 0.325301, mean_q: 3.906012
 72282/100000: episode: 1648, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 41.408, mean reward: 2.588 [2.115, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.304, 10.100], loss: 0.094228, mae: 0.311098, mean_q: 3.941030
 72292/100000: episode: 1649, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 19.947, mean reward: 1.995 [1.874, 2.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.138, 10.100], loss: 0.086745, mae: 0.302112, mean_q: 3.935258
 72303/100000: episode: 1650, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 22.695, mean reward: 2.063 [1.761, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.144, 10.100], loss: 0.165204, mae: 0.351259, mean_q: 3.974689
 72321/100000: episode: 1651, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 53.903, mean reward: 2.995 [2.169, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.243, 10.100], loss: 0.162519, mae: 0.354853, mean_q: 3.971245
 72341/100000: episode: 1652, duration: 0.167s, episode steps: 20, steps per second: 120, episode reward: 62.770, mean reward: 3.139 [2.641, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.329, 10.100], loss: 0.102361, mae: 0.315027, mean_q: 3.944186
 72361/100000: episode: 1653, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 105.083, mean reward: 5.254 [2.977, 19.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.996, 10.100], loss: 0.087095, mae: 0.306349, mean_q: 3.927488
 72372/100000: episode: 1654, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 24.872, mean reward: 2.261 [1.986, 2.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.233, 10.100], loss: 0.105219, mae: 0.327476, mean_q: 3.879959
 72388/100000: episode: 1655, duration: 0.112s, episode steps: 16, steps per second: 143, episode reward: 38.077, mean reward: 2.380 [1.966, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.946, 10.100], loss: 0.143655, mae: 0.354432, mean_q: 3.966406
 72399/100000: episode: 1656, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 23.223, mean reward: 2.111 [1.852, 2.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.244, 10.100], loss: 0.129634, mae: 0.335160, mean_q: 4.038235
 72409/100000: episode: 1657, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 23.132, mean reward: 2.313 [2.027, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.138, 10.100], loss: 0.127523, mae: 0.339252, mean_q: 3.934969
 72420/100000: episode: 1658, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 24.986, mean reward: 2.271 [1.759, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.327, 10.100], loss: 0.159318, mae: 0.343490, mean_q: 3.978034
 72432/100000: episode: 1659, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 23.190, mean reward: 1.933 [1.609, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.156, 10.100], loss: 0.132595, mae: 0.320893, mean_q: 4.033497
 72450/100000: episode: 1660, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 44.056, mean reward: 2.448 [2.136, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.174, 10.100], loss: 0.081126, mae: 0.293838, mean_q: 3.949757
 72468/100000: episode: 1661, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 52.188, mean reward: 2.899 [2.128, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.314, 10.100], loss: 0.088665, mae: 0.307752, mean_q: 3.944492
 72488/100000: episode: 1662, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 53.267, mean reward: 2.663 [2.165, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.377, 10.100], loss: 0.103724, mae: 0.318398, mean_q: 3.994717
 72500/100000: episode: 1663, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 28.013, mean reward: 2.334 [1.828, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.416, 10.100], loss: 0.225600, mae: 0.336496, mean_q: 3.974448
 72511/100000: episode: 1664, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 27.868, mean reward: 2.533 [1.994, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.187, 10.100], loss: 0.154235, mae: 0.338997, mean_q: 4.090056
 72531/100000: episode: 1665, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 45.669, mean reward: 2.283 [1.846, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.175, 10.100], loss: 0.120098, mae: 0.335356, mean_q: 4.042053
 72543/100000: episode: 1666, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 26.565, mean reward: 2.214 [1.983, 2.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.221, 10.100], loss: 0.087741, mae: 0.297932, mean_q: 3.996510
 72555/100000: episode: 1667, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 34.838, mean reward: 2.903 [2.451, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.301, 10.100], loss: 0.085661, mae: 0.292416, mean_q: 3.971134
 72566/100000: episode: 1668, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 28.333, mean reward: 2.576 [2.177, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.361, 10.100], loss: 0.094438, mae: 0.319637, mean_q: 4.006681
 72582/100000: episode: 1669, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 37.434, mean reward: 2.340 [1.942, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.505, 10.100], loss: 0.132041, mae: 0.351447, mean_q: 3.975845
 72602/100000: episode: 1670, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 62.565, mean reward: 3.128 [2.262, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.371, 10.100], loss: 0.325226, mae: 0.353839, mean_q: 3.949060
 72618/100000: episode: 1671, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 46.448, mean reward: 2.903 [2.216, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.241, 10.100], loss: 0.102387, mae: 0.309616, mean_q: 3.998553
 72630/100000: episode: 1672, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 31.887, mean reward: 2.657 [2.312, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.269, 10.100], loss: 0.081744, mae: 0.302918, mean_q: 4.027810
 72641/100000: episode: 1673, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 25.301, mean reward: 2.300 [1.984, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.102, 10.100], loss: 0.089726, mae: 0.291869, mean_q: 3.908767
 72661/100000: episode: 1674, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 58.590, mean reward: 2.930 [2.518, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.294, 10.100], loss: 0.095137, mae: 0.315502, mean_q: 4.016741
 72679/100000: episode: 1675, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 53.502, mean reward: 2.972 [2.179, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.418, 10.100], loss: 0.372824, mae: 0.404403, mean_q: 4.124045
 72697/100000: episode: 1676, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 62.436, mean reward: 3.469 [2.434, 6.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.197, 10.100], loss: 0.126202, mae: 0.324472, mean_q: 4.007774
 72708/100000: episode: 1677, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 26.157, mean reward: 2.378 [1.835, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.274, 10.100], loss: 0.489020, mae: 0.401617, mean_q: 4.138951
 72726/100000: episode: 1678, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 39.201, mean reward: 2.178 [1.576, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.797, 10.100], loss: 0.121710, mae: 0.338314, mean_q: 4.061614
 72738/100000: episode: 1679, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 36.734, mean reward: 3.061 [2.344, 6.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.298, 10.100], loss: 0.125305, mae: 0.324673, mean_q: 3.985467
 72749/100000: episode: 1680, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 24.658, mean reward: 2.242 [1.981, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.155, 10.100], loss: 0.217934, mae: 0.345949, mean_q: 4.084937
 72759/100000: episode: 1681, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 25.315, mean reward: 2.531 [1.962, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.281, 10.100], loss: 0.128223, mae: 0.324281, mean_q: 4.009598
 72770/100000: episode: 1682, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 25.606, mean reward: 2.328 [1.949, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.348, 10.100], loss: 0.260967, mae: 0.389393, mean_q: 4.127112
 72786/100000: episode: 1683, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 41.824, mean reward: 2.614 [1.922, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.323, 10.100], loss: 0.118888, mae: 0.323946, mean_q: 4.018437
 72797/100000: episode: 1684, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 25.160, mean reward: 2.287 [1.851, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.518, 10.100], loss: 0.113073, mae: 0.318346, mean_q: 4.016761
 72817/100000: episode: 1685, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 74.077, mean reward: 3.704 [2.425, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.887, 10.100], loss: 0.178483, mae: 0.378520, mean_q: 4.079548
 72833/100000: episode: 1686, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 42.710, mean reward: 2.669 [2.154, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.455, 10.100], loss: 0.182106, mae: 0.378870, mean_q: 4.084103
 72843/100000: episode: 1687, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 18.965, mean reward: 1.896 [1.603, 2.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.647, 10.100], loss: 0.174498, mae: 0.383133, mean_q: 4.158803
 72854/100000: episode: 1688, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 23.034, mean reward: 2.094 [1.908, 2.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.707, 10.100], loss: 0.124012, mae: 0.334695, mean_q: 4.109632
 72870/100000: episode: 1689, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 32.478, mean reward: 2.030 [1.502, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.164, 10.100], loss: 0.142808, mae: 0.338798, mean_q: 4.043087
 72881/100000: episode: 1690, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 25.403, mean reward: 2.309 [1.790, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.128, 10.100], loss: 0.129080, mae: 0.337598, mean_q: 4.053877
 72892/100000: episode: 1691, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 28.692, mean reward: 2.608 [2.091, 3.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.339, 10.100], loss: 0.114290, mae: 0.336437, mean_q: 4.146435
 72908/100000: episode: 1692, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 34.498, mean reward: 2.156 [1.702, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.113, 10.100], loss: 0.374178, mae: 0.377434, mean_q: 4.115506
 72919/100000: episode: 1693, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 22.858, mean reward: 2.078 [1.626, 2.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.989, 10.100], loss: 0.266133, mae: 0.377095, mean_q: 4.100495
 72935/100000: episode: 1694, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 35.952, mean reward: 2.247 [1.600, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.382, 10.100], loss: 0.151824, mae: 0.369849, mean_q: 4.053998
 72947/100000: episode: 1695, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 37.216, mean reward: 3.101 [2.405, 4.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.185, 10.100], loss: 0.172896, mae: 0.340015, mean_q: 4.068016
 72958/100000: episode: 1696, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 20.694, mean reward: 1.881 [1.592, 2.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.209, 10.100], loss: 0.189297, mae: 0.393383, mean_q: 4.178756
 72974/100000: episode: 1697, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 32.264, mean reward: 2.017 [1.735, 2.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.193, 10.100], loss: 0.134568, mae: 0.360789, mean_q: 4.055105
 72994/100000: episode: 1698, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 51.492, mean reward: 2.575 [1.838, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.489, 10.100], loss: 0.116408, mae: 0.339919, mean_q: 4.125554
 73014/100000: episode: 1699, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 46.851, mean reward: 2.343 [1.772, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.897, 10.100], loss: 0.134169, mae: 0.330577, mean_q: 4.057861
 73030/100000: episode: 1700, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 54.462, mean reward: 3.404 [2.584, 5.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.785, 10.100], loss: 0.231417, mae: 0.380533, mean_q: 4.107131
 73050/100000: episode: 1701, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 55.535, mean reward: 2.777 [2.210, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.281, 10.100], loss: 0.196127, mae: 0.371990, mean_q: 4.180680
 73061/100000: episode: 1702, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 25.220, mean reward: 2.293 [2.055, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.227, 10.100], loss: 0.125736, mae: 0.337883, mean_q: 3.978579
 73077/100000: episode: 1703, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 35.100, mean reward: 2.194 [1.680, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.238, 10.100], loss: 0.142223, mae: 0.359956, mean_q: 4.117228
 73089/100000: episode: 1704, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 28.903, mean reward: 2.409 [2.087, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.236, 10.100], loss: 0.136997, mae: 0.370423, mean_q: 4.081682
 73107/100000: episode: 1705, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 39.638, mean reward: 2.202 [1.803, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.369, 10.100], loss: 0.132899, mae: 0.357293, mean_q: 4.136907
 73118/100000: episode: 1706, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 29.665, mean reward: 2.697 [2.380, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.275, 10.100], loss: 0.114211, mae: 0.324330, mean_q: 4.090668
 73134/100000: episode: 1707, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 37.509, mean reward: 2.344 [1.878, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.381, 10.100], loss: 0.140851, mae: 0.351167, mean_q: 4.073573
 73144/100000: episode: 1708, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 20.278, mean reward: 2.028 [1.724, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.100], loss: 0.123633, mae: 0.344210, mean_q: 4.111387
 73155/100000: episode: 1709, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 25.728, mean reward: 2.339 [1.799, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.355, 10.100], loss: 0.117555, mae: 0.349500, mean_q: 4.136952
 73166/100000: episode: 1710, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 22.599, mean reward: 2.054 [1.558, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.064, 10.100], loss: 0.115919, mae: 0.336093, mean_q: 4.103892
 73182/100000: episode: 1711, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 55.076, mean reward: 3.442 [2.595, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.548, 10.100], loss: 0.109498, mae: 0.325196, mean_q: 4.085309
 73192/100000: episode: 1712, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 21.463, mean reward: 2.146 [1.936, 2.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.344, 10.100], loss: 0.100960, mae: 0.342239, mean_q: 4.130052
 73202/100000: episode: 1713, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 20.476, mean reward: 2.048 [1.815, 2.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.438, 10.100], loss: 0.165424, mae: 0.351545, mean_q: 4.132355
 73214/100000: episode: 1714, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 36.381, mean reward: 3.032 [2.674, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.255, 10.100], loss: 0.198931, mae: 0.393686, mean_q: 4.110475
 73226/100000: episode: 1715, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 30.035, mean reward: 2.503 [2.149, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.288, 10.100], loss: 0.179683, mae: 0.412926, mean_q: 4.187123
 73237/100000: episode: 1716, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 28.616, mean reward: 2.601 [2.272, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.474, 10.100], loss: 0.274431, mae: 0.376692, mean_q: 4.109710
 73249/100000: episode: 1717, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 28.944, mean reward: 2.412 [1.940, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.319, 10.100], loss: 0.148323, mae: 0.362955, mean_q: 4.109836
 73260/100000: episode: 1718, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 22.497, mean reward: 2.045 [1.820, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.976, 10.100], loss: 0.151777, mae: 0.338640, mean_q: 4.160280
 73270/100000: episode: 1719, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 19.075, mean reward: 1.908 [1.612, 2.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.714, 10.100], loss: 0.129790, mae: 0.356562, mean_q: 4.147369
[Info] 2-TH LEVEL FOUND: 6.71881103515625, Considering 10/90 traces
 73281/100000: episode: 1720, duration: 4.729s, episode steps: 11, steps per second: 2, episode reward: 23.031, mean reward: 2.094 [1.802, 2.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.564, 10.100], loss: 0.135585, mae: 0.348665, mean_q: 4.057215
 73294/100000: episode: 1721, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 56.976, mean reward: 4.383 [2.780, 9.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.538, 10.100], loss: 0.123163, mae: 0.348013, mean_q: 4.167778
 73302/100000: episode: 1722, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 27.976, mean reward: 3.497 [2.626, 5.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.269, 10.100], loss: 0.144909, mae: 0.366569, mean_q: 4.221162
 73312/100000: episode: 1723, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 54.950, mean reward: 5.495 [2.928, 9.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.283, 10.100], loss: 0.204875, mae: 0.369321, mean_q: 4.137415
 73325/100000: episode: 1724, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 42.001, mean reward: 3.231 [2.693, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.492, 10.100], loss: 0.118527, mae: 0.341569, mean_q: 4.136693
 73336/100000: episode: 1725, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 43.731, mean reward: 3.976 [2.741, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.253, 10.100], loss: 0.125626, mae: 0.345905, mean_q: 4.111769
 73346/100000: episode: 1726, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 31.029, mean reward: 3.103 [2.823, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.343, 10.100], loss: 0.269697, mae: 0.413549, mean_q: 4.343036
 73356/100000: episode: 1727, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 25.976, mean reward: 2.598 [2.256, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.687, 10.100], loss: 0.189167, mae: 0.398952, mean_q: 4.190194
 73366/100000: episode: 1728, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 42.500, mean reward: 4.250 [2.817, 5.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.465, 10.100], loss: 0.192454, mae: 0.391927, mean_q: 4.175364
 73374/100000: episode: 1729, duration: 0.091s, episode steps: 8, steps per second: 88, episode reward: 25.200, mean reward: 3.150 [2.730, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.447, 10.100], loss: 0.089071, mae: 0.304309, mean_q: 4.261260
 73385/100000: episode: 1730, duration: 0.108s, episode steps: 11, steps per second: 102, episode reward: 56.223, mean reward: 5.111 [3.219, 15.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.331, 10.100], loss: 0.137468, mae: 0.336755, mean_q: 4.190714
 73397/100000: episode: 1731, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 32.122, mean reward: 2.677 [2.492, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.373, 10.100], loss: 0.594036, mae: 0.463693, mean_q: 4.369512
 73408/100000: episode: 1732, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 30.915, mean reward: 2.810 [2.258, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.307, 10.100], loss: 0.131249, mae: 0.359069, mean_q: 4.099002
 73418/100000: episode: 1733, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 27.740, mean reward: 2.774 [2.312, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.304, 10.100], loss: 0.114945, mae: 0.325328, mean_q: 4.117174
 73428/100000: episode: 1734, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 24.595, mean reward: 2.459 [2.031, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.227, 10.100], loss: 0.122404, mae: 0.350899, mean_q: 4.153928
 73439/100000: episode: 1735, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 46.078, mean reward: 4.189 [3.233, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.540, 10.100], loss: 0.254635, mae: 0.389899, mean_q: 4.233321
 73449/100000: episode: 1736, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 32.895, mean reward: 3.290 [2.485, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.381, 10.100], loss: 0.171468, mae: 0.373026, mean_q: 4.140446
 73459/100000: episode: 1737, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 26.058, mean reward: 2.606 [2.184, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.394, 10.100], loss: 0.605079, mae: 0.465331, mean_q: 4.335061
 73470/100000: episode: 1738, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 43.530, mean reward: 3.957 [2.630, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.405, 10.100], loss: 0.183590, mae: 0.383655, mean_q: 4.226879
 73481/100000: episode: 1739, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 35.463, mean reward: 3.224 [2.738, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.257, 10.100], loss: 0.170850, mae: 0.386798, mean_q: 4.269985
 73491/100000: episode: 1740, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 27.358, mean reward: 2.736 [2.309, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.355, 10.100], loss: 0.141554, mae: 0.365231, mean_q: 4.342690
 73504/100000: episode: 1741, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 66.251, mean reward: 5.096 [3.513, 9.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.435, 10.100], loss: 0.139355, mae: 0.360744, mean_q: 4.196595
 73514/100000: episode: 1742, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 28.289, mean reward: 2.829 [2.166, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.241, 10.100], loss: 0.139897, mae: 0.348938, mean_q: 4.338249
 73524/100000: episode: 1743, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 39.238, mean reward: 3.924 [3.351, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.795, 10.100], loss: 0.125795, mae: 0.351614, mean_q: 4.131648
 73537/100000: episode: 1744, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 65.565, mean reward: 5.043 [3.615, 8.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.494, 10.100], loss: 0.188385, mae: 0.390022, mean_q: 4.267024
 73547/100000: episode: 1745, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 30.993, mean reward: 3.099 [2.689, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.295, 10.100], loss: 0.155224, mae: 0.359669, mean_q: 4.211219
 73555/100000: episode: 1746, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 40.432, mean reward: 5.054 [2.904, 7.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.461, 10.100], loss: 0.149274, mae: 0.369547, mean_q: 4.274062
 73565/100000: episode: 1747, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 31.254, mean reward: 3.125 [2.527, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.338, 10.100], loss: 0.218916, mae: 0.406288, mean_q: 4.297462
 73576/100000: episode: 1748, duration: 0.069s, episode steps: 11, steps per second: 158, episode reward: 45.263, mean reward: 4.115 [3.041, 6.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.855, 10.100], loss: 0.337305, mae: 0.453279, mean_q: 4.359633
 73585/100000: episode: 1749, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 23.514, mean reward: 2.613 [2.131, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.308, 10.100], loss: 0.146961, mae: 0.371821, mean_q: 4.183098
 73594/100000: episode: 1750, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 24.919, mean reward: 2.769 [2.371, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.336, 10.100], loss: 0.276486, mae: 0.453733, mean_q: 4.416558
 73604/100000: episode: 1751, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 30.237, mean reward: 3.024 [2.692, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.380, 10.100], loss: 0.133718, mae: 0.360086, mean_q: 4.233102
 73615/100000: episode: 1752, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 31.398, mean reward: 2.854 [2.147, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.260, 10.100], loss: 0.124391, mae: 0.350127, mean_q: 4.223567
 73624/100000: episode: 1753, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 32.826, mean reward: 3.647 [2.596, 5.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.459, 10.100], loss: 0.182467, mae: 0.395311, mean_q: 4.293837
 73633/100000: episode: 1754, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 24.801, mean reward: 2.756 [2.314, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.307, 10.100], loss: 0.176859, mae: 0.376133, mean_q: 4.221288
 73643/100000: episode: 1755, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 41.348, mean reward: 4.135 [2.869, 7.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.574, 10.100], loss: 0.210281, mae: 0.435928, mean_q: 4.358597
 73654/100000: episode: 1756, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 39.062, mean reward: 3.551 [3.088, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.393, 10.100], loss: 0.184344, mae: 0.375304, mean_q: 4.319443
 73665/100000: episode: 1757, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 49.752, mean reward: 4.523 [3.216, 7.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.431, 10.100], loss: 0.176622, mae: 0.371220, mean_q: 4.314715
 73675/100000: episode: 1758, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 35.089, mean reward: 3.509 [2.808, 4.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.652, 10.100], loss: 0.187100, mae: 0.386984, mean_q: 4.203961
 73687/100000: episode: 1759, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 52.081, mean reward: 4.340 [2.937, 5.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.297, 10.100], loss: 0.115352, mae: 0.358598, mean_q: 4.309547
 73697/100000: episode: 1760, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 28.594, mean reward: 2.859 [2.569, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.277, 10.100], loss: 0.110507, mae: 0.335402, mean_q: 4.291806
 73710/100000: episode: 1761, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 40.396, mean reward: 3.107 [2.236, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.152, 10.100], loss: 0.184956, mae: 0.375951, mean_q: 4.306685
 73720/100000: episode: 1762, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 40.353, mean reward: 4.035 [3.034, 5.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.259, 10.100], loss: 0.435301, mae: 0.478862, mean_q: 4.453251
 73730/100000: episode: 1763, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 30.568, mean reward: 3.057 [2.594, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.283, 10.100], loss: 0.422785, mae: 0.447469, mean_q: 4.389435
 73740/100000: episode: 1764, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 24.478, mean reward: 2.448 [2.187, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.367, 10.100], loss: 0.179891, mae: 0.410875, mean_q: 4.392899
 73750/100000: episode: 1765, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 27.046, mean reward: 2.705 [2.330, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.411, 10.100], loss: 0.171847, mae: 0.403479, mean_q: 4.365156
 73761/100000: episode: 1766, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 36.771, mean reward: 3.343 [2.311, 5.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.638, 10.100], loss: 0.180102, mae: 0.400554, mean_q: 4.343716
 73771/100000: episode: 1767, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 33.379, mean reward: 3.338 [2.734, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.376, 10.100], loss: 0.159238, mae: 0.403584, mean_q: 4.341438
 73780/100000: episode: 1768, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 26.654, mean reward: 2.962 [2.321, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.213, 10.100], loss: 0.184913, mae: 0.380876, mean_q: 4.340055
 73790/100000: episode: 1769, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 29.411, mean reward: 2.941 [2.560, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.922, 10.100], loss: 0.251534, mae: 0.398544, mean_q: 4.341421
 73800/100000: episode: 1770, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 32.287, mean reward: 3.229 [2.537, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.383, 10.100], loss: 0.164250, mae: 0.403249, mean_q: 4.287945
 73810/100000: episode: 1771, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 29.533, mean reward: 2.953 [2.399, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.378, 10.100], loss: 0.291927, mae: 0.465884, mean_q: 4.487664
 73820/100000: episode: 1772, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 27.702, mean reward: 2.770 [2.350, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.338, 10.100], loss: 0.174369, mae: 0.403428, mean_q: 4.466342
 73830/100000: episode: 1773, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 35.608, mean reward: 3.561 [3.210, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.835, 10.100], loss: 0.303223, mae: 0.488184, mean_q: 4.499293
 73843/100000: episode: 1774, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 47.590, mean reward: 3.661 [2.917, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.902, 10.100], loss: 0.486340, mae: 0.499024, mean_q: 4.499000
 73852/100000: episode: 1775, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 31.619, mean reward: 3.513 [3.141, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.413, 10.100], loss: 0.149449, mae: 0.371771, mean_q: 4.244195
 73863/100000: episode: 1776, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 33.532, mean reward: 3.048 [2.527, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.312, 10.100], loss: 0.191179, mae: 0.388841, mean_q: 4.449239
 73873/100000: episode: 1777, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 32.301, mean reward: 3.230 [2.661, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.365, 10.100], loss: 0.233796, mae: 0.430838, mean_q: 4.426435
 73881/100000: episode: 1778, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 33.428, mean reward: 4.178 [3.518, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.387, 10.100], loss: 0.174097, mae: 0.406140, mean_q: 4.342202
 73892/100000: episode: 1779, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 26.647, mean reward: 2.422 [2.173, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.259, 10.100], loss: 0.222185, mae: 0.464263, mean_q: 4.570875
 73903/100000: episode: 1780, duration: 0.080s, episode steps: 11, steps per second: 138, episode reward: 40.300, mean reward: 3.664 [2.885, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.376, 10.100], loss: 0.216622, mae: 0.452406, mean_q: 4.482315
 73915/100000: episode: 1781, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 49.702, mean reward: 4.142 [2.754, 6.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.403, 10.100], loss: 0.188918, mae: 0.399261, mean_q: 4.307817
 73925/100000: episode: 1782, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 44.432, mean reward: 4.443 [2.472, 10.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.017, 10.100], loss: 0.247351, mae: 0.457838, mean_q: 4.406172
 73935/100000: episode: 1783, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 25.406, mean reward: 2.541 [2.297, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.296, 10.100], loss: 0.910241, mae: 0.533568, mean_q: 4.626298
 73945/100000: episode: 1784, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 36.976, mean reward: 3.698 [2.813, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.420, 10.100], loss: 0.584423, mae: 0.506173, mean_q: 4.430770
 73956/100000: episode: 1785, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 39.367, mean reward: 3.579 [2.535, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.299, 10.100], loss: 0.214774, mae: 0.456886, mean_q: 4.507826
 73969/100000: episode: 1786, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 35.729, mean reward: 2.748 [2.386, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.186, 10.100], loss: 0.183739, mae: 0.421882, mean_q: 4.399887
 73981/100000: episode: 1787, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 45.547, mean reward: 3.796 [3.036, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.388, 10.100], loss: 0.221858, mae: 0.418442, mean_q: 4.304893
 73989/100000: episode: 1788, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 22.035, mean reward: 2.754 [2.362, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.329, 10.100], loss: 0.420836, mae: 0.467662, mean_q: 4.618052
 73999/100000: episode: 1789, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 24.844, mean reward: 2.484 [2.163, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.369, 10.100], loss: 0.574888, mae: 0.535422, mean_q: 4.556978
 74010/100000: episode: 1790, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 46.971, mean reward: 4.270 [3.070, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.455, 10.100], loss: 0.393675, mae: 0.459842, mean_q: 4.492661
 74020/100000: episode: 1791, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 33.685, mean reward: 3.368 [2.229, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.685, 10.100], loss: 0.167771, mae: 0.409955, mean_q: 4.421442
 74030/100000: episode: 1792, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 31.712, mean reward: 3.171 [2.384, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.246, 10.100], loss: 0.309967, mae: 0.454307, mean_q: 4.555850
 74041/100000: episode: 1793, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 38.206, mean reward: 3.473 [2.760, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.424, 10.100], loss: 0.242389, mae: 0.410640, mean_q: 4.366323
 74051/100000: episode: 1794, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 33.527, mean reward: 3.353 [2.650, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.980, 10.100], loss: 0.643678, mae: 0.516054, mean_q: 4.633260
 74060/100000: episode: 1795, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 30.919, mean reward: 3.435 [2.861, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.404, 10.100], loss: 0.266080, mae: 0.473457, mean_q: 4.464743
 74071/100000: episode: 1796, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 35.756, mean reward: 3.251 [2.965, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.414, 10.100], loss: 0.377687, mae: 0.454009, mean_q: 4.609673
 74079/100000: episode: 1797, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 32.882, mean reward: 4.110 [3.393, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.397, 10.100], loss: 0.281379, mae: 0.416520, mean_q: 4.302052
 74090/100000: episode: 1798, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 31.745, mean reward: 2.886 [2.354, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.392, 10.100], loss: 0.255008, mae: 0.507169, mean_q: 4.565434
 74101/100000: episode: 1799, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 37.219, mean reward: 3.384 [2.951, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.272, 10.100], loss: 0.238781, mae: 0.453016, mean_q: 4.683029
 74112/100000: episode: 1800, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 42.262, mean reward: 3.842 [2.908, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.493, 10.100], loss: 0.595062, mae: 0.478135, mean_q: 4.589769
 74123/100000: episode: 1801, duration: 0.112s, episode steps: 11, steps per second: 98, episode reward: 34.311, mean reward: 3.119 [2.717, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.251, 10.100], loss: 0.194999, mae: 0.395916, mean_q: 4.362344
 74133/100000: episode: 1802, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 29.997, mean reward: 3.000 [2.562, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.389, 10.100], loss: 0.197246, mae: 0.439464, mean_q: 4.577220
 74146/100000: episode: 1803, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 43.902, mean reward: 3.377 [2.771, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.461, 10.100], loss: 0.192913, mae: 0.413669, mean_q: 4.357985
 74157/100000: episode: 1804, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 35.738, mean reward: 3.249 [2.430, 5.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.306, 10.100], loss: 0.248490, mae: 0.443508, mean_q: 4.631809
 74165/100000: episode: 1805, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 26.959, mean reward: 3.370 [2.611, 4.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.441, 10.100], loss: 0.165622, mae: 0.397745, mean_q: 4.473742
 74175/100000: episode: 1806, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 31.758, mean reward: 3.176 [2.472, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.349, 10.100], loss: 0.174647, mae: 0.425118, mean_q: 4.544393
 74186/100000: episode: 1807, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 29.019, mean reward: 2.638 [2.347, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.320, 10.100], loss: 0.200506, mae: 0.470709, mean_q: 4.594679
 74195/100000: episode: 1808, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 26.662, mean reward: 2.962 [2.306, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.360, 10.100], loss: 0.142347, mae: 0.372403, mean_q: 4.559743
 74203/100000: episode: 1809, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 31.266, mean reward: 3.908 [2.962, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.406, 10.100], loss: 0.153000, mae: 0.366966, mean_q: 4.593062
[Info] 3-TH LEVEL FOUND: 7.631532192230225, Considering 10/90 traces
 74214/100000: episode: 1810, duration: 4.247s, episode steps: 11, steps per second: 3, episode reward: 50.522, mean reward: 4.593 [3.161, 7.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.334, 10.100], loss: 0.259359, mae: 0.453860, mean_q: 4.489409
 74226/100000: episode: 1811, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 67.603, mean reward: 5.634 [3.364, 7.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.452, 10.100], loss: 0.243484, mae: 0.445109, mean_q: 4.642043
 74235/100000: episode: 1812, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 35.581, mean reward: 3.953 [3.072, 5.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.441, 10.100], loss: 0.169378, mae: 0.397565, mean_q: 4.418971
 74244/100000: episode: 1813, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 32.403, mean reward: 3.600 [2.796, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.510, 10.100], loss: 0.688702, mae: 0.523578, mean_q: 4.654988
 74255/100000: episode: 1814, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 54.065, mean reward: 4.915 [3.339, 9.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.393, 10.100], loss: 0.282090, mae: 0.469994, mean_q: 4.571236
 74265/100000: episode: 1815, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 36.195, mean reward: 3.620 [2.777, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.337, 10.100], loss: 0.217000, mae: 0.466531, mean_q: 4.662035
 74276/100000: episode: 1816, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 45.365, mean reward: 4.124 [3.462, 4.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.553, 10.100], loss: 0.214405, mae: 0.462556, mean_q: 4.538420
 74285/100000: episode: 1817, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 33.176, mean reward: 3.686 [2.514, 5.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.460, 10.100], loss: 0.210059, mae: 0.450849, mean_q: 4.577838
 74294/100000: episode: 1818, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 45.165, mean reward: 5.018 [3.657, 7.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.461, 10.100], loss: 0.648059, mae: 0.509373, mean_q: 4.749889
 74303/100000: episode: 1819, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 34.115, mean reward: 3.791 [3.208, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.433, 10.100], loss: 0.651560, mae: 0.508119, mean_q: 4.493618
 74312/100000: episode: 1820, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 34.748, mean reward: 3.861 [3.375, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.422, 10.100], loss: 0.208914, mae: 0.477440, mean_q: 4.631983
 74321/100000: episode: 1821, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 44.203, mean reward: 4.911 [3.751, 6.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.436, 10.100], loss: 0.402612, mae: 0.503943, mean_q: 4.600749
 74330/100000: episode: 1822, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 29.516, mean reward: 3.280 [2.763, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.344, 10.100], loss: 0.251285, mae: 0.483618, mean_q: 4.726396
 74339/100000: episode: 1823, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 40.228, mean reward: 4.470 [3.596, 5.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.091, 10.100], loss: 0.119620, mae: 0.331134, mean_q: 4.444250
 74349/100000: episode: 1824, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 38.242, mean reward: 3.824 [2.978, 5.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.367, 10.100], loss: 0.264643, mae: 0.432030, mean_q: 4.663734
 74359/100000: episode: 1825, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 43.576, mean reward: 4.358 [3.244, 6.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.377, 10.100], loss: 0.223564, mae: 0.436559, mean_q: 4.565808
 74368/100000: episode: 1826, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 33.518, mean reward: 3.724 [2.872, 6.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.555, 10.100], loss: 0.227228, mae: 0.464235, mean_q: 4.719290
 74377/100000: episode: 1827, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 38.384, mean reward: 4.265 [3.819, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.465, 10.100], loss: 0.195924, mae: 0.445747, mean_q: 4.654257
 74387/100000: episode: 1828, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 38.096, mean reward: 3.810 [3.094, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.373, 10.100], loss: 0.355793, mae: 0.528990, mean_q: 4.841554
 74399/100000: episode: 1829, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 60.417, mean reward: 5.035 [3.351, 6.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.398, 10.100], loss: 0.464879, mae: 0.528437, mean_q: 4.730415
 74408/100000: episode: 1830, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 30.440, mean reward: 3.382 [2.334, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.330, 10.100], loss: 0.257874, mae: 0.438374, mean_q: 4.749237
 74417/100000: episode: 1831, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 38.347, mean reward: 4.261 [3.180, 5.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.249, 10.100], loss: 0.509222, mae: 0.512516, mean_q: 4.828889
 74429/100000: episode: 1832, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 58.038, mean reward: 4.836 [4.212, 6.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.914, 10.100], loss: 0.199166, mae: 0.422526, mean_q: 4.542710
 74438/100000: episode: 1833, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 35.120, mean reward: 3.902 [3.376, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.258, 10.100], loss: 0.214798, mae: 0.471897, mean_q: 4.696628
 74448/100000: episode: 1834, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 44.953, mean reward: 4.495 [3.050, 6.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.377, 10.100], loss: 0.201112, mae: 0.409715, mean_q: 4.651717
 74457/100000: episode: 1835, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 51.369, mean reward: 5.708 [3.768, 9.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.510, 10.100], loss: 0.191213, mae: 0.405415, mean_q: 4.666831
 74469/100000: episode: 1836, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 159.447, mean reward: 13.287 [4.476, 57.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.676, 10.100], loss: 0.363821, mae: 0.523789, mean_q: 4.719755
 74478/100000: episode: 1837, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 32.655, mean reward: 3.628 [2.535, 6.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.368, 10.100], loss: 0.297679, mae: 0.504747, mean_q: 4.931288
 74487/100000: episode: 1838, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 33.119, mean reward: 3.680 [3.373, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.139, 10.100], loss: 0.360640, mae: 0.535730, mean_q: 4.841699
 74497/100000: episode: 1839, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 42.239, mean reward: 4.224 [3.199, 5.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.444, 10.100], loss: 0.245240, mae: 0.440864, mean_q: 4.714658
 74506/100000: episode: 1840, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 47.135, mean reward: 5.237 [3.856, 7.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.486, 10.100], loss: 0.443412, mae: 0.520104, mean_q: 4.929484
 74515/100000: episode: 1841, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 36.652, mean reward: 4.072 [3.207, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.346, 10.100], loss: 0.458990, mae: 0.569036, mean_q: 4.832402
 74524/100000: episode: 1842, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 32.444, mean reward: 3.605 [3.112, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.479, 10.100], loss: 0.408632, mae: 0.567438, mean_q: 5.018035
 74536/100000: episode: 1843, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 90.830, mean reward: 7.569 [4.664, 17.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.727, 10.100], loss: 0.264861, mae: 0.467572, mean_q: 4.787522
 74545/100000: episode: 1844, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 52.521, mean reward: 5.836 [3.201, 8.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.473, 10.100], loss: 0.393211, mae: 0.553201, mean_q: 4.943314
 74554/100000: episode: 1845, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 52.015, mean reward: 5.779 [3.557, 12.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.477, 10.100], loss: 0.194392, mae: 0.407547, mean_q: 4.806828
 74563/100000: episode: 1846, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 29.604, mean reward: 3.289 [2.999, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.461, 10.100], loss: 0.431173, mae: 0.548050, mean_q: 5.105547
 74572/100000: episode: 1847, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 42.369, mean reward: 4.708 [4.088, 5.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.075, 10.100], loss: 0.289220, mae: 0.497123, mean_q: 4.887402
 74581/100000: episode: 1848, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 30.616, mean reward: 3.402 [2.721, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.205, 10.100], loss: 0.334674, mae: 0.585416, mean_q: 5.092989
 74591/100000: episode: 1849, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 35.789, mean reward: 3.579 [2.874, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.430, 10.100], loss: 0.312603, mae: 0.513800, mean_q: 4.803648
 74600/100000: episode: 1850, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 48.303, mean reward: 5.367 [4.016, 6.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.519, 10.100], loss: 0.357024, mae: 0.518950, mean_q: 4.853590
 74609/100000: episode: 1851, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 71.448, mean reward: 7.939 [4.943, 16.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.390, 10.100], loss: 0.267968, mae: 0.489136, mean_q: 4.802690
 74618/100000: episode: 1852, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 41.460, mean reward: 4.607 [3.262, 7.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.396, 10.100], loss: 0.352482, mae: 0.509846, mean_q: 4.995606
 74627/100000: episode: 1853, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 33.009, mean reward: 3.668 [3.161, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.337, 10.100], loss: 0.814751, mae: 0.525008, mean_q: 4.763635
 74636/100000: episode: 1854, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 44.535, mean reward: 4.948 [3.530, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.568, 10.100], loss: 1.327553, mae: 0.672297, mean_q: 4.961955
 74645/100000: episode: 1855, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 32.998, mean reward: 3.666 [2.688, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.343, 10.100], loss: 0.462471, mae: 0.640756, mean_q: 5.316179
 74654/100000: episode: 1856, duration: 0.071s, episode steps: 9, steps per second: 127, episode reward: 35.198, mean reward: 3.911 [3.047, 5.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.396, 10.100], loss: 0.506992, mae: 0.644772, mean_q: 4.985592
 74663/100000: episode: 1857, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 33.533, mean reward: 3.726 [3.346, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.432, 10.100], loss: 0.365751, mae: 0.564095, mean_q: 4.985499
 74672/100000: episode: 1858, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 40.676, mean reward: 4.520 [4.055, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.482, 10.100], loss: 0.375861, mae: 0.508996, mean_q: 4.959944
 74681/100000: episode: 1859, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 39.800, mean reward: 4.422 [3.484, 6.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.372, 10.100], loss: 0.463623, mae: 0.636175, mean_q: 5.212429
 74690/100000: episode: 1860, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 37.091, mean reward: 4.121 [2.920, 4.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.377, 10.100], loss: 0.319869, mae: 0.502533, mean_q: 4.768771
 74699/100000: episode: 1861, duration: 0.073s, episode steps: 9, steps per second: 124, episode reward: 30.644, mean reward: 3.405 [2.878, 4.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.408, 10.100], loss: 0.666702, mae: 0.690022, mean_q: 5.339014
 74708/100000: episode: 1862, duration: 0.086s, episode steps: 9, steps per second: 104, episode reward: 29.978, mean reward: 3.331 [2.780, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.670, 10.100], loss: 0.961772, mae: 0.693394, mean_q: 4.925534
 74717/100000: episode: 1863, duration: 0.104s, episode steps: 9, steps per second: 87, episode reward: 27.943, mean reward: 3.105 [2.646, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.318, 10.100], loss: 0.549004, mae: 0.675726, mean_q: 5.099105
 74726/100000: episode: 1864, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 53.075, mean reward: 5.897 [3.369, 7.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.330, 10.100], loss: 0.389577, mae: 0.554463, mean_q: 5.353834
 74735/100000: episode: 1865, duration: 0.098s, episode steps: 9, steps per second: 91, episode reward: 29.787, mean reward: 3.310 [2.539, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.442, 10.100], loss: 0.421718, mae: 0.555579, mean_q: 4.945886
 74744/100000: episode: 1866, duration: 0.126s, episode steps: 9, steps per second: 71, episode reward: 37.026, mean reward: 4.114 [2.996, 5.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.436, 10.100], loss: 0.424810, mae: 0.573965, mean_q: 5.194537
 74753/100000: episode: 1867, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 29.329, mean reward: 3.259 [2.888, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.407, 10.100], loss: 0.383132, mae: 0.528529, mean_q: 5.050194
 74762/100000: episode: 1868, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 29.463, mean reward: 3.274 [2.354, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.320, 10.100], loss: 0.315760, mae: 0.516038, mean_q: 4.937533
 74772/100000: episode: 1869, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 44.821, mean reward: 4.482 [3.400, 7.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.388, 10.100], loss: 0.568370, mae: 0.623849, mean_q: 5.104860
 74784/100000: episode: 1870, duration: 0.119s, episode steps: 12, steps per second: 101, episode reward: 44.137, mean reward: 3.678 [2.492, 6.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.409, 10.100], loss: 0.340340, mae: 0.535526, mean_q: 4.950100
 74796/100000: episode: 1871, duration: 0.125s, episode steps: 12, steps per second: 96, episode reward: 50.931, mean reward: 4.244 [3.452, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.453, 10.100], loss: 0.738028, mae: 0.603642, mean_q: 4.997981
 74805/100000: episode: 1872, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 38.764, mean reward: 4.307 [3.369, 5.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.300, 10.100], loss: 0.385346, mae: 0.573918, mean_q: 5.036542
 74817/100000: episode: 1873, duration: 0.097s, episode steps: 12, steps per second: 123, episode reward: 57.654, mean reward: 4.804 [3.666, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.533, 10.100], loss: 0.302824, mae: 0.531409, mean_q: 5.111269
 74829/100000: episode: 1874, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 34.922, mean reward: 2.910 [2.568, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.561, 10.100], loss: 0.442550, mae: 0.585324, mean_q: 5.064599
 74838/100000: episode: 1875, duration: 0.063s, episode steps: 9, steps per second: 144, episode reward: 121.832, mean reward: 13.537 [4.696, 43.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.479, 10.100], loss: 0.442390, mae: 0.571111, mean_q: 5.019820
 74847/100000: episode: 1876, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 29.834, mean reward: 3.315 [2.931, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.467, 10.100], loss: 0.274715, mae: 0.470486, mean_q: 5.001062
 74856/100000: episode: 1877, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 41.127, mean reward: 4.570 [4.052, 5.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.349, 10.100], loss: 5.293455, mae: 1.038200, mean_q: 5.642502
 74865/100000: episode: 1878, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 36.091, mean reward: 4.010 [3.410, 5.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.687, 10.100], loss: 0.791163, mae: 0.765256, mean_q: 4.873584
 74874/100000: episode: 1879, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 34.695, mean reward: 3.855 [2.877, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.517, 10.100], loss: 0.846965, mae: 0.791658, mean_q: 5.507351
 74883/100000: episode: 1880, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 69.493, mean reward: 7.721 [3.456, 13.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.403, 10.100], loss: 0.866128, mae: 0.744931, mean_q: 4.776583
 74893/100000: episode: 1881, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 36.795, mean reward: 3.679 [2.802, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.365, 10.100], loss: 4.382676, mae: 0.944045, mean_q: 5.716552
 74902/100000: episode: 1882, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 36.008, mean reward: 4.001 [3.181, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.493, 10.100], loss: 0.799523, mae: 0.695477, mean_q: 5.128965
 74912/100000: episode: 1883, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 51.466, mean reward: 5.147 [2.986, 13.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.571, 10.100], loss: 0.526021, mae: 0.618042, mean_q: 5.378458
 74921/100000: episode: 1884, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 46.878, mean reward: 5.209 [3.095, 6.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.357, 10.100], loss: 0.505815, mae: 0.641345, mean_q: 5.140059
 74930/100000: episode: 1885, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 54.505, mean reward: 6.056 [4.296, 9.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.561, 10.100], loss: 0.568187, mae: 0.662655, mean_q: 5.050898
 74942/100000: episode: 1886, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 52.316, mean reward: 4.360 [3.526, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.422, 10.100], loss: 0.552671, mae: 0.654367, mean_q: 5.314182
 74951/100000: episode: 1887, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 34.619, mean reward: 3.847 [2.990, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.493, 10.100], loss: 0.565048, mae: 0.603504, mean_q: 5.169386
 74960/100000: episode: 1888, duration: 0.092s, episode steps: 9, steps per second: 98, episode reward: 179.791, mean reward: 19.977 [3.505, 111.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.862, 10.100], loss: 0.608257, mae: 0.652940, mean_q: 5.486874
 74972/100000: episode: 1889, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 53.810, mean reward: 4.484 [3.490, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.116, 10.100], loss: 2.354526, mae: 0.807975, mean_q: 5.548553
 74981/100000: episode: 1890, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 41.077, mean reward: 4.564 [3.554, 6.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.433, 10.100], loss: 0.625624, mae: 0.607284, mean_q: 5.274195
 74990/100000: episode: 1891, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 53.825, mean reward: 5.981 [3.171, 7.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.443, 10.100], loss: 0.299905, mae: 0.535600, mean_q: 5.245806
 74999/100000: episode: 1892, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 38.904, mean reward: 4.323 [3.466, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.457, 10.100], loss: 0.422115, mae: 0.593152, mean_q: 5.257636
 75008/100000: episode: 1893, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 28.597, mean reward: 3.177 [2.764, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.339, 10.100], loss: 0.451373, mae: 0.601815, mean_q: 5.403115
 75020/100000: episode: 1894, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 61.231, mean reward: 5.103 [3.564, 7.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.492, 10.100], loss: 0.667449, mae: 0.628835, mean_q: 5.334914
 75029/100000: episode: 1895, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 31.447, mean reward: 3.494 [3.052, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.484, 10.100], loss: 0.604914, mae: 0.653445, mean_q: 5.088839
 75038/100000: episode: 1896, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 34.117, mean reward: 3.791 [2.950, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.280, 10.100], loss: 0.493390, mae: 0.642740, mean_q: 5.443372
 75047/100000: episode: 1897, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 44.071, mean reward: 4.897 [2.920, 8.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.361, 10.100], loss: 0.457195, mae: 0.602518, mean_q: 5.165802
 75056/100000: episode: 1898, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 37.567, mean reward: 4.174 [3.455, 5.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.513, 10.100], loss: 0.694921, mae: 0.661926, mean_q: 5.608481
 75067/100000: episode: 1899, duration: 0.096s, episode steps: 11, steps per second: 115, episode reward: 42.656, mean reward: 3.878 [2.776, 7.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.411, 10.100], loss: 0.696156, mae: 0.646667, mean_q: 5.241899
[Info] 4-TH LEVEL FOUND: 8.998210906982422, Considering 10/90 traces
 75076/100000: episode: 1900, duration: 5.159s, episode steps: 9, steps per second: 2, episode reward: 32.927, mean reward: 3.659 [2.907, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.454, 10.100], loss: 0.796685, mae: 0.700438, mean_q: 5.638433
 75086/100000: episode: 1901, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 42.447, mean reward: 4.245 [3.350, 4.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.441, 10.100], loss: 1.084682, mae: 0.736268, mean_q: 5.622411
 75092/100000: episode: 1902, duration: 0.043s, episode steps: 6, steps per second: 141, episode reward: 24.633, mean reward: 4.105 [3.673, 5.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.535, 10.100], loss: 0.592287, mae: 0.832831, mean_q: 5.972022
 75098/100000: episode: 1903, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 34.205, mean reward: 5.701 [4.414, 8.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.511, 10.100], loss: 0.710350, mae: 0.799587, mean_q: 4.699459
 75107/100000: episode: 1904, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 62.846, mean reward: 6.983 [3.931, 14.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.510, 10.100], loss: 0.589200, mae: 0.739670, mean_q: 5.473425
 75117/100000: episode: 1905, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 41.893, mean reward: 4.189 [3.294, 5.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.532, 10.100], loss: 1.103717, mae: 0.777370, mean_q: 5.345494
 75127/100000: episode: 1906, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 49.122, mean reward: 4.912 [3.973, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.456, 10.100], loss: 0.501402, mae: 0.642756, mean_q: 5.569980
 75136/100000: episode: 1907, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 76.358, mean reward: 8.484 [6.995, 11.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.449, 10.100], loss: 2.855287, mae: 0.813571, mean_q: 5.437420
 75146/100000: episode: 1908, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 132.555, mean reward: 13.255 [4.678, 49.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.605, 10.100], loss: 1.189822, mae: 0.709528, mean_q: 5.591928
[Info] FALSIFICATION!
 75152/100000: episode: 1909, duration: 0.252s, episode steps: 6, steps per second: 24, episode reward: 1030.170, mean reward: 171.695 [3.333, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.450, 10.093], loss: 0.433151, mae: 0.622388, mean_q: 5.134642
[Info] FALSIFICATION!
 75160/100000: episode: 1910, duration: 0.346s, episode steps: 8, steps per second: 23, episode reward: 1151.954, mean reward: 143.994 [3.332, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.533, 10.100], loss: 0.415081, mae: 0.564069, mean_q: 5.359958
 75170/100000: episode: 1911, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 48.812, mean reward: 4.881 [3.815, 5.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.456, 10.100], loss: 0.638888, mae: 0.637677, mean_q: 5.526172
 75180/100000: episode: 1912, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 50.100, mean reward: 5.010 [3.448, 11.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.646, 10.100], loss: 8.283817, mae: 0.853442, mean_q: 5.582062
 75187/100000: episode: 1913, duration: 0.061s, episode steps: 7, steps per second: 115, episode reward: 42.600, mean reward: 6.086 [4.429, 8.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.974, 10.100], loss: 3.818403, mae: 1.280902, mean_q: 6.464890
 75193/100000: episode: 1914, duration: 0.072s, episode steps: 6, steps per second: 83, episode reward: 25.521, mean reward: 4.253 [3.303, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.604, 10.100], loss: 0.879469, mae: 0.782874, mean_q: 5.248139
 75199/100000: episode: 1915, duration: 0.063s, episode steps: 6, steps per second: 95, episode reward: 34.358, mean reward: 5.726 [4.947, 7.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.573, 10.100], loss: 0.596973, mae: 0.705139, mean_q: 5.840144
 75209/100000: episode: 1916, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 31.120, mean reward: 3.112 [2.812, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.330, 10.100], loss: 0.812438, mae: 0.781493, mean_q: 5.561673
 75218/100000: episode: 1917, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 51.258, mean reward: 5.695 [4.977, 6.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.440, 10.100], loss: 0.745159, mae: 0.825031, mean_q: 5.805315
 75226/100000: episode: 1918, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 30.453, mean reward: 3.807 [2.942, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.723, 10.100], loss: 3.250725, mae: 0.851375, mean_q: 5.583923
 75236/100000: episode: 1919, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 51.560, mean reward: 5.156 [4.250, 6.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.592, 10.100], loss: 1540.261841, mae: 4.359914, mean_q: 6.237782
 75246/100000: episode: 1920, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 258.338, mean reward: 25.834 [6.854, 62.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.532, 10.100], loss: 1538.211670, mae: 9.015422, mean_q: 11.646746
 75252/100000: episode: 1921, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 28.131, mean reward: 4.688 [4.212, 5.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.457, 10.100], loss: 17.309052, mae: 3.283417, mean_q: 8.261180
 75259/100000: episode: 1922, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 172.358, mean reward: 24.623 [6.474, 91.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.537, 10.100], loss: 2.438253, mae: 1.859169, mean_q: 4.580627
 75265/100000: episode: 1923, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 39.140, mean reward: 6.523 [5.327, 8.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.477, 10.100], loss: 3.054531, mae: 2.121463, mean_q: 3.932534
 75273/100000: episode: 1924, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 33.106, mean reward: 4.138 [3.542, 4.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.486, 10.100], loss: 11.463099, mae: 1.923006, mean_q: 5.296824
 75283/100000: episode: 1925, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 38.424, mean reward: 3.842 [3.018, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.375, 10.100], loss: 1.979494, mae: 1.266073, mean_q: 6.483681
 75290/100000: episode: 1926, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 27.624, mean reward: 3.946 [3.300, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.718, 10.100], loss: 1.585663, mae: 1.169294, mean_q: 6.737714
 75299/100000: episode: 1927, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 46.884, mean reward: 5.209 [3.467, 6.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.436, 10.100], loss: 22.807873, mae: 1.329724, mean_q: 5.772918
 75306/100000: episode: 1928, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 28.717, mean reward: 4.102 [3.673, 5.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.420, 10.100], loss: 1.669568, mae: 1.079760, mean_q: 5.768253
[Info] FALSIFICATION!
 75311/100000: episode: 1929, duration: 0.340s, episode steps: 5, steps per second: 15, episode reward: 1064.189, mean reward: 212.838 [6.283, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.345, 10.067], loss: 3.346807, mae: 1.748280, mean_q: 7.201675
 75321/100000: episode: 1930, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 50.279, mean reward: 5.028 [4.037, 8.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.562, 10.100], loss: 5.824433, mae: 2.603808, mean_q: 8.318526
 75331/100000: episode: 1931, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 72.651, mean reward: 7.265 [4.614, 9.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.633, 10.100], loss: 1.578055, mae: 1.181803, mean_q: 6.019571
[Info] FALSIFICATION!
 75337/100000: episode: 1932, duration: 0.334s, episode steps: 6, steps per second: 18, episode reward: 1067.726, mean reward: 177.954 [6.047, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.578, 10.098], loss: 5.426183, mae: 1.585485, mean_q: 5.202639
 75347/100000: episode: 1933, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 39.056, mean reward: 3.906 [2.883, 5.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.429, 10.100], loss: 1535.201172, mae: 4.626135, mean_q: 6.588190
 75354/100000: episode: 1934, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 42.177, mean reward: 6.025 [4.375, 12.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.010, 10.100], loss: 22.388571, mae: 3.291006, mean_q: 8.977021
 75364/100000: episode: 1935, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 43.078, mean reward: 4.308 [4.077, 4.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.518, 10.100], loss: 9.301768, mae: 1.586564, mean_q: 6.917624
 75374/100000: episode: 1936, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 64.540, mean reward: 6.454 [3.858, 9.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.508, 10.100], loss: 3049.479736, mae: 7.486715, mean_q: 6.252844
 75384/100000: episode: 1937, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 43.252, mean reward: 4.325 [3.620, 6.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.445, 10.100], loss: 19.708187, mae: 4.439182, mean_q: 10.597127
 75394/100000: episode: 1938, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 48.186, mean reward: 4.819 [2.715, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.491, 10.100], loss: 14.407374, mae: 2.529700, mean_q: 8.590140
 75403/100000: episode: 1939, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 48.053, mean reward: 5.339 [3.973, 9.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.627, 10.100], loss: 4.860449, mae: 1.404324, mean_q: 6.117858
 75409/100000: episode: 1940, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 33.179, mean reward: 5.530 [4.467, 7.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.547, 10.100], loss: 3.086107, mae: 1.493100, mean_q: 5.482623
 75418/100000: episode: 1941, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 48.760, mean reward: 5.418 [4.092, 7.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.487, 10.100], loss: 1.550192, mae: 1.245413, mean_q: 5.767432
 75426/100000: episode: 1942, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 37.303, mean reward: 4.663 [3.514, 6.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.659, 10.100], loss: 1.529538, mae: 1.094808, mean_q: 6.404114
 75432/100000: episode: 1943, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 48.915, mean reward: 8.153 [7.110, 11.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.476, 10.100], loss: 5.664591, mae: 1.291755, mean_q: 6.348835
 75441/100000: episode: 1944, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 158.300, mean reward: 17.589 [4.402, 112.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.408, 10.100], loss: 1.280264, mae: 0.913551, mean_q: 6.375864
 75449/100000: episode: 1945, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 128.520, mean reward: 16.065 [3.594, 73.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.442, 10.100], loss: 1911.667480, mae: 4.809173, mean_q: 6.228851
 75456/100000: episode: 1946, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 26.761, mean reward: 3.823 [3.270, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.554, 10.100], loss: 3.763602, mae: 1.922747, mean_q: 7.763967
 75466/100000: episode: 1947, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 75.439, mean reward: 7.544 [5.332, 10.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.474, 10.100], loss: 8.649158, mae: 2.459099, mean_q: 8.237813
 75476/100000: episode: 1948, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 38.944, mean reward: 3.894 [3.263, 5.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.450, 10.100], loss: 2.065877, mae: 1.148075, mean_q: 6.430719
 75482/100000: episode: 1949, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 35.308, mean reward: 5.885 [4.390, 7.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.613, 10.100], loss: 1.344744, mae: 1.022167, mean_q: 5.969948
 75489/100000: episode: 1950, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 36.556, mean reward: 5.222 [4.683, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.653, 10.100], loss: 6.232777, mae: 1.301103, mean_q: 5.980886
 75498/100000: episode: 1951, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 39.639, mean reward: 4.404 [2.292, 11.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.597, 10.100], loss: 2.982557, mae: 1.068012, mean_q: 6.279308
 75507/100000: episode: 1952, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 75.611, mean reward: 8.401 [3.962, 17.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.458, 10.100], loss: 1.259643, mae: 1.002337, mean_q: 6.825826
 75515/100000: episode: 1953, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 33.730, mean reward: 4.216 [3.403, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.416, 10.100], loss: 35.786964, mae: 1.865253, mean_q: 6.822521
 75525/100000: episode: 1954, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 45.041, mean reward: 4.504 [3.066, 6.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.428, 10.100], loss: 13.616091, mae: 1.427361, mean_q: 6.851542
 75531/100000: episode: 1955, duration: 0.053s, episode steps: 6, steps per second: 114, episode reward: 41.630, mean reward: 6.938 [5.115, 9.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.604, 10.100], loss: 3.790712, mae: 1.090161, mean_q: 6.431280
 75537/100000: episode: 1956, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 33.892, mean reward: 5.649 [3.874, 8.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.618, 10.100], loss: 1.486875, mae: 0.963066, mean_q: 6.268687
 75544/100000: episode: 1957, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 47.365, mean reward: 6.766 [4.958, 12.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.853, 10.100], loss: 2183.910889, mae: 5.615093, mean_q: 6.906481
 75553/100000: episode: 1958, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 35.732, mean reward: 3.970 [3.121, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.454, 10.100], loss: 18.597855, mae: 2.613384, mean_q: 8.321840
 75562/100000: episode: 1959, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 59.123, mean reward: 6.569 [5.236, 8.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.419, 10.100], loss: 19.386459, mae: 2.649476, mean_q: 8.495963
 75572/100000: episode: 1960, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 50.992, mean reward: 5.099 [4.203, 5.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.497, 10.100], loss: 27.605398, mae: 1.821862, mean_q: 7.225255
 75582/100000: episode: 1961, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 60.130, mean reward: 6.013 [3.721, 9.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.534, 10.100], loss: 3.830237, mae: 1.303611, mean_q: 6.474554
 75591/100000: episode: 1962, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 47.118, mean reward: 5.235 [2.887, 9.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.556, 10.100], loss: 2.626616, mae: 1.152957, mean_q: 6.465251
 75601/100000: episode: 1963, duration: 0.080s, episode steps: 10, steps per second: 124, episode reward: 36.403, mean reward: 3.640 [3.053, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.449, 10.100], loss: 1523.329346, mae: 4.760886, mean_q: 7.816659
 75610/100000: episode: 1964, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 47.452, mean reward: 5.272 [3.906, 8.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.451, 10.100], loss: 5.724258, mae: 2.468572, mean_q: 8.927553
 75619/100000: episode: 1965, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 46.676, mean reward: 5.186 [3.935, 7.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.463, 10.100], loss: 5.922065, mae: 1.636548, mean_q: 7.669466
 75625/100000: episode: 1966, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 38.019, mean reward: 6.337 [5.256, 8.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.747, 10.100], loss: 1.888619, mae: 1.174473, mean_q: 6.553388
 75631/100000: episode: 1967, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 58.540, mean reward: 9.757 [4.363, 30.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.530, 10.100], loss: 37.352489, mae: 1.838754, mean_q: 6.488880
 75640/100000: episode: 1968, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 35.750, mean reward: 3.972 [3.519, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.350, 10.100], loss: 9.906269, mae: 1.361351, mean_q: 6.852647
 75649/100000: episode: 1969, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 39.033, mean reward: 4.337 [3.483, 5.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.562, 10.100], loss: 7.266416, mae: 1.434808, mean_q: 7.197715
 75659/100000: episode: 1970, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 43.555, mean reward: 4.355 [3.807, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.467, 10.100], loss: 1524.827393, mae: 4.480983, mean_q: 7.250267
 75669/100000: episode: 1971, duration: 0.073s, episode steps: 10, steps per second: 136, episode reward: 62.411, mean reward: 6.241 [3.871, 11.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.525, 10.100], loss: 5.415657, mae: 2.428447, mean_q: 8.918601
 75679/100000: episode: 1972, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 106.670, mean reward: 10.667 [3.689, 19.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.484, 10.100], loss: 2.779692, mae: 1.464791, mean_q: 7.769750
 75689/100000: episode: 1973, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 72.197, mean reward: 7.220 [3.610, 11.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.387, 10.100], loss: 4.623393, mae: 1.385221, mean_q: 6.623630
 75699/100000: episode: 1974, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 56.210, mean reward: 5.621 [3.899, 8.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.404, 10.100], loss: 8.108157, mae: 1.614880, mean_q: 6.373808
 75709/100000: episode: 1975, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 68.407, mean reward: 6.841 [5.058, 9.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.344, 10.100], loss: 3028.947754, mae: 7.667718, mean_q: 7.699046
 75719/100000: episode: 1976, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 131.988, mean reward: 13.199 [4.991, 24.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.532, 10.100], loss: 3060.775146, mae: 13.894865, mean_q: 14.405312
 75729/100000: episode: 1977, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 49.078, mean reward: 4.908 [3.032, 6.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.256, 10.100], loss: 33.460625, mae: 6.096023, mean_q: 12.932680
 75739/100000: episode: 1978, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 39.699, mean reward: 3.970 [3.389, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.408, 10.100], loss: 5.301542, mae: 1.931147, mean_q: 7.018317
 75747/100000: episode: 1979, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 29.091, mean reward: 3.636 [3.275, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.452, 10.100], loss: 13.374181, mae: 2.488797, mean_q: 5.722184
 75756/100000: episode: 1980, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 37.518, mean reward: 4.169 [3.266, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.323, 10.100], loss: 9.578421, mae: 1.933626, mean_q: 6.316531
 75765/100000: episode: 1981, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 47.622, mean reward: 5.291 [3.687, 8.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.553, 10.100], loss: 3.968266, mae: 1.509316, mean_q: 7.227924
 75771/100000: episode: 1982, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 45.697, mean reward: 7.616 [3.979, 19.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.945, 10.100], loss: 4.272645, mae: 1.516757, mean_q: 7.338696
 75781/100000: episode: 1983, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 59.627, mean reward: 5.963 [4.359, 7.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.378, 10.100], loss: 9.310647, mae: 1.596034, mean_q: 7.764585
 75787/100000: episode: 1984, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 33.657, mean reward: 5.610 [4.974, 6.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.651, 10.100], loss: 2538.101318, mae: 6.290237, mean_q: 7.129228
 75793/100000: episode: 1985, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 33.530, mean reward: 5.588 [3.909, 8.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.577, 10.100], loss: 36.047588, mae: 3.063927, mean_q: 9.271171
 75801/100000: episode: 1986, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 65.490, mean reward: 8.186 [4.318, 14.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.613, 10.100], loss: 6.692294, mae: 2.332036, mean_q: 9.017602
 75811/100000: episode: 1987, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 52.509, mean reward: 5.251 [2.781, 9.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.542, 10.100], loss: 4.337594, mae: 1.575739, mean_q: 7.867346
 75820/100000: episode: 1988, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 49.636, mean reward: 5.515 [3.299, 11.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.474, 10.100], loss: 32.079483, mae: 2.184738, mean_q: 6.926773
 75830/100000: episode: 1989, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 42.464, mean reward: 4.246 [2.924, 8.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.500, 10.100], loss: 1521.192261, mae: 4.342463, mean_q: 7.144862
[Info] Complete ISplit Iteration
[Info] Levels: [5.0143533, 6.718811, 7.631532, 8.998211, 18.72085]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.55]
[Info] Error Prob: 5.500000000000002e-05

 75840/100000: episode: 1990, duration: 4.943s, episode steps: 10, steps per second: 2, episode reward: 42.777, mean reward: 4.278 [3.424, 7.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.353, 10.100], loss: 1514.223755, mae: 5.696568, mean_q: 9.672432
 75940/100000: episode: 1991, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.952, mean reward: 1.840 [1.455, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.187, 10.098], loss: 313.426483, mae: 3.263355, mean_q: 8.511513
 76040/100000: episode: 1992, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.318, mean reward: 1.993 [1.455, 6.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.486, 10.098], loss: 157.265915, mae: 1.850900, mean_q: 7.393884
 76140/100000: episode: 1993, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.761, mean reward: 1.978 [1.484, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.417, 10.098], loss: 771.159119, mae: 4.347876, mean_q: 9.214745
 76240/100000: episode: 1994, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.396, mean reward: 1.914 [1.459, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.842, 10.435], loss: 913.489990, mae: 4.586148, mean_q: 9.120094
 76340/100000: episode: 1995, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 189.928, mean reward: 1.899 [1.464, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.162], loss: 311.454987, mae: 2.986359, mean_q: 8.487440
 76440/100000: episode: 1996, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.491, mean reward: 1.995 [1.447, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.859, 10.098], loss: 161.394821, mae: 2.066130, mean_q: 7.543339
 76540/100000: episode: 1997, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 189.801, mean reward: 1.898 [1.459, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.859, 10.098], loss: 7.717226, mae: 1.743853, mean_q: 7.402565
 76640/100000: episode: 1998, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 188.355, mean reward: 1.884 [1.456, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.012, 10.176], loss: 615.622559, mae: 3.451643, mean_q: 8.504495
 76740/100000: episode: 1999, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 209.684, mean reward: 2.097 [1.496, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.127, 10.098], loss: 464.727844, mae: 3.199715, mean_q: 8.162858
 76840/100000: episode: 2000, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.708, mean reward: 2.007 [1.468, 4.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.051, 10.098], loss: 467.693512, mae: 3.223467, mean_q: 8.479230
 76940/100000: episode: 2001, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.291, mean reward: 1.893 [1.506, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.876, 10.218], loss: 462.135864, mae: 3.141845, mean_q: 8.495713
 77040/100000: episode: 2002, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 192.997, mean reward: 1.930 [1.458, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.714, 10.098], loss: 1061.872925, mae: 5.458638, mean_q: 9.795069
 77140/100000: episode: 2003, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 216.350, mean reward: 2.163 [1.487, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.270, 10.098], loss: 163.384247, mae: 2.376304, mean_q: 7.754603
 77240/100000: episode: 2004, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 199.264, mean reward: 1.993 [1.467, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.360, 10.249], loss: 309.762177, mae: 2.590936, mean_q: 7.983162
 77340/100000: episode: 2005, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 198.049, mean reward: 1.980 [1.547, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.746, 10.098], loss: 461.606812, mae: 3.003698, mean_q: 8.065611
 77440/100000: episode: 2006, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.292, mean reward: 1.893 [1.479, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.242, 10.098], loss: 607.568359, mae: 3.641556, mean_q: 9.052569
 77540/100000: episode: 2007, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 184.912, mean reward: 1.849 [1.461, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.098], loss: 159.619019, mae: 2.227197, mean_q: 7.715419
 77640/100000: episode: 2008, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 203.071, mean reward: 2.031 [1.545, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.098], loss: 313.611420, mae: 2.780142, mean_q: 7.986992
 77740/100000: episode: 2009, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 188.316, mean reward: 1.883 [1.447, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.648, 10.339], loss: 160.056580, mae: 1.977826, mean_q: 7.240134
 77840/100000: episode: 2010, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 184.911, mean reward: 1.849 [1.436, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.098], loss: 914.040527, mae: 4.515897, mean_q: 9.203113
 77940/100000: episode: 2011, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.107, mean reward: 1.871 [1.494, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.572, 10.098], loss: 463.470673, mae: 3.400204, mean_q: 8.285633
 78040/100000: episode: 2012, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 193.707, mean reward: 1.937 [1.448, 5.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.098], loss: 160.670670, mae: 2.193711, mean_q: 7.426674
 78140/100000: episode: 2013, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 187.853, mean reward: 1.879 [1.450, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.098], loss: 759.793213, mae: 3.473362, mean_q: 8.047008
 78240/100000: episode: 2014, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 192.384, mean reward: 1.924 [1.477, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.567, 10.230], loss: 618.265686, mae: 3.589721, mean_q: 8.496710
 78340/100000: episode: 2015, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.649, mean reward: 1.866 [1.441, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.972, 10.098], loss: 311.574097, mae: 2.994437, mean_q: 8.247898
 78440/100000: episode: 2016, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.265, mean reward: 1.923 [1.466, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.629, 10.098], loss: 459.286743, mae: 2.983558, mean_q: 8.077975
 78540/100000: episode: 2017, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 203.308, mean reward: 2.033 [1.457, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.879, 10.098], loss: 612.636719, mae: 3.390986, mean_q: 7.849778
 78640/100000: episode: 2018, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 181.086, mean reward: 1.811 [1.452, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.200], loss: 757.685547, mae: 4.397151, mean_q: 8.891425
 78740/100000: episode: 2019, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 192.432, mean reward: 1.924 [1.480, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.364, 10.098], loss: 605.189331, mae: 3.763526, mean_q: 8.756884
 78840/100000: episode: 2020, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.632, mean reward: 1.866 [1.448, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.234, 10.098], loss: 12.176190, mae: 1.607192, mean_q: 7.068981
 78940/100000: episode: 2021, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 181.008, mean reward: 1.810 [1.446, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.098], loss: 609.367615, mae: 3.502532, mean_q: 7.961936
 79040/100000: episode: 2022, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 179.898, mean reward: 1.799 [1.451, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.178, 10.098], loss: 307.194092, mae: 2.333303, mean_q: 7.258073
 79140/100000: episode: 2023, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 194.889, mean reward: 1.949 [1.438, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.050, 10.098], loss: 313.788055, mae: 2.446512, mean_q: 7.142131
 79240/100000: episode: 2024, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 176.926, mean reward: 1.769 [1.445, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.988, 10.235], loss: 308.744995, mae: 2.212444, mean_q: 7.033899
 79340/100000: episode: 2025, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 209.275, mean reward: 2.093 [1.447, 5.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.255], loss: 308.286591, mae: 2.275743, mean_q: 7.077243
 79440/100000: episode: 2026, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 178.957, mean reward: 1.790 [1.483, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.158], loss: 759.794739, mae: 4.007510, mean_q: 7.813764
 79540/100000: episode: 2027, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 196.198, mean reward: 1.962 [1.473, 6.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.021, 10.098], loss: 307.840118, mae: 2.181028, mean_q: 6.432133
 79640/100000: episode: 2028, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 211.352, mean reward: 2.114 [1.480, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.065, 10.194], loss: 306.748566, mae: 2.183596, mean_q: 6.858677
 79740/100000: episode: 2029, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 186.031, mean reward: 1.860 [1.443, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.652, 10.178], loss: 308.086884, mae: 2.091928, mean_q: 6.558904
 79840/100000: episode: 2030, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.191, mean reward: 1.872 [1.447, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.513, 10.421], loss: 455.577393, mae: 2.381249, mean_q: 6.256282
 79940/100000: episode: 2031, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 178.048, mean reward: 1.780 [1.442, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.554, 10.205], loss: 602.152466, mae: 3.059244, mean_q: 7.105999
 80040/100000: episode: 2032, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 213.531, mean reward: 2.135 [1.482, 10.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.413, 10.144], loss: 881.614624, mae: 3.773481, mean_q: 7.297953
 80140/100000: episode: 2033, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 211.580, mean reward: 2.116 [1.481, 5.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.098], loss: 452.367889, mae: 2.782645, mean_q: 6.934607
 80240/100000: episode: 2034, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 182.097, mean reward: 1.821 [1.473, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.098], loss: 8.524807, mae: 1.186383, mean_q: 5.578885
 80340/100000: episode: 2035, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 199.978, mean reward: 2.000 [1.448, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.219, 10.237], loss: 2.158354, mae: 0.766003, mean_q: 4.781402
 80440/100000: episode: 2036, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.928, mean reward: 1.919 [1.467, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.152, 10.311], loss: 1.667082, mae: 0.676970, mean_q: 4.569745
 80540/100000: episode: 2037, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 193.137, mean reward: 1.931 [1.461, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.066, 10.098], loss: 0.966412, mae: 0.527617, mean_q: 4.315307
 80640/100000: episode: 2038, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 176.366, mean reward: 1.764 [1.454, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.373, 10.098], loss: 0.765431, mae: 0.458574, mean_q: 4.151554
 80740/100000: episode: 2039, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.152, mean reward: 1.942 [1.437, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.303, 10.106], loss: 0.329819, mae: 0.361010, mean_q: 3.935445
 80840/100000: episode: 2040, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 186.230, mean reward: 1.862 [1.459, 5.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.480, 10.118], loss: 0.134777, mae: 0.309449, mean_q: 3.841534
 80940/100000: episode: 2041, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 193.613, mean reward: 1.936 [1.465, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.952, 10.258], loss: 0.118933, mae: 0.300600, mean_q: 3.808754
 81040/100000: episode: 2042, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.042, mean reward: 1.960 [1.464, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.847, 10.098], loss: 0.111313, mae: 0.299736, mean_q: 3.814198
 81140/100000: episode: 2043, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 185.691, mean reward: 1.857 [1.473, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.353, 10.269], loss: 0.111393, mae: 0.295644, mean_q: 3.793541
 81240/100000: episode: 2044, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 203.077, mean reward: 2.031 [1.499, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.689, 10.425], loss: 0.100654, mae: 0.299187, mean_q: 3.802070
 81340/100000: episode: 2045, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 181.977, mean reward: 1.820 [1.473, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.508, 10.098], loss: 0.107387, mae: 0.292169, mean_q: 3.817765
 81440/100000: episode: 2046, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.065, mean reward: 1.921 [1.473, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.507, 10.282], loss: 0.099093, mae: 0.297317, mean_q: 3.822204
 81540/100000: episode: 2047, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 207.314, mean reward: 2.073 [1.502, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.332], loss: 0.123453, mae: 0.307633, mean_q: 3.842677
 81640/100000: episode: 2048, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 200.097, mean reward: 2.001 [1.455, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.709, 10.148], loss: 0.115571, mae: 0.302787, mean_q: 3.825500
 81740/100000: episode: 2049, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 185.897, mean reward: 1.859 [1.464, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.201, 10.240], loss: 0.097019, mae: 0.296324, mean_q: 3.830133
 81840/100000: episode: 2050, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 190.459, mean reward: 1.905 [1.447, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.417, 10.098], loss: 0.121790, mae: 0.305183, mean_q: 3.811848
 81940/100000: episode: 2051, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 202.050, mean reward: 2.020 [1.471, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.945, 10.098], loss: 0.110802, mae: 0.300738, mean_q: 3.810186
 82040/100000: episode: 2052, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 183.905, mean reward: 1.839 [1.490, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.609, 10.098], loss: 0.083262, mae: 0.282093, mean_q: 3.807774
 82140/100000: episode: 2053, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 229.743, mean reward: 2.297 [1.486, 5.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.752, 10.098], loss: 0.117783, mae: 0.297775, mean_q: 3.817473
 82240/100000: episode: 2054, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 196.230, mean reward: 1.962 [1.455, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.488, 10.207], loss: 0.093143, mae: 0.292421, mean_q: 3.816988
 82340/100000: episode: 2055, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 200.581, mean reward: 2.006 [1.443, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.715, 10.192], loss: 0.121442, mae: 0.305581, mean_q: 3.823181
 82440/100000: episode: 2056, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 231.365, mean reward: 2.314 [1.466, 9.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.173, 10.237], loss: 0.100872, mae: 0.290249, mean_q: 3.798721
 82540/100000: episode: 2057, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: 188.719, mean reward: 1.887 [1.478, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.959, 10.176], loss: 0.096734, mae: 0.292418, mean_q: 3.811244
 82640/100000: episode: 2058, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 183.832, mean reward: 1.838 [1.437, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.179, 10.151], loss: 0.106371, mae: 0.296979, mean_q: 3.803997
 82740/100000: episode: 2059, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.982, mean reward: 1.900 [1.450, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.724, 10.098], loss: 0.114509, mae: 0.309752, mean_q: 3.824574
 82840/100000: episode: 2060, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.277, mean reward: 1.883 [1.473, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.184, 10.098], loss: 0.150952, mae: 0.321003, mean_q: 3.846455
 82940/100000: episode: 2061, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 204.942, mean reward: 2.049 [1.455, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.115, 10.253], loss: 0.123285, mae: 0.313241, mean_q: 3.828956
 83040/100000: episode: 2062, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 201.953, mean reward: 2.020 [1.472, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.319], loss: 0.125465, mae: 0.305212, mean_q: 3.821409
 83140/100000: episode: 2063, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.920, mean reward: 1.879 [1.449, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.098], loss: 0.121472, mae: 0.315863, mean_q: 3.831012
 83240/100000: episode: 2064, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 196.338, mean reward: 1.963 [1.464, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.857, 10.098], loss: 0.104639, mae: 0.304967, mean_q: 3.831902
 83340/100000: episode: 2065, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 202.928, mean reward: 2.029 [1.460, 5.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.615, 10.275], loss: 0.135849, mae: 0.316668, mean_q: 3.835635
 83440/100000: episode: 2066, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.501, mean reward: 1.895 [1.470, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.573, 10.098], loss: 0.112343, mae: 0.312630, mean_q: 3.843975
 83540/100000: episode: 2067, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 204.127, mean reward: 2.041 [1.473, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.584, 10.098], loss: 0.109783, mae: 0.316778, mean_q: 3.844816
 83640/100000: episode: 2068, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 181.950, mean reward: 1.820 [1.453, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.098], loss: 0.103586, mae: 0.310714, mean_q: 3.841951
 83740/100000: episode: 2069, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.788, mean reward: 1.938 [1.453, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.682, 10.199], loss: 0.122891, mae: 0.317429, mean_q: 3.851784
 83840/100000: episode: 2070, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 192.208, mean reward: 1.922 [1.475, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.409, 10.098], loss: 0.117218, mae: 0.311436, mean_q: 3.850971
 83940/100000: episode: 2071, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 197.131, mean reward: 1.971 [1.472, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.095, 10.098], loss: 0.113493, mae: 0.310235, mean_q: 3.854003
 84040/100000: episode: 2072, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.738, mean reward: 1.907 [1.467, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.241], loss: 0.104001, mae: 0.311188, mean_q: 3.839334
 84140/100000: episode: 2073, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 189.704, mean reward: 1.897 [1.490, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.361, 10.240], loss: 0.125394, mae: 0.328917, mean_q: 3.855023
 84240/100000: episode: 2074, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 216.575, mean reward: 2.166 [1.444, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.340], loss: 0.090840, mae: 0.296921, mean_q: 3.842130
 84340/100000: episode: 2075, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.302, mean reward: 1.903 [1.470, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.252, 10.160], loss: 0.106653, mae: 0.304273, mean_q: 3.853773
 84440/100000: episode: 2076, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.828, mean reward: 2.018 [1.457, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.098], loss: 0.110719, mae: 0.314207, mean_q: 3.849170
 84540/100000: episode: 2077, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 181.367, mean reward: 1.814 [1.458, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.464, 10.098], loss: 0.116734, mae: 0.321403, mean_q: 3.850353
 84640/100000: episode: 2078, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 179.401, mean reward: 1.794 [1.462, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.432, 10.098], loss: 0.133874, mae: 0.319831, mean_q: 3.859998
 84740/100000: episode: 2079, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.452, mean reward: 1.905 [1.462, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.177, 10.133], loss: 0.113611, mae: 0.314790, mean_q: 3.858045
 84840/100000: episode: 2080, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.787, mean reward: 1.918 [1.509, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.334, 10.098], loss: 0.101428, mae: 0.318993, mean_q: 3.851494
 84940/100000: episode: 2081, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.101, mean reward: 1.971 [1.465, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.098], loss: 0.113086, mae: 0.313671, mean_q: 3.847169
 85040/100000: episode: 2082, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 204.415, mean reward: 2.044 [1.448, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.104], loss: 0.093448, mae: 0.308328, mean_q: 3.859399
 85140/100000: episode: 2083, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.088, mean reward: 1.811 [1.478, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.524, 10.098], loss: 0.106771, mae: 0.306244, mean_q: 3.862015
 85240/100000: episode: 2084, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.147, mean reward: 1.831 [1.479, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.098], loss: 0.102996, mae: 0.316156, mean_q: 3.854914
 85340/100000: episode: 2085, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.739, mean reward: 1.907 [1.435, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.943, 10.237], loss: 0.117164, mae: 0.314945, mean_q: 3.852470
 85440/100000: episode: 2086, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 220.921, mean reward: 2.209 [1.503, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.628, 10.098], loss: 0.116423, mae: 0.321746, mean_q: 3.853364
 85540/100000: episode: 2087, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 185.339, mean reward: 1.853 [1.445, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.098], loss: 0.108773, mae: 0.320195, mean_q: 3.854515
 85640/100000: episode: 2088, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.644, mean reward: 1.936 [1.482, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.098], loss: 0.102539, mae: 0.308612, mean_q: 3.856394
 85740/100000: episode: 2089, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.841, mean reward: 1.978 [1.452, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.410, 10.098], loss: 0.096856, mae: 0.309269, mean_q: 3.861959
[Info] 1-TH LEVEL FOUND: 4.430399417877197, Considering 10/90 traces
 85840/100000: episode: 2090, duration: 4.875s, episode steps: 100, steps per second: 21, episode reward: 187.704, mean reward: 1.877 [1.443, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.233], loss: 0.095882, mae: 0.305138, mean_q: 3.864710
 85935/100000: episode: 2091, duration: 0.487s, episode steps: 95, steps per second: 195, episode reward: 175.654, mean reward: 1.849 [1.454, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.067, 10.100], loss: 0.103871, mae: 0.313563, mean_q: 3.854593
 85964/100000: episode: 2092, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 52.302, mean reward: 1.804 [1.504, 2.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.916, 10.100], loss: 0.130117, mae: 0.319004, mean_q: 3.869611
 86063/100000: episode: 2093, duration: 0.512s, episode steps: 99, steps per second: 193, episode reward: 197.634, mean reward: 1.996 [1.458, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.725, 10.100], loss: 0.114072, mae: 0.318122, mean_q: 3.864938
 86067/100000: episode: 2094, duration: 0.027s, episode steps: 4, steps per second: 145, episode reward: 15.238, mean reward: 3.809 [3.297, 4.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.270, 10.100], loss: 0.100600, mae: 0.310099, mean_q: 3.894299
 86070/100000: episode: 2095, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 8.184, mean reward: 2.728 [2.492, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.306, 10.100], loss: 0.071187, mae: 0.294833, mean_q: 3.900683
 86099/100000: episode: 2096, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 60.317, mean reward: 2.080 [1.595, 2.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.100], loss: 0.098750, mae: 0.317624, mean_q: 3.880688
 86128/100000: episode: 2097, duration: 0.179s, episode steps: 29, steps per second: 162, episode reward: 55.091, mean reward: 1.900 [1.489, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.127, 10.100], loss: 0.115335, mae: 0.319221, mean_q: 3.858476
 86223/100000: episode: 2098, duration: 0.520s, episode steps: 95, steps per second: 183, episode reward: 230.524, mean reward: 2.427 [1.454, 4.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.349, 10.100], loss: 0.101020, mae: 0.313435, mean_q: 3.866811
 86234/100000: episode: 2099, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 34.369, mean reward: 3.124 [2.444, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.173, 10.100], loss: 0.127680, mae: 0.336140, mean_q: 3.851774
 86330/100000: episode: 2100, duration: 0.525s, episode steps: 96, steps per second: 183, episode reward: 190.780, mean reward: 1.987 [1.525, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.283, 10.100], loss: 0.103548, mae: 0.316912, mean_q: 3.883529
 86430/100000: episode: 2101, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.766, mean reward: 1.898 [1.468, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.941, 10.100], loss: 0.117550, mae: 0.327712, mean_q: 3.893800
 86441/100000: episode: 2102, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 29.162, mean reward: 2.651 [2.075, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.407, 10.100], loss: 0.098434, mae: 0.324559, mean_q: 3.938896
 86445/100000: episode: 2103, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 14.259, mean reward: 3.565 [3.320, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.404, 10.100], loss: 0.121117, mae: 0.335190, mean_q: 3.849188
 86474/100000: episode: 2104, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 78.321, mean reward: 2.701 [2.005, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.630, 10.100], loss: 0.116995, mae: 0.332735, mean_q: 3.911712
 86501/100000: episode: 2105, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 66.144, mean reward: 2.450 [1.794, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.242, 10.100], loss: 0.103665, mae: 0.320281, mean_q: 3.900735
 86600/100000: episode: 2106, duration: 0.501s, episode steps: 99, steps per second: 198, episode reward: 188.115, mean reward: 1.900 [1.462, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.752, 10.339], loss: 0.103440, mae: 0.314775, mean_q: 3.896492
 86696/100000: episode: 2107, duration: 0.485s, episode steps: 96, steps per second: 198, episode reward: 194.301, mean reward: 2.024 [1.497, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.213, 10.353], loss: 0.122005, mae: 0.329426, mean_q: 3.897912
 86707/100000: episode: 2108, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 33.067, mean reward: 3.006 [2.281, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.370, 10.100], loss: 0.098593, mae: 0.307511, mean_q: 3.844698
 86718/100000: episode: 2109, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 35.173, mean reward: 3.198 [2.292, 5.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.379, 10.100], loss: 0.104844, mae: 0.312934, mean_q: 3.917677
 86729/100000: episode: 2110, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 32.004, mean reward: 2.909 [2.168, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.259, 10.100], loss: 0.135420, mae: 0.354956, mean_q: 3.997894
 86732/100000: episode: 2111, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 8.287, mean reward: 2.762 [2.358, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.354, 10.100], loss: 0.097263, mae: 0.297747, mean_q: 3.906223
 86761/100000: episode: 2112, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 89.098, mean reward: 3.072 [2.033, 6.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.236, 10.100], loss: 0.114089, mae: 0.335830, mean_q: 3.914223
 86772/100000: episode: 2113, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 26.937, mean reward: 2.449 [2.012, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.311, 10.100], loss: 0.170268, mae: 0.336021, mean_q: 3.943153
 86867/100000: episode: 2114, duration: 0.575s, episode steps: 95, steps per second: 165, episode reward: 179.076, mean reward: 1.885 [1.456, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.849, 10.209], loss: 0.140696, mae: 0.341041, mean_q: 3.916026
 86963/100000: episode: 2115, duration: 0.500s, episode steps: 96, steps per second: 192, episode reward: 191.777, mean reward: 1.998 [1.455, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-1.698, 10.279], loss: 0.112827, mae: 0.314398, mean_q: 3.896703
 86974/100000: episode: 2116, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 42.054, mean reward: 3.823 [2.903, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.883, 10.100], loss: 0.125636, mae: 0.336320, mean_q: 3.948189
 87003/100000: episode: 2117, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 55.533, mean reward: 1.915 [1.525, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.718, 10.116], loss: 0.103614, mae: 0.311207, mean_q: 3.911247
 87007/100000: episode: 2118, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 10.204, mean reward: 2.551 [2.176, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.297, 10.100], loss: 0.110983, mae: 0.301093, mean_q: 3.842241
 87018/100000: episode: 2119, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 44.520, mean reward: 4.047 [2.832, 7.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.215, 10.100], loss: 0.152724, mae: 0.374018, mean_q: 3.954642
 87021/100000: episode: 2120, duration: 0.028s, episode steps: 3, steps per second: 106, episode reward: 10.208, mean reward: 3.403 [2.702, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.330, 10.100], loss: 0.104905, mae: 0.356926, mean_q: 3.968172
 87048/100000: episode: 2121, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 67.241, mean reward: 2.490 [1.984, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.446, 10.100], loss: 0.144446, mae: 0.326450, mean_q: 3.911313
 87077/100000: episode: 2122, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 54.392, mean reward: 1.876 [1.499, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.881, 10.137], loss: 0.126785, mae: 0.332890, mean_q: 3.955111
 87088/100000: episode: 2123, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 33.568, mean reward: 3.052 [2.247, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.178, 10.100], loss: 0.156055, mae: 0.362487, mean_q: 3.975467
 87117/100000: episode: 2124, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 59.206, mean reward: 2.042 [1.533, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.609, 10.100], loss: 0.118109, mae: 0.340618, mean_q: 3.953554
 87146/100000: episode: 2125, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 56.978, mean reward: 1.965 [1.727, 2.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.445, 10.100], loss: 0.111884, mae: 0.318445, mean_q: 3.931663
 87175/100000: episode: 2126, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 93.625, mean reward: 3.228 [1.907, 6.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.626, 10.100], loss: 0.102634, mae: 0.306988, mean_q: 3.915629
 87178/100000: episode: 2127, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 7.759, mean reward: 2.586 [2.547, 2.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.327, 10.100], loss: 0.180006, mae: 0.390429, mean_q: 3.958110
 87189/100000: episode: 2128, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 28.873, mean reward: 2.625 [1.882, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.256, 10.100], loss: 0.222844, mae: 0.361705, mean_q: 3.948150
 87285/100000: episode: 2129, duration: 0.506s, episode steps: 96, steps per second: 190, episode reward: 181.506, mean reward: 1.891 [1.444, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.608, 10.100], loss: 0.139987, mae: 0.347457, mean_q: 3.967580
 87289/100000: episode: 2130, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 14.337, mean reward: 3.584 [3.434, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.305, 10.100], loss: 0.119533, mae: 0.325151, mean_q: 3.879326
 87300/100000: episode: 2131, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 30.636, mean reward: 2.785 [2.073, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.650, 10.100], loss: 0.104441, mae: 0.316776, mean_q: 3.964009
 87327/100000: episode: 2132, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 81.825, mean reward: 3.031 [2.198, 5.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.685, 10.100], loss: 0.134841, mae: 0.338555, mean_q: 3.935869
 87331/100000: episode: 2133, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 11.911, mean reward: 2.978 [2.531, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.306, 10.100], loss: 0.139856, mae: 0.343983, mean_q: 3.900835
 87342/100000: episode: 2134, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 33.555, mean reward: 3.050 [2.484, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.374, 10.100], loss: 0.095453, mae: 0.301368, mean_q: 3.963481
 87371/100000: episode: 2135, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 94.576, mean reward: 3.261 [2.194, 5.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.480, 10.100], loss: 0.161386, mae: 0.355105, mean_q: 3.969973
 87466/100000: episode: 2136, duration: 0.475s, episode steps: 95, steps per second: 200, episode reward: 189.875, mean reward: 1.999 [1.469, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.163, 10.100], loss: 0.128527, mae: 0.331259, mean_q: 3.943216
 87470/100000: episode: 2137, duration: 0.035s, episode steps: 4, steps per second: 116, episode reward: 13.123, mean reward: 3.281 [2.975, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.376, 10.100], loss: 0.175078, mae: 0.413466, mean_q: 4.147232
 87566/100000: episode: 2138, duration: 0.486s, episode steps: 96, steps per second: 197, episode reward: 210.899, mean reward: 2.197 [1.455, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.876, 10.137], loss: 0.139652, mae: 0.352279, mean_q: 3.966508
 87662/100000: episode: 2139, duration: 0.512s, episode steps: 96, steps per second: 187, episode reward: 193.144, mean reward: 2.012 [1.442, 5.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.786, 10.276], loss: 0.136518, mae: 0.339540, mean_q: 3.981165
 87689/100000: episode: 2140, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 90.433, mean reward: 3.349 [2.393, 5.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.529, 10.100], loss: 0.147363, mae: 0.350583, mean_q: 3.980397
 87716/100000: episode: 2141, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 69.775, mean reward: 2.584 [1.874, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.274, 10.100], loss: 0.136504, mae: 0.340591, mean_q: 3.969310
 87745/100000: episode: 2142, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 65.130, mean reward: 2.246 [1.807, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.847, 10.100], loss: 0.166167, mae: 0.367181, mean_q: 4.029677
 87774/100000: episode: 2143, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 62.618, mean reward: 2.159 [1.750, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.984, 10.100], loss: 0.158269, mae: 0.365067, mean_q: 4.024593
 87803/100000: episode: 2144, duration: 0.224s, episode steps: 29, steps per second: 130, episode reward: 57.009, mean reward: 1.966 [1.608, 2.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.669, 10.100], loss: 0.124504, mae: 0.336912, mean_q: 3.982615
 87807/100000: episode: 2145, duration: 0.040s, episode steps: 4, steps per second: 100, episode reward: 11.319, mean reward: 2.830 [2.568, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.377, 10.100], loss: 0.164045, mae: 0.369092, mean_q: 4.044421
 87810/100000: episode: 2146, duration: 0.035s, episode steps: 3, steps per second: 85, episode reward: 7.352, mean reward: 2.451 [2.309, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.428, 10.100], loss: 0.085977, mae: 0.309128, mean_q: 3.950782
 87814/100000: episode: 2147, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 15.274, mean reward: 3.818 [3.558, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.300, 10.100], loss: 0.102855, mae: 0.309583, mean_q: 3.934461
 87817/100000: episode: 2148, duration: 0.033s, episode steps: 3, steps per second: 92, episode reward: 7.426, mean reward: 2.475 [2.214, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.681, 10.100], loss: 0.065404, mae: 0.277754, mean_q: 4.023905
 87821/100000: episode: 2149, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 12.239, mean reward: 3.060 [2.638, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.309, 10.100], loss: 0.121333, mae: 0.349112, mean_q: 4.041809
 87832/100000: episode: 2150, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 40.666, mean reward: 3.697 [2.648, 7.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.413, 10.100], loss: 0.138592, mae: 0.344876, mean_q: 3.942156
 87843/100000: episode: 2151, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 44.889, mean reward: 4.081 [2.681, 7.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.531, 10.100], loss: 0.145426, mae: 0.362454, mean_q: 4.043491
 87846/100000: episode: 2152, duration: 0.029s, episode steps: 3, steps per second: 104, episode reward: 9.001, mean reward: 3.000 [2.309, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.231, 10.100], loss: 0.214303, mae: 0.397014, mean_q: 4.065597
 87875/100000: episode: 2153, duration: 0.175s, episode steps: 29, steps per second: 166, episode reward: 77.066, mean reward: 2.657 [1.945, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.800, 10.100], loss: 0.128621, mae: 0.329113, mean_q: 4.058646
 87902/100000: episode: 2154, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 80.507, mean reward: 2.982 [2.162, 5.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.145, 10.100], loss: 0.146557, mae: 0.351077, mean_q: 4.026244
 87929/100000: episode: 2155, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 69.880, mean reward: 2.588 [1.911, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.087, 10.100], loss: 0.129929, mae: 0.339343, mean_q: 4.009030
 87958/100000: episode: 2156, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 67.053, mean reward: 2.312 [1.756, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.712, 10.100], loss: 0.163484, mae: 0.354609, mean_q: 4.031386
 88057/100000: episode: 2157, duration: 0.487s, episode steps: 99, steps per second: 203, episode reward: 182.396, mean reward: 1.842 [1.501, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.791, 10.258], loss: 0.148092, mae: 0.357420, mean_q: 4.065876
 88084/100000: episode: 2158, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 68.567, mean reward: 2.540 [2.099, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.212, 10.100], loss: 0.126806, mae: 0.321002, mean_q: 4.031417
 88113/100000: episode: 2159, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 56.516, mean reward: 1.949 [1.575, 2.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.547, 10.100], loss: 0.125974, mae: 0.336500, mean_q: 4.084106
 88142/100000: episode: 2160, duration: 0.164s, episode steps: 29, steps per second: 176, episode reward: 91.623, mean reward: 3.159 [2.224, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.401, 10.100], loss: 0.170882, mae: 0.372132, mean_q: 4.093818
 88169/100000: episode: 2161, duration: 0.173s, episode steps: 27, steps per second: 156, episode reward: 81.542, mean reward: 3.020 [2.068, 6.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.836, 10.100], loss: 0.186617, mae: 0.388494, mean_q: 4.083936
 88264/100000: episode: 2162, duration: 0.508s, episode steps: 95, steps per second: 187, episode reward: 195.872, mean reward: 2.062 [1.494, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.176, 10.142], loss: 0.141595, mae: 0.351915, mean_q: 4.076063
 88293/100000: episode: 2163, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 84.951, mean reward: 2.929 [1.889, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.428, 10.100], loss: 0.137881, mae: 0.348075, mean_q: 4.097364
 88322/100000: episode: 2164, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 64.402, mean reward: 2.221 [1.508, 4.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.798, 10.100], loss: 0.146344, mae: 0.354461, mean_q: 4.078582
 88351/100000: episode: 2165, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 81.287, mean reward: 2.803 [2.021, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.444, 10.100], loss: 0.181423, mae: 0.378506, mean_q: 4.109596
 88378/100000: episode: 2166, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 73.667, mean reward: 2.728 [1.896, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.715, 10.100], loss: 0.161904, mae: 0.373198, mean_q: 4.137196
 88381/100000: episode: 2167, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 7.295, mean reward: 2.432 [2.382, 2.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.254, 10.100], loss: 0.134886, mae: 0.340457, mean_q: 4.025625
 88385/100000: episode: 2168, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 11.771, mean reward: 2.943 [2.624, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.337, 10.100], loss: 0.135567, mae: 0.369308, mean_q: 4.149873
 88485/100000: episode: 2169, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 196.234, mean reward: 1.962 [1.438, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.869, 10.264], loss: 0.160344, mae: 0.371880, mean_q: 4.149643
 88581/100000: episode: 2170, duration: 0.480s, episode steps: 96, steps per second: 200, episode reward: 182.270, mean reward: 1.899 [1.480, 5.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.488, 10.202], loss: 0.167663, mae: 0.375567, mean_q: 4.140014
 88610/100000: episode: 2171, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 52.266, mean reward: 1.802 [1.493, 2.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.100], loss: 0.180292, mae: 0.373102, mean_q: 4.108927
 88613/100000: episode: 2172, duration: 0.026s, episode steps: 3, steps per second: 115, episode reward: 7.934, mean reward: 2.645 [2.539, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.242, 10.100], loss: 0.197137, mae: 0.402136, mean_q: 4.195742
 88709/100000: episode: 2173, duration: 0.525s, episode steps: 96, steps per second: 183, episode reward: 209.911, mean reward: 2.187 [1.530, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.646, 10.261], loss: 0.155198, mae: 0.362366, mean_q: 4.122155
 88736/100000: episode: 2174, duration: 0.162s, episode steps: 27, steps per second: 166, episode reward: 98.001, mean reward: 3.630 [2.023, 10.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.532, 10.100], loss: 0.147819, mae: 0.348395, mean_q: 4.101462
 88836/100000: episode: 2175, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.813, mean reward: 1.858 [1.495, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.518, 10.100], loss: 0.172342, mae: 0.379114, mean_q: 4.163226
 88865/100000: episode: 2176, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 58.048, mean reward: 2.002 [1.708, 2.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.195, 10.100], loss: 0.200831, mae: 0.404765, mean_q: 4.167935
 88868/100000: episode: 2177, duration: 0.023s, episode steps: 3, steps per second: 128, episode reward: 6.918, mean reward: 2.306 [2.094, 2.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.322, 10.100], loss: 0.155385, mae: 0.372639, mean_q: 3.962892
 88897/100000: episode: 2178, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 53.026, mean reward: 1.828 [1.468, 2.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.566, 10.141], loss: 0.190089, mae: 0.368052, mean_q: 4.138678
 88900/100000: episode: 2179, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 8.371, mean reward: 2.790 [2.665, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.340, 10.100], loss: 0.158939, mae: 0.374569, mean_q: 4.047432
[Info] 2-TH LEVEL FOUND: 6.14218282699585, Considering 10/90 traces
 88999/100000: episode: 2180, duration: 5.363s, episode steps: 99, steps per second: 18, episode reward: 182.252, mean reward: 1.841 [1.437, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-1.116, 10.175], loss: 0.155222, mae: 0.366754, mean_q: 4.133934
 89012/100000: episode: 2181, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 40.398, mean reward: 3.108 [2.380, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.754, 10.100], loss: 0.135820, mae: 0.348958, mean_q: 4.061040
 89032/100000: episode: 2182, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 47.950, mean reward: 2.398 [1.933, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.527, 10.100], loss: 0.173085, mae: 0.374186, mean_q: 4.157992
 89055/100000: episode: 2183, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 54.141, mean reward: 2.354 [1.892, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.181, 10.100], loss: 0.180331, mae: 0.388785, mean_q: 4.161064
 89059/100000: episode: 2184, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 19.964, mean reward: 4.991 [3.980, 6.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.456, 10.100], loss: 0.212019, mae: 0.416162, mean_q: 4.112913
 89083/100000: episode: 2185, duration: 0.164s, episode steps: 24, steps per second: 147, episode reward: 67.455, mean reward: 2.811 [2.239, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.633, 10.100], loss: 0.137382, mae: 0.356988, mean_q: 4.117451
 89098/100000: episode: 2186, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 53.772, mean reward: 3.585 [2.525, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.427, 10.100], loss: 0.158431, mae: 0.370345, mean_q: 4.102720
 89102/100000: episode: 2187, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 16.534, mean reward: 4.134 [3.432, 5.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.458, 10.100], loss: 0.142737, mae: 0.375068, mean_q: 4.262437
 89122/100000: episode: 2188, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 47.569, mean reward: 2.378 [1.841, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.302, 10.100], loss: 0.160858, mae: 0.381318, mean_q: 4.095120
 89126/100000: episode: 2189, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 25.440, mean reward: 6.360 [5.080, 8.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.361, 10.100], loss: 0.160969, mae: 0.388853, mean_q: 4.242723
 89130/100000: episode: 2190, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 17.659, mean reward: 4.415 [3.989, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.491, 10.100], loss: 0.179434, mae: 0.398329, mean_q: 4.243503
 89145/100000: episode: 2191, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 48.283, mean reward: 3.219 [2.616, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.413, 10.100], loss: 0.233252, mae: 0.397163, mean_q: 4.201444
 89170/100000: episode: 2192, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 83.186, mean reward: 3.327 [2.642, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.260, 10.100], loss: 0.141548, mae: 0.371579, mean_q: 4.202872
 89195/100000: episode: 2193, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 82.336, mean reward: 3.293 [2.399, 5.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.742, 10.100], loss: 0.174356, mae: 0.390507, mean_q: 4.215649
 89215/100000: episode: 2194, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 66.305, mean reward: 3.315 [2.292, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.295, 10.100], loss: 0.141808, mae: 0.364546, mean_q: 4.207322
 89232/100000: episode: 2195, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 69.387, mean reward: 4.082 [2.319, 7.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.893, 10.100], loss: 0.213523, mae: 0.410338, mean_q: 4.234463
 89252/100000: episode: 2196, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 53.846, mean reward: 2.692 [2.045, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.737, 10.100], loss: 0.137994, mae: 0.366959, mean_q: 4.219645
 89267/100000: episode: 2197, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 44.207, mean reward: 2.947 [2.139, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.724, 10.100], loss: 0.152933, mae: 0.379057, mean_q: 4.310451
 89287/100000: episode: 2198, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 45.963, mean reward: 2.298 [2.091, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.166, 10.100], loss: 0.165293, mae: 0.361004, mean_q: 4.184513
 89304/100000: episode: 2199, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 46.993, mean reward: 2.764 [2.314, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.326, 10.100], loss: 0.227087, mae: 0.409871, mean_q: 4.292512
 89324/100000: episode: 2200, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 55.569, mean reward: 2.778 [2.488, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.456, 10.100], loss: 0.202437, mae: 0.416134, mean_q: 4.324110
 89344/100000: episode: 2201, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 52.921, mean reward: 2.646 [2.258, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.471, 10.100], loss: 0.191123, mae: 0.387793, mean_q: 4.242682
 89348/100000: episode: 2202, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 15.710, mean reward: 3.928 [3.373, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.455, 10.100], loss: 0.204096, mae: 0.407639, mean_q: 4.281917
 89352/100000: episode: 2203, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 25.616, mean reward: 6.404 [4.241, 10.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.420, 10.100], loss: 0.163997, mae: 0.353291, mean_q: 4.063334
 89375/100000: episode: 2204, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 81.617, mean reward: 3.549 [2.614, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.820, 10.100], loss: 0.183305, mae: 0.382601, mean_q: 4.258623
 89392/100000: episode: 2205, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 52.989, mean reward: 3.117 [2.407, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.328, 10.100], loss: 0.152926, mae: 0.386188, mean_q: 4.155528
 89416/100000: episode: 2206, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 75.650, mean reward: 3.152 [2.488, 4.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.566, 10.100], loss: 0.202302, mae: 0.416221, mean_q: 4.289346
 89439/100000: episode: 2207, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 70.326, mean reward: 3.058 [1.991, 5.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.245, 10.100], loss: 0.282431, mae: 0.461007, mean_q: 4.319541
 89459/100000: episode: 2208, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 57.114, mean reward: 2.856 [1.826, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.251, 10.100], loss: 0.190125, mae: 0.402709, mean_q: 4.287525
 89479/100000: episode: 2209, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 53.113, mean reward: 2.656 [1.994, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.328, 10.100], loss: 0.215445, mae: 0.425875, mean_q: 4.339622
 89499/100000: episode: 2210, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 73.938, mean reward: 3.697 [2.665, 6.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.475, 10.100], loss: 0.163800, mae: 0.387578, mean_q: 4.262956
 89514/100000: episode: 2211, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 37.136, mean reward: 2.476 [2.039, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.117, 10.100], loss: 0.180634, mae: 0.397324, mean_q: 4.326561
 89534/100000: episode: 2212, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 49.596, mean reward: 2.480 [1.811, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.951, 10.100], loss: 0.233222, mae: 0.416757, mean_q: 4.352140
 89554/100000: episode: 2213, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 62.313, mean reward: 3.116 [2.226, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.433, 10.100], loss: 0.190406, mae: 0.409737, mean_q: 4.319013
 89569/100000: episode: 2214, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 38.857, mean reward: 2.590 [1.875, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.424, 10.100], loss: 0.247392, mae: 0.441701, mean_q: 4.485058
 89594/100000: episode: 2215, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 82.656, mean reward: 3.306 [2.259, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.751, 10.100], loss: 0.196905, mae: 0.409442, mean_q: 4.301224
 89617/100000: episode: 2216, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 75.674, mean reward: 3.290 [2.730, 5.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.581, 10.100], loss: 0.164213, mae: 0.386981, mean_q: 4.381529
 89630/100000: episode: 2217, duration: 0.097s, episode steps: 13, steps per second: 134, episode reward: 35.650, mean reward: 2.742 [2.410, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.449, 10.100], loss: 0.228102, mae: 0.416124, mean_q: 4.423211
 89643/100000: episode: 2218, duration: 0.085s, episode steps: 13, steps per second: 154, episode reward: 34.686, mean reward: 2.668 [2.011, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.393, 10.100], loss: 0.192598, mae: 0.413438, mean_q: 4.431565
 89667/100000: episode: 2219, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 63.073, mean reward: 2.628 [2.135, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.665, 10.100], loss: 0.191203, mae: 0.403946, mean_q: 4.397775
 89691/100000: episode: 2220, duration: 0.138s, episode steps: 24, steps per second: 173, episode reward: 68.011, mean reward: 2.834 [1.856, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.333, 10.100], loss: 0.195433, mae: 0.421387, mean_q: 4.437894
 89708/100000: episode: 2221, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 56.720, mean reward: 3.336 [2.211, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.778, 10.100], loss: 0.211518, mae: 0.442613, mean_q: 4.450673
 89723/100000: episode: 2222, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 60.737, mean reward: 4.049 [2.657, 6.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.409, 10.100], loss: 0.174749, mae: 0.417545, mean_q: 4.352284
 89743/100000: episode: 2223, duration: 0.128s, episode steps: 20, steps per second: 157, episode reward: 60.512, mean reward: 3.026 [2.119, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.296, 10.100], loss: 0.186432, mae: 0.406833, mean_q: 4.383857
 89758/100000: episode: 2224, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 48.680, mean reward: 3.245 [2.826, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.432, 10.100], loss: 0.245827, mae: 0.446895, mean_q: 4.453746
 89782/100000: episode: 2225, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 51.395, mean reward: 2.141 [1.523, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.045, 10.100], loss: 0.244043, mae: 0.425322, mean_q: 4.397445
 89797/100000: episode: 2226, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 49.182, mean reward: 3.279 [2.441, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.477, 10.100], loss: 0.198112, mae: 0.400005, mean_q: 4.397825
 89822/100000: episode: 2227, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 81.734, mean reward: 3.269 [2.220, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.344, 10.100], loss: 0.221409, mae: 0.424425, mean_q: 4.425246
 89839/100000: episode: 2228, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 47.478, mean reward: 2.793 [2.232, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.232, 10.100], loss: 0.233180, mae: 0.431985, mean_q: 4.428794
 89862/100000: episode: 2229, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 69.340, mean reward: 3.015 [2.111, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.986, 10.100], loss: 0.170677, mae: 0.413640, mean_q: 4.527419
 89877/100000: episode: 2230, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 49.805, mean reward: 3.320 [2.541, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.352, 10.100], loss: 0.222709, mae: 0.420642, mean_q: 4.372122
 89892/100000: episode: 2231, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 43.680, mean reward: 2.912 [2.416, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.188, 10.100], loss: 0.147922, mae: 0.373287, mean_q: 4.410858
 89896/100000: episode: 2232, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 17.941, mean reward: 4.485 [4.002, 5.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.463, 10.100], loss: 0.374630, mae: 0.461950, mean_q: 4.472442
 89909/100000: episode: 2233, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 32.219, mean reward: 2.478 [2.141, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.296, 10.100], loss: 0.226127, mae: 0.444241, mean_q: 4.533339
 89926/100000: episode: 2234, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 43.277, mean reward: 2.546 [2.166, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.401, 10.100], loss: 0.196730, mae: 0.404681, mean_q: 4.560147
 89949/100000: episode: 2235, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 76.426, mean reward: 3.323 [2.456, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.319, 10.100], loss: 0.217609, mae: 0.422131, mean_q: 4.500988
 89953/100000: episode: 2236, duration: 0.035s, episode steps: 4, steps per second: 114, episode reward: 16.371, mean reward: 4.093 [3.915, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.425, 10.100], loss: 0.209820, mae: 0.447828, mean_q: 4.702561
 89968/100000: episode: 2237, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 45.780, mean reward: 3.052 [2.655, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.320, 10.100], loss: 0.251475, mae: 0.452557, mean_q: 4.481452
 89988/100000: episode: 2238, duration: 0.110s, episode steps: 20, steps per second: 183, episode reward: 82.904, mean reward: 4.145 [2.570, 5.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.400, 10.100], loss: 0.200266, mae: 0.422416, mean_q: 4.514731
 90011/100000: episode: 2239, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 61.160, mean reward: 2.659 [2.106, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.341, 10.100], loss: 0.288188, mae: 0.480069, mean_q: 4.469560
 90034/100000: episode: 2240, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 49.933, mean reward: 2.171 [1.501, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.100], loss: 0.247560, mae: 0.440415, mean_q: 4.603015
 90038/100000: episode: 2241, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 23.461, mean reward: 5.865 [4.672, 9.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.508, 10.100], loss: 0.288377, mae: 0.463089, mean_q: 4.367669
 90042/100000: episode: 2242, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 16.062, mean reward: 4.016 [3.255, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.495, 10.100], loss: 0.193344, mae: 0.427527, mean_q: 4.440899
 90067/100000: episode: 2243, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 105.621, mean reward: 4.225 [2.545, 6.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.195, 10.100], loss: 0.207419, mae: 0.442926, mean_q: 4.590559
 90080/100000: episode: 2244, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 36.689, mean reward: 2.822 [2.140, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.323, 10.100], loss: 0.183573, mae: 0.425650, mean_q: 4.687944
 90100/100000: episode: 2245, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 57.610, mean reward: 2.881 [2.072, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.674, 10.100], loss: 0.323602, mae: 0.480235, mean_q: 4.560018
 90120/100000: episode: 2246, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 43.107, mean reward: 2.155 [1.797, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.964, 10.100], loss: 0.246269, mae: 0.471265, mean_q: 4.575225
 90135/100000: episode: 2247, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 41.756, mean reward: 2.784 [2.199, 4.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.319, 10.100], loss: 0.248319, mae: 0.442520, mean_q: 4.580923
 90139/100000: episode: 2248, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 17.997, mean reward: 4.499 [3.520, 5.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.706, 10.100], loss: 0.200326, mae: 0.464502, mean_q: 4.728562
 90164/100000: episode: 2249, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 80.108, mean reward: 3.204 [1.894, 6.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.269, 10.100], loss: 0.291334, mae: 0.448942, mean_q: 4.631526
 90188/100000: episode: 2250, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 58.211, mean reward: 2.425 [2.002, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.270, 10.100], loss: 0.201812, mae: 0.440277, mean_q: 4.658375
 90211/100000: episode: 2251, duration: 0.113s, episode steps: 23, steps per second: 204, episode reward: 68.698, mean reward: 2.987 [2.561, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.477, 10.100], loss: 0.251204, mae: 0.458652, mean_q: 4.673027
 90215/100000: episode: 2252, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 15.764, mean reward: 3.941 [2.986, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.518, 10.100], loss: 0.254235, mae: 0.481552, mean_q: 4.478021
 90235/100000: episode: 2253, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 47.908, mean reward: 2.395 [1.861, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.572, 10.100], loss: 0.274358, mae: 0.471965, mean_q: 4.656887
 90239/100000: episode: 2254, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 21.467, mean reward: 5.367 [3.900, 6.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.428, 10.100], loss: 0.250676, mae: 0.459306, mean_q: 4.774235
 90259/100000: episode: 2255, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 51.461, mean reward: 2.573 [2.195, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.180, 10.100], loss: 0.217696, mae: 0.436558, mean_q: 4.697728
 90272/100000: episode: 2256, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 48.479, mean reward: 3.729 [2.295, 6.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.693, 10.100], loss: 0.288919, mae: 0.504547, mean_q: 4.838189
 90276/100000: episode: 2257, duration: 0.031s, episode steps: 4, steps per second: 131, episode reward: 15.455, mean reward: 3.864 [3.611, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.472, 10.100], loss: 0.198688, mae: 0.413334, mean_q: 4.679270
 90291/100000: episode: 2258, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 49.398, mean reward: 3.293 [2.807, 4.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.695, 10.100], loss: 0.272859, mae: 0.471752, mean_q: 4.688815
 90314/100000: episode: 2259, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 63.051, mean reward: 2.741 [2.015, 7.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.224, 10.100], loss: 0.220188, mae: 0.432350, mean_q: 4.554987
 90338/100000: episode: 2260, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 85.083, mean reward: 3.545 [2.310, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.576, 10.100], loss: 0.260677, mae: 0.473781, mean_q: 4.642456
 90358/100000: episode: 2261, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 71.064, mean reward: 3.553 [2.353, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.545, 10.100], loss: 0.300733, mae: 0.506074, mean_q: 4.714534
 90378/100000: episode: 2262, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 80.217, mean reward: 4.011 [2.521, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.341, 10.100], loss: 0.255886, mae: 0.472803, mean_q: 4.659474
 90395/100000: episode: 2263, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 49.949, mean reward: 2.938 [2.215, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.581, 10.100], loss: 0.287356, mae: 0.471075, mean_q: 4.713319
 90412/100000: episode: 2264, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 57.178, mean reward: 3.363 [2.649, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.374, 10.100], loss: 0.236347, mae: 0.458876, mean_q: 4.679876
 90436/100000: episode: 2265, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 69.071, mean reward: 2.878 [1.925, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.490, 10.100], loss: 0.324150, mae: 0.473901, mean_q: 4.720110
 90459/100000: episode: 2266, duration: 0.180s, episode steps: 23, steps per second: 128, episode reward: 78.313, mean reward: 3.405 [2.222, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.285, 10.100], loss: 0.310754, mae: 0.509061, mean_q: 4.791068
 90482/100000: episode: 2267, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 70.364, mean reward: 3.059 [2.290, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.610, 10.100], loss: 0.281812, mae: 0.472908, mean_q: 4.757909
 90502/100000: episode: 2268, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 57.074, mean reward: 2.854 [2.193, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.211, 10.100], loss: 0.256978, mae: 0.492362, mean_q: 4.839189
 90519/100000: episode: 2269, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 48.373, mean reward: 2.845 [2.420, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.747, 10.100], loss: 0.261628, mae: 0.474929, mean_q: 4.674433
[Info] 3-TH LEVEL FOUND: 7.020390033721924, Considering 10/90 traces
 90536/100000: episode: 2270, duration: 4.556s, episode steps: 17, steps per second: 4, episode reward: 41.681, mean reward: 2.452 [2.009, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.142, 10.100], loss: 0.300128, mae: 0.493417, mean_q: 4.867912
 90547/100000: episode: 2271, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 49.085, mean reward: 4.462 [4.081, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.391, 10.100], loss: 0.392032, mae: 0.526041, mean_q: 4.932431
 90559/100000: episode: 2272, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 43.353, mean reward: 3.613 [2.840, 5.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.637, 10.100], loss: 0.306970, mae: 0.522894, mean_q: 4.951153
 90571/100000: episode: 2273, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 56.707, mean reward: 4.726 [2.845, 10.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.746, 10.100], loss: 0.270660, mae: 0.505377, mean_q: 4.684763
 90585/100000: episode: 2274, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 60.827, mean reward: 4.345 [2.867, 6.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.309, 10.100], loss: 0.314102, mae: 0.509082, mean_q: 4.652477
 90586/100000: episode: 2275, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 9.037, mean reward: 9.037 [9.037, 9.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.509, 10.100], loss: 0.331910, mae: 0.583866, mean_q: 5.282675
 90597/100000: episode: 2276, duration: 0.065s, episode steps: 11, steps per second: 171, episode reward: 39.141, mean reward: 3.558 [2.491, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.253, 10.100], loss: 0.260466, mae: 0.505679, mean_q: 4.741215
 90607/100000: episode: 2277, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 39.791, mean reward: 3.979 [3.297, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.902, 10.100], loss: 0.198002, mae: 0.429663, mean_q: 4.863656
 90619/100000: episode: 2278, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 36.014, mean reward: 3.001 [2.278, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.317, 10.100], loss: 0.248860, mae: 0.465880, mean_q: 4.763345
 90620/100000: episode: 2279, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 7.492, mean reward: 7.492 [7.492, 7.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.584, 10.100], loss: 0.169101, mae: 0.439191, mean_q: 4.657492
 90632/100000: episode: 2280, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 39.769, mean reward: 3.314 [2.779, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.642, 10.100], loss: 0.360040, mae: 0.491313, mean_q: 4.830028
 90644/100000: episode: 2281, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 45.914, mean reward: 3.826 [2.925, 5.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.424, 10.100], loss: 0.226529, mae: 0.446026, mean_q: 4.856731
 90645/100000: episode: 2282, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 6.371, mean reward: 6.371 [6.371, 6.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.534, 10.100], loss: 0.424044, mae: 0.600833, mean_q: 4.983661
 90646/100000: episode: 2283, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 8.633, mean reward: 8.633 [8.633, 8.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.544, 10.100], loss: 0.274730, mae: 0.470604, mean_q: 4.791184
 90661/100000: episode: 2284, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 45.217, mean reward: 3.014 [2.522, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.217, 10.100], loss: 0.246206, mae: 0.453974, mean_q: 4.773213
 90671/100000: episode: 2285, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 49.239, mean reward: 4.924 [3.407, 8.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.388, 10.100], loss: 0.232339, mae: 0.491677, mean_q: 4.867005
 90683/100000: episode: 2286, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 58.377, mean reward: 4.865 [3.018, 9.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.371, 10.100], loss: 0.281662, mae: 0.493200, mean_q: 5.020576
 90698/100000: episode: 2287, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 50.789, mean reward: 3.386 [2.284, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.437, 10.100], loss: 0.294200, mae: 0.487620, mean_q: 4.881479
 90710/100000: episode: 2288, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 57.878, mean reward: 4.823 [3.678, 7.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.498, 10.100], loss: 0.281226, mae: 0.451048, mean_q: 4.917916
 90721/100000: episode: 2289, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 52.303, mean reward: 4.755 [4.102, 6.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.441, 10.100], loss: 0.330474, mae: 0.532090, mean_q: 4.950217
 90733/100000: episode: 2290, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 36.332, mean reward: 3.028 [2.590, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.758, 10.100], loss: 0.247513, mae: 0.478574, mean_q: 4.867094
 90745/100000: episode: 2291, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 60.918, mean reward: 5.077 [2.997, 10.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.463, 10.100], loss: 0.335922, mae: 0.540881, mean_q: 4.974150
 90755/100000: episode: 2292, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 30.723, mean reward: 3.072 [2.578, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.207, 10.100], loss: 0.263820, mae: 0.485198, mean_q: 4.885451
 90765/100000: episode: 2293, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 42.664, mean reward: 4.266 [3.453, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.766, 10.100], loss: 0.296926, mae: 0.528353, mean_q: 4.991887
 90777/100000: episode: 2294, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 37.672, mean reward: 3.139 [2.683, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.428, 10.100], loss: 0.240981, mae: 0.462823, mean_q: 5.037123
 90778/100000: episode: 2295, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 5.273, mean reward: 5.273 [5.273, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.524, 10.100], loss: 0.340051, mae: 0.595822, mean_q: 4.884628
 90790/100000: episode: 2296, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 40.565, mean reward: 3.380 [2.914, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.389, 10.100], loss: 0.213435, mae: 0.452836, mean_q: 4.845870
 90800/100000: episode: 2297, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 33.584, mean reward: 3.358 [3.014, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.359, 10.100], loss: 0.286664, mae: 0.527431, mean_q: 4.981905
 90810/100000: episode: 2298, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 33.868, mean reward: 3.387 [2.976, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.352, 10.100], loss: 0.244101, mae: 0.444210, mean_q: 4.768868
 90820/100000: episode: 2299, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 35.599, mean reward: 3.560 [2.708, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.404, 10.100], loss: 0.316740, mae: 0.532894, mean_q: 5.022617
 90834/100000: episode: 2300, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 69.188, mean reward: 4.942 [3.892, 6.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.546, 10.100], loss: 0.318131, mae: 0.511309, mean_q: 5.043518
 90851/100000: episode: 2301, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 63.673, mean reward: 3.745 [2.680, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.296, 10.100], loss: 0.377023, mae: 0.571052, mean_q: 4.897773
 90861/100000: episode: 2302, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 25.132, mean reward: 2.513 [2.186, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.837, 10.100], loss: 0.442362, mae: 0.691077, mean_q: 5.076556
 90871/100000: episode: 2303, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 59.110, mean reward: 5.911 [3.746, 10.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.376, 10.100], loss: 0.340349, mae: 0.529675, mean_q: 4.844935
 90883/100000: episode: 2304, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 34.197, mean reward: 2.850 [2.336, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.676, 10.100], loss: 0.409629, mae: 0.566979, mean_q: 5.072437
 90895/100000: episode: 2305, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 50.724, mean reward: 4.227 [2.787, 5.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.528, 10.100], loss: 0.334150, mae: 0.544254, mean_q: 5.084601
 90907/100000: episode: 2306, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 36.002, mean reward: 3.000 [2.498, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.360, 10.100], loss: 0.359211, mae: 0.574691, mean_q: 5.043073
 90919/100000: episode: 2307, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 34.155, mean reward: 2.846 [2.115, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.336, 10.100], loss: 0.447480, mae: 0.598585, mean_q: 4.987185
 90920/100000: episode: 2308, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 6.869, mean reward: 6.869 [6.869, 6.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.570, 10.100], loss: 0.296441, mae: 0.565278, mean_q: 5.307178
 90932/100000: episode: 2309, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 33.544, mean reward: 2.795 [2.466, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.530, 10.100], loss: 0.420610, mae: 0.564267, mean_q: 4.991368
 90943/100000: episode: 2310, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 66.614, mean reward: 6.056 [3.430, 8.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.325, 10.100], loss: 0.274244, mae: 0.475266, mean_q: 4.953087
 90958/100000: episode: 2311, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 67.737, mean reward: 4.516 [3.081, 9.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.374, 10.100], loss: 0.367861, mae: 0.545875, mean_q: 4.970677
 90968/100000: episode: 2312, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 57.931, mean reward: 5.793 [3.857, 12.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.420, 10.100], loss: 0.284293, mae: 0.498224, mean_q: 5.002808
 90978/100000: episode: 2313, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 33.726, mean reward: 3.373 [2.269, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.276, 10.100], loss: 0.430165, mae: 0.595423, mean_q: 5.160608
 90990/100000: episode: 2314, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 42.572, mean reward: 3.548 [2.682, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.428, 10.100], loss: 0.365863, mae: 0.544766, mean_q: 5.157741
 91000/100000: episode: 2315, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 29.400, mean reward: 2.940 [2.791, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.835, 10.100], loss: 0.378823, mae: 0.537781, mean_q: 5.057415
 91001/100000: episode: 2316, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 6.954, mean reward: 6.954 [6.954, 6.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.555, 10.100], loss: 0.488637, mae: 0.667925, mean_q: 5.520033
 91015/100000: episode: 2317, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 50.653, mean reward: 3.618 [2.583, 6.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.338, 10.100], loss: 0.298531, mae: 0.505053, mean_q: 5.058097
 91027/100000: episode: 2318, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 44.441, mean reward: 3.703 [2.952, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.452, 10.100], loss: 0.416344, mae: 0.575315, mean_q: 5.024817
 91039/100000: episode: 2319, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 42.017, mean reward: 3.501 [2.831, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.278, 10.100], loss: 0.324296, mae: 0.524215, mean_q: 5.038055
 91054/100000: episode: 2320, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 63.437, mean reward: 4.229 [2.821, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.324, 10.100], loss: 0.343574, mae: 0.544671, mean_q: 5.123700
 91055/100000: episode: 2321, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 11.221, mean reward: 11.221 [11.221, 11.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.553, 10.100], loss: 0.269133, mae: 0.564884, mean_q: 5.220709
 91056/100000: episode: 2322, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 11.971, mean reward: 11.971 [11.971, 11.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.473, 10.100], loss: 0.390668, mae: 0.570272, mean_q: 5.435760
 91068/100000: episode: 2323, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 40.125, mean reward: 3.344 [2.547, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.518, 10.100], loss: 0.500835, mae: 0.602156, mean_q: 5.158459
 91078/100000: episode: 2324, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 40.532, mean reward: 4.053 [3.200, 5.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.294, 10.100], loss: 0.381769, mae: 0.548461, mean_q: 5.129080
 91095/100000: episode: 2325, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 73.403, mean reward: 4.318 [3.166, 15.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.862, 10.100], loss: 0.465357, mae: 0.605413, mean_q: 4.995588
 91107/100000: episode: 2326, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 42.157, mean reward: 3.513 [2.847, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.241, 10.100], loss: 0.291702, mae: 0.549859, mean_q: 5.046491
 91121/100000: episode: 2327, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 64.816, mean reward: 4.630 [3.481, 6.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.504, 10.100], loss: 0.309010, mae: 0.549725, mean_q: 5.137845
 91133/100000: episode: 2328, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 56.540, mean reward: 4.712 [3.191, 5.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.672, 10.100], loss: 0.386595, mae: 0.587805, mean_q: 5.229949
 91134/100000: episode: 2329, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 6.031, mean reward: 6.031 [6.031, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.507, 10.100], loss: 0.176204, mae: 0.453644, mean_q: 5.055151
 91144/100000: episode: 2330, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 45.909, mean reward: 4.591 [3.344, 6.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.323, 10.100], loss: 0.271677, mae: 0.496186, mean_q: 5.077846
 91156/100000: episode: 2331, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 40.521, mean reward: 3.377 [2.293, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.252, 10.100], loss: 0.641666, mae: 0.681681, mean_q: 5.323934
 91166/100000: episode: 2332, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 51.017, mean reward: 5.102 [3.734, 6.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.097, 10.100], loss: 0.389897, mae: 0.570066, mean_q: 4.887634
 91178/100000: episode: 2333, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 43.324, mean reward: 3.610 [2.718, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.458, 10.100], loss: 0.273589, mae: 0.545348, mean_q: 5.230382
 91193/100000: episode: 2334, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 42.299, mean reward: 2.820 [2.396, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.390, 10.100], loss: 0.315704, mae: 0.542151, mean_q: 5.117231
 91210/100000: episode: 2335, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 63.450, mean reward: 3.732 [2.683, 5.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.096, 10.100], loss: 0.298707, mae: 0.509909, mean_q: 5.217568
 91225/100000: episode: 2336, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 50.737, mean reward: 3.382 [2.399, 6.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.432, 10.100], loss: 0.291561, mae: 0.501236, mean_q: 5.191457
 91239/100000: episode: 2337, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 63.844, mean reward: 4.560 [2.999, 9.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.286, 10.100], loss: 0.517859, mae: 0.572539, mean_q: 5.263873
 91249/100000: episode: 2338, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 36.123, mean reward: 3.612 [2.932, 4.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.344, 10.100], loss: 0.374515, mae: 0.569250, mean_q: 5.043778
 91266/100000: episode: 2339, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 52.782, mean reward: 3.105 [2.325, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.485, 10.100], loss: 0.346009, mae: 0.531180, mean_q: 5.243530
 91267/100000: episode: 2340, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 5.794, mean reward: 5.794 [5.794, 5.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.501, 10.100], loss: 0.173395, mae: 0.495249, mean_q: 5.153051
 91277/100000: episode: 2341, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 29.984, mean reward: 2.998 [2.084, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.345, 10.100], loss: 0.417220, mae: 0.556008, mean_q: 5.243077
 91289/100000: episode: 2342, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 48.486, mean reward: 4.041 [3.029, 6.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.348, 10.100], loss: 0.390308, mae: 0.574322, mean_q: 5.274096
 91303/100000: episode: 2343, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 76.780, mean reward: 5.484 [3.830, 7.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.525, 10.100], loss: 0.307349, mae: 0.507471, mean_q: 5.129118
 91314/100000: episode: 2344, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 51.885, mean reward: 4.717 [3.043, 6.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.352, 10.100], loss: 0.380337, mae: 0.571939, mean_q: 5.263155
 91328/100000: episode: 2345, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 51.334, mean reward: 3.667 [2.846, 5.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.382, 10.100], loss: 0.378799, mae: 0.567180, mean_q: 5.129363
 91329/100000: episode: 2346, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 7.648, mean reward: 7.648 [7.648, 7.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.526, 10.100], loss: 0.480204, mae: 0.639649, mean_q: 5.511093
 91339/100000: episode: 2347, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 44.297, mean reward: 4.430 [3.960, 5.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.451, 10.100], loss: 0.434186, mae: 0.622950, mean_q: 5.331922
 91340/100000: episode: 2348, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 6.105, mean reward: 6.105 [6.105, 6.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.555, 10.100], loss: 0.554266, mae: 0.636102, mean_q: 5.208349
 91357/100000: episode: 2349, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 59.971, mean reward: 3.528 [2.625, 5.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.384, 10.100], loss: 0.390879, mae: 0.581405, mean_q: 5.260280
 91369/100000: episode: 2350, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 59.660, mean reward: 4.972 [2.764, 8.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.390, 10.100], loss: 0.245013, mae: 0.503063, mean_q: 5.300251
 91381/100000: episode: 2351, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 35.905, mean reward: 2.992 [2.717, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.417, 10.100], loss: 0.337321, mae: 0.564814, mean_q: 5.292577
 91393/100000: episode: 2352, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 49.840, mean reward: 4.153 [3.150, 5.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.536, 10.100], loss: 0.524503, mae: 0.643099, mean_q: 5.447049
 91405/100000: episode: 2353, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 63.771, mean reward: 5.314 [2.896, 8.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.520, 10.100], loss: 0.324610, mae: 0.567163, mean_q: 5.291599
 91417/100000: episode: 2354, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 35.183, mean reward: 2.932 [2.541, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.336, 10.100], loss: 0.382611, mae: 0.567760, mean_q: 5.211382
 91427/100000: episode: 2355, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 48.134, mean reward: 4.813 [3.675, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.414, 10.100], loss: 0.518791, mae: 0.702806, mean_q: 5.256572
 91428/100000: episode: 2356, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 7.275, mean reward: 7.275 [7.275, 7.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.501, 10.100], loss: 0.532305, mae: 0.683761, mean_q: 5.016231
 91445/100000: episode: 2357, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 87.866, mean reward: 5.169 [2.697, 7.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.729, 10.100], loss: 0.467977, mae: 0.618478, mean_q: 5.499158
 91456/100000: episode: 2358, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 41.928, mean reward: 3.812 [2.945, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.542, 10.100], loss: 0.434807, mae: 0.602537, mean_q: 5.302620
 91457/100000: episode: 2359, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 8.607, mean reward: 8.607 [8.607, 8.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.546, 10.100], loss: 0.239406, mae: 0.530189, mean_q: 5.325945
[Info] 4-TH LEVEL FOUND: 7.814341068267822, Considering 10/90 traces
 91467/100000: episode: 2360, duration: 4.273s, episode steps: 10, steps per second: 2, episode reward: 26.936, mean reward: 2.694 [2.128, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.393, 10.100], loss: 0.325660, mae: 0.553972, mean_q: 5.263869
 91476/100000: episode: 2361, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 34.908, mean reward: 3.879 [3.334, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.409, 10.100], loss: 0.367627, mae: 0.553111, mean_q: 5.272171
 91485/100000: episode: 2362, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 44.246, mean reward: 4.916 [3.126, 7.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.344, 10.100], loss: 0.450395, mae: 0.635740, mean_q: 5.666872
 91494/100000: episode: 2363, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 48.467, mean reward: 5.385 [3.764, 7.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.371, 10.100], loss: 0.433923, mae: 0.601456, mean_q: 5.390868
 91503/100000: episode: 2364, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 41.763, mean reward: 4.640 [3.130, 6.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.374, 10.100], loss: 0.401919, mae: 0.593627, mean_q: 5.429000
 91513/100000: episode: 2365, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 71.266, mean reward: 7.127 [4.161, 14.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.440, 10.100], loss: 0.686817, mae: 0.700350, mean_q: 5.572514
 91521/100000: episode: 2366, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 43.525, mean reward: 5.441 [3.938, 7.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.369, 10.100], loss: 0.527387, mae: 0.697295, mean_q: 5.362443
 91531/100000: episode: 2367, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 49.065, mean reward: 4.907 [3.608, 6.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.384, 10.100], loss: 0.373949, mae: 0.571789, mean_q: 5.303349
 91540/100000: episode: 2368, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 34.937, mean reward: 3.882 [3.130, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.474, 10.100], loss: 0.300553, mae: 0.552891, mean_q: 5.403469
 91548/100000: episode: 2369, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 43.147, mean reward: 5.393 [3.828, 8.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.598, 10.100], loss: 0.223927, mae: 0.472903, mean_q: 5.268091
 91557/100000: episode: 2370, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 35.194, mean reward: 3.910 [3.209, 5.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.450, 10.100], loss: 0.644384, mae: 0.689568, mean_q: 5.672103
 91565/100000: episode: 2371, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 50.235, mean reward: 6.279 [3.309, 10.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.490, 10.100], loss: 0.357164, mae: 0.564605, mean_q: 5.583225
 91574/100000: episode: 2372, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 38.043, mean reward: 4.227 [2.924, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.452, 10.100], loss: 0.361740, mae: 0.553864, mean_q: 5.410244
 91582/100000: episode: 2373, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 29.286, mean reward: 3.661 [3.141, 5.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.345, 10.100], loss: 0.425141, mae: 0.651197, mean_q: 5.404062
 91592/100000: episode: 2374, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 42.378, mean reward: 4.238 [3.209, 5.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.340, 10.100], loss: 0.419276, mae: 0.669929, mean_q: 5.676414
 91602/100000: episode: 2375, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 52.407, mean reward: 5.241 [3.321, 7.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.390, 10.100], loss: 0.591840, mae: 0.616938, mean_q: 5.448630
 91610/100000: episode: 2376, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 52.525, mean reward: 6.566 [3.923, 9.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.533, 10.100], loss: 0.369395, mae: 0.581489, mean_q: 5.424307
 91618/100000: episode: 2377, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 37.363, mean reward: 4.670 [3.144, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.386, 10.100], loss: 0.426768, mae: 0.602519, mean_q: 5.389219
 91626/100000: episode: 2378, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 1127.296, mean reward: 140.912 [5.233, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.936, 10.100], loss: 0.412258, mae: 0.653109, mean_q: 5.645633
 91634/100000: episode: 2379, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 24.232, mean reward: 3.029 [2.507, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.362, 10.100], loss: 1.099690, mae: 0.738106, mean_q: 5.629954
 91643/100000: episode: 2380, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 38.572, mean reward: 4.286 [3.475, 5.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.425, 10.100], loss: 0.501887, mae: 0.655677, mean_q: 5.631503
 91652/100000: episode: 2381, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 35.451, mean reward: 3.939 [3.382, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.966, 10.100], loss: 0.478090, mae: 0.692842, mean_q: 5.721928
 91661/100000: episode: 2382, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 34.058, mean reward: 3.784 [3.204, 4.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.564, 10.100], loss: 0.763091, mae: 0.697046, mean_q: 5.523478
 91669/100000: episode: 2383, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 39.590, mean reward: 4.949 [3.259, 6.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.599, 10.100], loss: 0.305781, mae: 0.558736, mean_q: 5.667613
 91678/100000: episode: 2384, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 47.653, mean reward: 5.295 [4.303, 9.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.577, 10.100], loss: 1710.699829, mae: 4.680917, mean_q: 6.229887
 91687/100000: episode: 2385, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 36.028, mean reward: 4.003 [2.914, 5.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.450, 10.100], loss: 3.632393, mae: 2.126684, mean_q: 6.858047
 91695/100000: episode: 2386, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 39.650, mean reward: 4.956 [4.568, 5.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.528, 10.100], loss: 1.683163, mae: 1.489512, mean_q: 4.671840
 91703/100000: episode: 2387, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 50.228, mean reward: 6.279 [4.000, 9.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.509, 10.100], loss: 1.095186, mae: 1.107331, mean_q: 6.640257
 91712/100000: episode: 2388, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 32.798, mean reward: 3.644 [2.983, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.414, 10.100], loss: 1.059223, mae: 0.945349, mean_q: 5.370840
 91721/100000: episode: 2389, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 30.042, mean reward: 3.338 [2.686, 4.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.409, 10.100], loss: 0.786141, mae: 0.864894, mean_q: 6.244244
 91730/100000: episode: 2390, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 29.223, mean reward: 3.247 [2.796, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.592, 10.100], loss: 1703.309326, mae: 4.255008, mean_q: 5.763741
 91738/100000: episode: 2391, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 33.218, mean reward: 4.152 [3.655, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.087, 10.100], loss: 7.301335, mae: 3.430036, mean_q: 9.307953
 91746/100000: episode: 2392, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 32.438, mean reward: 4.055 [3.093, 5.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.499, 10.100], loss: 1.840840, mae: 1.431115, mean_q: 6.796961
 91755/100000: episode: 2393, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 54.231, mean reward: 6.026 [3.842, 10.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.483, 10.100], loss: 1.634695, mae: 1.545880, mean_q: 4.696230
 91763/100000: episode: 2394, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 40.380, mean reward: 5.048 [3.659, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.388, 10.100], loss: 1.050128, mae: 0.952885, mean_q: 5.544386
 91773/100000: episode: 2395, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 42.831, mean reward: 4.283 [3.565, 5.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.858, 10.100], loss: 1.221817, mae: 0.977500, mean_q: 6.439134
 91781/100000: episode: 2396, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 83.356, mean reward: 10.420 [3.077, 30.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.152, 10.100], loss: 0.738986, mae: 0.773070, mean_q: 6.159088
 91791/100000: episode: 2397, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 48.823, mean reward: 4.882 [4.154, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.595, 10.100], loss: 0.788737, mae: 0.803346, mean_q: 5.637202
 91799/100000: episode: 2398, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 203.205, mean reward: 25.401 [4.718, 130.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.703, 10.100], loss: 1913.260254, mae: 4.789621, mean_q: 6.245676
 91809/100000: episode: 2399, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 49.824, mean reward: 4.982 [3.228, 10.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.835, 10.100], loss: 1525.733887, mae: 6.736929, mean_q: 9.584544
 91817/100000: episode: 2400, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 51.594, mean reward: 6.449 [3.817, 18.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.638, 10.100], loss: 8.668833, mae: 3.490625, mean_q: 9.706746
 91827/100000: episode: 2401, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 37.458, mean reward: 3.746 [3.205, 4.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.368, 10.100], loss: 26.382690, mae: 1.796371, mean_q: 5.642453
 91835/100000: episode: 2402, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 34.435, mean reward: 4.304 [3.800, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.469, 10.100], loss: 1.722859, mae: 1.408276, mean_q: 5.393462
 91845/100000: episode: 2403, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 33.218, mean reward: 3.322 [2.779, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.285, 10.100], loss: 1.549361, mae: 1.155897, mean_q: 6.073707
 91855/100000: episode: 2404, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 44.190, mean reward: 4.419 [3.435, 8.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.408, 10.100], loss: 1534.999634, mae: 4.327079, mean_q: 6.639457
 91865/100000: episode: 2405, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 39.758, mean reward: 3.976 [3.387, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.457, 10.100], loss: 5.195409, mae: 2.720456, mean_q: 8.766342
 91873/100000: episode: 2406, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 44.583, mean reward: 5.573 [4.122, 7.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.595, 10.100], loss: 1.212078, mae: 1.125994, mean_q: 6.844603
 91881/100000: episode: 2407, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 46.822, mean reward: 5.853 [4.702, 7.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.466, 10.100], loss: 1.306695, mae: 1.127100, mean_q: 5.793311
 91890/100000: episode: 2408, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 35.448, mean reward: 3.939 [3.183, 5.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.490, 10.100], loss: 1.258404, mae: 1.069729, mean_q: 5.727039
 91898/100000: episode: 2409, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 44.038, mean reward: 5.505 [4.563, 6.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.446, 10.100], loss: 4.081635, mae: 1.002682, mean_q: 6.095567
 91907/100000: episode: 2410, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 39.659, mean reward: 4.407 [2.832, 6.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.397, 10.100], loss: 1.005981, mae: 0.944171, mean_q: 6.715690
 91915/100000: episode: 2411, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 83.368, mean reward: 10.421 [3.535, 22.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.892, 10.100], loss: 1.120695, mae: 0.936757, mean_q: 6.444561
 91924/100000: episode: 2412, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 32.943, mean reward: 3.660 [3.038, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.468, 10.100], loss: 0.906930, mae: 0.797011, mean_q: 6.190606
 91932/100000: episode: 2413, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 34.215, mean reward: 4.277 [3.023, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.422, 10.100], loss: 0.684660, mae: 0.739566, mean_q: 6.037495
 91940/100000: episode: 2414, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 33.315, mean reward: 4.164 [3.279, 5.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.549, 10.100], loss: 0.769465, mae: 0.723472, mean_q: 6.029435
 91948/100000: episode: 2415, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 38.150, mean reward: 4.769 [4.215, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.438, 10.100], loss: 2.613569, mae: 0.990947, mean_q: 6.157772
 91958/100000: episode: 2416, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 59.951, mean reward: 5.995 [3.711, 13.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.622, 10.100], loss: 25.986307, mae: 1.305359, mean_q: 6.467273
 91968/100000: episode: 2417, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 53.769, mean reward: 5.377 [4.279, 6.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.524, 10.100], loss: 0.803067, mae: 0.821677, mean_q: 6.732179
 91976/100000: episode: 2418, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 48.057, mean reward: 6.007 [3.566, 12.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.522, 10.100], loss: 1.044621, mae: 0.857128, mean_q: 6.413888
 91985/100000: episode: 2419, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 36.164, mean reward: 4.018 [3.156, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.483, 10.100], loss: 0.877641, mae: 0.780371, mean_q: 6.105743
 91994/100000: episode: 2420, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 47.716, mean reward: 5.302 [3.693, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.570, 10.100], loss: 1.108737, mae: 0.827987, mean_q: 5.977136
 92004/100000: episode: 2421, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 64.043, mean reward: 6.404 [4.807, 9.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.412, 10.100], loss: 1.008619, mae: 0.735592, mean_q: 6.104439
[Info] FALSIFICATION!
 92011/100000: episode: 2422, duration: 0.432s, episode steps: 7, steps per second: 16, episode reward: 1036.589, mean reward: 148.084 [4.290, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.539, 10.098], loss: 1.060025, mae: 0.891217, mean_q: 6.129340
 92019/100000: episode: 2423, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 27.833, mean reward: 3.479 [2.922, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.449, 10.100], loss: 1.404472, mae: 0.962706, mean_q: 6.196246
 92027/100000: episode: 2424, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 32.020, mean reward: 4.002 [3.766, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.505, 10.100], loss: 1.192654, mae: 0.879203, mean_q: 6.127244
 92035/100000: episode: 2425, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 49.916, mean reward: 6.240 [4.237, 9.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.403, 10.100], loss: 0.803639, mae: 0.807893, mean_q: 6.400773
 92043/100000: episode: 2426, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 38.097, mean reward: 4.762 [3.670, 6.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.509, 10.100], loss: 0.872409, mae: 0.807296, mean_q: 6.023932
 92051/100000: episode: 2427, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 43.875, mean reward: 5.484 [4.074, 7.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.438, 10.100], loss: 1918.529663, mae: 5.344870, mean_q: 7.254216
 92060/100000: episode: 2428, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 47.300, mean reward: 5.256 [3.178, 8.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.315, 10.100], loss: 1694.338013, mae: 6.440838, mean_q: 9.290487
 92069/100000: episode: 2429, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 87.053, mean reward: 9.673 [4.010, 33.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.596, 10.100], loss: 7.148261, mae: 3.119245, mean_q: 9.533787
 92078/100000: episode: 2430, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 52.100, mean reward: 5.789 [3.528, 9.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.574, 10.100], loss: 2.353467, mae: 1.547893, mean_q: 7.037524
 92086/100000: episode: 2431, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 54.498, mean reward: 6.812 [4.702, 8.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.418, 10.100], loss: 1.658959, mae: 1.422904, mean_q: 5.357412
 92096/100000: episode: 2432, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 209.107, mean reward: 20.911 [3.967, 124.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.456, 10.100], loss: 1.564204, mae: 1.243248, mean_q: 5.827251
 92104/100000: episode: 2433, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 102.259, mean reward: 12.782 [4.772, 45.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.755, 10.100], loss: 1.172371, mae: 0.954920, mean_q: 6.869954
 92112/100000: episode: 2434, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 53.380, mean reward: 6.673 [5.446, 7.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.584, 10.100], loss: 1910.871582, mae: 5.519577, mean_q: 7.606467
 92120/100000: episode: 2435, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 41.607, mean reward: 5.201 [3.677, 7.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.434, 10.100], loss: 3.270272, mae: 2.079131, mean_q: 8.370596
 92129/100000: episode: 2436, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 33.302, mean reward: 3.700 [3.343, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.760, 10.100], loss: 2.230328, mae: 1.465182, mean_q: 7.697612
 92139/100000: episode: 2437, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 44.097, mean reward: 4.410 [3.716, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.374, 10.100], loss: 1527.472656, mae: 4.233075, mean_q: 6.290442
 92148/100000: episode: 2438, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 27.354, mean reward: 3.039 [2.587, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.361, 10.100], loss: 2.153511, mae: 1.393807, mean_q: 7.780478
 92158/100000: episode: 2439, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 50.380, mean reward: 5.038 [3.038, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.596, 10.100], loss: 1523.198242, mae: 5.532419, mean_q: 8.997080
 92167/100000: episode: 2440, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 51.121, mean reward: 5.680 [4.189, 8.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.536, 10.100], loss: 3.383191, mae: 1.929903, mean_q: 8.385403
 92176/100000: episode: 2441, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 33.196, mean reward: 3.688 [3.220, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.343, 10.100], loss: 1.671080, mae: 1.233662, mean_q: 7.012482
 92185/100000: episode: 2442, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 45.678, mean reward: 5.075 [3.412, 8.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.558, 10.100], loss: 1696.241577, mae: 4.684260, mean_q: 6.525445
 92193/100000: episode: 2443, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 52.267, mean reward: 6.533 [5.373, 9.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.621, 10.100], loss: 3.622399, mae: 1.985296, mean_q: 8.333327
 92203/100000: episode: 2444, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 88.404, mean reward: 8.840 [5.904, 12.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.630, 10.100], loss: 4.136374, mae: 1.571687, mean_q: 7.938972
 92212/100000: episode: 2445, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 46.472, mean reward: 5.164 [3.708, 9.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.448, 10.100], loss: 1.677551, mae: 1.197199, mean_q: 7.029094
 92220/100000: episode: 2446, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 64.492, mean reward: 8.061 [4.980, 13.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.768, 10.100], loss: 1.757035, mae: 1.244620, mean_q: 6.496095
 92229/100000: episode: 2447, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 30.801, mean reward: 3.422 [2.786, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.380, 10.100], loss: 3.354496, mae: 1.383248, mean_q: 6.380121
 92238/100000: episode: 2448, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 44.083, mean reward: 4.898 [3.551, 7.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.753, 10.100], loss: 3.422784, mae: 1.404168, mean_q: 6.837353
 92247/100000: episode: 2449, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 42.500, mean reward: 4.722 [3.220, 6.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.558, 10.100], loss: 1.286160, mae: 1.090709, mean_q: 6.620964
[Info] Complete ISplit Iteration
[Info] Levels: [4.4303994, 6.142183, 7.02039, 7.814341, 13.212765]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.05]
[Info] Error Prob: 5.000000000000002e-06

 92255/100000: episode: 2450, duration: 4.663s, episode steps: 8, steps per second: 2, episode reward: 26.431, mean reward: 3.304 [2.823, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.530, 10.100], loss: 1.117485, mae: 0.917247, mean_q: 6.520627
 92355/100000: episode: 2451, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 204.566, mean reward: 2.046 [1.464, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.098], loss: 2.092064, mae: 1.064063, mean_q: 6.645828
 92455/100000: episode: 2452, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 175.610, mean reward: 1.756 [1.476, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.609, 10.098], loss: 312.057587, mae: 1.884831, mean_q: 6.877804
 92555/100000: episode: 2453, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 184.025, mean reward: 1.840 [1.467, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.090, 10.098], loss: 461.052155, mae: 3.423494, mean_q: 8.383842
 92655/100000: episode: 2454, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 232.722, mean reward: 2.327 [1.465, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.547, 10.443], loss: 1.903462, mae: 1.078359, mean_q: 6.561804
 92755/100000: episode: 2455, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 208.750, mean reward: 2.088 [1.468, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.066, 10.319], loss: 762.139160, mae: 3.585674, mean_q: 8.062974
 92855/100000: episode: 2456, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 207.471, mean reward: 2.075 [1.470, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.308, 10.339], loss: 314.783325, mae: 2.221955, mean_q: 7.054836
 92955/100000: episode: 2457, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 193.080, mean reward: 1.931 [1.453, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.305], loss: 313.509827, mae: 2.745161, mean_q: 7.976082
 93055/100000: episode: 2458, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 232.126, mean reward: 2.321 [1.477, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.828, 10.275], loss: 156.391998, mae: 1.720616, mean_q: 7.201550
 93155/100000: episode: 2459, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 178.094, mean reward: 1.781 [1.472, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.475, 10.098], loss: 310.930725, mae: 2.039956, mean_q: 7.151682
 93255/100000: episode: 2460, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.532, mean reward: 1.935 [1.474, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.360, 10.343], loss: 153.585236, mae: 1.711312, mean_q: 7.143416
 93355/100000: episode: 2461, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 197.836, mean reward: 1.978 [1.486, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.951, 10.098], loss: 305.684662, mae: 1.867631, mean_q: 6.944863
 93455/100000: episode: 2462, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.151, mean reward: 1.872 [1.496, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.732, 10.131], loss: 310.184204, mae: 2.368320, mean_q: 7.490377
 93555/100000: episode: 2463, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 184.609, mean reward: 1.846 [1.449, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.718, 10.179], loss: 158.627670, mae: 1.636523, mean_q: 6.982349
 93655/100000: episode: 2464, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 182.675, mean reward: 1.827 [1.447, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.489, 10.150], loss: 465.411102, mae: 2.500352, mean_q: 7.189302
 93755/100000: episode: 2465, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 213.825, mean reward: 2.138 [1.472, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.349, 10.385], loss: 6.280674, mae: 1.513671, mean_q: 6.924856
 93855/100000: episode: 2466, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 194.567, mean reward: 1.946 [1.487, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.211, 10.251], loss: 163.406616, mae: 1.743341, mean_q: 6.901840
 93955/100000: episode: 2467, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 192.646, mean reward: 1.926 [1.497, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.368, 10.098], loss: 6.532482, mae: 1.059272, mean_q: 6.377537
 94055/100000: episode: 2468, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 191.370, mean reward: 1.914 [1.463, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.709, 10.259], loss: 612.377136, mae: 2.753670, mean_q: 6.820435
 94155/100000: episode: 2469, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 181.929, mean reward: 1.819 [1.492, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.095, 10.098], loss: 307.242523, mae: 2.686043, mean_q: 7.664410
 94255/100000: episode: 2470, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.248, mean reward: 1.992 [1.507, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.098], loss: 309.947083, mae: 2.097079, mean_q: 7.091250
 94355/100000: episode: 2471, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 208.734, mean reward: 2.087 [1.452, 5.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.915, 10.098], loss: 155.074783, mae: 1.451730, mean_q: 6.406922
 94455/100000: episode: 2472, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 181.026, mean reward: 1.810 [1.450, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.328, 10.098], loss: 156.270920, mae: 1.775147, mean_q: 6.776330
 94555/100000: episode: 2473, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 196.832, mean reward: 1.968 [1.469, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.834, 10.130], loss: 9.294299, mae: 1.091889, mean_q: 6.178572
 94655/100000: episode: 2474, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 182.372, mean reward: 1.824 [1.452, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.496, 10.164], loss: 154.278244, mae: 1.485772, mean_q: 6.308302
 94755/100000: episode: 2475, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 184.580, mean reward: 1.846 [1.450, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.098], loss: 455.398193, mae: 2.376181, mean_q: 6.725151
 94855/100000: episode: 2476, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.685, mean reward: 1.977 [1.448, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.444, 10.098], loss: 605.341064, mae: 3.107654, mean_q: 7.180476
 94955/100000: episode: 2477, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 182.167, mean reward: 1.822 [1.460, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.326, 10.098], loss: 302.883972, mae: 2.183578, mean_q: 6.859464
 95055/100000: episode: 2478, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 199.540, mean reward: 1.995 [1.443, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.386], loss: 160.242859, mae: 1.512970, mean_q: 6.301658
 95155/100000: episode: 2479, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 216.184, mean reward: 2.162 [1.469, 5.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.378, 10.098], loss: 4.831920, mae: 1.184561, mean_q: 6.217912
 95255/100000: episode: 2480, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 195.496, mean reward: 1.955 [1.471, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.328, 10.221], loss: 3.516645, mae: 0.901097, mean_q: 5.779752
 95355/100000: episode: 2481, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.566, mean reward: 1.856 [1.467, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.665, 10.131], loss: 305.066620, mae: 1.755307, mean_q: 6.128355
 95455/100000: episode: 2482, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 201.266, mean reward: 2.013 [1.443, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.913, 10.204], loss: 2.771609, mae: 1.097087, mean_q: 6.050747
 95555/100000: episode: 2483, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 239.551, mean reward: 2.396 [1.465, 5.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.461, 10.544], loss: 157.172104, mae: 1.495676, mean_q: 5.961801
 95655/100000: episode: 2484, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 199.333, mean reward: 1.993 [1.464, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.122], loss: 154.295532, mae: 1.320040, mean_q: 5.880405
 95755/100000: episode: 2485, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.402, mean reward: 2.004 [1.514, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.731, 10.378], loss: 309.784637, mae: 1.769565, mean_q: 5.887403
 95855/100000: episode: 2486, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.177, mean reward: 1.862 [1.441, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.098], loss: 155.744720, mae: 1.772987, mean_q: 6.135872
 95955/100000: episode: 2487, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.821, mean reward: 1.918 [1.462, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.481, 10.118], loss: 157.147263, mae: 1.146322, mean_q: 5.347368
 96055/100000: episode: 2488, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 200.628, mean reward: 2.006 [1.491, 5.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.974, 10.175], loss: 307.418152, mae: 1.955627, mean_q: 5.935556
 96155/100000: episode: 2489, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.464, mean reward: 1.915 [1.461, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.066, 10.265], loss: 454.377045, mae: 2.532123, mean_q: 6.348224
 96255/100000: episode: 2490, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 212.238, mean reward: 2.122 [1.487, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.774, 10.098], loss: 307.579193, mae: 2.141434, mean_q: 6.337423
 96355/100000: episode: 2491, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.522, mean reward: 1.845 [1.440, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.508, 10.098], loss: 151.281540, mae: 1.468944, mean_q: 5.812245
 96455/100000: episode: 2492, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 186.871, mean reward: 1.869 [1.438, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.471, 10.330], loss: 300.191437, mae: 1.578016, mean_q: 5.533023
 96555/100000: episode: 2493, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 180.482, mean reward: 1.805 [1.465, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.264, 10.098], loss: 153.405258, mae: 1.418760, mean_q: 5.567107
 96655/100000: episode: 2494, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.394, mean reward: 1.824 [1.446, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.454, 10.124], loss: 10.427727, mae: 0.871226, mean_q: 4.975318
 96755/100000: episode: 2495, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 188.846, mean reward: 1.888 [1.477, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.849, 10.112], loss: 3.543013, mae: 0.632087, mean_q: 4.742406
 96855/100000: episode: 2496, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.503, mean reward: 1.875 [1.441, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.309, 10.153], loss: 2.773639, mae: 0.546106, mean_q: 4.425436
 96955/100000: episode: 2497, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 212.974, mean reward: 2.130 [1.490, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.929, 10.098], loss: 152.729446, mae: 0.997378, mean_q: 4.590914
 97055/100000: episode: 2498, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 197.813, mean reward: 1.978 [1.434, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.103, 10.098], loss: 0.510250, mae: 0.472954, mean_q: 4.176747
 97155/100000: episode: 2499, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 183.583, mean reward: 1.836 [1.451, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.554, 10.099], loss: 0.240687, mae: 0.404059, mean_q: 3.988557
 97255/100000: episode: 2500, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 197.920, mean reward: 1.979 [1.447, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.585, 10.098], loss: 0.136935, mae: 0.373623, mean_q: 3.879801
 97355/100000: episode: 2501, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 191.802, mean reward: 1.918 [1.453, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.789, 10.098], loss: 0.136635, mae: 0.368123, mean_q: 3.859120
 97455/100000: episode: 2502, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 188.653, mean reward: 1.887 [1.447, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.113], loss: 0.128819, mae: 0.369310, mean_q: 3.872472
 97555/100000: episode: 2503, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 205.166, mean reward: 2.052 [1.518, 5.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.844, 10.098], loss: 0.124286, mae: 0.369559, mean_q: 3.878822
 97655/100000: episode: 2504, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.132, mean reward: 2.001 [1.520, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.098], loss: 0.122326, mae: 0.365359, mean_q: 3.878878
 97755/100000: episode: 2505, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.034, mean reward: 1.890 [1.503, 2.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.519, 10.098], loss: 0.118551, mae: 0.346812, mean_q: 3.839784
 97855/100000: episode: 2506, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 199.755, mean reward: 1.998 [1.488, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.646, 10.191], loss: 0.126847, mae: 0.361802, mean_q: 3.851994
 97955/100000: episode: 2507, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 197.162, mean reward: 1.972 [1.470, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.542, 10.098], loss: 0.112220, mae: 0.345768, mean_q: 3.840659
 98055/100000: episode: 2508, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 190.054, mean reward: 1.901 [1.465, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.200], loss: 0.115223, mae: 0.345801, mean_q: 3.830983
 98155/100000: episode: 2509, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 194.556, mean reward: 1.946 [1.473, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.744, 10.156], loss: 0.113514, mae: 0.338011, mean_q: 3.823186
 98255/100000: episode: 2510, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.096, mean reward: 1.881 [1.490, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.693, 10.173], loss: 0.106536, mae: 0.338576, mean_q: 3.833756
 98355/100000: episode: 2511, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.939, mean reward: 1.959 [1.477, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.511, 10.208], loss: 0.117563, mae: 0.342484, mean_q: 3.816471
 98455/100000: episode: 2512, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.226, mean reward: 1.892 [1.486, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.525, 10.199], loss: 0.118384, mae: 0.345302, mean_q: 3.836416
 98555/100000: episode: 2513, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 217.349, mean reward: 2.173 [1.436, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.098], loss: 0.109943, mae: 0.338386, mean_q: 3.840120
 98655/100000: episode: 2514, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.369, mean reward: 1.874 [1.449, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.368, 10.098], loss: 0.107733, mae: 0.336816, mean_q: 3.843299
 98755/100000: episode: 2515, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.282, mean reward: 1.943 [1.453, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.412, 10.098], loss: 0.111916, mae: 0.332659, mean_q: 3.822697
 98855/100000: episode: 2516, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.599, mean reward: 1.906 [1.441, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.158, 10.251], loss: 0.115790, mae: 0.342374, mean_q: 3.852686
 98955/100000: episode: 2517, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 191.713, mean reward: 1.917 [1.475, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.098], loss: 0.112906, mae: 0.342079, mean_q: 3.854483
 99055/100000: episode: 2518, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.903, mean reward: 1.869 [1.443, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.079, 10.106], loss: 0.115123, mae: 0.337590, mean_q: 3.834383
 99155/100000: episode: 2519, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.335, mean reward: 1.933 [1.463, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.221, 10.364], loss: 0.116250, mae: 0.335997, mean_q: 3.855967
 99255/100000: episode: 2520, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.386, mean reward: 1.884 [1.471, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.392, 10.098], loss: 0.114841, mae: 0.328918, mean_q: 3.857078
 99355/100000: episode: 2521, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.844, mean reward: 1.858 [1.442, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.195, 10.235], loss: 0.106695, mae: 0.322603, mean_q: 3.830151
 99455/100000: episode: 2522, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 206.372, mean reward: 2.064 [1.470, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.548, 10.158], loss: 0.104159, mae: 0.327502, mean_q: 3.829449
 99555/100000: episode: 2523, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 185.743, mean reward: 1.857 [1.437, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.365, 10.159], loss: 0.110812, mae: 0.332388, mean_q: 3.840292
 99655/100000: episode: 2524, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 223.736, mean reward: 2.237 [1.438, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.503, 10.522], loss: 0.102969, mae: 0.317647, mean_q: 3.847645
 99755/100000: episode: 2525, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 209.616, mean reward: 2.096 [1.468, 5.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.558, 10.101], loss: 0.104404, mae: 0.318640, mean_q: 3.844803
 99855/100000: episode: 2526, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 199.669, mean reward: 1.997 [1.429, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.350, 10.341], loss: 0.115081, mae: 0.337103, mean_q: 3.877487
 99955/100000: episode: 2527, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 191.416, mean reward: 1.914 [1.432, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.098], loss: 0.116490, mae: 0.337698, mean_q: 3.867759
done, took 668.057 seconds
[Info] End Importance Splitting. Falsification occurred 15 times.
