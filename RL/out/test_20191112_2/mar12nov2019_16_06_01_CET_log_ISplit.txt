Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.168s, episode steps: 100, steps per second: 597, episode reward: 190.018, mean reward: 1.900 [1.472, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 187.671, mean reward: 1.877 [1.463, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 191.754, mean reward: 1.918 [1.467, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.221], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 200.636, mean reward: 2.006 [1.471, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.293], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 177.800, mean reward: 1.778 [1.435, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.046, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.076s, episode steps: 100, steps per second: 1322, episode reward: 219.414, mean reward: 2.194 [1.462, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.875, 10.369], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.087s, episode steps: 100, steps per second: 1148, episode reward: 200.201, mean reward: 2.002 [1.532, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.747, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.086s, episode steps: 100, steps per second: 1169, episode reward: 198.060, mean reward: 1.981 [1.483, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.811, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.091s, episode steps: 100, steps per second: 1104, episode reward: 187.275, mean reward: 1.873 [1.446, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.026, 10.306], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.069s, episode steps: 100, steps per second: 1453, episode reward: 183.752, mean reward: 1.838 [1.450, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.814, 10.111], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 199.009, mean reward: 1.990 [1.491, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.616, 10.136], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.069s, episode steps: 100, steps per second: 1457, episode reward: 181.999, mean reward: 1.820 [1.461, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.091s, episode steps: 100, steps per second: 1104, episode reward: 184.377, mean reward: 1.844 [1.464, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.469, 10.120], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.088s, episode steps: 100, steps per second: 1131, episode reward: 203.948, mean reward: 2.039 [1.516, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.310, 10.274], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.086s, episode steps: 100, steps per second: 1166, episode reward: 188.854, mean reward: 1.889 [1.456, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.965, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.074s, episode steps: 100, steps per second: 1357, episode reward: 185.356, mean reward: 1.854 [1.453, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.992, 10.116], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 199.723, mean reward: 1.997 [1.468, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.127, 10.494], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 189.546, mean reward: 1.895 [1.472, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.371, 10.381], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.067s, episode steps: 100, steps per second: 1489, episode reward: 199.646, mean reward: 1.996 [1.446, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.781, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.082s, episode steps: 100, steps per second: 1218, episode reward: 196.706, mean reward: 1.967 [1.471, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.393, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.102s, episode steps: 100, steps per second: 982, episode reward: 197.896, mean reward: 1.979 [1.465, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.998, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.085s, episode steps: 100, steps per second: 1179, episode reward: 191.255, mean reward: 1.913 [1.463, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.205, 10.395], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.075s, episode steps: 100, steps per second: 1325, episode reward: 215.561, mean reward: 2.156 [1.491, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.842, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.070s, episode steps: 100, steps per second: 1435, episode reward: 190.886, mean reward: 1.909 [1.467, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.727, 10.258], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.067s, episode steps: 100, steps per second: 1495, episode reward: 187.784, mean reward: 1.878 [1.477, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.125, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.063s, episode steps: 100, steps per second: 1592, episode reward: 189.476, mean reward: 1.895 [1.455, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.109], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 180.137, mean reward: 1.801 [1.456, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.601, 10.186], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.076s, episode steps: 100, steps per second: 1321, episode reward: 214.730, mean reward: 2.147 [1.480, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.412, 10.303], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.063s, episode steps: 100, steps per second: 1581, episode reward: 253.305, mean reward: 2.533 [1.482, 7.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.923, 10.163], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.063s, episode steps: 100, steps per second: 1592, episode reward: 192.846, mean reward: 1.928 [1.437, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.227, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.084s, episode steps: 100, steps per second: 1184, episode reward: 209.577, mean reward: 2.096 [1.487, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.317, 10.341], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.076s, episode steps: 100, steps per second: 1312, episode reward: 185.439, mean reward: 1.854 [1.440, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.595, 10.185], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 191.841, mean reward: 1.918 [1.445, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.063s, episode steps: 100, steps per second: 1586, episode reward: 201.122, mean reward: 2.011 [1.498, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.358, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 181.938, mean reward: 1.819 [1.488, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.165], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 194.242, mean reward: 1.942 [1.449, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.222, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 198.432, mean reward: 1.984 [1.459, 4.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.417, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.070s, episode steps: 100, steps per second: 1429, episode reward: 180.363, mean reward: 1.804 [1.453, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.630, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 200.959, mean reward: 2.010 [1.449, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.293], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.063s, episode steps: 100, steps per second: 1589, episode reward: 186.757, mean reward: 1.868 [1.460, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.192], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 199.160, mean reward: 1.992 [1.447, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.566, 10.262], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 193.081, mean reward: 1.931 [1.498, 5.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 201.802, mean reward: 2.018 [1.478, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.246, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.063s, episode steps: 100, steps per second: 1598, episode reward: 186.946, mean reward: 1.869 [1.475, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.862, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: 182.995, mean reward: 1.830 [1.436, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.994, 10.325], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.072s, episode steps: 100, steps per second: 1382, episode reward: 199.673, mean reward: 1.997 [1.446, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.562, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 204.476, mean reward: 2.045 [1.503, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.331, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.063s, episode steps: 100, steps per second: 1596, episode reward: 204.703, mean reward: 2.047 [1.433, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.595, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 220.505, mean reward: 2.205 [1.504, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.728, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.063s, episode steps: 100, steps per second: 1587, episode reward: 186.100, mean reward: 1.861 [1.477, 7.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.530, 10.225], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 189.416, mean reward: 1.894 [1.454, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.660, 10.098], loss: 0.266205, mae: 0.518852, mean_q: 2.308701
  5200/100000: episode: 52, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.049, mean reward: 1.930 [1.471, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.211, 10.098], loss: 0.111601, mae: 0.321282, mean_q: 2.949504
  5300/100000: episode: 53, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 189.544, mean reward: 1.895 [1.457, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.095, 10.126], loss: 0.104860, mae: 0.314917, mean_q: 3.266710
  5400/100000: episode: 54, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 203.627, mean reward: 2.036 [1.473, 4.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.340, 10.098], loss: 0.113916, mae: 0.316184, mean_q: 3.491302
  5500/100000: episode: 55, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 187.027, mean reward: 1.870 [1.472, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.416, 10.237], loss: 0.120936, mae: 0.323487, mean_q: 3.649700
  5600/100000: episode: 56, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 206.760, mean reward: 2.068 [1.520, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.654, 10.098], loss: 0.139505, mae: 0.325566, mean_q: 3.730286
  5700/100000: episode: 57, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 185.226, mean reward: 1.852 [1.491, 2.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.930, 10.098], loss: 0.108443, mae: 0.314866, mean_q: 3.784052
  5800/100000: episode: 58, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 195.419, mean reward: 1.954 [1.444, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.717, 10.098], loss: 0.118818, mae: 0.315771, mean_q: 3.818456
  5900/100000: episode: 59, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.573, mean reward: 1.996 [1.494, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.446, 10.136], loss: 0.098348, mae: 0.308152, mean_q: 3.833071
  6000/100000: episode: 60, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 195.586, mean reward: 1.956 [1.493, 5.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.946, 10.098], loss: 0.112550, mae: 0.322184, mean_q: 3.840677
  6100/100000: episode: 61, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 188.402, mean reward: 1.884 [1.460, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.856, 10.098], loss: 0.105341, mae: 0.312720, mean_q: 3.845792
  6200/100000: episode: 62, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 199.869, mean reward: 1.999 [1.455, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.110, 10.098], loss: 0.108174, mae: 0.310411, mean_q: 3.865030
  6300/100000: episode: 63, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 185.022, mean reward: 1.850 [1.444, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.967, 10.098], loss: 0.097933, mae: 0.297530, mean_q: 3.858563
  6400/100000: episode: 64, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 188.653, mean reward: 1.887 [1.482, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.844, 10.226], loss: 0.116664, mae: 0.319150, mean_q: 3.868974
  6500/100000: episode: 65, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 191.417, mean reward: 1.914 [1.448, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.098], loss: 0.105081, mae: 0.312552, mean_q: 3.891192
  6600/100000: episode: 66, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 202.062, mean reward: 2.021 [1.463, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.103, 10.098], loss: 0.127950, mae: 0.325869, mean_q: 3.898810
  6700/100000: episode: 67, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 186.627, mean reward: 1.866 [1.491, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.334, 10.207], loss: 0.103002, mae: 0.310490, mean_q: 3.874502
  6800/100000: episode: 68, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 213.904, mean reward: 2.139 [1.461, 5.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.961, 10.344], loss: 0.117350, mae: 0.316591, mean_q: 3.877043
  6900/100000: episode: 69, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 192.038, mean reward: 1.920 [1.478, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.098], loss: 0.106952, mae: 0.315836, mean_q: 3.886743
  7000/100000: episode: 70, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 185.704, mean reward: 1.857 [1.446, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.490, 10.098], loss: 0.124062, mae: 0.325005, mean_q: 3.898368
  7100/100000: episode: 71, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 219.714, mean reward: 2.197 [1.491, 5.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.451], loss: 0.110691, mae: 0.315930, mean_q: 3.875557
  7200/100000: episode: 72, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 190.842, mean reward: 1.908 [1.444, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.098], loss: 0.121465, mae: 0.330338, mean_q: 3.905227
  7300/100000: episode: 73, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 186.250, mean reward: 1.863 [1.448, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.803, 10.098], loss: 0.114591, mae: 0.318325, mean_q: 3.904890
  7400/100000: episode: 74, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 203.807, mean reward: 2.038 [1.490, 5.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.063, 10.098], loss: 0.105425, mae: 0.311108, mean_q: 3.883201
  7500/100000: episode: 75, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 190.809, mean reward: 1.908 [1.476, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.281], loss: 0.105026, mae: 0.308817, mean_q: 3.886669
  7600/100000: episode: 76, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 189.351, mean reward: 1.894 [1.459, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.263, 10.204], loss: 0.111233, mae: 0.316385, mean_q: 3.891597
  7700/100000: episode: 77, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 214.750, mean reward: 2.147 [1.450, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.563, 10.111], loss: 0.121100, mae: 0.327570, mean_q: 3.914713
  7800/100000: episode: 78, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 196.377, mean reward: 1.964 [1.468, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.351, 10.098], loss: 0.112538, mae: 0.324504, mean_q: 3.888955
  7900/100000: episode: 79, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 191.676, mean reward: 1.917 [1.451, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.369, 10.098], loss: 0.130357, mae: 0.321696, mean_q: 3.874963
  8000/100000: episode: 80, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 230.957, mean reward: 2.310 [1.552, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.098], loss: 0.134988, mae: 0.333253, mean_q: 3.884406
  8100/100000: episode: 81, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 194.939, mean reward: 1.949 [1.459, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.250, 10.307], loss: 0.100705, mae: 0.304422, mean_q: 3.863732
  8200/100000: episode: 82, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 198.300, mean reward: 1.983 [1.502, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.897, 10.098], loss: 0.105650, mae: 0.313448, mean_q: 3.859902
  8300/100000: episode: 83, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 196.837, mean reward: 1.968 [1.453, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.301], loss: 0.101347, mae: 0.303044, mean_q: 3.878642
  8400/100000: episode: 84, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 198.238, mean reward: 1.982 [1.467, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.256], loss: 0.110677, mae: 0.323280, mean_q: 3.880788
  8500/100000: episode: 85, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 191.968, mean reward: 1.920 [1.485, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.620, 10.098], loss: 0.114109, mae: 0.318452, mean_q: 3.891768
  8600/100000: episode: 86, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.607, mean reward: 1.866 [1.457, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.599, 10.098], loss: 0.104202, mae: 0.310191, mean_q: 3.880576
  8700/100000: episode: 87, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 181.919, mean reward: 1.819 [1.441, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.681, 10.098], loss: 0.125392, mae: 0.327110, mean_q: 3.892025
  8800/100000: episode: 88, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 202.516, mean reward: 2.025 [1.472, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.693, 10.118], loss: 0.111791, mae: 0.315574, mean_q: 3.877481
  8900/100000: episode: 89, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 204.966, mean reward: 2.050 [1.492, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.027, 10.098], loss: 0.116857, mae: 0.322748, mean_q: 3.873962
  9000/100000: episode: 90, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 195.885, mean reward: 1.959 [1.500, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.098], loss: 0.093016, mae: 0.302552, mean_q: 3.879824
  9100/100000: episode: 91, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 177.541, mean reward: 1.775 [1.458, 2.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.143], loss: 0.105172, mae: 0.308047, mean_q: 3.881738
  9200/100000: episode: 92, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 213.025, mean reward: 2.130 [1.476, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.809, 10.220], loss: 0.111264, mae: 0.315793, mean_q: 3.886997
  9300/100000: episode: 93, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 207.568, mean reward: 2.076 [1.485, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.221, 10.299], loss: 0.113475, mae: 0.321149, mean_q: 3.880329
  9400/100000: episode: 94, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.593, mean reward: 1.896 [1.468, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.295, 10.098], loss: 0.095885, mae: 0.307067, mean_q: 3.878097
  9500/100000: episode: 95, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 203.617, mean reward: 2.036 [1.491, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.321, 10.212], loss: 0.101120, mae: 0.310562, mean_q: 3.902273
  9600/100000: episode: 96, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 191.284, mean reward: 1.913 [1.458, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.440, 10.198], loss: 0.099258, mae: 0.301569, mean_q: 3.901598
  9700/100000: episode: 97, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 236.860, mean reward: 2.369 [1.434, 5.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.706, 10.098], loss: 0.114495, mae: 0.318294, mean_q: 3.894974
  9800/100000: episode: 98, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.054, mean reward: 1.971 [1.447, 5.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.098], loss: 0.101094, mae: 0.308211, mean_q: 3.885522
  9900/100000: episode: 99, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 205.014, mean reward: 2.050 [1.448, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.457, 10.447], loss: 0.110447, mae: 0.317632, mean_q: 3.889880
 10000/100000: episode: 100, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 228.920, mean reward: 2.289 [1.488, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.192, 10.098], loss: 0.125091, mae: 0.333438, mean_q: 3.916949
 10100/100000: episode: 101, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 188.822, mean reward: 1.888 [1.494, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.765, 10.098], loss: 0.112605, mae: 0.320695, mean_q: 3.904338
 10200/100000: episode: 102, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 204.892, mean reward: 2.049 [1.481, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.695, 10.098], loss: 0.112629, mae: 0.329603, mean_q: 3.915953
 10300/100000: episode: 103, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 181.156, mean reward: 1.812 [1.455, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.400, 10.164], loss: 0.107318, mae: 0.316026, mean_q: 3.910631
 10400/100000: episode: 104, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 189.721, mean reward: 1.897 [1.438, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.173, 10.306], loss: 0.094545, mae: 0.301581, mean_q: 3.910737
 10500/100000: episode: 105, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 182.458, mean reward: 1.825 [1.475, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.201], loss: 0.113850, mae: 0.322878, mean_q: 3.924226
 10600/100000: episode: 106, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 195.110, mean reward: 1.951 [1.477, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: 0.115641, mae: 0.322979, mean_q: 3.911129
 10700/100000: episode: 107, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 187.220, mean reward: 1.872 [1.451, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.345, 10.129], loss: 0.101374, mae: 0.311116, mean_q: 3.903702
 10800/100000: episode: 108, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 196.588, mean reward: 1.966 [1.473, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.336], loss: 0.107397, mae: 0.315149, mean_q: 3.915544
 10900/100000: episode: 109, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 231.003, mean reward: 2.310 [1.556, 13.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.246, 10.303], loss: 0.132921, mae: 0.322605, mean_q: 3.912243
 11000/100000: episode: 110, duration: 0.656s, episode steps: 100, steps per second: 153, episode reward: 199.763, mean reward: 1.998 [1.432, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.430, 10.098], loss: 0.120754, mae: 0.330327, mean_q: 3.921997
 11100/100000: episode: 111, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 184.522, mean reward: 1.845 [1.446, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.282, 10.098], loss: 0.121492, mae: 0.326025, mean_q: 3.915421
 11200/100000: episode: 112, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: 181.804, mean reward: 1.818 [1.482, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.025, 10.166], loss: 0.105851, mae: 0.317722, mean_q: 3.906156
 11300/100000: episode: 113, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 209.229, mean reward: 2.092 [1.459, 6.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.613, 10.283], loss: 0.107379, mae: 0.322607, mean_q: 3.912246
 11400/100000: episode: 114, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 194.191, mean reward: 1.942 [1.441, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.909, 10.098], loss: 0.121334, mae: 0.319264, mean_q: 3.907549
 11500/100000: episode: 115, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 195.127, mean reward: 1.951 [1.440, 5.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.280, 10.098], loss: 0.128762, mae: 0.326060, mean_q: 3.942628
 11600/100000: episode: 116, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 193.322, mean reward: 1.933 [1.454, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.042, 10.098], loss: 0.107041, mae: 0.311800, mean_q: 3.919675
 11700/100000: episode: 117, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 191.934, mean reward: 1.919 [1.494, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.206, 10.098], loss: 0.131936, mae: 0.327316, mean_q: 3.919442
 11800/100000: episode: 118, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 189.773, mean reward: 1.898 [1.445, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.410, 10.174], loss: 0.139003, mae: 0.328844, mean_q: 3.938455
 11900/100000: episode: 119, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 181.931, mean reward: 1.819 [1.461, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.797, 10.214], loss: 0.114939, mae: 0.316449, mean_q: 3.910545
 12000/100000: episode: 120, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 199.987, mean reward: 2.000 [1.447, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.242, 10.237], loss: 0.132930, mae: 0.325729, mean_q: 3.896928
 12100/100000: episode: 121, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 188.006, mean reward: 1.880 [1.438, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.380, 10.098], loss: 0.101416, mae: 0.300454, mean_q: 3.892360
 12200/100000: episode: 122, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 194.869, mean reward: 1.949 [1.457, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.547, 10.222], loss: 0.144649, mae: 0.326124, mean_q: 3.909026
 12300/100000: episode: 123, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 203.427, mean reward: 2.034 [1.507, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.213, 10.098], loss: 0.131632, mae: 0.320756, mean_q: 3.906679
 12400/100000: episode: 124, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 187.301, mean reward: 1.873 [1.451, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.258], loss: 0.129137, mae: 0.324039, mean_q: 3.908732
 12500/100000: episode: 125, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.875, mean reward: 1.969 [1.517, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.191, 10.098], loss: 0.102847, mae: 0.310293, mean_q: 3.899917
 12600/100000: episode: 126, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 242.564, mean reward: 2.426 [1.566, 6.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.389, 10.098], loss: 0.121431, mae: 0.334142, mean_q: 3.921403
 12700/100000: episode: 127, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 187.743, mean reward: 1.877 [1.468, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.505, 10.098], loss: 0.125507, mae: 0.321213, mean_q: 3.922859
 12800/100000: episode: 128, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 192.849, mean reward: 1.928 [1.449, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.518, 10.098], loss: 0.106770, mae: 0.320518, mean_q: 3.923392
 12900/100000: episode: 129, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 193.403, mean reward: 1.934 [1.473, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.584, 10.189], loss: 0.115260, mae: 0.325541, mean_q: 3.937010
 13000/100000: episode: 130, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: 182.692, mean reward: 1.827 [1.436, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.349, 10.151], loss: 0.101369, mae: 0.305503, mean_q: 3.909592
 13100/100000: episode: 131, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 191.465, mean reward: 1.915 [1.465, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.936, 10.133], loss: 0.137315, mae: 0.322429, mean_q: 3.912078
 13200/100000: episode: 132, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 221.418, mean reward: 2.214 [1.527, 7.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.509, 10.144], loss: 0.113361, mae: 0.313000, mean_q: 3.902464
 13300/100000: episode: 133, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 199.478, mean reward: 1.995 [1.474, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.551, 10.098], loss: 0.104968, mae: 0.312072, mean_q: 3.918903
 13400/100000: episode: 134, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 183.675, mean reward: 1.837 [1.512, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.165, 10.242], loss: 0.093322, mae: 0.296968, mean_q: 3.907477
 13500/100000: episode: 135, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 199.661, mean reward: 1.997 [1.462, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.144, 10.104], loss: 0.107937, mae: 0.310384, mean_q: 3.899900
 13600/100000: episode: 136, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 186.720, mean reward: 1.867 [1.482, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.237, 10.137], loss: 0.144659, mae: 0.322249, mean_q: 3.912566
 13700/100000: episode: 137, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 213.902, mean reward: 2.139 [1.518, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.842, 10.098], loss: 0.120281, mae: 0.324912, mean_q: 3.916372
 13800/100000: episode: 138, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 188.870, mean reward: 1.889 [1.452, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.195], loss: 0.141924, mae: 0.330754, mean_q: 3.925387
 13900/100000: episode: 139, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 195.538, mean reward: 1.955 [1.467, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.668, 10.175], loss: 0.150583, mae: 0.327211, mean_q: 3.896179
 14000/100000: episode: 140, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 203.688, mean reward: 2.037 [1.481, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.248, 10.176], loss: 0.123033, mae: 0.311298, mean_q: 3.889227
 14100/100000: episode: 141, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 193.566, mean reward: 1.936 [1.460, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.825, 10.368], loss: 0.107571, mae: 0.310574, mean_q: 3.907772
 14200/100000: episode: 142, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 208.659, mean reward: 2.087 [1.442, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.405, 10.098], loss: 0.106407, mae: 0.306832, mean_q: 3.908693
 14300/100000: episode: 143, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 254.362, mean reward: 2.544 [1.464, 5.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.674, 10.416], loss: 0.121462, mae: 0.325521, mean_q: 3.909398
 14400/100000: episode: 144, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 179.413, mean reward: 1.794 [1.445, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.453, 10.098], loss: 0.162553, mae: 0.337306, mean_q: 3.921004
 14500/100000: episode: 145, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 206.098, mean reward: 2.061 [1.498, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.487, 10.098], loss: 0.109754, mae: 0.316071, mean_q: 3.912095
 14600/100000: episode: 146, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 189.986, mean reward: 1.900 [1.465, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.406, 10.098], loss: 0.145727, mae: 0.335788, mean_q: 3.927752
 14700/100000: episode: 147, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 192.738, mean reward: 1.927 [1.496, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.917, 10.270], loss: 0.117008, mae: 0.325683, mean_q: 3.924590
 14800/100000: episode: 148, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 181.986, mean reward: 1.820 [1.454, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.425, 10.119], loss: 0.119718, mae: 0.326771, mean_q: 3.917336
 14900/100000: episode: 149, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 189.621, mean reward: 1.896 [1.445, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.936, 10.098], loss: 0.103329, mae: 0.308905, mean_q: 3.884463
[Info] 1-TH LEVEL FOUND: 4.764358043670654, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.394s, episode steps: 100, steps per second: 19, episode reward: 188.525, mean reward: 1.885 [1.456, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.511, 10.219], loss: 0.108410, mae: 0.310540, mean_q: 3.875212
 15025/100000: episode: 151, duration: 0.157s, episode steps: 25, steps per second: 160, episode reward: 58.762, mean reward: 2.350 [1.939, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.504, 10.323], loss: 0.103931, mae: 0.320057, mean_q: 3.926024
 15069/100000: episode: 152, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 85.660, mean reward: 1.947 [1.497, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.748, 10.110], loss: 0.284547, mae: 0.351747, mean_q: 3.916178
 15097/100000: episode: 153, duration: 0.167s, episode steps: 28, steps per second: 167, episode reward: 66.924, mean reward: 2.390 [1.704, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.865, 10.210], loss: 0.116733, mae: 0.328193, mean_q: 3.902501
 15123/100000: episode: 154, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 58.093, mean reward: 2.234 [1.783, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.366, 10.327], loss: 0.100051, mae: 0.308333, mean_q: 3.890157
 15155/100000: episode: 155, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 80.347, mean reward: 2.511 [1.928, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.220, 10.354], loss: 0.100181, mae: 0.299517, mean_q: 3.873729
 15180/100000: episode: 156, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 58.111, mean reward: 2.324 [1.693, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.739, 10.291], loss: 0.117902, mae: 0.322156, mean_q: 3.913087
 15219/100000: episode: 157, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 73.875, mean reward: 1.894 [1.456, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.589, 10.100], loss: 0.159715, mae: 0.339514, mean_q: 3.916010
 15241/100000: episode: 158, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 59.075, mean reward: 2.685 [1.806, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.343], loss: 0.106408, mae: 0.316998, mean_q: 3.894183
 15280/100000: episode: 159, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 90.056, mean reward: 2.309 [1.783, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.617, 10.249], loss: 0.097319, mae: 0.310574, mean_q: 3.904843
 15378/100000: episode: 160, duration: 0.657s, episode steps: 98, steps per second: 149, episode reward: 203.530, mean reward: 2.077 [1.527, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-1.618, 10.253], loss: 0.112750, mae: 0.319332, mean_q: 3.924559
 15476/100000: episode: 161, duration: 0.550s, episode steps: 98, steps per second: 178, episode reward: 196.666, mean reward: 2.007 [1.452, 3.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.985, 10.329], loss: 0.109018, mae: 0.318158, mean_q: 3.945511
 15508/100000: episode: 162, duration: 0.189s, episode steps: 32, steps per second: 170, episode reward: 85.548, mean reward: 2.673 [1.878, 5.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.847, 10.281], loss: 0.089372, mae: 0.292322, mean_q: 3.921819
 15552/100000: episode: 163, duration: 0.250s, episode steps: 44, steps per second: 176, episode reward: 136.594, mean reward: 3.104 [2.341, 6.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.134, 10.432], loss: 0.172843, mae: 0.350415, mean_q: 3.948339
 15650/100000: episode: 164, duration: 0.509s, episode steps: 98, steps per second: 192, episode reward: 179.954, mean reward: 1.836 [1.445, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.277, 10.199], loss: 0.147397, mae: 0.328523, mean_q: 3.936762
 15689/100000: episode: 165, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 93.359, mean reward: 2.394 [1.946, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.072, 10.346], loss: 0.105044, mae: 0.322209, mean_q: 3.927562
 15728/100000: episode: 166, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 97.519, mean reward: 2.500 [1.896, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.035, 10.320], loss: 0.112622, mae: 0.315042, mean_q: 3.958418
 15750/100000: episode: 167, duration: 0.104s, episode steps: 22, steps per second: 212, episode reward: 45.046, mean reward: 2.048 [1.652, 2.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.325, 10.329], loss: 0.192702, mae: 0.336538, mean_q: 3.963913
 15772/100000: episode: 168, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 56.987, mean reward: 2.590 [1.998, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.319], loss: 0.098928, mae: 0.319106, mean_q: 3.960140
 15804/100000: episode: 169, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 64.347, mean reward: 2.011 [1.667, 2.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.066, 10.326], loss: 0.137168, mae: 0.342944, mean_q: 3.976580
 15830/100000: episode: 170, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 53.628, mean reward: 2.063 [1.724, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.191, 10.283], loss: 0.096401, mae: 0.311356, mean_q: 3.958325
 15869/100000: episode: 171, duration: 0.237s, episode steps: 39, steps per second: 165, episode reward: 100.883, mean reward: 2.587 [1.754, 5.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.863, 10.256], loss: 0.117146, mae: 0.332643, mean_q: 3.977190
 15908/100000: episode: 172, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 72.163, mean reward: 1.850 [1.495, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.425, 10.131], loss: 0.087855, mae: 0.292861, mean_q: 3.962712
 15933/100000: episode: 173, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 81.034, mean reward: 3.241 [2.499, 5.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.982, 10.514], loss: 0.116661, mae: 0.307232, mean_q: 3.959934
 15959/100000: episode: 174, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 107.704, mean reward: 4.142 [1.733, 5.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.131, 10.531], loss: 0.092123, mae: 0.300870, mean_q: 3.942797
 15988/100000: episode: 175, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 82.130, mean reward: 2.832 [1.893, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.904, 10.394], loss: 0.142003, mae: 0.346379, mean_q: 3.952281
 16086/100000: episode: 176, duration: 0.486s, episode steps: 98, steps per second: 202, episode reward: 189.212, mean reward: 1.931 [1.458, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-1.237, 10.150], loss: 0.127803, mae: 0.341183, mean_q: 4.001687
 16114/100000: episode: 177, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 69.397, mean reward: 2.478 [1.982, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.657, 10.389], loss: 0.098322, mae: 0.308143, mean_q: 3.982981
 16212/100000: episode: 178, duration: 0.494s, episode steps: 98, steps per second: 199, episode reward: 179.798, mean reward: 1.835 [1.479, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.286, 10.276], loss: 0.124923, mae: 0.334084, mean_q: 4.027233
 16251/100000: episode: 179, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 91.935, mean reward: 2.357 [1.743, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.132, 10.361], loss: 0.143316, mae: 0.360103, mean_q: 4.054427
 16290/100000: episode: 180, duration: 0.189s, episode steps: 39, steps per second: 207, episode reward: 100.011, mean reward: 2.564 [1.877, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.608, 10.461], loss: 0.127118, mae: 0.340761, mean_q: 4.007075
 16388/100000: episode: 181, duration: 0.477s, episode steps: 98, steps per second: 205, episode reward: 193.010, mean reward: 1.969 [1.454, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.831, 10.376], loss: 0.137659, mae: 0.348705, mean_q: 4.016213
 16427/100000: episode: 182, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 106.715, mean reward: 2.736 [2.132, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.067, 10.398], loss: 0.143348, mae: 0.351956, mean_q: 4.033488
 16452/100000: episode: 183, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 73.106, mean reward: 2.924 [2.272, 5.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.414, 10.394], loss: 0.132993, mae: 0.344214, mean_q: 4.013980
 16491/100000: episode: 184, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 77.695, mean reward: 1.992 [1.534, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.770, 10.127], loss: 0.115371, mae: 0.322074, mean_q: 4.015251
 16535/100000: episode: 185, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 133.657, mean reward: 3.038 [2.094, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.129, 10.386], loss: 0.122401, mae: 0.344841, mean_q: 4.036217
 16560/100000: episode: 186, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 85.957, mean reward: 3.438 [2.390, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.036, 10.492], loss: 0.145223, mae: 0.357032, mean_q: 4.073390
 16582/100000: episode: 187, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 40.608, mean reward: 1.846 [1.455, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.533, 10.100], loss: 0.140312, mae: 0.357220, mean_q: 4.084126
 16604/100000: episode: 188, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 57.047, mean reward: 2.593 [2.086, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.526, 10.395], loss: 0.121781, mae: 0.336135, mean_q: 4.093387
 16648/100000: episode: 189, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 103.630, mean reward: 2.355 [1.550, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.366, 10.392], loss: 0.158727, mae: 0.365420, mean_q: 4.075052
 16687/100000: episode: 190, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 93.839, mean reward: 2.406 [1.543, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.595, 10.333], loss: 0.132746, mae: 0.349494, mean_q: 4.112552
 16712/100000: episode: 191, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 82.000, mean reward: 3.280 [2.267, 5.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.882, 10.295], loss: 0.134425, mae: 0.351342, mean_q: 4.102521
 16741/100000: episode: 192, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 76.059, mean reward: 2.623 [1.960, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.470], loss: 0.135644, mae: 0.339277, mean_q: 4.110203
 16769/100000: episode: 193, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 76.250, mean reward: 2.723 [2.098, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.670, 10.412], loss: 0.128757, mae: 0.358324, mean_q: 4.135561
 16813/100000: episode: 194, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 98.016, mean reward: 2.228 [1.600, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.147, 10.191], loss: 0.123805, mae: 0.343844, mean_q: 4.131201
 16857/100000: episode: 195, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 95.813, mean reward: 2.178 [1.507, 4.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.130, 10.104], loss: 0.126205, mae: 0.343022, mean_q: 4.144178
 16885/100000: episode: 196, duration: 0.138s, episode steps: 28, steps per second: 202, episode reward: 72.566, mean reward: 2.592 [1.929, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.315], loss: 0.128099, mae: 0.354319, mean_q: 4.131534
 16917/100000: episode: 197, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 94.233, mean reward: 2.945 [2.155, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.808, 10.452], loss: 0.133287, mae: 0.359598, mean_q: 4.164656
 16946/100000: episode: 198, duration: 0.138s, episode steps: 29, steps per second: 210, episode reward: 123.664, mean reward: 4.264 [2.250, 9.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.448, 10.518], loss: 0.180524, mae: 0.381104, mean_q: 4.150300
 16990/100000: episode: 199, duration: 0.237s, episode steps: 44, steps per second: 185, episode reward: 96.481, mean reward: 2.193 [1.523, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.390, 10.100], loss: 0.118616, mae: 0.347068, mean_q: 4.192108
 17016/100000: episode: 200, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 60.565, mean reward: 2.329 [1.791, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.084, 10.349], loss: 0.168127, mae: 0.361260, mean_q: 4.201680
 17041/100000: episode: 201, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 52.693, mean reward: 2.108 [1.545, 2.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.186], loss: 0.163960, mae: 0.387221, mean_q: 4.136130
 17073/100000: episode: 202, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 86.174, mean reward: 2.693 [2.208, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.656, 10.299], loss: 0.138276, mae: 0.369159, mean_q: 4.225955
 17101/100000: episode: 203, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 66.973, mean reward: 2.392 [1.723, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.037, 10.161], loss: 0.139756, mae: 0.362247, mean_q: 4.199536
 17130/100000: episode: 204, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 75.377, mean reward: 2.599 [1.762, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.282, 10.230], loss: 0.128069, mae: 0.356324, mean_q: 4.180323
 17152/100000: episode: 205, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 54.276, mean reward: 2.467 [2.067, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.390], loss: 0.163753, mae: 0.376998, mean_q: 4.254868
 17177/100000: episode: 206, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 49.290, mean reward: 1.972 [1.459, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.100], loss: 0.133726, mae: 0.351852, mean_q: 4.205160
 17202/100000: episode: 207, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 60.492, mean reward: 2.420 [1.679, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.069, 10.171], loss: 0.131214, mae: 0.359348, mean_q: 4.225418
 17241/100000: episode: 208, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 105.441, mean reward: 2.704 [2.042, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.222, 10.232], loss: 0.137481, mae: 0.361543, mean_q: 4.191851
 17263/100000: episode: 209, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 55.915, mean reward: 2.542 [1.997, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.904, 10.392], loss: 0.165098, mae: 0.388677, mean_q: 4.286979
 17292/100000: episode: 210, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 90.598, mean reward: 3.124 [2.251, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.403, 10.515], loss: 0.153522, mae: 0.368767, mean_q: 4.263481
 17317/100000: episode: 211, duration: 0.156s, episode steps: 25, steps per second: 161, episode reward: 59.254, mean reward: 2.370 [1.509, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.045, 10.336], loss: 0.166405, mae: 0.363953, mean_q: 4.273568
 17361/100000: episode: 212, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 107.928, mean reward: 2.453 [1.697, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.177, 10.217], loss: 0.143861, mae: 0.376412, mean_q: 4.255725
 17405/100000: episode: 213, duration: 0.207s, episode steps: 44, steps per second: 213, episode reward: 100.047, mean reward: 2.274 [1.673, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.853, 10.427], loss: 0.155823, mae: 0.393668, mean_q: 4.294435
 17427/100000: episode: 214, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 58.603, mean reward: 2.664 [2.232, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.898, 10.447], loss: 0.144763, mae: 0.375117, mean_q: 4.262848
 17466/100000: episode: 215, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 127.728, mean reward: 3.275 [2.240, 6.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.320, 10.509], loss: 0.156773, mae: 0.381579, mean_q: 4.308946
 17488/100000: episode: 216, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 51.676, mean reward: 2.349 [1.981, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.955, 10.368], loss: 0.162005, mae: 0.389909, mean_q: 4.288875
 17532/100000: episode: 217, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 109.332, mean reward: 2.485 [1.507, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.605, 10.166], loss: 0.140189, mae: 0.356010, mean_q: 4.258363
 17564/100000: episode: 218, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 85.430, mean reward: 2.670 [1.701, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.279, 10.221], loss: 0.113838, mae: 0.336883, mean_q: 4.283948
 17592/100000: episode: 219, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 64.648, mean reward: 2.309 [1.625, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.680, 10.289], loss: 0.139773, mae: 0.373942, mean_q: 4.288718
 17620/100000: episode: 220, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 64.440, mean reward: 2.301 [1.489, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.757, 10.153], loss: 0.171091, mae: 0.408806, mean_q: 4.309273
 17718/100000: episode: 221, duration: 0.507s, episode steps: 98, steps per second: 193, episode reward: 198.968, mean reward: 2.030 [1.465, 5.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-0.957, 10.260], loss: 0.147609, mae: 0.371839, mean_q: 4.285481
 17816/100000: episode: 222, duration: 0.513s, episode steps: 98, steps per second: 191, episode reward: 191.910, mean reward: 1.958 [1.471, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-1.021, 10.119], loss: 0.161661, mae: 0.380922, mean_q: 4.314911
 17844/100000: episode: 223, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 59.808, mean reward: 2.136 [1.794, 2.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.336], loss: 0.153802, mae: 0.400624, mean_q: 4.236910
 17942/100000: episode: 224, duration: 0.491s, episode steps: 98, steps per second: 200, episode reward: 211.737, mean reward: 2.161 [1.485, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.202, 10.100], loss: 0.143678, mae: 0.364908, mean_q: 4.317598
 17964/100000: episode: 225, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 60.342, mean reward: 2.743 [2.375, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.333, 10.481], loss: 0.115494, mae: 0.344818, mean_q: 4.337075
 18003/100000: episode: 226, duration: 0.185s, episode steps: 39, steps per second: 211, episode reward: 104.570, mean reward: 2.681 [1.754, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.719, 10.391], loss: 0.158819, mae: 0.378560, mean_q: 4.318477
 18047/100000: episode: 227, duration: 0.208s, episode steps: 44, steps per second: 211, episode reward: 103.665, mean reward: 2.356 [1.670, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.640, 10.291], loss: 0.161574, mae: 0.387789, mean_q: 4.356543
 18075/100000: episode: 228, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 68.064, mean reward: 2.431 [2.009, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.384], loss: 0.152707, mae: 0.378812, mean_q: 4.367831
 18119/100000: episode: 229, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 107.301, mean reward: 2.439 [1.627, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.018, 10.188], loss: 0.130740, mae: 0.354965, mean_q: 4.327579
 18158/100000: episode: 230, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 114.800, mean reward: 2.944 [1.871, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.035, 10.276], loss: 0.141510, mae: 0.374670, mean_q: 4.372745
 18197/100000: episode: 231, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 86.107, mean reward: 2.208 [1.715, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.233, 10.318], loss: 0.129020, mae: 0.343875, mean_q: 4.331910
 18241/100000: episode: 232, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 97.171, mean reward: 2.208 [1.483, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.362, 10.179], loss: 0.174573, mae: 0.399341, mean_q: 4.366105
 18263/100000: episode: 233, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 52.477, mean reward: 2.385 [1.921, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.374], loss: 0.166041, mae: 0.388419, mean_q: 4.392197
 18288/100000: episode: 234, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 75.519, mean reward: 3.021 [2.414, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.076, 10.457], loss: 0.123796, mae: 0.347379, mean_q: 4.344061
 18327/100000: episode: 235, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 81.540, mean reward: 2.091 [1.464, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.069, 10.191], loss: 0.126137, mae: 0.349074, mean_q: 4.349525
 18356/100000: episode: 236, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 78.642, mean reward: 2.712 [2.207, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.525], loss: 0.149104, mae: 0.385044, mean_q: 4.339145
 18388/100000: episode: 237, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 71.942, mean reward: 2.248 [1.548, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.063, 10.196], loss: 0.135790, mae: 0.375296, mean_q: 4.367274
 18414/100000: episode: 238, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 69.529, mean reward: 2.674 [1.955, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.210, 10.311], loss: 0.166930, mae: 0.413332, mean_q: 4.451098
 18439/100000: episode: 239, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 50.534, mean reward: 2.021 [1.607, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.263], loss: 0.144867, mae: 0.379649, mean_q: 4.382371
[Info] 2-TH LEVEL FOUND: 6.317222595214844, Considering 10/90 traces
 18537/100000: episode: 240, duration: 4.544s, episode steps: 98, steps per second: 22, episode reward: 182.740, mean reward: 1.865 [1.471, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.594, 10.100], loss: 0.134917, mae: 0.368397, mean_q: 4.405963
 18561/100000: episode: 241, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 82.601, mean reward: 3.442 [2.252, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.799, 10.376], loss: 0.136670, mae: 0.357877, mean_q: 4.393468
 18566/100000: episode: 242, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 17.626, mean reward: 3.525 [2.774, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.392], loss: 0.092918, mae: 0.325823, mean_q: 4.377163
 18590/100000: episode: 243, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 57.548, mean reward: 2.398 [1.785, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.167, 10.276], loss: 0.122349, mae: 0.347496, mean_q: 4.327014
 18612/100000: episode: 244, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 59.719, mean reward: 2.715 [2.043, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.318], loss: 0.137009, mae: 0.372426, mean_q: 4.410952
 18640/100000: episode: 245, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 76.224, mean reward: 2.722 [1.851, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.681, 10.302], loss: 0.142619, mae: 0.377797, mean_q: 4.399787
 18670/100000: episode: 246, duration: 0.142s, episode steps: 30, steps per second: 211, episode reward: 80.572, mean reward: 2.686 [2.004, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.303, 10.334], loss: 0.142675, mae: 0.371997, mean_q: 4.424230
 18694/100000: episode: 247, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 135.449, mean reward: 5.644 [3.571, 19.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.869, 10.505], loss: 0.152933, mae: 0.374700, mean_q: 4.358429
 18709/100000: episode: 248, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 62.267, mean reward: 4.151 [2.947, 8.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.222, 10.622], loss: 0.185968, mae: 0.405899, mean_q: 4.461975
 18737/100000: episode: 249, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 73.301, mean reward: 2.618 [1.960, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.328], loss: 0.144880, mae: 0.379658, mean_q: 4.415800
 18752/100000: episode: 250, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 76.386, mean reward: 5.092 [3.452, 7.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.185, 10.505], loss: 0.171774, mae: 0.400772, mean_q: 4.470289
 18780/100000: episode: 251, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 104.122, mean reward: 3.719 [2.547, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.540], loss: 0.311693, mae: 0.413282, mean_q: 4.480573
 18813/100000: episode: 252, duration: 0.199s, episode steps: 33, steps per second: 166, episode reward: 128.076, mean reward: 3.881 [2.646, 5.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.198, 10.504], loss: 0.154409, mae: 0.384655, mean_q: 4.490572
 18843/100000: episode: 253, duration: 0.177s, episode steps: 30, steps per second: 170, episode reward: 84.244, mean reward: 2.808 [1.879, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.350, 10.303], loss: 0.165119, mae: 0.395999, mean_q: 4.507844
 18865/100000: episode: 254, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 71.902, mean reward: 3.268 [2.520, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.337, 10.506], loss: 0.150623, mae: 0.386267, mean_q: 4.441812
 18895/100000: episode: 255, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 74.391, mean reward: 2.480 [1.597, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.310, 10.204], loss: 0.209955, mae: 0.453690, mean_q: 4.572349
 18923/100000: episode: 256, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 85.001, mean reward: 3.036 [2.302, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.223, 10.500], loss: 0.165547, mae: 0.396606, mean_q: 4.549970
 18947/100000: episode: 257, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 77.359, mean reward: 3.223 [2.151, 7.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-2.327, 10.360], loss: 0.156019, mae: 0.387536, mean_q: 4.562949
 18952/100000: episode: 258, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 21.101, mean reward: 4.220 [3.543, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.599], loss: 0.139038, mae: 0.363060, mean_q: 4.378973
 18957/100000: episode: 259, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 20.131, mean reward: 4.026 [2.477, 6.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.537, 10.352], loss: 0.159885, mae: 0.377362, mean_q: 4.570517
 18972/100000: episode: 260, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 46.607, mean reward: 3.107 [2.459, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.321, 10.431], loss: 0.171178, mae: 0.414073, mean_q: 4.582597
 19002/100000: episode: 261, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 102.602, mean reward: 3.420 [1.692, 7.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.716, 10.396], loss: 0.165821, mae: 0.393520, mean_q: 4.581915
 19030/100000: episode: 262, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 71.332, mean reward: 2.548 [1.680, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.010, 10.241], loss: 0.207869, mae: 0.442852, mean_q: 4.595898
 19046/100000: episode: 263, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 50.761, mean reward: 3.173 [2.701, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.526], loss: 0.183260, mae: 0.390056, mean_q: 4.572087
 19062/100000: episode: 264, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 58.279, mean reward: 3.642 [2.561, 4.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.166, 10.463], loss: 0.424286, mae: 0.437522, mean_q: 4.681026
 19086/100000: episode: 265, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 88.000, mean reward: 3.667 [2.173, 5.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.236, 10.403], loss: 0.162256, mae: 0.401325, mean_q: 4.550107
 19105/100000: episode: 266, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 89.018, mean reward: 4.685 [2.923, 7.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.101, 10.609], loss: 0.372116, mae: 0.456255, mean_q: 4.642381
 19129/100000: episode: 267, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 76.084, mean reward: 3.170 [2.292, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.514, 10.403], loss: 0.175846, mae: 0.410977, mean_q: 4.657153
 19145/100000: episode: 268, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 51.611, mean reward: 3.226 [1.909, 5.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.326], loss: 0.162553, mae: 0.383286, mean_q: 4.617798
 19150/100000: episode: 269, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 21.481, mean reward: 4.296 [2.741, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.327], loss: 0.183423, mae: 0.409605, mean_q: 4.815344
 19172/100000: episode: 270, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 65.575, mean reward: 2.981 [1.776, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.259, 10.235], loss: 0.160350, mae: 0.399230, mean_q: 4.660230
 19194/100000: episode: 271, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 65.360, mean reward: 2.971 [2.209, 7.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.894, 10.460], loss: 0.165857, mae: 0.380441, mean_q: 4.688365
 19224/100000: episode: 272, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 75.256, mean reward: 2.509 [1.827, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.035, 10.231], loss: 0.202442, mae: 0.398132, mean_q: 4.683017
 19246/100000: episode: 273, duration: 0.146s, episode steps: 22, steps per second: 150, episode reward: 55.868, mean reward: 2.539 [2.009, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.385], loss: 0.175456, mae: 0.407315, mean_q: 4.668060
 19279/100000: episode: 274, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 98.921, mean reward: 2.998 [2.178, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.090, 10.469], loss: 0.180580, mae: 0.429507, mean_q: 4.709175
 19301/100000: episode: 275, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 72.156, mean reward: 3.280 [1.842, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.357], loss: 0.198943, mae: 0.423272, mean_q: 4.626671
 19325/100000: episode: 276, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 82.256, mean reward: 3.427 [2.574, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.659, 10.472], loss: 0.416412, mae: 0.514108, mean_q: 4.796046
 19349/100000: episode: 277, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 100.494, mean reward: 4.187 [3.146, 6.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.545], loss: 0.215037, mae: 0.457556, mean_q: 4.725288
 19382/100000: episode: 278, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 87.040, mean reward: 2.638 [1.721, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.145, 10.252], loss: 0.200189, mae: 0.426698, mean_q: 4.722037
 19404/100000: episode: 279, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 83.978, mean reward: 3.817 [2.831, 6.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.155, 10.548], loss: 0.542903, mae: 0.498579, mean_q: 4.764583
 19420/100000: episode: 280, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 58.050, mean reward: 3.628 [2.892, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.496], loss: 0.178945, mae: 0.420745, mean_q: 4.669492
 19442/100000: episode: 281, duration: 0.140s, episode steps: 22, steps per second: 158, episode reward: 76.457, mean reward: 3.475 [2.526, 6.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.453, 10.456], loss: 0.212659, mae: 0.435445, mean_q: 4.833161
 19447/100000: episode: 282, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 17.748, mean reward: 3.550 [3.006, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.497], loss: 0.232893, mae: 0.479769, mean_q: 4.879717
 19480/100000: episode: 283, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 92.592, mean reward: 2.806 [2.274, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.263, 10.411], loss: 0.370481, mae: 0.476339, mean_q: 4.791165
 19496/100000: episode: 284, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 62.931, mean reward: 3.933 [3.225, 5.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.851, 10.508], loss: 0.229801, mae: 0.462901, mean_q: 4.781551
 19526/100000: episode: 285, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 196.706, mean reward: 6.557 [3.239, 18.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.934, 10.561], loss: 0.201117, mae: 0.439434, mean_q: 4.845671
 19554/100000: episode: 286, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 76.887, mean reward: 2.746 [1.765, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.699, 10.313], loss: 0.311762, mae: 0.463042, mean_q: 4.793789
 19576/100000: episode: 287, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 94.114, mean reward: 4.278 [3.315, 5.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.540, 10.532], loss: 0.378200, mae: 0.471382, mean_q: 4.846084
 19606/100000: episode: 288, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 72.477, mean reward: 2.416 [1.549, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.385, 10.178], loss: 0.321946, mae: 0.449305, mean_q: 4.806100
 19636/100000: episode: 289, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 79.058, mean reward: 2.635 [1.964, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.823, 10.365], loss: 0.223848, mae: 0.475894, mean_q: 4.905602
 19652/100000: episode: 290, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 56.553, mean reward: 3.535 [2.673, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.471], loss: 0.183650, mae: 0.437852, mean_q: 4.933074
 19682/100000: episode: 291, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 82.932, mean reward: 2.764 [2.241, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.718, 10.451], loss: 0.366354, mae: 0.495793, mean_q: 4.912096
 19706/100000: episode: 292, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 103.803, mean reward: 4.325 [2.880, 13.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.848, 10.572], loss: 0.435894, mae: 0.542130, mean_q: 5.016015
 19730/100000: episode: 293, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 128.614, mean reward: 5.359 [3.095, 12.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.706, 10.502], loss: 0.494219, mae: 0.515008, mean_q: 4.949850
 19735/100000: episode: 294, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 24.918, mean reward: 4.984 [3.820, 6.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.497], loss: 0.124386, mae: 0.393401, mean_q: 4.925737
 19768/100000: episode: 295, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 157.032, mean reward: 4.759 [2.673, 11.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.442, 10.594], loss: 0.211312, mae: 0.447148, mean_q: 4.914238
 19792/100000: episode: 296, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 99.166, mean reward: 4.132 [2.249, 7.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.582, 10.425], loss: 0.381991, mae: 0.508257, mean_q: 5.043329
 19814/100000: episode: 297, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 72.354, mean reward: 3.289 [2.765, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.073, 10.448], loss: 0.519748, mae: 0.586192, mean_q: 4.985961
 19819/100000: episode: 298, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 19.251, mean reward: 3.850 [3.381, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.483, 10.455], loss: 0.274967, mae: 0.518427, mean_q: 4.862676
 19838/100000: episode: 299, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 54.416, mean reward: 2.864 [2.264, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.451], loss: 0.419826, mae: 0.527534, mean_q: 5.064283
 19868/100000: episode: 300, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 92.897, mean reward: 3.097 [2.096, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.201, 10.485], loss: 0.401326, mae: 0.525267, mean_q: 5.033554
 19873/100000: episode: 301, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 20.207, mean reward: 4.041 [3.437, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.561], loss: 0.214466, mae: 0.447529, mean_q: 5.028113
 19903/100000: episode: 302, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 88.636, mean reward: 2.955 [1.846, 4.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.433, 10.275], loss: 0.431948, mae: 0.523949, mean_q: 5.042118
 19927/100000: episode: 303, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 85.410, mean reward: 3.559 [2.249, 5.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.337, 10.445], loss: 0.238330, mae: 0.467667, mean_q: 5.082322
 19960/100000: episode: 304, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 108.747, mean reward: 3.295 [2.319, 6.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.420, 10.508], loss: 0.348032, mae: 0.502912, mean_q: 5.130693
 19993/100000: episode: 305, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 131.087, mean reward: 3.972 [2.373, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.979, 10.439], loss: 0.502930, mae: 0.516165, mean_q: 5.158317
 20017/100000: episode: 306, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 68.675, mean reward: 2.861 [2.146, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.399], loss: 0.353374, mae: 0.509214, mean_q: 5.178150
 20039/100000: episode: 307, duration: 0.104s, episode steps: 22, steps per second: 211, episode reward: 55.362, mean reward: 2.516 [1.828, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.154, 10.285], loss: 0.242824, mae: 0.472052, mean_q: 4.963342
 20061/100000: episode: 308, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 66.400, mean reward: 3.018 [2.479, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.466, 10.484], loss: 0.485869, mae: 0.567649, mean_q: 5.158364
 20066/100000: episode: 309, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 15.809, mean reward: 3.162 [2.519, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.443], loss: 0.714535, mae: 0.764472, mean_q: 5.603547
 20099/100000: episode: 310, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 86.619, mean reward: 2.625 [1.834, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.338], loss: 0.361586, mae: 0.563364, mean_q: 5.089834
 20115/100000: episode: 311, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 59.764, mean reward: 3.735 [2.148, 6.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.390], loss: 0.383401, mae: 0.523045, mean_q: 5.184312
 20148/100000: episode: 312, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 95.314, mean reward: 2.888 [2.190, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.445, 10.459], loss: 0.503613, mae: 0.567114, mean_q: 5.232830
 20181/100000: episode: 313, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 109.084, mean reward: 3.306 [2.148, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.222, 10.386], loss: 0.309833, mae: 0.513986, mean_q: 5.144498
 20196/100000: episode: 314, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 43.198, mean reward: 2.880 [1.871, 5.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.359], loss: 0.620400, mae: 0.599797, mean_q: 5.278969
 20215/100000: episode: 315, duration: 0.131s, episode steps: 19, steps per second: 145, episode reward: 61.814, mean reward: 3.253 [2.195, 4.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.362], loss: 0.309184, mae: 0.534231, mean_q: 5.255446
 20239/100000: episode: 316, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 69.440, mean reward: 2.893 [1.645, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.706, 10.323], loss: 0.486397, mae: 0.539439, mean_q: 5.228687
[Info] FALSIFICATION!
 20246/100000: episode: 317, duration: 0.447s, episode steps: 7, steps per second: 16, episode reward: 1021.385, mean reward: 145.912 [3.028, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.125, 9.404], loss: 0.266768, mae: 0.508357, mean_q: 5.276820
 20268/100000: episode: 318, duration: 0.171s, episode steps: 22, steps per second: 129, episode reward: 150.302, mean reward: 6.832 [3.088, 13.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.083, 10.597], loss: 0.222378, mae: 0.457749, mean_q: 5.213151
 20292/100000: episode: 319, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 84.455, mean reward: 3.519 [2.763, 5.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.457], loss: 0.249874, mae: 0.453205, mean_q: 5.167226
 20308/100000: episode: 320, duration: 0.077s, episode steps: 16, steps per second: 209, episode reward: 61.538, mean reward: 3.846 [2.572, 5.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.416], loss: 0.622934, mae: 0.571107, mean_q: 5.175643
 20341/100000: episode: 321, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 131.930, mean reward: 3.998 [2.811, 8.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.298, 10.504], loss: 0.455614, mae: 0.542016, mean_q: 5.292701
 20357/100000: episode: 322, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 45.686, mean reward: 2.855 [2.467, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.467, 10.432], loss: 0.349449, mae: 0.552885, mean_q: 5.342243
 20385/100000: episode: 323, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 91.477, mean reward: 3.267 [2.379, 7.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.564, 10.361], loss: 0.380845, mae: 0.527664, mean_q: 5.343811
 20400/100000: episode: 324, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 56.268, mean reward: 3.751 [2.631, 5.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.274, 10.506], loss: 0.440402, mae: 0.553768, mean_q: 5.364439
 20419/100000: episode: 325, duration: 0.094s, episode steps: 19, steps per second: 202, episode reward: 71.040, mean reward: 3.739 [2.408, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.547, 10.509], loss: 0.293505, mae: 0.529565, mean_q: 5.353239
 20435/100000: episode: 326, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 48.404, mean reward: 3.025 [2.390, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.378, 10.462], loss: 960.665466, mae: 2.424788, mean_q: 5.297259
 20454/100000: episode: 327, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 50.016, mean reward: 2.632 [2.304, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.334, 10.448], loss: 8.419453, mae: 3.310481, mean_q: 6.343001
 20487/100000: episode: 328, duration: 0.154s, episode steps: 33, steps per second: 215, episode reward: 92.462, mean reward: 2.802 [1.900, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.195, 10.369], loss: 1.555856, mae: 1.230877, mean_q: 5.174009
 20511/100000: episode: 329, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 72.902, mean reward: 3.038 [2.265, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.111, 10.499], loss: 0.467399, mae: 0.646456, mean_q: 5.203191
[Info] Complete ISplit Iteration
[Info] Levels: [4.764358, 6.3172226, 8.7545185]
[Info] Cond. Prob: [0.1, 0.1, 0.25]
[Info] Error Prob: 0.0025000000000000005

 20533/100000: episode: 330, duration: 4.695s, episode steps: 22, steps per second: 5, episode reward: 81.071, mean reward: 3.685 [2.457, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.608], loss: 0.545756, mae: 0.634606, mean_q: 5.306483
 20633/100000: episode: 331, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 203.445, mean reward: 2.034 [1.482, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.313, 10.354], loss: 154.068024, mae: 0.859407, mean_q: 5.285209
 20733/100000: episode: 332, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.345, mean reward: 1.843 [1.471, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.628, 10.177], loss: 1.543795, mae: 1.114890, mean_q: 5.507113
 20833/100000: episode: 333, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 199.696, mean reward: 1.997 [1.485, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.058, 10.098], loss: 154.797165, mae: 1.276819, mean_q: 5.673327
 20933/100000: episode: 334, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 182.195, mean reward: 1.822 [1.447, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.078, 10.098], loss: 0.528501, mae: 0.600036, mean_q: 5.263278
 21033/100000: episode: 335, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 207.589, mean reward: 2.076 [1.480, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.098], loss: 154.656845, mae: 1.208166, mean_q: 5.549655
 21133/100000: episode: 336, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 197.742, mean reward: 1.977 [1.497, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.599, 10.347], loss: 0.433815, mae: 0.588063, mean_q: 5.351733
 21233/100000: episode: 337, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 188.367, mean reward: 1.884 [1.461, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.373], loss: 308.623352, mae: 1.832239, mean_q: 6.002540
 21333/100000: episode: 338, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.180, mean reward: 1.852 [1.468, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.513, 10.141], loss: 154.505188, mae: 1.257936, mean_q: 5.693797
 21433/100000: episode: 339, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 201.135, mean reward: 2.011 [1.462, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.872, 10.098], loss: 0.532670, mae: 0.691352, mean_q: 5.332973
 21533/100000: episode: 340, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 198.060, mean reward: 1.981 [1.482, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.107], loss: 0.497726, mae: 0.602920, mean_q: 5.332165
 21633/100000: episode: 341, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 190.720, mean reward: 1.907 [1.458, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.746, 10.098], loss: 0.528236, mae: 0.589368, mean_q: 5.338259
 21733/100000: episode: 342, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 183.766, mean reward: 1.838 [1.429, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.528, 10.249], loss: 0.464816, mae: 0.575938, mean_q: 5.338857
 21833/100000: episode: 343, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 198.817, mean reward: 1.988 [1.485, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.104, 10.373], loss: 154.492233, mae: 1.178529, mean_q: 5.506774
 21933/100000: episode: 344, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 220.700, mean reward: 2.207 [1.456, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.098], loss: 0.395195, mae: 0.547822, mean_q: 5.242789
 22033/100000: episode: 345, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 213.930, mean reward: 2.139 [1.467, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.219], loss: 0.325188, mae: 0.508161, mean_q: 5.143064
 22133/100000: episode: 346, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 185.747, mean reward: 1.857 [1.441, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.920, 10.211], loss: 0.395712, mae: 0.527552, mean_q: 5.155504
 22233/100000: episode: 347, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 198.127, mean reward: 1.981 [1.462, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.659, 10.098], loss: 0.377639, mae: 0.516656, mean_q: 5.162561
 22333/100000: episode: 348, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 178.823, mean reward: 1.788 [1.444, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.754, 10.123], loss: 0.424518, mae: 0.515170, mean_q: 5.090587
 22433/100000: episode: 349, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.114, mean reward: 1.851 [1.500, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.098], loss: 0.346650, mae: 0.491043, mean_q: 5.037999
 22533/100000: episode: 350, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.620, mean reward: 1.816 [1.451, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.022, 10.098], loss: 0.373802, mae: 0.484601, mean_q: 5.034159
 22633/100000: episode: 351, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 199.029, mean reward: 1.990 [1.440, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.258, 10.098], loss: 0.352840, mae: 0.485697, mean_q: 5.015303
 22733/100000: episode: 352, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.258, mean reward: 1.903 [1.453, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.558, 10.098], loss: 154.883423, mae: 1.196093, mean_q: 5.281650
 22833/100000: episode: 353, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 224.102, mean reward: 2.241 [1.437, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.664, 10.579], loss: 0.496504, mae: 0.548552, mean_q: 5.147579
 22933/100000: episode: 354, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 202.314, mean reward: 2.023 [1.504, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.047, 10.098], loss: 0.311430, mae: 0.489296, mean_q: 4.986860
 23033/100000: episode: 355, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 246.925, mean reward: 2.469 [1.448, 8.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.021, 10.298], loss: 0.273548, mae: 0.458227, mean_q: 4.936511
 23133/100000: episode: 356, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.438, mean reward: 1.884 [1.457, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.068, 10.098], loss: 0.327066, mae: 0.480150, mean_q: 4.987881
 23233/100000: episode: 357, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.596, mean reward: 1.876 [1.462, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.911, 10.098], loss: 309.596985, mae: 1.959628, mean_q: 5.448240
 23333/100000: episode: 358, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 218.408, mean reward: 2.184 [1.470, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.764, 10.098], loss: 154.398773, mae: 1.150054, mean_q: 5.318505
 23433/100000: episode: 359, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 186.168, mean reward: 1.862 [1.440, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.098], loss: 0.572303, mae: 0.580168, mean_q: 5.038762
 23533/100000: episode: 360, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 188.722, mean reward: 1.887 [1.463, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.350, 10.105], loss: 154.349030, mae: 1.136044, mean_q: 5.175705
 23633/100000: episode: 361, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 202.467, mean reward: 2.025 [1.463, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.131, 10.392], loss: 154.086884, mae: 1.133600, mean_q: 5.189114
 23733/100000: episode: 362, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 180.776, mean reward: 1.808 [1.450, 2.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.203, 10.098], loss: 0.562593, mae: 0.584250, mean_q: 4.988824
 23833/100000: episode: 363, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 206.339, mean reward: 2.063 [1.492, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.562, 10.189], loss: 307.591217, mae: 1.446294, mean_q: 5.207177
 23933/100000: episode: 364, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 181.336, mean reward: 1.813 [1.459, 2.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.438, 10.098], loss: 458.604462, mae: 2.481623, mean_q: 6.127739
 24033/100000: episode: 365, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 182.414, mean reward: 1.824 [1.436, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.763, 10.098], loss: 153.987289, mae: 1.231872, mean_q: 5.303403
 24133/100000: episode: 366, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.082, mean reward: 1.871 [1.461, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.137], loss: 0.557156, mae: 0.595132, mean_q: 4.714041
 24233/100000: episode: 367, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.297, mean reward: 1.973 [1.463, 5.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.140], loss: 306.649780, mae: 1.515550, mean_q: 5.206743
 24333/100000: episode: 368, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 183.362, mean reward: 1.834 [1.461, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.147, 10.098], loss: 153.806793, mae: 1.222670, mean_q: 5.185919
 24433/100000: episode: 369, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.361, mean reward: 2.014 [1.460, 7.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.034, 10.336], loss: 457.271179, mae: 2.080865, mean_q: 5.438930
 24533/100000: episode: 370, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 192.258, mean reward: 1.923 [1.494, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.871, 10.296], loss: 1.039166, mae: 0.822237, mean_q: 4.991632
 24633/100000: episode: 371, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 208.438, mean reward: 2.084 [1.475, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.487, 10.179], loss: 0.391989, mae: 0.539616, mean_q: 4.564884
 24733/100000: episode: 372, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.985, mean reward: 1.830 [1.434, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.098], loss: 0.310648, mae: 0.468197, mean_q: 4.409770
 24833/100000: episode: 373, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 186.766, mean reward: 1.868 [1.454, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.503, 10.098], loss: 153.430618, mae: 0.781316, mean_q: 4.350552
 24933/100000: episode: 374, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 180.001, mean reward: 1.800 [1.440, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.445, 10.098], loss: 153.730499, mae: 1.175543, mean_q: 4.702517
 25033/100000: episode: 375, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 190.543, mean reward: 1.905 [1.437, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.251], loss: 0.408850, mae: 0.517952, mean_q: 4.238534
 25133/100000: episode: 376, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 207.690, mean reward: 2.077 [1.496, 6.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.098], loss: 0.253153, mae: 0.433013, mean_q: 4.146218
 25233/100000: episode: 377, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 199.399, mean reward: 1.994 [1.527, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.094, 10.098], loss: 153.404526, mae: 0.905071, mean_q: 4.320496
 25333/100000: episode: 378, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 194.645, mean reward: 1.946 [1.490, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.116, 10.098], loss: 0.244513, mae: 0.419251, mean_q: 3.975594
 25433/100000: episode: 379, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 185.173, mean reward: 1.852 [1.484, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.823, 10.234], loss: 0.166855, mae: 0.367167, mean_q: 3.982202
 25533/100000: episode: 380, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.524, mean reward: 1.825 [1.468, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.170], loss: 0.140694, mae: 0.335851, mean_q: 3.887739
 25633/100000: episode: 381, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 193.373, mean reward: 1.934 [1.516, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.303, 10.098], loss: 0.116554, mae: 0.333394, mean_q: 3.853671
 25733/100000: episode: 382, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 184.467, mean reward: 1.845 [1.448, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.296, 10.127], loss: 0.126543, mae: 0.336636, mean_q: 3.874947
 25833/100000: episode: 383, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.929, mean reward: 1.839 [1.446, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.255], loss: 0.098438, mae: 0.312302, mean_q: 3.831227
 25933/100000: episode: 384, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.380, mean reward: 1.914 [1.469, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.308, 10.310], loss: 0.124078, mae: 0.331280, mean_q: 3.857987
 26033/100000: episode: 385, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.237, mean reward: 1.862 [1.451, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.158], loss: 0.105496, mae: 0.307220, mean_q: 3.849518
 26133/100000: episode: 386, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.661, mean reward: 1.937 [1.453, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.226, 10.098], loss: 0.110711, mae: 0.316485, mean_q: 3.848472
 26233/100000: episode: 387, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.096, mean reward: 1.971 [1.482, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.258, 10.098], loss: 0.119373, mae: 0.320832, mean_q: 3.851577
 26333/100000: episode: 388, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.082, mean reward: 1.961 [1.498, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.148, 10.166], loss: 0.100631, mae: 0.312366, mean_q: 3.840375
 26433/100000: episode: 389, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 222.187, mean reward: 2.222 [1.455, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.098], loss: 0.115194, mae: 0.315585, mean_q: 3.839869
 26533/100000: episode: 390, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 199.591, mean reward: 1.996 [1.440, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.804, 10.098], loss: 0.121129, mae: 0.321202, mean_q: 3.863541
 26633/100000: episode: 391, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.446, mean reward: 1.864 [1.467, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.115, 10.098], loss: 0.123541, mae: 0.320557, mean_q: 3.860318
 26733/100000: episode: 392, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 190.795, mean reward: 1.908 [1.440, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.350, 10.100], loss: 0.108950, mae: 0.318034, mean_q: 3.869270
 26833/100000: episode: 393, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 201.618, mean reward: 2.016 [1.498, 5.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.430, 10.310], loss: 0.111538, mae: 0.320024, mean_q: 3.846356
 26933/100000: episode: 394, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.498, mean reward: 1.895 [1.454, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.343, 10.414], loss: 0.097129, mae: 0.309505, mean_q: 3.840181
 27033/100000: episode: 395, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 186.858, mean reward: 1.869 [1.472, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.098], loss: 0.103719, mae: 0.310957, mean_q: 3.836032
 27133/100000: episode: 396, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 202.562, mean reward: 2.026 [1.434, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.590, 10.098], loss: 0.118277, mae: 0.319943, mean_q: 3.827716
 27233/100000: episode: 397, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 190.809, mean reward: 1.908 [1.457, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.502, 10.098], loss: 0.111183, mae: 0.308358, mean_q: 3.836637
 27333/100000: episode: 398, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.402, mean reward: 1.884 [1.460, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.281, 10.098], loss: 0.096256, mae: 0.303768, mean_q: 3.828839
 27433/100000: episode: 399, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.236, mean reward: 1.912 [1.473, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.098], loss: 0.092314, mae: 0.299923, mean_q: 3.818863
 27533/100000: episode: 400, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.936, mean reward: 1.849 [1.443, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.098], loss: 0.101764, mae: 0.303862, mean_q: 3.849265
 27633/100000: episode: 401, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 186.997, mean reward: 1.870 [1.450, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.098], loss: 0.094061, mae: 0.301836, mean_q: 3.839148
 27733/100000: episode: 402, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.557, mean reward: 1.886 [1.477, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.280, 10.235], loss: 0.102464, mae: 0.304693, mean_q: 3.852861
 27833/100000: episode: 403, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 193.906, mean reward: 1.939 [1.464, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.893, 10.104], loss: 0.108669, mae: 0.311416, mean_q: 3.842073
 27933/100000: episode: 404, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.844, mean reward: 1.888 [1.456, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.372], loss: 0.099160, mae: 0.302638, mean_q: 3.831291
 28033/100000: episode: 405, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.078, mean reward: 1.961 [1.458, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.004, 10.098], loss: 0.088548, mae: 0.289371, mean_q: 3.823341
 28133/100000: episode: 406, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 185.901, mean reward: 1.859 [1.449, 4.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.958, 10.098], loss: 0.102279, mae: 0.301280, mean_q: 3.807417
 28233/100000: episode: 407, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.308, mean reward: 1.953 [1.452, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.447, 10.303], loss: 0.090722, mae: 0.291745, mean_q: 3.799732
 28333/100000: episode: 408, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.302, mean reward: 1.793 [1.473, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.975, 10.247], loss: 0.088304, mae: 0.291035, mean_q: 3.805692
 28433/100000: episode: 409, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.756, mean reward: 2.008 [1.515, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.087563, mae: 0.287628, mean_q: 3.802920
 28533/100000: episode: 410, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.318, mean reward: 1.903 [1.486, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.421, 10.198], loss: 0.101663, mae: 0.304858, mean_q: 3.819948
 28633/100000: episode: 411, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.001, mean reward: 1.960 [1.492, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.966, 10.122], loss: 0.088567, mae: 0.283675, mean_q: 3.800447
 28733/100000: episode: 412, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 180.632, mean reward: 1.806 [1.465, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.098], loss: 0.088838, mae: 0.285603, mean_q: 3.810934
 28833/100000: episode: 413, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 201.949, mean reward: 2.019 [1.460, 6.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.172, 10.304], loss: 0.090991, mae: 0.283297, mean_q: 3.798939
 28933/100000: episode: 414, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 194.187, mean reward: 1.942 [1.459, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.746, 10.160], loss: 0.095951, mae: 0.289016, mean_q: 3.813242
 29033/100000: episode: 415, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 186.814, mean reward: 1.868 [1.451, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.098], loss: 0.081736, mae: 0.283651, mean_q: 3.805475
 29133/100000: episode: 416, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 212.084, mean reward: 2.121 [1.528, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.098], loss: 0.104863, mae: 0.293668, mean_q: 3.813145
 29233/100000: episode: 417, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 184.726, mean reward: 1.847 [1.442, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.802, 10.259], loss: 0.070495, mae: 0.268901, mean_q: 3.807780
 29333/100000: episode: 418, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 189.911, mean reward: 1.899 [1.496, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.881, 10.098], loss: 0.084603, mae: 0.280006, mean_q: 3.816333
 29433/100000: episode: 419, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 188.956, mean reward: 1.890 [1.465, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.972, 10.098], loss: 0.076707, mae: 0.282060, mean_q: 3.811752
 29533/100000: episode: 420, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 186.089, mean reward: 1.861 [1.437, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.177, 10.152], loss: 0.082801, mae: 0.282284, mean_q: 3.804792
 29633/100000: episode: 421, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 203.290, mean reward: 2.033 [1.456, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.699, 10.195], loss: 0.076899, mae: 0.275290, mean_q: 3.812409
 29733/100000: episode: 422, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 208.011, mean reward: 2.080 [1.459, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.460, 10.333], loss: 0.080683, mae: 0.278471, mean_q: 3.806383
 29833/100000: episode: 423, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 184.235, mean reward: 1.842 [1.464, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.103, 10.196], loss: 0.084099, mae: 0.288917, mean_q: 3.827801
 29933/100000: episode: 424, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 211.150, mean reward: 2.111 [1.437, 5.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.316, 10.497], loss: 0.068849, mae: 0.270250, mean_q: 3.794935
 30033/100000: episode: 425, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 186.166, mean reward: 1.862 [1.465, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.098], loss: 0.089059, mae: 0.286273, mean_q: 3.817623
 30133/100000: episode: 426, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.278, mean reward: 1.853 [1.442, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.107], loss: 0.073546, mae: 0.271690, mean_q: 3.816052
 30233/100000: episode: 427, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 193.240, mean reward: 1.932 [1.462, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.753, 10.098], loss: 0.074865, mae: 0.275630, mean_q: 3.796504
 30333/100000: episode: 428, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.034, mean reward: 1.830 [1.454, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.081, 10.229], loss: 0.086931, mae: 0.290454, mean_q: 3.808951
 30433/100000: episode: 429, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 189.462, mean reward: 1.895 [1.455, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.568, 10.296], loss: 0.071670, mae: 0.272372, mean_q: 3.802430
[Info] 1-TH LEVEL FOUND: 5.26617956161499, Considering 10/90 traces
 30533/100000: episode: 430, duration: 4.679s, episode steps: 100, steps per second: 21, episode reward: 196.450, mean reward: 1.965 [1.477, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.025, 10.098], loss: 0.085818, mae: 0.292063, mean_q: 3.801037
 30550/100000: episode: 431, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 41.454, mean reward: 2.438 [1.791, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.243, 10.302], loss: 0.104936, mae: 0.295844, mean_q: 3.834766
 30567/100000: episode: 432, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 50.553, mean reward: 2.974 [2.132, 5.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.165, 10.346], loss: 0.084814, mae: 0.286555, mean_q: 3.846249
 30597/100000: episode: 433, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 92.364, mean reward: 3.079 [2.191, 4.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.316, 10.541], loss: 0.079606, mae: 0.279432, mean_q: 3.794533
 30642/100000: episode: 434, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 118.068, mean reward: 2.624 [1.943, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.405, 10.406], loss: 0.086032, mae: 0.290597, mean_q: 3.839265
 30688/100000: episode: 435, duration: 0.240s, episode steps: 46, steps per second: 192, episode reward: 93.533, mean reward: 2.033 [1.537, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.652, 10.100], loss: 0.084631, mae: 0.294146, mean_q: 3.837610
 30701/100000: episode: 436, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 34.599, mean reward: 2.661 [2.324, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.405], loss: 0.074049, mae: 0.278514, mean_q: 3.880032
 30747/100000: episode: 437, duration: 0.255s, episode steps: 46, steps per second: 180, episode reward: 132.520, mean reward: 2.881 [2.158, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.444, 10.396], loss: 0.093852, mae: 0.296862, mean_q: 3.858136
 30760/100000: episode: 438, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 46.337, mean reward: 3.564 [2.591, 4.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.431], loss: 0.079094, mae: 0.292251, mean_q: 3.931850
 30799/100000: episode: 439, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 94.331, mean reward: 2.419 [1.793, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.100, 10.311], loss: 0.080192, mae: 0.284918, mean_q: 3.866229
 30845/100000: episode: 440, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 94.572, mean reward: 2.056 [1.648, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.852, 10.100], loss: 0.084427, mae: 0.294668, mean_q: 3.871331
 30884/100000: episode: 441, duration: 0.181s, episode steps: 39, steps per second: 215, episode reward: 97.151, mean reward: 2.491 [1.939, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.371, 10.327], loss: 0.097487, mae: 0.306341, mean_q: 3.923313
 30897/100000: episode: 442, duration: 0.093s, episode steps: 13, steps per second: 140, episode reward: 45.181, mean reward: 3.475 [2.746, 6.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.342, 10.424], loss: 0.103095, mae: 0.308712, mean_q: 3.915135
 30914/100000: episode: 443, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 36.284, mean reward: 2.134 [1.693, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.633, 10.259], loss: 0.084286, mae: 0.292726, mean_q: 3.903749
 30953/100000: episode: 444, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 123.739, mean reward: 3.173 [1.841, 5.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.251, 10.450], loss: 0.079719, mae: 0.286765, mean_q: 3.917586
 30970/100000: episode: 445, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 38.452, mean reward: 2.262 [1.848, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.299], loss: 0.107218, mae: 0.315179, mean_q: 3.878228
 30987/100000: episode: 446, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 41.745, mean reward: 2.456 [1.998, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.339], loss: 0.074057, mae: 0.285673, mean_q: 3.893633
 31026/100000: episode: 447, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 121.788, mean reward: 3.123 [2.187, 5.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.103, 10.472], loss: 0.075849, mae: 0.281380, mean_q: 3.922534
 31043/100000: episode: 448, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 39.377, mean reward: 2.316 [1.798, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.522, 10.338], loss: 0.088860, mae: 0.285659, mean_q: 3.916506
 31090/100000: episode: 449, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 99.319, mean reward: 2.113 [1.535, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.706, 10.157], loss: 0.099369, mae: 0.303143, mean_q: 3.924750
 31137/100000: episode: 450, duration: 0.224s, episode steps: 47, steps per second: 209, episode reward: 116.012, mean reward: 2.468 [1.551, 6.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.498, 10.229], loss: 0.095407, mae: 0.303588, mean_q: 3.953656
 31176/100000: episode: 451, duration: 0.225s, episode steps: 39, steps per second: 173, episode reward: 100.034, mean reward: 2.565 [1.902, 4.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.111, 10.267], loss: 0.091638, mae: 0.295023, mean_q: 3.948245
 31223/100000: episode: 452, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 95.987, mean reward: 2.042 [1.568, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.236, 10.131], loss: 0.093289, mae: 0.306329, mean_q: 3.975146
 31262/100000: episode: 453, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 81.684, mean reward: 2.094 [1.618, 2.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.099, 10.372], loss: 0.090715, mae: 0.295002, mean_q: 3.946478
 31279/100000: episode: 454, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 44.112, mean reward: 2.595 [2.017, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.702, 10.301], loss: 0.090590, mae: 0.295407, mean_q: 3.956964
 31292/100000: episode: 455, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 35.943, mean reward: 2.765 [2.266, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.394, 10.329], loss: 0.085074, mae: 0.310245, mean_q: 4.045205
 31338/100000: episode: 456, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 96.411, mean reward: 2.096 [1.520, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.540, 10.163], loss: 0.100029, mae: 0.291754, mean_q: 3.937085
 31355/100000: episode: 457, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 41.329, mean reward: 2.431 [1.892, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.395], loss: 0.090013, mae: 0.294798, mean_q: 3.971509
 31402/100000: episode: 458, duration: 0.243s, episode steps: 47, steps per second: 194, episode reward: 106.880, mean reward: 2.274 [1.596, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-1.070, 10.260], loss: 0.092388, mae: 0.306899, mean_q: 3.989056
 31449/100000: episode: 459, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 109.237, mean reward: 2.324 [1.450, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.240, 10.100], loss: 0.093177, mae: 0.302409, mean_q: 4.006904
 31462/100000: episode: 460, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 35.675, mean reward: 2.744 [2.171, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.218, 10.375], loss: 0.100407, mae: 0.318168, mean_q: 4.002542
 31475/100000: episode: 461, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 94.567, mean reward: 7.274 [2.381, 38.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.295, 10.543], loss: 0.092617, mae: 0.291530, mean_q: 4.020395
 31521/100000: episode: 462, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 124.964, mean reward: 2.717 [2.075, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.203, 10.500], loss: 0.097010, mae: 0.305569, mean_q: 4.018719
 31534/100000: episode: 463, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 32.757, mean reward: 2.520 [2.106, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.375], loss: 0.093432, mae: 0.305737, mean_q: 3.991191
 31547/100000: episode: 464, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 38.888, mean reward: 2.991 [2.137, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.752, 10.253], loss: 0.098927, mae: 0.311991, mean_q: 3.996720
 31592/100000: episode: 465, duration: 0.219s, episode steps: 45, steps per second: 205, episode reward: 125.293, mean reward: 2.784 [2.031, 7.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.232, 10.490], loss: 0.097308, mae: 0.307634, mean_q: 4.034078
 31639/100000: episode: 466, duration: 0.289s, episode steps: 47, steps per second: 163, episode reward: 185.540, mean reward: 3.948 [2.517, 14.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.287, 10.584], loss: 0.114126, mae: 0.306581, mean_q: 4.061022
 31669/100000: episode: 467, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 65.659, mean reward: 2.189 [1.471, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.062, 10.148], loss: 0.771921, mae: 0.377294, mean_q: 4.110656
 31686/100000: episode: 468, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 42.995, mean reward: 2.529 [2.118, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.135, 10.398], loss: 0.140667, mae: 0.355294, mean_q: 4.016515
 31703/100000: episode: 469, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 39.654, mean reward: 2.333 [1.763, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.238, 10.258], loss: 0.160459, mae: 0.364197, mean_q: 4.108691
 31750/100000: episode: 470, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 114.315, mean reward: 2.432 [1.733, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.240, 10.222], loss: 0.168579, mae: 0.348806, mean_q: 4.132292
 31767/100000: episode: 471, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 53.014, mean reward: 3.118 [2.208, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.544], loss: 0.109209, mae: 0.307656, mean_q: 4.064983
 31817/100000: episode: 472, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 130.101, mean reward: 2.602 [1.941, 5.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.383, 10.373], loss: 0.113408, mae: 0.321227, mean_q: 4.106091
 31867/100000: episode: 473, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 136.814, mean reward: 2.736 [1.648, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.357, 10.267], loss: 0.648474, mae: 0.426105, mean_q: 4.149581
 31897/100000: episode: 474, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 86.728, mean reward: 2.891 [2.066, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.681, 10.331], loss: 0.831809, mae: 0.403143, mean_q: 4.186130
 31914/100000: episode: 475, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 40.877, mean reward: 2.405 [1.843, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.314], loss: 1.290709, mae: 0.466047, mean_q: 4.256467
 31931/100000: episode: 476, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 50.610, mean reward: 2.977 [2.308, 5.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.522], loss: 0.157609, mae: 0.374009, mean_q: 4.124192
 31948/100000: episode: 477, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 38.039, mean reward: 2.238 [1.660, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.696, 10.201], loss: 0.253682, mae: 0.389359, mean_q: 4.220059
 31994/100000: episode: 478, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 107.243, mean reward: 2.331 [1.747, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.274, 10.396], loss: 0.096080, mae: 0.306072, mean_q: 4.076771
 32040/100000: episode: 479, duration: 0.228s, episode steps: 46, steps per second: 201, episode reward: 135.983, mean reward: 2.956 [1.933, 5.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.351, 10.319], loss: 0.549382, mae: 0.365486, mean_q: 4.176916
 32087/100000: episode: 480, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 149.574, mean reward: 3.182 [2.462, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.260, 10.491], loss: 0.187551, mae: 0.365377, mean_q: 4.174203
 32137/100000: episode: 481, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 119.496, mean reward: 2.390 [1.728, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.804, 10.353], loss: 0.153170, mae: 0.363741, mean_q: 4.199775
 32176/100000: episode: 482, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 78.118, mean reward: 2.003 [1.509, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.690, 10.260], loss: 0.133452, mae: 0.362715, mean_q: 4.269134
 32206/100000: episode: 483, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 77.801, mean reward: 2.593 [1.826, 5.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.694, 10.230], loss: 0.770073, mae: 0.392078, mean_q: 4.207146
 32252/100000: episode: 484, duration: 0.262s, episode steps: 46, steps per second: 176, episode reward: 108.915, mean reward: 2.368 [1.589, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.741, 10.149], loss: 0.112220, mae: 0.332089, mean_q: 4.202293
 32269/100000: episode: 485, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 37.892, mean reward: 2.229 [1.935, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.337, 10.411], loss: 0.133566, mae: 0.349339, mean_q: 4.179148
 32314/100000: episode: 486, duration: 0.239s, episode steps: 45, steps per second: 189, episode reward: 111.971, mean reward: 2.488 [1.702, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.587, 10.263], loss: 0.117215, mae: 0.328573, mean_q: 4.219872
 32331/100000: episode: 487, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 47.006, mean reward: 2.765 [2.298, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.517], loss: 0.132815, mae: 0.330547, mean_q: 4.224563
 32376/100000: episode: 488, duration: 0.266s, episode steps: 45, steps per second: 169, episode reward: 103.991, mean reward: 2.311 [1.544, 5.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.702, 10.238], loss: 0.570375, mae: 0.394324, mean_q: 4.245329
 32421/100000: episode: 489, duration: 0.276s, episode steps: 45, steps per second: 163, episode reward: 110.012, mean reward: 2.445 [1.626, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.242, 10.211], loss: 0.580847, mae: 0.416378, mean_q: 4.280636
 32438/100000: episode: 490, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 78.733, mean reward: 4.631 [2.126, 18.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.658], loss: 0.158528, mae: 0.372708, mean_q: 4.254727
 32455/100000: episode: 491, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 38.889, mean reward: 2.288 [1.931, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.286], loss: 0.173658, mae: 0.370044, mean_q: 4.247413
 32502/100000: episode: 492, duration: 0.237s, episode steps: 47, steps per second: 198, episode reward: 84.652, mean reward: 1.801 [1.472, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.953, 10.100], loss: 0.148592, mae: 0.366160, mean_q: 4.316505
 32515/100000: episode: 493, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 36.308, mean reward: 2.793 [2.345, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.465], loss: 0.108177, mae: 0.328986, mean_q: 4.226606
 32532/100000: episode: 494, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 58.161, mean reward: 3.421 [1.972, 8.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.496], loss: 0.141970, mae: 0.362565, mean_q: 4.354791
 32545/100000: episode: 495, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 44.523, mean reward: 3.425 [2.608, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.437], loss: 0.157170, mae: 0.362028, mean_q: 4.344819
 32595/100000: episode: 496, duration: 0.283s, episode steps: 50, steps per second: 177, episode reward: 314.831, mean reward: 6.297 [1.890, 132.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.441, 10.680], loss: 1.721622, mae: 0.507079, mean_q: 4.414359
 32625/100000: episode: 497, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 82.604, mean reward: 2.753 [2.301, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.346], loss: 0.356062, mae: 0.474533, mean_q: 4.291141
 32655/100000: episode: 498, duration: 0.143s, episode steps: 30, steps per second: 209, episode reward: 74.317, mean reward: 2.477 [1.862, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.471, 10.419], loss: 0.176313, mae: 0.386907, mean_q: 4.320096
 32668/100000: episode: 499, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 31.088, mean reward: 2.391 [2.026, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.440], loss: 0.173817, mae: 0.387323, mean_q: 4.381037
 32714/100000: episode: 500, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 81.621, mean reward: 1.774 [1.467, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.791, 10.100], loss: 0.382545, mae: 0.418502, mean_q: 4.419349
 32753/100000: episode: 501, duration: 0.242s, episode steps: 39, steps per second: 161, episode reward: 161.288, mean reward: 4.136 [2.125, 11.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.571, 10.528], loss: 7.213448, mae: 0.753100, mean_q: 4.326392
 32803/100000: episode: 502, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 101.184, mean reward: 2.024 [1.458, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.498, 10.370], loss: 0.220931, mae: 0.407545, mean_q: 4.399478
 32853/100000: episode: 503, duration: 0.252s, episode steps: 50, steps per second: 198, episode reward: 183.751, mean reward: 3.675 [2.116, 10.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.129, 10.551], loss: 0.144718, mae: 0.375569, mean_q: 4.387627
 32899/100000: episode: 504, duration: 0.232s, episode steps: 46, steps per second: 198, episode reward: 107.192, mean reward: 2.330 [1.823, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.153, 10.185], loss: 0.711802, mae: 0.466440, mean_q: 4.435808
 32916/100000: episode: 505, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 36.539, mean reward: 2.149 [1.832, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.687, 10.267], loss: 0.184475, mae: 0.425569, mean_q: 4.375801
 32963/100000: episode: 506, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 94.080, mean reward: 2.002 [1.455, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.387, 10.146], loss: 0.320502, mae: 0.428423, mean_q: 4.416118
 33009/100000: episode: 507, duration: 0.264s, episode steps: 46, steps per second: 174, episode reward: 111.176, mean reward: 2.417 [1.899, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.403, 10.356], loss: 0.226073, mae: 0.398405, mean_q: 4.423327
 33055/100000: episode: 508, duration: 0.229s, episode steps: 46, steps per second: 201, episode reward: 119.481, mean reward: 2.597 [1.871, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.335, 10.414], loss: 0.196602, mae: 0.410532, mean_q: 4.490111
 33105/100000: episode: 509, duration: 0.245s, episode steps: 50, steps per second: 204, episode reward: 110.004, mean reward: 2.200 [1.602, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.601, 10.139], loss: 0.196426, mae: 0.393285, mean_q: 4.452749
 33151/100000: episode: 510, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 98.239, mean reward: 2.136 [1.521, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.900, 10.163], loss: 0.195578, mae: 0.387485, mean_q: 4.430458
 33197/100000: episode: 511, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 103.880, mean reward: 2.258 [1.726, 5.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.013, 10.347], loss: 17.750427, mae: 0.997487, mean_q: 4.710640
 33236/100000: episode: 512, duration: 0.188s, episode steps: 39, steps per second: 208, episode reward: 101.282, mean reward: 2.597 [1.971, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.396, 10.488], loss: 0.986117, mae: 0.707777, mean_q: 4.546186
 33275/100000: episode: 513, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 80.904, mean reward: 2.074 [1.481, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.259, 10.100], loss: 0.717092, mae: 0.450780, mean_q: 4.513706
 33322/100000: episode: 514, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 90.283, mean reward: 1.921 [1.463, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.238, 10.349], loss: 0.213927, mae: 0.409457, mean_q: 4.514942
 33367/100000: episode: 515, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 98.967, mean reward: 2.199 [1.513, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.306, 10.180], loss: 0.187502, mae: 0.379065, mean_q: 4.514203
 33384/100000: episode: 516, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 41.695, mean reward: 2.453 [2.015, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.327], loss: 0.183431, mae: 0.386851, mean_q: 4.472301
 33401/100000: episode: 517, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 42.995, mean reward: 2.529 [2.117, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.311], loss: 1.323921, mae: 0.491947, mean_q: 4.661632
 33447/100000: episode: 518, duration: 0.267s, episode steps: 46, steps per second: 172, episode reward: 129.632, mean reward: 2.818 [1.524, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.222, 10.117], loss: 0.282740, mae: 0.433588, mean_q: 4.586763
 33493/100000: episode: 519, duration: 0.243s, episode steps: 46, steps per second: 190, episode reward: 114.143, mean reward: 2.481 [1.886, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.925, 10.391], loss: 0.146555, mae: 0.364074, mean_q: 4.489771
[Info] 2-TH LEVEL FOUND: 6.12579345703125, Considering 10/90 traces
 33540/100000: episode: 520, duration: 4.604s, episode steps: 47, steps per second: 10, episode reward: 114.378, mean reward: 2.434 [1.672, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.040, 10.432], loss: 5.842224, mae: 0.600834, mean_q: 4.640600
 33552/100000: episode: 521, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 33.667, mean reward: 2.806 [2.322, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.401], loss: 22.425415, mae: 1.073410, mean_q: 4.622978
 33561/100000: episode: 522, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 32.324, mean reward: 3.592 [2.807, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.455], loss: 0.512252, mae: 0.703526, mean_q: 4.965895
 33573/100000: episode: 523, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 37.676, mean reward: 3.140 [2.633, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.046, 10.449], loss: 0.378951, mae: 0.534921, mean_q: 4.218912
 33582/100000: episode: 524, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 34.682, mean reward: 3.854 [2.645, 5.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.652, 10.441], loss: 0.240052, mae: 0.491162, mean_q: 4.775162
 33618/100000: episode: 525, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 201.414, mean reward: 5.595 [2.741, 11.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.155, 10.622], loss: 0.716366, mae: 0.439370, mean_q: 4.652049
 33639/100000: episode: 526, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 110.641, mean reward: 5.269 [2.867, 10.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.503], loss: 0.248329, mae: 0.418610, mean_q: 4.599010
 33660/100000: episode: 527, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 84.071, mean reward: 4.003 [2.428, 6.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.762, 10.481], loss: 0.149157, mae: 0.386337, mean_q: 4.589922
 33674/100000: episode: 528, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 49.101, mean reward: 3.507 [2.984, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.559, 10.473], loss: 0.466104, mae: 0.482403, mean_q: 4.622540
 33686/100000: episode: 529, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 35.277, mean reward: 2.940 [2.220, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.404], loss: 0.185657, mae: 0.403429, mean_q: 4.596459
 33699/100000: episode: 530, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 37.006, mean reward: 2.847 [1.919, 4.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.302, 10.334], loss: 0.211214, mae: 0.421559, mean_q: 4.789474
 33713/100000: episode: 531, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 50.751, mean reward: 3.625 [2.451, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.056, 10.379], loss: 0.219534, mae: 0.420145, mean_q: 4.577671
 33722/100000: episode: 532, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 32.414, mean reward: 3.602 [2.889, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.114, 10.534], loss: 0.167260, mae: 0.429646, mean_q: 4.767963
 33758/100000: episode: 533, duration: 0.240s, episode steps: 36, steps per second: 150, episode reward: 219.044, mean reward: 6.085 [2.147, 12.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.172, 10.398], loss: 0.208864, mae: 0.421485, mean_q: 4.694088
 33771/100000: episode: 534, duration: 0.110s, episode steps: 13, steps per second: 119, episode reward: 35.666, mean reward: 2.744 [2.321, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.389], loss: 20.490305, mae: 1.019788, mean_q: 5.134941
 33807/100000: episode: 535, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 154.044, mean reward: 4.279 [2.133, 7.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.705, 10.370], loss: 0.501774, mae: 0.572507, mean_q: 4.731287
 33821/100000: episode: 536, duration: 0.127s, episode steps: 14, steps per second: 110, episode reward: 73.698, mean reward: 5.264 [2.708, 21.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.745], loss: 0.243960, mae: 0.459113, mean_q: 4.746618
 33830/100000: episode: 537, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 33.882, mean reward: 3.765 [3.301, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.221, 10.501], loss: 0.852130, mae: 0.545686, mean_q: 4.799865
 33842/100000: episode: 538, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 33.252, mean reward: 2.771 [2.463, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.448], loss: 0.311833, mae: 0.481852, mean_q: 4.941921
 33878/100000: episode: 539, duration: 0.291s, episode steps: 36, steps per second: 124, episode reward: 149.383, mean reward: 4.150 [2.283, 7.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.206, 10.409], loss: 0.301269, mae: 0.463214, mean_q: 4.782548
 33901/100000: episode: 540, duration: 0.148s, episode steps: 23, steps per second: 155, episode reward: 141.766, mean reward: 6.164 [3.463, 12.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.270, 10.647], loss: 0.214860, mae: 0.431009, mean_q: 4.813918
 33914/100000: episode: 541, duration: 0.097s, episode steps: 13, steps per second: 134, episode reward: 37.119, mean reward: 2.855 [2.450, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.514], loss: 0.397179, mae: 0.462151, mean_q: 4.797901
 33950/100000: episode: 542, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 146.079, mean reward: 4.058 [2.025, 9.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.813, 10.322], loss: 0.281839, mae: 0.461570, mean_q: 4.875401
 33959/100000: episode: 543, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 22.471, mean reward: 2.497 [2.195, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.464], loss: 0.275650, mae: 0.491827, mean_q: 5.013292
 33968/100000: episode: 544, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 42.752, mean reward: 4.750 [3.498, 8.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-1.274, 10.620], loss: 0.791707, mae: 0.595302, mean_q: 5.172616
 33977/100000: episode: 545, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 36.718, mean reward: 4.080 [3.083, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.443], loss: 0.318396, mae: 0.439158, mean_q: 4.888520
 33990/100000: episode: 546, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 43.069, mean reward: 3.313 [2.637, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.646, 10.411], loss: 0.284736, mae: 0.466464, mean_q: 4.917144
 34013/100000: episode: 547, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 130.713, mean reward: 5.683 [2.592, 27.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.526], loss: 0.275667, mae: 0.487654, mean_q: 4.876186
 34022/100000: episode: 548, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 27.305, mean reward: 3.034 [2.493, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.453], loss: 1.202779, mae: 0.570542, mean_q: 5.065113
 34036/100000: episode: 549, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 39.184, mean reward: 2.799 [2.095, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.817, 10.419], loss: 0.336298, mae: 0.496268, mean_q: 4.856997
 34050/100000: episode: 550, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 44.401, mean reward: 3.171 [2.206, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.128, 10.521], loss: 1.703818, mae: 0.586763, mean_q: 5.006035
 34064/100000: episode: 551, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 51.881, mean reward: 3.706 [2.475, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.467], loss: 0.261673, mae: 0.468435, mean_q: 4.880639
 34076/100000: episode: 552, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 46.328, mean reward: 3.861 [2.628, 6.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.299, 10.480], loss: 0.286813, mae: 0.468691, mean_q: 5.074189
 34112/100000: episode: 553, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 200.864, mean reward: 5.580 [3.427, 10.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.168, 10.621], loss: 0.347531, mae: 0.482405, mean_q: 5.068417
 34124/100000: episode: 554, duration: 0.102s, episode steps: 12, steps per second: 118, episode reward: 31.996, mean reward: 2.666 [2.012, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.347], loss: 0.831986, mae: 0.614496, mean_q: 5.158178
 34145/100000: episode: 555, duration: 0.168s, episode steps: 21, steps per second: 125, episode reward: 66.446, mean reward: 3.164 [2.138, 5.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.135, 10.390], loss: 12.721250, mae: 0.671725, mean_q: 4.984850
 34157/100000: episode: 556, duration: 0.087s, episode steps: 12, steps per second: 137, episode reward: 30.284, mean reward: 2.524 [2.047, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.180, 10.435], loss: 0.776600, mae: 0.923147, mean_q: 5.280924
 34180/100000: episode: 557, duration: 0.160s, episode steps: 23, steps per second: 144, episode reward: 105.114, mean reward: 4.570 [3.339, 5.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.757, 10.608], loss: 0.402933, mae: 0.634214, mean_q: 4.992957
 34194/100000: episode: 558, duration: 0.069s, episode steps: 14, steps per second: 201, episode reward: 57.106, mean reward: 4.079 [3.080, 5.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.507, 10.512], loss: 0.390761, mae: 0.565599, mean_q: 5.188355
 34207/100000: episode: 559, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 44.804, mean reward: 3.446 [2.628, 5.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.141, 10.547], loss: 20.749014, mae: 1.080925, mean_q: 5.500013
 34230/100000: episode: 560, duration: 0.179s, episode steps: 23, steps per second: 128, episode reward: 182.772, mean reward: 7.947 [2.091, 61.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.131, 10.360], loss: 1.297627, mae: 0.670925, mean_q: 5.015226
 34243/100000: episode: 561, duration: 0.089s, episode steps: 13, steps per second: 145, episode reward: 38.517, mean reward: 2.963 [2.521, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.342], loss: 2.124974, mae: 0.651967, mean_q: 5.197340
 34279/100000: episode: 562, duration: 0.248s, episode steps: 36, steps per second: 145, episode reward: 142.201, mean reward: 3.950 [1.686, 9.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.159, 10.248], loss: 0.623874, mae: 0.563910, mean_q: 5.120273
 34302/100000: episode: 563, duration: 0.233s, episode steps: 23, steps per second: 99, episode reward: 94.784, mean reward: 4.121 [2.761, 6.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.158, 10.487], loss: 12.231385, mae: 0.776987, mean_q: 5.167099
 34311/100000: episode: 564, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 69.010, mean reward: 7.668 [3.955, 15.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.428, 10.725], loss: 0.667197, mae: 0.832505, mean_q: 5.531784
 34324/100000: episode: 565, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 44.388, mean reward: 3.414 [2.861, 5.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.781, 10.466], loss: 0.369173, mae: 0.576526, mean_q: 4.899603
 34337/100000: episode: 566, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 38.676, mean reward: 2.975 [2.555, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.503], loss: 22.032444, mae: 1.505762, mean_q: 6.213676
 34350/100000: episode: 567, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 41.054, mean reward: 3.158 [2.733, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.624, 10.429], loss: 5.319770, mae: 0.994820, mean_q: 4.693398
 34359/100000: episode: 568, duration: 0.083s, episode steps: 9, steps per second: 108, episode reward: 30.129, mean reward: 3.348 [2.322, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.149, 10.445], loss: 0.661557, mae: 0.862009, mean_q: 5.661150
 34372/100000: episode: 569, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 38.279, mean reward: 2.945 [2.589, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.473], loss: 0.889348, mae: 0.663247, mean_q: 4.851508
 34408/100000: episode: 570, duration: 0.238s, episode steps: 36, steps per second: 152, episode reward: 119.234, mean reward: 3.312 [2.419, 6.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.141, 10.370], loss: 9.731487, mae: 0.909164, mean_q: 5.401517
 34422/100000: episode: 571, duration: 0.105s, episode steps: 14, steps per second: 134, episode reward: 68.001, mean reward: 4.857 [2.908, 8.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.251, 10.592], loss: 2.433218, mae: 0.722671, mean_q: 5.462396
 34435/100000: episode: 572, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 45.225, mean reward: 3.479 [2.613, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.563], loss: 0.493501, mae: 0.662900, mean_q: 5.482788
 34444/100000: episode: 573, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 36.964, mean reward: 4.107 [2.852, 6.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.172, 10.582], loss: 0.449893, mae: 0.607703, mean_q: 5.433594
 34456/100000: episode: 574, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 34.654, mean reward: 2.888 [2.390, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.475], loss: 1.052395, mae: 0.652216, mean_q: 5.277260
 34470/100000: episode: 575, duration: 0.106s, episode steps: 14, steps per second: 133, episode reward: 45.589, mean reward: 3.256 [2.427, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.534], loss: 0.371303, mae: 0.539167, mean_q: 5.226562
 34506/100000: episode: 576, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 215.981, mean reward: 5.999 [2.400, 18.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.309, 10.380], loss: 0.581520, mae: 0.580436, mean_q: 5.380445
 34542/100000: episode: 577, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 117.950, mean reward: 3.276 [2.069, 7.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.375, 10.338], loss: 0.969241, mae: 0.589393, mean_q: 5.293289
[Info] FALSIFICATION!
 34564/100000: episode: 578, duration: 0.382s, episode steps: 22, steps per second: 58, episode reward: 1624.140, mean reward: 73.825 [4.892, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.515, 9.842], loss: 12.504121, mae: 1.018450, mean_q: 5.819756
 34576/100000: episode: 579, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 42.733, mean reward: 3.561 [2.762, 4.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.054, 10.530], loss: 7.802040, mae: 3.038637, mean_q: 8.276381
 34597/100000: episode: 580, duration: 0.156s, episode steps: 21, steps per second: 135, episode reward: 70.766, mean reward: 3.370 [2.188, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.483, 10.389], loss: 2.763905, mae: 1.507662, mean_q: 4.363687
[Info] FALSIFICATION!
 34606/100000: episode: 581, duration: 0.304s, episode steps: 9, steps per second: 30, episode reward: 1198.600, mean reward: 133.178 [4.803, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-0.228, 8.384], loss: 0.715868, mae: 1.014731, mean_q: 6.133440
 34620/100000: episode: 582, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 61.629, mean reward: 4.402 [2.924, 9.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.456, 10.515], loss: 1.136215, mae: 0.700889, mean_q: 5.367709
 34629/100000: episode: 583, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 30.431, mean reward: 3.381 [3.072, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.488], loss: 3.143696, mae: 0.895941, mean_q: 5.556483
 34642/100000: episode: 584, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 39.830, mean reward: 3.064 [2.372, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.396], loss: 0.548199, mae: 0.750700, mean_q: 5.684427
 34654/100000: episode: 585, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 32.824, mean reward: 2.735 [2.036, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.402], loss: 7.571859, mae: 0.979243, mean_q: 5.683224
 34675/100000: episode: 586, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 140.414, mean reward: 6.686 [4.114, 10.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.124, 10.697], loss: 4.782760, mae: 0.968526, mean_q: 5.839365
 34688/100000: episode: 587, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 33.046, mean reward: 2.542 [2.202, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.353], loss: 23.685703, mae: 1.367598, mean_q: 6.148745
 34711/100000: episode: 588, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 125.276, mean reward: 5.447 [3.352, 9.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.633], loss: 128.017197, mae: 1.736465, mean_q: 6.346089
 34732/100000: episode: 589, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 76.941, mean reward: 3.664 [2.190, 6.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.374], loss: 1.381539, mae: 0.862826, mean_q: 5.513897
 34745/100000: episode: 590, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 47.027, mean reward: 3.617 [2.681, 5.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.494], loss: 0.713704, mae: 0.722682, mean_q: 5.753074
 34757/100000: episode: 591, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 42.250, mean reward: 3.521 [2.730, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.611, 10.556], loss: 6.074682, mae: 0.843351, mean_q: 5.620470
 34770/100000: episode: 592, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 44.959, mean reward: 3.458 [2.929, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.513], loss: 0.579588, mae: 0.669254, mean_q: 5.589889
 34782/100000: episode: 593, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 36.281, mean reward: 3.023 [2.446, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.476], loss: 1.056341, mae: 0.650811, mean_q: 5.622014
 34803/100000: episode: 594, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 86.120, mean reward: 4.101 [2.614, 5.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.500], loss: 728.179382, mae: 2.918991, mean_q: 6.761251
 34812/100000: episode: 595, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 40.760, mean reward: 4.529 [2.889, 5.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.546], loss: 8.511217, mae: 1.795485, mean_q: 6.332292
 34821/100000: episode: 596, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 36.187, mean reward: 4.021 [3.157, 4.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.576], loss: 1.903481, mae: 1.346837, mean_q: 4.991775
 34835/100000: episode: 597, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 46.560, mean reward: 3.326 [2.616, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.447, 10.438], loss: 1.493906, mae: 0.988156, mean_q: 6.049058
 34844/100000: episode: 598, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 41.653, mean reward: 4.628 [3.567, 5.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.601], loss: 1705.769653, mae: 4.243116, mean_q: 5.788521
 34858/100000: episode: 599, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 89.049, mean reward: 6.361 [3.184, 12.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.662], loss: 6.301789, mae: 2.922731, mean_q: 8.610964
 34867/100000: episode: 600, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 34.923, mean reward: 3.880 [3.248, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.553], loss: 1.440298, mae: 1.219481, mean_q: 4.869144
 34888/100000: episode: 601, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 113.783, mean reward: 5.418 [2.887, 8.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.611, 10.498], loss: 13.572124, mae: 1.239251, mean_q: 5.679052
 34911/100000: episode: 602, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 154.489, mean reward: 6.717 [2.780, 20.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.569], loss: 1.208571, mae: 0.819409, mean_q: 6.324994
 34924/100000: episode: 603, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 41.945, mean reward: 3.227 [2.606, 5.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.556, 10.403], loss: 4.900348, mae: 0.896044, mean_q: 6.132202
 34933/100000: episode: 604, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 32.738, mean reward: 3.638 [3.267, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.518], loss: 1717.510620, mae: 5.026855, mean_q: 6.762293
 34946/100000: episode: 605, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 32.851, mean reward: 2.527 [1.988, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.433], loss: 5.042485, mae: 2.453103, mean_q: 8.476585
 34960/100000: episode: 606, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 48.117, mean reward: 3.437 [2.946, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.334, 10.537], loss: 206.809845, mae: 2.222064, mean_q: 5.977108
 34996/100000: episode: 607, duration: 0.214s, episode steps: 36, steps per second: 169, episode reward: 108.457, mean reward: 3.013 [1.712, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.689, 10.379], loss: 9.298799, mae: 1.293186, mean_q: 6.701134
 35005/100000: episode: 608, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 30.562, mean reward: 3.396 [2.950, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.493], loss: 0.644967, mae: 0.716825, mean_q: 5.969554
 35014/100000: episode: 609, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 26.824, mean reward: 2.980 [2.632, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.585, 10.409], loss: 1701.580444, mae: 4.853841, mean_q: 7.191587
[Info] Complete ISplit Iteration
[Info] Levels: [5.2661796, 6.1257935, 14.234234]
[Info] Cond. Prob: [0.1, 0.1, 0.04]
[Info] Error Prob: 0.0004000000000000001

 35028/100000: episode: 610, duration: 4.420s, episode steps: 14, steps per second: 3, episode reward: 49.640, mean reward: 3.546 [2.443, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.357], loss: 22.595989, mae: 3.015868, mean_q: 8.899004
 35128/100000: episode: 611, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 183.696, mean reward: 1.837 [1.517, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.168], loss: 239.175476, mae: 2.114798, mean_q: 6.794913
 35228/100000: episode: 612, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 180.715, mean reward: 1.807 [1.472, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.661, 10.207], loss: 336.632416, mae: 2.256651, mean_q: 6.874682
 35328/100000: episode: 613, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 192.935, mean reward: 1.929 [1.514, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.047, 10.233], loss: 335.710388, mae: 2.438431, mean_q: 7.289208
 35428/100000: episode: 614, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 229.042, mean reward: 2.290 [1.461, 7.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.649, 10.098], loss: 673.505615, mae: 3.285257, mean_q: 7.424811
 35528/100000: episode: 615, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.450, mean reward: 1.964 [1.457, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.938, 10.498], loss: 312.021088, mae: 2.871430, mean_q: 7.622835
 35628/100000: episode: 616, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 205.184, mean reward: 2.052 [1.451, 4.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.467, 10.218], loss: 160.606155, mae: 1.398227, mean_q: 6.632041
 35728/100000: episode: 617, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.537, mean reward: 1.925 [1.436, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.166, 10.217], loss: 238.847046, mae: 2.261769, mean_q: 7.297060
 35828/100000: episode: 618, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 212.684, mean reward: 2.127 [1.535, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.475, 10.246], loss: 462.852509, mae: 2.668838, mean_q: 7.245736
 35928/100000: episode: 619, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.477, mean reward: 1.935 [1.447, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.688, 10.176], loss: 185.789597, mae: 1.821562, mean_q: 6.951627
 36028/100000: episode: 620, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 211.929, mean reward: 2.119 [1.479, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.193, 10.229], loss: 215.506332, mae: 2.041048, mean_q: 7.062543
 36128/100000: episode: 621, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.880, mean reward: 1.919 [1.445, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.503, 10.098], loss: 30.834784, mae: 1.104801, mean_q: 6.392245
 36228/100000: episode: 622, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 187.549, mean reward: 1.875 [1.453, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.821, 10.184], loss: 807.637268, mae: 4.211197, mean_q: 8.025572
 36328/100000: episode: 623, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 194.737, mean reward: 1.947 [1.457, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.877, 10.098], loss: 183.573792, mae: 1.708952, mean_q: 6.680637
 36428/100000: episode: 624, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 180.633, mean reward: 1.806 [1.448, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.392, 10.263], loss: 6.028387, mae: 0.951856, mean_q: 6.191378
 36528/100000: episode: 625, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 207.876, mean reward: 2.079 [1.459, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.433, 10.318], loss: 310.835632, mae: 1.915623, mean_q: 6.527030
 36628/100000: episode: 626, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 191.180, mean reward: 1.912 [1.486, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.425, 10.098], loss: 307.699127, mae: 1.967299, mean_q: 6.676123
 36728/100000: episode: 627, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.151, mean reward: 1.842 [1.467, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.525, 10.098], loss: 182.618225, mae: 1.603525, mean_q: 6.315701
 36828/100000: episode: 628, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.744, mean reward: 1.887 [1.470, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.982, 10.116], loss: 2.899509, mae: 0.818317, mean_q: 5.865374
 36928/100000: episode: 629, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 184.627, mean reward: 1.846 [1.458, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.410, 10.159], loss: 154.855835, mae: 1.201904, mean_q: 5.990244
 37028/100000: episode: 630, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 224.983, mean reward: 2.250 [1.519, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.829, 10.098], loss: 158.579880, mae: 1.473518, mean_q: 6.152390
 37128/100000: episode: 631, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 200.940, mean reward: 2.009 [1.451, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.424, 10.502], loss: 306.472046, mae: 1.760254, mean_q: 6.373497
 37228/100000: episode: 632, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.079, mean reward: 2.021 [1.441, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.497, 10.297], loss: 185.223557, mae: 1.492970, mean_q: 6.137987
 37328/100000: episode: 633, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.414, mean reward: 1.864 [1.466, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.682, 10.098], loss: 157.061569, mae: 1.353590, mean_q: 6.051013
 37428/100000: episode: 634, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 196.777, mean reward: 1.968 [1.497, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.074, 10.180], loss: 308.737701, mae: 2.071934, mean_q: 6.493933
 37528/100000: episode: 635, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.831, mean reward: 1.998 [1.473, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.828, 10.249], loss: 183.320160, mae: 1.494229, mean_q: 6.142521
 37628/100000: episode: 636, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.680, mean reward: 1.907 [1.526, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.618, 10.098], loss: 458.258240, mae: 2.343860, mean_q: 6.594929
 37728/100000: episode: 637, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.823, mean reward: 1.928 [1.455, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.707, 10.213], loss: 153.386276, mae: 1.188080, mean_q: 5.910252
 37828/100000: episode: 638, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 195.234, mean reward: 1.952 [1.486, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.405, 10.224], loss: 509.653076, mae: 3.087251, mean_q: 7.030702
 37928/100000: episode: 639, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.056, mean reward: 1.951 [1.470, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.208], loss: 206.216232, mae: 1.491059, mean_q: 5.724228
 38028/100000: episode: 640, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.907, mean reward: 1.979 [1.442, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.519, 10.110], loss: 149.874603, mae: 1.778206, mean_q: 6.186163
 38128/100000: episode: 641, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.257, mean reward: 1.913 [1.449, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.098], loss: 232.192810, mae: 2.094397, mean_q: 6.346182
 38228/100000: episode: 642, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 208.695, mean reward: 2.087 [1.461, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.262, 10.098], loss: 29.484600, mae: 0.911507, mean_q: 5.551468
 38328/100000: episode: 643, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.871, mean reward: 1.839 [1.458, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.669, 10.112], loss: 30.440334, mae: 1.006636, mean_q: 5.513037
 38428/100000: episode: 644, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 185.793, mean reward: 1.858 [1.470, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.219, 10.138], loss: 31.017584, mae: 1.090547, mean_q: 5.549949
 38528/100000: episode: 645, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 234.556, mean reward: 2.346 [1.493, 10.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.281], loss: 505.765228, mae: 2.796356, mean_q: 6.403615
 38628/100000: episode: 646, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.390, mean reward: 1.994 [1.500, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.578, 10.282], loss: 151.549301, mae: 1.625949, mean_q: 5.639952
 38728/100000: episode: 647, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.384, mean reward: 1.934 [1.497, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.941, 10.197], loss: 376.572723, mae: 2.498514, mean_q: 6.260197
 38828/100000: episode: 648, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 195.660, mean reward: 1.957 [1.466, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.604, 10.192], loss: 149.309921, mae: 1.503340, mean_q: 5.734694
 38928/100000: episode: 649, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.903, mean reward: 1.829 [1.456, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.762, 10.098], loss: 28.804836, mae: 0.936193, mean_q: 5.074703
 39028/100000: episode: 650, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.239, mean reward: 2.032 [1.532, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.999, 10.098], loss: 292.253387, mae: 1.519608, mean_q: 5.192481
 39128/100000: episode: 651, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.563, mean reward: 1.976 [1.460, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.083, 10.098], loss: 184.208786, mae: 1.746932, mean_q: 5.503709
 39228/100000: episode: 652, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.392, mean reward: 1.864 [1.461, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.061, 10.241], loss: 181.199966, mae: 1.386405, mean_q: 5.036029
 39328/100000: episode: 653, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 197.655, mean reward: 1.977 [1.479, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.893, 10.098], loss: 173.681015, mae: 1.549125, mean_q: 5.454605
 39428/100000: episode: 654, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.704, mean reward: 1.887 [1.445, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.098], loss: 1.525766, mae: 0.644639, mean_q: 4.606448
 39528/100000: episode: 655, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.723, mean reward: 2.017 [1.460, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.562, 10.098], loss: 1.039507, mae: 0.537530, mean_q: 4.254495
 39628/100000: episode: 656, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 183.880, mean reward: 1.839 [1.459, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.216], loss: 0.414318, mae: 0.515828, mean_q: 4.209028
 39728/100000: episode: 657, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.717, mean reward: 1.917 [1.491, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.636, 10.152], loss: 0.321173, mae: 0.470572, mean_q: 4.100180
 39828/100000: episode: 658, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.424, mean reward: 1.934 [1.447, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.935, 10.098], loss: 0.306202, mae: 0.451499, mean_q: 4.027881
 39928/100000: episode: 659, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 197.889, mean reward: 1.979 [1.524, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.201, 10.158], loss: 0.190170, mae: 0.401730, mean_q: 3.930230
 40028/100000: episode: 660, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 182.448, mean reward: 1.824 [1.457, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.509, 10.201], loss: 0.130415, mae: 0.361065, mean_q: 3.848310
 40128/100000: episode: 661, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.409, mean reward: 2.014 [1.433, 5.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.634, 10.195], loss: 0.169154, mae: 0.371530, mean_q: 3.858163
 40228/100000: episode: 662, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 187.101, mean reward: 1.871 [1.457, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.759, 10.310], loss: 0.123814, mae: 0.342319, mean_q: 3.864334
 40328/100000: episode: 663, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.824, mean reward: 2.038 [1.467, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.181, 10.098], loss: 0.137377, mae: 0.348107, mean_q: 3.881830
 40428/100000: episode: 664, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.483, mean reward: 1.995 [1.457, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.098], loss: 0.130249, mae: 0.348577, mean_q: 3.890916
 40528/100000: episode: 665, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 194.581, mean reward: 1.946 [1.443, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.303, 10.109], loss: 0.118749, mae: 0.329494, mean_q: 3.877746
 40628/100000: episode: 666, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.936, mean reward: 1.909 [1.434, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.790, 10.098], loss: 0.120529, mae: 0.337516, mean_q: 3.898067
 40728/100000: episode: 667, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 205.359, mean reward: 2.054 [1.483, 5.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.098], loss: 0.107132, mae: 0.321829, mean_q: 3.877077
 40828/100000: episode: 668, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 193.500, mean reward: 1.935 [1.465, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.537, 10.098], loss: 0.126138, mae: 0.319320, mean_q: 3.855857
 40928/100000: episode: 669, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 186.296, mean reward: 1.863 [1.446, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.664, 10.199], loss: 0.106462, mae: 0.312635, mean_q: 3.849741
 41028/100000: episode: 670, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 184.501, mean reward: 1.845 [1.457, 5.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.527, 10.144], loss: 0.095777, mae: 0.309403, mean_q: 3.838911
 41128/100000: episode: 671, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 201.821, mean reward: 2.018 [1.443, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.392], loss: 0.086660, mae: 0.302245, mean_q: 3.834238
 41228/100000: episode: 672, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.250, mean reward: 2.002 [1.463, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.015, 10.259], loss: 0.094625, mae: 0.307404, mean_q: 3.843097
 41328/100000: episode: 673, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 197.039, mean reward: 1.970 [1.453, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.904, 10.212], loss: 0.092978, mae: 0.308564, mean_q: 3.856632
 41428/100000: episode: 674, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 237.961, mean reward: 2.380 [1.449, 6.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.403, 10.098], loss: 0.113960, mae: 0.318359, mean_q: 3.845135
 41528/100000: episode: 675, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 180.056, mean reward: 1.801 [1.453, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.201, 10.098], loss: 0.089522, mae: 0.301210, mean_q: 3.857345
 41628/100000: episode: 676, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 191.066, mean reward: 1.911 [1.471, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.836, 10.500], loss: 0.105962, mae: 0.312835, mean_q: 3.865805
 41728/100000: episode: 677, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 207.098, mean reward: 2.071 [1.455, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.641, 10.166], loss: 0.106300, mae: 0.306999, mean_q: 3.858959
 41828/100000: episode: 678, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 181.791, mean reward: 1.818 [1.458, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.125, 10.098], loss: 0.107412, mae: 0.318779, mean_q: 3.873995
 41928/100000: episode: 679, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 185.359, mean reward: 1.854 [1.448, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.633, 10.098], loss: 0.101762, mae: 0.320668, mean_q: 3.876045
 42028/100000: episode: 680, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 189.990, mean reward: 1.900 [1.499, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.601, 10.098], loss: 0.109147, mae: 0.317003, mean_q: 3.898417
 42128/100000: episode: 681, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 191.885, mean reward: 1.919 [1.481, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.110, 10.173], loss: 0.095516, mae: 0.312106, mean_q: 3.888197
 42228/100000: episode: 682, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 193.827, mean reward: 1.938 [1.433, 2.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.432, 10.098], loss: 0.089632, mae: 0.302156, mean_q: 3.849453
 42328/100000: episode: 683, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 182.425, mean reward: 1.824 [1.455, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.073, 10.211], loss: 0.091692, mae: 0.301378, mean_q: 3.858502
 42428/100000: episode: 684, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 200.786, mean reward: 2.008 [1.466, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.500, 10.098], loss: 0.092504, mae: 0.307806, mean_q: 3.852982
 42528/100000: episode: 685, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 183.992, mean reward: 1.840 [1.459, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.304, 10.239], loss: 0.097231, mae: 0.310506, mean_q: 3.881835
 42628/100000: episode: 686, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 212.622, mean reward: 2.126 [1.450, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.431, 10.098], loss: 0.089512, mae: 0.302079, mean_q: 3.849733
 42728/100000: episode: 687, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 183.845, mean reward: 1.838 [1.429, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.351], loss: 0.089199, mae: 0.306193, mean_q: 3.869904
 42828/100000: episode: 688, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 199.170, mean reward: 1.992 [1.448, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.261, 10.098], loss: 0.094971, mae: 0.304033, mean_q: 3.871520
 42928/100000: episode: 689, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 209.503, mean reward: 2.095 [1.479, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.298, 10.098], loss: 0.092298, mae: 0.306152, mean_q: 3.864378
 43028/100000: episode: 690, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 182.815, mean reward: 1.828 [1.465, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.342, 10.108], loss: 0.100795, mae: 0.296604, mean_q: 3.829025
 43128/100000: episode: 691, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 187.112, mean reward: 1.871 [1.479, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.320, 10.098], loss: 0.094205, mae: 0.308234, mean_q: 3.862458
 43228/100000: episode: 692, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 185.531, mean reward: 1.855 [1.490, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.072, 10.251], loss: 0.086416, mae: 0.299710, mean_q: 3.858299
 43328/100000: episode: 693, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 186.785, mean reward: 1.868 [1.478, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.152], loss: 0.095160, mae: 0.310298, mean_q: 3.852475
 43428/100000: episode: 694, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: 192.197, mean reward: 1.922 [1.460, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.097, 10.098], loss: 0.095633, mae: 0.301541, mean_q: 3.852381
 43528/100000: episode: 695, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 207.649, mean reward: 2.076 [1.505, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.793, 10.177], loss: 0.084094, mae: 0.291990, mean_q: 3.841998
 43628/100000: episode: 696, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 196.945, mean reward: 1.969 [1.488, 5.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.098], loss: 0.095401, mae: 0.304553, mean_q: 3.828887
 43728/100000: episode: 697, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 197.605, mean reward: 1.976 [1.431, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.138], loss: 0.097347, mae: 0.302294, mean_q: 3.841583
 43828/100000: episode: 698, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.952, mean reward: 1.900 [1.490, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.532, 10.377], loss: 0.091434, mae: 0.301042, mean_q: 3.852123
 43928/100000: episode: 699, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 195.851, mean reward: 1.959 [1.454, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.086, 10.098], loss: 0.091848, mae: 0.301467, mean_q: 3.839240
 44028/100000: episode: 700, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 190.012, mean reward: 1.900 [1.437, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.654, 10.185], loss: 0.097914, mae: 0.305492, mean_q: 3.851595
 44128/100000: episode: 701, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 199.990, mean reward: 2.000 [1.521, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.921, 10.098], loss: 0.095277, mae: 0.305103, mean_q: 3.846017
 44228/100000: episode: 702, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.630, mean reward: 1.926 [1.497, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.756, 10.098], loss: 0.086252, mae: 0.295796, mean_q: 3.827090
 44328/100000: episode: 703, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.532, mean reward: 2.035 [1.444, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.160], loss: 0.097757, mae: 0.308409, mean_q: 3.856049
 44428/100000: episode: 704, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 194.028, mean reward: 1.940 [1.475, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.126, 10.135], loss: 0.090501, mae: 0.299749, mean_q: 3.839752
 44528/100000: episode: 705, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.301, mean reward: 1.903 [1.459, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.607, 10.100], loss: 0.093159, mae: 0.300334, mean_q: 3.832954
 44628/100000: episode: 706, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.176, mean reward: 1.812 [1.454, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.521, 10.098], loss: 0.093321, mae: 0.299362, mean_q: 3.846393
 44728/100000: episode: 707, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 192.774, mean reward: 1.928 [1.460, 5.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.048, 10.098], loss: 0.095382, mae: 0.305065, mean_q: 3.847112
 44828/100000: episode: 708, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 198.867, mean reward: 1.989 [1.442, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.229, 10.098], loss: 0.102100, mae: 0.314788, mean_q: 3.853237
 44928/100000: episode: 709, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.622, mean reward: 1.856 [1.533, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.143], loss: 0.097383, mae: 0.309426, mean_q: 3.859142
[Info] 1-TH LEVEL FOUND: 5.180947303771973, Considering 10/90 traces
 45028/100000: episode: 710, duration: 4.802s, episode steps: 100, steps per second: 21, episode reward: 220.845, mean reward: 2.208 [1.449, 6.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.441, 10.631], loss: 0.096804, mae: 0.308361, mean_q: 3.857879
 45076/100000: episode: 711, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 100.689, mean reward: 2.098 [1.547, 4.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.764, 10.272], loss: 0.098954, mae: 0.312796, mean_q: 3.852129
 45118/100000: episode: 712, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 121.750, mean reward: 2.899 [2.206, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.164, 10.426], loss: 0.101855, mae: 0.309687, mean_q: 3.877819
 45157/100000: episode: 713, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 96.794, mean reward: 2.482 [1.701, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.703, 10.314], loss: 0.100561, mae: 0.309049, mean_q: 3.840865
 45196/100000: episode: 714, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 88.760, mean reward: 2.276 [1.717, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.472, 10.288], loss: 0.100048, mae: 0.310984, mean_q: 3.865073
 45239/100000: episode: 715, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 100.545, mean reward: 2.338 [1.797, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.990, 10.343], loss: 0.089204, mae: 0.294814, mean_q: 3.867077
 45289/100000: episode: 716, duration: 0.271s, episode steps: 50, steps per second: 185, episode reward: 91.860, mean reward: 1.837 [1.491, 2.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.847, 10.100], loss: 0.100907, mae: 0.317502, mean_q: 3.913629
 45331/100000: episode: 717, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 108.527, mean reward: 2.584 [1.652, 6.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.398, 10.100], loss: 0.109227, mae: 0.329645, mean_q: 3.939971
 45379/100000: episode: 718, duration: 0.238s, episode steps: 48, steps per second: 201, episode reward: 141.073, mean reward: 2.939 [1.602, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.645, 10.186], loss: 0.121917, mae: 0.321808, mean_q: 3.931788
 45418/100000: episode: 719, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 83.915, mean reward: 2.152 [1.593, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.526, 10.155], loss: 0.110514, mae: 0.322032, mean_q: 3.918816
 45457/100000: episode: 720, duration: 0.217s, episode steps: 39, steps per second: 179, episode reward: 86.166, mean reward: 2.209 [1.770, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.307, 10.319], loss: 0.087675, mae: 0.287215, mean_q: 3.896914
 45505/100000: episode: 721, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 122.109, mean reward: 2.544 [1.616, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.274, 10.342], loss: 0.114368, mae: 0.331311, mean_q: 3.942390
 45555/100000: episode: 722, duration: 0.298s, episode steps: 50, steps per second: 168, episode reward: 159.660, mean reward: 3.193 [1.582, 21.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.156, 10.209], loss: 0.107280, mae: 0.312132, mean_q: 3.926518
 45602/100000: episode: 723, duration: 0.226s, episode steps: 47, steps per second: 208, episode reward: 95.743, mean reward: 2.037 [1.553, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.201, 10.188], loss: 0.122809, mae: 0.330686, mean_q: 3.964612
 45641/100000: episode: 724, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 125.384, mean reward: 3.215 [2.063, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.938, 10.506], loss: 0.120298, mae: 0.323386, mean_q: 3.968985
 45680/100000: episode: 725, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 78.112, mean reward: 2.003 [1.508, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.517, 10.256], loss: 0.134832, mae: 0.348929, mean_q: 3.992372
 45719/100000: episode: 726, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 81.461, mean reward: 2.089 [1.651, 2.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.747, 10.288], loss: 0.121555, mae: 0.330004, mean_q: 3.967816
 45764/100000: episode: 727, duration: 0.220s, episode steps: 45, steps per second: 205, episode reward: 118.253, mean reward: 2.628 [1.672, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.167, 10.530], loss: 0.130673, mae: 0.333770, mean_q: 3.989676
 45811/100000: episode: 728, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 95.563, mean reward: 2.033 [1.540, 5.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.730, 10.100], loss: 0.146048, mae: 0.353505, mean_q: 4.007710
 45861/100000: episode: 729, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 127.342, mean reward: 2.547 [1.797, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.897, 10.389], loss: 0.121354, mae: 0.339257, mean_q: 3.996300
 45908/100000: episode: 730, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 104.849, mean reward: 2.231 [1.659, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.083, 10.245], loss: 0.142932, mae: 0.343197, mean_q: 3.990114
 45947/100000: episode: 731, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 89.314, mean reward: 2.290 [1.847, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.035, 10.397], loss: 0.117565, mae: 0.324075, mean_q: 4.020474
 45990/100000: episode: 732, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 91.472, mean reward: 2.127 [1.630, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.423, 10.307], loss: 0.104178, mae: 0.316127, mean_q: 3.965667
 46038/100000: episode: 733, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 99.896, mean reward: 2.081 [1.500, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.313, 10.442], loss: 0.255328, mae: 0.369486, mean_q: 4.035408
 46085/100000: episode: 734, duration: 0.263s, episode steps: 47, steps per second: 179, episode reward: 140.538, mean reward: 2.990 [2.041, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.488, 10.412], loss: 0.127265, mae: 0.335317, mean_q: 4.035171
 46127/100000: episode: 735, duration: 0.289s, episode steps: 42, steps per second: 145, episode reward: 122.964, mean reward: 2.928 [2.201, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.378, 10.409], loss: 0.107849, mae: 0.325786, mean_q: 4.018106
 46172/100000: episode: 736, duration: 0.287s, episode steps: 45, steps per second: 157, episode reward: 92.702, mean reward: 2.060 [1.640, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.574, 10.100], loss: 0.132210, mae: 0.335822, mean_q: 4.022194
 46222/100000: episode: 737, duration: 0.338s, episode steps: 50, steps per second: 148, episode reward: 116.995, mean reward: 2.340 [1.820, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.646, 10.325], loss: 0.120071, mae: 0.340562, mean_q: 4.063983
 46261/100000: episode: 738, duration: 0.306s, episode steps: 39, steps per second: 128, episode reward: 84.924, mean reward: 2.178 [1.494, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.679, 10.265], loss: 0.271653, mae: 0.365887, mean_q: 4.106670
 46313/100000: episode: 739, duration: 0.477s, episode steps: 52, steps per second: 109, episode reward: 100.852, mean reward: 1.939 [1.465, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.346, 10.100], loss: 0.234308, mae: 0.350796, mean_q: 4.045993
 46363/100000: episode: 740, duration: 0.442s, episode steps: 50, steps per second: 113, episode reward: 115.278, mean reward: 2.306 [1.500, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.685, 10.135], loss: 0.127268, mae: 0.349949, mean_q: 4.059529
 46402/100000: episode: 741, duration: 0.262s, episode steps: 39, steps per second: 149, episode reward: 93.930, mean reward: 2.408 [1.781, 5.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.098, 10.460], loss: 0.108296, mae: 0.313864, mean_q: 4.044473
 46444/100000: episode: 742, duration: 0.326s, episode steps: 42, steps per second: 129, episode reward: 92.818, mean reward: 2.210 [1.536, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.639, 10.188], loss: 0.106599, mae: 0.325652, mean_q: 4.053545
 46494/100000: episode: 743, duration: 0.483s, episode steps: 50, steps per second: 104, episode reward: 120.655, mean reward: 2.413 [1.650, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.940, 10.176], loss: 0.108508, mae: 0.324560, mean_q: 4.073482
 46539/100000: episode: 744, duration: 0.433s, episode steps: 45, steps per second: 104, episode reward: 96.186, mean reward: 2.137 [1.747, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.671, 10.224], loss: 0.117294, mae: 0.325580, mean_q: 4.057122
 46589/100000: episode: 745, duration: 0.457s, episode steps: 50, steps per second: 109, episode reward: 136.266, mean reward: 2.725 [1.614, 7.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.606, 10.200], loss: 0.118792, mae: 0.327688, mean_q: 4.075219
 46636/100000: episode: 746, duration: 0.346s, episode steps: 47, steps per second: 136, episode reward: 100.354, mean reward: 2.135 [1.459, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.255, 10.100], loss: 0.244991, mae: 0.365729, mean_q: 4.078879
 46683/100000: episode: 747, duration: 0.282s, episode steps: 47, steps per second: 167, episode reward: 99.371, mean reward: 2.114 [1.484, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.271, 10.130], loss: 0.136239, mae: 0.341812, mean_q: 4.101945
 46735/100000: episode: 748, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 109.948, mean reward: 2.114 [1.468, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.354, 10.191], loss: 0.117105, mae: 0.328651, mean_q: 4.077953
 46780/100000: episode: 749, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 96.886, mean reward: 2.153 [1.663, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.332, 10.227], loss: 0.111269, mae: 0.325039, mean_q: 4.097292
 46825/100000: episode: 750, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 96.312, mean reward: 2.140 [1.742, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.682, 10.254], loss: 0.226838, mae: 0.342303, mean_q: 4.074568
 46875/100000: episode: 751, duration: 0.252s, episode steps: 50, steps per second: 198, episode reward: 110.779, mean reward: 2.216 [1.545, 6.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.883, 10.239], loss: 0.229663, mae: 0.357092, mean_q: 4.147724
 46918/100000: episode: 752, duration: 0.218s, episode steps: 43, steps per second: 197, episode reward: 101.336, mean reward: 2.357 [1.844, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.187, 10.412], loss: 0.124559, mae: 0.346074, mean_q: 4.138879
 46963/100000: episode: 753, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 97.751, mean reward: 2.172 [1.558, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.727, 10.220], loss: 0.113581, mae: 0.338763, mean_q: 4.128915
 47008/100000: episode: 754, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 97.992, mean reward: 2.178 [1.617, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.019, 10.343], loss: 0.141125, mae: 0.352834, mean_q: 4.164585
 47060/100000: episode: 755, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 131.032, mean reward: 2.520 [1.648, 4.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.630, 10.348], loss: 0.100190, mae: 0.327046, mean_q: 4.115991
 47105/100000: episode: 756, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 88.512, mean reward: 1.967 [1.603, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.160, 10.324], loss: 0.264407, mae: 0.363137, mean_q: 4.174589
 47157/100000: episode: 757, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 123.992, mean reward: 2.384 [1.637, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.332, 10.240], loss: 0.120219, mae: 0.341425, mean_q: 4.182302
 47202/100000: episode: 758, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 94.916, mean reward: 2.109 [1.809, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.153, 10.329], loss: 0.099663, mae: 0.315250, mean_q: 4.153708
 47247/100000: episode: 759, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 98.390, mean reward: 2.186 [1.626, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.957, 10.394], loss: 0.132270, mae: 0.352727, mean_q: 4.203156
 47290/100000: episode: 760, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 89.570, mean reward: 2.083 [1.450, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.273, 10.276], loss: 0.148140, mae: 0.358286, mean_q: 4.214312
 47340/100000: episode: 761, duration: 0.261s, episode steps: 50, steps per second: 191, episode reward: 105.613, mean reward: 2.112 [1.634, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.122, 10.221], loss: 0.115881, mae: 0.332642, mean_q: 4.160898
 47388/100000: episode: 762, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 166.456, mean reward: 3.468 [2.032, 6.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.280, 10.387], loss: 0.272924, mae: 0.385336, mean_q: 4.209362
 47430/100000: episode: 763, duration: 0.247s, episode steps: 42, steps per second: 170, episode reward: 117.421, mean reward: 2.796 [1.780, 5.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.034, 10.300], loss: 0.119287, mae: 0.332095, mean_q: 4.224058
 47473/100000: episode: 764, duration: 0.232s, episode steps: 43, steps per second: 186, episode reward: 88.252, mean reward: 2.052 [1.547, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-1.014, 10.265], loss: 0.111570, mae: 0.338249, mean_q: 4.198575
 47516/100000: episode: 765, duration: 0.219s, episode steps: 43, steps per second: 196, episode reward: 91.432, mean reward: 2.126 [1.545, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.615, 10.330], loss: 0.135900, mae: 0.339897, mean_q: 4.213546
 47566/100000: episode: 766, duration: 0.288s, episode steps: 50, steps per second: 174, episode reward: 109.608, mean reward: 2.192 [1.641, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.326, 10.202], loss: 0.241745, mae: 0.369337, mean_q: 4.225632
 47572/100000: episode: 767, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 25.664, mean reward: 4.277 [3.878, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.467], loss: 0.147793, mae: 0.323777, mean_q: 4.174343
 47614/100000: episode: 768, duration: 0.328s, episode steps: 42, steps per second: 128, episode reward: 99.387, mean reward: 2.366 [1.478, 8.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.609, 10.100], loss: 0.284192, mae: 0.395600, mean_q: 4.245862
 47648/100000: episode: 769, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 85.361, mean reward: 2.511 [1.678, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.238, 10.279], loss: 0.116971, mae: 0.338405, mean_q: 4.218962
 47687/100000: episode: 770, duration: 0.246s, episode steps: 39, steps per second: 159, episode reward: 90.899, mean reward: 2.331 [1.447, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.287, 10.153], loss: 0.427549, mae: 0.397100, mean_q: 4.258268
 47693/100000: episode: 771, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 22.253, mean reward: 3.709 [3.452, 4.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.445], loss: 0.133155, mae: 0.349508, mean_q: 4.269703
 47699/100000: episode: 772, duration: 0.046s, episode steps: 6, steps per second: 131, episode reward: 18.877, mean reward: 3.146 [2.945, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.478], loss: 0.115574, mae: 0.333076, mean_q: 4.234436
 47744/100000: episode: 773, duration: 0.281s, episode steps: 45, steps per second: 160, episode reward: 115.609, mean reward: 2.569 [1.978, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.527, 10.366], loss: 0.135316, mae: 0.354023, mean_q: 4.268136
 47789/100000: episode: 774, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 117.742, mean reward: 2.616 [1.908, 6.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.338, 10.471], loss: 0.124181, mae: 0.352251, mean_q: 4.238771
 47828/100000: episode: 775, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 81.729, mean reward: 2.096 [1.766, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.587, 10.346], loss: 0.121950, mae: 0.340342, mean_q: 4.272412
 47880/100000: episode: 776, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 117.645, mean reward: 2.262 [1.517, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.052, 10.100], loss: 0.133852, mae: 0.348554, mean_q: 4.233589
 47914/100000: episode: 777, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 70.875, mean reward: 2.085 [1.506, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.801, 10.187], loss: 0.151494, mae: 0.359386, mean_q: 4.288730
 47920/100000: episode: 778, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 23.210, mean reward: 3.868 [3.239, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.430], loss: 0.076584, mae: 0.281261, mean_q: 4.147881
 47970/100000: episode: 779, duration: 0.243s, episode steps: 50, steps per second: 206, episode reward: 101.330, mean reward: 2.027 [1.483, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.931, 10.186], loss: 0.246468, mae: 0.371321, mean_q: 4.283216
 47976/100000: episode: 780, duration: 0.044s, episode steps: 6, steps per second: 138, episode reward: 23.825, mean reward: 3.971 [3.231, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.590], loss: 0.185231, mae: 0.408054, mean_q: 4.353027
 47982/100000: episode: 781, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 27.457, mean reward: 4.576 [3.544, 6.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.056, 10.566], loss: 0.115698, mae: 0.351608, mean_q: 4.343185
 48025/100000: episode: 782, duration: 0.428s, episode steps: 43, steps per second: 100, episode reward: 109.546, mean reward: 2.548 [1.844, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-1.143, 10.349], loss: 0.145924, mae: 0.350943, mean_q: 4.280049
 48064/100000: episode: 783, duration: 0.368s, episode steps: 39, steps per second: 106, episode reward: 82.154, mean reward: 2.107 [1.630, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.507, 10.313], loss: 0.284035, mae: 0.366456, mean_q: 4.322449
 48109/100000: episode: 784, duration: 0.549s, episode steps: 45, steps per second: 82, episode reward: 91.276, mean reward: 2.028 [1.476, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.464, 10.239], loss: 0.271237, mae: 0.377033, mean_q: 4.309092
 48115/100000: episode: 785, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 22.611, mean reward: 3.769 [3.249, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.572], loss: 0.128525, mae: 0.390634, mean_q: 4.328903
 48121/100000: episode: 786, duration: 0.074s, episode steps: 6, steps per second: 81, episode reward: 30.047, mean reward: 5.008 [4.605, 5.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.554], loss: 0.174365, mae: 0.362327, mean_q: 4.242841
 48173/100000: episode: 787, duration: 0.671s, episode steps: 52, steps per second: 78, episode reward: 103.428, mean reward: 1.989 [1.463, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.999, 10.290], loss: 0.134486, mae: 0.363702, mean_q: 4.307999
 48207/100000: episode: 788, duration: 0.460s, episode steps: 34, steps per second: 74, episode reward: 77.566, mean reward: 2.281 [1.835, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.486, 10.375], loss: 0.129461, mae: 0.348828, mean_q: 4.315233
 48254/100000: episode: 789, duration: 0.684s, episode steps: 47, steps per second: 69, episode reward: 117.019, mean reward: 2.490 [1.807, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.887, 10.280], loss: 0.153915, mae: 0.360300, mean_q: 4.339824
 48288/100000: episode: 790, duration: 0.330s, episode steps: 34, steps per second: 103, episode reward: 80.750, mean reward: 2.375 [2.017, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.302, 10.359], loss: 0.172005, mae: 0.394359, mean_q: 4.391165
 48335/100000: episode: 791, duration: 0.756s, episode steps: 47, steps per second: 62, episode reward: 109.601, mean reward: 2.332 [1.851, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.194, 10.407], loss: 0.295101, mae: 0.399363, mean_q: 4.375955
 48380/100000: episode: 792, duration: 0.476s, episode steps: 45, steps per second: 95, episode reward: 89.092, mean reward: 1.980 [1.505, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.172, 10.253], loss: 0.268292, mae: 0.384096, mean_q: 4.375205
 48432/100000: episode: 793, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 110.931, mean reward: 2.133 [1.447, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.365, 10.422], loss: 0.231445, mae: 0.363606, mean_q: 4.336205
 48482/100000: episode: 794, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 99.591, mean reward: 1.992 [1.621, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.772, 10.235], loss: 0.127759, mae: 0.343930, mean_q: 4.329490
 48527/100000: episode: 795, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 97.593, mean reward: 2.169 [1.469, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.692, 10.139], loss: 0.250610, mae: 0.371903, mean_q: 4.362266
 48575/100000: episode: 796, duration: 0.273s, episode steps: 48, steps per second: 176, episode reward: 96.905, mean reward: 2.019 [1.470, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.983, 10.100], loss: 0.115984, mae: 0.331699, mean_q: 4.321173
 48581/100000: episode: 797, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 24.371, mean reward: 4.062 [3.205, 4.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.506], loss: 0.117117, mae: 0.348382, mean_q: 4.172476
 48615/100000: episode: 798, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 88.668, mean reward: 2.608 [1.891, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.325], loss: 0.146234, mae: 0.347626, mean_q: 4.297839
 48658/100000: episode: 799, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 117.534, mean reward: 2.733 [1.771, 7.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.243, 10.263], loss: 0.146738, mae: 0.376616, mean_q: 4.359117
[Info] 2-TH LEVEL FOUND: 6.544282913208008, Considering 10/90 traces
 48708/100000: episode: 800, duration: 5.099s, episode steps: 50, steps per second: 10, episode reward: 132.655, mean reward: 2.653 [2.050, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.436, 10.460], loss: 0.352918, mae: 0.388621, mean_q: 4.350110
 48747/100000: episode: 801, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 84.557, mean reward: 2.168 [1.571, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.260, 10.160], loss: 0.163346, mae: 0.362114, mean_q: 4.412529
 48752/100000: episode: 802, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 23.349, mean reward: 4.670 [4.028, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.588], loss: 0.146292, mae: 0.377523, mean_q: 4.514159
 48757/100000: episode: 803, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: 33.919, mean reward: 6.784 [5.454, 11.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.682], loss: 0.164621, mae: 0.386332, mean_q: 4.299933
 48794/100000: episode: 804, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 122.584, mean reward: 3.313 [2.007, 10.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.932, 10.388], loss: 0.329468, mae: 0.419783, mean_q: 4.449342
 48833/100000: episode: 805, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 91.054, mean reward: 2.335 [1.472, 4.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.489, 10.100], loss: 0.127433, mae: 0.362076, mean_q: 4.445785
 48838/100000: episode: 806, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 23.330, mean reward: 4.666 [4.168, 5.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.529], loss: 0.113814, mae: 0.352475, mean_q: 4.462806
 48843/100000: episode: 807, duration: 0.044s, episode steps: 5, steps per second: 113, episode reward: 20.414, mean reward: 4.083 [3.468, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.551], loss: 0.124095, mae: 0.349081, mean_q: 4.319617
 48860/100000: episode: 808, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 61.388, mean reward: 3.611 [2.655, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.832, 10.406], loss: 0.254402, mae: 0.420579, mean_q: 4.408717
 48899/100000: episode: 809, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 86.641, mean reward: 2.222 [1.463, 6.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.150, 10.133], loss: 0.167758, mae: 0.380810, mean_q: 4.477157
 48938/100000: episode: 810, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 119.683, mean reward: 3.069 [1.752, 4.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.098, 10.286], loss: 0.271799, mae: 0.377156, mean_q: 4.493716
 48943/100000: episode: 811, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 18.710, mean reward: 3.742 [3.362, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-1.011, 10.535], loss: 0.123927, mae: 0.363174, mean_q: 4.405341
 48948/100000: episode: 812, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: 39.225, mean reward: 7.845 [4.511, 19.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.747], loss: 0.129423, mae: 0.345403, mean_q: 4.362796
 48987/100000: episode: 813, duration: 0.234s, episode steps: 39, steps per second: 167, episode reward: 125.314, mean reward: 3.213 [2.215, 7.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.941, 10.291], loss: 0.196288, mae: 0.399091, mean_q: 4.503670
 49021/100000: episode: 814, duration: 0.194s, episode steps: 34, steps per second: 176, episode reward: 97.945, mean reward: 2.881 [2.175, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.410, 10.422], loss: 0.471683, mae: 0.397452, mean_q: 4.435614
 49038/100000: episode: 815, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 80.930, mean reward: 4.761 [3.408, 8.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.528], loss: 0.537319, mae: 0.488110, mean_q: 4.603147
 49072/100000: episode: 816, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 92.569, mean reward: 2.723 [2.025, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.073, 10.293], loss: 0.159399, mae: 0.390245, mean_q: 4.507395
 49077/100000: episode: 817, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 20.340, mean reward: 4.068 [3.823, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.564], loss: 0.250175, mae: 0.413618, mean_q: 4.549548
 49116/100000: episode: 818, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 90.261, mean reward: 2.314 [1.662, 5.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.081, 10.179], loss: 0.171616, mae: 0.394391, mean_q: 4.599640
 49133/100000: episode: 819, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 115.562, mean reward: 6.798 [3.597, 13.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.194, 10.502], loss: 0.188361, mae: 0.390328, mean_q: 4.584728
 49172/100000: episode: 820, duration: 0.233s, episode steps: 39, steps per second: 167, episode reward: 115.771, mean reward: 2.968 [1.812, 6.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.010, 10.318], loss: 0.352610, mae: 0.472476, mean_q: 4.607708
 49177/100000: episode: 821, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 24.974, mean reward: 4.995 [4.107, 6.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.061, 10.629], loss: 0.243715, mae: 0.398156, mean_q: 4.497902
 49216/100000: episode: 822, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 117.163, mean reward: 3.004 [1.800, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.966, 10.296], loss: 0.177185, mae: 0.396967, mean_q: 4.579674
 49250/100000: episode: 823, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 99.860, mean reward: 2.937 [2.298, 4.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.106, 10.435], loss: 0.246802, mae: 0.425315, mean_q: 4.583213
 49289/100000: episode: 824, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 107.448, mean reward: 2.755 [1.961, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.137, 10.391], loss: 0.202602, mae: 0.412656, mean_q: 4.624411
 49328/100000: episode: 825, duration: 0.249s, episode steps: 39, steps per second: 156, episode reward: 105.114, mean reward: 2.695 [1.742, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.469, 10.350], loss: 0.510332, mae: 0.515686, mean_q: 4.670046
 49333/100000: episode: 826, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 31.482, mean reward: 6.296 [4.307, 8.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.684], loss: 0.199002, mae: 0.400315, mean_q: 4.486166
 49338/100000: episode: 827, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 37.711, mean reward: 7.542 [5.062, 14.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.577], loss: 0.191541, mae: 0.410902, mean_q: 4.845704
 49343/100000: episode: 828, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 19.737, mean reward: 3.947 [3.487, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.425], loss: 0.134907, mae: 0.351560, mean_q: 4.690250
 49382/100000: episode: 829, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 125.290, mean reward: 3.213 [2.052, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.136, 10.391], loss: 0.364996, mae: 0.440217, mean_q: 4.709737
 49387/100000: episode: 830, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 30.109, mean reward: 6.022 [4.105, 9.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.689], loss: 0.223529, mae: 0.449486, mean_q: 4.613713
 49392/100000: episode: 831, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 24.028, mean reward: 4.806 [4.230, 6.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.579], loss: 0.136030, mae: 0.381429, mean_q: 4.714523
 49409/100000: episode: 832, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 100.118, mean reward: 5.889 [3.734, 11.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.699, 10.692], loss: 0.457742, mae: 0.449405, mean_q: 4.671427
 49414/100000: episode: 833, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 22.714, mean reward: 4.543 [4.220, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.501], loss: 0.452103, mae: 0.454290, mean_q: 4.624692
 49453/100000: episode: 834, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 113.440, mean reward: 2.909 [1.648, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.151, 10.325], loss: 0.236815, mae: 0.425875, mean_q: 4.658471
 49458/100000: episode: 835, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 25.478, mean reward: 5.096 [4.394, 6.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.602], loss: 0.202365, mae: 0.374265, mean_q: 4.661168
 49463/100000: episode: 836, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 79.860, mean reward: 15.972 [4.955, 46.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.159, 10.772], loss: 0.522855, mae: 0.482020, mean_q: 4.739085
[Info] FALSIFICATION!
 49467/100000: episode: 837, duration: 0.283s, episode steps: 4, steps per second: 14, episode reward: 1017.459, mean reward: 254.365 [4.137, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.616 [-0.195, 7.336], loss: 0.617289, mae: 0.569817, mean_q: 4.885290
 49506/100000: episode: 838, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 75.385, mean reward: 1.933 [1.534, 2.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.483, 10.100], loss: 1156.938721, mae: 5.389194, mean_q: 7.334260
 49545/100000: episode: 839, duration: 0.230s, episode steps: 39, steps per second: 169, episode reward: 92.369, mean reward: 2.368 [1.474, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.784, 10.100], loss: 1.416087, mae: 1.070979, mean_q: 4.730838
 49579/100000: episode: 840, duration: 0.202s, episode steps: 34, steps per second: 168, episode reward: 152.209, mean reward: 4.477 [2.711, 10.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.535, 10.618], loss: 0.580954, mae: 0.720194, mean_q: 4.460941
 49618/100000: episode: 841, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 114.609, mean reward: 2.939 [1.842, 5.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.123, 10.308], loss: 1.303263, mae: 0.709743, mean_q: 4.514491
 49655/100000: episode: 842, duration: 0.207s, episode steps: 37, steps per second: 178, episode reward: 115.396, mean reward: 3.119 [2.197, 5.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.324, 10.412], loss: 0.452073, mae: 0.635825, mean_q: 4.535634
 49694/100000: episode: 843, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 130.816, mean reward: 3.354 [2.021, 11.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.901, 10.360], loss: 0.575216, mae: 0.618523, mean_q: 4.546803
 49731/100000: episode: 844, duration: 0.191s, episode steps: 37, steps per second: 193, episode reward: 201.993, mean reward: 5.459 [2.110, 12.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.636, 10.618], loss: 0.459937, mae: 0.577239, mean_q: 4.592025
 49770/100000: episode: 845, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 137.620, mean reward: 3.529 [2.111, 5.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.130, 10.343], loss: 393.679016, mae: 1.856684, mean_q: 5.374973
 49775/100000: episode: 846, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 28.031, mean reward: 5.606 [4.509, 6.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.268, 10.564], loss: 3.084715, mae: 1.469899, mean_q: 5.486403
 49780/100000: episode: 847, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 27.866, mean reward: 5.573 [5.261, 6.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.597], loss: 2.150025, mae: 1.384948, mean_q: 4.921994
 49819/100000: episode: 848, duration: 0.223s, episode steps: 39, steps per second: 175, episode reward: 109.270, mean reward: 2.802 [1.430, 5.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.135, 10.100], loss: 1.110666, mae: 0.917413, mean_q: 4.484144
 49824/100000: episode: 849, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 29.437, mean reward: 5.887 [4.628, 6.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.539], loss: 0.965891, mae: 0.857173, mean_q: 4.799631
 49863/100000: episode: 850, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 125.071, mean reward: 3.207 [1.836, 6.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.394, 10.260], loss: 1.289182, mae: 0.754598, mean_q: 4.702033
 49902/100000: episode: 851, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 85.823, mean reward: 2.201 [1.511, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.354, 10.143], loss: 0.606625, mae: 0.727086, mean_q: 4.728266
 49941/100000: episode: 852, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 126.717, mean reward: 3.249 [2.089, 4.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.282, 10.317], loss: 767.716980, mae: 3.876956, mean_q: 6.836188
 49978/100000: episode: 853, duration: 0.242s, episode steps: 37, steps per second: 153, episode reward: 94.903, mean reward: 2.565 [1.500, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.376, 10.211], loss: 3.672683, mae: 1.650935, mean_q: 4.463562
 49995/100000: episode: 854, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 96.376, mean reward: 5.669 [3.713, 9.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.526], loss: 1.810424, mae: 1.102654, mean_q: 4.583813
 50032/100000: episode: 855, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 94.843, mean reward: 2.563 [2.114, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.035, 10.322], loss: 0.895686, mae: 0.864324, mean_q: 4.943083
 50071/100000: episode: 856, duration: 0.204s, episode steps: 39, steps per second: 192, episode reward: 126.436, mean reward: 3.242 [2.055, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.549, 10.487], loss: 1.626883, mae: 0.823491, mean_q: 5.028985
 50110/100000: episode: 857, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 101.345, mean reward: 2.599 [1.488, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.272, 10.209], loss: 390.250824, mae: 2.271996, mean_q: 6.004140
 50149/100000: episode: 858, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 162.890, mean reward: 4.177 [2.088, 8.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.448, 10.534], loss: 753.725891, mae: 2.701776, mean_q: 4.960744
 50186/100000: episode: 859, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 96.286, mean reward: 2.602 [2.032, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.826, 10.416], loss: 15.484054, mae: 2.890795, mean_q: 6.966997
 50223/100000: episode: 860, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 89.161, mean reward: 2.410 [1.759, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.038, 10.286], loss: 2.269298, mae: 1.257571, mean_q: 4.609340
 50259/100000: episode: 861, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 88.468, mean reward: 2.457 [1.547, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.226, 10.115], loss: 2.800778, mae: 1.046976, mean_q: 5.108689
 50298/100000: episode: 862, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 110.791, mean reward: 2.841 [2.116, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.224, 10.453], loss: 0.881741, mae: 0.806877, mean_q: 5.113313
 50303/100000: episode: 863, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 35.367, mean reward: 7.073 [4.627, 8.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.647], loss: 1.533965, mae: 0.860059, mean_q: 5.213151
 50339/100000: episode: 864, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 173.897, mean reward: 4.830 [2.427, 16.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.395, 10.383], loss: 0.992069, mae: 0.783081, mean_q: 5.142091
 50378/100000: episode: 865, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 81.451, mean reward: 2.088 [1.559, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.672, 10.259], loss: 0.818514, mae: 0.775955, mean_q: 5.257727
 50415/100000: episode: 866, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 79.793, mean reward: 2.157 [1.659, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.832, 10.292], loss: 407.551727, mae: 2.035249, mean_q: 5.817643
 50454/100000: episode: 867, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 107.218, mean reward: 2.749 [2.141, 4.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.457, 10.350], loss: 2.778660, mae: 1.077550, mean_q: 5.355857
 50491/100000: episode: 868, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 97.374, mean reward: 2.632 [2.103, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.035, 10.384], loss: 1.467942, mae: 0.876502, mean_q: 5.226705
 50530/100000: episode: 869, duration: 0.222s, episode steps: 39, steps per second: 175, episode reward: 109.872, mean reward: 2.817 [2.325, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.382, 10.439], loss: 382.537354, mae: 2.151225, mean_q: 6.047866
 50569/100000: episode: 870, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 92.372, mean reward: 2.369 [1.552, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.318, 10.210], loss: 743.243164, mae: 4.089756, mean_q: 7.376325
 50605/100000: episode: 871, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 108.298, mean reward: 3.008 [1.502, 15.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.082, 10.119], loss: 799.993347, mae: 3.733874, mean_q: 6.501858
 50639/100000: episode: 872, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 108.439, mean reward: 3.189 [2.259, 6.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.984, 10.488], loss: 12.330702, mae: 2.017977, mean_q: 6.405552
 50678/100000: episode: 873, duration: 0.226s, episode steps: 39, steps per second: 173, episode reward: 166.331, mean reward: 4.265 [2.271, 12.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.366, 10.383], loss: 3.493579, mae: 1.192788, mean_q: 5.300028
 50717/100000: episode: 874, duration: 0.239s, episode steps: 39, steps per second: 163, episode reward: 100.961, mean reward: 2.589 [1.802, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.109, 10.346], loss: 1.978998, mae: 1.058416, mean_q: 5.574748
 50734/100000: episode: 875, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 66.494, mean reward: 3.911 [3.143, 5.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.723, 10.463], loss: 2.455411, mae: 1.000821, mean_q: 5.569344
 50773/100000: episode: 876, duration: 0.373s, episode steps: 39, steps per second: 105, episode reward: 88.643, mean reward: 2.273 [1.668, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.313, 10.158], loss: 1.936315, mae: 0.924429, mean_q: 5.558473
 50778/100000: episode: 877, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 17.058, mean reward: 3.412 [3.039, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.887, 10.492], loss: 0.656134, mae: 0.790779, mean_q: 5.274381
 50814/100000: episode: 878, duration: 0.311s, episode steps: 36, steps per second: 116, episode reward: 93.472, mean reward: 2.596 [2.075, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.053, 10.375], loss: 0.878936, mae: 0.743199, mean_q: 5.318459
 50850/100000: episode: 879, duration: 0.326s, episode steps: 36, steps per second: 111, episode reward: 95.988, mean reward: 2.666 [2.205, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.191, 10.387], loss: 418.590881, mae: 2.247948, mean_q: 6.229200
 50886/100000: episode: 880, duration: 0.240s, episode steps: 36, steps per second: 150, episode reward: 112.005, mean reward: 3.111 [2.264, 5.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.726, 10.463], loss: 2.219311, mae: 0.941551, mean_q: 5.215857
 50925/100000: episode: 881, duration: 0.282s, episode steps: 39, steps per second: 138, episode reward: 128.580, mean reward: 3.297 [1.869, 5.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.111, 10.490], loss: 377.447632, mae: 1.653258, mean_q: 5.508110
 50962/100000: episode: 882, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 93.388, mean reward: 2.524 [1.888, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.384, 10.311], loss: 4.930416, mae: 1.487020, mean_q: 6.317857
 50967/100000: episode: 883, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 36.487, mean reward: 7.297 [4.280, 13.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.752, 10.575], loss: 3.342188, mae: 1.125595, mean_q: 5.476995
 51004/100000: episode: 884, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 111.539, mean reward: 3.015 [2.066, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.794, 10.345], loss: 2.110558, mae: 0.900560, mean_q: 5.327005
 51043/100000: episode: 885, duration: 0.230s, episode steps: 39, steps per second: 170, episode reward: 96.291, mean reward: 2.469 [1.458, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.879, 10.145], loss: 380.868042, mae: 2.226196, mean_q: 6.309981
 51080/100000: episode: 886, duration: 0.256s, episode steps: 37, steps per second: 145, episode reward: 135.057, mean reward: 3.650 [2.273, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.762, 10.565], loss: 5.213890, mae: 1.290758, mean_q: 5.866658
 51119/100000: episode: 887, duration: 0.260s, episode steps: 39, steps per second: 150, episode reward: 83.119, mean reward: 2.131 [1.522, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.977, 10.307], loss: 1.593836, mae: 0.913146, mean_q: 5.442328
 51136/100000: episode: 888, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 63.687, mean reward: 3.746 [2.893, 5.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.585, 10.435], loss: 4.282997, mae: 1.024521, mean_q: 5.632982
 51175/100000: episode: 889, duration: 0.236s, episode steps: 39, steps per second: 165, episode reward: 74.886, mean reward: 1.920 [1.584, 2.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.240, 10.100], loss: 2.277761, mae: 0.942879, mean_q: 5.550988
[Info] Complete ISplit Iteration
[Info] Levels: [5.1809473, 6.544283, 46.257175]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 51209/100000: episode: 890, duration: 4.891s, episode steps: 34, steps per second: 7, episode reward: 101.243, mean reward: 2.978 [1.787, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.252, 10.254], loss: 434.670349, mae: 1.788367, mean_q: 5.584576
 51309/100000: episode: 891, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 188.131, mean reward: 1.881 [1.455, 2.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.257, 10.098], loss: 3.229039, mae: 1.034044, mean_q: 5.650659
 51409/100000: episode: 892, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 191.802, mean reward: 1.918 [1.495, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.298], loss: 150.744370, mae: 1.414301, mean_q: 5.788032
 51509/100000: episode: 893, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 188.965, mean reward: 1.890 [1.483, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.049, 10.345], loss: 1.194595, mae: 0.843337, mean_q: 5.414396
 51609/100000: episode: 894, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 184.689, mean reward: 1.847 [1.441, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.356, 10.098], loss: 1.747844, mae: 0.796967, mean_q: 5.390202
 51709/100000: episode: 895, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 218.762, mean reward: 2.188 [1.479, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.098], loss: 0.699441, mae: 0.698846, mean_q: 5.275721
 51809/100000: episode: 896, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 190.949, mean reward: 1.909 [1.451, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.939, 10.170], loss: 153.920593, mae: 1.228682, mean_q: 5.573601
 51909/100000: episode: 897, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 205.278, mean reward: 2.053 [1.477, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.098], loss: 0.905735, mae: 0.754938, mean_q: 5.280423
 52009/100000: episode: 898, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 212.992, mean reward: 2.130 [1.523, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.098], loss: 298.941406, mae: 1.837883, mean_q: 5.813623
 52109/100000: episode: 899, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 189.467, mean reward: 1.895 [1.452, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.358, 10.098], loss: 3.304653, mae: 1.161327, mean_q: 5.495962
 52209/100000: episode: 900, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 207.749, mean reward: 2.077 [1.447, 5.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.233, 10.446], loss: 1.591588, mae: 0.846925, mean_q: 5.348218
 52309/100000: episode: 901, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 181.053, mean reward: 1.811 [1.459, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.098], loss: 150.383041, mae: 1.301566, mean_q: 5.602921
 52409/100000: episode: 902, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 185.194, mean reward: 1.852 [1.498, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.828, 10.098], loss: 1.419716, mae: 0.856987, mean_q: 5.151904
 52509/100000: episode: 903, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 191.122, mean reward: 1.911 [1.440, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.580, 10.098], loss: 149.811462, mae: 1.206419, mean_q: 5.394865
 52609/100000: episode: 904, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 177.656, mean reward: 1.777 [1.448, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.221, 10.098], loss: 2.267223, mae: 0.982675, mean_q: 5.240734
 52709/100000: episode: 905, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 195.894, mean reward: 1.959 [1.541, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.125, 10.229], loss: 1.124868, mae: 0.735142, mean_q: 5.108049
 52809/100000: episode: 906, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 187.702, mean reward: 1.877 [1.485, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.869, 10.136], loss: 294.992645, mae: 1.957290, mean_q: 5.827629
 52909/100000: episode: 907, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 235.595, mean reward: 2.356 [1.455, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.428, 10.351], loss: 290.314789, mae: 1.853262, mean_q: 5.571723
 53009/100000: episode: 908, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 193.932, mean reward: 1.939 [1.466, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.845, 10.294], loss: 288.243042, mae: 2.037182, mean_q: 6.029716
 53109/100000: episode: 909, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 178.512, mean reward: 1.785 [1.448, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.724, 10.151], loss: 289.369751, mae: 2.296747, mean_q: 6.110851
 53209/100000: episode: 910, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 193.206, mean reward: 1.932 [1.474, 3.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.905, 10.145], loss: 282.250061, mae: 2.352042, mean_q: 6.224699
 53309/100000: episode: 911, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 192.583, mean reward: 1.926 [1.453, 4.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.098], loss: 146.296066, mae: 1.524430, mean_q: 5.555497
 53409/100000: episode: 912, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 211.399, mean reward: 2.114 [1.479, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.098], loss: 6.959759, mae: 1.274531, mean_q: 5.537851
 53509/100000: episode: 913, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 197.829, mean reward: 1.978 [1.464, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.613, 10.098], loss: 148.485107, mae: 1.557755, mean_q: 5.648501
 53609/100000: episode: 914, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.024, mean reward: 1.880 [1.532, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.144, 10.098], loss: 2.821750, mae: 0.848736, mean_q: 5.223740
 53709/100000: episode: 915, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.661, mean reward: 1.817 [1.440, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.771, 10.098], loss: 1.750885, mae: 0.782712, mean_q: 5.128294
 53809/100000: episode: 916, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 196.606, mean reward: 1.966 [1.482, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.467, 10.098], loss: 286.544617, mae: 1.746652, mean_q: 5.522326
 53909/100000: episode: 917, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 192.975, mean reward: 1.930 [1.474, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.995, 10.098], loss: 145.628113, mae: 1.548020, mean_q: 5.479200
 54009/100000: episode: 918, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 195.465, mean reward: 1.955 [1.442, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.920, 10.098], loss: 145.966949, mae: 1.358575, mean_q: 5.138681
 54109/100000: episode: 919, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 197.846, mean reward: 1.978 [1.471, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.994, 10.437], loss: 144.873077, mae: 1.856489, mean_q: 5.619455
 54209/100000: episode: 920, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 196.065, mean reward: 1.961 [1.485, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.836, 10.098], loss: 142.871262, mae: 1.294798, mean_q: 5.226399
 54309/100000: episode: 921, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 198.544, mean reward: 1.985 [1.481, 7.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.187], loss: 143.422134, mae: 1.478476, mean_q: 5.337932
 54409/100000: episode: 922, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 196.350, mean reward: 1.964 [1.444, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.386, 10.098], loss: 142.499146, mae: 1.363267, mean_q: 5.046454
 54509/100000: episode: 923, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 185.095, mean reward: 1.851 [1.485, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.660, 10.098], loss: 4.954131, mae: 0.876319, mean_q: 4.812820
 54609/100000: episode: 924, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 191.539, mean reward: 1.915 [1.489, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.331], loss: 2.258886, mae: 0.745493, mean_q: 4.676509
 54709/100000: episode: 925, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 203.360, mean reward: 2.034 [1.433, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.361, 10.268], loss: 1.276257, mae: 0.590380, mean_q: 4.520132
 54809/100000: episode: 926, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 194.433, mean reward: 1.944 [1.483, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.862, 10.158], loss: 1.924567, mae: 0.621356, mean_q: 4.514796
 54909/100000: episode: 927, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 189.348, mean reward: 1.893 [1.447, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.682, 10.155], loss: 0.583952, mae: 0.536495, mean_q: 4.379774
 55009/100000: episode: 928, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 216.175, mean reward: 2.162 [1.490, 4.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.614, 10.098], loss: 0.627138, mae: 0.515056, mean_q: 4.331643
 55109/100000: episode: 929, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.378, mean reward: 1.934 [1.440, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.740, 10.404], loss: 0.864094, mae: 0.518215, mean_q: 4.339458
 55209/100000: episode: 930, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 193.496, mean reward: 1.935 [1.456, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.206], loss: 0.505072, mae: 0.486491, mean_q: 4.258254
 55309/100000: episode: 931, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 177.731, mean reward: 1.777 [1.469, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.583, 10.098], loss: 0.363783, mae: 0.459750, mean_q: 4.200808
 55409/100000: episode: 932, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 189.982, mean reward: 1.900 [1.451, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.574, 10.111], loss: 0.872174, mae: 0.466793, mean_q: 4.219956
 55509/100000: episode: 933, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 186.482, mean reward: 1.865 [1.452, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.043, 10.267], loss: 0.517630, mae: 0.446100, mean_q: 4.165451
 55609/100000: episode: 934, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 185.898, mean reward: 1.859 [1.453, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.098], loss: 0.430669, mae: 0.429383, mean_q: 4.108748
 55709/100000: episode: 935, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 189.353, mean reward: 1.894 [1.468, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.400, 10.108], loss: 0.155626, mae: 0.389662, mean_q: 4.045074
 55809/100000: episode: 936, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 219.360, mean reward: 2.194 [1.460, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.083, 10.410], loss: 0.371557, mae: 0.413886, mean_q: 4.046291
 55909/100000: episode: 937, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 180.403, mean reward: 1.804 [1.454, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.057, 10.125], loss: 0.231552, mae: 0.394837, mean_q: 3.997300
 56009/100000: episode: 938, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 196.816, mean reward: 1.968 [1.447, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.766, 10.098], loss: 0.143161, mae: 0.361905, mean_q: 3.930757
 56109/100000: episode: 939, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.048, mean reward: 1.920 [1.482, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.512, 10.098], loss: 0.131378, mae: 0.352485, mean_q: 3.894689
 56209/100000: episode: 940, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 213.696, mean reward: 2.137 [1.502, 5.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.565, 10.266], loss: 0.132740, mae: 0.348829, mean_q: 3.879011
 56309/100000: episode: 941, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 183.450, mean reward: 1.834 [1.458, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.098], loss: 0.129228, mae: 0.349577, mean_q: 3.851196
 56409/100000: episode: 942, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 204.605, mean reward: 2.046 [1.451, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.114, 10.098], loss: 0.128174, mae: 0.347813, mean_q: 3.865918
 56509/100000: episode: 943, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 211.170, mean reward: 2.112 [1.519, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.320, 10.112], loss: 0.133448, mae: 0.352296, mean_q: 3.863819
 56609/100000: episode: 944, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.892, mean reward: 1.979 [1.450, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.233, 10.098], loss: 0.127380, mae: 0.354669, mean_q: 3.883770
 56709/100000: episode: 945, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 196.723, mean reward: 1.967 [1.524, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.937, 10.098], loss: 0.126670, mae: 0.353709, mean_q: 3.873386
 56809/100000: episode: 946, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 194.141, mean reward: 1.941 [1.455, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.673, 10.098], loss: 0.117669, mae: 0.342013, mean_q: 3.870098
 56909/100000: episode: 947, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 198.574, mean reward: 1.986 [1.444, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.829, 10.098], loss: 0.111338, mae: 0.330401, mean_q: 3.848202
 57009/100000: episode: 948, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 213.604, mean reward: 2.136 [1.492, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.885, 10.128], loss: 0.121500, mae: 0.338342, mean_q: 3.846745
 57109/100000: episode: 949, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.930, mean reward: 1.869 [1.471, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.617, 10.194], loss: 0.132584, mae: 0.351479, mean_q: 3.859503
 57209/100000: episode: 950, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 187.487, mean reward: 1.875 [1.499, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.441, 10.098], loss: 0.131838, mae: 0.346175, mean_q: 3.879968
 57309/100000: episode: 951, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.392, mean reward: 1.914 [1.439, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.098], loss: 0.137144, mae: 0.342872, mean_q: 3.864735
 57409/100000: episode: 952, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 193.929, mean reward: 1.939 [1.514, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.208], loss: 0.119210, mae: 0.336715, mean_q: 3.859315
 57509/100000: episode: 953, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 208.197, mean reward: 2.082 [1.471, 3.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.457], loss: 0.125600, mae: 0.342344, mean_q: 3.847067
 57609/100000: episode: 954, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 186.614, mean reward: 1.866 [1.474, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.451, 10.098], loss: 0.114769, mae: 0.335218, mean_q: 3.879537
 57709/100000: episode: 955, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 190.563, mean reward: 1.906 [1.445, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.616, 10.118], loss: 0.104519, mae: 0.321783, mean_q: 3.866378
 57809/100000: episode: 956, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 216.047, mean reward: 2.160 [1.456, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.116, 10.098], loss: 0.129201, mae: 0.352261, mean_q: 3.874642
 57909/100000: episode: 957, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.672, mean reward: 1.977 [1.453, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.269, 10.231], loss: 0.118302, mae: 0.340978, mean_q: 3.855791
 58009/100000: episode: 958, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.428, mean reward: 1.914 [1.455, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.219], loss: 0.120108, mae: 0.339418, mean_q: 3.867820
 58109/100000: episode: 959, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 182.185, mean reward: 1.822 [1.440, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.007, 10.138], loss: 0.131597, mae: 0.340827, mean_q: 3.853850
 58209/100000: episode: 960, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 207.941, mean reward: 2.079 [1.450, 5.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.385, 10.347], loss: 0.107521, mae: 0.331125, mean_q: 3.872255
 58309/100000: episode: 961, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.446, mean reward: 1.914 [1.507, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.289], loss: 0.115761, mae: 0.331089, mean_q: 3.870963
 58409/100000: episode: 962, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 197.917, mean reward: 1.979 [1.460, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.648, 10.098], loss: 0.113188, mae: 0.333143, mean_q: 3.868510
 58509/100000: episode: 963, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 188.519, mean reward: 1.885 [1.481, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.235, 10.175], loss: 0.111334, mae: 0.329150, mean_q: 3.871305
 58609/100000: episode: 964, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 181.190, mean reward: 1.812 [1.455, 2.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.252], loss: 0.107973, mae: 0.325049, mean_q: 3.867992
 58709/100000: episode: 965, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 189.489, mean reward: 1.895 [1.518, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.402, 10.267], loss: 0.104606, mae: 0.323246, mean_q: 3.834157
 58809/100000: episode: 966, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 180.916, mean reward: 1.809 [1.437, 2.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.158, 10.153], loss: 0.121977, mae: 0.333269, mean_q: 3.862388
 58909/100000: episode: 967, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 240.530, mean reward: 2.405 [1.477, 6.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.331, 10.269], loss: 0.105930, mae: 0.328560, mean_q: 3.862911
 59009/100000: episode: 968, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 175.563, mean reward: 1.756 [1.468, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.036, 10.198], loss: 0.115143, mae: 0.330189, mean_q: 3.873305
 59109/100000: episode: 969, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 209.395, mean reward: 2.094 [1.465, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.542, 10.152], loss: 0.110054, mae: 0.316711, mean_q: 3.869625
 59209/100000: episode: 970, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 233.563, mean reward: 2.336 [1.436, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.784, 10.422], loss: 0.125352, mae: 0.338656, mean_q: 3.891158
 59309/100000: episode: 971, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 192.239, mean reward: 1.922 [1.478, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.713, 10.098], loss: 0.110874, mae: 0.328560, mean_q: 3.883260
 59409/100000: episode: 972, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.175, mean reward: 1.942 [1.455, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.541, 10.098], loss: 0.109285, mae: 0.320229, mean_q: 3.880770
 59509/100000: episode: 973, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.836, mean reward: 1.958 [1.445, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.349, 10.293], loss: 0.122979, mae: 0.347011, mean_q: 3.907996
 59609/100000: episode: 974, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 210.257, mean reward: 2.103 [1.486, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.611, 10.098], loss: 0.120591, mae: 0.334740, mean_q: 3.893053
 59709/100000: episode: 975, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 198.608, mean reward: 1.986 [1.473, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.672, 10.189], loss: 0.103587, mae: 0.316290, mean_q: 3.880905
 59809/100000: episode: 976, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 206.328, mean reward: 2.063 [1.469, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.809, 10.460], loss: 0.117880, mae: 0.333035, mean_q: 3.874428
 59909/100000: episode: 977, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.139, mean reward: 1.971 [1.450, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.194], loss: 0.115769, mae: 0.339503, mean_q: 3.904441
 60009/100000: episode: 978, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 184.717, mean reward: 1.847 [1.433, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.110, 10.219], loss: 0.113126, mae: 0.331117, mean_q: 3.901547
 60109/100000: episode: 979, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 202.058, mean reward: 2.021 [1.469, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.541, 10.236], loss: 0.102675, mae: 0.316419, mean_q: 3.875831
 60209/100000: episode: 980, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 177.764, mean reward: 1.778 [1.457, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.268], loss: 0.114900, mae: 0.332545, mean_q: 3.879979
 60309/100000: episode: 981, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.452, mean reward: 1.865 [1.452, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.950, 10.098], loss: 0.107935, mae: 0.335012, mean_q: 3.893661
 60409/100000: episode: 982, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.203, mean reward: 1.942 [1.500, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.098], loss: 0.110215, mae: 0.325822, mean_q: 3.894968
 60509/100000: episode: 983, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 199.470, mean reward: 1.995 [1.492, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.414, 10.413], loss: 0.114414, mae: 0.331452, mean_q: 3.887897
 60609/100000: episode: 984, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 194.862, mean reward: 1.949 [1.441, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.098], loss: 0.117847, mae: 0.339915, mean_q: 3.905398
 60709/100000: episode: 985, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 185.296, mean reward: 1.853 [1.436, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.631, 10.098], loss: 0.110342, mae: 0.327457, mean_q: 3.886071
 60809/100000: episode: 986, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 197.602, mean reward: 1.976 [1.495, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.594, 10.327], loss: 0.113885, mae: 0.331227, mean_q: 3.888681
 60909/100000: episode: 987, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.484, mean reward: 1.855 [1.449, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.615, 10.098], loss: 0.104386, mae: 0.321861, mean_q: 3.884077
 61009/100000: episode: 988, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 192.164, mean reward: 1.922 [1.453, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.171], loss: 0.107786, mae: 0.325173, mean_q: 3.887611
 61109/100000: episode: 989, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.657, mean reward: 2.077 [1.450, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.860, 10.098], loss: 0.115470, mae: 0.338489, mean_q: 3.913353
[Info] 1-TH LEVEL FOUND: 5.759289741516113, Considering 10/90 traces
 61209/100000: episode: 990, duration: 4.993s, episode steps: 100, steps per second: 20, episode reward: 237.085, mean reward: 2.371 [1.440, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.023, 10.098], loss: 0.115097, mae: 0.336385, mean_q: 3.898506
 61245/100000: episode: 991, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 111.972, mean reward: 3.110 [2.105, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.299, 10.461], loss: 0.113013, mae: 0.332136, mean_q: 3.929782
 61267/100000: episode: 992, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 50.207, mean reward: 2.282 [1.930, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.320], loss: 0.138307, mae: 0.366045, mean_q: 3.956704
 61288/100000: episode: 993, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 54.743, mean reward: 2.607 [2.201, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.505, 10.296], loss: 0.117634, mae: 0.335369, mean_q: 3.903238
 61309/100000: episode: 994, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 45.683, mean reward: 2.175 [1.578, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.098, 10.237], loss: 0.125153, mae: 0.345908, mean_q: 3.973269
 61331/100000: episode: 995, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 76.463, mean reward: 3.476 [2.699, 4.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.478, 10.405], loss: 0.118846, mae: 0.338046, mean_q: 3.926123
 61426/100000: episode: 996, duration: 0.545s, episode steps: 95, steps per second: 174, episode reward: 187.238, mean reward: 1.971 [1.481, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.865, 10.326], loss: 0.136327, mae: 0.358095, mean_q: 3.967480
 61446/100000: episode: 997, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 44.191, mean reward: 2.210 [1.825, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.378, 10.298], loss: 0.140784, mae: 0.365337, mean_q: 3.968400
 61467/100000: episode: 998, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 55.169, mean reward: 2.627 [2.231, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.459], loss: 0.122537, mae: 0.341964, mean_q: 3.930362
 61487/100000: episode: 999, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 61.389, mean reward: 3.069 [2.481, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.344, 10.448], loss: 0.144197, mae: 0.368486, mean_q: 3.986945
 61509/100000: episode: 1000, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 51.199, mean reward: 2.327 [2.147, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.149, 10.345], loss: 0.116187, mae: 0.353424, mean_q: 3.938119
 61531/100000: episode: 1001, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 77.063, mean reward: 3.503 [2.638, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.170, 10.521], loss: 0.122039, mae: 0.329307, mean_q: 3.947456
 61566/100000: episode: 1002, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 70.008, mean reward: 2.000 [1.576, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.412, 10.238], loss: 0.152456, mae: 0.373022, mean_q: 3.990703
 61593/100000: episode: 1003, duration: 0.163s, episode steps: 27, steps per second: 165, episode reward: 49.405, mean reward: 1.830 [1.467, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.567, 10.100], loss: 0.112063, mae: 0.340758, mean_q: 3.948808
 61628/100000: episode: 1004, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 82.164, mean reward: 2.348 [1.580, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.702, 10.318], loss: 0.144966, mae: 0.364656, mean_q: 3.950742
 61648/100000: episode: 1005, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 61.403, mean reward: 3.070 [2.182, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.279, 10.469], loss: 0.108527, mae: 0.326267, mean_q: 3.966784
 61670/100000: episode: 1006, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 79.046, mean reward: 3.593 [2.510, 8.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.306, 10.404], loss: 0.137122, mae: 0.357581, mean_q: 3.959950
 61692/100000: episode: 1007, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 69.406, mean reward: 3.155 [2.098, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.794, 10.409], loss: 0.141832, mae: 0.378791, mean_q: 4.048615
 61719/100000: episode: 1008, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 63.302, mean reward: 2.345 [1.634, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.406, 10.299], loss: 0.141627, mae: 0.356942, mean_q: 3.971378
 61740/100000: episode: 1009, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 82.336, mean reward: 3.921 [2.711, 6.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.437, 10.573], loss: 0.150306, mae: 0.375581, mean_q: 4.043159
 61835/100000: episode: 1010, duration: 0.484s, episode steps: 95, steps per second: 196, episode reward: 176.807, mean reward: 1.861 [1.440, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-2.414, 10.197], loss: 0.131182, mae: 0.350913, mean_q: 4.004114
 61857/100000: episode: 1011, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 65.349, mean reward: 2.970 [1.938, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.350], loss: 0.117138, mae: 0.319344, mean_q: 3.989264
 61893/100000: episode: 1012, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 85.570, mean reward: 2.377 [1.650, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.277], loss: 0.142732, mae: 0.366974, mean_q: 4.027169
 61924/100000: episode: 1013, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 95.408, mean reward: 3.078 [2.232, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.378, 10.509], loss: 0.136283, mae: 0.362340, mean_q: 4.039735
 61955/100000: episode: 1014, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 60.838, mean reward: 1.963 [1.493, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.236, 10.115], loss: 0.145767, mae: 0.368335, mean_q: 4.011584
 61982/100000: episode: 1015, duration: 0.168s, episode steps: 27, steps per second: 160, episode reward: 55.362, mean reward: 2.050 [1.568, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.267, 10.100], loss: 0.137637, mae: 0.351697, mean_q: 3.992179
 62009/100000: episode: 1016, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 54.111, mean reward: 2.004 [1.667, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.331, 10.215], loss: 0.120649, mae: 0.340294, mean_q: 4.024687
 62036/100000: episode: 1017, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 63.286, mean reward: 2.344 [1.741, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.401, 10.370], loss: 0.134639, mae: 0.364273, mean_q: 4.005098
 62063/100000: episode: 1018, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 61.516, mean reward: 2.278 [1.564, 10.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.588, 10.139], loss: 0.132633, mae: 0.347915, mean_q: 4.009521
 62090/100000: episode: 1019, duration: 0.160s, episode steps: 27, steps per second: 168, episode reward: 57.636, mean reward: 2.135 [1.664, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.496, 10.177], loss: 0.150320, mae: 0.369155, mean_q: 4.076434
 62185/100000: episode: 1020, duration: 0.467s, episode steps: 95, steps per second: 203, episode reward: 202.806, mean reward: 2.135 [1.465, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.871, 10.100], loss: 0.139042, mae: 0.362193, mean_q: 4.048906
 62216/100000: episode: 1021, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 69.832, mean reward: 2.253 [1.893, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.709, 10.306], loss: 0.122849, mae: 0.346945, mean_q: 4.008206
 62238/100000: episode: 1022, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 62.741, mean reward: 2.852 [1.712, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.864, 10.235], loss: 0.151920, mae: 0.385762, mean_q: 4.016881
 62259/100000: episode: 1023, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 66.988, mean reward: 3.190 [2.720, 4.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.434], loss: 0.134932, mae: 0.373733, mean_q: 4.059061
 62281/100000: episode: 1024, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 72.338, mean reward: 3.288 [2.707, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.116, 10.433], loss: 0.120579, mae: 0.360453, mean_q: 4.062689
 62303/100000: episode: 1025, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 108.341, mean reward: 4.925 [3.333, 7.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.639], loss: 0.124401, mae: 0.354668, mean_q: 4.056224
 62330/100000: episode: 1026, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 69.382, mean reward: 2.570 [1.868, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.244, 10.332], loss: 0.127190, mae: 0.355277, mean_q: 4.104871
 62351/100000: episode: 1027, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 62.854, mean reward: 2.993 [2.278, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.587, 10.404], loss: 0.161087, mae: 0.363276, mean_q: 4.107763
 62373/100000: episode: 1028, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 51.819, mean reward: 2.355 [1.848, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.192, 10.326], loss: 0.147506, mae: 0.370635, mean_q: 4.138305
 62404/100000: episode: 1029, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 83.181, mean reward: 2.683 [1.935, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.047, 10.253], loss: 0.154882, mae: 0.376873, mean_q: 4.118686
 62425/100000: episode: 1030, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 59.902, mean reward: 2.852 [2.223, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.411], loss: 0.176198, mae: 0.387232, mean_q: 4.116999
 62452/100000: episode: 1031, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 62.619, mean reward: 2.319 [1.839, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.196, 10.290], loss: 0.145534, mae: 0.367968, mean_q: 4.103717
 62487/100000: episode: 1032, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 75.625, mean reward: 2.161 [1.612, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.379, 10.172], loss: 0.199116, mae: 0.399066, mean_q: 4.146271
 62509/100000: episode: 1033, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 60.748, mean reward: 2.761 [2.162, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.203, 10.434], loss: 0.164918, mae: 0.390557, mean_q: 4.132005
 62530/100000: episode: 1034, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 61.189, mean reward: 2.914 [1.570, 6.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.642, 10.191], loss: 0.164281, mae: 0.386220, mean_q: 4.156019
 62550/100000: episode: 1035, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 51.999, mean reward: 2.600 [2.188, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.134, 10.403], loss: 0.129384, mae: 0.367459, mean_q: 4.157825
 62572/100000: episode: 1036, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 57.899, mean reward: 2.632 [2.106, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.377], loss: 0.169886, mae: 0.405223, mean_q: 4.168901
 62599/100000: episode: 1037, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 62.950, mean reward: 2.331 [1.498, 5.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.744, 10.163], loss: 0.141397, mae: 0.387771, mean_q: 4.193053
 62621/100000: episode: 1038, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 57.004, mean reward: 2.591 [1.803, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.101, 10.370], loss: 0.139460, mae: 0.357532, mean_q: 4.117894
 62642/100000: episode: 1039, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 54.664, mean reward: 2.603 [2.055, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.027, 10.402], loss: 0.147797, mae: 0.370250, mean_q: 4.204005
 62669/100000: episode: 1040, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 71.357, mean reward: 2.643 [1.784, 6.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.822, 10.287], loss: 0.142382, mae: 0.373330, mean_q: 4.178239
 62700/100000: episode: 1041, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 75.199, mean reward: 2.426 [1.796, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.927, 10.314], loss: 0.149449, mae: 0.365322, mean_q: 4.148462
 62722/100000: episode: 1042, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 70.845, mean reward: 3.220 [2.346, 5.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.228, 10.405], loss: 0.158267, mae: 0.355788, mean_q: 4.189063
 62744/100000: episode: 1043, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 64.494, mean reward: 2.932 [2.212, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.157, 10.381], loss: 0.152992, mae: 0.381934, mean_q: 4.180992
 62780/100000: episode: 1044, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 77.867, mean reward: 2.163 [1.537, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.742, 10.138], loss: 0.147364, mae: 0.368445, mean_q: 4.218519
 62816/100000: episode: 1045, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 90.495, mean reward: 2.514 [1.915, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.556, 10.331], loss: 0.145263, mae: 0.364769, mean_q: 4.217543
 62847/100000: episode: 1046, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 70.626, mean reward: 2.278 [1.685, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.567, 10.215], loss: 0.131149, mae: 0.348694, mean_q: 4.209249
 62869/100000: episode: 1047, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 57.511, mean reward: 2.614 [2.017, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.451, 10.449], loss: 0.154056, mae: 0.376300, mean_q: 4.272434
 62905/100000: episode: 1048, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 80.842, mean reward: 2.246 [1.695, 4.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.429, 10.230], loss: 0.179414, mae: 0.375124, mean_q: 4.207663
 62927/100000: episode: 1049, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 56.678, mean reward: 2.576 [1.965, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.163, 10.455], loss: 0.160388, mae: 0.373261, mean_q: 4.185813
 62958/100000: episode: 1050, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 84.679, mean reward: 2.732 [2.064, 6.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.882, 10.463], loss: 0.183612, mae: 0.397998, mean_q: 4.319870
 62989/100000: episode: 1051, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 71.611, mean reward: 2.310 [1.523, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.361, 10.167], loss: 0.141088, mae: 0.379117, mean_q: 4.224973
 63016/100000: episode: 1052, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 50.196, mean reward: 1.859 [1.499, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.758, 10.100], loss: 0.161495, mae: 0.384590, mean_q: 4.293316
 63043/100000: episode: 1053, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 56.380, mean reward: 2.088 [1.517, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.585, 10.258], loss: 0.139271, mae: 0.353977, mean_q: 4.222984
 63087/100000: episode: 1054, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 83.603, mean reward: 1.900 [1.499, 2.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.360, 10.333], loss: 0.141324, mae: 0.368889, mean_q: 4.231235
 63109/100000: episode: 1055, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 59.180, mean reward: 2.690 [2.355, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.097, 10.430], loss: 0.152128, mae: 0.374638, mean_q: 4.244374
 63204/100000: episode: 1056, duration: 0.528s, episode steps: 95, steps per second: 180, episode reward: 180.344, mean reward: 1.898 [1.525, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.237, 10.223], loss: 0.122806, mae: 0.350292, mean_q: 4.211679
 63248/100000: episode: 1057, duration: 0.259s, episode steps: 44, steps per second: 170, episode reward: 116.215, mean reward: 2.641 [1.768, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.307, 10.521], loss: 0.168960, mae: 0.390865, mean_q: 4.263777
 63270/100000: episode: 1058, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 49.765, mean reward: 2.262 [1.793, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.878, 10.358], loss: 0.124138, mae: 0.341457, mean_q: 4.216590
 63365/100000: episode: 1059, duration: 0.613s, episode steps: 95, steps per second: 155, episode reward: 172.374, mean reward: 1.814 [1.445, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.566, 10.199], loss: 0.174625, mae: 0.381060, mean_q: 4.246354
 63385/100000: episode: 1060, duration: 0.141s, episode steps: 20, steps per second: 142, episode reward: 60.762, mean reward: 3.038 [2.293, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.960, 10.356], loss: 0.179293, mae: 0.415183, mean_q: 4.308875
 63412/100000: episode: 1061, duration: 0.236s, episode steps: 27, steps per second: 114, episode reward: 56.145, mean reward: 2.079 [1.709, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.333], loss: 0.174816, mae: 0.376795, mean_q: 4.255055
 63507/100000: episode: 1062, duration: 0.742s, episode steps: 95, steps per second: 128, episode reward: 179.941, mean reward: 1.894 [1.475, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.536, 10.251], loss: 0.153584, mae: 0.370389, mean_q: 4.245358
 63529/100000: episode: 1063, duration: 0.148s, episode steps: 22, steps per second: 149, episode reward: 92.333, mean reward: 4.197 [2.995, 7.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.077, 10.471], loss: 0.145739, mae: 0.373349, mean_q: 4.264251
 63551/100000: episode: 1064, duration: 0.151s, episode steps: 22, steps per second: 146, episode reward: 100.589, mean reward: 4.572 [2.817, 17.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.770, 10.511], loss: 0.189628, mae: 0.390731, mean_q: 4.218808
 63573/100000: episode: 1065, duration: 0.206s, episode steps: 22, steps per second: 107, episode reward: 73.226, mean reward: 3.328 [2.177, 9.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.070, 10.567], loss: 0.160024, mae: 0.384670, mean_q: 4.261120
 63608/100000: episode: 1066, duration: 0.289s, episode steps: 35, steps per second: 121, episode reward: 82.249, mean reward: 2.350 [1.669, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.423, 10.225], loss: 0.207673, mae: 0.394707, mean_q: 4.347342
 63635/100000: episode: 1067, duration: 0.166s, episode steps: 27, steps per second: 162, episode reward: 60.226, mean reward: 2.231 [1.726, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.950, 10.386], loss: 0.213681, mae: 0.395694, mean_q: 4.255840
 63662/100000: episode: 1068, duration: 0.175s, episode steps: 27, steps per second: 154, episode reward: 54.759, mean reward: 2.028 [1.554, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.379, 10.319], loss: 0.200025, mae: 0.407340, mean_q: 4.326577
 63682/100000: episode: 1069, duration: 0.127s, episode steps: 20, steps per second: 158, episode reward: 56.444, mean reward: 2.822 [2.386, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.005, 10.427], loss: 0.267288, mae: 0.406831, mean_q: 4.292371
 63709/100000: episode: 1070, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 51.365, mean reward: 1.902 [1.549, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.531, 10.127], loss: 0.177356, mae: 0.394504, mean_q: 4.319941
 63745/100000: episode: 1071, duration: 0.253s, episode steps: 36, steps per second: 142, episode reward: 94.088, mean reward: 2.614 [1.538, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.913, 10.100], loss: 0.144021, mae: 0.378698, mean_q: 4.314281
 63767/100000: episode: 1072, duration: 0.157s, episode steps: 22, steps per second: 140, episode reward: 62.927, mean reward: 2.860 [2.392, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.424], loss: 0.175266, mae: 0.391305, mean_q: 4.334583
 63798/100000: episode: 1073, duration: 0.284s, episode steps: 31, steps per second: 109, episode reward: 67.356, mean reward: 2.173 [1.605, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.841, 10.232], loss: 0.157495, mae: 0.367267, mean_q: 4.314837
 63825/100000: episode: 1074, duration: 0.178s, episode steps: 27, steps per second: 151, episode reward: 55.028, mean reward: 2.038 [1.635, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.275], loss: 0.147878, mae: 0.365217, mean_q: 4.235250
 63861/100000: episode: 1075, duration: 0.281s, episode steps: 36, steps per second: 128, episode reward: 88.381, mean reward: 2.455 [1.619, 7.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.035, 10.292], loss: 0.282725, mae: 0.422205, mean_q: 4.326544
 63896/100000: episode: 1076, duration: 0.312s, episode steps: 35, steps per second: 112, episode reward: 94.204, mean reward: 2.692 [1.909, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.544, 10.520], loss: 0.179475, mae: 0.396695, mean_q: 4.300674
 63991/100000: episode: 1077, duration: 0.623s, episode steps: 95, steps per second: 152, episode reward: 181.286, mean reward: 1.908 [1.460, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.945, 10.102], loss: 0.207427, mae: 0.399099, mean_q: 4.337372
 64086/100000: episode: 1078, duration: 0.615s, episode steps: 95, steps per second: 155, episode reward: 194.786, mean reward: 2.050 [1.495, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.604, 10.195], loss: 0.182815, mae: 0.395107, mean_q: 4.379550
 64181/100000: episode: 1079, duration: 0.503s, episode steps: 95, steps per second: 189, episode reward: 177.345, mean reward: 1.867 [1.456, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.832, 10.100], loss: 0.173488, mae: 0.377543, mean_q: 4.347877
[Info] 2-TH LEVEL FOUND: 6.926198482513428, Considering 10/90 traces
 64212/100000: episode: 1080, duration: 4.394s, episode steps: 31, steps per second: 7, episode reward: 76.109, mean reward: 2.455 [1.952, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.294, 10.414], loss: 0.135076, mae: 0.355432, mean_q: 4.373659
 64232/100000: episode: 1081, duration: 0.131s, episode steps: 20, steps per second: 153, episode reward: 59.276, mean reward: 2.964 [2.106, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.745, 10.368], loss: 0.182160, mae: 0.408851, mean_q: 4.389734
 64239/100000: episode: 1082, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 27.071, mean reward: 3.867 [2.211, 6.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.563], loss: 0.125476, mae: 0.355873, mean_q: 4.340871
 64257/100000: episode: 1083, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 60.910, mean reward: 3.384 [2.529, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.339, 10.483], loss: 0.123505, mae: 0.339046, mean_q: 4.296310
 64277/100000: episode: 1084, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 56.688, mean reward: 2.834 [2.006, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.191, 10.323], loss: 0.219106, mae: 0.400245, mean_q: 4.367866
 64294/100000: episode: 1085, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 88.889, mean reward: 5.229 [3.520, 6.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.184, 10.540], loss: 0.128575, mae: 0.345059, mean_q: 4.326821
 64301/100000: episode: 1086, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 33.368, mean reward: 4.767 [4.123, 6.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.067, 10.520], loss: 0.182083, mae: 0.405339, mean_q: 4.440367
 64322/100000: episode: 1087, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 59.209, mean reward: 2.819 [2.472, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.138, 10.491], loss: 0.331988, mae: 0.421829, mean_q: 4.354563
 64341/100000: episode: 1088, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 61.814, mean reward: 3.253 [2.717, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.525], loss: 0.229741, mae: 0.410044, mean_q: 4.434939
 64359/100000: episode: 1089, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 77.384, mean reward: 4.299 [2.746, 8.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.379, 10.622], loss: 0.176648, mae: 0.418643, mean_q: 4.451563
 64366/100000: episode: 1090, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 31.460, mean reward: 4.494 [3.856, 5.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.649, 10.511], loss: 0.218583, mae: 0.440943, mean_q: 4.225132
 64386/100000: episode: 1091, duration: 0.096s, episode steps: 20, steps per second: 209, episode reward: 64.764, mean reward: 3.238 [2.486, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.559, 10.462], loss: 0.201921, mae: 0.433975, mean_q: 4.514025
 64403/100000: episode: 1092, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 69.779, mean reward: 4.105 [3.503, 5.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.524], loss: 0.172912, mae: 0.419064, mean_q: 4.496283
 64410/100000: episode: 1093, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 27.502, mean reward: 3.929 [3.420, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.530], loss: 0.147705, mae: 0.388276, mean_q: 4.589644
 64428/100000: episode: 1094, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 52.581, mean reward: 2.921 [2.356, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.619, 10.512], loss: 0.588294, mae: 0.518856, mean_q: 4.478830
 64447/100000: episode: 1095, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 73.861, mean reward: 3.887 [2.695, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.493], loss: 0.211701, mae: 0.476885, mean_q: 4.512751
 64468/100000: episode: 1096, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 77.409, mean reward: 3.686 [2.790, 5.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.494], loss: 0.208066, mae: 0.430842, mean_q: 4.523010
 64486/100000: episode: 1097, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 54.760, mean reward: 3.042 [1.758, 9.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.576, 10.297], loss: 0.281793, mae: 0.441387, mean_q: 4.484076
 64503/100000: episode: 1098, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 77.295, mean reward: 4.547 [3.313, 6.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.523], loss: 0.212530, mae: 0.426449, mean_q: 4.420432
 64521/100000: episode: 1099, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 53.960, mean reward: 2.998 [2.379, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.318, 10.435], loss: 0.233880, mae: 0.424763, mean_q: 4.574870
 64539/100000: episode: 1100, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 45.735, mean reward: 2.541 [2.205, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.247, 10.359], loss: 0.239974, mae: 0.439246, mean_q: 4.512124
 64546/100000: episode: 1101, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 34.607, mean reward: 4.944 [3.987, 7.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.566], loss: 0.161951, mae: 0.399559, mean_q: 4.465300
 64565/100000: episode: 1102, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 53.076, mean reward: 2.793 [2.260, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.007, 10.340], loss: 0.199210, mae: 0.425277, mean_q: 4.540006
 64585/100000: episode: 1103, duration: 0.104s, episode steps: 20, steps per second: 191, episode reward: 65.365, mean reward: 3.268 [2.716, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.263, 10.468], loss: 0.364450, mae: 0.453725, mean_q: 4.537849
 64604/100000: episode: 1104, duration: 0.120s, episode steps: 19, steps per second: 159, episode reward: 51.659, mean reward: 2.719 [2.087, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.371], loss: 0.258380, mae: 0.472930, mean_q: 4.542823
 64623/100000: episode: 1105, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 59.967, mean reward: 3.156 [1.859, 4.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.293], loss: 0.392263, mae: 0.485082, mean_q: 4.600226
 64643/100000: episode: 1106, duration: 0.124s, episode steps: 20, steps per second: 162, episode reward: 68.966, mean reward: 3.448 [2.573, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.168, 10.374], loss: 0.175434, mae: 0.414390, mean_q: 4.536097
 64661/100000: episode: 1107, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 68.058, mean reward: 3.781 [2.151, 6.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.379], loss: 0.171806, mae: 0.395352, mean_q: 4.524067
 64679/100000: episode: 1108, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 53.292, mean reward: 2.961 [2.381, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.163, 10.511], loss: 0.379221, mae: 0.478050, mean_q: 4.579401
 64699/100000: episode: 1109, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 63.446, mean reward: 3.172 [2.480, 5.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.829, 10.438], loss: 0.232823, mae: 0.451561, mean_q: 4.533200
 64719/100000: episode: 1110, duration: 0.133s, episode steps: 20, steps per second: 150, episode reward: 50.014, mean reward: 2.501 [1.674, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.629, 10.233], loss: 0.290332, mae: 0.503904, mean_q: 4.648774
 64739/100000: episode: 1111, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 66.526, mean reward: 3.326 [2.152, 5.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.225, 10.428], loss: 0.185334, mae: 0.420105, mean_q: 4.545582
 64758/100000: episode: 1112, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 60.779, mean reward: 3.199 [2.557, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.122, 10.475], loss: 0.204503, mae: 0.440250, mean_q: 4.530733
 64778/100000: episode: 1113, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 62.524, mean reward: 3.126 [2.376, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.385], loss: 0.238932, mae: 0.475190, mean_q: 4.583948
 64785/100000: episode: 1114, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 45.720, mean reward: 6.531 [5.139, 9.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.647], loss: 0.197824, mae: 0.451323, mean_q: 4.628860
 64792/100000: episode: 1115, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 35.121, mean reward: 5.017 [4.003, 6.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.538], loss: 0.215633, mae: 0.450512, mean_q: 4.679710
 64811/100000: episode: 1116, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 63.140, mean reward: 3.323 [2.816, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.435], loss: 0.202907, mae: 0.442381, mean_q: 4.764224
 64830/100000: episode: 1117, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 57.563, mean reward: 3.030 [2.478, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.037, 10.498], loss: 0.243221, mae: 0.454668, mean_q: 4.750546
 64850/100000: episode: 1118, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 46.027, mean reward: 2.301 [1.831, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.345], loss: 0.240096, mae: 0.477926, mean_q: 4.689170
 64871/100000: episode: 1119, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 57.313, mean reward: 2.729 [2.165, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.462], loss: 0.257131, mae: 0.454091, mean_q: 4.700148
 64888/100000: episode: 1120, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 50.957, mean reward: 2.997 [2.629, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.338, 10.495], loss: 0.198974, mae: 0.433144, mean_q: 4.571688
 64906/100000: episode: 1121, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 77.318, mean reward: 4.295 [3.443, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.609], loss: 0.317774, mae: 0.468456, mean_q: 4.693715
[Info] FALSIFICATION!
 64909/100000: episode: 1122, duration: 0.272s, episode steps: 3, steps per second: 11, episode reward: 1017.114, mean reward: 339.038 [4.750, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.034, 9.559], loss: 0.268621, mae: 0.509219, mean_q: 4.566513
 64927/100000: episode: 1123, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 64.172, mean reward: 3.565 [2.305, 5.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.351], loss: 0.223949, mae: 0.465608, mean_q: 4.734115
 64946/100000: episode: 1124, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 46.764, mean reward: 2.461 [1.662, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.210], loss: 0.213727, mae: 0.432544, mean_q: 4.634404
 64964/100000: episode: 1125, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 70.670, mean reward: 3.926 [2.477, 5.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.539], loss: 858.939758, mae: 3.762691, mean_q: 5.477134
 64982/100000: episode: 1126, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 61.861, mean reward: 3.437 [2.383, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.354, 10.391], loss: 0.977287, mae: 1.002029, mean_q: 5.053895
 64989/100000: episode: 1127, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 23.921, mean reward: 3.417 [3.195, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.541], loss: 0.663159, mae: 0.804467, mean_q: 4.663764
 65007/100000: episode: 1128, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 70.583, mean reward: 3.921 [2.579, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.867, 10.525], loss: 0.449063, mae: 0.699376, mean_q: 4.848075
 65025/100000: episode: 1129, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 54.946, mean reward: 3.053 [1.693, 4.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.353, 10.200], loss: 0.498428, mae: 0.673430, mean_q: 4.563203
 65046/100000: episode: 1130, duration: 0.139s, episode steps: 21, steps per second: 151, episode reward: 53.603, mean reward: 2.553 [1.853, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.345], loss: 0.432392, mae: 0.639640, mean_q: 4.656837
 65064/100000: episode: 1131, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 63.024, mean reward: 3.501 [2.197, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.106, 10.482], loss: 0.310740, mae: 0.545244, mean_q: 4.629804
 65082/100000: episode: 1132, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 68.577, mean reward: 3.810 [2.157, 6.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.491], loss: 0.285204, mae: 0.542997, mean_q: 4.568890
 65099/100000: episode: 1133, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 84.018, mean reward: 4.942 [3.300, 7.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.037, 10.528], loss: 0.384148, mae: 0.585190, mean_q: 4.697486
 65119/100000: episode: 1134, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 66.032, mean reward: 3.302 [2.737, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.472], loss: 0.378694, mae: 0.574682, mean_q: 4.667630
 65140/100000: episode: 1135, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 67.917, mean reward: 3.234 [2.493, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.509, 10.360], loss: 0.352511, mae: 0.576282, mean_q: 4.650094
 65159/100000: episode: 1136, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 47.307, mean reward: 2.490 [1.965, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.980, 10.435], loss: 0.268294, mae: 0.523836, mean_q: 4.766390
 65178/100000: episode: 1137, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 74.614, mean reward: 3.927 [2.999, 5.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.155, 10.550], loss: 0.276290, mae: 0.515676, mean_q: 4.694721
 65198/100000: episode: 1138, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 71.853, mean reward: 3.593 [2.699, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.281, 10.433], loss: 0.285259, mae: 0.511329, mean_q: 4.664595
 65216/100000: episode: 1139, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 76.196, mean reward: 4.233 [2.898, 7.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.300, 10.492], loss: 0.371367, mae: 0.575733, mean_q: 4.916866
 65236/100000: episode: 1140, duration: 0.130s, episode steps: 20, steps per second: 153, episode reward: 56.706, mean reward: 2.835 [1.992, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.324], loss: 0.521042, mae: 0.603269, mean_q: 4.801789
 65243/100000: episode: 1141, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 26.231, mean reward: 3.747 [3.083, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.548], loss: 0.471616, mae: 0.574955, mean_q: 4.708083
 65262/100000: episode: 1142, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 65.123, mean reward: 3.428 [2.252, 5.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.426, 10.439], loss: 0.279834, mae: 0.507831, mean_q: 4.820879
 65280/100000: episode: 1143, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 54.777, mean reward: 3.043 [2.282, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.728, 10.454], loss: 0.278867, mae: 0.530228, mean_q: 4.819241
 65298/100000: episode: 1144, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 57.475, mean reward: 3.193 [2.484, 4.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.195, 10.497], loss: 0.284713, mae: 0.517319, mean_q: 4.696197
 65305/100000: episode: 1145, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 31.183, mean reward: 4.455 [4.049, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.612], loss: 0.291798, mae: 0.538544, mean_q: 5.017887
 65322/100000: episode: 1146, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 63.608, mean reward: 3.742 [2.825, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.134, 10.474], loss: 907.471802, mae: 3.217029, mean_q: 6.009115
 65341/100000: episode: 1147, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 71.258, mean reward: 3.750 [2.432, 9.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.215, 10.457], loss: 1.496687, mae: 1.380336, mean_q: 5.028430
 65359/100000: episode: 1148, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 48.388, mean reward: 2.688 [2.080, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.355], loss: 0.522994, mae: 0.748396, mean_q: 4.630926
 65379/100000: episode: 1149, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 51.028, mean reward: 2.551 [2.043, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.413], loss: 0.469096, mae: 0.677950, mean_q: 4.983682
 65400/100000: episode: 1150, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 69.214, mean reward: 3.296 [2.635, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.415], loss: 731.122559, mae: 2.138072, mean_q: 4.989053
 65421/100000: episode: 1151, duration: 0.114s, episode steps: 21, steps per second: 183, episode reward: 61.160, mean reward: 2.912 [1.960, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.325], loss: 2.429579, mae: 1.735585, mean_q: 6.347470
[Info] FALSIFICATION!
 65436/100000: episode: 1152, duration: 0.337s, episode steps: 15, steps per second: 45, episode reward: 1094.749, mean reward: 72.983 [3.342, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.705, 10.442], loss: 0.860692, mae: 1.032534, mean_q: 4.155633
 65443/100000: episode: 1153, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 50.525, mean reward: 7.218 [4.749, 10.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.612, 10.706], loss: 0.530236, mae: 0.736323, mean_q: 4.897079
 65463/100000: episode: 1154, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 64.478, mean reward: 3.224 [2.143, 4.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.280, 10.401], loss: 0.580572, mae: 0.787061, mean_q: 5.265721
 65484/100000: episode: 1155, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 72.331, mean reward: 3.444 [2.719, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.529], loss: 734.369263, mae: 4.576351, mean_q: 8.144826
 65504/100000: episode: 1156, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 66.802, mean reward: 3.340 [2.778, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.194, 10.540], loss: 1.273478, mae: 1.245075, mean_q: 4.639985
 65522/100000: episode: 1157, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 80.738, mean reward: 4.485 [2.947, 7.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.589], loss: 1.128787, mae: 0.995625, mean_q: 4.973045
 65542/100000: episode: 1158, duration: 0.138s, episode steps: 20, steps per second: 144, episode reward: 60.711, mean reward: 3.036 [2.507, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.206, 10.454], loss: 0.671753, mae: 0.852491, mean_q: 5.500949
 65560/100000: episode: 1159, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 60.218, mean reward: 3.345 [2.516, 6.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.960, 10.393], loss: 0.829575, mae: 0.794814, mean_q: 5.227123
 65577/100000: episode: 1160, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 63.005, mean reward: 3.706 [3.107, 4.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.578, 10.476], loss: 0.513502, mae: 0.711954, mean_q: 5.164038
 65595/100000: episode: 1161, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 54.110, mean reward: 3.006 [2.558, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.927, 10.438], loss: 0.770480, mae: 0.802797, mean_q: 5.360844
 65615/100000: episode: 1162, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 49.324, mean reward: 2.466 [1.985, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.402], loss: 0.596213, mae: 0.724336, mean_q: 5.383086
 65633/100000: episode: 1163, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 98.953, mean reward: 5.497 [3.830, 9.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.497], loss: 0.580207, mae: 0.676614, mean_q: 5.178148
 65651/100000: episode: 1164, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 69.377, mean reward: 3.854 [2.573, 6.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.456], loss: 854.811768, mae: 3.554867, mean_q: 6.995094
 65668/100000: episode: 1165, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 57.749, mean reward: 3.397 [2.477, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.431], loss: 0.978546, mae: 0.971470, mean_q: 5.493175
 65686/100000: episode: 1166, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 104.536, mean reward: 5.808 [3.444, 8.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.594, 10.621], loss: 0.836184, mae: 0.842232, mean_q: 5.094519
 65705/100000: episode: 1167, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 50.526, mean reward: 2.659 [1.796, 3.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.268], loss: 1613.708496, mae: 5.278491, mean_q: 7.227773
 65724/100000: episode: 1168, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 59.327, mean reward: 3.122 [2.039, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.427, 10.336], loss: 4.279406, mae: 2.214294, mean_q: 7.284127
 65744/100000: episode: 1169, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 69.357, mean reward: 3.468 [2.668, 5.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.128, 10.418], loss: 1.073378, mae: 1.000859, mean_q: 4.995917
[Info] Complete ISplit Iteration
[Info] Levels: [5.7592897, 6.9261985, 10.082483]
[Info] Cond. Prob: [0.1, 0.1, 0.11]
[Info] Error Prob: 0.0011000000000000003

 65762/100000: episode: 1170, duration: 4.555s, episode steps: 18, steps per second: 4, episode reward: 60.977, mean reward: 3.388 [2.520, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.752, 10.542], loss: 1.033234, mae: 0.976660, mean_q: 5.610563
 65862/100000: episode: 1171, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 184.746, mean reward: 1.847 [1.445, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.098], loss: 0.698095, mae: 0.807629, mean_q: 5.459542
 65962/100000: episode: 1172, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.564, mean reward: 1.836 [1.487, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.249, 10.098], loss: 307.523773, mae: 1.511874, mean_q: 5.619342
 66062/100000: episode: 1173, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 185.873, mean reward: 1.859 [1.451, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.557, 10.302], loss: 308.544800, mae: 2.092508, mean_q: 6.206524
 66162/100000: episode: 1174, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 215.889, mean reward: 2.159 [1.509, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.226, 10.236], loss: 308.246887, mae: 2.161520, mean_q: 6.375107
 66262/100000: episode: 1175, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 201.130, mean reward: 2.011 [1.440, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.561, 10.164], loss: 154.411514, mae: 1.121451, mean_q: 5.452914
 66362/100000: episode: 1176, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 217.294, mean reward: 2.173 [1.554, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.601, 10.098], loss: 461.939545, mae: 2.772165, mean_q: 7.091668
 66462/100000: episode: 1177, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.045, mean reward: 1.950 [1.434, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.270, 10.272], loss: 154.246384, mae: 1.213881, mean_q: 5.678008
 66562/100000: episode: 1178, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.286, mean reward: 1.833 [1.448, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.098], loss: 154.836349, mae: 1.390167, mean_q: 5.834898
 66662/100000: episode: 1179, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 195.343, mean reward: 1.953 [1.486, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.239], loss: 154.749603, mae: 1.287492, mean_q: 5.821341
 66762/100000: episode: 1180, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.150, mean reward: 1.822 [1.459, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.709, 10.157], loss: 308.137085, mae: 1.799581, mean_q: 6.006139
 66862/100000: episode: 1181, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 194.248, mean reward: 1.942 [1.460, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.729, 10.110], loss: 307.169556, mae: 1.649193, mean_q: 5.711099
 66962/100000: episode: 1182, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.320, mean reward: 1.893 [1.488, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.481, 10.183], loss: 1.022469, mae: 0.969003, mean_q: 5.501409
 67062/100000: episode: 1183, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 218.377, mean reward: 2.184 [1.461, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.546, 10.098], loss: 0.601480, mae: 0.732369, mean_q: 5.162512
 67162/100000: episode: 1184, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.008, mean reward: 1.960 [1.466, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.504, 10.098], loss: 154.858704, mae: 1.275207, mean_q: 5.456551
 67262/100000: episode: 1185, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 207.427, mean reward: 2.074 [1.555, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.262, 10.329], loss: 0.522362, mae: 0.695669, mean_q: 5.131783
 67362/100000: episode: 1186, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 226.595, mean reward: 2.266 [1.533, 5.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.850, 10.251], loss: 461.481201, mae: 2.041055, mean_q: 5.737487
 67462/100000: episode: 1187, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 194.249, mean reward: 1.942 [1.457, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.557, 10.380], loss: 308.215240, mae: 1.982273, mean_q: 6.106703
 67562/100000: episode: 1188, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 183.484, mean reward: 1.835 [1.492, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.339, 10.159], loss: 0.753710, mae: 0.810710, mean_q: 5.245386
 67662/100000: episode: 1189, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 195.988, mean reward: 1.960 [1.473, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.975, 10.220], loss: 154.655609, mae: 1.242732, mean_q: 5.502690
 67762/100000: episode: 1190, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 202.716, mean reward: 2.027 [1.462, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.928, 10.277], loss: 306.443115, mae: 1.996376, mean_q: 6.105207
 67862/100000: episode: 1191, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 226.668, mean reward: 2.267 [1.506, 4.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.940, 10.158], loss: 0.815066, mae: 0.858449, mean_q: 5.133480
 67962/100000: episode: 1192, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 186.241, mean reward: 1.862 [1.483, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.141, 10.098], loss: 0.667779, mae: 0.749862, mean_q: 4.973348
 68062/100000: episode: 1193, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 182.634, mean reward: 1.826 [1.459, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.501, 10.098], loss: 309.130280, mae: 1.933371, mean_q: 5.698887
 68162/100000: episode: 1194, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 183.531, mean reward: 1.835 [1.476, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.654, 10.178], loss: 307.412933, mae: 1.550894, mean_q: 5.423481
 68262/100000: episode: 1195, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 202.592, mean reward: 2.026 [1.468, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.477, 10.098], loss: 307.345398, mae: 2.015554, mean_q: 6.015608
 68362/100000: episode: 1196, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 184.402, mean reward: 1.844 [1.434, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.589, 10.197], loss: 460.327698, mae: 2.293208, mean_q: 5.890018
 68462/100000: episode: 1197, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.295, mean reward: 1.993 [1.474, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.554, 10.098], loss: 153.870636, mae: 1.503744, mean_q: 5.654900
 68562/100000: episode: 1198, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 230.441, mean reward: 2.304 [1.459, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.009, 10.098], loss: 153.436935, mae: 1.317794, mean_q: 5.412491
 68662/100000: episode: 1199, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 211.698, mean reward: 2.117 [1.458, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.146, 10.098], loss: 0.783749, mae: 0.787319, mean_q: 5.008986
 68762/100000: episode: 1200, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 221.715, mean reward: 2.217 [1.487, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.273, 10.098], loss: 0.598794, mae: 0.696462, mean_q: 4.857715
 68862/100000: episode: 1201, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.056, mean reward: 1.971 [1.453, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.464, 10.271], loss: 307.646881, mae: 1.522782, mean_q: 5.270195
 68962/100000: episode: 1202, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.347, mean reward: 1.923 [1.450, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.958, 10.165], loss: 154.978821, mae: 1.377331, mean_q: 5.551966
 69062/100000: episode: 1203, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 208.729, mean reward: 2.087 [1.460, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.832, 10.098], loss: 0.793826, mae: 0.779137, mean_q: 5.111636
 69162/100000: episode: 1204, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 210.998, mean reward: 2.110 [1.491, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.049, 10.147], loss: 153.416977, mae: 1.115548, mean_q: 5.136511
 69262/100000: episode: 1205, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 208.458, mean reward: 2.085 [1.473, 5.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.098], loss: 0.786878, mae: 0.756072, mean_q: 4.941195
 69362/100000: episode: 1206, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 177.194, mean reward: 1.772 [1.450, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.291], loss: 154.376389, mae: 0.943847, mean_q: 4.777884
 69462/100000: episode: 1207, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 199.886, mean reward: 1.999 [1.474, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.171, 10.098], loss: 307.383606, mae: 1.672782, mean_q: 5.418486
 69562/100000: episode: 1208, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 203.230, mean reward: 2.032 [1.484, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.456, 10.098], loss: 0.949086, mae: 0.819982, mean_q: 4.896430
 69662/100000: episode: 1209, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.685, mean reward: 1.977 [1.510, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.284, 10.098], loss: 154.206833, mae: 0.939888, mean_q: 4.690495
 69762/100000: episode: 1210, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.044, mean reward: 1.960 [1.460, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.292], loss: 0.795855, mae: 0.777889, mean_q: 4.939741
 69862/100000: episode: 1211, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.194, mean reward: 1.922 [1.441, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.104, 10.455], loss: 154.411926, mae: 1.118282, mean_q: 4.945796
 69962/100000: episode: 1212, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.233, mean reward: 1.882 [1.486, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.070, 10.207], loss: 0.480018, mae: 0.562469, mean_q: 4.498711
 70062/100000: episode: 1213, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 181.904, mean reward: 1.819 [1.441, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.034, 10.098], loss: 154.232864, mae: 1.017316, mean_q: 4.723501
 70162/100000: episode: 1214, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 191.346, mean reward: 1.913 [1.474, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.263, 10.098], loss: 154.073151, mae: 1.065073, mean_q: 4.819314
 70262/100000: episode: 1215, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 179.115, mean reward: 1.791 [1.439, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.098], loss: 0.363493, mae: 0.507381, mean_q: 4.323895
 70362/100000: episode: 1216, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 207.427, mean reward: 2.074 [1.451, 3.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.931, 10.378], loss: 154.078384, mae: 1.037669, mean_q: 4.647226
 70462/100000: episode: 1217, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 179.686, mean reward: 1.797 [1.440, 2.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.223, 10.098], loss: 0.319727, mae: 0.482325, mean_q: 4.232913
 70562/100000: episode: 1218, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 192.628, mean reward: 1.926 [1.448, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.659, 10.165], loss: 0.252521, mae: 0.439671, mean_q: 4.136033
 70662/100000: episode: 1219, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 199.373, mean reward: 1.994 [1.441, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.245, 10.439], loss: 0.193534, mae: 0.403657, mean_q: 4.022749
 70762/100000: episode: 1220, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.300, mean reward: 1.913 [1.443, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.558, 10.180], loss: 0.155404, mae: 0.386256, mean_q: 3.944123
 70862/100000: episode: 1221, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 239.315, mean reward: 2.393 [1.441, 5.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.795, 10.098], loss: 0.146662, mae: 0.383823, mean_q: 3.939749
 70962/100000: episode: 1222, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 221.251, mean reward: 2.213 [1.435, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.959, 10.419], loss: 0.159876, mae: 0.390151, mean_q: 3.933276
 71062/100000: episode: 1223, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.716, mean reward: 1.927 [1.474, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.152], loss: 0.148323, mae: 0.374454, mean_q: 3.943505
 71162/100000: episode: 1224, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 175.750, mean reward: 1.758 [1.458, 2.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.824, 10.213], loss: 0.143834, mae: 0.373518, mean_q: 3.906260
 71262/100000: episode: 1225, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.012, mean reward: 1.990 [1.434, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.203, 10.098], loss: 0.138108, mae: 0.361931, mean_q: 3.915311
 71362/100000: episode: 1226, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.235, mean reward: 1.902 [1.476, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.863, 10.370], loss: 0.129784, mae: 0.359234, mean_q: 3.938424
 71462/100000: episode: 1227, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 188.136, mean reward: 1.881 [1.434, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.098], loss: 0.133407, mae: 0.361460, mean_q: 3.930929
 71562/100000: episode: 1228, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.121, mean reward: 1.951 [1.469, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.816, 10.180], loss: 0.125168, mae: 0.348258, mean_q: 3.925955
 71662/100000: episode: 1229, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.756, mean reward: 1.878 [1.451, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.049, 10.203], loss: 0.119359, mae: 0.344685, mean_q: 3.907147
 71762/100000: episode: 1230, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 185.358, mean reward: 1.854 [1.466, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.354, 10.098], loss: 0.127074, mae: 0.349419, mean_q: 3.914088
 71862/100000: episode: 1231, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 178.552, mean reward: 1.786 [1.458, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.230, 10.098], loss: 0.138791, mae: 0.360854, mean_q: 3.915501
 71962/100000: episode: 1232, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 225.912, mean reward: 2.259 [1.442, 5.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.193, 10.539], loss: 0.121085, mae: 0.348291, mean_q: 3.907601
 72062/100000: episode: 1233, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.026, mean reward: 1.870 [1.447, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.516, 10.098], loss: 0.113509, mae: 0.337171, mean_q: 3.902396
 72162/100000: episode: 1234, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 199.542, mean reward: 1.995 [1.513, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.243, 10.098], loss: 0.126815, mae: 0.345954, mean_q: 3.911777
 72262/100000: episode: 1235, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 192.684, mean reward: 1.927 [1.463, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.173], loss: 0.123907, mae: 0.341704, mean_q: 3.903849
 72362/100000: episode: 1236, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 212.897, mean reward: 2.129 [1.473, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.676, 10.291], loss: 0.119841, mae: 0.338794, mean_q: 3.904132
 72462/100000: episode: 1237, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.968, mean reward: 1.810 [1.442, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.365, 10.314], loss: 0.131148, mae: 0.354269, mean_q: 3.908128
 72562/100000: episode: 1238, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.162, mean reward: 1.922 [1.469, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.098], loss: 0.117187, mae: 0.335954, mean_q: 3.904897
 72662/100000: episode: 1239, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 201.239, mean reward: 2.012 [1.454, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.611, 10.183], loss: 0.107106, mae: 0.326737, mean_q: 3.886259
 72762/100000: episode: 1240, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.458, mean reward: 1.915 [1.476, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.233, 10.251], loss: 0.121001, mae: 0.336564, mean_q: 3.897040
 72862/100000: episode: 1241, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 193.722, mean reward: 1.937 [1.460, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.838, 10.098], loss: 0.099073, mae: 0.320968, mean_q: 3.881181
 72962/100000: episode: 1242, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.580, mean reward: 1.936 [1.479, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.424, 10.098], loss: 0.108913, mae: 0.323888, mean_q: 3.880514
 73062/100000: episode: 1243, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 192.690, mean reward: 1.927 [1.435, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.575, 10.261], loss: 0.110020, mae: 0.329806, mean_q: 3.891840
 73162/100000: episode: 1244, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.724, mean reward: 1.897 [1.453, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.114, 10.098], loss: 0.121708, mae: 0.347895, mean_q: 3.897018
 73262/100000: episode: 1245, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 189.725, mean reward: 1.897 [1.461, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.086, 10.098], loss: 0.110896, mae: 0.327561, mean_q: 3.905471
 73362/100000: episode: 1246, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.120, mean reward: 1.901 [1.457, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.399, 10.251], loss: 0.126566, mae: 0.336716, mean_q: 3.901228
 73462/100000: episode: 1247, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 205.214, mean reward: 2.052 [1.462, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.219, 10.152], loss: 0.121097, mae: 0.335203, mean_q: 3.898185
 73562/100000: episode: 1248, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 199.451, mean reward: 1.995 [1.475, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.987, 10.271], loss: 0.121396, mae: 0.331188, mean_q: 3.880257
 73662/100000: episode: 1249, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 189.187, mean reward: 1.892 [1.443, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.050, 10.098], loss: 0.109595, mae: 0.325886, mean_q: 3.892576
 73762/100000: episode: 1250, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 198.547, mean reward: 1.985 [1.481, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.801, 10.098], loss: 0.101033, mae: 0.320225, mean_q: 3.870976
 73862/100000: episode: 1251, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 190.832, mean reward: 1.908 [1.465, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.787, 10.098], loss: 0.112662, mae: 0.326789, mean_q: 3.860959
 73962/100000: episode: 1252, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.331, mean reward: 1.923 [1.494, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.786, 10.319], loss: 0.108177, mae: 0.320036, mean_q: 3.873228
 74062/100000: episode: 1253, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 228.303, mean reward: 2.283 [1.473, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.178, 10.098], loss: 0.096310, mae: 0.307727, mean_q: 3.863061
 74162/100000: episode: 1254, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 192.994, mean reward: 1.930 [1.478, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.226], loss: 0.097834, mae: 0.309170, mean_q: 3.843267
 74262/100000: episode: 1255, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 273.796, mean reward: 2.738 [1.496, 9.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.184, 10.098], loss: 0.108627, mae: 0.319931, mean_q: 3.849764
 74362/100000: episode: 1256, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 202.391, mean reward: 2.024 [1.519, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.703, 10.098], loss: 0.136651, mae: 0.343657, mean_q: 3.878416
 74462/100000: episode: 1257, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 179.218, mean reward: 1.792 [1.440, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.094, 10.181], loss: 0.162441, mae: 0.355569, mean_q: 3.884967
 74562/100000: episode: 1258, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.660, mean reward: 1.887 [1.477, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.606, 10.098], loss: 0.139771, mae: 0.346476, mean_q: 3.902368
 74662/100000: episode: 1259, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 191.415, mean reward: 1.914 [1.480, 8.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.098], loss: 0.143021, mae: 0.343778, mean_q: 3.893978
 74762/100000: episode: 1260, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 187.593, mean reward: 1.876 [1.492, 2.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.544, 10.098], loss: 0.154528, mae: 0.352838, mean_q: 3.891388
 74862/100000: episode: 1261, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 182.217, mean reward: 1.822 [1.457, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.695, 10.098], loss: 0.125060, mae: 0.340640, mean_q: 3.879954
 74962/100000: episode: 1262, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 198.845, mean reward: 1.988 [1.452, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.087, 10.344], loss: 0.127119, mae: 0.333650, mean_q: 3.874108
 75062/100000: episode: 1263, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 187.478, mean reward: 1.875 [1.446, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.180, 10.194], loss: 0.147091, mae: 0.352383, mean_q: 3.901910
 75162/100000: episode: 1264, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 213.429, mean reward: 2.134 [1.499, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.619, 10.159], loss: 0.113911, mae: 0.323412, mean_q: 3.862484
 75262/100000: episode: 1265, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 203.099, mean reward: 2.031 [1.535, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.363, 10.098], loss: 0.139098, mae: 0.347159, mean_q: 3.889737
 75362/100000: episode: 1266, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 191.691, mean reward: 1.917 [1.438, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.765, 10.098], loss: 0.138835, mae: 0.340744, mean_q: 3.909038
 75462/100000: episode: 1267, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 185.148, mean reward: 1.851 [1.460, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.024, 10.098], loss: 0.114218, mae: 0.329146, mean_q: 3.894302
 75562/100000: episode: 1268, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 191.221, mean reward: 1.912 [1.446, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.301, 10.102], loss: 0.119745, mae: 0.332280, mean_q: 3.906919
 75662/100000: episode: 1269, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 193.145, mean reward: 1.931 [1.434, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.153, 10.345], loss: 0.151410, mae: 0.357601, mean_q: 3.889949
[Info] 1-TH LEVEL FOUND: 5.302620887756348, Considering 10/90 traces
 75762/100000: episode: 1270, duration: 5.332s, episode steps: 100, steps per second: 19, episode reward: 180.511, mean reward: 1.805 [1.453, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.239, 10.098], loss: 0.117201, mae: 0.332968, mean_q: 3.906480
 75782/100000: episode: 1271, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 36.741, mean reward: 1.837 [1.513, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.187], loss: 0.128308, mae: 0.346073, mean_q: 3.908519
 75799/100000: episode: 1272, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 49.716, mean reward: 2.924 [2.441, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.519, 10.100], loss: 0.124453, mae: 0.335965, mean_q: 3.865010
 75817/100000: episode: 1273, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 51.423, mean reward: 2.857 [2.269, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.051, 10.100], loss: 0.084609, mae: 0.302896, mean_q: 3.873166
 75837/100000: episode: 1274, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 59.094, mean reward: 2.955 [2.132, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.515], loss: 0.109040, mae: 0.318090, mean_q: 3.879510
 75874/100000: episode: 1275, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 97.404, mean reward: 2.633 [1.882, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.109, 10.299], loss: 0.102835, mae: 0.315857, mean_q: 3.891100
 75891/100000: episode: 1276, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 51.828, mean reward: 3.049 [2.488, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.432, 10.100], loss: 0.098849, mae: 0.317625, mean_q: 3.894047
 75911/100000: episode: 1277, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 46.972, mean reward: 2.349 [2.044, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.372], loss: 0.116639, mae: 0.320920, mean_q: 3.884563
 75948/100000: episode: 1278, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 75.756, mean reward: 2.047 [1.522, 2.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.680, 10.303], loss: 0.130944, mae: 0.333295, mean_q: 3.876924
 75970/100000: episode: 1279, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 57.150, mean reward: 2.598 [2.237, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.351, 10.378], loss: 0.119127, mae: 0.329923, mean_q: 3.898581
 76007/100000: episode: 1280, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 78.694, mean reward: 2.127 [1.554, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.098, 10.188], loss: 0.105873, mae: 0.322717, mean_q: 3.889080
 76027/100000: episode: 1281, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 50.925, mean reward: 2.546 [2.081, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.401], loss: 0.141137, mae: 0.341044, mean_q: 3.933989
 76047/100000: episode: 1282, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 40.029, mean reward: 2.001 [1.535, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.165, 10.182], loss: 0.176030, mae: 0.372113, mean_q: 3.938041
 76062/100000: episode: 1283, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 55.161, mean reward: 3.677 [2.839, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.218, 10.100], loss: 0.131795, mae: 0.365207, mean_q: 3.899271
 76078/100000: episode: 1284, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 34.938, mean reward: 2.184 [1.913, 2.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.515, 10.341], loss: 0.157327, mae: 0.337201, mean_q: 3.882062
 76098/100000: episode: 1285, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 37.116, mean reward: 1.856 [1.513, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.429, 10.100], loss: 0.131966, mae: 0.345158, mean_q: 3.900479
 76118/100000: episode: 1286, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 34.494, mean reward: 1.725 [1.502, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.166, 10.100], loss: 0.130666, mae: 0.337242, mean_q: 3.919626
 76138/100000: episode: 1287, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 74.615, mean reward: 3.731 [2.728, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.116, 10.543], loss: 0.128333, mae: 0.327789, mean_q: 3.908635
 76156/100000: episode: 1288, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 55.455, mean reward: 3.081 [2.223, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.162, 10.100], loss: 0.111797, mae: 0.332844, mean_q: 4.001486
 76174/100000: episode: 1289, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 51.027, mean reward: 2.835 [2.191, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.233, 10.100], loss: 0.132942, mae: 0.352261, mean_q: 3.910048
 76190/100000: episode: 1290, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 45.316, mean reward: 2.832 [2.318, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.452], loss: 0.145238, mae: 0.345915, mean_q: 3.928499
 76215/100000: episode: 1291, duration: 0.171s, episode steps: 25, steps per second: 146, episode reward: 59.046, mean reward: 2.362 [1.972, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.002, 10.429], loss: 0.150700, mae: 0.350606, mean_q: 3.935081
 76237/100000: episode: 1292, duration: 0.135s, episode steps: 22, steps per second: 162, episode reward: 60.257, mean reward: 2.739 [1.963, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.501], loss: 0.151531, mae: 0.348937, mean_q: 3.895019
 76257/100000: episode: 1293, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 42.888, mean reward: 2.144 [1.697, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.875, 10.358], loss: 0.181071, mae: 0.369999, mean_q: 3.939017
 76282/100000: episode: 1294, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 85.106, mean reward: 3.404 [2.185, 11.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.467], loss: 0.156413, mae: 0.360313, mean_q: 3.955446
 76307/100000: episode: 1295, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 56.575, mean reward: 2.263 [1.709, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.866, 10.250], loss: 0.187964, mae: 0.389152, mean_q: 3.930195
 76324/100000: episode: 1296, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 45.435, mean reward: 2.673 [2.136, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.306, 10.100], loss: 0.173564, mae: 0.405194, mean_q: 3.991594
 76344/100000: episode: 1297, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 34.769, mean reward: 1.738 [1.438, 2.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.048, 10.224], loss: 0.149998, mae: 0.355541, mean_q: 3.960093
 76364/100000: episode: 1298, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 57.099, mean reward: 2.855 [2.144, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.562, 10.338], loss: 0.121827, mae: 0.349583, mean_q: 3.978420
 76384/100000: episode: 1299, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 63.832, mean reward: 3.192 [2.397, 5.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.305, 10.420], loss: 0.120883, mae: 0.338386, mean_q: 3.993168
 76404/100000: episode: 1300, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 48.036, mean reward: 2.402 [2.078, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.837, 10.361], loss: 0.141978, mae: 0.348159, mean_q: 3.987353
 76420/100000: episode: 1301, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 46.051, mean reward: 2.878 [2.241, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.562, 10.472], loss: 0.148175, mae: 0.378796, mean_q: 4.037015
 76436/100000: episode: 1302, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 37.783, mean reward: 2.361 [1.983, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.389], loss: 0.152567, mae: 0.367359, mean_q: 3.966080
 76456/100000: episode: 1303, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 31.968, mean reward: 1.598 [1.459, 1.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.146, 10.197], loss: 0.141849, mae: 0.363765, mean_q: 3.977337
 76498/100000: episode: 1304, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 105.390, mean reward: 2.509 [1.932, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.189, 10.278], loss: 0.126400, mae: 0.347166, mean_q: 3.964314
 76518/100000: episode: 1305, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 36.998, mean reward: 1.850 [1.529, 2.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.115, 10.240], loss: 0.215246, mae: 0.392331, mean_q: 4.028283
 76538/100000: episode: 1306, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 50.225, mean reward: 2.511 [2.242, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.088, 10.428], loss: 0.132834, mae: 0.346212, mean_q: 3.951740
 76556/100000: episode: 1307, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 43.791, mean reward: 2.433 [1.924, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.191, 10.100], loss: 0.112484, mae: 0.335288, mean_q: 3.952624
 76598/100000: episode: 1308, duration: 0.288s, episode steps: 42, steps per second: 146, episode reward: 102.215, mean reward: 2.434 [1.675, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.374, 10.441], loss: 0.156894, mae: 0.358791, mean_q: 4.003710
 76614/100000: episode: 1309, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 40.328, mean reward: 2.521 [2.288, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.376], loss: 0.123976, mae: 0.342933, mean_q: 3.972713
 76634/100000: episode: 1310, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 49.573, mean reward: 2.479 [1.796, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.374], loss: 0.174927, mae: 0.397485, mean_q: 4.080907
 76676/100000: episode: 1311, duration: 0.229s, episode steps: 42, steps per second: 184, episode reward: 194.756, mean reward: 4.637 [2.717, 16.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.968, 10.624], loss: 0.175928, mae: 0.372519, mean_q: 4.016915
 76718/100000: episode: 1312, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 122.917, mean reward: 2.927 [2.011, 4.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.493, 10.596], loss: 0.214899, mae: 0.402691, mean_q: 4.066045
 76733/100000: episode: 1313, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 73.295, mean reward: 4.886 [3.153, 8.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.255, 10.100], loss: 0.162987, mae: 0.390414, mean_q: 4.076413
 76750/100000: episode: 1314, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 49.931, mean reward: 2.937 [2.525, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.609, 10.100], loss: 0.213577, mae: 0.392925, mean_q: 4.111524
 76770/100000: episode: 1315, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 36.836, mean reward: 1.842 [1.466, 2.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.200, 10.100], loss: 0.182490, mae: 0.369702, mean_q: 4.107837
 76787/100000: episode: 1316, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 40.970, mean reward: 2.410 [2.002, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.143, 10.100], loss: 0.271259, mae: 0.399275, mean_q: 4.130045
 76824/100000: episode: 1317, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 96.826, mean reward: 2.617 [1.693, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.306, 10.274], loss: 0.185695, mae: 0.396754, mean_q: 4.057953
 76846/100000: episode: 1318, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 42.188, mean reward: 1.918 [1.512, 2.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.050, 10.115], loss: 0.211398, mae: 0.386488, mean_q: 4.104261
 76888/100000: episode: 1319, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 98.964, mean reward: 2.356 [1.567, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.184, 10.529], loss: 0.198737, mae: 0.411954, mean_q: 4.114630
 76908/100000: episode: 1320, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 63.742, mean reward: 3.187 [2.282, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.147, 10.384], loss: 0.151823, mae: 0.376052, mean_q: 4.105932
 76933/100000: episode: 1321, duration: 0.154s, episode steps: 25, steps per second: 162, episode reward: 60.931, mean reward: 2.437 [1.811, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.357], loss: 0.156710, mae: 0.368067, mean_q: 4.138478
 76975/100000: episode: 1322, duration: 0.269s, episode steps: 42, steps per second: 156, episode reward: 98.568, mean reward: 2.347 [1.547, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.803, 10.100], loss: 0.231883, mae: 0.413485, mean_q: 4.126062
 77012/100000: episode: 1323, duration: 0.254s, episode steps: 37, steps per second: 145, episode reward: 102.669, mean reward: 2.775 [2.246, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.202, 10.374], loss: 0.202813, mae: 0.398484, mean_q: 4.202021
 77030/100000: episode: 1324, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 40.157, mean reward: 2.231 [1.667, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.731, 10.100], loss: 0.221891, mae: 0.409574, mean_q: 4.166261
 77052/100000: episode: 1325, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 63.180, mean reward: 2.872 [2.025, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.470], loss: 0.180385, mae: 0.404627, mean_q: 4.163918
 77069/100000: episode: 1326, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 57.502, mean reward: 3.382 [2.612, 5.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.315, 10.100], loss: 0.149447, mae: 0.387231, mean_q: 4.181176
 77089/100000: episode: 1327, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 42.063, mean reward: 2.103 [1.794, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.415, 10.270], loss: 0.183101, mae: 0.378796, mean_q: 4.098628
 77107/100000: episode: 1328, duration: 0.122s, episode steps: 18, steps per second: 147, episode reward: 51.816, mean reward: 2.879 [2.447, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.267, 10.100], loss: 0.130931, mae: 0.353827, mean_q: 4.143262
 77125/100000: episode: 1329, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 48.397, mean reward: 2.689 [1.920, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.347, 10.100], loss: 0.213056, mae: 0.429464, mean_q: 4.247069
 77167/100000: episode: 1330, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 122.995, mean reward: 2.928 [1.722, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.256, 10.321], loss: 0.241961, mae: 0.409831, mean_q: 4.186463
 77182/100000: episode: 1331, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 42.614, mean reward: 2.841 [1.940, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.198, 10.100], loss: 0.328345, mae: 0.417154, mean_q: 4.237777
 77202/100000: episode: 1332, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 47.046, mean reward: 2.352 [1.783, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.240, 10.182], loss: 0.148751, mae: 0.366643, mean_q: 4.193695
 77222/100000: episode: 1333, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 52.033, mean reward: 2.602 [2.321, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.648, 10.269], loss: 0.175639, mae: 0.371639, mean_q: 4.202461
 77247/100000: episode: 1334, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 117.783, mean reward: 4.711 [2.361, 11.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.482], loss: 0.270333, mae: 0.404600, mean_q: 4.232340
 77264/100000: episode: 1335, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 51.526, mean reward: 3.031 [2.562, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.306, 10.100], loss: 0.243779, mae: 0.434676, mean_q: 4.279588
 77289/100000: episode: 1336, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 57.345, mean reward: 2.294 [1.820, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.893, 10.345], loss: 0.176346, mae: 0.391608, mean_q: 4.236788
 77314/100000: episode: 1337, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 72.842, mean reward: 2.914 [2.355, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.408], loss: 0.149480, mae: 0.376246, mean_q: 4.263031
 77334/100000: episode: 1338, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 57.666, mean reward: 2.883 [2.175, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.894, 10.454], loss: 0.190847, mae: 0.391662, mean_q: 4.244911
 77376/100000: episode: 1339, duration: 0.254s, episode steps: 42, steps per second: 166, episode reward: 121.827, mean reward: 2.901 [1.633, 5.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.862, 10.192], loss: 0.169676, mae: 0.382184, mean_q: 4.217810
 77418/100000: episode: 1340, duration: 0.242s, episode steps: 42, steps per second: 174, episode reward: 122.124, mean reward: 2.908 [2.085, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.360, 10.515], loss: 0.172352, mae: 0.399512, mean_q: 4.300343
 77460/100000: episode: 1341, duration: 0.235s, episode steps: 42, steps per second: 178, episode reward: 115.500, mean reward: 2.750 [1.753, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.825, 10.305], loss: 0.274925, mae: 0.443354, mean_q: 4.282574
 77480/100000: episode: 1342, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 35.876, mean reward: 1.794 [1.520, 2.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.039, 10.100], loss: 0.186656, mae: 0.416847, mean_q: 4.304456
 77522/100000: episode: 1343, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 127.442, mean reward: 3.034 [1.759, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.396, 10.245], loss: 0.214670, mae: 0.428797, mean_q: 4.338889
 77537/100000: episode: 1344, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 47.206, mean reward: 3.147 [2.012, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.543, 10.100], loss: 0.263592, mae: 0.441256, mean_q: 4.386983
 77553/100000: episode: 1345, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 32.232, mean reward: 2.015 [1.636, 2.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.097, 10.238], loss: 0.301212, mae: 0.432976, mean_q: 4.336441
 77595/100000: episode: 1346, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 92.868, mean reward: 2.211 [1.604, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.944, 10.288], loss: 0.168123, mae: 0.374860, mean_q: 4.338802
 77615/100000: episode: 1347, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 42.477, mean reward: 2.124 [1.719, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.252], loss: 0.151172, mae: 0.370704, mean_q: 4.289659
 77640/100000: episode: 1348, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 78.884, mean reward: 3.155 [2.371, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.251, 10.374], loss: 0.175011, mae: 0.386390, mean_q: 4.367336
 77660/100000: episode: 1349, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 34.875, mean reward: 1.744 [1.546, 2.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.070, 10.112], loss: 0.193291, mae: 0.404862, mean_q: 4.285903
 77680/100000: episode: 1350, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 37.045, mean reward: 1.852 [1.492, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.095, 10.239], loss: 0.214780, mae: 0.418261, mean_q: 4.348901
 77695/100000: episode: 1351, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 52.597, mean reward: 3.506 [2.755, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.314, 10.100], loss: 0.209423, mae: 0.409759, mean_q: 4.294792
 77712/100000: episode: 1352, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 45.576, mean reward: 2.681 [2.033, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.146, 10.100], loss: 0.207100, mae: 0.422538, mean_q: 4.313267
 77737/100000: episode: 1353, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 67.675, mean reward: 2.707 [2.269, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.043, 10.440], loss: 0.287285, mae: 0.445743, mean_q: 4.431233
 77752/100000: episode: 1354, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 70.393, mean reward: 4.693 [3.758, 6.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.973, 10.100], loss: 0.186180, mae: 0.405024, mean_q: 4.436965
 77770/100000: episode: 1355, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 42.394, mean reward: 2.355 [1.957, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.344, 10.100], loss: 0.236842, mae: 0.451524, mean_q: 4.347532
 77807/100000: episode: 1356, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 79.910, mean reward: 2.160 [1.451, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.929, 10.179], loss: 0.197295, mae: 0.420281, mean_q: 4.411726
 77827/100000: episode: 1357, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 41.691, mean reward: 2.085 [1.484, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.430, 10.100], loss: 0.252554, mae: 0.432046, mean_q: 4.411674
 77842/100000: episode: 1358, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 99.357, mean reward: 6.624 [3.819, 28.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.785, 10.100], loss: 0.174605, mae: 0.386664, mean_q: 4.385596
 77858/100000: episode: 1359, duration: 0.129s, episode steps: 16, steps per second: 124, episode reward: 46.600, mean reward: 2.912 [2.126, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.920, 10.453], loss: 0.249593, mae: 0.461704, mean_q: 4.374828
[Info] 2-TH LEVEL FOUND: 7.996610164642334, Considering 10/90 traces
 77874/100000: episode: 1360, duration: 4.682s, episode steps: 16, steps per second: 3, episode reward: 43.011, mean reward: 2.688 [2.149, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-2.205, 10.452], loss: 0.886401, mae: 0.511966, mean_q: 4.411801
 77886/100000: episode: 1361, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 34.546, mean reward: 2.879 [2.565, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.351, 10.100], loss: 0.444427, mae: 0.501233, mean_q: 4.369550
 77898/100000: episode: 1362, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 44.169, mean reward: 3.681 [2.789, 5.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.333, 10.100], loss: 0.310569, mae: 0.481660, mean_q: 4.515882
 77914/100000: episode: 1363, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 100.920, mean reward: 6.308 [3.856, 12.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.075, 10.549], loss: 0.192276, mae: 0.400248, mean_q: 4.406628
 77926/100000: episode: 1364, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 41.437, mean reward: 3.453 [3.202, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.669, 10.100], loss: 0.292448, mae: 0.447602, mean_q: 4.416360
 77939/100000: episode: 1365, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 48.519, mean reward: 3.732 [3.036, 6.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.352, 10.100], loss: 0.195474, mae: 0.413202, mean_q: 4.507930
 77956/100000: episode: 1366, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 54.706, mean reward: 3.218 [2.323, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.364], loss: 0.344634, mae: 0.473806, mean_q: 4.434205
 77969/100000: episode: 1367, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 51.491, mean reward: 3.961 [2.392, 6.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.506, 10.100], loss: 0.356009, mae: 0.454972, mean_q: 4.396592
 77981/100000: episode: 1368, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 44.281, mean reward: 3.690 [2.438, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.446, 10.100], loss: 0.177766, mae: 0.414124, mean_q: 4.443172
 77994/100000: episode: 1369, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 41.119, mean reward: 3.163 [2.040, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.381, 10.100], loss: 0.180919, mae: 0.404783, mean_q: 4.467388
 78007/100000: episode: 1370, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 56.053, mean reward: 4.312 [3.314, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.451, 10.100], loss: 0.206254, mae: 0.407873, mean_q: 4.454935
 78023/100000: episode: 1371, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 65.553, mean reward: 4.097 [2.817, 7.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.532], loss: 0.214166, mae: 0.416585, mean_q: 4.438841
 78040/100000: episode: 1372, duration: 0.114s, episode steps: 17, steps per second: 150, episode reward: 92.754, mean reward: 5.456 [3.344, 14.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.600], loss: 0.318539, mae: 0.452025, mean_q: 4.595858
 78050/100000: episode: 1373, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 39.690, mean reward: 3.969 [2.960, 5.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.334, 10.100], loss: 0.391496, mae: 0.496490, mean_q: 4.518006
 78062/100000: episode: 1374, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 42.865, mean reward: 3.572 [2.743, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.376, 10.100], loss: 0.346492, mae: 0.516992, mean_q: 4.537285
 78073/100000: episode: 1375, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 48.956, mean reward: 4.451 [3.614, 5.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.474, 10.100], loss: 0.173507, mae: 0.402695, mean_q: 4.547220
 78084/100000: episode: 1376, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 42.317, mean reward: 3.847 [2.421, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.987, 10.100], loss: 0.313993, mae: 0.478368, mean_q: 4.655221
 78100/100000: episode: 1377, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 80.992, mean reward: 5.062 [3.173, 10.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.532], loss: 0.260993, mae: 0.407476, mean_q: 4.555515
 78113/100000: episode: 1378, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 51.864, mean reward: 3.990 [2.712, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.454, 10.100], loss: 0.211806, mae: 0.443402, mean_q: 4.563502
 78126/100000: episode: 1379, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 56.778, mean reward: 4.368 [3.661, 5.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.364, 10.100], loss: 0.268762, mae: 0.485679, mean_q: 4.488776
 78139/100000: episode: 1380, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 42.185, mean reward: 3.245 [2.844, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.267, 10.100], loss: 0.426041, mae: 0.487328, mean_q: 4.523473
 78150/100000: episode: 1381, duration: 0.080s, episode steps: 11, steps per second: 137, episode reward: 31.829, mean reward: 2.894 [2.317, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.203, 10.100], loss: 1.149820, mae: 0.532851, mean_q: 4.605449
 78167/100000: episode: 1382, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 126.669, mean reward: 7.451 [4.188, 15.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.227, 10.557], loss: 0.330251, mae: 0.502333, mean_q: 4.737393
 78179/100000: episode: 1383, duration: 0.093s, episode steps: 12, steps per second: 129, episode reward: 30.882, mean reward: 2.574 [2.193, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.060, 10.100], loss: 0.229704, mae: 0.454000, mean_q: 4.579240
 78192/100000: episode: 1384, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 49.976, mean reward: 3.844 [2.569, 6.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.236, 10.100], loss: 1.013096, mae: 0.596077, mean_q: 4.692698
 78204/100000: episode: 1385, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 42.377, mean reward: 3.531 [2.662, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.409, 10.100], loss: 1.056046, mae: 0.529664, mean_q: 4.607646
 78217/100000: episode: 1386, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 41.543, mean reward: 3.196 [2.164, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.172, 10.100], loss: 0.538907, mae: 0.579210, mean_q: 4.739516
 78230/100000: episode: 1387, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 45.961, mean reward: 3.535 [2.236, 6.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.216, 10.100], loss: 0.322899, mae: 0.474013, mean_q: 4.555524
 78243/100000: episode: 1388, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 41.573, mean reward: 3.198 [2.794, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.355, 10.100], loss: 0.355385, mae: 0.528157, mean_q: 4.811429
 78254/100000: episode: 1389, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 51.114, mean reward: 4.647 [3.757, 5.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.697, 10.100], loss: 0.688592, mae: 0.608229, mean_q: 4.860473
 78264/100000: episode: 1390, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 38.983, mean reward: 3.898 [3.122, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.542, 10.100], loss: 0.247148, mae: 0.437203, mean_q: 4.684294
 78274/100000: episode: 1391, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 47.260, mean reward: 4.726 [3.359, 9.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.423, 10.100], loss: 0.209105, mae: 0.430220, mean_q: 4.730304
 78286/100000: episode: 1392, duration: 0.059s, episode steps: 12, steps per second: 205, episode reward: 44.490, mean reward: 3.707 [2.628, 6.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.223, 10.100], loss: 0.190425, mae: 0.413638, mean_q: 4.687901
 78298/100000: episode: 1393, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 51.204, mean reward: 4.267 [2.777, 6.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.158, 10.100], loss: 0.226567, mae: 0.462194, mean_q: 4.787836
 78315/100000: episode: 1394, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 110.908, mean reward: 6.524 [3.577, 19.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.366, 10.556], loss: 0.245211, mae: 0.441663, mean_q: 4.651831
 78331/100000: episode: 1395, duration: 0.110s, episode steps: 16, steps per second: 146, episode reward: 104.502, mean reward: 6.531 [4.042, 11.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.257, 10.611], loss: 0.463976, mae: 0.510153, mean_q: 4.721043
 78344/100000: episode: 1396, duration: 0.084s, episode steps: 13, steps per second: 156, episode reward: 52.126, mean reward: 4.010 [3.147, 5.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.430, 10.100], loss: 0.287653, mae: 0.455519, mean_q: 4.716312
 78360/100000: episode: 1397, duration: 0.108s, episode steps: 16, steps per second: 147, episode reward: 88.371, mean reward: 5.523 [2.959, 10.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.555], loss: 0.365764, mae: 0.533504, mean_q: 4.802280
 78372/100000: episode: 1398, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 39.033, mean reward: 3.253 [2.753, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.366, 10.100], loss: 0.664201, mae: 0.561237, mean_q: 4.873484
 78383/100000: episode: 1399, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 37.323, mean reward: 3.393 [2.971, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.281, 10.100], loss: 0.452277, mae: 0.552150, mean_q: 4.798151
 78394/100000: episode: 1400, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 33.586, mean reward: 3.053 [2.432, 4.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.363, 10.100], loss: 0.440015, mae: 0.603743, mean_q: 5.008955
 78406/100000: episode: 1401, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 42.113, mean reward: 3.509 [3.034, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.425, 10.100], loss: 0.318993, mae: 0.501599, mean_q: 4.859061
 78422/100000: episode: 1402, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 78.156, mean reward: 4.885 [2.897, 11.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.480], loss: 0.621521, mae: 0.618229, mean_q: 4.870626
 78434/100000: episode: 1403, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 55.894, mean reward: 4.658 [2.876, 7.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.618, 10.100], loss: 0.470510, mae: 0.603564, mean_q: 5.020457
 78447/100000: episode: 1404, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 52.416, mean reward: 4.032 [2.978, 5.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.238, 10.100], loss: 0.414178, mae: 0.513752, mean_q: 4.931935
 78458/100000: episode: 1405, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 37.949, mean reward: 3.450 [2.571, 7.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.301, 10.100], loss: 0.495667, mae: 0.578471, mean_q: 4.887322
 78471/100000: episode: 1406, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 50.097, mean reward: 3.854 [3.290, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.551, 10.100], loss: 0.601075, mae: 0.576500, mean_q: 5.003280
 78484/100000: episode: 1407, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 84.023, mean reward: 6.463 [4.132, 8.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.287, 10.100], loss: 0.303521, mae: 0.509450, mean_q: 4.855535
 78497/100000: episode: 1408, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 46.737, mean reward: 3.595 [3.013, 4.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.498, 10.100], loss: 0.543736, mae: 0.617759, mean_q: 4.973022
 78510/100000: episode: 1409, duration: 0.097s, episode steps: 13, steps per second: 133, episode reward: 68.333, mean reward: 5.256 [3.915, 8.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.372, 10.100], loss: 0.274184, mae: 0.481939, mean_q: 4.682238
 78523/100000: episode: 1410, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 58.267, mean reward: 4.482 [3.579, 6.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.377, 10.100], loss: 0.506477, mae: 0.629614, mean_q: 4.988721
 78533/100000: episode: 1411, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 36.405, mean reward: 3.640 [2.920, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.405, 10.100], loss: 0.663222, mae: 0.656564, mean_q: 5.083460
 78546/100000: episode: 1412, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 38.134, mean reward: 2.933 [2.382, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.410, 10.100], loss: 0.357487, mae: 0.512828, mean_q: 4.911914
 78559/100000: episode: 1413, duration: 0.096s, episode steps: 13, steps per second: 136, episode reward: 45.744, mean reward: 3.519 [2.927, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.421, 10.100], loss: 0.378882, mae: 0.574872, mean_q: 4.975791
 78571/100000: episode: 1414, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 37.925, mean reward: 3.160 [2.354, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.320, 10.100], loss: 0.360170, mae: 0.504270, mean_q: 5.009141
 78581/100000: episode: 1415, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 28.981, mean reward: 2.898 [2.266, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.635, 10.100], loss: 0.435462, mae: 0.549622, mean_q: 4.854425
 78594/100000: episode: 1416, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 38.042, mean reward: 2.926 [2.057, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.199, 10.100], loss: 0.441557, mae: 0.588854, mean_q: 4.965731
 78607/100000: episode: 1417, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 53.666, mean reward: 4.128 [2.846, 6.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.382, 10.100], loss: 0.601559, mae: 0.639456, mean_q: 5.095490
 78624/100000: episode: 1418, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 86.253, mean reward: 5.074 [3.701, 6.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.049, 10.424], loss: 0.410281, mae: 0.557841, mean_q: 5.031106
 78636/100000: episode: 1419, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 51.441, mean reward: 4.287 [3.684, 5.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.472, 10.100], loss: 0.401935, mae: 0.599204, mean_q: 5.098889
 78647/100000: episode: 1420, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 34.662, mean reward: 3.151 [2.856, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.537, 10.100], loss: 0.317264, mae: 0.520163, mean_q: 5.054846
 78657/100000: episode: 1421, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 40.927, mean reward: 4.093 [3.306, 5.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.426, 10.100], loss: 0.393757, mae: 0.529814, mean_q: 4.821628
 78670/100000: episode: 1422, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 74.261, mean reward: 5.712 [3.014, 14.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.585, 10.100], loss: 0.438870, mae: 0.536730, mean_q: 5.060588
 78686/100000: episode: 1423, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 81.164, mean reward: 5.073 [2.848, 9.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.166, 10.558], loss: 0.429322, mae: 0.577380, mean_q: 5.139241
 78703/100000: episode: 1424, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 135.176, mean reward: 7.952 [4.293, 30.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.689, 10.635], loss: 0.656386, mae: 0.628847, mean_q: 5.096768
 78716/100000: episode: 1425, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 100.659, mean reward: 7.743 [4.156, 13.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.577, 10.100], loss: 0.789768, mae: 0.661432, mean_q: 5.091124
 78728/100000: episode: 1426, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 45.005, mean reward: 3.750 [2.440, 6.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.307, 10.100], loss: 0.741644, mae: 0.662588, mean_q: 5.329744
 78740/100000: episode: 1427, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 53.999, mean reward: 4.500 [2.754, 12.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.395, 10.100], loss: 0.481801, mae: 0.538945, mean_q: 5.035275
 78753/100000: episode: 1428, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 41.810, mean reward: 3.216 [2.353, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.419, 10.100], loss: 0.833715, mae: 0.672794, mean_q: 5.171573
 78766/100000: episode: 1429, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 51.649, mean reward: 3.973 [2.972, 5.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.539, 10.100], loss: 0.543134, mae: 0.589601, mean_q: 4.984544
 78779/100000: episode: 1430, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 63.836, mean reward: 4.910 [2.889, 8.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.487, 10.100], loss: 0.538666, mae: 0.602715, mean_q: 5.279548
 78792/100000: episode: 1431, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 43.900, mean reward: 3.377 [2.638, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.412, 10.100], loss: 1.091511, mae: 0.600378, mean_q: 5.059065
 78804/100000: episode: 1432, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 56.518, mean reward: 4.710 [3.302, 8.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.402, 10.100], loss: 0.422649, mae: 0.589868, mean_q: 5.079116
 78817/100000: episode: 1433, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 84.423, mean reward: 6.494 [5.026, 8.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.467, 10.100], loss: 0.614271, mae: 0.616629, mean_q: 5.198390
 78830/100000: episode: 1434, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 58.149, mean reward: 4.473 [3.498, 6.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.604, 10.100], loss: 0.555885, mae: 0.630539, mean_q: 5.289096
 78843/100000: episode: 1435, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 37.970, mean reward: 2.921 [2.345, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.374, 10.100], loss: 0.732002, mae: 0.737910, mean_q: 5.445757
 78856/100000: episode: 1436, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 51.408, mean reward: 3.954 [2.443, 7.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.312, 10.100], loss: 0.465869, mae: 0.575707, mean_q: 5.135749
 78869/100000: episode: 1437, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 53.231, mean reward: 4.095 [3.022, 6.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.404, 10.100], loss: 0.704341, mae: 0.654958, mean_q: 5.157422
 78882/100000: episode: 1438, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 45.238, mean reward: 3.480 [2.348, 9.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.432, 10.100], loss: 0.478974, mae: 0.594564, mean_q: 5.328731
 78894/100000: episode: 1439, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 35.993, mean reward: 2.999 [2.528, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.251, 10.100], loss: 0.754530, mae: 0.668078, mean_q: 5.292006
 78910/100000: episode: 1440, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 182.906, mean reward: 11.432 [4.181, 34.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.328, 10.618], loss: 0.391376, mae: 0.543037, mean_q: 5.212171
 78920/100000: episode: 1441, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 33.273, mean reward: 3.327 [2.685, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.311, 10.100], loss: 0.590926, mae: 0.602486, mean_q: 5.306891
 78931/100000: episode: 1442, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 38.316, mean reward: 3.483 [3.193, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.490, 10.100], loss: 0.633547, mae: 0.721478, mean_q: 5.503141
 78948/100000: episode: 1443, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 77.384, mean reward: 4.552 [3.355, 6.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.216, 10.539], loss: 1.220568, mae: 0.718941, mean_q: 5.395563
 78960/100000: episode: 1444, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 46.285, mean reward: 3.857 [2.719, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.634, 10.100], loss: 1.346447, mae: 0.688025, mean_q: 5.137126
 78973/100000: episode: 1445, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 49.554, mean reward: 3.812 [2.514, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.238, 10.100], loss: 0.698387, mae: 0.730401, mean_q: 5.397388
 78983/100000: episode: 1446, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 48.692, mean reward: 4.869 [3.432, 8.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.454, 10.100], loss: 0.392715, mae: 0.577468, mean_q: 5.162339
 78995/100000: episode: 1447, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 39.806, mean reward: 3.317 [2.830, 5.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.313, 10.100], loss: 0.721783, mae: 0.705410, mean_q: 5.250409
 79008/100000: episode: 1448, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 43.591, mean reward: 3.353 [2.795, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.358, 10.100], loss: 1.582785, mae: 0.775958, mean_q: 5.503127
 79024/100000: episode: 1449, duration: 0.113s, episode steps: 16, steps per second: 142, episode reward: 57.316, mean reward: 3.582 [1.886, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.369], loss: 0.610856, mae: 0.670547, mean_q: 5.256790
[Info] 3-TH LEVEL FOUND: 10.974940299987793, Considering 10/90 traces
 79036/100000: episode: 1450, duration: 4.373s, episode steps: 12, steps per second: 3, episode reward: 53.322, mean reward: 4.444 [2.658, 10.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.313, 10.100], loss: 0.468530, mae: 0.592562, mean_q: 5.322989
 79053/100000: episode: 1451, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 74.554, mean reward: 4.386 [3.441, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.480], loss: 0.415025, mae: 0.578932, mean_q: 5.235317
 79070/100000: episode: 1452, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 54.495, mean reward: 3.206 [2.376, 6.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.465], loss: 0.587793, mae: 0.658371, mean_q: 5.363723
 79087/100000: episode: 1453, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 104.835, mean reward: 6.167 [4.002, 11.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.948, 10.600], loss: 1.490094, mae: 0.842432, mean_q: 5.539584
 79104/100000: episode: 1454, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 60.000, mean reward: 3.529 [2.299, 5.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.169, 10.426], loss: 1.190852, mae: 0.823307, mean_q: 5.625883
 79121/100000: episode: 1455, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 76.149, mean reward: 4.479 [3.309, 6.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.625, 10.611], loss: 1.454147, mae: 0.799760, mean_q: 5.412302
 79138/100000: episode: 1456, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 122.483, mean reward: 7.205 [3.550, 23.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.098, 10.480], loss: 1.324808, mae: 0.776895, mean_q: 5.667787
 79155/100000: episode: 1457, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 96.103, mean reward: 5.653 [3.670, 10.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.610], loss: 1.062335, mae: 0.680656, mean_q: 5.381397
 79167/100000: episode: 1458, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 55.445, mean reward: 4.620 [3.320, 7.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.484, 10.501], loss: 1.514917, mae: 0.874511, mean_q: 5.660828
 79181/100000: episode: 1459, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 134.994, mean reward: 9.642 [5.225, 16.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.258, 10.647], loss: 0.723520, mae: 0.764078, mean_q: 5.630073
 79193/100000: episode: 1460, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 240.252, mean reward: 20.021 [4.108, 142.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.554, 10.467], loss: 2.171505, mae: 0.933937, mean_q: 5.719021
[Info] FALSIFICATION!
 79201/100000: episode: 1461, duration: 0.242s, episode steps: 8, steps per second: 33, episode reward: 1065.188, mean reward: 133.149 [3.641, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.016, 10.631], loss: 0.578151, mae: 0.658107, mean_q: 5.201489
 79218/100000: episode: 1462, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 160.681, mean reward: 9.452 [4.272, 22.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.302, 10.670], loss: 18.589748, mae: 1.408744, mean_q: 5.919967
[Info] FALSIFICATION!
 79235/100000: episode: 1463, duration: 0.253s, episode steps: 17, steps per second: 67, episode reward: 2380.044, mean reward: 140.003 [3.813, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.717, 10.624], loss: 2.525055, mae: 1.156641, mean_q: 5.700650
 79252/100000: episode: 1464, duration: 0.108s, episode steps: 17, steps per second: 158, episode reward: 106.063, mean reward: 6.239 [3.916, 8.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.051, 10.499], loss: 3.480413, mae: 1.142104, mean_q: 6.061484
 79269/100000: episode: 1465, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 84.630, mean reward: 4.978 [3.339, 10.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.729, 10.530], loss: 7.669465, mae: 1.100963, mean_q: 6.007892
 79286/100000: episode: 1466, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 108.262, mean reward: 6.368 [4.211, 11.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.159, 10.600], loss: 4.419962, mae: 1.145269, mean_q: 5.929593
 79303/100000: episode: 1467, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 65.041, mean reward: 3.826 [2.732, 7.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.290, 10.451], loss: 3.781344, mae: 1.074204, mean_q: 6.290763
 79320/100000: episode: 1468, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 65.503, mean reward: 3.853 [3.176, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.525, 10.475], loss: 1804.184570, mae: 9.176377, mean_q: 11.461316
 79337/100000: episode: 1469, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 110.627, mean reward: 6.507 [3.770, 13.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.523], loss: 21.828142, mae: 2.562795, mean_q: 4.450814
 79354/100000: episode: 1470, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 286.818, mean reward: 16.872 [3.128, 194.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.376, 10.506], loss: 4.665781, mae: 1.601843, mean_q: 4.851026
 79371/100000: episode: 1471, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 83.147, mean reward: 4.891 [3.494, 7.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.630, 10.519], loss: 1.227274, mae: 1.074228, mean_q: 5.753650
 79388/100000: episode: 1472, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 84.777, mean reward: 4.987 [3.476, 7.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.490, 10.589], loss: 913.635254, mae: 4.504627, mean_q: 8.036485
 79405/100000: episode: 1473, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 82.097, mean reward: 4.829 [3.497, 6.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.404, 10.553], loss: 1.897317, mae: 1.253512, mean_q: 5.608685
 79422/100000: episode: 1474, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 61.748, mean reward: 3.632 [2.719, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.457], loss: 4.213315, mae: 1.318665, mean_q: 5.572323
 79439/100000: episode: 1475, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 69.471, mean reward: 4.087 [3.181, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.589], loss: 21.668930, mae: 1.626613, mean_q: 6.240925
 79456/100000: episode: 1476, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 83.520, mean reward: 4.913 [3.627, 6.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.597, 10.554], loss: 36.545715, mae: 1.811033, mean_q: 6.552337
 79470/100000: episode: 1477, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 91.449, mean reward: 6.532 [4.122, 12.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.657, 10.446], loss: 40.631504, mae: 1.705391, mean_q: 6.357686
 79484/100000: episode: 1478, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 192.101, mean reward: 13.722 [4.328, 92.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.539], loss: 2164.093994, mae: 6.370883, mean_q: 7.175429
 79501/100000: episode: 1479, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 670.350, mean reward: 39.432 [3.292, 463.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.176, 10.610], loss: 1790.500488, mae: 11.745115, mean_q: 14.211952
 79518/100000: episode: 1480, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 88.752, mean reward: 5.221 [2.725, 12.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.471], loss: 4.770245, mae: 2.084355, mean_q: 6.687253
 79535/100000: episode: 1481, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 71.678, mean reward: 4.216 [2.806, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.671, 10.613], loss: 906.324463, mae: 3.306618, mean_q: 5.729046
 79552/100000: episode: 1482, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 83.592, mean reward: 4.917 [3.550, 6.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.707, 10.561], loss: 5.315517, mae: 2.312521, mean_q: 8.224988
 79566/100000: episode: 1483, duration: 0.088s, episode steps: 14, steps per second: 158, episode reward: 72.758, mean reward: 5.197 [4.225, 6.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.609], loss: 1.853293, mae: 1.272607, mean_q: 6.869075
 79583/100000: episode: 1484, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 82.411, mean reward: 4.848 [3.428, 8.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.627], loss: 185.831924, mae: 2.136057, mean_q: 6.964124
 79600/100000: episode: 1485, duration: 0.129s, episode steps: 17, steps per second: 132, episode reward: 111.461, mean reward: 6.557 [3.800, 11.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.639], loss: 2.063366, mae: 1.415812, mean_q: 7.253632
 79617/100000: episode: 1486, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 92.582, mean reward: 5.446 [2.245, 19.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.535, 10.479], loss: 184.515381, mae: 1.918339, mean_q: 6.525007
[Info] FALSIFICATION!
 79628/100000: episode: 1487, duration: 0.324s, episode steps: 11, steps per second: 34, episode reward: 1115.650, mean reward: 101.423 [3.764, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.017, 10.357], loss: 1649.259766, mae: 6.031398, mean_q: 8.212017
 79645/100000: episode: 1488, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 80.353, mean reward: 4.727 [2.846, 7.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.770, 10.543], loss: 25.226511, mae: 5.065001, mean_q: 11.654226
 79657/100000: episode: 1489, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 72.981, mean reward: 6.082 [4.026, 8.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.337, 10.584], loss: 4.317948, mae: 1.696434, mean_q: 7.583862
 79674/100000: episode: 1490, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 62.582, mean reward: 3.681 [2.930, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.684, 10.499], loss: 3.353082, mae: 1.593321, mean_q: 6.280499
 79691/100000: episode: 1491, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 76.687, mean reward: 4.511 [3.586, 7.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.550], loss: 2.263895, mae: 1.187475, mean_q: 6.702846
 79708/100000: episode: 1492, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 111.922, mean reward: 6.584 [4.318, 10.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.416, 10.582], loss: 1803.190308, mae: 7.135330, mean_q: 9.996104
 79725/100000: episode: 1493, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 160.525, mean reward: 9.443 [5.064, 14.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.491], loss: 1051.892578, mae: 5.196777, mean_q: 8.847890
 79742/100000: episode: 1494, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 58.150, mean reward: 3.421 [2.360, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.173, 10.420], loss: 8.118770, mae: 2.460498, mean_q: 9.153539
 79759/100000: episode: 1495, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 127.496, mean reward: 7.500 [3.510, 18.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.294, 10.581], loss: 2.479516, mae: 1.339278, mean_q: 7.377332
 79773/100000: episode: 1496, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 63.072, mean reward: 4.505 [3.110, 8.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.674, 10.579], loss: 1.856896, mae: 1.182487, mean_q: 7.210478
 79790/100000: episode: 1497, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 79.048, mean reward: 4.650 [3.503, 5.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.422], loss: 908.775330, mae: 3.575882, mean_q: 8.214104
 79807/100000: episode: 1498, duration: 0.117s, episode steps: 17, steps per second: 146, episode reward: 96.552, mean reward: 5.680 [3.466, 9.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.310, 10.463], loss: 7.656114, mae: 2.194433, mean_q: 8.893170
 79824/100000: episode: 1499, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 51.789, mean reward: 3.046 [2.254, 4.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.403], loss: 4.531663, mae: 1.329776, mean_q: 6.974330
 79841/100000: episode: 1500, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 285.615, mean reward: 16.801 [4.871, 136.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.254, 10.782], loss: 182.531158, mae: 1.944051, mean_q: 7.547855
 79858/100000: episode: 1501, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 112.125, mean reward: 6.596 [3.998, 22.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.608], loss: 1056.479980, mae: 5.439458, mean_q: 9.785540
 79875/100000: episode: 1502, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 99.990, mean reward: 5.882 [3.580, 8.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.397, 10.550], loss: 11.166132, mae: 2.558107, mean_q: 9.470101
 79892/100000: episode: 1503, duration: 0.119s, episode steps: 17, steps per second: 142, episode reward: 85.398, mean reward: 5.023 [3.856, 8.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.542, 10.595], loss: 881.997498, mae: 3.074353, mean_q: 7.535020
 79906/100000: episode: 1504, duration: 0.091s, episode steps: 14, steps per second: 155, episode reward: 208.099, mean reward: 14.864 [4.846, 39.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.655], loss: 29.243914, mae: 3.146977, mean_q: 9.733777
 79923/100000: episode: 1505, duration: 0.119s, episode steps: 17, steps per second: 143, episode reward: 80.693, mean reward: 4.747 [3.032, 6.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.154, 10.475], loss: 1072.959595, mae: 4.221530, mean_q: 8.220975
 79940/100000: episode: 1506, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 134.597, mean reward: 7.917 [3.728, 22.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.337, 10.636], loss: 7.699246, mae: 2.770604, mean_q: 10.033164
 79957/100000: episode: 1507, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 76.398, mean reward: 4.494 [3.111, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.433], loss: 3.713007, mae: 1.590697, mean_q: 7.752388
 79974/100000: episode: 1508, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 111.596, mean reward: 6.564 [4.463, 14.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.455, 10.485], loss: 937.088501, mae: 4.441554, mean_q: 8.400809
 79991/100000: episode: 1509, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 90.675, mean reward: 5.334 [3.516, 8.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.319, 10.470], loss: 10.601704, mae: 2.931875, mean_q: 9.836264
 80003/100000: episode: 1510, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 64.030, mean reward: 5.336 [3.419, 7.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.537], loss: 2.794491, mae: 1.278116, mean_q: 7.548806
 80020/100000: episode: 1511, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 54.291, mean reward: 3.194 [2.583, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.478], loss: 1059.086914, mae: 4.944655, mean_q: 9.186117
 80037/100000: episode: 1512, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 184.167, mean reward: 10.833 [3.852, 33.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.563], loss: 1800.211182, mae: 7.160588, mean_q: 10.781792
 80054/100000: episode: 1513, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 91.800, mean reward: 5.400 [3.614, 14.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.528], loss: 12.040187, mae: 3.135111, mean_q: 10.579725
 80071/100000: episode: 1514, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 80.512, mean reward: 4.736 [3.400, 7.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.145, 10.484], loss: 1837.533203, mae: 6.334716, mean_q: 9.456120
 80088/100000: episode: 1515, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 120.465, mean reward: 7.086 [3.978, 16.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.860, 10.494], loss: 909.278198, mae: 7.576325, mean_q: 13.312454
 80105/100000: episode: 1516, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 161.256, mean reward: 9.486 [3.321, 29.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.255, 10.758], loss: 6.411138, mae: 2.421019, mean_q: 9.986527
 80122/100000: episode: 1517, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 61.810, mean reward: 3.636 [2.653, 4.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.723, 10.504], loss: 918.318787, mae: 4.297577, mean_q: 9.567139
 80139/100000: episode: 1518, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 123.311, mean reward: 7.254 [3.558, 17.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.674], loss: 11.107515, mae: 2.140530, mean_q: 9.593328
 80156/100000: episode: 1519, duration: 0.109s, episode steps: 17, steps per second: 155, episode reward: 58.957, mean reward: 3.468 [2.463, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.672, 10.452], loss: 908.000122, mae: 3.286441, mean_q: 8.303239
 80173/100000: episode: 1520, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 77.066, mean reward: 4.533 [3.306, 8.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.840, 10.470], loss: 39.134335, mae: 2.989239, mean_q: 10.458301
 80190/100000: episode: 1521, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 76.370, mean reward: 4.492 [3.265, 7.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.649], loss: 4.608182, mae: 1.862397, mean_q: 9.293203
 80207/100000: episode: 1522, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 57.127, mean reward: 3.360 [2.714, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.504], loss: 5.266931, mae: 2.012803, mean_q: 9.429266
 80224/100000: episode: 1523, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 73.322, mean reward: 4.313 [3.117, 5.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.211, 10.518], loss: 3.697057, mae: 1.626828, mean_q: 8.683418
 80241/100000: episode: 1524, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 110.979, mean reward: 6.528 [3.769, 22.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.202, 10.640], loss: 184.312836, mae: 2.292841, mean_q: 8.473447
 80255/100000: episode: 1525, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 128.409, mean reward: 9.172 [6.679, 12.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.591], loss: 218.646042, mae: 2.592645, mean_q: 9.544777
 80272/100000: episode: 1526, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 112.977, mean reward: 6.646 [4.418, 11.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.435, 10.423], loss: 42.471027, mae: 3.799829, mean_q: 11.570512
 80284/100000: episode: 1527, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 204.643, mean reward: 17.054 [5.428, 74.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.625], loss: 50.136761, mae: 2.915257, mean_q: 10.190364
 80298/100000: episode: 1528, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 70.643, mean reward: 5.046 [3.283, 9.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.525], loss: 19.061384, mae: 1.962974, mean_q: 8.496559
 80315/100000: episode: 1529, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 145.540, mean reward: 8.561 [3.843, 36.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-1.171, 10.573], loss: 4.524555, mae: 1.728189, mean_q: 8.766180
 80332/100000: episode: 1530, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 61.747, mean reward: 3.632 [2.603, 6.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.361], loss: 259.041290, mae: 3.373879, mean_q: 9.045997
 80349/100000: episode: 1531, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 113.779, mean reward: 6.693 [4.023, 11.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.637], loss: 1746.498779, mae: 6.887615, mean_q: 11.373468
 80363/100000: episode: 1532, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 58.866, mean reward: 4.205 [2.787, 6.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.808, 10.509], loss: 50.228645, mae: 4.377099, mean_q: 12.262519
 80380/100000: episode: 1533, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 79.937, mean reward: 4.702 [3.547, 9.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.519], loss: 1770.354614, mae: 6.599798, mean_q: 10.826138
 80392/100000: episode: 1534, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 45.809, mean reward: 3.817 [2.949, 5.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.469], loss: 1260.941406, mae: 7.389206, mean_q: 13.045410
 80409/100000: episode: 1535, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 86.686, mean reward: 5.099 [3.104, 8.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.562], loss: 17.604076, mae: 2.452290, mean_q: 10.456937
 80426/100000: episode: 1536, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 63.191, mean reward: 3.717 [2.943, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.082, 10.434], loss: 23.816086, mae: 1.966781, mean_q: 8.769566
 80443/100000: episode: 1537, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 82.140, mean reward: 4.832 [3.453, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.602], loss: 8.468935, mae: 1.567894, mean_q: 9.044818
 80460/100000: episode: 1538, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 90.428, mean reward: 5.319 [4.339, 7.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.600], loss: 5.905554, mae: 1.714741, mean_q: 9.174678
 80477/100000: episode: 1539, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 113.924, mean reward: 6.701 [4.847, 20.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.670], loss: 906.641418, mae: 3.856538, mean_q: 9.415627
[Info] Complete ISplit Iteration
[Info] Levels: [5.302621, 7.99661, 10.97494, 17.336664]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.58]
[Info] Error Prob: 0.0005800000000000001

 80494/100000: episode: 1540, duration: 4.616s, episode steps: 17, steps per second: 4, episode reward: 131.380, mean reward: 7.728 [3.849, 13.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.659], loss: 12.039248, mae: 3.019660, mean_q: 11.267464
 80594/100000: episode: 1541, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 192.919, mean reward: 1.929 [1.446, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.169, 10.098], loss: 193.888611, mae: 2.489708, mean_q: 9.353799
 80694/100000: episode: 1542, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 195.405, mean reward: 1.954 [1.487, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.310], loss: 770.003540, mae: 4.354375, mean_q: 10.386524
 80794/100000: episode: 1543, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 224.573, mean reward: 2.246 [1.559, 8.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.098], loss: 201.525818, mae: 2.660427, mean_q: 9.522651
 80894/100000: episode: 1544, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.580, mean reward: 1.886 [1.458, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.176], loss: 464.967865, mae: 3.397521, mean_q: 10.047754
 80994/100000: episode: 1545, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.458, mean reward: 1.905 [1.486, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.079, 10.299], loss: 649.780518, mae: 4.132276, mean_q: 10.511710
 81094/100000: episode: 1546, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 193.115, mean reward: 1.931 [1.458, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.971, 10.251], loss: 158.504211, mae: 2.023780, mean_q: 8.917785
 81194/100000: episode: 1547, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.767, mean reward: 1.918 [1.488, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.098], loss: 8.608035, mae: 1.669693, mean_q: 8.560101
 81294/100000: episode: 1548, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 202.374, mean reward: 2.024 [1.543, 4.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.285, 10.182], loss: 310.568481, mae: 2.296794, mean_q: 8.415261
 81394/100000: episode: 1549, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 204.471, mean reward: 2.045 [1.472, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.534, 10.098], loss: 908.707092, mae: 5.077450, mean_q: 10.727090
 81494/100000: episode: 1550, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.336, mean reward: 1.953 [1.447, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.940, 10.429], loss: 222.607407, mae: 2.432687, mean_q: 9.011730
 81594/100000: episode: 1551, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 186.794, mean reward: 1.868 [1.447, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.106, 10.115], loss: 348.288757, mae: 2.959108, mean_q: 9.422113
 81694/100000: episode: 1552, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 202.179, mean reward: 2.022 [1.513, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.163, 10.098], loss: 924.437744, mae: 4.932928, mean_q: 10.745741
 81794/100000: episode: 1553, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.745, mean reward: 1.967 [1.441, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.322, 10.098], loss: 614.029175, mae: 3.813741, mean_q: 9.874712
 81894/100000: episode: 1554, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 241.779, mean reward: 2.418 [1.452, 8.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.314, 10.587], loss: 306.091492, mae: 2.501873, mean_q: 9.054956
 81994/100000: episode: 1555, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.877, mean reward: 1.979 [1.494, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.133], loss: 914.869080, mae: 5.031154, mean_q: 10.611281
 82094/100000: episode: 1556, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.042, mean reward: 1.910 [1.468, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.974, 10.199], loss: 539.615601, mae: 3.724792, mean_q: 10.147191
 82194/100000: episode: 1557, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 187.759, mean reward: 1.878 [1.452, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.619, 10.251], loss: 307.448547, mae: 2.726229, mean_q: 9.215445
 82294/100000: episode: 1558, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 195.161, mean reward: 1.952 [1.477, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.259, 10.214], loss: 338.888672, mae: 2.710760, mean_q: 9.317449
 82394/100000: episode: 1559, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.530, mean reward: 1.905 [1.458, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.952, 10.201], loss: 309.727448, mae: 2.530680, mean_q: 9.004206
 82494/100000: episode: 1560, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 209.804, mean reward: 2.098 [1.469, 10.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.722, 10.098], loss: 380.094116, mae: 2.972979, mean_q: 8.920583
 82594/100000: episode: 1561, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.043, mean reward: 1.890 [1.476, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.724, 10.184], loss: 611.559509, mae: 3.859613, mean_q: 9.768225
 82694/100000: episode: 1562, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 190.865, mean reward: 1.909 [1.437, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.422, 10.098], loss: 745.639404, mae: 4.423974, mean_q: 10.599665
 82794/100000: episode: 1563, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 205.899, mean reward: 2.059 [1.546, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.350], loss: 497.539795, mae: 2.990019, mean_q: 9.103462
 82894/100000: episode: 1564, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 186.632, mean reward: 1.866 [1.463, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.259], loss: 656.015930, mae: 4.115634, mean_q: 10.293971
 82994/100000: episode: 1565, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.793, mean reward: 1.828 [1.456, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.828, 10.098], loss: 322.084015, mae: 2.676194, mean_q: 9.180631
 83094/100000: episode: 1566, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 184.120, mean reward: 1.841 [1.431, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.525, 10.098], loss: 169.256775, mae: 2.064723, mean_q: 8.394747
 83194/100000: episode: 1567, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 204.989, mean reward: 2.050 [1.484, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.382, 10.098], loss: 497.159760, mae: 3.276186, mean_q: 9.050321
 83294/100000: episode: 1568, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 195.030, mean reward: 1.950 [1.458, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.479, 10.375], loss: 316.429779, mae: 2.185654, mean_q: 7.911262
 83394/100000: episode: 1569, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 206.829, mean reward: 2.068 [1.499, 5.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.728, 10.098], loss: 311.216766, mae: 2.376140, mean_q: 8.227817
 83494/100000: episode: 1570, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 213.951, mean reward: 2.140 [1.465, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.200, 10.098], loss: 604.897034, mae: 3.716623, mean_q: 9.149282
 83594/100000: episode: 1571, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 208.780, mean reward: 2.088 [1.447, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.835, 10.098], loss: 469.183777, mae: 3.199709, mean_q: 8.718398
 83694/100000: episode: 1572, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 197.843, mean reward: 1.978 [1.487, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.956, 10.098], loss: 311.949646, mae: 2.326020, mean_q: 8.179856
 83794/100000: episode: 1573, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.334, mean reward: 1.993 [1.543, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.589, 10.113], loss: 490.016235, mae: 2.977650, mean_q: 8.524163
 83894/100000: episode: 1574, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 202.361, mean reward: 2.024 [1.472, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.905, 10.313], loss: 331.808075, mae: 2.767488, mean_q: 8.536304
 83994/100000: episode: 1575, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 202.665, mean reward: 2.027 [1.499, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.819, 10.098], loss: 467.511200, mae: 2.646991, mean_q: 7.658728
 84094/100000: episode: 1576, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.341, mean reward: 1.853 [1.441, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.277, 10.329], loss: 647.993591, mae: 3.995115, mean_q: 9.269308
 84194/100000: episode: 1577, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 192.384, mean reward: 1.924 [1.480, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.241, 10.098], loss: 150.054489, mae: 1.615598, mean_q: 7.157809
 84294/100000: episode: 1578, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.643, mean reward: 1.876 [1.447, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.590, 10.098], loss: 208.351379, mae: 1.729336, mean_q: 6.785195
 84394/100000: episode: 1579, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.204, mean reward: 1.952 [1.478, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 33.404770, mae: 1.058274, mean_q: 6.253560
 84494/100000: episode: 1580, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 183.093, mean reward: 1.831 [1.456, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.281, 10.098], loss: 3.055888, mae: 0.854956, mean_q: 5.726938
 84594/100000: episode: 1581, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 194.420, mean reward: 1.944 [1.474, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.699, 10.098], loss: 2.848921, mae: 0.780522, mean_q: 5.462762
 84694/100000: episode: 1582, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 214.006, mean reward: 2.140 [1.483, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.033, 10.358], loss: 1.763269, mae: 0.693683, mean_q: 5.161283
 84794/100000: episode: 1583, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.333, mean reward: 1.993 [1.504, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.804, 10.150], loss: 6.854462, mae: 0.819656, mean_q: 5.150561
 84894/100000: episode: 1584, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 215.154, mean reward: 2.152 [1.525, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.931, 10.205], loss: 1.999227, mae: 0.661002, mean_q: 4.894414
 84994/100000: episode: 1585, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 201.776, mean reward: 2.018 [1.489, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.181, 10.152], loss: 0.580522, mae: 0.522739, mean_q: 4.590418
 85094/100000: episode: 1586, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 184.460, mean reward: 1.845 [1.448, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.098], loss: 1.338398, mae: 0.550468, mean_q: 4.528047
 85194/100000: episode: 1587, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 195.188, mean reward: 1.952 [1.439, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.681, 10.209], loss: 0.500629, mae: 0.473846, mean_q: 4.306046
 85294/100000: episode: 1588, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 202.109, mean reward: 2.021 [1.465, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.410, 10.098], loss: 0.256440, mae: 0.422944, mean_q: 4.109602
 85394/100000: episode: 1589, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 191.029, mean reward: 1.910 [1.439, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.309, 10.270], loss: 0.186458, mae: 0.399752, mean_q: 4.002287
 85494/100000: episode: 1590, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 195.846, mean reward: 1.958 [1.484, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.231, 10.098], loss: 0.197923, mae: 0.391823, mean_q: 3.934320
 85594/100000: episode: 1591, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 191.931, mean reward: 1.919 [1.496, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.071, 10.098], loss: 0.137491, mae: 0.369709, mean_q: 3.923879
 85694/100000: episode: 1592, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 205.133, mean reward: 2.051 [1.472, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.882, 10.098], loss: 0.145937, mae: 0.359952, mean_q: 3.915656
 85794/100000: episode: 1593, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 182.367, mean reward: 1.824 [1.449, 5.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.176, 10.218], loss: 0.125159, mae: 0.350972, mean_q: 3.908846
 85894/100000: episode: 1594, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 184.563, mean reward: 1.846 [1.437, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.712, 10.098], loss: 0.152555, mae: 0.360274, mean_q: 3.908882
 85994/100000: episode: 1595, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 191.683, mean reward: 1.917 [1.488, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.129, 10.098], loss: 0.131011, mae: 0.357038, mean_q: 3.895205
 86094/100000: episode: 1596, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 200.897, mean reward: 2.009 [1.470, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.152, 10.098], loss: 0.138179, mae: 0.370594, mean_q: 3.888342
 86194/100000: episode: 1597, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 208.682, mean reward: 2.087 [1.455, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.026, 10.449], loss: 0.123712, mae: 0.357644, mean_q: 3.895661
 86294/100000: episode: 1598, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.698, mean reward: 1.957 [1.439, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.277, 10.309], loss: 0.121205, mae: 0.350791, mean_q: 3.898747
 86394/100000: episode: 1599, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 202.100, mean reward: 2.021 [1.599, 5.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.505, 10.098], loss: 0.146466, mae: 0.367575, mean_q: 3.894007
 86494/100000: episode: 1600, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 200.136, mean reward: 2.001 [1.457, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.970, 10.317], loss: 0.143049, mae: 0.365698, mean_q: 3.923266
 86594/100000: episode: 1601, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.810, mean reward: 1.878 [1.459, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.942, 10.131], loss: 0.139956, mae: 0.357631, mean_q: 3.912803
 86694/100000: episode: 1602, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 204.077, mean reward: 2.041 [1.437, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.094, 10.098], loss: 0.137452, mae: 0.355478, mean_q: 3.900175
 86794/100000: episode: 1603, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.853, mean reward: 2.009 [1.467, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.646, 10.098], loss: 0.133447, mae: 0.350763, mean_q: 3.893388
 86894/100000: episode: 1604, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 181.180, mean reward: 1.812 [1.446, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.984, 10.098], loss: 0.133256, mae: 0.350613, mean_q: 3.899901
 86994/100000: episode: 1605, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 195.441, mean reward: 1.954 [1.471, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.985, 10.267], loss: 0.124606, mae: 0.348284, mean_q: 3.901400
 87094/100000: episode: 1606, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 218.700, mean reward: 2.187 [1.439, 11.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.383, 10.255], loss: 0.117978, mae: 0.348312, mean_q: 3.896846
 87194/100000: episode: 1607, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 186.749, mean reward: 1.867 [1.441, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.164, 10.129], loss: 0.126058, mae: 0.337021, mean_q: 3.882615
 87294/100000: episode: 1608, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 196.536, mean reward: 1.965 [1.450, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.408, 10.215], loss: 0.135672, mae: 0.343505, mean_q: 3.895934
 87394/100000: episode: 1609, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 181.508, mean reward: 1.815 [1.438, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.437, 10.132], loss: 0.137776, mae: 0.343269, mean_q: 3.903258
 87494/100000: episode: 1610, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.499, mean reward: 1.875 [1.453, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.739, 10.098], loss: 0.102753, mae: 0.328991, mean_q: 3.872133
 87594/100000: episode: 1611, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 236.815, mean reward: 2.368 [1.478, 5.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.364, 10.596], loss: 0.102521, mae: 0.333607, mean_q: 3.881189
 87694/100000: episode: 1612, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 228.437, mean reward: 2.284 [1.531, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.894, 10.098], loss: 0.108581, mae: 0.333004, mean_q: 3.885319
 87794/100000: episode: 1613, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.262, mean reward: 1.873 [1.469, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.386, 10.271], loss: 0.138639, mae: 0.347393, mean_q: 3.916621
 87894/100000: episode: 1614, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.104, mean reward: 1.921 [1.460, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.047, 10.098], loss: 0.131652, mae: 0.345671, mean_q: 3.897571
 87994/100000: episode: 1615, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 236.811, mean reward: 2.368 [1.475, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.255], loss: 0.125214, mae: 0.349239, mean_q: 3.904791
 88094/100000: episode: 1616, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 187.959, mean reward: 1.880 [1.436, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.508, 10.306], loss: 0.128989, mae: 0.354748, mean_q: 3.920158
 88194/100000: episode: 1617, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 187.251, mean reward: 1.873 [1.442, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.006, 10.098], loss: 0.131126, mae: 0.342292, mean_q: 3.935579
 88294/100000: episode: 1618, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 218.094, mean reward: 2.181 [1.474, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.439], loss: 0.138101, mae: 0.349889, mean_q: 3.938512
 88394/100000: episode: 1619, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 182.120, mean reward: 1.821 [1.474, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.581, 10.251], loss: 0.135081, mae: 0.355015, mean_q: 3.934258
 88494/100000: episode: 1620, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 196.313, mean reward: 1.963 [1.455, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.073, 10.301], loss: 0.155515, mae: 0.362717, mean_q: 3.932148
 88594/100000: episode: 1621, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 221.750, mean reward: 2.218 [1.521, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.081, 10.098], loss: 0.134753, mae: 0.355624, mean_q: 3.948622
 88694/100000: episode: 1622, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 190.481, mean reward: 1.905 [1.453, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.488, 10.098], loss: 0.113953, mae: 0.343962, mean_q: 3.929840
 88794/100000: episode: 1623, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 195.670, mean reward: 1.957 [1.464, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.901, 10.098], loss: 0.145524, mae: 0.360851, mean_q: 3.943549
 88894/100000: episode: 1624, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 188.194, mean reward: 1.882 [1.478, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.263, 10.098], loss: 0.111927, mae: 0.339062, mean_q: 3.918049
 88994/100000: episode: 1625, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 206.500, mean reward: 2.065 [1.470, 6.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.170], loss: 0.152317, mae: 0.350180, mean_q: 3.921582
 89094/100000: episode: 1626, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 190.909, mean reward: 1.909 [1.499, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.686, 10.258], loss: 0.121932, mae: 0.349424, mean_q: 3.908356
 89194/100000: episode: 1627, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 175.893, mean reward: 1.759 [1.483, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.219], loss: 0.116842, mae: 0.340700, mean_q: 3.913556
 89294/100000: episode: 1628, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 201.498, mean reward: 2.015 [1.452, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.504, 10.237], loss: 0.124177, mae: 0.341564, mean_q: 3.920189
 89394/100000: episode: 1629, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 200.520, mean reward: 2.005 [1.456, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.835, 10.473], loss: 0.119131, mae: 0.341846, mean_q: 3.908715
 89494/100000: episode: 1630, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.794, mean reward: 1.978 [1.488, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.199], loss: 0.122711, mae: 0.350579, mean_q: 3.909573
 89594/100000: episode: 1631, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.961, mean reward: 1.900 [1.458, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.815, 10.115], loss: 0.118064, mae: 0.342317, mean_q: 3.915921
 89694/100000: episode: 1632, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 217.085, mean reward: 2.171 [1.456, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.217, 10.098], loss: 0.116219, mae: 0.342549, mean_q: 3.925239
 89794/100000: episode: 1633, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 179.671, mean reward: 1.797 [1.481, 2.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.428, 10.098], loss: 0.134804, mae: 0.345547, mean_q: 3.929785
 89894/100000: episode: 1634, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 190.408, mean reward: 1.904 [1.505, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.444, 10.315], loss: 0.110332, mae: 0.337181, mean_q: 3.900011
 89994/100000: episode: 1635, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.551, mean reward: 1.826 [1.463, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.098], loss: 0.111513, mae: 0.332847, mean_q: 3.906030
 90094/100000: episode: 1636, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 179.352, mean reward: 1.794 [1.460, 2.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.167, 10.112], loss: 0.118508, mae: 0.332471, mean_q: 3.884117
 90194/100000: episode: 1637, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 185.092, mean reward: 1.851 [1.448, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.171, 10.098], loss: 0.130948, mae: 0.345644, mean_q: 3.897473
 90294/100000: episode: 1638, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 189.332, mean reward: 1.893 [1.475, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.206], loss: 0.110665, mae: 0.313233, mean_q: 3.884366
 90394/100000: episode: 1639, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 220.835, mean reward: 2.208 [1.560, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.745, 10.469], loss: 0.127839, mae: 0.338614, mean_q: 3.897113
[Info] 1-TH LEVEL FOUND: 5.3125505447387695, Considering 10/90 traces
 90494/100000: episode: 1640, duration: 4.730s, episode steps: 100, steps per second: 21, episode reward: 183.353, mean reward: 1.834 [1.434, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.549, 10.098], loss: 0.113291, mae: 0.332494, mean_q: 3.898390
 90504/100000: episode: 1641, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 34.127, mean reward: 3.413 [2.874, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.343, 10.474], loss: 0.238108, mae: 0.380476, mean_q: 3.975391
 90521/100000: episode: 1642, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 51.753, mean reward: 3.044 [2.025, 5.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.365], loss: 0.100976, mae: 0.324687, mean_q: 3.867644
 90539/100000: episode: 1643, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 36.070, mean reward: 2.004 [1.508, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.094, 10.103], loss: 0.108211, mae: 0.325097, mean_q: 3.898042
 90553/100000: episode: 1644, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 32.205, mean reward: 2.300 [1.725, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.462, 10.302], loss: 0.203526, mae: 0.359238, mean_q: 3.865932
 90570/100000: episode: 1645, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 47.964, mean reward: 2.821 [2.170, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.317, 10.272], loss: 0.111238, mae: 0.340596, mean_q: 3.915535
 90584/100000: episode: 1646, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 36.326, mean reward: 2.595 [1.745, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.356], loss: 0.094093, mae: 0.320764, mean_q: 3.891191
 90598/100000: episode: 1647, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 29.754, mean reward: 2.125 [1.743, 2.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.160, 10.355], loss: 0.114618, mae: 0.341397, mean_q: 3.899800
 90615/100000: episode: 1648, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 38.420, mean reward: 2.260 [1.991, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.675, 10.346], loss: 0.162003, mae: 0.357384, mean_q: 3.939517
 90633/100000: episode: 1649, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 34.907, mean reward: 1.939 [1.536, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.171], loss: 0.097199, mae: 0.319682, mean_q: 3.858524
 90647/100000: episode: 1650, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 32.176, mean reward: 2.298 [1.888, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.261, 10.427], loss: 0.118698, mae: 0.325056, mean_q: 3.909072
 90657/100000: episode: 1651, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 34.722, mean reward: 3.472 [2.393, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.374], loss: 0.123849, mae: 0.323671, mean_q: 3.846030
 90672/100000: episode: 1652, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 39.783, mean reward: 2.652 [2.062, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.355], loss: 0.101013, mae: 0.319360, mean_q: 3.948459
 90689/100000: episode: 1653, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 37.002, mean reward: 2.177 [1.619, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.359, 10.352], loss: 0.101541, mae: 0.329963, mean_q: 3.936003
 90705/100000: episode: 1654, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 57.233, mean reward: 3.577 [3.139, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.493], loss: 0.136188, mae: 0.363192, mean_q: 3.941094
 90720/100000: episode: 1655, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 45.756, mean reward: 3.050 [2.296, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.457], loss: 0.134766, mae: 0.371505, mean_q: 3.902319
 90737/100000: episode: 1656, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 37.598, mean reward: 2.212 [1.564, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.229, 10.163], loss: 0.159453, mae: 0.332200, mean_q: 3.907433
 90753/100000: episode: 1657, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 27.934, mean reward: 1.746 [1.492, 2.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.242, 10.213], loss: 0.106174, mae: 0.312888, mean_q: 3.880217
 90769/100000: episode: 1658, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 56.502, mean reward: 3.531 [2.210, 5.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.502], loss: 0.140608, mae: 0.370138, mean_q: 3.982004
 90786/100000: episode: 1659, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 45.171, mean reward: 2.657 [2.058, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.458, 10.375], loss: 0.123359, mae: 0.357436, mean_q: 3.908115
 90803/100000: episode: 1660, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 31.183, mean reward: 1.834 [1.686, 2.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.237], loss: 0.233038, mae: 0.354568, mean_q: 3.955272
 90813/100000: episode: 1661, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 36.061, mean reward: 3.606 [2.654, 4.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.946, 10.478], loss: 0.113408, mae: 0.339154, mean_q: 3.948604
 90829/100000: episode: 1662, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 62.300, mean reward: 3.894 [2.376, 7.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.767, 10.602], loss: 0.145325, mae: 0.366243, mean_q: 3.973523
 90846/100000: episode: 1663, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 39.253, mean reward: 2.309 [2.018, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.265], loss: 0.147276, mae: 0.365955, mean_q: 3.961112
 90864/100000: episode: 1664, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 33.402, mean reward: 1.856 [1.460, 2.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.138, 10.171], loss: 0.151600, mae: 0.384392, mean_q: 3.991143
 90881/100000: episode: 1665, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 33.035, mean reward: 1.943 [1.558, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.439, 10.218], loss: 0.239728, mae: 0.398936, mean_q: 4.063690
 90891/100000: episode: 1666, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 33.669, mean reward: 3.367 [2.499, 4.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.411], loss: 0.117436, mae: 0.341631, mean_q: 3.880606
 90908/100000: episode: 1667, duration: 0.120s, episode steps: 17, steps per second: 142, episode reward: 42.249, mean reward: 2.485 [2.039, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.090, 10.423], loss: 0.123062, mae: 0.355862, mean_q: 3.944489
 90918/100000: episode: 1668, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 38.344, mean reward: 3.834 [2.903, 4.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.540], loss: 0.141150, mae: 0.378508, mean_q: 3.987504
 90935/100000: episode: 1669, duration: 0.099s, episode steps: 17, steps per second: 173, episode reward: 39.609, mean reward: 2.330 [1.808, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.320, 10.296], loss: 0.136777, mae: 0.363081, mean_q: 3.972072
 90952/100000: episode: 1670, duration: 0.129s, episode steps: 17, steps per second: 132, episode reward: 47.152, mean reward: 2.774 [2.024, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.140, 10.548], loss: 0.154052, mae: 0.364948, mean_q: 3.964051
 90968/100000: episode: 1671, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 46.165, mean reward: 2.885 [2.254, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.423], loss: 0.135784, mae: 0.374957, mean_q: 3.965751
 90983/100000: episode: 1672, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 33.108, mean reward: 2.207 [1.832, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.155, 10.311], loss: 0.123315, mae: 0.358392, mean_q: 4.016201
 90997/100000: episode: 1673, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 33.452, mean reward: 2.389 [1.797, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.169, 10.288], loss: 0.134549, mae: 0.362665, mean_q: 4.030600
 91013/100000: episode: 1674, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 46.038, mean reward: 2.877 [2.349, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.425], loss: 0.106889, mae: 0.334167, mean_q: 3.977416
 91027/100000: episode: 1675, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 27.784, mean reward: 1.985 [1.731, 2.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.094, 10.244], loss: 0.119485, mae: 0.359755, mean_q: 3.985410
 91045/100000: episode: 1676, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 38.335, mean reward: 2.130 [1.767, 2.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.776, 10.363], loss: 0.127412, mae: 0.369841, mean_q: 4.031528
 91061/100000: episode: 1677, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 41.682, mean reward: 2.605 [2.035, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.326], loss: 0.125841, mae: 0.345941, mean_q: 3.966073
 91078/100000: episode: 1678, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 44.929, mean reward: 2.643 [2.254, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.268], loss: 0.169239, mae: 0.347442, mean_q: 4.021952
 91095/100000: episode: 1679, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 44.687, mean reward: 2.629 [2.071, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.306, 10.357], loss: 0.154757, mae: 0.369841, mean_q: 3.988601
 91111/100000: episode: 1680, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 56.627, mean reward: 3.539 [2.628, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.345, 10.600], loss: 0.130331, mae: 0.361779, mean_q: 4.025154
 91125/100000: episode: 1681, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 35.931, mean reward: 2.567 [2.175, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.341], loss: 0.159132, mae: 0.345525, mean_q: 3.938403
 91141/100000: episode: 1682, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 36.539, mean reward: 2.284 [1.804, 5.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-3.194, 10.266], loss: 0.117027, mae: 0.343362, mean_q: 3.984641
 91159/100000: episode: 1683, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 42.823, mean reward: 2.379 [1.954, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.749, 10.444], loss: 0.244225, mae: 0.395323, mean_q: 4.033121
 91173/100000: episode: 1684, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 34.652, mean reward: 2.475 [1.947, 4.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.306, 10.345], loss: 0.132813, mae: 0.349402, mean_q: 4.059357
 91188/100000: episode: 1685, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 43.836, mean reward: 2.922 [2.336, 4.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.449], loss: 0.121251, mae: 0.354105, mean_q: 3.950022
 91205/100000: episode: 1686, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 41.691, mean reward: 2.452 [1.825, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.407], loss: 0.122027, mae: 0.353183, mean_q: 3.997488
 91221/100000: episode: 1687, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 35.808, mean reward: 2.238 [1.485, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.139, 10.135], loss: 0.132564, mae: 0.362945, mean_q: 3.986603
 91239/100000: episode: 1688, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 36.166, mean reward: 2.009 [1.807, 2.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.315], loss: 0.131811, mae: 0.378082, mean_q: 3.981828
 91255/100000: episode: 1689, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 32.325, mean reward: 2.020 [1.649, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.228, 10.220], loss: 0.108863, mae: 0.340504, mean_q: 3.955206
 91265/100000: episode: 1690, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 44.551, mean reward: 4.455 [2.956, 7.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.517], loss: 0.127463, mae: 0.362580, mean_q: 4.003054
 91282/100000: episode: 1691, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 40.015, mean reward: 2.354 [1.605, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.152], loss: 0.133980, mae: 0.374165, mean_q: 4.026054
 91296/100000: episode: 1692, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 28.731, mean reward: 2.052 [1.640, 2.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.052, 10.233], loss: 0.136506, mae: 0.351433, mean_q: 3.975839
 91311/100000: episode: 1693, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 31.684, mean reward: 2.112 [1.758, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.300], loss: 0.206162, mae: 0.379402, mean_q: 4.026149
 91329/100000: episode: 1694, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 34.617, mean reward: 1.923 [1.595, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.197, 10.215], loss: 0.147721, mae: 0.373488, mean_q: 4.009048
 91346/100000: episode: 1695, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 43.337, mean reward: 2.549 [2.006, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.255, 10.371], loss: 0.118493, mae: 0.360175, mean_q: 4.029483
 91363/100000: episode: 1696, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 34.038, mean reward: 2.002 [1.470, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.146, 10.179], loss: 0.130786, mae: 0.358665, mean_q: 4.006523
 91378/100000: episode: 1697, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 51.127, mean reward: 3.408 [2.666, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.751, 10.490], loss: 0.165275, mae: 0.369010, mean_q: 3.988787
 91392/100000: episode: 1698, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 36.656, mean reward: 2.618 [2.126, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.127, 10.336], loss: 0.108058, mae: 0.340894, mean_q: 3.989517
 91409/100000: episode: 1699, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 49.507, mean reward: 2.912 [2.401, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.383], loss: 0.153098, mae: 0.376822, mean_q: 4.051577
 91427/100000: episode: 1700, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 38.345, mean reward: 2.130 [1.667, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.061, 10.199], loss: 0.206154, mae: 0.416388, mean_q: 4.044845
 91445/100000: episode: 1701, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 38.199, mean reward: 2.122 [1.684, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.897, 10.189], loss: 0.145417, mae: 0.381323, mean_q: 4.045831
 91461/100000: episode: 1702, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 49.555, mean reward: 3.097 [2.212, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.408], loss: 0.144721, mae: 0.385542, mean_q: 4.036008
 91478/100000: episode: 1703, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 32.421, mean reward: 1.907 [1.466, 2.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.124, 10.138], loss: 0.242157, mae: 0.431438, mean_q: 4.107772
 91495/100000: episode: 1704, duration: 0.081s, episode steps: 17, steps per second: 210, episode reward: 40.234, mean reward: 2.367 [1.736, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.490, 10.295], loss: 0.192635, mae: 0.433875, mean_q: 4.173104
 91505/100000: episode: 1705, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 51.329, mean reward: 5.133 [3.861, 7.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.743, 10.565], loss: 0.174729, mae: 0.398742, mean_q: 3.979566
 91520/100000: episode: 1706, duration: 0.100s, episode steps: 15, steps per second: 149, episode reward: 43.969, mean reward: 2.931 [2.271, 5.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.405], loss: 0.128798, mae: 0.356764, mean_q: 4.019987
 91536/100000: episode: 1707, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 43.429, mean reward: 2.714 [1.913, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.375], loss: 0.205379, mae: 0.407396, mean_q: 4.120813
 91550/100000: episode: 1708, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 38.625, mean reward: 2.759 [2.256, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.518], loss: 0.157790, mae: 0.397082, mean_q: 4.057466
 91566/100000: episode: 1709, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 48.424, mean reward: 3.027 [2.502, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.743, 10.447], loss: 0.240326, mae: 0.416395, mean_q: 4.161614
 91584/100000: episode: 1710, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 31.297, mean reward: 1.739 [1.453, 2.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.076, 10.255], loss: 0.164232, mae: 0.402659, mean_q: 4.139304
 91600/100000: episode: 1711, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 32.825, mean reward: 2.052 [1.850, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.351, 10.242], loss: 0.169150, mae: 0.374338, mean_q: 4.028828
 91617/100000: episode: 1712, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 54.020, mean reward: 3.178 [2.159, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.042, 10.391], loss: 0.205542, mae: 0.408781, mean_q: 4.167582
 91633/100000: episode: 1713, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 33.346, mean reward: 2.084 [1.569, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.775, 10.249], loss: 0.140239, mae: 0.369694, mean_q: 4.076637
 91650/100000: episode: 1714, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 32.856, mean reward: 1.933 [1.594, 2.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.396, 10.238], loss: 0.173486, mae: 0.388972, mean_q: 4.068790
 91660/100000: episode: 1715, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 36.038, mean reward: 3.604 [2.666, 5.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.442], loss: 0.203892, mae: 0.406104, mean_q: 4.122757
 91675/100000: episode: 1716, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 30.733, mean reward: 2.049 [1.808, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.322], loss: 0.222987, mae: 0.443720, mean_q: 4.158763
 91689/100000: episode: 1717, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 35.466, mean reward: 2.533 [2.248, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.232, 10.357], loss: 0.230896, mae: 0.419167, mean_q: 4.179442
 91706/100000: episode: 1718, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 41.585, mean reward: 2.446 [1.808, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.411, 10.257], loss: 0.154838, mae: 0.384485, mean_q: 4.045601
 91723/100000: episode: 1719, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 39.588, mean reward: 2.329 [1.696, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.560, 10.356], loss: 0.206916, mae: 0.388394, mean_q: 4.112494
 91733/100000: episode: 1720, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 39.892, mean reward: 3.989 [3.041, 6.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.067, 10.475], loss: 0.175187, mae: 0.423793, mean_q: 4.206973
 91749/100000: episode: 1721, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 43.351, mean reward: 2.709 [2.168, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.091, 10.373], loss: 0.176331, mae: 0.395269, mean_q: 4.122285
 91766/100000: episode: 1722, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 36.832, mean reward: 2.167 [1.531, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.221, 10.227], loss: 0.245767, mae: 0.423647, mean_q: 4.120640
 91782/100000: episode: 1723, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 33.214, mean reward: 2.076 [1.509, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.328, 10.176], loss: 0.212518, mae: 0.382930, mean_q: 4.150204
 91799/100000: episode: 1724, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 46.808, mean reward: 2.753 [1.896, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.168, 10.349], loss: 0.181023, mae: 0.392810, mean_q: 4.113262
 91814/100000: episode: 1725, duration: 0.099s, episode steps: 15, steps per second: 151, episode reward: 33.127, mean reward: 2.208 [1.789, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.066, 10.450], loss: 0.184396, mae: 0.392038, mean_q: 4.159576
 91831/100000: episode: 1726, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 51.886, mean reward: 3.052 [2.044, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.682, 10.414], loss: 0.135619, mae: 0.373395, mean_q: 4.125188
 91841/100000: episode: 1727, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 33.890, mean reward: 3.389 [2.732, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.104, 10.490], loss: 0.180676, mae: 0.386589, mean_q: 4.148831
 91858/100000: episode: 1728, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 32.615, mean reward: 1.919 [1.496, 2.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.250, 10.170], loss: 0.233958, mae: 0.399557, mean_q: 4.158100
 91874/100000: episode: 1729, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 45.949, mean reward: 2.872 [2.071, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.401], loss: 0.283486, mae: 0.440518, mean_q: 4.134151
[Info] 2-TH LEVEL FOUND: 7.354211330413818, Considering 18/82 traces
 91889/100000: episode: 1730, duration: 4.274s, episode steps: 15, steps per second: 4, episode reward: 44.531, mean reward: 2.969 [2.224, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.455], loss: 0.196918, mae: 0.425056, mean_q: 4.074973
 91898/100000: episode: 1731, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 39.424, mean reward: 4.380 [3.706, 5.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.611], loss: 0.145590, mae: 0.407638, mean_q: 4.231366
 91907/100000: episode: 1732, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 30.246, mean reward: 3.361 [2.740, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.463], loss: 0.266584, mae: 0.425494, mean_q: 4.096060
 91916/100000: episode: 1733, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 28.821, mean reward: 3.202 [2.809, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.535], loss: 0.186014, mae: 0.431702, mean_q: 4.225395
 91926/100000: episode: 1734, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 36.317, mean reward: 3.632 [2.895, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.475], loss: 0.288637, mae: 0.436993, mean_q: 4.023033
 91936/100000: episode: 1735, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 39.584, mean reward: 3.958 [3.022, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.561], loss: 0.176550, mae: 0.426252, mean_q: 4.151681
 91946/100000: episode: 1736, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 32.782, mean reward: 3.278 [1.893, 6.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.241, 10.367], loss: 0.157857, mae: 0.377534, mean_q: 4.195286
 91956/100000: episode: 1737, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 34.347, mean reward: 3.435 [2.778, 5.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.459], loss: 0.178428, mae: 0.401651, mean_q: 4.092790
 91970/100000: episode: 1738, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 48.150, mean reward: 3.439 [2.814, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.320, 10.460], loss: 0.203885, mae: 0.395496, mean_q: 4.133823
 91980/100000: episode: 1739, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 38.296, mean reward: 3.830 [3.010, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.255, 10.517], loss: 0.158347, mae: 0.390091, mean_q: 4.212270
 91990/100000: episode: 1740, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 59.870, mean reward: 5.987 [4.699, 7.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.173, 10.558], loss: 0.180905, mae: 0.430080, mean_q: 4.270754
 92000/100000: episode: 1741, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 43.713, mean reward: 4.371 [2.601, 6.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.419, 10.520], loss: 0.148491, mae: 0.392667, mean_q: 4.221288
 92010/100000: episode: 1742, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 35.471, mean reward: 3.547 [2.170, 5.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.399], loss: 0.164922, mae: 0.383912, mean_q: 4.218174
 92020/100000: episode: 1743, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 47.612, mean reward: 4.761 [3.308, 7.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.664, 10.552], loss: 0.157663, mae: 0.396212, mean_q: 4.160547
 92030/100000: episode: 1744, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 49.586, mean reward: 4.959 [3.494, 7.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.191, 10.647], loss: 0.195448, mae: 0.420981, mean_q: 4.365115
 92039/100000: episode: 1745, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 33.069, mean reward: 3.674 [3.036, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.473], loss: 0.160766, mae: 0.386459, mean_q: 4.314058
 92046/100000: episode: 1746, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 18.347, mean reward: 2.621 [2.160, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.524, 10.412], loss: 0.186852, mae: 0.414294, mean_q: 4.210594
 92056/100000: episode: 1747, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 35.556, mean reward: 3.556 [2.859, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.499], loss: 0.254958, mae: 0.489468, mean_q: 4.266623
 92066/100000: episode: 1748, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 31.604, mean reward: 3.160 [2.139, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.457], loss: 0.215673, mae: 0.449245, mean_q: 4.349532
 92076/100000: episode: 1749, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 43.512, mean reward: 4.351 [3.800, 5.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.542], loss: 0.169530, mae: 0.392764, mean_q: 4.251885
 92089/100000: episode: 1750, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 31.432, mean reward: 2.418 [1.829, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.325], loss: 0.260464, mae: 0.463313, mean_q: 4.259222
 92099/100000: episode: 1751, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 66.283, mean reward: 6.628 [4.001, 16.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.496], loss: 0.186750, mae: 0.423011, mean_q: 4.397854
 92109/100000: episode: 1752, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 41.318, mean reward: 4.132 [3.431, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.586], loss: 0.244545, mae: 0.462004, mean_q: 4.211774
 92119/100000: episode: 1753, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 37.881, mean reward: 3.788 [3.466, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.498], loss: 0.180489, mae: 0.419905, mean_q: 4.398398
 92129/100000: episode: 1754, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 32.029, mean reward: 3.203 [2.268, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.291, 10.403], loss: 0.273019, mae: 0.493263, mean_q: 4.293457
 92139/100000: episode: 1755, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 39.761, mean reward: 3.976 [2.721, 5.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.514], loss: 0.247685, mae: 0.456006, mean_q: 4.324496
 92149/100000: episode: 1756, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 35.912, mean reward: 3.591 [2.809, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.505], loss: 0.202978, mae: 0.445431, mean_q: 4.233700
 92159/100000: episode: 1757, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 37.467, mean reward: 3.747 [3.009, 4.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.553], loss: 0.252341, mae: 0.447667, mean_q: 4.314105
 92174/100000: episode: 1758, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 48.646, mean reward: 3.243 [2.383, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.233, 10.465], loss: 0.244727, mae: 0.486668, mean_q: 4.407748
 92184/100000: episode: 1759, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 49.748, mean reward: 4.975 [2.179, 14.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.988, 10.482], loss: 0.331520, mae: 0.438143, mean_q: 4.330872
 92194/100000: episode: 1760, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 37.708, mean reward: 3.771 [2.999, 5.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.316, 10.540], loss: 0.221430, mae: 0.453606, mean_q: 4.442484
 92204/100000: episode: 1761, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 45.726, mean reward: 4.573 [3.786, 7.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.624], loss: 0.562726, mae: 0.541687, mean_q: 4.591983
 92219/100000: episode: 1762, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 53.338, mean reward: 3.556 [2.610, 5.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.503], loss: 0.272692, mae: 0.484106, mean_q: 4.280359
 92229/100000: episode: 1763, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 41.667, mean reward: 4.167 [2.767, 6.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.513], loss: 0.253441, mae: 0.475208, mean_q: 4.418616
 92239/100000: episode: 1764, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 40.932, mean reward: 4.093 [3.051, 4.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.188, 10.393], loss: 0.256596, mae: 0.432739, mean_q: 4.319451
 92248/100000: episode: 1765, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 29.705, mean reward: 3.301 [2.920, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.955, 10.515], loss: 0.231181, mae: 0.431951, mean_q: 4.265000
 92258/100000: episode: 1766, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 53.245, mean reward: 5.325 [3.687, 7.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.660], loss: 0.154221, mae: 0.422914, mean_q: 4.331863
 92268/100000: episode: 1767, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 34.390, mean reward: 3.439 [2.951, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.204, 10.485], loss: 0.219319, mae: 0.450488, mean_q: 4.432689
 92278/100000: episode: 1768, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 44.691, mean reward: 4.469 [3.105, 6.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.544], loss: 0.279225, mae: 0.480795, mean_q: 4.332771
 92288/100000: episode: 1769, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 58.233, mean reward: 5.823 [4.560, 7.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.593], loss: 0.264513, mae: 0.504980, mean_q: 4.451410
 92303/100000: episode: 1770, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 44.879, mean reward: 2.992 [2.357, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.090, 10.349], loss: 0.252136, mae: 0.469989, mean_q: 4.383182
 92313/100000: episode: 1771, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 40.146, mean reward: 4.015 [2.919, 5.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.623], loss: 0.248151, mae: 0.432045, mean_q: 4.349851
 92322/100000: episode: 1772, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 32.907, mean reward: 3.656 [3.168, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.561], loss: 0.276393, mae: 0.503275, mean_q: 4.480112
 92332/100000: episode: 1773, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 71.372, mean reward: 7.137 [4.170, 10.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.582], loss: 0.196183, mae: 0.452382, mean_q: 4.446713
 92339/100000: episode: 1774, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 18.858, mean reward: 2.694 [2.395, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.337, 10.395], loss: 0.638917, mae: 0.505718, mean_q: 4.453355
 92349/100000: episode: 1775, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 32.335, mean reward: 3.234 [2.491, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.429], loss: 0.266364, mae: 0.497404, mean_q: 4.558281
 92363/100000: episode: 1776, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 46.953, mean reward: 3.354 [2.580, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.422, 10.535], loss: 0.334282, mae: 0.480329, mean_q: 4.525795
 92373/100000: episode: 1777, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 53.823, mean reward: 5.382 [4.339, 6.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.584], loss: 0.221347, mae: 0.446440, mean_q: 4.380856
 92388/100000: episode: 1778, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 48.283, mean reward: 3.219 [2.404, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.123, 10.393], loss: 0.172634, mae: 0.440072, mean_q: 4.384439
 92395/100000: episode: 1779, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 23.216, mean reward: 3.317 [2.659, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.580], loss: 0.213399, mae: 0.450971, mean_q: 4.413489
 92405/100000: episode: 1780, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 35.412, mean reward: 3.541 [2.068, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.504], loss: 0.211867, mae: 0.469395, mean_q: 4.466612
 92415/100000: episode: 1781, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 45.683, mean reward: 4.568 [3.659, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.552], loss: 0.281746, mae: 0.530290, mean_q: 4.649834
 92424/100000: episode: 1782, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 44.329, mean reward: 4.925 [3.188, 9.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.579], loss: 0.234483, mae: 0.464542, mean_q: 4.424388
 92434/100000: episode: 1783, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 61.964, mean reward: 6.196 [4.369, 12.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-1.077, 10.520], loss: 0.189515, mae: 0.454804, mean_q: 4.339600
 92444/100000: episode: 1784, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 42.934, mean reward: 4.293 [3.489, 6.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.548], loss: 0.303232, mae: 0.522426, mean_q: 4.562178
 92451/100000: episode: 1785, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 19.194, mean reward: 2.742 [2.454, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.163, 10.456], loss: 0.295952, mae: 0.501155, mean_q: 4.342478
 92461/100000: episode: 1786, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 33.073, mean reward: 3.307 [2.264, 6.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.415], loss: 0.347444, mae: 0.527139, mean_q: 4.744436
 92468/100000: episode: 1787, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 18.822, mean reward: 2.689 [2.474, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.450], loss: 0.325294, mae: 0.506699, mean_q: 4.365417
 92478/100000: episode: 1788, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 41.738, mean reward: 4.174 [2.948, 5.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.445], loss: 0.553448, mae: 0.637133, mean_q: 4.757253
 92488/100000: episode: 1789, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 33.392, mean reward: 3.339 [2.300, 5.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.419], loss: 0.457726, mae: 0.560801, mean_q: 4.326565
 92497/100000: episode: 1790, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 56.901, mean reward: 6.322 [4.146, 10.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.690], loss: 0.319132, mae: 0.546801, mean_q: 4.524244
 92512/100000: episode: 1791, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 46.094, mean reward: 3.073 [2.380, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.436], loss: 0.246631, mae: 0.457448, mean_q: 4.421130
 92522/100000: episode: 1792, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 40.739, mean reward: 4.074 [3.509, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.416, 10.465], loss: 0.319906, mae: 0.482851, mean_q: 4.622434
 92532/100000: episode: 1793, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 35.208, mean reward: 3.521 [2.933, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.435], loss: 0.279872, mae: 0.500311, mean_q: 4.542661
 92542/100000: episode: 1794, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 44.071, mean reward: 4.407 [4.057, 4.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.408, 10.528], loss: 0.298192, mae: 0.516174, mean_q: 4.675925
 92552/100000: episode: 1795, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 60.592, mean reward: 6.059 [4.935, 7.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.621], loss: 0.333657, mae: 0.532377, mean_q: 4.592381
 92562/100000: episode: 1796, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 39.935, mean reward: 3.993 [3.109, 6.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.477], loss: 0.551299, mae: 0.559224, mean_q: 4.731135
 92571/100000: episode: 1797, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 35.264, mean reward: 3.918 [2.918, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.478], loss: 0.256146, mae: 0.495894, mean_q: 4.593470
 92581/100000: episode: 1798, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 35.040, mean reward: 3.504 [2.473, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.395], loss: 0.371166, mae: 0.584929, mean_q: 4.646163
 92591/100000: episode: 1799, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 29.375, mean reward: 2.937 [2.582, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.405], loss: 0.320667, mae: 0.532614, mean_q: 4.560536
 92606/100000: episode: 1800, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 38.662, mean reward: 2.577 [2.200, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.189, 10.366], loss: 0.320063, mae: 0.522104, mean_q: 4.624012
 92616/100000: episode: 1801, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 38.032, mean reward: 3.803 [3.306, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.464], loss: 0.274391, mae: 0.509486, mean_q: 4.751297
 92626/100000: episode: 1802, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 47.341, mean reward: 4.734 [3.823, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.597], loss: 0.255722, mae: 0.500547, mean_q: 4.650960
 92636/100000: episode: 1803, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 41.982, mean reward: 4.198 [2.815, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.566], loss: 0.282741, mae: 0.491379, mean_q: 4.690513
 92650/100000: episode: 1804, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 41.323, mean reward: 2.952 [2.221, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.426], loss: 0.322411, mae: 0.500184, mean_q: 4.602180
 92660/100000: episode: 1805, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 56.484, mean reward: 5.648 [3.924, 8.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.685, 10.656], loss: 0.278266, mae: 0.492173, mean_q: 4.736219
 92670/100000: episode: 1806, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 38.033, mean reward: 3.803 [2.640, 8.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.447], loss: 0.286667, mae: 0.505545, mean_q: 4.812167
 92680/100000: episode: 1807, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 36.210, mean reward: 3.621 [2.467, 5.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.419], loss: 0.364288, mae: 0.515675, mean_q: 4.526675
 92694/100000: episode: 1808, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 50.149, mean reward: 3.582 [3.144, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.416, 10.480], loss: 0.306751, mae: 0.513441, mean_q: 4.582821
 92703/100000: episode: 1809, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 36.070, mean reward: 4.008 [3.100, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.543], loss: 0.269989, mae: 0.488233, mean_q: 4.614713
 92710/100000: episode: 1810, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 26.702, mean reward: 3.815 [3.453, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.526], loss: 0.252196, mae: 0.498093, mean_q: 4.662051
 92720/100000: episode: 1811, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 36.113, mean reward: 3.611 [3.045, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.426], loss: 0.368881, mae: 0.557070, mean_q: 4.678096
[Info] 3-TH LEVEL FOUND: 7.98561429977417, Considering 10/90 traces
 92735/100000: episode: 1812, duration: 4.138s, episode steps: 15, steps per second: 4, episode reward: 44.004, mean reward: 2.934 [2.195, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.246, 10.579], loss: 0.350328, mae: 0.539037, mean_q: 4.759651
 92743/100000: episode: 1813, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 34.725, mean reward: 4.341 [2.801, 7.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.285, 10.482], loss: 0.482649, mae: 0.527135, mean_q: 4.494380
 92751/100000: episode: 1814, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 34.555, mean reward: 4.319 [3.587, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.617, 10.574], loss: 0.758884, mae: 0.632933, mean_q: 4.897728
 92759/100000: episode: 1815, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 1048.793, mean reward: 131.099 [3.881, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.148, 10.339], loss: 0.511120, mae: 0.582400, mean_q: 4.746728
 92765/100000: episode: 1816, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 35.360, mean reward: 5.893 [4.734, 8.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.548], loss: 0.354830, mae: 0.544958, mean_q: 4.916651
 92773/100000: episode: 1817, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 82.501, mean reward: 10.313 [3.518, 28.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.729], loss: 1.435113, mae: 0.607156, mean_q: 4.724404
 92781/100000: episode: 1818, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 47.228, mean reward: 5.904 [3.565, 11.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.697], loss: 0.504099, mae: 0.610675, mean_q: 4.921135
 92790/100000: episode: 1819, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 57.899, mean reward: 6.433 [3.965, 11.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.417, 10.583], loss: 0.313124, mae: 0.505522, mean_q: 4.593919
 92798/100000: episode: 1820, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 94.993, mean reward: 11.874 [3.371, 23.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.602, 10.750], loss: 0.391380, mae: 0.575589, mean_q: 4.782648
 92799/100000: episode: 1821, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 5.865, mean reward: 5.865 [5.865, 5.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.035, 10.609], loss: 0.458901, mae: 0.633912, mean_q: 4.878115
 92805/100000: episode: 1822, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 38.807, mean reward: 6.468 [3.749, 9.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.600], loss: 0.362037, mae: 0.541092, mean_q: 4.634646
 92813/100000: episode: 1823, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 38.933, mean reward: 4.867 [3.503, 9.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.405], loss: 0.337220, mae: 0.536567, mean_q: 4.669954
 92819/100000: episode: 1824, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 40.149, mean reward: 6.691 [5.064, 11.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.502], loss: 0.822920, mae: 0.653944, mean_q: 4.844071
 92827/100000: episode: 1825, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 90.969, mean reward: 11.371 [4.766, 24.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.750], loss: 0.332712, mae: 0.556187, mean_q: 4.541398
 92836/100000: episode: 1826, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 33.440, mean reward: 3.716 [3.181, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.146, 10.533], loss: 0.370760, mae: 0.565260, mean_q: 4.873246
 92844/100000: episode: 1827, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 32.042, mean reward: 4.005 [3.477, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.558], loss: 0.460542, mae: 0.582628, mean_q: 4.883055
 92853/100000: episode: 1828, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 30.185, mean reward: 3.354 [2.815, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.480], loss: 0.330053, mae: 0.532251, mean_q: 4.870045
 92861/100000: episode: 1829, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 45.297, mean reward: 5.662 [4.107, 7.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.122, 10.557], loss: 0.328407, mae: 0.499732, mean_q: 4.444696
 92869/100000: episode: 1830, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 30.831, mean reward: 3.854 [2.884, 4.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.531], loss: 0.917309, mae: 0.712869, mean_q: 5.172979
 92877/100000: episode: 1831, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 38.890, mean reward: 4.861 [3.698, 6.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.638], loss: 1939.381470, mae: 4.425704, mean_q: 4.523114
 92885/100000: episode: 1832, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 38.676, mean reward: 4.835 [3.838, 5.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.603], loss: 8.765399, mae: 3.343724, mean_q: 8.380157
 92893/100000: episode: 1833, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 74.147, mean reward: 9.268 [6.480, 15.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.545], loss: 1.355509, mae: 1.121069, mean_q: 5.280036
 92901/100000: episode: 1834, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 30.131, mean reward: 3.766 [2.923, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.547], loss: 2.085881, mae: 1.453950, mean_q: 3.858132
 92909/100000: episode: 1835, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 54.214, mean reward: 6.777 [4.575, 9.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.180, 10.690], loss: 1.257617, mae: 1.042888, mean_q: 4.256708
 92917/100000: episode: 1836, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 33.969, mean reward: 4.246 [3.299, 5.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.642, 10.502], loss: 1946.319336, mae: 4.985458, mean_q: 5.438824
 92925/100000: episode: 1837, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 32.138, mean reward: 4.017 [3.640, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.492], loss: 3.178087, mae: 2.245620, mean_q: 7.266428
 92933/100000: episode: 1838, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 39.045, mean reward: 4.881 [3.621, 6.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.620], loss: 2.041410, mae: 1.569439, mean_q: 6.601131
 92941/100000: episode: 1839, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 40.616, mean reward: 5.077 [4.187, 5.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.599], loss: 0.807924, mae: 0.861708, mean_q: 5.063058
 92949/100000: episode: 1840, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 29.246, mean reward: 3.656 [3.047, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.506], loss: 1.429488, mae: 1.001745, mean_q: 4.625992
 92955/100000: episode: 1841, duration: 0.031s, episode steps: 6, steps per second: 192, episode reward: 22.664, mean reward: 3.777 [2.980, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.284, 10.443], loss: 0.479931, mae: 0.698114, mean_q: 5.129353
 92956/100000: episode: 1842, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 4.306, mean reward: 4.306 [4.306, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.548], loss: 0.307616, mae: 0.628552, mean_q: 5.620613
 92964/100000: episode: 1843, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 27.761, mean reward: 3.470 [3.055, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.493], loss: 0.725719, mae: 0.751298, mean_q: 5.208777
 92972/100000: episode: 1844, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 48.996, mean reward: 6.124 [3.931, 11.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.690], loss: 0.835188, mae: 0.710145, mean_q: 5.069436
 92978/100000: episode: 1845, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 38.308, mean reward: 6.385 [3.934, 8.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.617], loss: 2.098473, mae: 0.736546, mean_q: 5.122365
 92986/100000: episode: 1846, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 39.547, mean reward: 4.943 [3.323, 7.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.571], loss: 0.408517, mae: 0.602039, mean_q: 5.150316
[Info] FALSIFICATION!
 92989/100000: episode: 1847, duration: 0.280s, episode steps: 3, steps per second: 11, episode reward: 1007.377, mean reward: 335.792 [3.444, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.796], loss: 1.288261, mae: 0.808616, mean_q: 5.304375
 92997/100000: episode: 1848, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 33.354, mean reward: 4.169 [3.540, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.598], loss: 0.793477, mae: 0.820943, mean_q: 5.159690
 93005/100000: episode: 1849, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 35.228, mean reward: 4.403 [2.929, 5.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.583], loss: 0.630403, mae: 0.638521, mean_q: 5.126191
 93013/100000: episode: 1850, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 26.841, mean reward: 3.355 [2.888, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.426, 10.495], loss: 0.616836, mae: 0.692594, mean_q: 5.146711
 93021/100000: episode: 1851, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 28.229, mean reward: 3.529 [2.567, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.217, 10.488], loss: 3851.408691, mae: 9.308930, mean_q: 6.488338
 93029/100000: episode: 1852, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 30.144, mean reward: 3.768 [2.937, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.491], loss: 7.949958, mae: 3.684865, mean_q: 8.970320
 93035/100000: episode: 1853, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 25.182, mean reward: 4.197 [3.400, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.883, 10.404], loss: 3.569195, mae: 2.265623, mean_q: 7.620291
 93043/100000: episode: 1854, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 34.624, mean reward: 4.328 [3.494, 5.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.581], loss: 1.930768, mae: 1.117256, mean_q: 5.966211
 93052/100000: episode: 1855, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 43.443, mean reward: 4.827 [4.159, 6.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.365, 10.592], loss: 0.384710, mae: 0.629112, mean_q: 5.008973
 93060/100000: episode: 1856, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 23.341, mean reward: 2.918 [2.280, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.370], loss: 1.621155, mae: 0.861751, mean_q: 4.631249
 93069/100000: episode: 1857, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 39.578, mean reward: 4.398 [3.327, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.536, 10.579], loss: 0.575563, mae: 0.690189, mean_q: 4.761143
 93077/100000: episode: 1858, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 31.382, mean reward: 3.923 [3.210, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.656, 10.460], loss: 0.634957, mae: 0.700027, mean_q: 5.407267
 93085/100000: episode: 1859, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 46.395, mean reward: 5.799 [4.342, 6.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.630], loss: 0.529696, mae: 0.704966, mean_q: 5.308250
 93093/100000: episode: 1860, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 50.193, mean reward: 6.274 [3.609, 14.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.538, 10.389], loss: 1941.751221, mae: 5.092614, mean_q: 6.145330
 93101/100000: episode: 1861, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 70.905, mean reward: 8.863 [4.108, 19.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.352, 10.546], loss: 3.405453, mae: 1.998938, mean_q: 6.880740
 93109/100000: episode: 1862, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 28.473, mean reward: 3.559 [3.017, 4.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.526], loss: 1.300083, mae: 1.295337, mean_q: 6.303221
 93117/100000: episode: 1863, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 30.669, mean reward: 3.834 [3.445, 4.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.531], loss: 1.259591, mae: 0.998237, mean_q: 6.064222
 93118/100000: episode: 1864, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 5.570, mean reward: 5.570 [5.570, 5.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.518], loss: 0.543548, mae: 0.725447, mean_q: 5.620440
 93124/100000: episode: 1865, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 39.816, mean reward: 6.636 [5.620, 9.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.542], loss: 0.811018, mae: 0.772700, mean_q: 5.413085
 93132/100000: episode: 1866, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 46.388, mean reward: 5.798 [4.969, 6.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.600], loss: 0.526773, mae: 0.686870, mean_q: 5.151585
 93140/100000: episode: 1867, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 29.298, mean reward: 3.662 [3.560, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.188, 10.513], loss: 0.603129, mae: 0.670259, mean_q: 5.238132
 93148/100000: episode: 1868, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 28.502, mean reward: 3.563 [2.881, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.476], loss: 0.981137, mae: 0.726585, mean_q: 5.239940
 93156/100000: episode: 1869, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 28.013, mean reward: 3.502 [2.648, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.538], loss: 0.694629, mae: 0.777450, mean_q: 5.426292
 93164/100000: episode: 1870, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 24.124, mean reward: 3.016 [2.604, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.458], loss: 1.358786, mae: 0.819668, mean_q: 5.459324
 93173/100000: episode: 1871, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 80.904, mean reward: 8.989 [4.165, 35.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.231, 10.512], loss: 0.630490, mae: 0.739649, mean_q: 5.397689
 93181/100000: episode: 1872, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 33.356, mean reward: 4.169 [3.582, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.536], loss: 1.714123, mae: 0.749051, mean_q: 5.426958
 93189/100000: episode: 1873, duration: 0.056s, episode steps: 8, steps per second: 144, episode reward: 43.191, mean reward: 5.399 [4.359, 6.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.598], loss: 0.836403, mae: 0.733729, mean_q: 5.362688
 93190/100000: episode: 1874, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 5.051, mean reward: 5.051 [5.051, 5.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.035, 10.588], loss: 0.915681, mae: 0.920007, mean_q: 5.355972
 93198/100000: episode: 1875, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 31.280, mean reward: 3.910 [3.395, 4.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.799, 10.588], loss: 0.702077, mae: 0.776308, mean_q: 5.441005
 93206/100000: episode: 1876, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 42.440, mean reward: 5.305 [3.675, 10.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.569], loss: 1.668011, mae: 0.836807, mean_q: 5.406535
 93214/100000: episode: 1877, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 37.954, mean reward: 4.744 [3.731, 5.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.520], loss: 0.986549, mae: 0.792712, mean_q: 5.581696
 93222/100000: episode: 1878, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 41.852, mean reward: 5.231 [4.082, 6.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.503], loss: 1.297995, mae: 0.793936, mean_q: 5.497660
 93230/100000: episode: 1879, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 25.831, mean reward: 3.229 [2.999, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.508], loss: 0.514347, mae: 0.654649, mean_q: 5.400559
 93238/100000: episode: 1880, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 71.614, mean reward: 8.952 [3.929, 31.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.684], loss: 0.636361, mae: 0.722320, mean_q: 5.347551
 93246/100000: episode: 1881, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 48.763, mean reward: 6.095 [3.713, 10.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.639], loss: 1.365337, mae: 0.749020, mean_q: 5.540586
 93254/100000: episode: 1882, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 32.373, mean reward: 4.047 [3.455, 5.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.588], loss: 0.473585, mae: 0.629109, mean_q: 5.464243
 93262/100000: episode: 1883, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 28.505, mean reward: 3.563 [3.225, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.516], loss: 0.727030, mae: 0.708833, mean_q: 5.413762
 93270/100000: episode: 1884, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 41.530, mean reward: 5.191 [4.176, 7.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.383, 10.550], loss: 0.848009, mae: 0.776528, mean_q: 5.521894
 93278/100000: episode: 1885, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 33.999, mean reward: 4.250 [3.003, 6.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.062, 10.595], loss: 2.461060, mae: 0.777498, mean_q: 5.302179
 93287/100000: episode: 1886, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 38.860, mean reward: 4.318 [3.207, 6.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.636], loss: 1710.484741, mae: 4.375198, mean_q: 5.724968
 93295/100000: episode: 1887, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 24.216, mean reward: 3.027 [2.657, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.421], loss: 15.835384, mae: 4.173889, mean_q: 9.743221
 93296/100000: episode: 1888, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 4.930, mean reward: 4.930 [4.930, 4.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.576], loss: 2.121940, mae: 1.315051, mean_q: 6.804928
 93304/100000: episode: 1889, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 26.288, mean reward: 3.286 [2.890, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.494], loss: 0.586163, mae: 0.718541, mean_q: 5.546007
 93312/100000: episode: 1890, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 31.231, mean reward: 3.904 [3.151, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.535, 10.507], loss: 1925.906982, mae: 4.686926, mean_q: 5.607266
 93321/100000: episode: 1891, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 32.149, mean reward: 3.572 [2.728, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.406], loss: 2.945596, mae: 1.401835, mean_q: 6.374784
 93329/100000: episode: 1892, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 34.491, mean reward: 4.311 [2.819, 8.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.536], loss: 2.242854, mae: 1.292119, mean_q: 6.496964
 93335/100000: episode: 1893, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 21.280, mean reward: 3.547 [3.207, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.471], loss: 1.828686, mae: 1.159449, mean_q: 6.044414
 93343/100000: episode: 1894, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 29.742, mean reward: 3.718 [3.267, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.504], loss: 0.710496, mae: 0.872381, mean_q: 5.989239
 93352/100000: episode: 1895, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 35.691, mean reward: 3.966 [3.364, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.931, 10.571], loss: 1724.878296, mae: 4.552907, mean_q: 6.033846
 93360/100000: episode: 1896, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 33.188, mean reward: 4.148 [2.835, 6.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.484], loss: 1934.119995, mae: 5.649148, mean_q: 7.118355
 93368/100000: episode: 1897, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 33.071, mean reward: 4.134 [2.920, 5.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.125, 10.493], loss: 5.877646, mae: 2.714605, mean_q: 8.056755
 93376/100000: episode: 1898, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 40.017, mean reward: 5.002 [3.643, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.056, 10.597], loss: 3.197038, mae: 2.093470, mean_q: 7.680054
[Info] FALSIFICATION!
 93377/100000: episode: 1899, duration: 0.227s, episode steps: 1, steps per second: 4, episode reward: 1000.000, mean reward: 1000.000 [1000.000, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.014, 10.764], loss: 2.276643, mae: 1.710676, mean_q: 7.345419
 93385/100000: episode: 1900, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 52.318, mean reward: 6.540 [3.973, 15.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.538], loss: 1.214797, mae: 1.136458, mean_q: 6.560383
 93391/100000: episode: 1901, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 22.876, mean reward: 3.813 [3.331, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.560], loss: 3.429786, mae: 1.092016, mean_q: 6.049210
[Info] Complete ISplit Iteration
[Info] Levels: [5.3125505, 7.3542113, 7.9856143, 8.003492]
[Info] Cond. Prob: [0.1, 0.18, 0.1, 0.48]
[Info] Error Prob: 0.000864

 93400/100000: episode: 1902, duration: 4.416s, episode steps: 9, steps per second: 2, episode reward: 35.508, mean reward: 3.945 [3.400, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.295, 10.526], loss: 1.368444, mae: 0.939081, mean_q: 5.859914
 93500/100000: episode: 1903, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 186.442, mean reward: 1.864 [1.476, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.640, 10.098], loss: 308.990692, mae: 1.696255, mean_q: 6.333235
 93600/100000: episode: 1904, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.659, mean reward: 2.067 [1.521, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.229, 10.098], loss: 308.640228, mae: 1.907132, mean_q: 6.655766
 93700/100000: episode: 1905, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 208.753, mean reward: 2.088 [1.445, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.521, 10.190], loss: 308.866974, mae: 1.781039, mean_q: 6.668278
 93800/100000: episode: 1906, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.548, mean reward: 1.945 [1.466, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.972, 10.098], loss: 156.767151, mae: 1.280815, mean_q: 6.190609
 93900/100000: episode: 1907, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 215.845, mean reward: 2.158 [1.450, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.192, 10.098], loss: 155.073135, mae: 1.191252, mean_q: 6.030371
 94000/100000: episode: 1908, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.543, mean reward: 1.945 [1.463, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.663, 10.193], loss: 617.506226, mae: 3.065704, mean_q: 7.422746
 94100/100000: episode: 1909, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 228.948, mean reward: 2.289 [1.565, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.390, 10.098], loss: 0.912692, mae: 0.755744, mean_q: 5.868613
 94200/100000: episode: 1910, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 218.125, mean reward: 2.181 [1.496, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.536, 10.247], loss: 309.785889, mae: 1.549906, mean_q: 6.132103
 94300/100000: episode: 1911, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 202.829, mean reward: 2.028 [1.466, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.098], loss: 310.031982, mae: 2.050961, mean_q: 6.939758
 94400/100000: episode: 1912, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 207.469, mean reward: 2.075 [1.446, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.340, 10.098], loss: 773.416504, mae: 3.131431, mean_q: 7.261569
 94500/100000: episode: 1913, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 198.563, mean reward: 1.986 [1.476, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.053, 10.164], loss: 157.018997, mae: 1.528941, mean_q: 6.796556
 94600/100000: episode: 1914, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 190.745, mean reward: 1.907 [1.445, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.733, 10.241], loss: 463.414886, mae: 1.987547, mean_q: 6.601953
 94700/100000: episode: 1915, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.866, mean reward: 1.889 [1.471, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.958, 10.135], loss: 156.762436, mae: 1.501341, mean_q: 6.711123
 94800/100000: episode: 1916, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.332, mean reward: 1.993 [1.479, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.421, 10.279], loss: 1.580137, mae: 0.840654, mean_q: 5.967279
 94900/100000: episode: 1917, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.995, mean reward: 1.900 [1.489, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.806, 10.165], loss: 614.828979, mae: 2.869198, mean_q: 7.318264
 95000/100000: episode: 1918, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 178.342, mean reward: 1.783 [1.453, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.166], loss: 156.535126, mae: 1.295590, mean_q: 6.397609
 95100/100000: episode: 1919, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 201.935, mean reward: 2.019 [1.457, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.401, 10.098], loss: 156.332687, mae: 1.275148, mean_q: 6.168372
 95200/100000: episode: 1920, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.493, mean reward: 1.905 [1.479, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.164, 10.098], loss: 154.846313, mae: 1.276967, mean_q: 6.196998
 95300/100000: episode: 1921, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.004, mean reward: 1.820 [1.448, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.406, 10.205], loss: 309.670593, mae: 1.526135, mean_q: 5.994066
 95400/100000: episode: 1922, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 193.452, mean reward: 1.935 [1.471, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.353, 10.112], loss: 309.420105, mae: 1.914803, mean_q: 6.732722
 95500/100000: episode: 1923, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.820, mean reward: 1.918 [1.432, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.952, 10.098], loss: 462.246826, mae: 1.896432, mean_q: 6.153292
 95600/100000: episode: 1924, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 210.858, mean reward: 2.109 [1.527, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.989, 10.198], loss: 618.348572, mae: 3.110373, mean_q: 7.607180
 95700/100000: episode: 1925, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.299, mean reward: 1.943 [1.444, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.318, 10.098], loss: 309.137115, mae: 1.745690, mean_q: 6.801273
 95800/100000: episode: 1926, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.053, mean reward: 1.951 [1.448, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.023, 10.273], loss: 309.699463, mae: 1.716932, mean_q: 6.525759
 95900/100000: episode: 1927, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 200.507, mean reward: 2.005 [1.464, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.619, 10.236], loss: 460.760437, mae: 2.229870, mean_q: 6.721857
 96000/100000: episode: 1928, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 195.015, mean reward: 1.950 [1.472, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.883, 10.098], loss: 616.215454, mae: 2.821530, mean_q: 7.270761
 96100/100000: episode: 1929, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 221.314, mean reward: 2.213 [1.447, 7.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.760, 10.098], loss: 616.575012, mae: 2.700985, mean_q: 6.987545
 96200/100000: episode: 1930, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 202.188, mean reward: 2.022 [1.587, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.903, 10.098], loss: 462.403564, mae: 2.359495, mean_q: 7.052355
 96300/100000: episode: 1931, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 198.012, mean reward: 1.980 [1.447, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.601, 10.117], loss: 614.073425, mae: 3.092522, mean_q: 7.679818
 96400/100000: episode: 1932, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.132, mean reward: 1.821 [1.472, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.415, 10.098], loss: 613.197754, mae: 2.640433, mean_q: 7.064951
 96500/100000: episode: 1933, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.605, mean reward: 1.936 [1.443, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.481, 10.098], loss: 611.301575, mae: 3.039179, mean_q: 7.859574
 96600/100000: episode: 1934, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.552, mean reward: 1.876 [1.465, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.563, 10.098], loss: 918.505127, mae: 3.566708, mean_q: 7.515948
 96700/100000: episode: 1935, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 200.397, mean reward: 2.004 [1.439, 8.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.545, 10.194], loss: 154.951797, mae: 1.715814, mean_q: 6.988395
 96800/100000: episode: 1936, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.213, mean reward: 1.812 [1.429, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.864, 10.098], loss: 462.774597, mae: 2.098542, mean_q: 6.267768
 96900/100000: episode: 1937, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 205.814, mean reward: 2.058 [1.465, 4.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.698, 10.098], loss: 154.408783, mae: 1.523750, mean_q: 6.358372
 97000/100000: episode: 1938, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 198.542, mean reward: 1.985 [1.444, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.078, 10.098], loss: 919.026245, mae: 3.476058, mean_q: 7.002233
 97100/100000: episode: 1939, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 228.726, mean reward: 2.287 [1.471, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.326], loss: 762.669128, mae: 3.652003, mean_q: 7.639138
 97200/100000: episode: 1940, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.229, mean reward: 1.892 [1.447, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.123], loss: 305.580872, mae: 2.068917, mean_q: 6.928933
 97300/100000: episode: 1941, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.877, mean reward: 1.879 [1.452, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.320, 10.098], loss: 305.603058, mae: 1.823879, mean_q: 6.279886
 97400/100000: episode: 1942, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 185.896, mean reward: 1.859 [1.485, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.415, 10.319], loss: 154.090378, mae: 1.161415, mean_q: 5.520203
 97500/100000: episode: 1943, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.388, mean reward: 1.844 [1.451, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.766, 10.134], loss: 458.976929, mae: 2.536735, mean_q: 6.395946
 97600/100000: episode: 1944, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 198.750, mean reward: 1.987 [1.505, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.913, 10.098], loss: 760.587708, mae: 2.877232, mean_q: 6.127183
 97700/100000: episode: 1945, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 212.987, mean reward: 2.130 [1.449, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.389, 10.098], loss: 305.391724, mae: 2.240778, mean_q: 6.432684
 97800/100000: episode: 1946, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 198.314, mean reward: 1.983 [1.503, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.556, 10.107], loss: 456.046967, mae: 2.226159, mean_q: 6.006492
 97900/100000: episode: 1947, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 190.782, mean reward: 1.908 [1.471, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.643, 10.098], loss: 303.399506, mae: 1.870562, mean_q: 5.850520
 98000/100000: episode: 1948, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 194.733, mean reward: 1.947 [1.459, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.351, 10.098], loss: 303.078918, mae: 1.394309, mean_q: 5.064936
 98100/100000: episode: 1949, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 204.211, mean reward: 2.042 [1.475, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.911, 10.106], loss: 152.022964, mae: 1.147002, mean_q: 5.046083
 98200/100000: episode: 1950, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 200.723, mean reward: 2.007 [1.482, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.790, 10.098], loss: 0.541522, mae: 0.579965, mean_q: 4.536234
 98300/100000: episode: 1951, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 182.682, mean reward: 1.827 [1.465, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.551, 10.200], loss: 151.862045, mae: 0.839590, mean_q: 4.363209
 98400/100000: episode: 1952, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 198.806, mean reward: 1.988 [1.448, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.993, 10.140], loss: 0.313587, mae: 0.488348, mean_q: 4.219064
 98500/100000: episode: 1953, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 201.516, mean reward: 2.015 [1.461, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.529, 10.148], loss: 0.204004, mae: 0.426938, mean_q: 4.096501
 98600/100000: episode: 1954, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 204.148, mean reward: 2.041 [1.480, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.399], loss: 0.183627, mae: 0.408709, mean_q: 4.027709
 98700/100000: episode: 1955, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.770, mean reward: 1.868 [1.497, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.706, 10.182], loss: 0.154278, mae: 0.380878, mean_q: 3.994561
 98800/100000: episode: 1956, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.441, mean reward: 1.824 [1.474, 2.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.103, 10.257], loss: 0.127251, mae: 0.359048, mean_q: 3.957480
 98900/100000: episode: 1957, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.614, mean reward: 1.886 [1.477, 4.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.307, 10.237], loss: 0.134653, mae: 0.357208, mean_q: 3.929909
 99000/100000: episode: 1958, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 197.502, mean reward: 1.975 [1.500, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.505, 10.291], loss: 0.134814, mae: 0.351650, mean_q: 3.925128
 99100/100000: episode: 1959, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.696, mean reward: 1.917 [1.451, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.815, 10.098], loss: 0.140935, mae: 0.353433, mean_q: 3.911116
 99200/100000: episode: 1960, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 192.269, mean reward: 1.923 [1.471, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.101, 10.098], loss: 0.137045, mae: 0.343029, mean_q: 3.894739
 99300/100000: episode: 1961, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.012, mean reward: 2.030 [1.491, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.776, 10.098], loss: 0.114214, mae: 0.329745, mean_q: 3.874056
 99400/100000: episode: 1962, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 223.250, mean reward: 2.232 [1.470, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.607, 10.491], loss: 0.119534, mae: 0.337236, mean_q: 3.888520
 99500/100000: episode: 1963, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 222.202, mean reward: 2.222 [1.508, 4.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.648, 10.351], loss: 0.122813, mae: 0.334125, mean_q: 3.881295
 99600/100000: episode: 1964, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.237, mean reward: 1.952 [1.438, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.197], loss: 0.116993, mae: 0.330170, mean_q: 3.886915
 99700/100000: episode: 1965, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 190.807, mean reward: 1.908 [1.470, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.098], loss: 0.116968, mae: 0.324467, mean_q: 3.880409
 99800/100000: episode: 1966, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 201.919, mean reward: 2.019 [1.466, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.489, 10.098], loss: 0.130353, mae: 0.336663, mean_q: 3.895514
 99900/100000: episode: 1967, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 189.646, mean reward: 1.896 [1.441, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.602, 10.227], loss: 0.120085, mae: 0.330725, mean_q: 3.894940
 100000/100000: episode: 1968, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.983, mean reward: 1.890 [1.470, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.478, 10.295], loss: 0.125513, mae: 0.326781, mean_q: 3.895431
done, took 643.621 seconds
[Info] End Importance Splitting. Falsification occurred 11 times.
