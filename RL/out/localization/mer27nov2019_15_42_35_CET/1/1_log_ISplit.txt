Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_15_42_35_CET/1, out/localization/mer27nov2019_15_42_35_CET/1/levels, out/localization/mer27nov2019_15_42_35_CET/1/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.187s, episode steps: 101, steps per second: 539, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.846, 10.100], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.088s, episode steps: 101, steps per second: 1143, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.759, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.069s, episode steps: 101, steps per second: 1463, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.703, 10.512], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.067s, episode steps: 101, steps per second: 1511, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.332, 10.100], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.067s, episode steps: 101, steps per second: 1496, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.100], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.067s, episode steps: 101, steps per second: 1499, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.941, 10.100], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.076s, episode steps: 101, steps per second: 1329, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.757, 10.100], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.072s, episode steps: 101, steps per second: 1398, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.386, 10.100], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.068s, episode steps: 101, steps per second: 1485, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.940, 10.181], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.071s, episode steps: 101, steps per second: 1426, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.158, 10.144], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.070s, episode steps: 101, steps per second: 1446, episode reward: 0.652, mean reward: 0.006 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.589, 10.177], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.097s, episode steps: 101, steps per second: 1040, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.144], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.105s, episode steps: 101, steps per second: 959, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.100], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.077s, episode steps: 101, steps per second: 1311, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.246, 10.153], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.081s, episode steps: 101, steps per second: 1250, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.905, 10.348], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.075s, episode steps: 101, steps per second: 1346, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.634, 10.100], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.067s, episode steps: 101, steps per second: 1501, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.225, 10.303], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.067s, episode steps: 101, steps per second: 1502, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.771, 10.100], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.117s, episode steps: 101, steps per second: 865, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.335, 10.401], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.077s, episode steps: 101, steps per second: 1304, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.428, 10.256], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.076s, episode steps: 101, steps per second: 1337, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.437, 10.256], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.076s, episode steps: 101, steps per second: 1330, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.215, 10.280], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.075s, episode steps: 101, steps per second: 1355, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.135, 10.146], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.115s, episode steps: 101, steps per second: 876, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.874, 10.165], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.072s, episode steps: 101, steps per second: 1412, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.419, 10.100], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.069s, episode steps: 101, steps per second: 1456, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.232, 10.158], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.088s, episode steps: 101, steps per second: 1153, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.191, 10.100], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.068s, episode steps: 101, steps per second: 1486, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.629, 10.188], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.068s, episode steps: 101, steps per second: 1482, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.750, 10.100], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.068s, episode steps: 101, steps per second: 1494, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.594, 10.324], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.079s, episode steps: 101, steps per second: 1273, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-2.051, 10.100], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.071s, episode steps: 101, steps per second: 1429, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.951, 10.179], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.077s, episode steps: 101, steps per second: 1310, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.651, 10.103], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.092, 10.340], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.068s, episode steps: 101, steps per second: 1488, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.288, 10.152], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.070s, episode steps: 101, steps per second: 1444, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.100], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.104s, episode steps: 101, steps per second: 971, episode reward: 0.876, mean reward: 0.009 [0.000, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.722, 10.446], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.103s, episode steps: 101, steps per second: 976, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.830, 10.100], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.071s, episode steps: 101, steps per second: 1432, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.180, 10.130], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.074s, episode steps: 101, steps per second: 1360, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.177, 10.177], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.076s, episode steps: 101, steps per second: 1322, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.798, 10.100], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.069s, episode steps: 101, steps per second: 1455, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.853, 10.253], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.069s, episode steps: 101, steps per second: 1466, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.630, 10.115], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.088s, episode steps: 101, steps per second: 1154, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.846, 10.123], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.117s, episode steps: 101, steps per second: 866, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.677, 10.255], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.107s, episode steps: 101, steps per second: 948, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.935, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.068s, episode steps: 101, steps per second: 1481, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.512, 10.247], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.066s, episode steps: 101, steps per second: 1523, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.532, 10.216], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.076s, episode steps: 101, steps per second: 1324, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.348, 10.295], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 1.087s, episode steps: 101, steps per second: 93, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.401, 10.100], loss: 0.021527, mae: 0.109811, mean_q: -0.658355
  5151/100000: episode: 51, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.286, 10.229], loss: 0.013600, mae: 0.069558, mean_q: -0.655370
  5252/100000: episode: 52, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.320, 10.100], loss: 0.011149, mae: 0.060220, mean_q: -0.648643
  5353/100000: episode: 53, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.729, 10.100], loss: 0.009084, mae: 0.050288, mean_q: -0.636059
  5454/100000: episode: 54, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.706, 10.157], loss: 0.008929, mae: 0.046949, mean_q: -0.622879
  5555/100000: episode: 55, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.065, 10.232], loss: 0.007196, mae: 0.045522, mean_q: -0.613591
  5656/100000: episode: 56, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.862, 10.100], loss: 0.006463, mae: 0.042511, mean_q: -0.604170
  5757/100000: episode: 57, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.085, 10.181], loss: 0.005612, mae: 0.042563, mean_q: -0.592282
  5858/100000: episode: 58, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.769, 10.212], loss: 0.003828, mae: 0.037754, mean_q: -0.579181
  5959/100000: episode: 59, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.358, 10.326], loss: 0.004420, mae: 0.041798, mean_q: -0.553704
  6060/100000: episode: 60, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.830, mean reward: 0.008 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.187, 10.425], loss: 0.002314, mae: 0.027416, mean_q: -0.547696
  6161/100000: episode: 61, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.455, 10.100], loss: 0.001571, mae: 0.025098, mean_q: -0.543125
  6262/100000: episode: 62, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.760, 10.212], loss: 0.001214, mae: 0.021993, mean_q: -0.527778
  6363/100000: episode: 63, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.531, 10.100], loss: 0.001056, mae: 0.019801, mean_q: -0.515504
  6464/100000: episode: 64, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.702, 10.135], loss: 0.000686, mae: 0.017942, mean_q: -0.507053
  6565/100000: episode: 65, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.244, 10.100], loss: 0.000456, mae: 0.014740, mean_q: -0.489005
  6666/100000: episode: 66, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.866, 10.156], loss: 0.000255, mae: 0.012045, mean_q: -0.474809
  6767/100000: episode: 67, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.414, 10.100], loss: 0.000174, mae: 0.010644, mean_q: -0.467595
  6868/100000: episode: 68, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.363], loss: 0.000123, mae: 0.010161, mean_q: -0.467059
  6969/100000: episode: 69, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.408, 10.100], loss: 0.000111, mae: 0.009286, mean_q: -0.448357
  7070/100000: episode: 70, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.541, 10.159], loss: 0.000103, mae: 0.009333, mean_q: -0.425677
  7171/100000: episode: 71, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.634, 10.100], loss: 0.000136, mae: 0.009833, mean_q: -0.413531
  7272/100000: episode: 72, duration: 0.724s, episode steps: 101, steps per second: 139, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.839, 10.100], loss: 0.000165, mae: 0.010851, mean_q: -0.385963
  7373/100000: episode: 73, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.044, 10.229], loss: 0.000180, mae: 0.012535, mean_q: -0.382230
  7474/100000: episode: 74, duration: 0.795s, episode steps: 101, steps per second: 127, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.964, 10.426], loss: 0.000158, mae: 0.010949, mean_q: -0.374353
  7575/100000: episode: 75, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.761, 10.100], loss: 0.000196, mae: 0.011983, mean_q: -0.343101
  7676/100000: episode: 76, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.442, 10.100], loss: 0.000171, mae: 0.011558, mean_q: -0.348267
  7777/100000: episode: 77, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.669, 10.227], loss: 0.000151, mae: 0.011548, mean_q: -0.324855
  7878/100000: episode: 78, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.360, 10.212], loss: 0.000142, mae: 0.010984, mean_q: -0.305295
  7979/100000: episode: 79, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.450, 10.318], loss: 0.000183, mae: 0.012175, mean_q: -0.294480
  8080/100000: episode: 80, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.155, 10.153], loss: 0.000140, mae: 0.010726, mean_q: -0.287651
  8181/100000: episode: 81, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.965, 10.100], loss: 0.000229, mae: 0.014495, mean_q: -0.262931
  8282/100000: episode: 82, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.425, 10.534], loss: 0.000155, mae: 0.011072, mean_q: -0.261342
  8383/100000: episode: 83, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.601, 10.241], loss: 0.000157, mae: 0.011211, mean_q: -0.231700
  8484/100000: episode: 84, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.062, 10.100], loss: 0.000199, mae: 0.013120, mean_q: -0.223140
  8585/100000: episode: 85, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.662, mean reward: 0.007 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.343, 10.138], loss: 0.000180, mae: 0.012168, mean_q: -0.197129
  8686/100000: episode: 86, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.276, 10.169], loss: 0.000147, mae: 0.011705, mean_q: -0.189412
  8787/100000: episode: 87, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.176], loss: 0.000141, mae: 0.011413, mean_q: -0.176047
  8888/100000: episode: 88, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.295, 10.276], loss: 0.000120, mae: 0.011321, mean_q: -0.157686
  8989/100000: episode: 89, duration: 0.751s, episode steps: 101, steps per second: 134, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.363, 10.100], loss: 0.000134, mae: 0.011637, mean_q: -0.147514
  9090/100000: episode: 90, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.538, 10.314], loss: 0.000127, mae: 0.011510, mean_q: -0.129856
  9191/100000: episode: 91, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.489, 10.100], loss: 0.000134, mae: 0.011361, mean_q: -0.101085
  9292/100000: episode: 92, duration: 0.660s, episode steps: 101, steps per second: 153, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.848, 10.361], loss: 0.000096, mae: 0.009870, mean_q: -0.094303
  9393/100000: episode: 93, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.986, 10.158], loss: 0.000116, mae: 0.011070, mean_q: -0.060971
  9494/100000: episode: 94, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.090, 10.174], loss: 0.000095, mae: 0.009811, mean_q: -0.051284
  9595/100000: episode: 95, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.444, 10.165], loss: 0.000108, mae: 0.010697, mean_q: -0.051159
  9696/100000: episode: 96, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.314, 10.100], loss: 0.000091, mae: 0.010077, mean_q: -0.013712
  9797/100000: episode: 97, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.864, mean reward: 0.009 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.564, 10.121], loss: 0.000104, mae: 0.010880, mean_q: 0.009092
  9898/100000: episode: 98, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.100], loss: 0.000084, mae: 0.009471, mean_q: -0.002571
  9999/100000: episode: 99, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.139, 10.100], loss: 0.000122, mae: 0.011859, mean_q: 0.016858
 10100/100000: episode: 100, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.291, 10.163], loss: 0.000110, mae: 0.011078, mean_q: 0.034257
 10201/100000: episode: 101, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.492, 10.100], loss: 0.000120, mae: 0.011757, mean_q: 0.053990
 10302/100000: episode: 102, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.948, 10.163], loss: 0.000127, mae: 0.012044, mean_q: 0.065222
 10403/100000: episode: 103, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.449, 10.100], loss: 0.000105, mae: 0.010956, mean_q: 0.073892
 10504/100000: episode: 104, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.430, 10.254], loss: 0.000113, mae: 0.010814, mean_q: 0.083899
 10605/100000: episode: 105, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.298, 10.100], loss: 0.000082, mae: 0.009397, mean_q: 0.108751
 10706/100000: episode: 106, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.307, 10.100], loss: 0.000107, mae: 0.010836, mean_q: 0.118544
 10807/100000: episode: 107, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.455, 10.239], loss: 0.000088, mae: 0.009226, mean_q: 0.141115
 10908/100000: episode: 108, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.410, 10.100], loss: 0.000093, mae: 0.009427, mean_q: 0.151111
 11009/100000: episode: 109, duration: 0.654s, episode steps: 101, steps per second: 154, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.583, 10.222], loss: 0.000099, mae: 0.010392, mean_q: 0.155018
 11110/100000: episode: 110, duration: 0.784s, episode steps: 101, steps per second: 129, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.929, 10.220], loss: 0.000096, mae: 0.010238, mean_q: 0.185881
 11211/100000: episode: 111, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.830, mean reward: 0.008 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.995, 10.148], loss: 0.000117, mae: 0.011477, mean_q: 0.210040
 11312/100000: episode: 112, duration: 0.754s, episode steps: 101, steps per second: 134, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.116], loss: 0.000093, mae: 0.009707, mean_q: 0.214427
 11413/100000: episode: 113, duration: 0.644s, episode steps: 101, steps per second: 157, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.778, 10.350], loss: 0.000070, mae: 0.008532, mean_q: 0.228941
 11514/100000: episode: 114, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.479, 10.229], loss: 0.000093, mae: 0.010012, mean_q: 0.224682
 11615/100000: episode: 115, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.088, 10.147], loss: 0.000111, mae: 0.010650, mean_q: 0.234919
 11716/100000: episode: 116, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.060, 10.324], loss: 0.000079, mae: 0.008951, mean_q: 0.268661
 11817/100000: episode: 117, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.725, 10.405], loss: 0.000085, mae: 0.009157, mean_q: 0.286080
 11918/100000: episode: 118, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.888, 10.116], loss: 0.000097, mae: 0.009815, mean_q: 0.318975
 12019/100000: episode: 119, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.179, 10.100], loss: 0.000120, mae: 0.011098, mean_q: 0.315079
 12120/100000: episode: 120, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.945, 10.100], loss: 0.000071, mae: 0.008330, mean_q: 0.321378
 12221/100000: episode: 121, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.781, 10.100], loss: 0.000075, mae: 0.009074, mean_q: 0.306423
 12322/100000: episode: 122, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.361, 10.100], loss: 0.000091, mae: 0.009381, mean_q: 0.334459
 12423/100000: episode: 123, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.527, 10.100], loss: 0.000058, mae: 0.007769, mean_q: 0.357805
 12524/100000: episode: 124, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.201, 10.100], loss: 0.000058, mae: 0.007607, mean_q: 0.373793
 12625/100000: episode: 125, duration: 0.651s, episode steps: 101, steps per second: 155, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.309, 10.469], loss: 0.000073, mae: 0.008854, mean_q: 0.389369
 12726/100000: episode: 126, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.629, 10.242], loss: 0.000067, mae: 0.008838, mean_q: 0.386966
 12827/100000: episode: 127, duration: 0.744s, episode steps: 101, steps per second: 136, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.618, 10.100], loss: 0.000091, mae: 0.009975, mean_q: 0.426135
 12928/100000: episode: 128, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.818, 10.126], loss: 0.000082, mae: 0.008862, mean_q: 0.420958
 13029/100000: episode: 129, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.181, 10.100], loss: 0.000070, mae: 0.008343, mean_q: 0.443109
 13130/100000: episode: 130, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.430, 10.100], loss: 0.000054, mae: 0.007205, mean_q: 0.464912
 13231/100000: episode: 131, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.205, 10.100], loss: 0.000051, mae: 0.007147, mean_q: 0.447464
 13332/100000: episode: 132, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.881, 10.347], loss: 0.000051, mae: 0.006814, mean_q: 0.466143
 13433/100000: episode: 133, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.849, mean reward: 0.008 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.610, 10.462], loss: 0.000047, mae: 0.006988, mean_q: 0.495319
 13534/100000: episode: 134, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.808, 10.100], loss: 0.000049, mae: 0.006865, mean_q: 0.494140
 13635/100000: episode: 135, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.100], loss: 0.000045, mae: 0.006843, mean_q: 0.509299
 13736/100000: episode: 136, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.376, 10.100], loss: 0.000041, mae: 0.006112, mean_q: 0.517947
 13837/100000: episode: 137, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.960, 10.352], loss: 0.000044, mae: 0.006600, mean_q: 0.535526
 13938/100000: episode: 138, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.728, 10.192], loss: 0.000042, mae: 0.006347, mean_q: 0.550091
 14039/100000: episode: 139, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.731, 10.100], loss: 0.000043, mae: 0.006679, mean_q: 0.554701
 14140/100000: episode: 140, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.619, mean reward: 0.006 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.174, 10.300], loss: 0.000064, mae: 0.007497, mean_q: 0.558254
 14241/100000: episode: 141, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.203], loss: 0.000068, mae: 0.008167, mean_q: 0.573252
 14342/100000: episode: 142, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.337, 10.100], loss: 0.000050, mae: 0.007002, mean_q: 0.590299
 14443/100000: episode: 143, duration: 0.670s, episode steps: 101, steps per second: 151, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.893, 10.121], loss: 0.000049, mae: 0.006871, mean_q: 0.597347
 14544/100000: episode: 144, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.954, 10.100], loss: 0.000044, mae: 0.006296, mean_q: 0.602596
 14645/100000: episode: 145, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.429, 10.166], loss: 0.000042, mae: 0.006304, mean_q: 0.618062
 14746/100000: episode: 146, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.411, 10.100], loss: 0.000052, mae: 0.006794, mean_q: 0.626366
 14847/100000: episode: 147, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.358, 10.426], loss: 0.000039, mae: 0.005764, mean_q: 0.631578
 14948/100000: episode: 148, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.811, 10.145], loss: 0.000044, mae: 0.005876, mean_q: 0.644437
 15049/100000: episode: 149, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.211, 10.100], loss: 0.000048, mae: 0.006508, mean_q: 0.657620
 15150/100000: episode: 150, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.653, 10.100], loss: 0.000032, mae: 0.005434, mean_q: 0.659866
 15251/100000: episode: 151, duration: 0.633s, episode steps: 101, steps per second: 159, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.546, 10.100], loss: 0.000035, mae: 0.005241, mean_q: 0.667169
 15352/100000: episode: 152, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.876, 10.100], loss: 0.000027, mae: 0.004689, mean_q: 0.676531
 15453/100000: episode: 153, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.721, 10.178], loss: 0.000053, mae: 0.007368, mean_q: 0.681849
 15554/100000: episode: 154, duration: 0.660s, episode steps: 101, steps per second: 153, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.959, 10.321], loss: 0.000043, mae: 0.006497, mean_q: 0.685736
 15655/100000: episode: 155, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.196, 10.108], loss: 0.000028, mae: 0.004887, mean_q: 0.684427
 15756/100000: episode: 156, duration: 0.644s, episode steps: 101, steps per second: 157, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.793, 10.100], loss: 0.000035, mae: 0.005864, mean_q: 0.695954
 15857/100000: episode: 157, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.302, 10.177], loss: 0.000022, mae: 0.004433, mean_q: 0.702775
 15958/100000: episode: 158, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.412, 10.170], loss: 0.000031, mae: 0.005219, mean_q: 0.704185
 16059/100000: episode: 159, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.100], loss: 0.000034, mae: 0.005792, mean_q: 0.706170
 16160/100000: episode: 160, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.719, 10.286], loss: 0.000031, mae: 0.005294, mean_q: 0.711552
 16261/100000: episode: 161, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.326, 10.100], loss: 0.000026, mae: 0.004743, mean_q: 0.715819
 16362/100000: episode: 162, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.894, 10.421], loss: 0.000034, mae: 0.005862, mean_q: 0.721692
 16463/100000: episode: 163, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.005, 10.100], loss: 0.000030, mae: 0.005294, mean_q: 0.725054
 16564/100000: episode: 164, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.737, 10.199], loss: 0.000031, mae: 0.005127, mean_q: 0.726431
 16665/100000: episode: 165, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.115], loss: 0.000033, mae: 0.005576, mean_q: 0.728915
 16766/100000: episode: 166, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.998, 10.126], loss: 0.000029, mae: 0.005207, mean_q: 0.729626
 16867/100000: episode: 167, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.649, mean reward: 0.006 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.401, 10.100], loss: 0.000031, mae: 0.005229, mean_q: 0.732951
 16968/100000: episode: 168, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.294, 10.233], loss: 0.000038, mae: 0.005514, mean_q: 0.734882
 17069/100000: episode: 169, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.036, 10.176], loss: 0.000039, mae: 0.005868, mean_q: 0.735944
 17170/100000: episode: 170, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.822, 10.138], loss: 0.000025, mae: 0.004736, mean_q: 0.735577
 17271/100000: episode: 171, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.987, 10.278], loss: 0.000023, mae: 0.004468, mean_q: 0.737075
 17372/100000: episode: 172, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.394, 10.100], loss: 0.000039, mae: 0.006171, mean_q: 0.737668
 17473/100000: episode: 173, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.420, 10.189], loss: 0.000035, mae: 0.005971, mean_q: 0.738905
 17574/100000: episode: 174, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.763, 10.100], loss: 0.000033, mae: 0.005377, mean_q: 0.739676
 17675/100000: episode: 175, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.797, 10.136], loss: 0.000036, mae: 0.006276, mean_q: 0.740183
 17776/100000: episode: 176, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.634, 10.148], loss: 0.000034, mae: 0.005669, mean_q: 0.741085
 17877/100000: episode: 177, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.712, 10.385], loss: 0.000025, mae: 0.004679, mean_q: 0.740692
 17978/100000: episode: 178, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.543, 10.395], loss: 0.000053, mae: 0.007032, mean_q: 0.741606
 18079/100000: episode: 179, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.652, 10.191], loss: 0.000026, mae: 0.004868, mean_q: 0.742516
 18180/100000: episode: 180, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.533, 10.297], loss: 0.000027, mae: 0.005360, mean_q: 0.742338
 18281/100000: episode: 181, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.461, 10.100], loss: 0.000038, mae: 0.005640, mean_q: 0.742152
 18382/100000: episode: 182, duration: 0.676s, episode steps: 101, steps per second: 150, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.901, 10.291], loss: 0.000044, mae: 0.006302, mean_q: 0.742801
 18483/100000: episode: 183, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.623, 10.106], loss: 0.000030, mae: 0.004950, mean_q: 0.742092
 18584/100000: episode: 184, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.875, 10.100], loss: 0.000022, mae: 0.003746, mean_q: 0.742874
 18685/100000: episode: 185, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.651, 10.100], loss: 0.000044, mae: 0.006484, mean_q: 0.741568
 18786/100000: episode: 186, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.605, 10.100], loss: 0.000033, mae: 0.004971, mean_q: 0.741533
 18887/100000: episode: 187, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.049, 10.187], loss: 0.000026, mae: 0.004778, mean_q: 0.741898
 18988/100000: episode: 188, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.636, 10.188], loss: 0.000034, mae: 0.005345, mean_q: 0.741253
 19089/100000: episode: 189, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.794, 10.100], loss: 0.000018, mae: 0.003700, mean_q: 0.741182
 19190/100000: episode: 190, duration: 0.741s, episode steps: 101, steps per second: 136, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.126, 10.144], loss: 0.000025, mae: 0.004518, mean_q: 0.740745
 19291/100000: episode: 191, duration: 0.629s, episode steps: 101, steps per second: 160, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.331, 10.136], loss: 0.000021, mae: 0.004055, mean_q: 0.740459
 19392/100000: episode: 192, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.860, mean reward: 0.009 [0.000, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.733, 10.296], loss: 0.000025, mae: 0.004573, mean_q: 0.739437
 19493/100000: episode: 193, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.679, 10.217], loss: 0.000021, mae: 0.004119, mean_q: 0.738060
 19594/100000: episode: 194, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.964, 10.100], loss: 0.000025, mae: 0.004088, mean_q: 0.738113
 19695/100000: episode: 195, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.816, mean reward: 0.008 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.774, 10.282], loss: 0.000019, mae: 0.003847, mean_q: 0.737453
 19796/100000: episode: 196, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.657, 10.100], loss: 0.000027, mae: 0.004666, mean_q: 0.737248
 19897/100000: episode: 197, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.405, 10.292], loss: 0.000031, mae: 0.005178, mean_q: 0.737557
 19998/100000: episode: 198, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.332, 10.360], loss: 0.000018, mae: 0.003247, mean_q: 0.736526
 20099/100000: episode: 199, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.725, 10.100], loss: 0.000026, mae: 0.004617, mean_q: 0.735770
[Info] 1-TH LEVEL FOUND: 0.7870579361915588, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.907s, episode steps: 101, steps per second: 17, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.739, 10.100], loss: 0.000029, mae: 0.004680, mean_q: 0.735619
 20299/100000: episode: 201, duration: 0.732s, episode steps: 99, steps per second: 135, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.389, 10.178], loss: 0.000036, mae: 0.005344, mean_q: 0.735551
 20397/100000: episode: 202, duration: 0.592s, episode steps: 98, steps per second: 165, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.626, 10.328], loss: 0.000028, mae: 0.004754, mean_q: 0.735325
 20497/100000: episode: 203, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.631, 10.188], loss: 0.000032, mae: 0.005297, mean_q: 0.734961
 20595/100000: episode: 204, duration: 0.609s, episode steps: 98, steps per second: 161, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.340, 10.100], loss: 0.000028, mae: 0.004270, mean_q: 0.735043
 20693/100000: episode: 205, duration: 0.581s, episode steps: 98, steps per second: 169, episode reward: 0.745, mean reward: 0.008 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.994, 10.103], loss: 0.000020, mae: 0.004009, mean_q: 0.734601
 20790/100000: episode: 206, duration: 0.667s, episode steps: 97, steps per second: 145, episode reward: 0.742, mean reward: 0.008 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.631, 10.184], loss: 0.000027, mae: 0.004745, mean_q: 0.734885
 20890/100000: episode: 207, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 0.841, mean reward: 0.008 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.162, 10.100], loss: 0.000023, mae: 0.004348, mean_q: 0.734760
 20986/100000: episode: 208, duration: 0.579s, episode steps: 96, steps per second: 166, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.797, 10.221], loss: 0.000031, mae: 0.004872, mean_q: 0.734845
 21084/100000: episode: 209, duration: 0.597s, episode steps: 98, steps per second: 164, episode reward: 0.743, mean reward: 0.008 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.646, 10.100], loss: 0.000017, mae: 0.003143, mean_q: 0.735204
 21181/100000: episode: 210, duration: 0.579s, episode steps: 97, steps per second: 168, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.267, 10.186], loss: 0.000033, mae: 0.005202, mean_q: 0.735059
 21279/100000: episode: 211, duration: 0.585s, episode steps: 98, steps per second: 168, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.801, 10.112], loss: 0.000029, mae: 0.004632, mean_q: 0.735374
 21376/100000: episode: 212, duration: 0.590s, episode steps: 97, steps per second: 165, episode reward: 0.729, mean reward: 0.008 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.791, 10.297], loss: 0.000018, mae: 0.003403, mean_q: 0.735325
 21473/100000: episode: 213, duration: 0.562s, episode steps: 97, steps per second: 173, episode reward: 0.728, mean reward: 0.008 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.575, 10.100], loss: 0.000024, mae: 0.004476, mean_q: 0.735162
 21573/100000: episode: 214, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.314, 10.184], loss: 0.000043, mae: 0.005801, mean_q: 0.735280
 21669/100000: episode: 215, duration: 0.586s, episode steps: 96, steps per second: 164, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.176, 10.225], loss: 0.000023, mae: 0.003532, mean_q: 0.735495
 21768/100000: episode: 216, duration: 0.550s, episode steps: 99, steps per second: 180, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.762, 10.100], loss: 0.000015, mae: 0.003275, mean_q: 0.735343
 21866/100000: episode: 217, duration: 0.614s, episode steps: 98, steps per second: 160, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.451, 10.111], loss: 0.000035, mae: 0.005687, mean_q: 0.735833
 21964/100000: episode: 218, duration: 0.679s, episode steps: 98, steps per second: 144, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.184, 10.100], loss: 0.000026, mae: 0.005084, mean_q: 0.735764
 22062/100000: episode: 219, duration: 0.622s, episode steps: 98, steps per second: 158, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.512, 10.100], loss: 0.000022, mae: 0.003880, mean_q: 0.736376
 22161/100000: episode: 220, duration: 0.584s, episode steps: 99, steps per second: 169, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-1.020, 10.300], loss: 0.000027, mae: 0.004585, mean_q: 0.736813
 22259/100000: episode: 221, duration: 0.614s, episode steps: 98, steps per second: 159, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.153, 10.154], loss: 0.000021, mae: 0.003716, mean_q: 0.736795
 22358/100000: episode: 222, duration: 0.602s, episode steps: 99, steps per second: 164, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.246, 10.195], loss: 0.000017, mae: 0.003714, mean_q: 0.736792
 22455/100000: episode: 223, duration: 0.610s, episode steps: 97, steps per second: 159, episode reward: 0.896, mean reward: 0.009 [0.000, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.529, 10.687], loss: 0.000021, mae: 0.003735, mean_q: 0.736862
 22554/100000: episode: 224, duration: 0.588s, episode steps: 99, steps per second: 168, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-1.534, 10.100], loss: 0.000028, mae: 0.004613, mean_q: 0.737135
 22651/100000: episode: 225, duration: 0.613s, episode steps: 97, steps per second: 158, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.379, 10.254], loss: 0.000024, mae: 0.003756, mean_q: 0.737202
 22749/100000: episode: 226, duration: 0.590s, episode steps: 98, steps per second: 166, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-1.055, 10.297], loss: 0.000033, mae: 0.005444, mean_q: 0.737865
 22845/100000: episode: 227, duration: 0.581s, episode steps: 96, steps per second: 165, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-1.404, 10.137], loss: 0.000020, mae: 0.003827, mean_q: 0.738111
 22944/100000: episode: 228, duration: 0.640s, episode steps: 99, steps per second: 155, episode reward: 0.743, mean reward: 0.008 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.772, 10.100], loss: 0.000020, mae: 0.003707, mean_q: 0.738525
 23041/100000: episode: 229, duration: 0.617s, episode steps: 97, steps per second: 157, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-0.829, 10.136], loss: 0.000018, mae: 0.003599, mean_q: 0.738770
 23139/100000: episode: 230, duration: 0.595s, episode steps: 98, steps per second: 165, episode reward: 0.750, mean reward: 0.008 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.175, 10.327], loss: 0.000013, mae: 0.003035, mean_q: 0.738758
 23235/100000: episode: 231, duration: 0.571s, episode steps: 96, steps per second: 168, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-1.318, 10.100], loss: 0.000019, mae: 0.003703, mean_q: 0.739163
 23333/100000: episode: 232, duration: 0.609s, episode steps: 98, steps per second: 161, episode reward: 0.742, mean reward: 0.008 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.752, 10.369], loss: 0.000014, mae: 0.002848, mean_q: 0.739693
 23431/100000: episode: 233, duration: 0.614s, episode steps: 98, steps per second: 160, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.974, 10.100], loss: 0.000016, mae: 0.003376, mean_q: 0.739565
 23527/100000: episode: 234, duration: 0.632s, episode steps: 96, steps per second: 152, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-1.144, 10.100], loss: 0.000039, mae: 0.005999, mean_q: 0.739878
 23625/100000: episode: 235, duration: 0.583s, episode steps: 98, steps per second: 168, episode reward: 0.753, mean reward: 0.008 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.647, 10.100], loss: 0.000020, mae: 0.004106, mean_q: 0.740733
 23723/100000: episode: 236, duration: 0.603s, episode steps: 98, steps per second: 163, episode reward: 1.008, mean reward: 0.010 [0.000, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-1.292, 10.222], loss: 0.000023, mae: 0.003847, mean_q: 0.740943
 23821/100000: episode: 237, duration: 0.599s, episode steps: 98, steps per second: 164, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.723, 10.280], loss: 0.000040, mae: 0.004670, mean_q: 0.741018
 23917/100000: episode: 238, duration: 0.608s, episode steps: 96, steps per second: 158, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.574, 10.295], loss: 0.000057, mae: 0.006516, mean_q: 0.741149
 24014/100000: episode: 239, duration: 0.612s, episode steps: 97, steps per second: 158, episode reward: 0.739, mean reward: 0.008 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-1.181, 10.100], loss: 0.000017, mae: 0.003313, mean_q: 0.741721
 24112/100000: episode: 240, duration: 0.608s, episode steps: 98, steps per second: 161, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.934, 10.280], loss: 0.000027, mae: 0.003532, mean_q: 0.742678
 24210/100000: episode: 241, duration: 0.780s, episode steps: 98, steps per second: 126, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-1.139, 10.100], loss: 0.000016, mae: 0.003164, mean_q: 0.742429
 24307/100000: episode: 242, duration: 0.597s, episode steps: 97, steps per second: 162, episode reward: 0.831, mean reward: 0.009 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.205, 10.277], loss: 0.000045, mae: 0.005664, mean_q: 0.743235
 24405/100000: episode: 243, duration: 0.573s, episode steps: 98, steps per second: 171, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.665, 10.163], loss: 0.000013, mae: 0.002614, mean_q: 0.743465
 24503/100000: episode: 244, duration: 0.578s, episode steps: 98, steps per second: 170, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.650, 10.100], loss: 0.000021, mae: 0.003532, mean_q: 0.743923
 24600/100000: episode: 245, duration: 0.577s, episode steps: 97, steps per second: 168, episode reward: 0.740, mean reward: 0.008 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-1.232, 10.365], loss: 0.000025, mae: 0.003148, mean_q: 0.744677
 24696/100000: episode: 246, duration: 0.533s, episode steps: 96, steps per second: 180, episode reward: 0.725, mean reward: 0.008 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.991, 10.100], loss: 0.000031, mae: 0.003597, mean_q: 0.744375
 24796/100000: episode: 247, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.470, 10.253], loss: 0.000019, mae: 0.003368, mean_q: 0.744699
 24892/100000: episode: 248, duration: 0.592s, episode steps: 96, steps per second: 162, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.657, 10.100], loss: 0.000015, mae: 0.002917, mean_q: 0.744560
 24992/100000: episode: 249, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.101, 10.100], loss: 0.000020, mae: 0.004169, mean_q: 0.744883
 25088/100000: episode: 250, duration: 0.626s, episode steps: 96, steps per second: 153, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.559, 10.118], loss: 0.000021, mae: 0.003589, mean_q: 0.745123
 25185/100000: episode: 251, duration: 0.570s, episode steps: 97, steps per second: 170, episode reward: 0.858, mean reward: 0.009 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.256, 10.277], loss: 0.000052, mae: 0.005137, mean_q: 0.745598
 25282/100000: episode: 252, duration: 0.609s, episode steps: 97, steps per second: 159, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.551, 10.216], loss: 0.000043, mae: 0.005473, mean_q: 0.745773
 25381/100000: episode: 253, duration: 0.599s, episode steps: 99, steps per second: 165, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.142, 10.215], loss: 0.000033, mae: 0.005085, mean_q: 0.746039
 25479/100000: episode: 254, duration: 0.607s, episode steps: 98, steps per second: 161, episode reward: 0.745, mean reward: 0.008 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.836, 10.106], loss: 0.000041, mae: 0.003979, mean_q: 0.746796
 25576/100000: episode: 255, duration: 0.581s, episode steps: 97, steps per second: 167, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.913, 10.201], loss: 0.000036, mae: 0.004528, mean_q: 0.746889
 25674/100000: episode: 256, duration: 0.598s, episode steps: 98, steps per second: 164, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.768, 10.100], loss: 0.000034, mae: 0.003958, mean_q: 0.747568
 25772/100000: episode: 257, duration: 0.665s, episode steps: 98, steps per second: 147, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.308, 10.260], loss: 0.000038, mae: 0.004841, mean_q: 0.747664
 25870/100000: episode: 258, duration: 0.560s, episode steps: 98, steps per second: 175, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.566, 10.169], loss: 0.000029, mae: 0.003891, mean_q: 0.747803
 25967/100000: episode: 259, duration: 0.852s, episode steps: 97, steps per second: 114, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.360, 10.302], loss: 0.000015, mae: 0.003074, mean_q: 0.747566
 26067/100000: episode: 260, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.893, 10.100], loss: 0.000016, mae: 0.003193, mean_q: 0.747709
 26164/100000: episode: 261, duration: 0.568s, episode steps: 97, steps per second: 171, episode reward: 0.746, mean reward: 0.008 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.896, 10.318], loss: 0.000019, mae: 0.003359, mean_q: 0.747718
 26262/100000: episode: 262, duration: 0.743s, episode steps: 98, steps per second: 132, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.168, 10.145], loss: 0.000028, mae: 0.004350, mean_q: 0.748433
 26358/100000: episode: 263, duration: 0.561s, episode steps: 96, steps per second: 171, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.512, 10.100], loss: 0.000020, mae: 0.003768, mean_q: 0.748566
 26454/100000: episode: 264, duration: 0.571s, episode steps: 96, steps per second: 168, episode reward: 0.721, mean reward: 0.008 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.995, 10.272], loss: 0.000022, mae: 0.003770, mean_q: 0.749242
 26552/100000: episode: 265, duration: 0.572s, episode steps: 98, steps per second: 171, episode reward: 0.753, mean reward: 0.008 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-1.008, 10.119], loss: 0.000014, mae: 0.002902, mean_q: 0.749027
 26651/100000: episode: 266, duration: 0.578s, episode steps: 99, steps per second: 171, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.808, 10.100], loss: 0.000018, mae: 0.003509, mean_q: 0.749031
 26748/100000: episode: 267, duration: 0.565s, episode steps: 97, steps per second: 172, episode reward: 0.728, mean reward: 0.008 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.623, 10.100], loss: 0.000049, mae: 0.004595, mean_q: 0.749362
 26844/100000: episode: 268, duration: 0.581s, episode steps: 96, steps per second: 165, episode reward: 0.754, mean reward: 0.008 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.011, 10.100], loss: 0.000021, mae: 0.003842, mean_q: 0.749725
 26943/100000: episode: 269, duration: 0.568s, episode steps: 99, steps per second: 174, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.890, 10.100], loss: 0.000034, mae: 0.004479, mean_q: 0.749437
 27040/100000: episode: 270, duration: 0.543s, episode steps: 97, steps per second: 179, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.838, 10.146], loss: 0.000037, mae: 0.004800, mean_q: 0.749685
 27136/100000: episode: 271, duration: 0.555s, episode steps: 96, steps per second: 173, episode reward: 0.746, mean reward: 0.008 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-1.102, 10.100], loss: 0.000045, mae: 0.005184, mean_q: 0.749744
 27232/100000: episode: 272, duration: 0.576s, episode steps: 96, steps per second: 167, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.830, 10.100], loss: 0.000032, mae: 0.003361, mean_q: 0.749804
 27332/100000: episode: 273, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.070, 10.100], loss: 0.000018, mae: 0.002923, mean_q: 0.749890
 27430/100000: episode: 274, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 0.752, mean reward: 0.008 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.006, 10.104], loss: 0.000035, mae: 0.003926, mean_q: 0.750603
 27529/100000: episode: 275, duration: 0.764s, episode steps: 99, steps per second: 130, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.685, 10.100], loss: 0.000012, mae: 0.002932, mean_q: 0.750015
 27627/100000: episode: 276, duration: 0.678s, episode steps: 98, steps per second: 144, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-1.069, 10.229], loss: 0.000010, mae: 0.002588, mean_q: 0.750214
 27725/100000: episode: 277, duration: 0.570s, episode steps: 98, steps per second: 172, episode reward: 0.736, mean reward: 0.008 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.822, 10.100], loss: 0.000027, mae: 0.003519, mean_q: 0.749938
 27823/100000: episode: 278, duration: 0.586s, episode steps: 98, steps per second: 167, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-1.153, 10.168], loss: 0.000010, mae: 0.002393, mean_q: 0.750032
 27921/100000: episode: 279, duration: 0.701s, episode steps: 98, steps per second: 140, episode reward: 0.736, mean reward: 0.008 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.778, 10.100], loss: 0.000021, mae: 0.003185, mean_q: 0.750224
 28019/100000: episode: 280, duration: 0.566s, episode steps: 98, steps per second: 173, episode reward: 0.833, mean reward: 0.009 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-2.038, 10.100], loss: 0.000013, mae: 0.002845, mean_q: 0.750068
 28118/100000: episode: 281, duration: 0.854s, episode steps: 99, steps per second: 116, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.316, 10.245], loss: 0.000013, mae: 0.002890, mean_q: 0.750541
 28216/100000: episode: 282, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.514, 10.253], loss: 0.000019, mae: 0.003221, mean_q: 0.750080
 28314/100000: episode: 283, duration: 0.585s, episode steps: 98, steps per second: 167, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.826, 10.110], loss: 0.000042, mae: 0.004863, mean_q: 0.750602
 28411/100000: episode: 284, duration: 0.858s, episode steps: 97, steps per second: 113, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.718, 10.100], loss: 0.000021, mae: 0.004060, mean_q: 0.750283
 28507/100000: episode: 285, duration: 0.584s, episode steps: 96, steps per second: 164, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-1.262, 10.100], loss: 0.000019, mae: 0.003361, mean_q: 0.750202
 28605/100000: episode: 286, duration: 0.704s, episode steps: 98, steps per second: 139, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.283, 10.100], loss: 0.000025, mae: 0.003348, mean_q: 0.750384
 28703/100000: episode: 287, duration: 0.585s, episode steps: 98, steps per second: 167, episode reward: 0.744, mean reward: 0.008 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.942, 10.100], loss: 0.000015, mae: 0.003059, mean_q: 0.750196
 28799/100000: episode: 288, duration: 0.591s, episode steps: 96, steps per second: 162, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.457, 10.277], loss: 0.000012, mae: 0.002572, mean_q: 0.750249
 28899/100000: episode: 289, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.760, 10.100], loss: 0.000006, mae: 0.001984, mean_q: 0.750033
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7870579361915588
1
 28995/100000: episode: 290, duration: 5.007s, episode steps: 96, steps per second: 19, episode reward: 0.738, mean reward: 0.008 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.966, 10.230], loss: 0.000008, mae: 0.002398, mean_q: 0.750343
 29096/100000: episode: 291, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.462, 10.100], loss: 0.000014, mae: 0.003112, mean_q: 0.750530
 29197/100000: episode: 292, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.085, 10.100], loss: 0.000015, mae: 0.002808, mean_q: 0.750730
 29298/100000: episode: 293, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.370, 10.100], loss: 0.000013, mae: 0.002890, mean_q: 0.750426
 29399/100000: episode: 294, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.155, 10.157], loss: 0.000009, mae: 0.002947, mean_q: 0.750302
 29500/100000: episode: 295, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.065, 10.338], loss: 0.000019, mae: 0.003246, mean_q: 0.750463
 29601/100000: episode: 296, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.663, 10.100], loss: 0.000019, mae: 0.003845, mean_q: 0.750642
 29702/100000: episode: 297, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.883, 10.100], loss: 0.000017, mae: 0.003042, mean_q: 0.750773
 29803/100000: episode: 298, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.183], loss: 0.000018, mae: 0.003527, mean_q: 0.750647
 29904/100000: episode: 299, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.142, 10.275], loss: 0.000024, mae: 0.004153, mean_q: 0.750727
 30005/100000: episode: 300, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.058, 10.100], loss: 0.000014, mae: 0.003161, mean_q: 0.750946
 30106/100000: episode: 301, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.265, 10.291], loss: 0.000012, mae: 0.002899, mean_q: 0.750876
 30207/100000: episode: 302, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.425, 10.100], loss: 0.000012, mae: 0.002433, mean_q: 0.750711
 30308/100000: episode: 303, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.871, 10.205], loss: 0.000012, mae: 0.002835, mean_q: 0.750296
 30409/100000: episode: 304, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.050, 10.324], loss: 0.000018, mae: 0.003319, mean_q: 0.750756
 30510/100000: episode: 305, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.992, 10.225], loss: 0.000020, mae: 0.003246, mean_q: 0.751217
 30611/100000: episode: 306, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.949, 10.232], loss: 0.000021, mae: 0.003836, mean_q: 0.750671
 30712/100000: episode: 307, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.100], loss: 0.000015, mae: 0.003076, mean_q: 0.750639
 30813/100000: episode: 308, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.662, mean reward: 0.007 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.978, 10.271], loss: 0.000013, mae: 0.002829, mean_q: 0.750723
 30914/100000: episode: 309, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.338, 10.100], loss: 0.000017, mae: 0.003468, mean_q: 0.750557
 31015/100000: episode: 310, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.032, 10.200], loss: 0.000009, mae: 0.002621, mean_q: 0.750505
 31116/100000: episode: 311, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.971, 10.155], loss: 0.000027, mae: 0.004202, mean_q: 0.750647
 31217/100000: episode: 312, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.998, 10.120], loss: 0.000022, mae: 0.003822, mean_q: 0.750697
 31318/100000: episode: 313, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.778, 10.100], loss: 0.000018, mae: 0.003387, mean_q: 0.750564
 31419/100000: episode: 314, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.593, 10.206], loss: 0.000017, mae: 0.003481, mean_q: 0.750174
 31520/100000: episode: 315, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.749, 10.197], loss: 0.000016, mae: 0.003029, mean_q: 0.750501
 31621/100000: episode: 316, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.041, 10.186], loss: 0.000023, mae: 0.003426, mean_q: 0.750169
 31722/100000: episode: 317, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.966, 10.100], loss: 0.000018, mae: 0.003284, mean_q: 0.749918
 31823/100000: episode: 318, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.191, 10.100], loss: 0.000014, mae: 0.002647, mean_q: 0.749904
 31924/100000: episode: 319, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.400, 10.223], loss: 0.000021, mae: 0.003685, mean_q: 0.749704
 32025/100000: episode: 320, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.641, mean reward: 0.006 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.248, 10.100], loss: 0.000016, mae: 0.002845, mean_q: 0.749710
 32126/100000: episode: 321, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.803, 10.169], loss: 0.000015, mae: 0.003015, mean_q: 0.749875
 32227/100000: episode: 322, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.962, 10.259], loss: 0.000025, mae: 0.003413, mean_q: 0.749507
 32328/100000: episode: 323, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.399, 10.100], loss: 0.000018, mae: 0.002993, mean_q: 0.749077
 32429/100000: episode: 324, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.580, 10.100], loss: 0.000017, mae: 0.003143, mean_q: 0.749321
 32530/100000: episode: 325, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.853, mean reward: 0.008 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.179, 10.100], loss: 0.000018, mae: 0.003256, mean_q: 0.748772
 32631/100000: episode: 326, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.580, 10.430], loss: 0.000019, mae: 0.003628, mean_q: 0.748864
 32732/100000: episode: 327, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.824, 10.182], loss: 0.000022, mae: 0.004068, mean_q: 0.748695
 32833/100000: episode: 328, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.997, 10.100], loss: 0.000021, mae: 0.003367, mean_q: 0.748582
 32934/100000: episode: 329, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.517, 10.108], loss: 0.000021, mae: 0.003490, mean_q: 0.748424
 33035/100000: episode: 330, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.135, 10.338], loss: 0.000010, mae: 0.002444, mean_q: 0.748501
 33136/100000: episode: 331, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.434, 10.192], loss: 0.000013, mae: 0.002965, mean_q: 0.748358
 33237/100000: episode: 332, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.350, 10.100], loss: 0.000016, mae: 0.003157, mean_q: 0.748233
 33338/100000: episode: 333, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.488, 10.201], loss: 0.000016, mae: 0.003177, mean_q: 0.747855
 33439/100000: episode: 334, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.826, 10.100], loss: 0.000022, mae: 0.003428, mean_q: 0.747813
 33540/100000: episode: 335, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.733, 10.200], loss: 0.000017, mae: 0.003512, mean_q: 0.747509
 33641/100000: episode: 336, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.620, 10.135], loss: 0.000020, mae: 0.003888, mean_q: 0.747176
 33742/100000: episode: 337, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.828, 10.350], loss: 0.000016, mae: 0.002943, mean_q: 0.747426
 33843/100000: episode: 338, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.664, mean reward: 0.007 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.059, 10.158], loss: 0.000009, mae: 0.002288, mean_q: 0.747210
 33944/100000: episode: 339, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.148, 10.100], loss: 0.000031, mae: 0.004102, mean_q: 0.747118
 34045/100000: episode: 340, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.905, 10.240], loss: 0.000015, mae: 0.003026, mean_q: 0.746654
 34146/100000: episode: 341, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.528, 10.332], loss: 0.000008, mae: 0.002174, mean_q: 0.746389
 34247/100000: episode: 342, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.653, 10.203], loss: 0.000026, mae: 0.003712, mean_q: 0.746596
 34348/100000: episode: 343, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.591, 10.100], loss: 0.000020, mae: 0.003322, mean_q: 0.746664
 34449/100000: episode: 344, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.721, 10.100], loss: 0.000015, mae: 0.002951, mean_q: 0.746214
 34550/100000: episode: 345, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.591, 10.100], loss: 0.000012, mae: 0.002566, mean_q: 0.745420
 34651/100000: episode: 346, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.910, 10.102], loss: 0.000015, mae: 0.003212, mean_q: 0.745292
 34752/100000: episode: 347, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.494, 10.356], loss: 0.000014, mae: 0.003146, mean_q: 0.744655
 34853/100000: episode: 348, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.884, 10.166], loss: 0.000017, mae: 0.002707, mean_q: 0.744525
 34954/100000: episode: 349, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.242, 10.228], loss: 0.000018, mae: 0.003632, mean_q: 0.744121
 35055/100000: episode: 350, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.314, 10.103], loss: 0.000014, mae: 0.002991, mean_q: 0.744078
 35156/100000: episode: 351, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.100], loss: 0.000017, mae: 0.003067, mean_q: 0.743810
 35257/100000: episode: 352, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.352, 10.390], loss: 0.000022, mae: 0.003994, mean_q: 0.743480
 35358/100000: episode: 353, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.493, 10.100], loss: 0.000013, mae: 0.002672, mean_q: 0.743186
 35459/100000: episode: 354, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.657, 10.304], loss: 0.000012, mae: 0.002508, mean_q: 0.742352
 35560/100000: episode: 355, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.870, 10.112], loss: 0.000018, mae: 0.003565, mean_q: 0.742400
 35661/100000: episode: 356, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.560, 10.243], loss: 0.000014, mae: 0.002489, mean_q: 0.742094
 35762/100000: episode: 357, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.930, 10.245], loss: 0.000016, mae: 0.003054, mean_q: 0.742189
 35863/100000: episode: 358, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.539, 10.100], loss: 0.000020, mae: 0.003394, mean_q: 0.741828
 35964/100000: episode: 359, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.816, mean reward: 0.008 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.408, 10.100], loss: 0.000017, mae: 0.003084, mean_q: 0.741597
 36065/100000: episode: 360, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.501, 10.100], loss: 0.000013, mae: 0.002696, mean_q: 0.741444
 36166/100000: episode: 361, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.416], loss: 0.000020, mae: 0.002772, mean_q: 0.741387
 36267/100000: episode: 362, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.270, 10.105], loss: 0.000015, mae: 0.003256, mean_q: 0.741273
 36368/100000: episode: 363, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.192, 10.338], loss: 0.000013, mae: 0.002772, mean_q: 0.740954
 36469/100000: episode: 364, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.243, 10.100], loss: 0.000015, mae: 0.002637, mean_q: 0.740948
 36570/100000: episode: 365, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.880, mean reward: 0.009 [0.000, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.636, 10.547], loss: 0.000019, mae: 0.003195, mean_q: 0.740608
 36671/100000: episode: 366, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.316, 10.261], loss: 0.000014, mae: 0.002935, mean_q: 0.740827
 36772/100000: episode: 367, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.301], loss: 0.000014, mae: 0.002896, mean_q: 0.740678
 36873/100000: episode: 368, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.771, 10.122], loss: 0.000025, mae: 0.003569, mean_q: 0.740539
 36974/100000: episode: 369, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.199, 10.223], loss: 0.000023, mae: 0.003622, mean_q: 0.740575
 37075/100000: episode: 370, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.015, 10.515], loss: 0.000017, mae: 0.003102, mean_q: 0.740589
 37176/100000: episode: 371, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.623, 10.429], loss: 0.000015, mae: 0.002922, mean_q: 0.740368
 37277/100000: episode: 372, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.358, 10.230], loss: 0.000009, mae: 0.002162, mean_q: 0.740278
 37378/100000: episode: 373, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.728, 10.100], loss: 0.000015, mae: 0.002574, mean_q: 0.740235
 37479/100000: episode: 374, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.806, 10.351], loss: 0.000022, mae: 0.003800, mean_q: 0.740176
 37580/100000: episode: 375, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.844, 10.360], loss: 0.000017, mae: 0.002989, mean_q: 0.740336
 37681/100000: episode: 376, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.011, 10.344], loss: 0.000019, mae: 0.003530, mean_q: 0.740449
 37782/100000: episode: 377, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.509, 10.100], loss: 0.000019, mae: 0.003260, mean_q: 0.740526
 37883/100000: episode: 378, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.636, 10.100], loss: 0.000021, mae: 0.003239, mean_q: 0.740544
 37984/100000: episode: 379, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.947, 10.100], loss: 0.000019, mae: 0.003236, mean_q: 0.740320
 38085/100000: episode: 380, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.894, 10.150], loss: 0.000019, mae: 0.002909, mean_q: 0.740097
 38186/100000: episode: 381, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.408, 10.100], loss: 0.000020, mae: 0.003365, mean_q: 0.740132
 38287/100000: episode: 382, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.592, 10.100], loss: 0.000019, mae: 0.003547, mean_q: 0.740002
 38388/100000: episode: 383, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.563, 10.133], loss: 0.000012, mae: 0.002634, mean_q: 0.740520
 38489/100000: episode: 384, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.216, 10.100], loss: 0.000017, mae: 0.003449, mean_q: 0.740443
 38590/100000: episode: 385, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.619, 10.124], loss: 0.000015, mae: 0.002967, mean_q: 0.740486
 38691/100000: episode: 386, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.393, 10.100], loss: 0.000016, mae: 0.003009, mean_q: 0.740669
 38792/100000: episode: 387, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.669, mean reward: 0.007 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.830, 10.233], loss: 0.000017, mae: 0.003525, mean_q: 0.739979
 38893/100000: episode: 388, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.528, 10.100], loss: 0.000016, mae: 0.002780, mean_q: 0.740176
 38994/100000: episode: 389, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.227, 10.100], loss: 0.000007, mae: 0.002325, mean_q: 0.740333
[Info] 1-TH LEVEL FOUND: 0.769480288028717, Considering 10/100 traces
 39095/100000: episode: 390, duration: 5.107s, episode steps: 101, steps per second: 20, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.076, 10.115], loss: 0.000014, mae: 0.002692, mean_q: 0.740169
 39169/100000: episode: 391, duration: 0.435s, episode steps: 74, steps per second: 170, episode reward: 0.702, mean reward: 0.009 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.131, 10.340], loss: 0.000013, mae: 0.002346, mean_q: 0.740345
 39242/100000: episode: 392, duration: 0.416s, episode steps: 73, steps per second: 175, episode reward: 0.784, mean reward: 0.011 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.837, 10.100], loss: 0.000015, mae: 0.003203, mean_q: 0.740505
 39315/100000: episode: 393, duration: 0.427s, episode steps: 73, steps per second: 171, episode reward: 0.659, mean reward: 0.009 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.633, 10.125], loss: 0.000015, mae: 0.002798, mean_q: 0.740580
 39387/100000: episode: 394, duration: 0.404s, episode steps: 72, steps per second: 178, episode reward: 0.698, mean reward: 0.010 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-1.080, 10.328], loss: 0.000021, mae: 0.003298, mean_q: 0.740494
 39462/100000: episode: 395, duration: 0.428s, episode steps: 75, steps per second: 175, episode reward: 0.677, mean reward: 0.009 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-0.997, 10.200], loss: 0.000026, mae: 0.004627, mean_q: 0.740242
 39536/100000: episode: 396, duration: 0.419s, episode steps: 74, steps per second: 177, episode reward: 0.671, mean reward: 0.009 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-2.102, 10.100], loss: 0.000022, mae: 0.003244, mean_q: 0.740097
 39610/100000: episode: 397, duration: 0.421s, episode steps: 74, steps per second: 176, episode reward: 0.706, mean reward: 0.010 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-1.840, 10.100], loss: 0.000015, mae: 0.002630, mean_q: 0.740332
 39684/100000: episode: 398, duration: 0.456s, episode steps: 74, steps per second: 162, episode reward: 0.850, mean reward: 0.011 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.692 [-0.368, 10.100], loss: 0.000019, mae: 0.002848, mean_q: 0.740779
 39757/100000: episode: 399, duration: 0.426s, episode steps: 73, steps per second: 171, episode reward: 0.706, mean reward: 0.010 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.202, 10.100], loss: 0.000010, mae: 0.002716, mean_q: 0.740465
 39829/100000: episode: 400, duration: 0.426s, episode steps: 72, steps per second: 169, episode reward: 0.770, mean reward: 0.011 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.245, 10.390], loss: 0.000012, mae: 0.002761, mean_q: 0.740461
 39902/100000: episode: 401, duration: 0.416s, episode steps: 73, steps per second: 175, episode reward: 0.771, mean reward: 0.011 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.703 [-0.568, 10.100], loss: 0.000009, mae: 0.002291, mean_q: 0.740312
 39974/100000: episode: 402, duration: 0.443s, episode steps: 72, steps per second: 162, episode reward: 0.705, mean reward: 0.010 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.612, 10.101], loss: 0.000026, mae: 0.004049, mean_q: 0.740169
 40048/100000: episode: 403, duration: 0.421s, episode steps: 74, steps per second: 176, episode reward: 0.740, mean reward: 0.010 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.959, 10.100], loss: 0.000022, mae: 0.003491, mean_q: 0.740399
 40123/100000: episode: 404, duration: 0.446s, episode steps: 75, steps per second: 168, episode reward: 0.753, mean reward: 0.010 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.133, 10.245], loss: 0.000016, mae: 0.003387, mean_q: 0.740634
 40197/100000: episode: 405, duration: 0.422s, episode steps: 74, steps per second: 175, episode reward: 0.747, mean reward: 0.010 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.698 [-0.881, 10.254], loss: 0.000020, mae: 0.003649, mean_q: 0.740297
 40272/100000: episode: 406, duration: 0.452s, episode steps: 75, steps per second: 166, episode reward: 0.794, mean reward: 0.011 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.203, 10.232], loss: 0.000021, mae: 0.003217, mean_q: 0.740036
 40346/100000: episode: 407, duration: 0.449s, episode steps: 74, steps per second: 165, episode reward: 0.723, mean reward: 0.010 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.266, 10.100], loss: 0.000008, mae: 0.002308, mean_q: 0.740386
 40418/100000: episode: 408, duration: 0.425s, episode steps: 72, steps per second: 169, episode reward: 0.715, mean reward: 0.010 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.697, 10.318], loss: 0.000023, mae: 0.003523, mean_q: 0.740188
 40493/100000: episode: 409, duration: 0.448s, episode steps: 75, steps per second: 167, episode reward: 0.836, mean reward: 0.011 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-0.714, 10.401], loss: 0.000015, mae: 0.002825, mean_q: 0.740517
 40567/100000: episode: 410, duration: 0.423s, episode steps: 74, steps per second: 175, episode reward: 0.730, mean reward: 0.010 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.703 [-0.437, 10.100], loss: 0.000019, mae: 0.003154, mean_q: 0.740446
 40641/100000: episode: 411, duration: 0.455s, episode steps: 74, steps per second: 163, episode reward: 0.747, mean reward: 0.010 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-1.864, 10.218], loss: 0.000020, mae: 0.003114, mean_q: 0.740023
 40716/100000: episode: 412, duration: 0.429s, episode steps: 75, steps per second: 175, episode reward: 0.774, mean reward: 0.010 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.179, 10.141], loss: 0.000024, mae: 0.003465, mean_q: 0.740103
 40788/100000: episode: 413, duration: 0.420s, episode steps: 72, steps per second: 171, episode reward: 0.819, mean reward: 0.011 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.844, 10.100], loss: 0.000022, mae: 0.003302, mean_q: 0.739986
 40861/100000: episode: 414, duration: 0.478s, episode steps: 73, steps per second: 153, episode reward: 0.780, mean reward: 0.011 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-0.517, 10.340], loss: 0.000021, mae: 0.003088, mean_q: 0.740332
 40935/100000: episode: 415, duration: 0.482s, episode steps: 74, steps per second: 154, episode reward: 0.795, mean reward: 0.011 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-1.340, 10.253], loss: 0.000013, mae: 0.003086, mean_q: 0.740321
 41007/100000: episode: 416, duration: 0.444s, episode steps: 72, steps per second: 162, episode reward: 0.731, mean reward: 0.010 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.723 [-1.689, 10.246], loss: 0.000027, mae: 0.004460, mean_q: 0.740730
 41081/100000: episode: 417, duration: 0.560s, episode steps: 74, steps per second: 132, episode reward: 0.723, mean reward: 0.010 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-1.363, 10.125], loss: 0.000022, mae: 0.003260, mean_q: 0.740600
 41155/100000: episode: 418, duration: 0.447s, episode steps: 74, steps per second: 166, episode reward: 0.723, mean reward: 0.010 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.748, 10.171], loss: 0.000016, mae: 0.003191, mean_q: 0.740586
 41229/100000: episode: 419, duration: 0.416s, episode steps: 74, steps per second: 178, episode reward: 0.718, mean reward: 0.010 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.791, 10.372], loss: 0.000017, mae: 0.002939, mean_q: 0.740944
 41303/100000: episode: 420, duration: 0.411s, episode steps: 74, steps per second: 180, episode reward: 0.689, mean reward: 0.009 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-1.065, 10.100], loss: 0.000020, mae: 0.003222, mean_q: 0.741093
 41376/100000: episode: 421, duration: 0.434s, episode steps: 73, steps per second: 168, episode reward: 0.778, mean reward: 0.011 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-1.115, 10.181], loss: 0.000026, mae: 0.003562, mean_q: 0.741051
 41450/100000: episode: 422, duration: 0.415s, episode steps: 74, steps per second: 178, episode reward: 0.785, mean reward: 0.011 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.065, 10.178], loss: 0.000015, mae: 0.002725, mean_q: 0.740327
 41524/100000: episode: 423, duration: 0.431s, episode steps: 74, steps per second: 172, episode reward: 0.737, mean reward: 0.010 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.670, 10.156], loss: 0.000029, mae: 0.003584, mean_q: 0.740820
 41597/100000: episode: 424, duration: 0.442s, episode steps: 73, steps per second: 165, episode reward: 0.722, mean reward: 0.010 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.373, 10.224], loss: 0.000019, mae: 0.003553, mean_q: 0.740835
 41672/100000: episode: 425, duration: 0.439s, episode steps: 75, steps per second: 171, episode reward: 0.724, mean reward: 0.010 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.026, 10.100], loss: 0.000015, mae: 0.003052, mean_q: 0.740654
 41745/100000: episode: 426, duration: 0.429s, episode steps: 73, steps per second: 170, episode reward: 0.687, mean reward: 0.009 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-1.337, 10.100], loss: 0.000017, mae: 0.002953, mean_q: 0.741039
 41817/100000: episode: 427, duration: 0.420s, episode steps: 72, steps per second: 171, episode reward: 0.781, mean reward: 0.011 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.865, 10.100], loss: 0.000018, mae: 0.003269, mean_q: 0.740794
 41891/100000: episode: 428, duration: 0.454s, episode steps: 74, steps per second: 163, episode reward: 0.771, mean reward: 0.010 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.701 [-1.271, 10.112], loss: 0.000011, mae: 0.002350, mean_q: 0.740616
 41963/100000: episode: 429, duration: 0.428s, episode steps: 72, steps per second: 168, episode reward: 0.718, mean reward: 0.010 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.734 [-1.316, 10.292], loss: 0.000013, mae: 0.002786, mean_q: 0.740616
 42035/100000: episode: 430, duration: 0.436s, episode steps: 72, steps per second: 165, episode reward: 0.688, mean reward: 0.010 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.542, 10.191], loss: 0.000024, mae: 0.003280, mean_q: 0.740830
 42109/100000: episode: 431, duration: 0.456s, episode steps: 74, steps per second: 162, episode reward: 0.785, mean reward: 0.011 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.043, 10.297], loss: 0.000014, mae: 0.002325, mean_q: 0.740825
 42182/100000: episode: 432, duration: 0.438s, episode steps: 73, steps per second: 167, episode reward: 0.851, mean reward: 0.012 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-1.437, 10.100], loss: 0.000018, mae: 0.002507, mean_q: 0.740737
 42256/100000: episode: 433, duration: 0.421s, episode steps: 74, steps per second: 176, episode reward: 0.797, mean reward: 0.011 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-0.465, 10.100], loss: 0.000010, mae: 0.002585, mean_q: 0.740450
 42330/100000: episode: 434, duration: 0.457s, episode steps: 74, steps per second: 162, episode reward: 0.736, mean reward: 0.010 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.843, 10.153], loss: 0.000020, mae: 0.003154, mean_q: 0.740636
 42404/100000: episode: 435, duration: 0.431s, episode steps: 74, steps per second: 172, episode reward: 0.664, mean reward: 0.009 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-1.547, 10.281], loss: 0.000019, mae: 0.003299, mean_q: 0.740564
 42478/100000: episode: 436, duration: 0.440s, episode steps: 74, steps per second: 168, episode reward: 0.720, mean reward: 0.010 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.701 [-1.055, 10.145], loss: 0.000015, mae: 0.002409, mean_q: 0.740565
 42550/100000: episode: 437, duration: 0.431s, episode steps: 72, steps per second: 167, episode reward: 0.683, mean reward: 0.009 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.790, 10.100], loss: 0.000012, mae: 0.002492, mean_q: 0.740600
 42623/100000: episode: 438, duration: 0.420s, episode steps: 73, steps per second: 174, episode reward: 0.663, mean reward: 0.009 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.710 [-0.550, 10.100], loss: 0.000020, mae: 0.002939, mean_q: 0.740683
 42695/100000: episode: 439, duration: 0.428s, episode steps: 72, steps per second: 168, episode reward: 0.867, mean reward: 0.012 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.731 [-0.964, 10.295], loss: 0.000013, mae: 0.002929, mean_q: 0.740635
 42767/100000: episode: 440, duration: 0.440s, episode steps: 72, steps per second: 164, episode reward: 0.813, mean reward: 0.011 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.667, 10.100], loss: 0.000018, mae: 0.003324, mean_q: 0.740655
 42839/100000: episode: 441, duration: 0.450s, episode steps: 72, steps per second: 160, episode reward: 0.777, mean reward: 0.011 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.692, 10.100], loss: 0.000020, mae: 0.003190, mean_q: 0.740546
 42912/100000: episode: 442, duration: 0.442s, episode steps: 73, steps per second: 165, episode reward: 0.692, mean reward: 0.009 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.833, 10.195], loss: 0.000025, mae: 0.003853, mean_q: 0.739898
 42986/100000: episode: 443, duration: 0.434s, episode steps: 74, steps per second: 170, episode reward: 0.739, mean reward: 0.010 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.754, 10.100], loss: 0.000019, mae: 0.003289, mean_q: 0.740331
 43060/100000: episode: 444, duration: 0.421s, episode steps: 74, steps per second: 176, episode reward: 0.674, mean reward: 0.009 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.298, 10.174], loss: 0.000020, mae: 0.002788, mean_q: 0.740050
 43132/100000: episode: 445, duration: 0.427s, episode steps: 72, steps per second: 169, episode reward: 0.699, mean reward: 0.010 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.269, 10.100], loss: 0.000017, mae: 0.002587, mean_q: 0.740402
 43205/100000: episode: 446, duration: 0.409s, episode steps: 73, steps per second: 179, episode reward: 0.828, mean reward: 0.011 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-1.374, 10.100], loss: 0.000027, mae: 0.003069, mean_q: 0.740422
 43279/100000: episode: 447, duration: 0.429s, episode steps: 74, steps per second: 172, episode reward: 0.757, mean reward: 0.010 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.938, 10.100], loss: 0.000030, mae: 0.003576, mean_q: 0.740475
 43354/100000: episode: 448, duration: 0.457s, episode steps: 75, steps per second: 164, episode reward: 0.787, mean reward: 0.010 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.692 [-0.253, 10.175], loss: 0.000027, mae: 0.003948, mean_q: 0.740799
 43428/100000: episode: 449, duration: 0.446s, episode steps: 74, steps per second: 166, episode reward: 0.798, mean reward: 0.011 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-1.038, 10.100], loss: 0.000021, mae: 0.002713, mean_q: 0.740771
 43502/100000: episode: 450, duration: 0.416s, episode steps: 74, steps per second: 178, episode reward: 0.866, mean reward: 0.012 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.321, 10.166], loss: 0.000014, mae: 0.002706, mean_q: 0.740380
 43575/100000: episode: 451, duration: 0.426s, episode steps: 73, steps per second: 171, episode reward: 0.770, mean reward: 0.011 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.558, 10.333], loss: 0.000028, mae: 0.004103, mean_q: 0.740416
 43649/100000: episode: 452, duration: 0.449s, episode steps: 74, steps per second: 165, episode reward: 0.698, mean reward: 0.009 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.243, 10.250], loss: 0.000030, mae: 0.003958, mean_q: 0.740558
 43721/100000: episode: 453, duration: 0.436s, episode steps: 72, steps per second: 165, episode reward: 0.715, mean reward: 0.010 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.874, 10.100], loss: 0.000029, mae: 0.003180, mean_q: 0.740742
 43794/100000: episode: 454, duration: 0.439s, episode steps: 73, steps per second: 166, episode reward: 0.696, mean reward: 0.010 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.799, 10.100], loss: 0.000025, mae: 0.003760, mean_q: 0.740238
 43866/100000: episode: 455, duration: 0.459s, episode steps: 72, steps per second: 157, episode reward: 0.803, mean reward: 0.011 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.718 [-0.433, 10.188], loss: 0.000032, mae: 0.004126, mean_q: 0.740494
 43940/100000: episode: 456, duration: 0.448s, episode steps: 74, steps per second: 165, episode reward: 0.696, mean reward: 0.009 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.917, 10.100], loss: 0.000026, mae: 0.003863, mean_q: 0.740206
 44014/100000: episode: 457, duration: 0.442s, episode steps: 74, steps per second: 167, episode reward: 0.725, mean reward: 0.010 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.703 [-1.410, 10.397], loss: 0.000026, mae: 0.003450, mean_q: 0.740626
 44086/100000: episode: 458, duration: 0.411s, episode steps: 72, steps per second: 175, episode reward: 0.796, mean reward: 0.011 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.512, 10.100], loss: 0.000023, mae: 0.002823, mean_q: 0.740377
 44158/100000: episode: 459, duration: 0.405s, episode steps: 72, steps per second: 178, episode reward: 0.722, mean reward: 0.010 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.725 [-0.677, 10.252], loss: 0.000039, mae: 0.005135, mean_q: 0.740296
 44232/100000: episode: 460, duration: 0.430s, episode steps: 74, steps per second: 172, episode reward: 0.740, mean reward: 0.010 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.273, 10.100], loss: 0.000026, mae: 0.002987, mean_q: 0.740567
 44306/100000: episode: 461, duration: 0.427s, episode steps: 74, steps per second: 173, episode reward: 0.680, mean reward: 0.009 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.707 [-0.282, 10.161], loss: 0.000036, mae: 0.004325, mean_q: 0.740745
 44380/100000: episode: 462, duration: 0.433s, episode steps: 74, steps per second: 171, episode reward: 0.695, mean reward: 0.009 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.304, 10.100], loss: 0.000028, mae: 0.003231, mean_q: 0.740999
 44453/100000: episode: 463, duration: 0.403s, episode steps: 73, steps per second: 181, episode reward: 0.742, mean reward: 0.010 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.892, 10.100], loss: 0.000029, mae: 0.003299, mean_q: 0.740970
 44525/100000: episode: 464, duration: 0.421s, episode steps: 72, steps per second: 171, episode reward: 0.745, mean reward: 0.010 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.943, 10.241], loss: 0.000029, mae: 0.003268, mean_q: 0.741066
 44599/100000: episode: 465, duration: 0.430s, episode steps: 74, steps per second: 172, episode reward: 0.734, mean reward: 0.010 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.708 [-0.672, 10.203], loss: 0.000035, mae: 0.003565, mean_q: 0.741734
 44674/100000: episode: 466, duration: 0.428s, episode steps: 75, steps per second: 175, episode reward: 0.738, mean reward: 0.010 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.686 [-0.288, 10.272], loss: 0.000015, mae: 0.002809, mean_q: 0.741625
 44749/100000: episode: 467, duration: 0.428s, episode steps: 75, steps per second: 175, episode reward: 0.850, mean reward: 0.011 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.970, 10.100], loss: 0.000024, mae: 0.002480, mean_q: 0.741840
 44823/100000: episode: 468, duration: 0.421s, episode steps: 74, steps per second: 176, episode reward: 0.728, mean reward: 0.010 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.246, 10.100], loss: 0.000028, mae: 0.003635, mean_q: 0.742083
 44896/100000: episode: 469, duration: 0.433s, episode steps: 73, steps per second: 169, episode reward: 0.766, mean reward: 0.010 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.517, 10.100], loss: 0.000017, mae: 0.002623, mean_q: 0.741980
 44968/100000: episode: 470, duration: 0.442s, episode steps: 72, steps per second: 163, episode reward: 0.729, mean reward: 0.010 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.185, 10.311], loss: 0.000014, mae: 0.002481, mean_q: 0.742183
 45043/100000: episode: 471, duration: 0.443s, episode steps: 75, steps per second: 169, episode reward: 0.771, mean reward: 0.010 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.682 [-1.370, 10.215], loss: 0.000018, mae: 0.002836, mean_q: 0.742443
 45117/100000: episode: 472, duration: 0.446s, episode steps: 74, steps per second: 166, episode reward: 0.715, mean reward: 0.010 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.703 [-1.747, 10.100], loss: 0.000030, mae: 0.003267, mean_q: 0.742342
 45192/100000: episode: 473, duration: 0.431s, episode steps: 75, steps per second: 174, episode reward: 0.715, mean reward: 0.010 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-1.178, 10.360], loss: 0.000024, mae: 0.003575, mean_q: 0.742094
 45266/100000: episode: 474, duration: 0.433s, episode steps: 74, steps per second: 171, episode reward: 0.855, mean reward: 0.012 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-1.290, 10.141], loss: 0.000031, mae: 0.003551, mean_q: 0.742693
 45340/100000: episode: 475, duration: 0.439s, episode steps: 74, steps per second: 168, episode reward: 0.739, mean reward: 0.010 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.383, 10.306], loss: 0.000030, mae: 0.003326, mean_q: 0.742873
 45415/100000: episode: 476, duration: 0.445s, episode steps: 75, steps per second: 169, episode reward: 0.709, mean reward: 0.009 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-1.204, 10.115], loss: 0.000025, mae: 0.003535, mean_q: 0.742929
 45490/100000: episode: 477, duration: 0.418s, episode steps: 75, steps per second: 179, episode reward: 0.766, mean reward: 0.010 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.674 [-0.655, 10.100], loss: 0.000018, mae: 0.002967, mean_q: 0.743241
 45564/100000: episode: 478, duration: 0.426s, episode steps: 74, steps per second: 174, episode reward: 0.878, mean reward: 0.012 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.342, 10.360], loss: 0.000023, mae: 0.002896, mean_q: 0.743625
 45638/100000: episode: 479, duration: 0.442s, episode steps: 74, steps per second: 167, episode reward: 0.841, mean reward: 0.011 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.713 [-0.201, 10.121], loss: 0.000040, mae: 0.005239, mean_q: 0.743815
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.769480288028717
1
 45712/100000: episode: 480, duration: 4.790s, episode steps: 74, steps per second: 15, episode reward: 0.776, mean reward: 0.010 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.454, 10.100], loss: 0.000018, mae: 0.002261, mean_q: 0.743697
 45813/100000: episode: 481, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.197, 10.208], loss: 0.000027, mae: 0.003333, mean_q: 0.743872
 45914/100000: episode: 482, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.100], loss: 0.000026, mae: 0.003107, mean_q: 0.744115
 46015/100000: episode: 483, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.572, 10.282], loss: 0.000025, mae: 0.002801, mean_q: 0.743942
 46116/100000: episode: 484, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.593, 10.174], loss: 0.000023, mae: 0.003504, mean_q: 0.744218
 46217/100000: episode: 485, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.884, 10.174], loss: 0.000015, mae: 0.003066, mean_q: 0.744265
 46318/100000: episode: 486, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.058, 10.100], loss: 0.000019, mae: 0.002408, mean_q: 0.744173
 46419/100000: episode: 487, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.198], loss: 0.000014, mae: 0.002295, mean_q: 0.744252
 46520/100000: episode: 488, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.335, 10.428], loss: 0.000023, mae: 0.002811, mean_q: 0.744084
 46621/100000: episode: 489, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.439, 10.143], loss: 0.000020, mae: 0.003006, mean_q: 0.743905
 46722/100000: episode: 490, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.368, 10.333], loss: 0.000035, mae: 0.004152, mean_q: 0.743941
 46823/100000: episode: 491, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.685, 10.100], loss: 0.000032, mae: 0.003914, mean_q: 0.744282
 46924/100000: episode: 492, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.026, 10.100], loss: 0.000020, mae: 0.002663, mean_q: 0.744017
 47025/100000: episode: 493, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.628, 10.158], loss: 0.000025, mae: 0.003008, mean_q: 0.744256
 47126/100000: episode: 494, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.102, 10.100], loss: 0.000022, mae: 0.002991, mean_q: 0.744753
 47227/100000: episode: 495, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.301, 10.175], loss: 0.000020, mae: 0.002786, mean_q: 0.744619
 47328/100000: episode: 496, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.795, 10.100], loss: 0.000022, mae: 0.003019, mean_q: 0.744498
 47429/100000: episode: 497, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.548, 10.116], loss: 0.000028, mae: 0.003529, mean_q: 0.744921
 47530/100000: episode: 498, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.899, 10.260], loss: 0.000027, mae: 0.003516, mean_q: 0.744637
 47631/100000: episode: 499, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.078, 10.100], loss: 0.000024, mae: 0.003128, mean_q: 0.745416
 47732/100000: episode: 500, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.269, 10.100], loss: 0.000024, mae: 0.003510, mean_q: 0.744794
 47833/100000: episode: 501, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.120, 10.243], loss: 0.000023, mae: 0.002711, mean_q: 0.745230
 47934/100000: episode: 502, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.333, 10.100], loss: 0.000025, mae: 0.003106, mean_q: 0.745449
 48035/100000: episode: 503, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.559, 10.102], loss: 0.000032, mae: 0.003490, mean_q: 0.745689
 48136/100000: episode: 504, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.263, 10.100], loss: 0.000025, mae: 0.003371, mean_q: 0.745842
 48237/100000: episode: 505, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.621, 10.149], loss: 0.000037, mae: 0.003932, mean_q: 0.745835
 48338/100000: episode: 506, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.007, 10.213], loss: 0.000027, mae: 0.003524, mean_q: 0.745880
 48439/100000: episode: 507, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.939, 10.332], loss: 0.000021, mae: 0.002500, mean_q: 0.746297
 48540/100000: episode: 508, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.790, 10.379], loss: 0.000018, mae: 0.002388, mean_q: 0.745981
 48641/100000: episode: 509, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.329, 10.100], loss: 0.000015, mae: 0.002588, mean_q: 0.746064
 48742/100000: episode: 510, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.100], loss: 0.000016, mae: 0.002563, mean_q: 0.745858
 48843/100000: episode: 511, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.379, 10.100], loss: 0.000020, mae: 0.003132, mean_q: 0.745961
 48944/100000: episode: 512, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.863, 10.341], loss: 0.000009, mae: 0.001902, mean_q: 0.745843
 49045/100000: episode: 513, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.717, 10.219], loss: 0.000015, mae: 0.002224, mean_q: 0.746084
 49146/100000: episode: 514, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.795, 10.216], loss: 0.000019, mae: 0.003082, mean_q: 0.746316
 49247/100000: episode: 515, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.017, 10.124], loss: 0.000015, mae: 0.003018, mean_q: 0.746316
 49348/100000: episode: 516, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.758, 10.100], loss: 0.000015, mae: 0.002482, mean_q: 0.746545
 49449/100000: episode: 517, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.662, 10.100], loss: 0.000024, mae: 0.002842, mean_q: 0.746702
 49550/100000: episode: 518, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.700, 10.151], loss: 0.000024, mae: 0.003641, mean_q: 0.745832
 49651/100000: episode: 519, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.254, 10.396], loss: 0.000016, mae: 0.002486, mean_q: 0.745817
 49752/100000: episode: 520, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.400, 10.243], loss: 0.000015, mae: 0.002453, mean_q: 0.745729
 49853/100000: episode: 521, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.640, 10.380], loss: 0.000014, mae: 0.002894, mean_q: 0.745978
 49954/100000: episode: 522, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.779, 10.100], loss: 0.000005, mae: 0.001534, mean_q: 0.745829
 50055/100000: episode: 523, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.465, 10.100], loss: 0.000014, mae: 0.003038, mean_q: 0.745601
 50156/100000: episode: 524, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.987, 10.221], loss: 0.000014, mae: 0.002414, mean_q: 0.745660
 50257/100000: episode: 525, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.100], loss: 0.000019, mae: 0.003178, mean_q: 0.745644
 50358/100000: episode: 526, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.414, 10.398], loss: 0.000016, mae: 0.003233, mean_q: 0.745476
 50459/100000: episode: 527, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.271, 10.100], loss: 0.000010, mae: 0.001944, mean_q: 0.745326
 50560/100000: episode: 528, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.213, 10.100], loss: 0.000011, mae: 0.002240, mean_q: 0.745397
 50661/100000: episode: 529, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.205], loss: 0.000012, mae: 0.002790, mean_q: 0.745490
 50762/100000: episode: 530, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.541, 10.100], loss: 0.000010, mae: 0.002375, mean_q: 0.745473
 50863/100000: episode: 531, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.817, 10.411], loss: 0.000013, mae: 0.002222, mean_q: 0.745493
 50964/100000: episode: 532, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.300, 10.243], loss: 0.000015, mae: 0.003019, mean_q: 0.745643
 51065/100000: episode: 533, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.906, 10.100], loss: 0.000011, mae: 0.002148, mean_q: 0.745702
 51166/100000: episode: 534, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.558, 10.210], loss: 0.000013, mae: 0.002532, mean_q: 0.745331
 51267/100000: episode: 535, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.649, 10.183], loss: 0.000021, mae: 0.002997, mean_q: 0.745904
 51368/100000: episode: 536, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.167, 10.268], loss: 0.000010, mae: 0.002394, mean_q: 0.745797
 51469/100000: episode: 537, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.304, 10.366], loss: 0.000015, mae: 0.002783, mean_q: 0.746280
 51570/100000: episode: 538, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.743, 10.422], loss: 0.000016, mae: 0.002875, mean_q: 0.745979
 51671/100000: episode: 539, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.631, 10.269], loss: 0.000008, mae: 0.002064, mean_q: 0.746016
 51772/100000: episode: 540, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.598, 10.445], loss: 0.000012, mae: 0.002549, mean_q: 0.746013
 51873/100000: episode: 541, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.100], loss: 0.000016, mae: 0.002670, mean_q: 0.746293
 51974/100000: episode: 542, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.977, 10.162], loss: 0.000010, mae: 0.002080, mean_q: 0.745981
 52075/100000: episode: 543, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.511, 10.450], loss: 0.000022, mae: 0.003680, mean_q: 0.745737
 52176/100000: episode: 544, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.100], loss: 0.000012, mae: 0.002250, mean_q: 0.745905
 52277/100000: episode: 545, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.979, 10.227], loss: 0.000009, mae: 0.002012, mean_q: 0.745984
 52378/100000: episode: 546, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.947, 10.100], loss: 0.000014, mae: 0.002748, mean_q: 0.746096
 52479/100000: episode: 547, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.253, 10.370], loss: 0.000011, mae: 0.002329, mean_q: 0.745844
 52580/100000: episode: 548, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.200], loss: 0.000015, mae: 0.002512, mean_q: 0.745814
 52681/100000: episode: 549, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.100], loss: 0.000010, mae: 0.002371, mean_q: 0.746247
 52782/100000: episode: 550, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.595, 10.100], loss: 0.000013, mae: 0.002318, mean_q: 0.746073
 52883/100000: episode: 551, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.184, 10.100], loss: 0.000009, mae: 0.002211, mean_q: 0.746155
 52984/100000: episode: 552, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.727, 10.100], loss: 0.000017, mae: 0.003481, mean_q: 0.746342
 53085/100000: episode: 553, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.798, 10.116], loss: 0.000012, mae: 0.002564, mean_q: 0.746225
 53186/100000: episode: 554, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.696, 10.184], loss: 0.000008, mae: 0.001856, mean_q: 0.746221
 53287/100000: episode: 555, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.808, 10.231], loss: 0.000011, mae: 0.002068, mean_q: 0.746228
 53388/100000: episode: 556, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.206, 10.236], loss: 0.000007, mae: 0.001638, mean_q: 0.746301
 53489/100000: episode: 557, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.097, 10.437], loss: 0.000011, mae: 0.002047, mean_q: 0.746329
 53590/100000: episode: 558, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.121, 10.100], loss: 0.000016, mae: 0.002363, mean_q: 0.746036
 53691/100000: episode: 559, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.220, 10.372], loss: 0.000007, mae: 0.001659, mean_q: 0.746058
 53792/100000: episode: 560, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.100], loss: 0.000010, mae: 0.001959, mean_q: 0.746259
 53893/100000: episode: 561, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.100], loss: 0.000010, mae: 0.001953, mean_q: 0.746091
 53994/100000: episode: 562, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.693, 10.219], loss: 0.000010, mae: 0.001775, mean_q: 0.746310
 54095/100000: episode: 563, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.843, 10.153], loss: 0.000013, mae: 0.002571, mean_q: 0.746248
 54196/100000: episode: 564, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.380, 10.395], loss: 0.000014, mae: 0.002564, mean_q: 0.746380
 54297/100000: episode: 565, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.885, 10.299], loss: 0.000014, mae: 0.002892, mean_q: 0.746216
 54398/100000: episode: 566, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.954, 10.100], loss: 0.000016, mae: 0.002839, mean_q: 0.746271
 54499/100000: episode: 567, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.337, 10.326], loss: 0.000014, mae: 0.002440, mean_q: 0.746168
 54600/100000: episode: 568, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.701, 10.176], loss: 0.000010, mae: 0.002069, mean_q: 0.746091
 54701/100000: episode: 569, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.643, 10.119], loss: 0.000009, mae: 0.001732, mean_q: 0.746044
 54802/100000: episode: 570, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.643, 10.188], loss: 0.000015, mae: 0.002425, mean_q: 0.745798
 54903/100000: episode: 571, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.560, 10.218], loss: 0.000013, mae: 0.002303, mean_q: 0.745751
 55004/100000: episode: 572, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.355, 10.100], loss: 0.000007, mae: 0.001880, mean_q: 0.745882
 55105/100000: episode: 573, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.622, 10.339], loss: 0.000007, mae: 0.002035, mean_q: 0.745725
 55206/100000: episode: 574, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.907, 10.206], loss: 0.000016, mae: 0.002346, mean_q: 0.745827
 55307/100000: episode: 575, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.657, 10.290], loss: 0.000015, mae: 0.002664, mean_q: 0.745533
 55408/100000: episode: 576, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.693, 10.100], loss: 0.000008, mae: 0.001716, mean_q: 0.745591
 55509/100000: episode: 577, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.657, mean reward: 0.007 [0.000, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.491, 10.106], loss: 0.000018, mae: 0.002778, mean_q: 0.745571
 55610/100000: episode: 578, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.866, 10.382], loss: 0.000008, mae: 0.001735, mean_q: 0.745342
 55711/100000: episode: 579, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.100], loss: 0.000007, mae: 0.001683, mean_q: 0.745277
[Info] 1-TH LEVEL FOUND: 0.7564342617988586, Considering 10/100 traces
 55812/100000: episode: 580, duration: 5.191s, episode steps: 101, steps per second: 19, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.730, 10.100], loss: 0.000015, mae: 0.002699, mean_q: 0.745274
 55906/100000: episode: 581, duration: 0.535s, episode steps: 94, steps per second: 176, episode reward: 0.706, mean reward: 0.008 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.628, 10.112], loss: 0.000010, mae: 0.001828, mean_q: 0.745161
 55994/100000: episode: 582, duration: 0.507s, episode steps: 88, steps per second: 174, episode reward: 0.782, mean reward: 0.009 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.572 [-1.386, 10.463], loss: 0.000013, mae: 0.002041, mean_q: 0.745243
 56088/100000: episode: 583, duration: 0.537s, episode steps: 94, steps per second: 175, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-1.091, 10.211], loss: 0.000007, mae: 0.001584, mean_q: 0.745309
 56183/100000: episode: 584, duration: 0.523s, episode steps: 95, steps per second: 182, episode reward: 0.740, mean reward: 0.008 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.997, 10.100], loss: 0.000007, mae: 0.001840, mean_q: 0.745297
 56278/100000: episode: 585, duration: 0.559s, episode steps: 95, steps per second: 170, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.864, 10.100], loss: 0.000009, mae: 0.002193, mean_q: 0.745035
 56373/100000: episode: 586, duration: 0.550s, episode steps: 95, steps per second: 173, episode reward: 0.752, mean reward: 0.008 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.774, 10.100], loss: 0.000005, mae: 0.001516, mean_q: 0.744991
 56467/100000: episode: 587, duration: 0.565s, episode steps: 94, steps per second: 166, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.013, 10.326], loss: 0.000011, mae: 0.002503, mean_q: 0.744907
 56561/100000: episode: 588, duration: 0.554s, episode steps: 94, steps per second: 170, episode reward: 0.753, mean reward: 0.008 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.928, 10.100], loss: 0.000012, mae: 0.002493, mean_q: 0.744861
 56656/100000: episode: 589, duration: 0.587s, episode steps: 95, steps per second: 162, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.324, 10.170], loss: 0.000014, mae: 0.002666, mean_q: 0.744673
 56750/100000: episode: 590, duration: 0.585s, episode steps: 94, steps per second: 161, episode reward: 0.714, mean reward: 0.008 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.342, 10.100], loss: 0.000009, mae: 0.001876, mean_q: 0.744836
 56845/100000: episode: 591, duration: 0.600s, episode steps: 95, steps per second: 158, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.307, 10.100], loss: 0.000008, mae: 0.001964, mean_q: 0.744570
 56940/100000: episode: 592, duration: 0.569s, episode steps: 95, steps per second: 167, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.592, 10.571], loss: 0.000012, mae: 0.002218, mean_q: 0.744617
 57034/100000: episode: 593, duration: 0.550s, episode steps: 94, steps per second: 171, episode reward: 0.896, mean reward: 0.010 [0.000, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.681, 10.274], loss: 0.000010, mae: 0.002048, mean_q: 0.744608
 57124/100000: episode: 594, duration: 0.526s, episode steps: 90, steps per second: 171, episode reward: 0.808, mean reward: 0.009 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.549 [-2.109, 10.264], loss: 0.000024, mae: 0.002565, mean_q: 0.744758
 57219/100000: episode: 595, duration: 0.554s, episode steps: 95, steps per second: 171, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.103, 10.100], loss: 0.000016, mae: 0.002535, mean_q: 0.744561
 57314/100000: episode: 596, duration: 0.550s, episode steps: 95, steps per second: 173, episode reward: 0.738, mean reward: 0.008 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.133, 10.326], loss: 0.000007, mae: 0.001589, mean_q: 0.744812
 57404/100000: episode: 597, duration: 0.523s, episode steps: 90, steps per second: 172, episode reward: 0.756, mean reward: 0.008 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.063, 10.100], loss: 0.000011, mae: 0.002283, mean_q: 0.744763
 57498/100000: episode: 598, duration: 0.522s, episode steps: 94, steps per second: 180, episode reward: 0.848, mean reward: 0.009 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-1.720, 10.335], loss: 0.000018, mae: 0.002967, mean_q: 0.744761
 57593/100000: episode: 599, duration: 0.577s, episode steps: 95, steps per second: 165, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.868, 10.147], loss: 0.000016, mae: 0.002843, mean_q: 0.744928
 57683/100000: episode: 600, duration: 0.533s, episode steps: 90, steps per second: 169, episode reward: 0.679, mean reward: 0.008 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.252, 10.100], loss: 0.000016, mae: 0.002205, mean_q: 0.744917
 57778/100000: episode: 601, duration: 0.543s, episode steps: 95, steps per second: 175, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.501, 10.100], loss: 0.000013, mae: 0.002354, mean_q: 0.744651
 57873/100000: episode: 602, duration: 0.534s, episode steps: 95, steps per second: 178, episode reward: 0.815, mean reward: 0.009 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.905, 10.100], loss: 0.000014, mae: 0.001941, mean_q: 0.745111
 57968/100000: episode: 603, duration: 0.566s, episode steps: 95, steps per second: 168, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.949, 10.187], loss: 0.000032, mae: 0.003554, mean_q: 0.744710
 58063/100000: episode: 604, duration: 0.558s, episode steps: 95, steps per second: 170, episode reward: 0.739, mean reward: 0.008 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.666, 10.237], loss: 0.000024, mae: 0.003015, mean_q: 0.744809
 58153/100000: episode: 605, duration: 0.539s, episode steps: 90, steps per second: 167, episode reward: 0.744, mean reward: 0.008 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.450, 10.157], loss: 0.000012, mae: 0.002229, mean_q: 0.744856
 58248/100000: episode: 606, duration: 0.571s, episode steps: 95, steps per second: 166, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.487, 10.100], loss: 0.000015, mae: 0.002432, mean_q: 0.745109
 58336/100000: episode: 607, duration: 0.521s, episode steps: 88, steps per second: 169, episode reward: 0.752, mean reward: 0.009 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-1.063, 10.100], loss: 0.000019, mae: 0.002376, mean_q: 0.745060
 58430/100000: episode: 608, duration: 0.549s, episode steps: 94, steps per second: 171, episode reward: 0.734, mean reward: 0.008 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.515, 10.155], loss: 0.000012, mae: 0.002492, mean_q: 0.744692
 58524/100000: episode: 609, duration: 0.541s, episode steps: 94, steps per second: 174, episode reward: 0.720, mean reward: 0.008 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.462, 10.100], loss: 0.000018, mae: 0.003258, mean_q: 0.744615
 58618/100000: episode: 610, duration: 0.565s, episode steps: 94, steps per second: 166, episode reward: 0.724, mean reward: 0.008 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.933, 10.324], loss: 0.000015, mae: 0.002208, mean_q: 0.744608
 58706/100000: episode: 611, duration: 0.525s, episode steps: 88, steps per second: 168, episode reward: 0.735, mean reward: 0.008 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.570 [-0.278, 10.100], loss: 0.000021, mae: 0.002504, mean_q: 0.744907
 58801/100000: episode: 612, duration: 0.559s, episode steps: 95, steps per second: 170, episode reward: 0.811, mean reward: 0.009 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.479, 10.128], loss: 0.000011, mae: 0.001701, mean_q: 0.744617
 58896/100000: episode: 613, duration: 0.595s, episode steps: 95, steps per second: 160, episode reward: 0.752, mean reward: 0.008 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-2.016, 10.297], loss: 0.000013, mae: 0.002171, mean_q: 0.744754
 58991/100000: episode: 614, duration: 0.605s, episode steps: 95, steps per second: 157, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-1.391, 10.100], loss: 0.000014, mae: 0.002040, mean_q: 0.744522
 59085/100000: episode: 615, duration: 0.598s, episode steps: 94, steps per second: 157, episode reward: 0.733, mean reward: 0.008 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-1.271, 10.140], loss: 0.000011, mae: 0.001945, mean_q: 0.744456
 59180/100000: episode: 616, duration: 0.584s, episode steps: 95, steps per second: 163, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.652, 10.146], loss: 0.000010, mae: 0.002155, mean_q: 0.744464
 59274/100000: episode: 617, duration: 0.546s, episode steps: 94, steps per second: 172, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.875, 10.100], loss: 0.000013, mae: 0.002456, mean_q: 0.744519
 59364/100000: episode: 618, duration: 0.577s, episode steps: 90, steps per second: 156, episode reward: 0.775, mean reward: 0.009 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.558 [-0.629, 10.112], loss: 0.000013, mae: 0.001860, mean_q: 0.744224
 59454/100000: episode: 619, duration: 1.095s, episode steps: 90, steps per second: 82, episode reward: 0.720, mean reward: 0.008 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.423, 10.100], loss: 0.000019, mae: 0.002596, mean_q: 0.744637
 59548/100000: episode: 620, duration: 0.945s, episode steps: 94, steps per second: 99, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.619, 10.309], loss: 0.000013, mae: 0.001871, mean_q: 0.744501
 59642/100000: episode: 621, duration: 0.763s, episode steps: 94, steps per second: 123, episode reward: 0.711, mean reward: 0.008 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.856, 10.100], loss: 0.000016, mae: 0.002391, mean_q: 0.744198
 59730/100000: episode: 622, duration: 0.587s, episode steps: 88, steps per second: 150, episode reward: 0.758, mean reward: 0.009 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.569 [-0.610, 10.321], loss: 0.000018, mae: 0.002525, mean_q: 0.744166
 59818/100000: episode: 623, duration: 0.688s, episode steps: 88, steps per second: 128, episode reward: 0.728, mean reward: 0.008 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.558 [-0.835, 10.224], loss: 0.000015, mae: 0.002459, mean_q: 0.744089
 59913/100000: episode: 624, duration: 0.909s, episode steps: 95, steps per second: 104, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.557, 10.104], loss: 0.000020, mae: 0.002555, mean_q: 0.744189
 60008/100000: episode: 625, duration: 0.980s, episode steps: 95, steps per second: 97, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.412, 10.100], loss: 0.000020, mae: 0.002533, mean_q: 0.744181
 60096/100000: episode: 626, duration: 0.886s, episode steps: 88, steps per second: 99, episode reward: 0.756, mean reward: 0.009 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.575 [-0.840, 10.100], loss: 0.000013, mae: 0.001878, mean_q: 0.744239
 60186/100000: episode: 627, duration: 0.520s, episode steps: 90, steps per second: 173, episode reward: 0.676, mean reward: 0.008 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.689, 10.197], loss: 0.000015, mae: 0.002795, mean_q: 0.743990
 60276/100000: episode: 628, duration: 0.662s, episode steps: 90, steps per second: 136, episode reward: 0.706, mean reward: 0.008 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.560 [-0.569, 10.283], loss: 0.000012, mae: 0.001922, mean_q: 0.744156
 60370/100000: episode: 629, duration: 0.774s, episode steps: 94, steps per second: 121, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.298, 10.100], loss: 0.000017, mae: 0.002619, mean_q: 0.743878
 60465/100000: episode: 630, duration: 0.641s, episode steps: 95, steps per second: 148, episode reward: 0.740, mean reward: 0.008 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.159, 10.100], loss: 0.000023, mae: 0.002402, mean_q: 0.744131
 60559/100000: episode: 631, duration: 0.604s, episode steps: 94, steps per second: 156, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.748, 10.130], loss: 0.000017, mae: 0.003032, mean_q: 0.744094
 60653/100000: episode: 632, duration: 0.565s, episode steps: 94, steps per second: 166, episode reward: 0.725, mean reward: 0.008 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.841, 10.100], loss: 0.000020, mae: 0.002424, mean_q: 0.744112
 60747/100000: episode: 633, duration: 0.527s, episode steps: 94, steps per second: 178, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-2.395, 10.127], loss: 0.000012, mae: 0.002080, mean_q: 0.743949
 60837/100000: episode: 634, duration: 0.522s, episode steps: 90, steps per second: 172, episode reward: 0.777, mean reward: 0.009 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.403, 10.111], loss: 0.000018, mae: 0.002068, mean_q: 0.743833
 60932/100000: episode: 635, duration: 0.547s, episode steps: 95, steps per second: 174, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.183, 10.100], loss: 0.000011, mae: 0.002046, mean_q: 0.743772
 61022/100000: episode: 636, duration: 0.556s, episode steps: 90, steps per second: 162, episode reward: 0.785, mean reward: 0.009 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.549 [-0.682, 10.100], loss: 0.000025, mae: 0.003050, mean_q: 0.743970
 61117/100000: episode: 637, duration: 0.587s, episode steps: 95, steps per second: 162, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.433, 10.100], loss: 0.000021, mae: 0.002808, mean_q: 0.743892
 61212/100000: episode: 638, duration: 0.592s, episode steps: 95, steps per second: 160, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.122, 10.100], loss: 0.000017, mae: 0.002275, mean_q: 0.743945
 61307/100000: episode: 639, duration: 0.612s, episode steps: 95, steps per second: 155, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.991, 10.100], loss: 0.000007, mae: 0.001470, mean_q: 0.743707
 61401/100000: episode: 640, duration: 0.579s, episode steps: 94, steps per second: 162, episode reward: 0.715, mean reward: 0.008 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.623, 10.100], loss: 0.000010, mae: 0.001459, mean_q: 0.743931
 61491/100000: episode: 641, duration: 0.587s, episode steps: 90, steps per second: 153, episode reward: 0.717, mean reward: 0.008 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.786, 10.100], loss: 0.000010, mae: 0.001524, mean_q: 0.743708
 61579/100000: episode: 642, duration: 0.533s, episode steps: 88, steps per second: 165, episode reward: 0.722, mean reward: 0.008 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.862, 10.100], loss: 0.000024, mae: 0.003069, mean_q: 0.743710
 61673/100000: episode: 643, duration: 0.550s, episode steps: 94, steps per second: 171, episode reward: 0.653, mean reward: 0.007 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.896, 10.100], loss: 0.000012, mae: 0.001718, mean_q: 0.743693
 61767/100000: episode: 644, duration: 0.601s, episode steps: 94, steps per second: 156, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.361, 10.100], loss: 0.000004, mae: 0.001164, mean_q: 0.743700
 61862/100000: episode: 645, duration: 0.583s, episode steps: 95, steps per second: 163, episode reward: 0.725, mean reward: 0.008 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.170, 10.100], loss: 0.000018, mae: 0.002197, mean_q: 0.743775
 61952/100000: episode: 646, duration: 0.628s, episode steps: 90, steps per second: 143, episode reward: 0.722, mean reward: 0.008 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-1.225, 10.100], loss: 0.000015, mae: 0.001893, mean_q: 0.743493
 62047/100000: episode: 647, duration: 0.700s, episode steps: 95, steps per second: 136, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.524, 10.100], loss: 0.000015, mae: 0.002115, mean_q: 0.743298
 62135/100000: episode: 648, duration: 0.532s, episode steps: 88, steps per second: 165, episode reward: 0.762, mean reward: 0.009 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.575 [-0.324, 10.274], loss: 0.000009, mae: 0.001757, mean_q: 0.743126
 62229/100000: episode: 649, duration: 0.567s, episode steps: 94, steps per second: 166, episode reward: 0.822, mean reward: 0.009 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.852, 10.102], loss: 0.000007, mae: 0.001756, mean_q: 0.743008
 62323/100000: episode: 650, duration: 0.534s, episode steps: 94, steps per second: 176, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.438, 10.100], loss: 0.000010, mae: 0.001850, mean_q: 0.743060
 62411/100000: episode: 651, duration: 0.501s, episode steps: 88, steps per second: 176, episode reward: 0.790, mean reward: 0.009 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-1.109, 10.158], loss: 0.000012, mae: 0.001475, mean_q: 0.742959
 62506/100000: episode: 652, duration: 0.548s, episode steps: 95, steps per second: 173, episode reward: 0.713, mean reward: 0.008 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.240, 10.242], loss: 0.000009, mae: 0.002018, mean_q: 0.742884
 62601/100000: episode: 653, duration: 0.570s, episode steps: 95, steps per second: 167, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.143, 10.130], loss: 0.000006, mae: 0.001409, mean_q: 0.742954
 62695/100000: episode: 654, duration: 0.552s, episode steps: 94, steps per second: 170, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.841, 10.100], loss: 0.000017, mae: 0.002623, mean_q: 0.742442
 62789/100000: episode: 655, duration: 0.595s, episode steps: 94, steps per second: 158, episode reward: 0.749, mean reward: 0.008 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-1.620, 10.298], loss: 0.000010, mae: 0.002187, mean_q: 0.742613
 62884/100000: episode: 656, duration: 0.591s, episode steps: 95, steps per second: 161, episode reward: 0.727, mean reward: 0.008 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.305, 10.100], loss: 0.000007, mae: 0.001718, mean_q: 0.742347
 62978/100000: episode: 657, duration: 0.562s, episode steps: 94, steps per second: 167, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.281, 10.100], loss: 0.000008, mae: 0.001532, mean_q: 0.742148
 63072/100000: episode: 658, duration: 0.577s, episode steps: 94, steps per second: 163, episode reward: 0.753, mean reward: 0.008 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.810, 10.204], loss: 0.000012, mae: 0.002231, mean_q: 0.742088
 63160/100000: episode: 659, duration: 0.533s, episode steps: 88, steps per second: 165, episode reward: 0.712, mean reward: 0.008 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.569 [-0.791, 10.100], loss: 0.000008, mae: 0.002200, mean_q: 0.741969
 63255/100000: episode: 660, duration: 0.730s, episode steps: 95, steps per second: 130, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.070, 10.298], loss: 0.000005, mae: 0.001164, mean_q: 0.742091
 63345/100000: episode: 661, duration: 0.550s, episode steps: 90, steps per second: 164, episode reward: 0.712, mean reward: 0.008 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.521, 10.245], loss: 0.000009, mae: 0.001853, mean_q: 0.741800
 63440/100000: episode: 662, duration: 0.566s, episode steps: 95, steps per second: 168, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.594, 10.356], loss: 0.000009, mae: 0.001651, mean_q: 0.741950
 63535/100000: episode: 663, duration: 0.588s, episode steps: 95, steps per second: 162, episode reward: 0.811, mean reward: 0.009 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.164, 10.131], loss: 0.000010, mae: 0.001542, mean_q: 0.741690
 63630/100000: episode: 664, duration: 0.638s, episode steps: 95, steps per second: 149, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.821, 10.100], loss: 0.000010, mae: 0.001847, mean_q: 0.741914
 63720/100000: episode: 665, duration: 0.537s, episode steps: 90, steps per second: 168, episode reward: 0.744, mean reward: 0.008 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-1.316, 10.148], loss: 0.000011, mae: 0.002071, mean_q: 0.741738
 63810/100000: episode: 666, duration: 0.528s, episode steps: 90, steps per second: 171, episode reward: 0.716, mean reward: 0.008 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-1.032, 10.181], loss: 0.000005, mae: 0.001244, mean_q: 0.741656
 63904/100000: episode: 667, duration: 0.548s, episode steps: 94, steps per second: 172, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.427, 10.100], loss: 0.000007, mae: 0.001501, mean_q: 0.741550
 63998/100000: episode: 668, duration: 0.594s, episode steps: 94, steps per second: 158, episode reward: 0.727, mean reward: 0.008 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-1.282, 10.100], loss: 0.000014, mae: 0.002115, mean_q: 0.741479
 64093/100000: episode: 669, duration: 0.534s, episode steps: 95, steps per second: 178, episode reward: 0.721, mean reward: 0.008 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.787, 10.100], loss: 0.000010, mae: 0.002079, mean_q: 0.741573
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7564342617988586
1
 64187/100000: episode: 670, duration: 5.396s, episode steps: 94, steps per second: 17, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.827, 10.100], loss: 0.000010, mae: 0.001593, mean_q: 0.741408
 64288/100000: episode: 671, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.277, 10.259], loss: 0.000008, mae: 0.001657, mean_q: 0.741355
 64389/100000: episode: 672, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.280, 10.120], loss: 0.000009, mae: 0.001519, mean_q: 0.741316
 64490/100000: episode: 673, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.894, 10.326], loss: 0.000010, mae: 0.001709, mean_q: 0.741242
 64591/100000: episode: 674, duration: 0.654s, episode steps: 101, steps per second: 154, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.154, 10.100], loss: 0.000010, mae: 0.002057, mean_q: 0.741015
 64692/100000: episode: 675, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.500, 10.217], loss: 0.000010, mae: 0.001909, mean_q: 0.740704
 64793/100000: episode: 676, duration: 0.881s, episode steps: 101, steps per second: 115, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.289, 10.287], loss: 0.000011, mae: 0.001982, mean_q: 0.740932
 64894/100000: episode: 677, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.053, 10.100], loss: 0.000014, mae: 0.002004, mean_q: 0.740935
 64995/100000: episode: 678, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.265, 10.284], loss: 0.000012, mae: 0.002141, mean_q: 0.740648
 65096/100000: episode: 679, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.885, 10.100], loss: 0.000010, mae: 0.001735, mean_q: 0.740732
 65197/100000: episode: 680, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.664, 10.384], loss: 0.000009, mae: 0.001981, mean_q: 0.740705
 65298/100000: episode: 681, duration: 0.749s, episode steps: 101, steps per second: 135, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.460, 10.107], loss: 0.000009, mae: 0.001591, mean_q: 0.740686
 65399/100000: episode: 682, duration: 0.785s, episode steps: 101, steps per second: 129, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.869, 10.100], loss: 0.000007, mae: 0.001480, mean_q: 0.740482
 65500/100000: episode: 683, duration: 1.014s, episode steps: 101, steps per second: 100, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.223], loss: 0.000007, mae: 0.001462, mean_q: 0.740454
 65601/100000: episode: 684, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.160, 10.100], loss: 0.000012, mae: 0.001997, mean_q: 0.740512
done, took 402.415 seconds
[Info] End Importance Splitting. Falsification occurred 0 times.
