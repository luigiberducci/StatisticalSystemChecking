Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
_________________________________________________________________
dense_4 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,777
Trainable params: 1,777
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_15_49_27_CET/1, out/localization/mer27nov2019_15_49_27_CET/1/levels, out/localization/mer27nov2019_15_49_27_CET/1/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.191s, episode steps: 101, steps per second: 528, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.837, 10.208], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.068s, episode steps: 101, steps per second: 1495, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.098s, episode steps: 101, steps per second: 1029, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.256, 10.241], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.105s, episode steps: 101, steps per second: 966, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.571, 10.302], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.084s, episode steps: 101, steps per second: 1203, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.718, 10.117], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.108s, episode steps: 101, steps per second: 935, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.978, 10.100], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.116s, episode steps: 101, steps per second: 872, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.866, 10.100], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.088s, episode steps: 101, steps per second: 1150, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.166, 10.100], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.085s, episode steps: 101, steps per second: 1187, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.696, 10.302], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.078s, episode steps: 101, steps per second: 1288, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.388, 10.269], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.069s, episode steps: 101, steps per second: 1470, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.957, 10.129], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.082s, episode steps: 101, steps per second: 1235, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.826, 10.319], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.081s, episode steps: 101, steps per second: 1243, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.987, 10.234], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.069s, episode steps: 101, steps per second: 1464, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.539, 10.322], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.073s, episode steps: 101, steps per second: 1384, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.720, 10.317], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.069s, episode steps: 101, steps per second: 1465, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.541, 10.123], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.069s, episode steps: 101, steps per second: 1472, episode reward: 0.902, mean reward: 0.009 [0.000, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.725, 10.100], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.069s, episode steps: 101, steps per second: 1464, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.641, 10.202], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.069s, episode steps: 101, steps per second: 1473, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.685, 10.409], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.069s, episode steps: 101, steps per second: 1473, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.307, 10.100], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.071s, episode steps: 101, steps per second: 1416, episode reward: 0.856, mean reward: 0.008 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.718, 10.360], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.070s, episode steps: 101, steps per second: 1445, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.289, 10.181], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.077s, episode steps: 101, steps per second: 1317, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.652, 10.204], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.068s, episode steps: 101, steps per second: 1475, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.746, 10.100], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.069s, episode steps: 101, steps per second: 1459, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.118, 10.100], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.079s, episode steps: 101, steps per second: 1280, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.490, 10.200], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.076s, episode steps: 101, steps per second: 1329, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.739, 10.109], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.069s, episode steps: 101, steps per second: 1457, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.833, 10.100], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.069s, episode steps: 101, steps per second: 1468, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.916, 10.100], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.073s, episode steps: 101, steps per second: 1390, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.713, 10.144], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.081s, episode steps: 101, steps per second: 1241, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.289, 10.200], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.072s, episode steps: 101, steps per second: 1400, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.340, 10.100], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.081s, episode steps: 101, steps per second: 1247, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.413, 10.200], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.071s, episode steps: 101, steps per second: 1431, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.434, 10.480], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.070s, episode steps: 101, steps per second: 1433, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.501, 10.100], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.081s, episode steps: 101, steps per second: 1248, episode reward: 0.656, mean reward: 0.006 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.385, 10.184], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.069s, episode steps: 101, steps per second: 1454, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.100], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.072s, episode steps: 101, steps per second: 1397, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.382, 10.100], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.078s, episode steps: 101, steps per second: 1292, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.229, 10.100], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.069s, episode steps: 101, steps per second: 1461, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.713, 10.311], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.069s, episode steps: 101, steps per second: 1466, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.862, 10.260], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.070s, episode steps: 101, steps per second: 1451, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.322, 10.185], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.077s, episode steps: 101, steps per second: 1320, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.147], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.081s, episode steps: 101, steps per second: 1246, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.577, 10.246], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.069s, episode steps: 101, steps per second: 1473, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.651, 10.100], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.080s, episode steps: 101, steps per second: 1260, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.033, 10.269], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.070s, episode steps: 101, steps per second: 1445, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.249, 10.100], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.076s, episode steps: 101, steps per second: 1333, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.358, 10.100], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.074s, episode steps: 101, steps per second: 1362, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.799, 10.272], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 1.191s, episode steps: 101, steps per second: 85, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.902, 10.155], loss: 0.007877, mae: 0.080472, mean_q: -0.120071
  5151/100000: episode: 51, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.410, 10.100], loss: 0.004247, mae: 0.045866, mean_q: -0.123684
  5252/100000: episode: 52, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.288, 10.100], loss: 0.002402, mae: 0.031548, mean_q: -0.105371
  5353/100000: episode: 53, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.257, 10.100], loss: 0.001591, mae: 0.024505, mean_q: -0.103887
  5454/100000: episode: 54, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.157, 10.100], loss: 0.000927, mae: 0.018507, mean_q: -0.092501
  5555/100000: episode: 55, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.100], loss: 0.000545, mae: 0.015577, mean_q: -0.076992
  5656/100000: episode: 56, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.350, 10.100], loss: 0.000231, mae: 0.012001, mean_q: -0.075994
  5757/100000: episode: 57, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.988, 10.100], loss: 0.000092, mae: 0.008351, mean_q: -0.080563
  5858/100000: episode: 58, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.705, 10.248], loss: 0.000080, mae: 0.008363, mean_q: -0.063427
  5959/100000: episode: 59, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.873, 10.362], loss: 0.000093, mae: 0.008690, mean_q: -0.051010
  6060/100000: episode: 60, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.333, 10.177], loss: 0.000078, mae: 0.008574, mean_q: -0.037158
  6161/100000: episode: 61, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.047, 10.132], loss: 0.000077, mae: 0.008231, mean_q: -0.025231
  6262/100000: episode: 62, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.593, 10.202], loss: 0.000087, mae: 0.009019, mean_q: -0.016364
  6363/100000: episode: 63, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.080, 10.192], loss: 0.000097, mae: 0.009149, mean_q: -0.027713
  6464/100000: episode: 64, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.939, 10.100], loss: 0.000088, mae: 0.009083, mean_q: -0.004506
  6565/100000: episode: 65, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.695, 10.222], loss: 0.000095, mae: 0.009236, mean_q: -0.001517
  6666/100000: episode: 66, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.911, 10.100], loss: 0.000095, mae: 0.009258, mean_q: 0.005672
  6767/100000: episode: 67, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.886, 10.416], loss: 0.000085, mae: 0.009249, mean_q: 0.006174
  6868/100000: episode: 68, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.004, 10.121], loss: 0.000109, mae: 0.010244, mean_q: 0.032573
  6969/100000: episode: 69, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.183, 10.269], loss: 0.000088, mae: 0.008866, mean_q: 0.025722
  7070/100000: episode: 70, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.174, 10.210], loss: 0.000076, mae: 0.009023, mean_q: 0.043323
  7171/100000: episode: 71, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.674, 10.249], loss: 0.000096, mae: 0.009752, mean_q: 0.041654
  7272/100000: episode: 72, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.593, 10.198], loss: 0.000067, mae: 0.008274, mean_q: 0.041893
  7373/100000: episode: 73, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.450, 10.216], loss: 0.000133, mae: 0.011463, mean_q: 0.043993
  7474/100000: episode: 74, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.460, 10.422], loss: 0.000077, mae: 0.009295, mean_q: 0.069066
  7575/100000: episode: 75, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.842, 10.100], loss: 0.000082, mae: 0.009140, mean_q: 0.069826
  7676/100000: episode: 76, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.639, 10.100], loss: 0.000071, mae: 0.008407, mean_q: 0.071400
  7777/100000: episode: 77, duration: 0.833s, episode steps: 101, steps per second: 121, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.697, 10.186], loss: 0.000117, mae: 0.010723, mean_q: 0.076622
  7878/100000: episode: 78, duration: 1.489s, episode steps: 101, steps per second: 68, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.673, 10.100], loss: 0.000102, mae: 0.010061, mean_q: 0.075852
  7979/100000: episode: 79, duration: 1.143s, episode steps: 101, steps per second: 88, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.293, 10.100], loss: 0.000083, mae: 0.009275, mean_q: 0.074173
  8080/100000: episode: 80, duration: 0.826s, episode steps: 101, steps per second: 122, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.465, 10.379], loss: 0.000072, mae: 0.008734, mean_q: 0.082498
  8181/100000: episode: 81, duration: 0.946s, episode steps: 101, steps per second: 107, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.151], loss: 0.000085, mae: 0.008996, mean_q: 0.094066
  8282/100000: episode: 82, duration: 0.826s, episode steps: 101, steps per second: 122, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.252, 10.128], loss: 0.000082, mae: 0.009109, mean_q: 0.113532
  8383/100000: episode: 83, duration: 0.913s, episode steps: 101, steps per second: 111, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.248, 10.208], loss: 0.000104, mae: 0.010007, mean_q: 0.103214
  8484/100000: episode: 84, duration: 0.821s, episode steps: 101, steps per second: 123, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.022, 10.263], loss: 0.000109, mae: 0.010423, mean_q: 0.119689
  8585/100000: episode: 85, duration: 0.923s, episode steps: 101, steps per second: 109, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.828, 10.266], loss: 0.000122, mae: 0.011040, mean_q: 0.125050
  8686/100000: episode: 86, duration: 0.906s, episode steps: 101, steps per second: 111, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.138, 10.100], loss: 0.000095, mae: 0.009761, mean_q: 0.121592
  8787/100000: episode: 87, duration: 0.853s, episode steps: 101, steps per second: 118, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.139, 10.116], loss: 0.000092, mae: 0.009686, mean_q: 0.135382
  8888/100000: episode: 88, duration: 0.969s, episode steps: 101, steps per second: 104, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.100], loss: 0.000147, mae: 0.011853, mean_q: 0.122452
  8989/100000: episode: 89, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.649, 10.142], loss: 0.000130, mae: 0.010968, mean_q: 0.143744
  9090/100000: episode: 90, duration: 0.873s, episode steps: 101, steps per second: 116, episode reward: 0.668, mean reward: 0.007 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.622, 10.157], loss: 0.000116, mae: 0.010451, mean_q: 0.141740
  9191/100000: episode: 91, duration: 1.056s, episode steps: 101, steps per second: 96, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.386, 10.199], loss: 0.000142, mae: 0.011366, mean_q: 0.161064
  9292/100000: episode: 92, duration: 0.699s, episode steps: 101, steps per second: 145, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.100], loss: 0.000110, mae: 0.010272, mean_q: 0.173054
  9393/100000: episode: 93, duration: 0.629s, episode steps: 101, steps per second: 160, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.520, 10.245], loss: 0.000126, mae: 0.010661, mean_q: 0.174115
  9494/100000: episode: 94, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.714, 10.100], loss: 0.000117, mae: 0.009798, mean_q: 0.185738
  9595/100000: episode: 95, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.682, 10.164], loss: 0.000123, mae: 0.010171, mean_q: 0.196954
  9696/100000: episode: 96, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.588, 10.225], loss: 0.000178, mae: 0.012126, mean_q: 0.180479
  9797/100000: episode: 97, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.601, 10.100], loss: 0.000147, mae: 0.011125, mean_q: 0.201302
  9898/100000: episode: 98, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.442, 10.100], loss: 0.000153, mae: 0.011769, mean_q: 0.227504
  9999/100000: episode: 99, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.350, 10.100], loss: 0.000141, mae: 0.010850, mean_q: 0.232881
 10100/100000: episode: 100, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.975, 10.411], loss: 0.000093, mae: 0.008864, mean_q: 0.235191
 10201/100000: episode: 101, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.764, 10.100], loss: 0.000084, mae: 0.008209, mean_q: 0.238284
 10302/100000: episode: 102, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.443, 10.100], loss: 0.000084, mae: 0.008296, mean_q: 0.265117
 10403/100000: episode: 103, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.960, 10.395], loss: 0.000097, mae: 0.009114, mean_q: 0.241965
 10504/100000: episode: 104, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.153, 10.100], loss: 0.000088, mae: 0.008514, mean_q: 0.273493
 10605/100000: episode: 105, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.147, 10.100], loss: 0.000093, mae: 0.009135, mean_q: 0.279265
 10706/100000: episode: 106, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.490, 10.100], loss: 0.000071, mae: 0.007773, mean_q: 0.308502
 10807/100000: episode: 107, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.465, 10.208], loss: 0.000069, mae: 0.007770, mean_q: 0.316231
 10908/100000: episode: 108, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.450, 10.100], loss: 0.000062, mae: 0.007523, mean_q: 0.304038
 11009/100000: episode: 109, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.605, 10.100], loss: 0.000054, mae: 0.006809, mean_q: 0.327566
 11110/100000: episode: 110, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.002, 10.100], loss: 0.000052, mae: 0.006867, mean_q: 0.334726
 11211/100000: episode: 111, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.189, 10.100], loss: 0.000048, mae: 0.006369, mean_q: 0.356648
 11312/100000: episode: 112, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.561, 10.278], loss: 0.000038, mae: 0.005663, mean_q: 0.374233
 11413/100000: episode: 113, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.537, 10.154], loss: 0.000051, mae: 0.006855, mean_q: 0.374502
 11514/100000: episode: 114, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.032, 10.332], loss: 0.000031, mae: 0.005475, mean_q: 0.383008
 11615/100000: episode: 115, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.974, 10.100], loss: 0.000038, mae: 0.005624, mean_q: 0.409573
 11716/100000: episode: 116, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.149, 10.296], loss: 0.000046, mae: 0.006610, mean_q: 0.402438
 11817/100000: episode: 117, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.318, 10.160], loss: 0.000045, mae: 0.006443, mean_q: 0.429569
 11918/100000: episode: 118, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.764, 10.100], loss: 0.000041, mae: 0.006040, mean_q: 0.421061
 12019/100000: episode: 119, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.100], loss: 0.000042, mae: 0.006202, mean_q: 0.446962
 12120/100000: episode: 120, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.818, 10.399], loss: 0.000031, mae: 0.005224, mean_q: 0.453803
 12221/100000: episode: 121, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.822, 10.362], loss: 0.000037, mae: 0.005954, mean_q: 0.473377
 12322/100000: episode: 122, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.500, 10.296], loss: 0.000056, mae: 0.007159, mean_q: 0.483760
 12423/100000: episode: 123, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.071, 10.100], loss: 0.000048, mae: 0.006492, mean_q: 0.505142
 12524/100000: episode: 124, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.593, 10.240], loss: 0.000054, mae: 0.007095, mean_q: 0.505020
 12625/100000: episode: 125, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.556, 10.100], loss: 0.000054, mae: 0.006855, mean_q: 0.524554
 12726/100000: episode: 126, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.939, 10.135], loss: 0.000047, mae: 0.006302, mean_q: 0.531459
 12827/100000: episode: 127, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.835, 10.100], loss: 0.000057, mae: 0.007323, mean_q: 0.535138
 12928/100000: episode: 128, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.603, 10.100], loss: 0.000040, mae: 0.006069, mean_q: 0.550299
 13029/100000: episode: 129, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.855, 10.164], loss: 0.000067, mae: 0.007632, mean_q: 0.558600
 13130/100000: episode: 130, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.369, 10.183], loss: 0.000054, mae: 0.007325, mean_q: 0.575632
 13231/100000: episode: 131, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.799, 10.144], loss: 0.000041, mae: 0.006018, mean_q: 0.591920
 13332/100000: episode: 132, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.453, 10.100], loss: 0.000066, mae: 0.007855, mean_q: 0.605474
 13433/100000: episode: 133, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.575, 10.100], loss: 0.000055, mae: 0.007400, mean_q: 0.607875
 13534/100000: episode: 134, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.485, 10.177], loss: 0.000039, mae: 0.005825, mean_q: 0.618754
 13635/100000: episode: 135, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.629, 10.112], loss: 0.000037, mae: 0.005554, mean_q: 0.625305
 13736/100000: episode: 136, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.100], loss: 0.000046, mae: 0.006308, mean_q: 0.638988
 13837/100000: episode: 137, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.100], loss: 0.000037, mae: 0.005677, mean_q: 0.649033
 13938/100000: episode: 138, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.021, 10.318], loss: 0.000038, mae: 0.005972, mean_q: 0.661523
 14039/100000: episode: 139, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.439, 10.100], loss: 0.000030, mae: 0.005081, mean_q: 0.666818
 14140/100000: episode: 140, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.750, 10.100], loss: 0.000027, mae: 0.004992, mean_q: 0.680488
 14241/100000: episode: 141, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.822, 10.217], loss: 0.000022, mae: 0.004251, mean_q: 0.686800
 14342/100000: episode: 142, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.733, 10.226], loss: 0.000021, mae: 0.004033, mean_q: 0.696428
 14443/100000: episode: 143, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.668, 10.100], loss: 0.000021, mae: 0.004208, mean_q: 0.701470
 14544/100000: episode: 144, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.489, 10.166], loss: 0.000029, mae: 0.004704, mean_q: 0.711568
 14645/100000: episode: 145, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.257, 10.100], loss: 0.000020, mae: 0.004035, mean_q: 0.715883
 14746/100000: episode: 146, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.723, 10.100], loss: 0.000019, mae: 0.003521, mean_q: 0.717841
 14847/100000: episode: 147, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.769, 10.327], loss: 0.000013, mae: 0.002810, mean_q: 0.726648
 14948/100000: episode: 148, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.417, 10.100], loss: 0.000015, mae: 0.003108, mean_q: 0.729171
 15049/100000: episode: 149, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.709, 10.100], loss: 0.000015, mae: 0.002988, mean_q: 0.731949
 15150/100000: episode: 150, duration: 0.654s, episode steps: 101, steps per second: 155, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.611, 10.496], loss: 0.000015, mae: 0.003305, mean_q: 0.736184
 15251/100000: episode: 151, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.181, 10.100], loss: 0.000019, mae: 0.003336, mean_q: 0.739742
 15352/100000: episode: 152, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.100], loss: 0.000011, mae: 0.002615, mean_q: 0.741831
 15453/100000: episode: 153, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.271, 10.100], loss: 0.000012, mae: 0.002988, mean_q: 0.744320
 15554/100000: episode: 154, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.476, 10.100], loss: 0.000012, mae: 0.002694, mean_q: 0.744764
 15655/100000: episode: 155, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.495, 10.103], loss: 0.000016, mae: 0.003041, mean_q: 0.745089
 15756/100000: episode: 156, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.501, 10.100], loss: 0.000012, mae: 0.002899, mean_q: 0.747361
 15857/100000: episode: 157, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.728, 10.114], loss: 0.000015, mae: 0.002984, mean_q: 0.748060
 15958/100000: episode: 158, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.533, 10.100], loss: 0.000017, mae: 0.002853, mean_q: 0.748622
 16059/100000: episode: 159, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.944, 10.230], loss: 0.000014, mae: 0.002880, mean_q: 0.749707
 16160/100000: episode: 160, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.832, 10.194], loss: 0.000010, mae: 0.002668, mean_q: 0.750107
 16261/100000: episode: 161, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.845, 10.116], loss: 0.000015, mae: 0.002646, mean_q: 0.749455
 16362/100000: episode: 162, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.335, 10.100], loss: 0.000016, mae: 0.003215, mean_q: 0.749443
 16463/100000: episode: 163, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.941, 10.333], loss: 0.000013, mae: 0.002963, mean_q: 0.749442
 16564/100000: episode: 164, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.314, 10.100], loss: 0.000014, mae: 0.002901, mean_q: 0.749429
 16665/100000: episode: 165, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.852, 10.174], loss: 0.000016, mae: 0.002627, mean_q: 0.748635
 16766/100000: episode: 166, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.581, 10.150], loss: 0.000014, mae: 0.002842, mean_q: 0.748612
 16867/100000: episode: 167, duration: 0.633s, episode steps: 101, steps per second: 159, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.796, 10.431], loss: 0.000016, mae: 0.003209, mean_q: 0.747389
 16968/100000: episode: 168, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.969, 10.228], loss: 0.000014, mae: 0.003053, mean_q: 0.747533
 17069/100000: episode: 169, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.467, 10.100], loss: 0.000014, mae: 0.002880, mean_q: 0.746520
 17170/100000: episode: 170, duration: 0.729s, episode steps: 101, steps per second: 139, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.301, 10.219], loss: 0.000014, mae: 0.002898, mean_q: 0.746131
 17271/100000: episode: 171, duration: 0.913s, episode steps: 101, steps per second: 111, episode reward: 0.663, mean reward: 0.007 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.064, 10.323], loss: 0.000019, mae: 0.003142, mean_q: 0.745500
 17372/100000: episode: 172, duration: 1.103s, episode steps: 101, steps per second: 92, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.412, 10.100], loss: 0.000016, mae: 0.002767, mean_q: 0.744436
 17473/100000: episode: 173, duration: 0.904s, episode steps: 101, steps per second: 112, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.055, 10.420], loss: 0.000013, mae: 0.002580, mean_q: 0.744463
 17574/100000: episode: 174, duration: 0.770s, episode steps: 101, steps per second: 131, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.452, 10.240], loss: 0.000009, mae: 0.002293, mean_q: 0.743867
 17675/100000: episode: 175, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.510, 10.158], loss: 0.000022, mae: 0.003840, mean_q: 0.743164
 17776/100000: episode: 176, duration: 0.884s, episode steps: 101, steps per second: 114, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.301, 10.100], loss: 0.000015, mae: 0.002706, mean_q: 0.742417
 17877/100000: episode: 177, duration: 0.784s, episode steps: 101, steps per second: 129, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.491, 10.100], loss: 0.000013, mae: 0.002975, mean_q: 0.741684
 17978/100000: episode: 178, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.560, 10.233], loss: 0.000012, mae: 0.002236, mean_q: 0.741424
 18079/100000: episode: 179, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.592, 10.100], loss: 0.000012, mae: 0.002360, mean_q: 0.741052
 18180/100000: episode: 180, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.820, 10.348], loss: 0.000012, mae: 0.002354, mean_q: 0.740560
 18281/100000: episode: 181, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.911, 10.229], loss: 0.000011, mae: 0.002365, mean_q: 0.740007
 18382/100000: episode: 182, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.730, 10.354], loss: 0.000011, mae: 0.002470, mean_q: 0.739780
 18483/100000: episode: 183, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.320, 10.100], loss: 0.000015, mae: 0.002440, mean_q: 0.739504
 18584/100000: episode: 184, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.389, 10.100], loss: 0.000012, mae: 0.002183, mean_q: 0.739099
 18685/100000: episode: 185, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.475, 10.100], loss: 0.000013, mae: 0.002563, mean_q: 0.738927
 18786/100000: episode: 186, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.302, 10.302], loss: 0.000021, mae: 0.003577, mean_q: 0.738725
 18887/100000: episode: 187, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.084, 10.134], loss: 0.000014, mae: 0.002429, mean_q: 0.738430
 18988/100000: episode: 188, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.198, 10.100], loss: 0.000014, mae: 0.002302, mean_q: 0.738036
 19089/100000: episode: 189, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.538, 10.196], loss: 0.000008, mae: 0.001862, mean_q: 0.737666
 19190/100000: episode: 190, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.498, 10.100], loss: 0.000011, mae: 0.002115, mean_q: 0.737333
 19291/100000: episode: 191, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.475, 10.100], loss: 0.000008, mae: 0.001851, mean_q: 0.736760
 19392/100000: episode: 192, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.724, 10.100], loss: 0.000019, mae: 0.002641, mean_q: 0.736954
 19493/100000: episode: 193, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.620, 10.196], loss: 0.000012, mae: 0.002440, mean_q: 0.736686
 19594/100000: episode: 194, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.430, 10.100], loss: 0.000012, mae: 0.002320, mean_q: 0.736383
 19695/100000: episode: 195, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.671, 10.316], loss: 0.000014, mae: 0.002373, mean_q: 0.736191
 19796/100000: episode: 196, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.668, mean reward: 0.007 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.516, 10.254], loss: 0.000013, mae: 0.001853, mean_q: 0.736319
 19897/100000: episode: 197, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.139, 10.222], loss: 0.000018, mae: 0.002342, mean_q: 0.736329
 19998/100000: episode: 198, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.692, 10.100], loss: 0.000020, mae: 0.002684, mean_q: 0.736224
 20099/100000: episode: 199, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.645, 10.371], loss: 0.000020, mae: 0.003093, mean_q: 0.736263
[Info] 1-TH LEVEL FOUND: 0.7481045722961426, Considering 100/100 traces
 20200/100000: episode: 200, duration: 5.755s, episode steps: 101, steps per second: 18, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.891, 10.215], loss: 0.000017, mae: 0.002681, mean_q: 0.736427
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7481045722961426
1
 20301/100000: episode: 201, duration: 5.436s, episode steps: 101, steps per second: 19, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.601, 10.139], loss: 0.000013, mae: 0.002212, mean_q: 0.736253
 20402/100000: episode: 202, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.924, 10.384], loss: 0.000013, mae: 0.002235, mean_q: 0.736192
 20503/100000: episode: 203, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.234, 10.337], loss: 0.000011, mae: 0.002137, mean_q: 0.736307
 20604/100000: episode: 204, duration: 0.731s, episode steps: 101, steps per second: 138, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.564, 10.100], loss: 0.000008, mae: 0.001908, mean_q: 0.736000
 20705/100000: episode: 205, duration: 0.750s, episode steps: 101, steps per second: 135, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.874, 10.194], loss: 0.000010, mae: 0.002103, mean_q: 0.736215
 20806/100000: episode: 206, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.121, 10.100], loss: 0.000006, mae: 0.001598, mean_q: 0.736153
 20907/100000: episode: 207, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: 0.664, mean reward: 0.007 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.430, 10.179], loss: 0.000008, mae: 0.001635, mean_q: 0.735968
 21008/100000: episode: 208, duration: 0.730s, episode steps: 101, steps per second: 138, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.428, 10.100], loss: 0.000014, mae: 0.002473, mean_q: 0.735920
 21109/100000: episode: 209, duration: 0.798s, episode steps: 101, steps per second: 126, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.587, 10.107], loss: 0.000008, mae: 0.001691, mean_q: 0.735617
 21210/100000: episode: 210, duration: 1.062s, episode steps: 101, steps per second: 95, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.022, 10.186], loss: 0.000006, mae: 0.001253, mean_q: 0.735709
 21311/100000: episode: 211, duration: 0.866s, episode steps: 101, steps per second: 117, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.417, 10.162], loss: 0.000012, mae: 0.001678, mean_q: 0.735747
 21412/100000: episode: 212, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.595, 10.100], loss: 0.000006, mae: 0.001754, mean_q: 0.735613
 21513/100000: episode: 213, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: 0.660, mean reward: 0.007 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.661, 10.100], loss: 0.000014, mae: 0.002570, mean_q: 0.735813
 21614/100000: episode: 214, duration: 0.947s, episode steps: 101, steps per second: 107, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.586, 10.100], loss: 0.000013, mae: 0.002789, mean_q: 0.736042
 21715/100000: episode: 215, duration: 1.276s, episode steps: 101, steps per second: 79, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.596, 10.231], loss: 0.000014, mae: 0.002488, mean_q: 0.735779
 21816/100000: episode: 216, duration: 0.671s, episode steps: 101, steps per second: 151, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.682, 10.100], loss: 0.000011, mae: 0.001849, mean_q: 0.735710
 21917/100000: episode: 217, duration: 0.670s, episode steps: 101, steps per second: 151, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.845, 10.204], loss: 0.000015, mae: 0.002741, mean_q: 0.735547
 22018/100000: episode: 218, duration: 0.784s, episode steps: 101, steps per second: 129, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.218, 10.452], loss: 0.000010, mae: 0.001482, mean_q: 0.735425
 22119/100000: episode: 219, duration: 0.686s, episode steps: 101, steps per second: 147, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.063, 10.100], loss: 0.000011, mae: 0.002139, mean_q: 0.734947
 22220/100000: episode: 220, duration: 0.652s, episode steps: 101, steps per second: 155, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.072, 10.100], loss: 0.000009, mae: 0.001895, mean_q: 0.734964
 22321/100000: episode: 221, duration: 0.697s, episode steps: 101, steps per second: 145, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.213, 10.130], loss: 0.000016, mae: 0.002354, mean_q: 0.735129
 22422/100000: episode: 222, duration: 0.719s, episode steps: 101, steps per second: 141, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.831, 10.100], loss: 0.000014, mae: 0.002241, mean_q: 0.734957
 22523/100000: episode: 223, duration: 0.686s, episode steps: 101, steps per second: 147, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.836, 10.100], loss: 0.000010, mae: 0.001934, mean_q: 0.734870
 22624/100000: episode: 224, duration: 0.751s, episode steps: 101, steps per second: 134, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.952, 10.100], loss: 0.000015, mae: 0.002220, mean_q: 0.734954
 22725/100000: episode: 225, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.484, 10.117], loss: 0.000011, mae: 0.002077, mean_q: 0.734763
 22826/100000: episode: 226, duration: 1.015s, episode steps: 101, steps per second: 99, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.342, 10.282], loss: 0.000009, mae: 0.001673, mean_q: 0.734551
 22927/100000: episode: 227, duration: 0.882s, episode steps: 101, steps per second: 114, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.100], loss: 0.000005, mae: 0.001374, mean_q: 0.734627
 23028/100000: episode: 228, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.380, 10.129], loss: 0.000013, mae: 0.001923, mean_q: 0.734375
 23129/100000: episode: 229, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.841, 10.100], loss: 0.000006, mae: 0.001197, mean_q: 0.734383
 23230/100000: episode: 230, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.001, 10.100], loss: 0.000009, mae: 0.002055, mean_q: 0.734404
 23331/100000: episode: 231, duration: 0.902s, episode steps: 101, steps per second: 112, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.870, 10.100], loss: 0.000014, mae: 0.002999, mean_q: 0.734319
 23432/100000: episode: 232, duration: 1.166s, episode steps: 101, steps per second: 87, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.345, 10.252], loss: 0.000010, mae: 0.001823, mean_q: 0.734301
 23533/100000: episode: 233, duration: 0.928s, episode steps: 101, steps per second: 109, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.511, 10.134], loss: 0.000010, mae: 0.002120, mean_q: 0.734476
 23634/100000: episode: 234, duration: 1.054s, episode steps: 101, steps per second: 96, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.863, 10.119], loss: 0.000012, mae: 0.002263, mean_q: 0.734714
 23735/100000: episode: 235, duration: 0.833s, episode steps: 101, steps per second: 121, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.263, 10.100], loss: 0.000006, mae: 0.001399, mean_q: 0.734922
 23836/100000: episode: 236, duration: 0.776s, episode steps: 101, steps per second: 130, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.252, 10.193], loss: 0.000015, mae: 0.002672, mean_q: 0.734994
 23937/100000: episode: 237, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.137, 10.197], loss: 0.000011, mae: 0.001895, mean_q: 0.735156
 24038/100000: episode: 238, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.147, 10.141], loss: 0.000013, mae: 0.002150, mean_q: 0.735130
 24139/100000: episode: 239, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.008, 10.100], loss: 0.000012, mae: 0.002188, mean_q: 0.735217
 24240/100000: episode: 240, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.546, 10.164], loss: 0.000006, mae: 0.001684, mean_q: 0.735268
 24341/100000: episode: 241, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.397, 10.236], loss: 0.000010, mae: 0.002245, mean_q: 0.735178
 24442/100000: episode: 242, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.632, 10.100], loss: 0.000006, mae: 0.001199, mean_q: 0.735213
 24543/100000: episode: 243, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.669, mean reward: 0.007 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.375, 10.169], loss: 0.000012, mae: 0.002160, mean_q: 0.735184
 24644/100000: episode: 244, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.870, 10.232], loss: 0.000014, mae: 0.002131, mean_q: 0.735323
 24745/100000: episode: 245, duration: 0.792s, episode steps: 101, steps per second: 127, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.594, 10.117], loss: 0.000014, mae: 0.002412, mean_q: 0.735438
 24846/100000: episode: 246, duration: 0.737s, episode steps: 101, steps per second: 137, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.544, 10.100], loss: 0.000023, mae: 0.003198, mean_q: 0.735840
 24947/100000: episode: 247, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.647, 10.100], loss: 0.000014, mae: 0.001645, mean_q: 0.735874
 25048/100000: episode: 248, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.815, 10.117], loss: 0.000011, mae: 0.002022, mean_q: 0.735775
 25149/100000: episode: 249, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.467, 10.228], loss: 0.000011, mae: 0.001911, mean_q: 0.736169
 25250/100000: episode: 250, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.440, 10.100], loss: 0.000015, mae: 0.002228, mean_q: 0.736177
 25351/100000: episode: 251, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.624, 10.213], loss: 0.000011, mae: 0.002001, mean_q: 0.736123
 25452/100000: episode: 252, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.567, 10.100], loss: 0.000010, mae: 0.001845, mean_q: 0.736073
 25553/100000: episode: 253, duration: 0.663s, episode steps: 101, steps per second: 152, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.992, 10.234], loss: 0.000014, mae: 0.002158, mean_q: 0.736193
 25654/100000: episode: 254, duration: 0.733s, episode steps: 101, steps per second: 138, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.367, 10.392], loss: 0.000014, mae: 0.001952, mean_q: 0.736506
 25755/100000: episode: 255, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.428, 10.100], loss: 0.000015, mae: 0.002015, mean_q: 0.736569
 25856/100000: episode: 256, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.141, 10.100], loss: 0.000012, mae: 0.001976, mean_q: 0.736577
 25957/100000: episode: 257, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.248, 10.238], loss: 0.000013, mae: 0.002037, mean_q: 0.736844
 26058/100000: episode: 258, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.075, 10.185], loss: 0.000015, mae: 0.001700, mean_q: 0.736952
 26159/100000: episode: 259, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.431, 10.100], loss: 0.000018, mae: 0.002852, mean_q: 0.736989
 26260/100000: episode: 260, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.885, 10.100], loss: 0.000010, mae: 0.001757, mean_q: 0.737148
 26361/100000: episode: 261, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.100], loss: 0.000007, mae: 0.001211, mean_q: 0.737186
 26462/100000: episode: 262, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.842, 10.100], loss: 0.000008, mae: 0.001461, mean_q: 0.737273
 26563/100000: episode: 263, duration: 0.762s, episode steps: 101, steps per second: 133, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.137, 10.139], loss: 0.000011, mae: 0.001849, mean_q: 0.737315
 26664/100000: episode: 264, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.212, 10.100], loss: 0.000011, mae: 0.002042, mean_q: 0.737282
 26765/100000: episode: 265, duration: 0.756s, episode steps: 101, steps per second: 134, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.147, 10.181], loss: 0.000011, mae: 0.001708, mean_q: 0.737236
 26866/100000: episode: 266, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.150], loss: 0.000012, mae: 0.002316, mean_q: 0.737129
 26967/100000: episode: 267, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.454, 10.100], loss: 0.000012, mae: 0.002538, mean_q: 0.737094
 27068/100000: episode: 268, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.986, mean reward: 0.010 [0.000, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.105, 10.100], loss: 0.000013, mae: 0.002570, mean_q: 0.737294
 27169/100000: episode: 269, duration: 0.702s, episode steps: 101, steps per second: 144, episode reward: 0.898, mean reward: 0.009 [0.000, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.580, 10.100], loss: 0.000007, mae: 0.001638, mean_q: 0.737376
 27270/100000: episode: 270, duration: 0.698s, episode steps: 101, steps per second: 145, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.190], loss: 0.000028, mae: 0.002443, mean_q: 0.737797
 27371/100000: episode: 271, duration: 0.726s, episode steps: 101, steps per second: 139, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.044, 10.164], loss: 0.000008, mae: 0.001157, mean_q: 0.737850
 27472/100000: episode: 272, duration: 1.007s, episode steps: 101, steps per second: 100, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.198, 10.101], loss: 0.000028, mae: 0.002843, mean_q: 0.738135
 27573/100000: episode: 273, duration: 0.994s, episode steps: 101, steps per second: 102, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.473, 10.196], loss: 0.000042, mae: 0.003166, mean_q: 0.738307
 27674/100000: episode: 274, duration: 0.741s, episode steps: 101, steps per second: 136, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.011, 10.100], loss: 0.000046, mae: 0.003686, mean_q: 0.738470
 27775/100000: episode: 275, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: 0.816, mean reward: 0.008 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.100], loss: 0.000021, mae: 0.002637, mean_q: 0.738715
 27876/100000: episode: 276, duration: 1.288s, episode steps: 101, steps per second: 78, episode reward: 0.935, mean reward: 0.009 [0.000, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.898, 10.100], loss: 0.000008, mae: 0.001470, mean_q: 0.738582
 27977/100000: episode: 277, duration: 0.771s, episode steps: 101, steps per second: 131, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.577, 10.312], loss: 0.000015, mae: 0.001923, mean_q: 0.738833
 28078/100000: episode: 278, duration: 0.671s, episode steps: 101, steps per second: 151, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.510, 10.108], loss: 0.000011, mae: 0.001269, mean_q: 0.738870
 28179/100000: episode: 279, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.355, 10.217], loss: 0.000011, mae: 0.001505, mean_q: 0.738975
 28280/100000: episode: 280, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.662, 10.100], loss: 0.000016, mae: 0.002190, mean_q: 0.739045
 28381/100000: episode: 281, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.243, 10.100], loss: 0.000012, mae: 0.001681, mean_q: 0.738733
 28482/100000: episode: 282, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.921, 10.239], loss: 0.000015, mae: 0.001827, mean_q: 0.738965
 28583/100000: episode: 283, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.179, 10.100], loss: 0.000027, mae: 0.002776, mean_q: 0.739132
 28684/100000: episode: 284, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.767, 10.370], loss: 0.000034, mae: 0.002950, mean_q: 0.739456
 28785/100000: episode: 285, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.098, 10.100], loss: 0.000025, mae: 0.003027, mean_q: 0.739390
 28886/100000: episode: 286, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.392, 10.241], loss: 0.000040, mae: 0.002793, mean_q: 0.739900
 28987/100000: episode: 287, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.511, 10.337], loss: 0.000030, mae: 0.002695, mean_q: 0.739889
 29088/100000: episode: 288, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.762, 10.251], loss: 0.000013, mae: 0.001706, mean_q: 0.739994
 29189/100000: episode: 289, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.100], loss: 0.000011, mae: 0.001757, mean_q: 0.739900
 29290/100000: episode: 290, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.424, 10.100], loss: 0.000020, mae: 0.001959, mean_q: 0.740168
 29391/100000: episode: 291, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.664, 10.277], loss: 0.000015, mae: 0.001660, mean_q: 0.740373
 29492/100000: episode: 292, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.677, 10.322], loss: 0.000020, mae: 0.001975, mean_q: 0.740371
 29593/100000: episode: 293, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.696, 10.100], loss: 0.000019, mae: 0.002495, mean_q: 0.740396
 29694/100000: episode: 294, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.576, 10.100], loss: 0.000014, mae: 0.001810, mean_q: 0.740417
 29795/100000: episode: 295, duration: 0.656s, episode steps: 101, steps per second: 154, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.917, 10.315], loss: 0.000020, mae: 0.002356, mean_q: 0.740609
 29896/100000: episode: 296, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.527, 10.128], loss: 0.000026, mae: 0.002299, mean_q: 0.740520
 29997/100000: episode: 297, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.729, 10.140], loss: 0.000010, mae: 0.001485, mean_q: 0.740477
 30098/100000: episode: 298, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.527, 10.100], loss: 0.000029, mae: 0.002655, mean_q: 0.740599
 30199/100000: episode: 299, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.100], loss: 0.000020, mae: 0.001936, mean_q: 0.740763
 30300/100000: episode: 300, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.489, 10.142], loss: 0.000009, mae: 0.001433, mean_q: 0.740730
[Info] 1-TH LEVEL FOUND: 0.7440811991691589, Considering 10/100 traces
 30401/100000: episode: 301, duration: 5.348s, episode steps: 101, steps per second: 19, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.515, 10.100], loss: 0.000010, mae: 0.001664, mean_q: 0.740499
 30402/100000: episode: 302, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.709, mean reward: 0.709 [0.709, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.391, 10.200], loss: 0.000003, mae: 0.002422, mean_q: 0.738309
 30403/100000: episode: 303, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.261, 10.100], loss: 0.000004, mae: 0.001234, mean_q: 0.741675
 30406/100000: episode: 304, duration: 0.029s, episode steps: 3, steps per second: 103, episode reward: 0.761, mean reward: 0.254 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.389, 10.100], loss: 0.000035, mae: 0.003793, mean_q: 0.738777
 30407/100000: episode: 305, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.639, mean reward: 0.639 [0.639, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.338, 10.100], loss: 0.000002, mae: 0.001927, mean_q: 0.741812
 30408/100000: episode: 306, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.700, mean reward: 0.700 [0.700, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.260, 10.100], loss: 0.000000, mae: 0.000523, mean_q: 0.740758
 30409/100000: episode: 307, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.602, mean reward: 0.602 [0.602, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.325, 10.100], loss: 0.000005, mae: 0.002960, mean_q: 0.737788
 30410/100000: episode: 308, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.746, mean reward: 0.746 [0.746, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.280, 10.200], loss: 0.000003, mae: 0.002360, mean_q: 0.742789
 30411/100000: episode: 309, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.655, mean reward: 0.655 [0.655, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.347, 10.100], loss: 0.000003, mae: 0.002423, mean_q: 0.742672
 30412/100000: episode: 310, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.354, 10.100], loss: 0.000004, mae: 0.001950, mean_q: 0.738731
 30413/100000: episode: 311, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.740, mean reward: 0.740 [0.740, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.252, 10.100], loss: 0.000000, mae: 0.000741, mean_q: 0.739672
 30414/100000: episode: 312, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.782, mean reward: 0.782 [0.782, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.286, 10.200], loss: 0.000002, mae: 0.001857, mean_q: 0.741713
 30415/100000: episode: 313, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.632, mean reward: 0.632 [0.632, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.298, 10.200], loss: 0.000098, mae: 0.004303, mean_q: 0.739263
 30416/100000: episode: 314, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.623, mean reward: 0.623 [0.623, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.235, 10.100], loss: 0.000176, mae: 0.004425, mean_q: 0.739181
 30417/100000: episode: 315, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.642, mean reward: 0.642 [0.642, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.227, 10.200], loss: 0.000001, mae: 0.001315, mean_q: 0.739127
 30418/100000: episode: 316, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.365, 10.100], loss: 0.000002, mae: 0.001828, mean_q: 0.742480
 30419/100000: episode: 317, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.789, mean reward: 0.789 [0.789, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.215, 10.200], loss: 0.000000, mae: 0.000283, mean_q: 0.739888
 30422/100000: episode: 318, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 0.694, mean reward: 0.231 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.404, 10.100], loss: 0.000005, mae: 0.001341, mean_q: 0.741851
 30423/100000: episode: 319, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.636, mean reward: 0.636 [0.636, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.387, 10.100], loss: 0.000000, mae: 0.000222, mean_q: 0.740248
 30424/100000: episode: 320, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.762, mean reward: 0.762 [0.762, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.247, 10.200], loss: 0.000001, mae: 0.001069, mean_q: 0.741685
 30425/100000: episode: 321, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.700, mean reward: 0.700 [0.700, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.298, 10.200], loss: 0.000027, mae: 0.001596, mean_q: 0.740937
 30426/100000: episode: 322, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.631, mean reward: 0.631 [0.631, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.288, 10.100], loss: 0.000002, mae: 0.002077, mean_q: 0.739053
 30427/100000: episode: 323, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.678, mean reward: 0.678 [0.678, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.250, 10.100], loss: 0.000062, mae: 0.003512, mean_q: 0.742409
 30428/100000: episode: 324, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.351, 10.100], loss: 0.000025, mae: 0.002508, mean_q: 0.739479
 30429/100000: episode: 325, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.837, mean reward: 0.837 [0.837, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.254, 10.200], loss: 0.000000, mae: 0.000866, mean_q: 0.740875
 30430/100000: episode: 326, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.341, 10.100], loss: 0.000003, mae: 0.001057, mean_q: 0.741046
 30431/100000: episode: 327, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.698, mean reward: 0.698 [0.698, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.245, 10.100], loss: 0.000006, mae: 0.002303, mean_q: 0.739209
 30432/100000: episode: 328, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.643, mean reward: 0.643 [0.643, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.364, 10.100], loss: 0.000000, mae: 0.000296, mean_q: 0.740297
 30433/100000: episode: 329, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.657, mean reward: 0.657 [0.657, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.283, 10.200], loss: 0.000000, mae: 0.000910, mean_q: 0.739566
 30434/100000: episode: 330, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.548, mean reward: 0.548 [0.548, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.221, 10.200], loss: 0.000001, mae: 0.001203, mean_q: 0.739111
 30435/100000: episode: 331, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.721, mean reward: 0.721 [0.721, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.301, 10.200], loss: 0.000002, mae: 0.001753, mean_q: 0.741893
 30436/100000: episode: 332, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.223, 10.100], loss: 0.000000, mae: 0.000405, mean_q: 0.740160
 30437/100000: episode: 333, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.300, 10.100], loss: 0.000046, mae: 0.004367, mean_q: 0.738506
 30438/100000: episode: 334, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.606, mean reward: 0.606 [0.606, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.241, 10.200], loss: 0.000720, mae: 0.009799, mean_q: 0.739662
 30439/100000: episode: 335, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.359, 10.100], loss: 0.000584, mae: 0.010305, mean_q: 0.735906
 30440/100000: episode: 336, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.689, mean reward: 0.689 [0.689, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.297, 10.200], loss: 0.000002, mae: 0.001892, mean_q: 0.741996
 30443/100000: episode: 337, duration: 0.023s, episode steps: 3, steps per second: 128, episode reward: 0.726, mean reward: 0.242 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.346, 10.100], loss: 0.000009, mae: 0.003648, mean_q: 0.740590
 30444/100000: episode: 338, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.539, mean reward: 0.539 [0.539, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.233, 10.200], loss: 0.000066, mae: 0.003978, mean_q: 0.739362
 30445/100000: episode: 339, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.584, mean reward: 0.584 [0.584, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.257, 10.100], loss: 0.000007, mae: 0.003515, mean_q: 0.743702
 30446/100000: episode: 340, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.687, mean reward: 0.687 [0.687, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.331, 10.200], loss: 0.000004, mae: 0.002410, mean_q: 0.737620
 30447/100000: episode: 341, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.275, 10.100], loss: 0.000085, mae: 0.007341, mean_q: 0.735971
 30448/100000: episode: 342, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.300, 10.100], loss: 0.000001, mae: 0.001407, mean_q: 0.741366
 30451/100000: episode: 343, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 0.734, mean reward: 0.245 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.401, 10.100], loss: 0.000003, mae: 0.002131, mean_q: 0.740299
 30454/100000: episode: 344, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 0.747, mean reward: 0.249 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.295, 10.100], loss: 0.000061, mae: 0.002418, mean_q: 0.739653
 30455/100000: episode: 345, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.655, mean reward: 0.655 [0.655, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.321, 10.200], loss: 0.000009, mae: 0.004108, mean_q: 0.735718
 30456/100000: episode: 346, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.550, mean reward: 0.550 [0.550, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.231, 10.200], loss: 0.000001, mae: 0.000977, mean_q: 0.739616
 30457/100000: episode: 347, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.708, mean reward: 0.708 [0.708, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.360, 10.200], loss: 0.000029, mae: 0.004140, mean_q: 0.742802
 30458/100000: episode: 348, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.657, mean reward: 0.657 [0.657, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.237, 10.200], loss: 0.000001, mae: 0.001358, mean_q: 0.738513
 30459/100000: episode: 349, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.743, mean reward: 0.743 [0.743, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.341, 10.200], loss: 0.000002, mae: 0.001393, mean_q: 0.738819
 30460/100000: episode: 350, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.645, mean reward: 0.645 [0.645, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.260, 10.200], loss: 0.000008, mae: 0.001549, mean_q: 0.739657
 30463/100000: episode: 351, duration: 0.028s, episode steps: 3, steps per second: 109, episode reward: 0.713, mean reward: 0.238 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.313, 10.100], loss: 0.000094, mae: 0.004558, mean_q: 0.737662
 30464/100000: episode: 352, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.609, mean reward: 0.609 [0.609, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.374, 10.100], loss: 0.000001, mae: 0.001102, mean_q: 0.740668
 30465/100000: episode: 353, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.702, mean reward: 0.702 [0.702, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.229, 10.200], loss: 0.000077, mae: 0.004536, mean_q: 0.742567
 30468/100000: episode: 354, duration: 0.020s, episode steps: 3, steps per second: 146, episode reward: 0.706, mean reward: 0.235 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.433, 10.100], loss: 0.000099, mae: 0.005670, mean_q: 0.738212
 30469/100000: episode: 355, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.771, mean reward: 0.771 [0.771, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.294, 10.200], loss: 0.000256, mae: 0.004924, mean_q: 0.740162
 30470/100000: episode: 356, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.578, mean reward: 0.578 [0.578, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.178, 10.200], loss: 0.000407, mae: 0.010113, mean_q: 0.740850
 30471/100000: episode: 357, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.301, 10.100], loss: 0.000039, mae: 0.004222, mean_q: 0.737481
 30472/100000: episode: 358, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.755, mean reward: 0.755 [0.755, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.259, 10.100], loss: 0.000006, mae: 0.003270, mean_q: 0.742895
 30473/100000: episode: 359, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.332, 10.100], loss: 0.000002, mae: 0.001568, mean_q: 0.740570
 30476/100000: episode: 360, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 0.707, mean reward: 0.236 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.373, 10.100], loss: 0.000051, mae: 0.002810, mean_q: 0.739635
 30477/100000: episode: 361, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.604, mean reward: 0.604 [0.604, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.302, 10.100], loss: 0.000000, mae: 0.000760, mean_q: 0.740420
 30478/100000: episode: 362, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.618, mean reward: 0.618 [0.618, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.250, 10.100], loss: 0.000020, mae: 0.001901, mean_q: 0.740459
 30479/100000: episode: 363, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.418, 10.100], loss: 0.000024, mae: 0.002658, mean_q: 0.738579
 30480/100000: episode: 364, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.663, mean reward: 0.663 [0.663, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.361, 10.100], loss: 0.000135, mae: 0.003845, mean_q: 0.739550
 30481/100000: episode: 365, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.380, 10.100], loss: 0.000001, mae: 0.000935, mean_q: 0.739364
 30482/100000: episode: 366, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.680, mean reward: 0.680 [0.680, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.354, 10.200], loss: 0.000556, mae: 0.008137, mean_q: 0.741961
 30483/100000: episode: 367, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 0.617, mean reward: 0.617 [0.617, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.278, 10.200], loss: 0.000048, mae: 0.007523, mean_q: 0.733576
 30486/100000: episode: 368, duration: 0.028s, episode steps: 3, steps per second: 108, episode reward: 0.770, mean reward: 0.257 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.427, 10.100], loss: 0.000011, mae: 0.004348, mean_q: 0.744012
 30487/100000: episode: 369, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.309, 10.100], loss: 0.000016, mae: 0.005342, mean_q: 0.735199
 30488/100000: episode: 370, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.595, mean reward: 0.595 [0.595, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.278, 10.100], loss: 0.000249, mae: 0.007461, mean_q: 0.739594
 30489/100000: episode: 371, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.722, mean reward: 0.722 [0.722, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.666, 10.200], loss: 0.000002, mae: 0.001685, mean_q: 0.738682
 30490/100000: episode: 372, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.597, mean reward: 0.597 [0.597, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.350, 10.100], loss: 0.000176, mae: 0.005462, mean_q: 0.741563
 30491/100000: episode: 373, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.265, 10.100], loss: 0.000015, mae: 0.005021, mean_q: 0.745310
 30494/100000: episode: 374, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 0.776, mean reward: 0.259 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.334, 10.100], loss: 0.000009, mae: 0.003640, mean_q: 0.737182
 30495/100000: episode: 375, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.736, mean reward: 0.736 [0.736, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.242, 10.100], loss: 0.000001, mae: 0.000939, mean_q: 0.740305
 30496/100000: episode: 376, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.644, mean reward: 0.644 [0.644, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.397, 10.100], loss: 0.000010, mae: 0.003997, mean_q: 0.736549
 30497/100000: episode: 377, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.629, mean reward: 0.629 [0.629, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.398, 10.100], loss: 0.000001, mae: 0.000957, mean_q: 0.741053
 30500/100000: episode: 378, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 0.904, mean reward: 0.301 [0.000, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.377, 10.100], loss: 0.000109, mae: 0.008023, mean_q: 0.733954
 30501/100000: episode: 379, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.650, mean reward: 0.650 [0.650, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.339, 10.200], loss: 0.000272, mae: 0.010115, mean_q: 0.744841
 30502/100000: episode: 380, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.350, 10.200], loss: 0.000138, mae: 0.008750, mean_q: 0.733217
 30503/100000: episode: 381, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.595, mean reward: 0.595 [0.595, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.279, 10.100], loss: 0.000742, mae: 0.010596, mean_q: 0.735294
 30506/100000: episode: 382, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 0.788, mean reward: 0.263 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.336, 10.100], loss: 0.000077, mae: 0.010920, mean_q: 0.745029
 30507/100000: episode: 383, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.641, mean reward: 0.641 [0.641, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.312, 10.100], loss: 0.000035, mae: 0.008178, mean_q: 0.732233
 30508/100000: episode: 384, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.632, mean reward: 0.632 [0.632, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.356, 10.100], loss: 0.000103, mae: 0.014186, mean_q: 0.754145
 30509/100000: episode: 385, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.549, mean reward: 0.549 [0.549, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.257, 10.200], loss: 0.000001, mae: 0.001352, mean_q: 0.739535
 30510/100000: episode: 386, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.339, 10.200], loss: 0.000084, mae: 0.012749, mean_q: 0.727635
 30511/100000: episode: 387, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.704, mean reward: 0.704 [0.704, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.254, 10.100], loss: 0.000022, mae: 0.006254, mean_q: 0.746358
 30512/100000: episode: 388, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.240, 10.100], loss: 0.000098, mae: 0.008473, mean_q: 0.746816
 30513/100000: episode: 389, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.579, mean reward: 0.579 [0.579, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.178, 10.200], loss: 0.000037, mae: 0.008563, mean_q: 0.732070
 30514/100000: episode: 390, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.716, mean reward: 0.716 [0.716, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.352, 10.200], loss: 0.000010, mae: 0.004491, mean_q: 0.736084
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7440811991691589
1
 30515/100000: episode: 391, duration: 4.981s, episode steps: 1, steps per second: 0, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.298, 10.100], loss: 0.000009, mae: 0.004234, mean_q: 0.745189
 30616/100000: episode: 392, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.964, 10.232], loss: 0.000110, mae: 0.006696, mean_q: 0.739252
 30717/100000: episode: 393, duration: 0.656s, episode steps: 101, steps per second: 154, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.364, 10.336], loss: 0.000099, mae: 0.006924, mean_q: 0.736019
 30818/100000: episode: 394, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.781, 10.121], loss: 0.000081, mae: 0.005451, mean_q: 0.734385
 30919/100000: episode: 395, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.714, 10.100], loss: 0.000070, mae: 0.004290, mean_q: 0.734555
 31020/100000: episode: 396, duration: 0.663s, episode steps: 101, steps per second: 152, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.013, 10.100], loss: 0.000048, mae: 0.003842, mean_q: 0.734129
 31121/100000: episode: 397, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.986, 10.296], loss: 0.000100, mae: 0.006243, mean_q: 0.733782
 31222/100000: episode: 398, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.764, 10.100], loss: 0.000078, mae: 0.005659, mean_q: 0.733662
 31323/100000: episode: 399, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.935, 10.100], loss: 0.000080, mae: 0.004609, mean_q: 0.733638
 31424/100000: episode: 400, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.986, 10.100], loss: 0.000056, mae: 0.004477, mean_q: 0.733650
 31525/100000: episode: 401, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.395, 10.124], loss: 0.000072, mae: 0.005465, mean_q: 0.732917
 31626/100000: episode: 402, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.211], loss: 0.000088, mae: 0.004838, mean_q: 0.733483
 31727/100000: episode: 403, duration: 0.651s, episode steps: 101, steps per second: 155, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.485, 10.439], loss: 0.000059, mae: 0.004294, mean_q: 0.733342
 31828/100000: episode: 404, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.815, 10.100], loss: 0.000072, mae: 0.004232, mean_q: 0.733078
 31929/100000: episode: 405, duration: 0.656s, episode steps: 101, steps per second: 154, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.155, 10.192], loss: 0.000086, mae: 0.005210, mean_q: 0.732962
 32030/100000: episode: 406, duration: 0.728s, episode steps: 101, steps per second: 139, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.415, 10.270], loss: 0.000058, mae: 0.005067, mean_q: 0.732693
 32131/100000: episode: 407, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.645, 10.100], loss: 0.000082, mae: 0.004980, mean_q: 0.733605
 32232/100000: episode: 408, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.100], loss: 0.000056, mae: 0.004209, mean_q: 0.732686
 32333/100000: episode: 409, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.187, 10.100], loss: 0.000042, mae: 0.003664, mean_q: 0.732768
 32434/100000: episode: 410, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.452, 10.100], loss: 0.000117, mae: 0.005542, mean_q: 0.733159
 32535/100000: episode: 411, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.881, 10.100], loss: 0.000073, mae: 0.004705, mean_q: 0.731970
 32636/100000: episode: 412, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.836, 10.133], loss: 0.000066, mae: 0.004065, mean_q: 0.732333
 32737/100000: episode: 413, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.960, 10.100], loss: 0.000071, mae: 0.003845, mean_q: 0.732822
 32838/100000: episode: 414, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.019, 10.100], loss: 0.000056, mae: 0.003571, mean_q: 0.732678
 32939/100000: episode: 415, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.930, 10.129], loss: 0.000071, mae: 0.004397, mean_q: 0.731767
 33040/100000: episode: 416, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.270, 10.100], loss: 0.000055, mae: 0.003601, mean_q: 0.731657
 33141/100000: episode: 417, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.705, 10.216], loss: 0.000073, mae: 0.004657, mean_q: 0.731596
 33242/100000: episode: 418, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.656, mean reward: 0.006 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.935, 10.227], loss: 0.000057, mae: 0.004294, mean_q: 0.731516
 33343/100000: episode: 419, duration: 0.653s, episode steps: 101, steps per second: 155, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.467, 10.366], loss: 0.000063, mae: 0.004999, mean_q: 0.730719
 33444/100000: episode: 420, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.670, 10.233], loss: 0.000062, mae: 0.004191, mean_q: 0.730940
 33545/100000: episode: 421, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.663, 10.233], loss: 0.000066, mae: 0.004480, mean_q: 0.730452
 33646/100000: episode: 422, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.448, 10.100], loss: 0.000077, mae: 0.005211, mean_q: 0.730491
 33747/100000: episode: 423, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.654, 10.160], loss: 0.000048, mae: 0.003724, mean_q: 0.730250
 33848/100000: episode: 424, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.590, 10.100], loss: 0.000058, mae: 0.003971, mean_q: 0.730300
 33949/100000: episode: 425, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.261, 10.296], loss: 0.000074, mae: 0.004336, mean_q: 0.730111
 34050/100000: episode: 426, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.381, 10.100], loss: 0.000073, mae: 0.004426, mean_q: 0.730060
 34151/100000: episode: 427, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.650, 10.401], loss: 0.000060, mae: 0.003936, mean_q: 0.729573
 34252/100000: episode: 428, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.846, 10.100], loss: 0.000058, mae: 0.005087, mean_q: 0.729613
 34353/100000: episode: 429, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.894, 10.100], loss: 0.000067, mae: 0.004295, mean_q: 0.729409
 34454/100000: episode: 430, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.108, 10.100], loss: 0.000045, mae: 0.004204, mean_q: 0.728693
 34555/100000: episode: 431, duration: 0.622s, episode steps: 101, steps per second: 163, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.100], loss: 0.000061, mae: 0.004305, mean_q: 0.728606
 34656/100000: episode: 432, duration: 0.684s, episode steps: 101, steps per second: 148, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.756, 10.100], loss: 0.000048, mae: 0.003746, mean_q: 0.728121
 34757/100000: episode: 433, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.696, 10.100], loss: 0.000058, mae: 0.003898, mean_q: 0.727647
 34858/100000: episode: 434, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.658, 10.331], loss: 0.000084, mae: 0.004316, mean_q: 0.727374
 34959/100000: episode: 435, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.241, 10.100], loss: 0.000072, mae: 0.004445, mean_q: 0.727551
 35060/100000: episode: 436, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.371, 10.100], loss: 0.000061, mae: 0.004540, mean_q: 0.727658
 35161/100000: episode: 437, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.391, 10.100], loss: 0.000067, mae: 0.004163, mean_q: 0.727112
 35262/100000: episode: 438, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.608, 10.109], loss: 0.000049, mae: 0.003858, mean_q: 0.727078
 35363/100000: episode: 439, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.462, 10.100], loss: 0.000065, mae: 0.004732, mean_q: 0.727338
 35464/100000: episode: 440, duration: 0.677s, episode steps: 101, steps per second: 149, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.055, 10.100], loss: 0.000029, mae: 0.002370, mean_q: 0.728086
 35565/100000: episode: 441, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.203, 10.100], loss: 0.000019, mae: 0.002353, mean_q: 0.728409
 35666/100000: episode: 442, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.611, 10.271], loss: 0.000017, mae: 0.001916, mean_q: 0.728547
 35767/100000: episode: 443, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.193, 10.100], loss: 0.000007, mae: 0.001758, mean_q: 0.728383
 35868/100000: episode: 444, duration: 0.660s, episode steps: 101, steps per second: 153, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.993, 10.100], loss: 0.000005, mae: 0.001017, mean_q: 0.728189
 35969/100000: episode: 445, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.068, 10.253], loss: 0.000014, mae: 0.001614, mean_q: 0.728310
 36070/100000: episode: 446, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.942, 10.100], loss: 0.000008, mae: 0.001486, mean_q: 0.728506
 36171/100000: episode: 447, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.951, 10.304], loss: 0.000014, mae: 0.002111, mean_q: 0.728551
 36272/100000: episode: 448, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.750, 10.100], loss: 0.000015, mae: 0.002113, mean_q: 0.728608
 36373/100000: episode: 449, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.439, 10.341], loss: 0.000010, mae: 0.001720, mean_q: 0.728633
 36474/100000: episode: 450, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.908, 10.100], loss: 0.000016, mae: 0.002432, mean_q: 0.728792
 36575/100000: episode: 451, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.140, 10.100], loss: 0.000009, mae: 0.001644, mean_q: 0.728315
 36676/100000: episode: 452, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.526, 10.309], loss: 0.000007, mae: 0.001513, mean_q: 0.728715
 36777/100000: episode: 453, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.195, 10.324], loss: 0.000006, mae: 0.001287, mean_q: 0.728710
 36878/100000: episode: 454, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.858, mean reward: 0.008 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.100], loss: 0.000014, mae: 0.002000, mean_q: 0.728931
 36979/100000: episode: 455, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.957, 10.239], loss: 0.000016, mae: 0.002602, mean_q: 0.728965
 37080/100000: episode: 456, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.520, 10.330], loss: 0.000012, mae: 0.001530, mean_q: 0.728848
 37181/100000: episode: 457, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.188, 10.100], loss: 0.000012, mae: 0.002379, mean_q: 0.728924
 37282/100000: episode: 458, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.884, 10.151], loss: 0.000011, mae: 0.002217, mean_q: 0.729100
 37383/100000: episode: 459, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.473, 10.349], loss: 0.000006, mae: 0.001589, mean_q: 0.729141
 37484/100000: episode: 460, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.489, 10.107], loss: 0.000008, mae: 0.001602, mean_q: 0.729627
 37585/100000: episode: 461, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.817, 10.227], loss: 0.000015, mae: 0.002305, mean_q: 0.729344
 37686/100000: episode: 462, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.720, 10.113], loss: 0.000007, mae: 0.001636, mean_q: 0.729422
 37787/100000: episode: 463, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.933, 10.474], loss: 0.000011, mae: 0.001946, mean_q: 0.729756
 37888/100000: episode: 464, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.269, 10.100], loss: 0.000007, mae: 0.001576, mean_q: 0.729672
 37989/100000: episode: 465, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.849, 10.201], loss: 0.000016, mae: 0.002472, mean_q: 0.729659
 38090/100000: episode: 466, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.842, mean reward: 0.008 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.570, 10.169], loss: 0.000012, mae: 0.002063, mean_q: 0.729866
 38191/100000: episode: 467, duration: 0.775s, episode steps: 101, steps per second: 130, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.320, 10.137], loss: 0.000012, mae: 0.002080, mean_q: 0.730170
 38292/100000: episode: 468, duration: 0.921s, episode steps: 101, steps per second: 110, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.356, 10.239], loss: 0.000007, mae: 0.001788, mean_q: 0.730475
 38393/100000: episode: 469, duration: 1.188s, episode steps: 101, steps per second: 85, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.532, 10.309], loss: 0.000008, mae: 0.001764, mean_q: 0.730768
 38494/100000: episode: 470, duration: 0.884s, episode steps: 101, steps per second: 114, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.232, 10.100], loss: 0.000017, mae: 0.002501, mean_q: 0.730727
 38595/100000: episode: 471, duration: 0.659s, episode steps: 101, steps per second: 153, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.420, 10.470], loss: 0.000016, mae: 0.002425, mean_q: 0.731196
 38696/100000: episode: 472, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.753, 10.164], loss: 0.000011, mae: 0.002211, mean_q: 0.731526
 38797/100000: episode: 473, duration: 0.732s, episode steps: 101, steps per second: 138, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.587, 10.384], loss: 0.000013, mae: 0.002485, mean_q: 0.731357
 38898/100000: episode: 474, duration: 0.929s, episode steps: 101, steps per second: 109, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.098, 10.202], loss: 0.000011, mae: 0.001873, mean_q: 0.731944
 38999/100000: episode: 475, duration: 1.090s, episode steps: 101, steps per second: 93, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.368, 10.280], loss: 0.000010, mae: 0.001952, mean_q: 0.731864
 39100/100000: episode: 476, duration: 0.982s, episode steps: 101, steps per second: 103, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.244, 10.164], loss: 0.000014, mae: 0.002237, mean_q: 0.731932
 39201/100000: episode: 477, duration: 0.923s, episode steps: 101, steps per second: 109, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.525, 10.100], loss: 0.000016, mae: 0.001988, mean_q: 0.732164
 39302/100000: episode: 478, duration: 1.046s, episode steps: 101, steps per second: 97, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.880, 10.100], loss: 0.000009, mae: 0.001651, mean_q: 0.732370
 39403/100000: episode: 479, duration: 1.069s, episode steps: 101, steps per second: 95, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.696, 10.100], loss: 0.000010, mae: 0.001645, mean_q: 0.732751
 39504/100000: episode: 480, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.006, 10.115], loss: 0.000009, mae: 0.002293, mean_q: 0.732828
 39605/100000: episode: 481, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.910, mean reward: 0.009 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.297, 10.342], loss: 0.000007, mae: 0.001508, mean_q: 0.733072
 39706/100000: episode: 482, duration: 0.794s, episode steps: 101, steps per second: 127, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.993, 10.100], loss: 0.000012, mae: 0.001542, mean_q: 0.733386
 39807/100000: episode: 483, duration: 0.809s, episode steps: 101, steps per second: 125, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.615, 10.100], loss: 0.000019, mae: 0.002107, mean_q: 0.733977
done, took 257.053 seconds
[Info] End Importance Splitting. Falsification occurred 0 times.
