Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_14_42_05_CET/2, out/localization/mer27nov2019_14_42_05_CET/2/levels, out/localization/mer27nov2019_14_42_05_CET/2/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.193s, episode steps: 101, steps per second: 522, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.248, 10.423], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.078s, episode steps: 101, steps per second: 1303, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.739, 10.349], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.075s, episode steps: 101, steps per second: 1350, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.741, 10.144], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.077s, episode steps: 101, steps per second: 1308, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.909, 10.100], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.083s, episode steps: 101, steps per second: 1222, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.683, 10.100], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.075s, episode steps: 101, steps per second: 1341, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.817, 10.476], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.097s, episode steps: 101, steps per second: 1044, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.557, 10.406], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.113s, episode steps: 101, steps per second: 897, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.172, 10.334], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.069s, episode steps: 101, steps per second: 1474, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.789, 10.100], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.070s, episode steps: 101, steps per second: 1449, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.078, 10.159], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.080s, episode steps: 101, steps per second: 1266, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.902, 10.100], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.074s, episode steps: 101, steps per second: 1363, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.977, 10.331], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.110s, episode steps: 101, steps per second: 918, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.932, 10.380], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.069s, episode steps: 101, steps per second: 1467, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.376, 10.100], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.069s, episode steps: 101, steps per second: 1464, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.996, 10.230], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.068s, episode steps: 101, steps per second: 1475, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.100], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.070s, episode steps: 101, steps per second: 1442, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.304, 10.144], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.092s, episode steps: 101, steps per second: 1097, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.327, 10.325], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.090s, episode steps: 101, steps per second: 1123, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.908, 10.468], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.111s, episode steps: 101, steps per second: 912, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.042, 10.465], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.085s, episode steps: 101, steps per second: 1193, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.092, 10.135], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.081s, episode steps: 101, steps per second: 1248, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.337], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.071s, episode steps: 101, steps per second: 1426, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.143, 10.283], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.069s, episode steps: 101, steps per second: 1462, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.246, 10.134], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.075s, episode steps: 101, steps per second: 1349, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.012, 10.227], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.079s, episode steps: 101, steps per second: 1283, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.254, 10.267], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.070s, episode steps: 101, steps per second: 1434, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.413, 10.100], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.081s, episode steps: 101, steps per second: 1249, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.916, 10.172], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.075s, episode steps: 101, steps per second: 1338, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.434, 10.284], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.089s, episode steps: 101, steps per second: 1135, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.614, 10.100], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.070s, episode steps: 101, steps per second: 1451, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.675, 10.190], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.074s, episode steps: 101, steps per second: 1360, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.765, 10.370], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.081s, episode steps: 101, steps per second: 1242, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.941, 10.100], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.077s, episode steps: 101, steps per second: 1308, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.071, 10.100], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.070s, episode steps: 101, steps per second: 1433, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.594, 10.100], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.105s, episode steps: 101, steps per second: 964, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.990, 10.109], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.099s, episode steps: 101, steps per second: 1022, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.911, 10.160], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.087s, episode steps: 101, steps per second: 1168, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.625, 10.245], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.069s, episode steps: 101, steps per second: 1470, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.949, 10.230], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.070s, episode steps: 101, steps per second: 1452, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.353, 10.164], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.083s, episode steps: 101, steps per second: 1221, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.307, 10.100], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.093s, episode steps: 101, steps per second: 1082, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.733, 10.295], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.083s, episode steps: 101, steps per second: 1221, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.630, 10.106], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.094s, episode steps: 101, steps per second: 1078, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.839, 10.125], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.069s, episode steps: 101, steps per second: 1474, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.490, 10.100], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.068s, episode steps: 101, steps per second: 1480, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.634, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.128s, episode steps: 101, steps per second: 788, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.107, 10.215], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.079s, episode steps: 101, steps per second: 1275, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.719, 10.100], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.074s, episode steps: 101, steps per second: 1360, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.267, 10.100], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 1.010s, episode steps: 101, steps per second: 100, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.990, 10.170], loss: 0.012732, mae: 0.073586, mean_q: -0.419707
  5151/100000: episode: 51, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.100], loss: 0.007791, mae: 0.044852, mean_q: -0.379391
  5252/100000: episode: 52, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.518, 10.281], loss: 0.005403, mae: 0.035072, mean_q: -0.363198
  5353/100000: episode: 53, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.196, 10.100], loss: 0.004767, mae: 0.035383, mean_q: -0.346232
  5454/100000: episode: 54, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: 0.919, mean reward: 0.009 [0.000, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.630, 10.100], loss: 0.004854, mae: 0.039865, mean_q: -0.329832
  5555/100000: episode: 55, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.830, 10.154], loss: 0.003174, mae: 0.034446, mean_q: -0.314257
  5656/100000: episode: 56, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.652, 10.100], loss: 0.001647, mae: 0.027254, mean_q: -0.308018
  5757/100000: episode: 57, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.865, 10.105], loss: 0.001306, mae: 0.024149, mean_q: -0.291245
  5858/100000: episode: 58, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.675, 10.431], loss: 0.000697, mae: 0.018433, mean_q: -0.278982
  5959/100000: episode: 59, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.647, 10.100], loss: 0.000570, mae: 0.018289, mean_q: -0.267490
  6060/100000: episode: 60, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.654, 10.254], loss: 0.000280, mae: 0.014245, mean_q: -0.248620
  6161/100000: episode: 61, duration: 0.750s, episode steps: 101, steps per second: 135, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.469, 10.237], loss: 0.000215, mae: 0.012332, mean_q: -0.244359
  6262/100000: episode: 62, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.361, 10.100], loss: 0.000153, mae: 0.011855, mean_q: -0.246985
  6363/100000: episode: 63, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.867, 10.118], loss: 0.000154, mae: 0.012106, mean_q: -0.221359
  6464/100000: episode: 64, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.129, 10.100], loss: 0.000131, mae: 0.011675, mean_q: -0.216950
  6565/100000: episode: 65, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.315, 10.100], loss: 0.000139, mae: 0.011921, mean_q: -0.197485
  6666/100000: episode: 66, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.208, 10.188], loss: 0.000148, mae: 0.012045, mean_q: -0.188238
  6767/100000: episode: 67, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.285, 10.100], loss: 0.000136, mae: 0.011679, mean_q: -0.179477
  6868/100000: episode: 68, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.740, 10.100], loss: 0.000149, mae: 0.012083, mean_q: -0.176252
  6969/100000: episode: 69, duration: 0.651s, episode steps: 101, steps per second: 155, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.306, 10.100], loss: 0.000176, mae: 0.013429, mean_q: -0.161101
  7070/100000: episode: 70, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.091, 10.263], loss: 0.000187, mae: 0.013280, mean_q: -0.145512
  7171/100000: episode: 71, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.696, 10.195], loss: 0.000167, mae: 0.013047, mean_q: -0.153651
  7272/100000: episode: 72, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.759, 10.107], loss: 0.000175, mae: 0.013132, mean_q: -0.136374
  7373/100000: episode: 73, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.390, 10.246], loss: 0.000191, mae: 0.013430, mean_q: -0.111670
  7474/100000: episode: 74, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.911, 10.166], loss: 0.000149, mae: 0.012210, mean_q: -0.117347
  7575/100000: episode: 75, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.476, 10.100], loss: 0.000156, mae: 0.012594, mean_q: -0.117323
  7676/100000: episode: 76, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.637, 10.100], loss: 0.000198, mae: 0.013725, mean_q: -0.079550
  7777/100000: episode: 77, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.211, 10.156], loss: 0.000165, mae: 0.012700, mean_q: -0.094445
  7878/100000: episode: 78, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.894, 10.100], loss: 0.000182, mae: 0.013226, mean_q: -0.076080
  7979/100000: episode: 79, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.718, 10.100], loss: 0.000165, mae: 0.012888, mean_q: -0.064567
  8080/100000: episode: 80, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.054, 10.221], loss: 0.000151, mae: 0.012017, mean_q: -0.046740
  8181/100000: episode: 81, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.055, 10.100], loss: 0.000199, mae: 0.014533, mean_q: -0.051108
  8282/100000: episode: 82, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.259, 10.317], loss: 0.000145, mae: 0.012215, mean_q: -0.035876
  8383/100000: episode: 83, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.183, 10.100], loss: 0.000196, mae: 0.014053, mean_q: -0.028004
  8484/100000: episode: 84, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.155, 10.276], loss: 0.000157, mae: 0.012382, mean_q: -0.007872
  8585/100000: episode: 85, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.108, 10.100], loss: 0.000158, mae: 0.012715, mean_q: -0.003614
  8686/100000: episode: 86, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.319, 10.354], loss: 0.000167, mae: 0.013022, mean_q: -0.003103
  8787/100000: episode: 87, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.190, 10.127], loss: 0.000141, mae: 0.011856, mean_q: 0.013438
  8888/100000: episode: 88, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.729, 10.100], loss: 0.000175, mae: 0.013351, mean_q: 0.023062
  8989/100000: episode: 89, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.483, 10.100], loss: 0.000140, mae: 0.012131, mean_q: 0.018525
  9090/100000: episode: 90, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.514, 10.395], loss: 0.000126, mae: 0.011349, mean_q: 0.046952
  9191/100000: episode: 91, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.701, 10.100], loss: 0.000112, mae: 0.010924, mean_q: 0.071795
  9292/100000: episode: 92, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.946, 10.443], loss: 0.000151, mae: 0.012257, mean_q: 0.056980
  9393/100000: episode: 93, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.127, 10.100], loss: 0.000124, mae: 0.011464, mean_q: 0.082752
  9494/100000: episode: 94, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.724, 10.291], loss: 0.000117, mae: 0.010950, mean_q: 0.075583
  9595/100000: episode: 95, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.493, 10.228], loss: 0.000120, mae: 0.011112, mean_q: 0.098388
  9696/100000: episode: 96, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.629, 10.100], loss: 0.000145, mae: 0.012407, mean_q: 0.129070
  9797/100000: episode: 97, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.276, 10.167], loss: 0.000128, mae: 0.011501, mean_q: 0.109488
  9898/100000: episode: 98, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.507, 10.139], loss: 0.000113, mae: 0.010693, mean_q: 0.126979
  9999/100000: episode: 99, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.100], loss: 0.000121, mae: 0.010753, mean_q: 0.144798
 10100/100000: episode: 100, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.944, 10.148], loss: 0.000112, mae: 0.010360, mean_q: 0.153842
 10201/100000: episode: 101, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.360, 10.159], loss: 0.000120, mae: 0.010584, mean_q: 0.160476
 10302/100000: episode: 102, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.550, 10.100], loss: 0.000126, mae: 0.011307, mean_q: 0.165954
 10403/100000: episode: 103, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.320, 10.119], loss: 0.000142, mae: 0.011664, mean_q: 0.201079
 10504/100000: episode: 104, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.878, mean reward: 0.009 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.895, 10.297], loss: 0.000126, mae: 0.011226, mean_q: 0.198071
 10605/100000: episode: 105, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.407, 10.257], loss: 0.000103, mae: 0.009527, mean_q: 0.209016
 10706/100000: episode: 106, duration: 0.599s, episode steps: 101, steps per second: 168, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.341, 10.164], loss: 0.000137, mae: 0.011687, mean_q: 0.219725
 10807/100000: episode: 107, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.316, 10.100], loss: 0.000110, mae: 0.010379, mean_q: 0.223005
 10908/100000: episode: 108, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.493, 10.100], loss: 0.000106, mae: 0.010109, mean_q: 0.243824
 11009/100000: episode: 109, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.919, 10.100], loss: 0.000119, mae: 0.010311, mean_q: 0.270566
 11110/100000: episode: 110, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.377, 10.132], loss: 0.000119, mae: 0.010791, mean_q: 0.258160
 11211/100000: episode: 111, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.560, 10.100], loss: 0.000095, mae: 0.009608, mean_q: 0.270289
 11312/100000: episode: 112, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.783, 10.100], loss: 0.000086, mae: 0.008759, mean_q: 0.309226
 11413/100000: episode: 113, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.121, 10.125], loss: 0.000123, mae: 0.010884, mean_q: 0.308164
 11514/100000: episode: 114, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.919, 10.100], loss: 0.000096, mae: 0.009195, mean_q: 0.303348
 11615/100000: episode: 115, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.306, 10.100], loss: 0.000079, mae: 0.008501, mean_q: 0.339343
 11716/100000: episode: 116, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.665, 10.100], loss: 0.000061, mae: 0.007646, mean_q: 0.348445
 11817/100000: episode: 117, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.545, 10.135], loss: 0.000065, mae: 0.007735, mean_q: 0.369839
 11918/100000: episode: 118, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.100], loss: 0.000059, mae: 0.007515, mean_q: 0.356111
 12019/100000: episode: 119, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.469, 10.100], loss: 0.000067, mae: 0.008364, mean_q: 0.371607
 12120/100000: episode: 120, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.100], loss: 0.000049, mae: 0.006582, mean_q: 0.386460
 12221/100000: episode: 121, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.173, 10.211], loss: 0.000047, mae: 0.006918, mean_q: 0.391670
 12322/100000: episode: 122, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.886, mean reward: 0.009 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.774, 10.530], loss: 0.000046, mae: 0.006731, mean_q: 0.415986
 12423/100000: episode: 123, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.549, 10.100], loss: 0.000075, mae: 0.008793, mean_q: 0.432148
 12524/100000: episode: 124, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.252, 10.372], loss: 0.000047, mae: 0.006601, mean_q: 0.440658
 12625/100000: episode: 125, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.269, 10.356], loss: 0.000055, mae: 0.007190, mean_q: 0.442855
 12726/100000: episode: 126, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.715, 10.100], loss: 0.000051, mae: 0.006835, mean_q: 0.468174
 12827/100000: episode: 127, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.052, 10.100], loss: 0.000046, mae: 0.006404, mean_q: 0.469213
 12928/100000: episode: 128, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.065, 10.267], loss: 0.000041, mae: 0.006106, mean_q: 0.495970
 13029/100000: episode: 129, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.071, 10.128], loss: 0.000047, mae: 0.006555, mean_q: 0.515214
 13130/100000: episode: 130, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.196, 10.129], loss: 0.000041, mae: 0.006376, mean_q: 0.522845
 13231/100000: episode: 131, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.378, 10.145], loss: 0.000044, mae: 0.006352, mean_q: 0.526096
 13332/100000: episode: 132, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.511, 10.100], loss: 0.000051, mae: 0.007493, mean_q: 0.545656
 13433/100000: episode: 133, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.751, 10.100], loss: 0.000036, mae: 0.005885, mean_q: 0.554324
 13534/100000: episode: 134, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.400, 10.167], loss: 0.000040, mae: 0.006206, mean_q: 0.564460
 13635/100000: episode: 135, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.112, 10.104], loss: 0.000034, mae: 0.005629, mean_q: 0.567950
 13736/100000: episode: 136, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.469, 10.254], loss: 0.000035, mae: 0.005893, mean_q: 0.588847
 13837/100000: episode: 137, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.940, 10.100], loss: 0.000036, mae: 0.005777, mean_q: 0.587330
 13938/100000: episode: 138, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.734, 10.258], loss: 0.000030, mae: 0.005373, mean_q: 0.603213
 14039/100000: episode: 139, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.037, 10.100], loss: 0.000038, mae: 0.005986, mean_q: 0.624420
 14140/100000: episode: 140, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.010, 10.154], loss: 0.000034, mae: 0.005705, mean_q: 0.629114
 14241/100000: episode: 141, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.853, 10.100], loss: 0.000031, mae: 0.005240, mean_q: 0.641350
 14342/100000: episode: 142, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.477, 10.100], loss: 0.000027, mae: 0.004673, mean_q: 0.647972
 14443/100000: episode: 143, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.133, 10.209], loss: 0.000027, mae: 0.004667, mean_q: 0.659920
 14544/100000: episode: 144, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.191, 10.189], loss: 0.000034, mae: 0.005648, mean_q: 0.668858
 14645/100000: episode: 145, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.950, 10.229], loss: 0.000047, mae: 0.006671, mean_q: 0.679952
 14746/100000: episode: 146, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.709, 10.100], loss: 0.000032, mae: 0.005290, mean_q: 0.685874
 14847/100000: episode: 147, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.594, 10.367], loss: 0.000039, mae: 0.005695, mean_q: 0.692330
 14948/100000: episode: 148, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.438, 10.159], loss: 0.000039, mae: 0.005793, mean_q: 0.700842
 15049/100000: episode: 149, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.533, 10.193], loss: 0.000033, mae: 0.005697, mean_q: 0.712167
 15150/100000: episode: 150, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.604, 10.118], loss: 0.000028, mae: 0.004680, mean_q: 0.717176
 15251/100000: episode: 151, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.100], loss: 0.000026, mae: 0.004334, mean_q: 0.722826
 15352/100000: episode: 152, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.578, 10.100], loss: 0.000031, mae: 0.005078, mean_q: 0.731398
 15453/100000: episode: 153, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.100], loss: 0.000032, mae: 0.005229, mean_q: 0.737049
 15554/100000: episode: 154, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.513, 10.100], loss: 0.000026, mae: 0.004319, mean_q: 0.740388
 15655/100000: episode: 155, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.503, 10.100], loss: 0.000029, mae: 0.004634, mean_q: 0.744067
 15756/100000: episode: 156, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.171, 10.100], loss: 0.000027, mae: 0.004957, mean_q: 0.747781
 15857/100000: episode: 157, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.033, 10.189], loss: 0.000022, mae: 0.004024, mean_q: 0.751315
 15958/100000: episode: 158, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.707, 10.141], loss: 0.000029, mae: 0.004924, mean_q: 0.753315
 16059/100000: episode: 159, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.927, 10.330], loss: 0.000029, mae: 0.005158, mean_q: 0.757278
 16160/100000: episode: 160, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.631, 10.230], loss: 0.000016, mae: 0.003744, mean_q: 0.758538
 16261/100000: episode: 161, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.590, 10.417], loss: 0.000022, mae: 0.004114, mean_q: 0.760122
 16362/100000: episode: 162, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.658, 10.251], loss: 0.000030, mae: 0.005195, mean_q: 0.762274
 16463/100000: episode: 163, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.658, mean reward: 0.007 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.306, 10.134], loss: 0.000027, mae: 0.004627, mean_q: 0.763472
 16564/100000: episode: 164, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.367, 10.100], loss: 0.000028, mae: 0.004463, mean_q: 0.763754
 16665/100000: episode: 165, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.852, 10.100], loss: 0.000034, mae: 0.005094, mean_q: 0.764507
 16766/100000: episode: 166, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.243, 10.100], loss: 0.000037, mae: 0.005159, mean_q: 0.765272
 16867/100000: episode: 167, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.001, 10.149], loss: 0.000045, mae: 0.006103, mean_q: 0.765281
 16968/100000: episode: 168, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.384, 10.503], loss: 0.000018, mae: 0.003762, mean_q: 0.766710
 17069/100000: episode: 169, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.386, 10.430], loss: 0.000048, mae: 0.006661, mean_q: 0.767535
 17170/100000: episode: 170, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.641, 10.297], loss: 0.000033, mae: 0.005078, mean_q: 0.767362
 17271/100000: episode: 171, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.327, 10.100], loss: 0.000021, mae: 0.003932, mean_q: 0.767646
 17372/100000: episode: 172, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.581, 10.203], loss: 0.000018, mae: 0.003146, mean_q: 0.767678
 17473/100000: episode: 173, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.371, 10.100], loss: 0.000019, mae: 0.003570, mean_q: 0.767486
 17574/100000: episode: 174, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.512, 10.218], loss: 0.000023, mae: 0.003995, mean_q: 0.767700
 17675/100000: episode: 175, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.264, 10.100], loss: 0.000020, mae: 0.003996, mean_q: 0.767175
 17776/100000: episode: 176, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.377, 10.100], loss: 0.000026, mae: 0.004386, mean_q: 0.767530
 17877/100000: episode: 177, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.009, 10.192], loss: 0.000019, mae: 0.003387, mean_q: 0.766780
 17978/100000: episode: 178, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.512, 10.303], loss: 0.000024, mae: 0.003811, mean_q: 0.767337
 18079/100000: episode: 179, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.113, 10.426], loss: 0.000024, mae: 0.004308, mean_q: 0.766625
 18180/100000: episode: 180, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.939, 10.248], loss: 0.000027, mae: 0.004756, mean_q: 0.766258
 18281/100000: episode: 181, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.655, 10.137], loss: 0.000023, mae: 0.004204, mean_q: 0.765700
 18382/100000: episode: 182, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.730, 10.100], loss: 0.000023, mae: 0.004072, mean_q: 0.765050
 18483/100000: episode: 183, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.600, 10.100], loss: 0.000020, mae: 0.003449, mean_q: 0.763750
 18584/100000: episode: 184, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.241, 10.186], loss: 0.000022, mae: 0.003419, mean_q: 0.763763
 18685/100000: episode: 185, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.511, 10.126], loss: 0.000022, mae: 0.004165, mean_q: 0.763167
 18786/100000: episode: 186, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.738, 10.204], loss: 0.000027, mae: 0.004208, mean_q: 0.762241
 18887/100000: episode: 187, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.217, 10.100], loss: 0.000024, mae: 0.004223, mean_q: 0.762020
 18988/100000: episode: 188, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.103, 10.127], loss: 0.000018, mae: 0.003458, mean_q: 0.761019
 19089/100000: episode: 189, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.994, 10.169], loss: 0.000036, mae: 0.005364, mean_q: 0.760783
 19190/100000: episode: 190, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.470, 10.100], loss: 0.000022, mae: 0.004294, mean_q: 0.760217
 19291/100000: episode: 191, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.889, mean reward: 0.009 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.115, 10.263], loss: 0.000019, mae: 0.003797, mean_q: 0.759757
 19392/100000: episode: 192, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.920, mean reward: 0.009 [0.000, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.373, 10.619], loss: 0.000021, mae: 0.003718, mean_q: 0.759434
 19493/100000: episode: 193, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.951, 10.339], loss: 0.000040, mae: 0.005441, mean_q: 0.759319
 19594/100000: episode: 194, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.101, 10.100], loss: 0.000028, mae: 0.004261, mean_q: 0.758615
 19695/100000: episode: 195, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.308, 10.100], loss: 0.000046, mae: 0.005450, mean_q: 0.758380
 19796/100000: episode: 196, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.029, 10.222], loss: 0.000025, mae: 0.003868, mean_q: 0.757733
 19897/100000: episode: 197, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.086, 10.215], loss: 0.000040, mae: 0.004829, mean_q: 0.758025
 19998/100000: episode: 198, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.847, 10.100], loss: 0.000035, mae: 0.004706, mean_q: 0.757356
 20099/100000: episode: 199, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.090, 10.100], loss: 0.000033, mae: 0.005160, mean_q: 0.757603
[Info] 1-TH LEVEL FOUND: 0.7862579226493835, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.645s, episode steps: 101, steps per second: 18, episode reward: 0.917, mean reward: 0.009 [0.000, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.591, 10.100], loss: 0.000028, mae: 0.004191, mean_q: 0.757196
 20298/100000: episode: 201, duration: 0.610s, episode steps: 98, steps per second: 161, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-1.553, 10.347], loss: 0.000038, mae: 0.004139, mean_q: 0.756995
 20316/100000: episode: 202, duration: 0.118s, episode steps: 18, steps per second: 152, episode reward: 0.799, mean reward: 0.044 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.471], loss: 0.000021, mae: 0.003429, mean_q: 0.757786
 20334/100000: episode: 203, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 0.832, mean reward: 0.046 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.337], loss: 0.000045, mae: 0.004017, mean_q: 0.757621
 20352/100000: episode: 204, duration: 0.120s, episode steps: 18, steps per second: 150, episode reward: 0.861, mean reward: 0.048 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.142, 10.464], loss: 0.000022, mae: 0.004787, mean_q: 0.757624
 20450/100000: episode: 205, duration: 0.552s, episode steps: 98, steps per second: 178, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.679, 10.340], loss: 0.000054, mae: 0.005552, mean_q: 0.757219
 20461/100000: episode: 206, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 0.700, mean reward: 0.064 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.331], loss: 0.000027, mae: 0.004521, mean_q: 0.755767
 20476/100000: episode: 207, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 0.812, mean reward: 0.054 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.544], loss: 0.000047, mae: 0.005823, mean_q: 0.756905
 20574/100000: episode: 208, duration: 0.576s, episode steps: 98, steps per second: 170, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.648, 10.100], loss: 0.000031, mae: 0.004607, mean_q: 0.757097
 20672/100000: episode: 209, duration: 0.602s, episode steps: 98, steps per second: 163, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.917, 10.125], loss: 0.000028, mae: 0.004020, mean_q: 0.757153
 20770/100000: episode: 210, duration: 0.565s, episode steps: 98, steps per second: 173, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.709, 10.100], loss: 0.000033, mae: 0.004854, mean_q: 0.756973
 20788/100000: episode: 211, duration: 0.114s, episode steps: 18, steps per second: 159, episode reward: 0.791, mean reward: 0.044 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.456], loss: 0.000036, mae: 0.005140, mean_q: 0.757873
 20886/100000: episode: 212, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.744, 10.100], loss: 0.000029, mae: 0.004527, mean_q: 0.757411
 20902/100000: episode: 213, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 0.889, mean reward: 0.056 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.584, 10.497], loss: 0.000046, mae: 0.005053, mean_q: 0.757225
 21000/100000: episode: 214, duration: 0.585s, episode steps: 98, steps per second: 168, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.886, 10.100], loss: 0.000047, mae: 0.004565, mean_q: 0.757785
 21020/100000: episode: 215, duration: 0.129s, episode steps: 20, steps per second: 155, episode reward: 0.816, mean reward: 0.041 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.088, 10.430], loss: 0.000053, mae: 0.005188, mean_q: 0.758085
 21038/100000: episode: 216, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 0.812, mean reward: 0.045 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.272, 10.553], loss: 0.000013, mae: 0.004034, mean_q: 0.757628
 21136/100000: episode: 217, duration: 0.588s, episode steps: 98, steps per second: 167, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.626, 10.100], loss: 0.000041, mae: 0.004711, mean_q: 0.758055
 21154/100000: episode: 218, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 0.812, mean reward: 0.045 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.096, 10.302], loss: 0.000034, mae: 0.004477, mean_q: 0.758710
 21172/100000: episode: 219, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 0.918, mean reward: 0.051 [0.000, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.260, 10.406], loss: 0.000032, mae: 0.005558, mean_q: 0.756963
 21190/100000: episode: 220, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 0.947, mean reward: 0.053 [0.000, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.535], loss: 0.000028, mae: 0.003554, mean_q: 0.757906
 21288/100000: episode: 221, duration: 0.609s, episode steps: 98, steps per second: 161, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.979, 10.188], loss: 0.000045, mae: 0.005205, mean_q: 0.758348
 21386/100000: episode: 222, duration: 0.568s, episode steps: 98, steps per second: 173, episode reward: 0.751, mean reward: 0.008 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.565, 10.100], loss: 0.000032, mae: 0.004369, mean_q: 0.758388
 21402/100000: episode: 223, duration: 0.111s, episode steps: 16, steps per second: 145, episode reward: 0.869, mean reward: 0.054 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.538, 10.499], loss: 0.000083, mae: 0.006024, mean_q: 0.759555
 21420/100000: episode: 224, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 0.795, mean reward: 0.044 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.252], loss: 0.000053, mae: 0.005360, mean_q: 0.758123
 21435/100000: episode: 225, duration: 0.098s, episode steps: 15, steps per second: 154, episode reward: 0.717, mean reward: 0.048 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.310], loss: 0.000057, mae: 0.005691, mean_q: 0.758624
 21450/100000: episode: 226, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 0.784, mean reward: 0.052 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.485], loss: 0.000050, mae: 0.004828, mean_q: 0.758632
 21470/100000: episode: 227, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 0.767, mean reward: 0.038 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.260, 10.258], loss: 0.000059, mae: 0.004955, mean_q: 0.759320
 21488/100000: episode: 228, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 0.745, mean reward: 0.041 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.482], loss: 0.000020, mae: 0.004836, mean_q: 0.758221
 21504/100000: episode: 229, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 0.994, mean reward: 0.062 [0.000, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.661], loss: 0.000026, mae: 0.004639, mean_q: 0.758653
 21602/100000: episode: 230, duration: 0.585s, episode steps: 98, steps per second: 167, episode reward: 0.755, mean reward: 0.008 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.202, 10.172], loss: 0.000043, mae: 0.005169, mean_q: 0.759160
 21617/100000: episode: 231, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 0.844, mean reward: 0.056 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.624, 10.429], loss: 0.000039, mae: 0.004562, mean_q: 0.760576
 21715/100000: episode: 232, duration: 0.586s, episode steps: 98, steps per second: 167, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-1.228, 10.183], loss: 0.000077, mae: 0.006209, mean_q: 0.760163
 21731/100000: episode: 233, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 0.866, mean reward: 0.054 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.463, 10.450], loss: 0.000044, mae: 0.004519, mean_q: 0.760670
 21749/100000: episode: 234, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 0.928, mean reward: 0.052 [0.000, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.484], loss: 0.000035, mae: 0.004495, mean_q: 0.760675
 21847/100000: episode: 235, duration: 0.556s, episode steps: 98, steps per second: 176, episode reward: 0.757, mean reward: 0.008 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.486, 10.100], loss: 0.000026, mae: 0.003737, mean_q: 0.760382
 21867/100000: episode: 236, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 0.842, mean reward: 0.042 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.510, 10.441], loss: 0.000033, mae: 0.003597, mean_q: 0.761641
 21883/100000: episode: 237, duration: 0.110s, episode steps: 16, steps per second: 145, episode reward: 0.773, mean reward: 0.048 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.426], loss: 0.000050, mae: 0.004552, mean_q: 0.761614
 21894/100000: episode: 238, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 0.729, mean reward: 0.066 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.462], loss: 0.000094, mae: 0.006122, mean_q: 0.761621
 21914/100000: episode: 239, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 0.834, mean reward: 0.042 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.518], loss: 0.000039, mae: 0.004964, mean_q: 0.761485
 21925/100000: episode: 240, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 0.803, mean reward: 0.073 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.974, 10.247], loss: 0.000053, mae: 0.005041, mean_q: 0.760109
 21943/100000: episode: 241, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 0.841, mean reward: 0.047 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.325, 10.526], loss: 0.000049, mae: 0.004092, mean_q: 0.761964
 21963/100000: episode: 242, duration: 0.132s, episode steps: 20, steps per second: 151, episode reward: 0.858, mean reward: 0.043 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.154, 10.509], loss: 0.000096, mae: 0.007728, mean_q: 0.762074
 21974/100000: episode: 243, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 0.744, mean reward: 0.068 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.515, 10.438], loss: 0.000059, mae: 0.007827, mean_q: 0.759163
 22072/100000: episode: 244, duration: 0.571s, episode steps: 98, steps per second: 172, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.113, 10.100], loss: 0.000039, mae: 0.004809, mean_q: 0.761778
 22170/100000: episode: 245, duration: 0.569s, episode steps: 98, steps per second: 172, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.988, 10.100], loss: 0.000032, mae: 0.003599, mean_q: 0.762246
 22188/100000: episode: 246, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 0.764, mean reward: 0.042 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.163, 10.357], loss: 0.000038, mae: 0.003557, mean_q: 0.760056
 22206/100000: episode: 247, duration: 0.119s, episode steps: 18, steps per second: 152, episode reward: 0.831, mean reward: 0.046 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.190, 10.299], loss: 0.000015, mae: 0.002934, mean_q: 0.762263
 22304/100000: episode: 248, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.936, 10.143], loss: 0.000056, mae: 0.006340, mean_q: 0.762503
 22402/100000: episode: 249, duration: 0.568s, episode steps: 98, steps per second: 173, episode reward: 0.854, mean reward: 0.009 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.375, 10.100], loss: 0.000031, mae: 0.003841, mean_q: 0.762879
 22422/100000: episode: 250, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 0.837, mean reward: 0.042 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.061, 10.445], loss: 0.000018, mae: 0.003665, mean_q: 0.762147
 22442/100000: episode: 251, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 0.816, mean reward: 0.041 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.834, 10.349], loss: 0.000082, mae: 0.006926, mean_q: 0.763273
 22460/100000: episode: 252, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 0.819, mean reward: 0.045 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.309, 10.558], loss: 0.000074, mae: 0.007944, mean_q: 0.764386
 22558/100000: episode: 253, duration: 0.593s, episode steps: 98, steps per second: 165, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.626, 10.136], loss: 0.000044, mae: 0.004283, mean_q: 0.764231
 22574/100000: episode: 254, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 0.775, mean reward: 0.048 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.185, 10.432], loss: 0.000048, mae: 0.005480, mean_q: 0.763996
 22592/100000: episode: 255, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 0.797, mean reward: 0.044 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.428, 10.463], loss: 0.000025, mae: 0.004502, mean_q: 0.763026
 22690/100000: episode: 256, duration: 0.588s, episode steps: 98, steps per second: 167, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.510, 10.100], loss: 0.000039, mae: 0.004286, mean_q: 0.764263
 22708/100000: episode: 257, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 0.784, mean reward: 0.044 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.407], loss: 0.000061, mae: 0.005314, mean_q: 0.766461
 22806/100000: episode: 258, duration: 0.569s, episode steps: 98, steps per second: 172, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.175, 10.100], loss: 0.000047, mae: 0.004199, mean_q: 0.764528
 22817/100000: episode: 259, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 0.732, mean reward: 0.067 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.440], loss: 0.000037, mae: 0.004007, mean_q: 0.765443
 22837/100000: episode: 260, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 0.757, mean reward: 0.038 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.423], loss: 0.000021, mae: 0.004096, mean_q: 0.764283
 22852/100000: episode: 261, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 0.744, mean reward: 0.050 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.870, 10.468], loss: 0.000048, mae: 0.005367, mean_q: 0.765196
 22867/100000: episode: 262, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 0.797, mean reward: 0.053 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.210, 10.488], loss: 0.000133, mae: 0.010148, mean_q: 0.765897
 22965/100000: episode: 263, duration: 0.562s, episode steps: 98, steps per second: 174, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.193, 10.297], loss: 0.000049, mae: 0.004938, mean_q: 0.766219
 23063/100000: episode: 264, duration: 0.557s, episode steps: 98, steps per second: 176, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.470, 10.100], loss: 0.000043, mae: 0.004690, mean_q: 0.765872
 23161/100000: episode: 265, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 0.917, mean reward: 0.009 [0.000, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.007, 10.100], loss: 0.000033, mae: 0.005231, mean_q: 0.766488
 23179/100000: episode: 266, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 0.826, mean reward: 0.046 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.588], loss: 0.000047, mae: 0.004429, mean_q: 0.766443
 23197/100000: episode: 267, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 0.775, mean reward: 0.043 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.924, 10.385], loss: 0.000059, mae: 0.005675, mean_q: 0.767196
 23215/100000: episode: 268, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 0.756, mean reward: 0.042 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.462], loss: 0.000061, mae: 0.006633, mean_q: 0.766058
 23230/100000: episode: 269, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 0.774, mean reward: 0.052 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.293, 10.511], loss: 0.000075, mae: 0.005193, mean_q: 0.767632
 23248/100000: episode: 270, duration: 0.116s, episode steps: 18, steps per second: 156, episode reward: 0.837, mean reward: 0.046 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.182, 10.409], loss: 0.000072, mae: 0.005828, mean_q: 0.767917
 23346/100000: episode: 271, duration: 0.560s, episode steps: 98, steps per second: 175, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-0.569, 10.303], loss: 0.000056, mae: 0.005383, mean_q: 0.768186
 23444/100000: episode: 272, duration: 0.597s, episode steps: 98, steps per second: 164, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.326, 10.100], loss: 0.000041, mae: 0.004312, mean_q: 0.767741
 23542/100000: episode: 273, duration: 0.601s, episode steps: 98, steps per second: 163, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.869, 10.280], loss: 0.000034, mae: 0.003700, mean_q: 0.768444
 23562/100000: episode: 274, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 0.837, mean reward: 0.042 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.691, 10.394], loss: 0.000045, mae: 0.004106, mean_q: 0.768306
 23582/100000: episode: 275, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 0.858, mean reward: 0.043 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.509, 10.564], loss: 0.000079, mae: 0.005138, mean_q: 0.767942
 23680/100000: episode: 276, duration: 0.585s, episode steps: 98, steps per second: 167, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-1.707, 10.100], loss: 0.000031, mae: 0.003992, mean_q: 0.768765
 23698/100000: episode: 277, duration: 0.118s, episode steps: 18, steps per second: 152, episode reward: 0.835, mean reward: 0.046 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.374, 10.495], loss: 0.000034, mae: 0.003261, mean_q: 0.767603
 23796/100000: episode: 278, duration: 0.568s, episode steps: 98, steps per second: 172, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-1.056, 10.100], loss: 0.000042, mae: 0.005083, mean_q: 0.768922
 23807/100000: episode: 279, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 0.705, mean reward: 0.064 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.183, 10.369], loss: 0.000046, mae: 0.006221, mean_q: 0.769030
 23825/100000: episode: 280, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 0.706, mean reward: 0.039 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.339, 10.310], loss: 0.000036, mae: 0.004154, mean_q: 0.770239
 23923/100000: episode: 281, duration: 0.587s, episode steps: 98, steps per second: 167, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.910, 10.100], loss: 0.000049, mae: 0.005002, mean_q: 0.769584
 23941/100000: episode: 282, duration: 0.118s, episode steps: 18, steps per second: 153, episode reward: 0.820, mean reward: 0.046 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.244, 10.316], loss: 0.000091, mae: 0.006679, mean_q: 0.769654
 23956/100000: episode: 283, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 0.756, mean reward: 0.050 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.513], loss: 0.000062, mae: 0.006118, mean_q: 0.770276
 23972/100000: episode: 284, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 0.821, mean reward: 0.051 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.665, 10.414], loss: 0.000023, mae: 0.004908, mean_q: 0.769539
 23992/100000: episode: 285, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 0.831, mean reward: 0.042 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.742, 10.558], loss: 0.000022, mae: 0.003575, mean_q: 0.769043
 24012/100000: episode: 286, duration: 0.133s, episode steps: 20, steps per second: 150, episode reward: 0.907, mean reward: 0.045 [0.000, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.001, 10.433], loss: 0.000037, mae: 0.004263, mean_q: 0.769692
 24110/100000: episode: 287, duration: 0.552s, episode steps: 98, steps per second: 177, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.042, 10.100], loss: 0.000057, mae: 0.004776, mean_q: 0.770298
 24208/100000: episode: 288, duration: 0.586s, episode steps: 98, steps per second: 167, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.850, 10.100], loss: 0.000030, mae: 0.003824, mean_q: 0.769879
 24224/100000: episode: 289, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 0.885, mean reward: 0.055 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.586], loss: 0.000035, mae: 0.003139, mean_q: 0.770036
[Info] 2-TH LEVEL FOUND: 0.8211775422096252, Considering 10/100 traces
 24322/100000: episode: 290, duration: 5.181s, episode steps: 98, steps per second: 19, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-1.839, 10.100], loss: 0.000048, mae: 0.005329, mean_q: 0.770666
 24336/100000: episode: 291, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 0.829, mean reward: 0.059 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.923, 10.538], loss: 0.000052, mae: 0.005443, mean_q: 0.771870
 24350/100000: episode: 292, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 0.919, mean reward: 0.066 [0.000, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-1.453, 10.549], loss: 0.000016, mae: 0.002873, mean_q: 0.769967
 24364/100000: episode: 293, duration: 0.095s, episode steps: 14, steps per second: 147, episode reward: 0.847, mean reward: 0.060 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.723, 10.628], loss: 0.000052, mae: 0.003955, mean_q: 0.771226
 24377/100000: episode: 294, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 0.794, mean reward: 0.061 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.553], loss: 0.000050, mae: 0.003736, mean_q: 0.772464
 24384/100000: episode: 295, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 0.771, mean reward: 0.110 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.456], loss: 0.000100, mae: 0.008294, mean_q: 0.772407
 24396/100000: episode: 296, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 0.846, mean reward: 0.070 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.268, 10.444], loss: 0.000067, mae: 0.006018, mean_q: 0.772732
 24403/100000: episode: 297, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 0.883, mean reward: 0.126 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.658], loss: 0.000065, mae: 0.006239, mean_q: 0.772175
 24410/100000: episode: 298, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 0.703, mean reward: 0.100 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.445], loss: 0.000043, mae: 0.007396, mean_q: 0.770023
 24424/100000: episode: 299, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 0.852, mean reward: 0.061 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.132, 10.546], loss: 0.000034, mae: 0.006359, mean_q: 0.771326
 24437/100000: episode: 300, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 0.838, mean reward: 0.064 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.118, 10.427], loss: 0.000019, mae: 0.003622, mean_q: 0.770741
 24450/100000: episode: 301, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 0.795, mean reward: 0.061 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-1.060, 10.508], loss: 0.000009, mae: 0.002914, mean_q: 0.770734
 24464/100000: episode: 302, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 0.920, mean reward: 0.066 [0.000, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.287, 10.428], loss: 0.000046, mae: 0.004578, mean_q: 0.772401
[Info] FALSIFICATION!
 24478/100000: episode: 303, duration: 0.582s, episode steps: 14, steps per second: 24, episode reward: 1.024, mean reward: 0.073 [0.000, 1.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.541], loss: 0.000131, mae: 0.007659, mean_q: 0.773670
 24489/100000: episode: 304, duration: 0.082s, episode steps: 11, steps per second: 134, episode reward: 0.851, mean reward: 0.077 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.135, 10.432], loss: 0.000124, mae: 0.008101, mean_q: 0.775021
 24496/100000: episode: 305, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 0.886, mean reward: 0.127 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.426], loss: 0.000117, mae: 0.004473, mean_q: 0.773531
 24503/100000: episode: 306, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 0.801, mean reward: 0.114 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.566], loss: 0.000023, mae: 0.004008, mean_q: 0.773096
 24515/100000: episode: 307, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 0.919, mean reward: 0.077 [0.000, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.323, 10.508], loss: 0.000024, mae: 0.003101, mean_q: 0.771271
 24527/100000: episode: 308, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 0.840, mean reward: 0.070 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.458], loss: 0.000055, mae: 0.005639, mean_q: 0.772861
 24541/100000: episode: 309, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 0.850, mean reward: 0.061 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.555], loss: 0.000082, mae: 0.005148, mean_q: 0.775528
 24547/100000: episode: 310, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 0.804, mean reward: 0.134 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.428], loss: 0.000009, mae: 0.003405, mean_q: 0.771684
 24559/100000: episode: 311, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 0.851, mean reward: 0.071 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.632], loss: 0.000065, mae: 0.004627, mean_q: 0.773992
 24573/100000: episode: 312, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 0.903, mean reward: 0.064 [0.000, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.620], loss: 0.000072, mae: 0.005638, mean_q: 0.774270
 24586/100000: episode: 313, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 0.874, mean reward: 0.067 [0.000, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.625], loss: 0.000036, mae: 0.005155, mean_q: 0.772721
 24592/100000: episode: 314, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 0.828, mean reward: 0.138 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.497], loss: 0.000176, mae: 0.009227, mean_q: 0.776093
 24598/100000: episode: 315, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 0.806, mean reward: 0.134 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.503], loss: 0.000033, mae: 0.005490, mean_q: 0.774642
 24612/100000: episode: 316, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 0.895, mean reward: 0.064 [0.000, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.580], loss: 0.000047, mae: 0.005333, mean_q: 0.773279
 24623/100000: episode: 317, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 0.867, mean reward: 0.079 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.502], loss: 0.000101, mae: 0.005220, mean_q: 0.775115
 24636/100000: episode: 318, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 0.923, mean reward: 0.071 [0.000, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.661], loss: 0.000028, mae: 0.005057, mean_q: 0.774634
 24649/100000: episode: 319, duration: 0.087s, episode steps: 13, steps per second: 150, episode reward: 0.771, mean reward: 0.059 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.500, 10.359], loss: 0.000049, mae: 0.005947, mean_q: 0.774357
 24661/100000: episode: 320, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 0.813, mean reward: 0.068 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.421, 10.437], loss: 0.000062, mae: 0.005070, mean_q: 0.776303
 24667/100000: episode: 321, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 0.827, mean reward: 0.138 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.587], loss: 0.000049, mae: 0.005614, mean_q: 0.773667
 24674/100000: episode: 322, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.769, mean reward: 0.110 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.514], loss: 0.000055, mae: 0.005513, mean_q: 0.774284
 24686/100000: episode: 323, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 0.877, mean reward: 0.073 [0.000, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.531, 10.465], loss: 0.000164, mae: 0.010845, mean_q: 0.775547
 24697/100000: episode: 324, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 0.838, mean reward: 0.076 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.549], loss: 0.000130, mae: 0.011143, mean_q: 0.775529
 24711/100000: episode: 325, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 0.905, mean reward: 0.065 [0.000, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.570], loss: 0.000094, mae: 0.007604, mean_q: 0.777743
 24725/100000: episode: 326, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 0.866, mean reward: 0.062 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.126, 10.573], loss: 0.000053, mae: 0.005532, mean_q: 0.775547
 24739/100000: episode: 327, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 0.909, mean reward: 0.065 [0.000, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.219, 10.521], loss: 0.000046, mae: 0.005938, mean_q: 0.774615
 24746/100000: episode: 328, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 0.887, mean reward: 0.127 [0.000, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.671], loss: 0.000331, mae: 0.010846, mean_q: 0.777028
 24757/100000: episode: 329, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 0.834, mean reward: 0.076 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.596], loss: 0.000083, mae: 0.009841, mean_q: 0.775101
 24771/100000: episode: 330, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 0.895, mean reward: 0.064 [0.000, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.706, 10.371], loss: 0.000069, mae: 0.006307, mean_q: 0.774773
 24777/100000: episode: 331, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 0.807, mean reward: 0.134 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.696, 10.528], loss: 0.000107, mae: 0.008710, mean_q: 0.776444
 24791/100000: episode: 332, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 0.842, mean reward: 0.060 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.540, 10.477], loss: 0.000074, mae: 0.005799, mean_q: 0.777851
 24798/100000: episode: 333, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 0.933, mean reward: 0.133 [0.000, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.696], loss: 0.000093, mae: 0.004194, mean_q: 0.775716
 24812/100000: episode: 334, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 0.897, mean reward: 0.064 [0.000, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.610], loss: 0.000049, mae: 0.004729, mean_q: 0.775851
 24826/100000: episode: 335, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 0.914, mean reward: 0.065 [0.000, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.582, 10.641], loss: 0.000031, mae: 0.004723, mean_q: 0.776938
 24833/100000: episode: 336, duration: 0.048s, episode steps: 7, steps per second: 144, episode reward: 0.855, mean reward: 0.122 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.623], loss: 0.000156, mae: 0.006777, mean_q: 0.777963
 24845/100000: episode: 337, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 0.884, mean reward: 0.074 [0.000, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.609, 10.456], loss: 0.000069, mae: 0.005779, mean_q: 0.776494
 24851/100000: episode: 338, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 0.987, mean reward: 0.164 [0.000, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.156, 10.370], loss: 0.000018, mae: 0.004713, mean_q: 0.774032
 24857/100000: episode: 339, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 0.847, mean reward: 0.141 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.606], loss: 0.000028, mae: 0.003564, mean_q: 0.778827
 24863/100000: episode: 340, duration: 0.043s, episode steps: 6, steps per second: 141, episode reward: 0.778, mean reward: 0.130 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.546], loss: 0.000025, mae: 0.003954, mean_q: 0.775530
 24869/100000: episode: 341, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 0.883, mean reward: 0.147 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.639], loss: 0.000057, mae: 0.003885, mean_q: 0.776384
 24880/100000: episode: 342, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 0.911, mean reward: 0.083 [0.000, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.200, 10.486], loss: 0.000053, mae: 0.004491, mean_q: 0.776529
 24886/100000: episode: 343, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 0.848, mean reward: 0.141 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.464], loss: 0.000082, mae: 0.006427, mean_q: 0.777846
 24892/100000: episode: 344, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 0.793, mean reward: 0.132 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.525], loss: 0.000056, mae: 0.006873, mean_q: 0.779105
 24904/100000: episode: 345, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 0.845, mean reward: 0.070 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.583, 10.556], loss: 0.000047, mae: 0.005203, mean_q: 0.778715
 24918/100000: episode: 346, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 0.877, mean reward: 0.063 [0.000, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.606], loss: 0.000060, mae: 0.004738, mean_q: 0.777260
 24932/100000: episode: 347, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 0.877, mean reward: 0.063 [0.000, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.486], loss: 0.000013, mae: 0.002602, mean_q: 0.776248
 24943/100000: episode: 348, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 0.918, mean reward: 0.083 [0.000, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.592], loss: 0.000097, mae: 0.005916, mean_q: 0.780572
 24954/100000: episode: 349, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 0.845, mean reward: 0.077 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.604], loss: 0.000048, mae: 0.005543, mean_q: 0.780273
 24968/100000: episode: 350, duration: 0.092s, episode steps: 14, steps per second: 151, episode reward: 1.003, mean reward: 0.072 [0.000, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.559], loss: 0.000145, mae: 0.006088, mean_q: 0.777251
 24982/100000: episode: 351, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 0.832, mean reward: 0.059 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.375], loss: 0.000097, mae: 0.005961, mean_q: 0.780606
 24989/100000: episode: 352, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.850, mean reward: 0.121 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.625], loss: 0.000201, mae: 0.007730, mean_q: 0.781834
 25000/100000: episode: 353, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 0.882, mean reward: 0.080 [0.000, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.505], loss: 0.000178, mae: 0.008533, mean_q: 0.779432
 25012/100000: episode: 354, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 0.870, mean reward: 0.073 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.595], loss: 0.000037, mae: 0.004656, mean_q: 0.780992
 25024/100000: episode: 355, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 0.902, mean reward: 0.075 [0.000, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.574], loss: 0.000076, mae: 0.005866, mean_q: 0.780782
 25037/100000: episode: 356, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 0.867, mean reward: 0.067 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.603], loss: 0.000081, mae: 0.006449, mean_q: 0.779810
 25050/100000: episode: 357, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 0.771, mean reward: 0.059 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.509], loss: 0.000066, mae: 0.005940, mean_q: 0.780867
 25064/100000: episode: 358, duration: 0.096s, episode steps: 14, steps per second: 145, episode reward: 0.835, mean reward: 0.060 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.405], loss: 0.000057, mae: 0.004658, mean_q: 0.782941
 25078/100000: episode: 359, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 0.815, mean reward: 0.058 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.329, 10.519], loss: 0.000059, mae: 0.006062, mean_q: 0.781587
 25092/100000: episode: 360, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 0.839, mean reward: 0.060 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.502, 10.570], loss: 0.000033, mae: 0.005146, mean_q: 0.781803
 25105/100000: episode: 361, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 0.897, mean reward: 0.069 [0.000, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.235, 10.621], loss: 0.000160, mae: 0.009663, mean_q: 0.782343
 25111/100000: episode: 362, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 0.854, mean reward: 0.142 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.254, 10.619], loss: 0.000120, mae: 0.009388, mean_q: 0.779600
 25118/100000: episode: 363, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.812, mean reward: 0.116 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.454], loss: 0.000120, mae: 0.009685, mean_q: 0.783683
 25130/100000: episode: 364, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 0.896, mean reward: 0.075 [0.000, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.631], loss: 0.000050, mae: 0.003758, mean_q: 0.778444
 25142/100000: episode: 365, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 0.910, mean reward: 0.076 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.685], loss: 0.000125, mae: 0.007071, mean_q: 0.782881
 25149/100000: episode: 366, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.828, mean reward: 0.118 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.929, 10.544], loss: 0.000157, mae: 0.012044, mean_q: 0.778690
 25160/100000: episode: 367, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 0.893, mean reward: 0.081 [0.000, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.500, 10.593], loss: 0.000116, mae: 0.008399, mean_q: 0.782493
 25174/100000: episode: 368, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 0.843, mean reward: 0.060 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.407], loss: 0.000127, mae: 0.007357, mean_q: 0.781321
 25181/100000: episode: 369, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 0.837, mean reward: 0.120 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.475], loss: 0.000120, mae: 0.007115, mean_q: 0.784051
 25195/100000: episode: 370, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 0.844, mean reward: 0.060 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.499, 10.619], loss: 0.000080, mae: 0.005241, mean_q: 0.783228
 25209/100000: episode: 371, duration: 0.099s, episode steps: 14, steps per second: 142, episode reward: 0.806, mean reward: 0.058 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.093, 10.454], loss: 0.000119, mae: 0.005841, mean_q: 0.785170
 25221/100000: episode: 372, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 0.883, mean reward: 0.074 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.633], loss: 0.000114, mae: 0.008242, mean_q: 0.783458
 25228/100000: episode: 373, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 0.831, mean reward: 0.119 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.539], loss: 0.000076, mae: 0.008947, mean_q: 0.783465
 25240/100000: episode: 374, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 0.904, mean reward: 0.075 [0.000, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.670], loss: 0.000105, mae: 0.007282, mean_q: 0.782653
 25246/100000: episode: 375, duration: 0.044s, episode steps: 6, steps per second: 136, episode reward: 0.768, mean reward: 0.128 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.358, 10.468], loss: 0.000047, mae: 0.004868, mean_q: 0.782614
 25253/100000: episode: 376, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 0.821, mean reward: 0.117 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.192, 10.577], loss: 0.000055, mae: 0.005316, mean_q: 0.786045
 25267/100000: episode: 377, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 0.935, mean reward: 0.067 [0.000, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.733, 10.566], loss: 0.000052, mae: 0.005225, mean_q: 0.783179
 25274/100000: episode: 378, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.905, mean reward: 0.129 [0.000, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.501, 10.551], loss: 0.000044, mae: 0.005080, mean_q: 0.787676
 25286/100000: episode: 379, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 0.860, mean reward: 0.072 [0.000, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.657, 10.507], loss: 0.000037, mae: 0.004492, mean_q: 0.784227
[Info] Complete ISplit Iteration
[Info] Levels: [0.7862579, 0.82117754, 0.8275051]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 25292/100000: episode: 380, duration: 4.685s, episode steps: 6, steps per second: 1, episode reward: 0.867, mean reward: 0.145 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.499], loss: 0.000073, mae: 0.006888, mean_q: 0.785132
 25393/100000: episode: 381, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.789, 10.100], loss: 0.000103, mae: 0.007043, mean_q: 0.784545
 25494/100000: episode: 382, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.967, 10.255], loss: 0.000119, mae: 0.007505, mean_q: 0.786194
 25595/100000: episode: 383, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.899, 10.218], loss: 0.000065, mae: 0.005485, mean_q: 0.786463
 25696/100000: episode: 384, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.294, 10.134], loss: 0.000071, mae: 0.005246, mean_q: 0.786896
 25797/100000: episode: 385, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.233, 10.100], loss: 0.000084, mae: 0.005671, mean_q: 0.787234
 25898/100000: episode: 386, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.804, 10.100], loss: 0.000105, mae: 0.008048, mean_q: 0.787101
 25999/100000: episode: 387, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.604, 10.209], loss: 0.000068, mae: 0.005252, mean_q: 0.788306
 26100/100000: episode: 388, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.229, 10.100], loss: 0.000072, mae: 0.005931, mean_q: 0.789154
 26201/100000: episode: 389, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.801, 10.290], loss: 0.000068, mae: 0.005413, mean_q: 0.789601
 26302/100000: episode: 390, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.378, 10.130], loss: 0.000070, mae: 0.005560, mean_q: 0.789853
 26403/100000: episode: 391, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.160], loss: 0.000076, mae: 0.005580, mean_q: 0.789586
 26504/100000: episode: 392, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.635, 10.227], loss: 0.000063, mae: 0.005614, mean_q: 0.789206
 26605/100000: episode: 393, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.100], loss: 0.000052, mae: 0.004570, mean_q: 0.789664
 26706/100000: episode: 394, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.100], loss: 0.000103, mae: 0.006438, mean_q: 0.788569
 26807/100000: episode: 395, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.214, 10.381], loss: 0.000051, mae: 0.004293, mean_q: 0.788386
 26908/100000: episode: 396, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.385, 10.103], loss: 0.000042, mae: 0.004210, mean_q: 0.787339
 27009/100000: episode: 397, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.179, 10.138], loss: 0.000057, mae: 0.004695, mean_q: 0.788336
 27110/100000: episode: 398, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.100], loss: 0.000053, mae: 0.005004, mean_q: 0.788328
 27211/100000: episode: 399, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.100], loss: 0.000054, mae: 0.004911, mean_q: 0.789389
 27312/100000: episode: 400, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.420, 10.100], loss: 0.000063, mae: 0.006147, mean_q: 0.787585
 27413/100000: episode: 401, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.343, 10.100], loss: 0.000061, mae: 0.004599, mean_q: 0.788519
 27514/100000: episode: 402, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.627, 10.100], loss: 0.000060, mae: 0.005395, mean_q: 0.787413
 27615/100000: episode: 403, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.780, 10.120], loss: 0.000045, mae: 0.004119, mean_q: 0.787231
 27716/100000: episode: 404, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.375, 10.461], loss: 0.000066, mae: 0.005306, mean_q: 0.787409
 27817/100000: episode: 405, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.470, 10.343], loss: 0.000064, mae: 0.005386, mean_q: 0.788416
 27918/100000: episode: 406, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.210, 10.249], loss: 0.000056, mae: 0.005235, mean_q: 0.787429
 28019/100000: episode: 407, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.667, 10.136], loss: 0.000048, mae: 0.004322, mean_q: 0.787879
 28120/100000: episode: 408, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.521, 10.334], loss: 0.000061, mae: 0.005380, mean_q: 0.787585
 28221/100000: episode: 409, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.248, 10.100], loss: 0.000076, mae: 0.006461, mean_q: 0.787060
 28322/100000: episode: 410, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.977, 10.240], loss: 0.000060, mae: 0.004830, mean_q: 0.787979
 28423/100000: episode: 411, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.100], loss: 0.000076, mae: 0.005988, mean_q: 0.788469
 28524/100000: episode: 412, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.100], loss: 0.000082, mae: 0.007156, mean_q: 0.787831
 28625/100000: episode: 413, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.118, 10.166], loss: 0.000065, mae: 0.006045, mean_q: 0.788336
 28726/100000: episode: 414, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.612, 10.145], loss: 0.000065, mae: 0.005020, mean_q: 0.788312
 28827/100000: episode: 415, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.976, 10.186], loss: 0.000045, mae: 0.003752, mean_q: 0.788029
 28928/100000: episode: 416, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.316, 10.377], loss: 0.000045, mae: 0.004399, mean_q: 0.787675
 29029/100000: episode: 417, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.874, 10.100], loss: 0.000060, mae: 0.004677, mean_q: 0.787607
 29130/100000: episode: 418, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.598, 10.100], loss: 0.000052, mae: 0.004685, mean_q: 0.789214
 29231/100000: episode: 419, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.678, 10.400], loss: 0.000047, mae: 0.004156, mean_q: 0.788290
 29332/100000: episode: 420, duration: 0.625s, episode steps: 101, steps per second: 161, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.596, 10.205], loss: 0.000040, mae: 0.004170, mean_q: 0.787267
 29433/100000: episode: 421, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.177, 10.386], loss: 0.000058, mae: 0.005275, mean_q: 0.786887
 29534/100000: episode: 422, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.626, 10.334], loss: 0.000048, mae: 0.004878, mean_q: 0.785737
 29635/100000: episode: 423, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.019, 10.360], loss: 0.000044, mae: 0.004213, mean_q: 0.785115
 29736/100000: episode: 424, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.898, 10.100], loss: 0.000027, mae: 0.003933, mean_q: 0.783833
 29837/100000: episode: 425, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.455, 10.100], loss: 0.000036, mae: 0.004169, mean_q: 0.782561
 29938/100000: episode: 426, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.384, 10.285], loss: 0.000033, mae: 0.003708, mean_q: 0.780105
 30039/100000: episode: 427, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.154, 10.368], loss: 0.000024, mae: 0.003025, mean_q: 0.779499
 30140/100000: episode: 428, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.764, 10.247], loss: 0.000024, mae: 0.003522, mean_q: 0.778563
 30241/100000: episode: 429, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.850, mean reward: 0.008 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.634, 10.100], loss: 0.000017, mae: 0.003300, mean_q: 0.777099
 30342/100000: episode: 430, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.483, 10.106], loss: 0.000021, mae: 0.003258, mean_q: 0.776258
 30443/100000: episode: 431, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.474, 10.349], loss: 0.000020, mae: 0.003498, mean_q: 0.776365
 30544/100000: episode: 432, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.237, 10.116], loss: 0.000026, mae: 0.003603, mean_q: 0.777096
 30645/100000: episode: 433, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.352, 10.100], loss: 0.000016, mae: 0.003212, mean_q: 0.776707
 30746/100000: episode: 434, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.633, 10.411], loss: 0.000025, mae: 0.003835, mean_q: 0.775664
 30847/100000: episode: 435, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.455, 10.368], loss: 0.000014, mae: 0.003054, mean_q: 0.776226
 30948/100000: episode: 436, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.333, 10.100], loss: 0.000018, mae: 0.002985, mean_q: 0.776820
 31049/100000: episode: 437, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.533, 10.267], loss: 0.000021, mae: 0.003897, mean_q: 0.776285
 31150/100000: episode: 438, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.449, 10.208], loss: 0.000024, mae: 0.003707, mean_q: 0.777007
 31251/100000: episode: 439, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.736, 10.100], loss: 0.000023, mae: 0.004106, mean_q: 0.777150
 31352/100000: episode: 440, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.877, 10.181], loss: 0.000019, mae: 0.003361, mean_q: 0.776888
 31453/100000: episode: 441, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.790, 10.140], loss: 0.000014, mae: 0.002919, mean_q: 0.776307
 31554/100000: episode: 442, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.082, 10.112], loss: 0.000018, mae: 0.003448, mean_q: 0.776428
 31655/100000: episode: 443, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.698, 10.187], loss: 0.000015, mae: 0.002976, mean_q: 0.776373
 31756/100000: episode: 444, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.109, 10.100], loss: 0.000019, mae: 0.003843, mean_q: 0.776604
 31857/100000: episode: 445, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.581, 10.141], loss: 0.000019, mae: 0.003179, mean_q: 0.775556
 31958/100000: episode: 446, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.327, 10.263], loss: 0.000012, mae: 0.002669, mean_q: 0.775537
 32059/100000: episode: 447, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.887, mean reward: 0.009 [0.000, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.570, 10.319], loss: 0.000017, mae: 0.002877, mean_q: 0.776005
 32160/100000: episode: 448, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.636, 10.100], loss: 0.000015, mae: 0.002753, mean_q: 0.776046
 32261/100000: episode: 449, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.561, 10.273], loss: 0.000011, mae: 0.002773, mean_q: 0.776290
 32362/100000: episode: 450, duration: 0.688s, episode steps: 101, steps per second: 147, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.966, 10.151], loss: 0.000025, mae: 0.004141, mean_q: 0.776166
 32463/100000: episode: 451, duration: 0.654s, episode steps: 101, steps per second: 154, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.571, 10.160], loss: 0.000020, mae: 0.003460, mean_q: 0.776183
 32564/100000: episode: 452, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.482, 10.216], loss: 0.000018, mae: 0.003315, mean_q: 0.775741
 32665/100000: episode: 453, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.114, 10.100], loss: 0.000017, mae: 0.003035, mean_q: 0.776021
 32766/100000: episode: 454, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.711, 10.308], loss: 0.000022, mae: 0.003549, mean_q: 0.775638
 32867/100000: episode: 455, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.233, 10.117], loss: 0.000012, mae: 0.002484, mean_q: 0.775546
 32968/100000: episode: 456, duration: 0.674s, episode steps: 101, steps per second: 150, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.967, 10.169], loss: 0.000017, mae: 0.003031, mean_q: 0.775062
 33069/100000: episode: 457, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.888, 10.111], loss: 0.000014, mae: 0.003035, mean_q: 0.774327
 33170/100000: episode: 458, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.430, 10.100], loss: 0.000018, mae: 0.003382, mean_q: 0.774578
 33271/100000: episode: 459, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.012, 10.321], loss: 0.000014, mae: 0.003025, mean_q: 0.774387
 33372/100000: episode: 460, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.022, 10.100], loss: 0.000020, mae: 0.003554, mean_q: 0.774592
 33473/100000: episode: 461, duration: 0.667s, episode steps: 101, steps per second: 151, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.139], loss: 0.000018, mae: 0.003421, mean_q: 0.774213
 33574/100000: episode: 462, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.392, 10.100], loss: 0.000013, mae: 0.003161, mean_q: 0.774167
 33675/100000: episode: 463, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.773, 10.100], loss: 0.000025, mae: 0.004022, mean_q: 0.772921
 33776/100000: episode: 464, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.927, mean reward: 0.009 [0.000, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.822, 10.100], loss: 0.000017, mae: 0.003024, mean_q: 0.772838
 33877/100000: episode: 465, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.388, 10.100], loss: 0.000027, mae: 0.003773, mean_q: 0.772307
 33978/100000: episode: 466, duration: 0.836s, episode steps: 101, steps per second: 121, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.781, 10.295], loss: 0.000014, mae: 0.002692, mean_q: 0.772396
 34079/100000: episode: 467, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.361, 10.261], loss: 0.000027, mae: 0.003998, mean_q: 0.772105
 34180/100000: episode: 468, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.822, 10.273], loss: 0.000019, mae: 0.002957, mean_q: 0.771986
 34281/100000: episode: 469, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.332, 10.100], loss: 0.000015, mae: 0.002859, mean_q: 0.771773
 34382/100000: episode: 470, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.687, 10.153], loss: 0.000009, mae: 0.002438, mean_q: 0.771311
 34483/100000: episode: 471, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.896, 10.402], loss: 0.000027, mae: 0.004458, mean_q: 0.771467
 34584/100000: episode: 472, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.416, 10.392], loss: 0.000026, mae: 0.003983, mean_q: 0.771036
 34685/100000: episode: 473, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.549, 10.135], loss: 0.000009, mae: 0.002502, mean_q: 0.770735
 34786/100000: episode: 474, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.907, 10.114], loss: 0.000013, mae: 0.002976, mean_q: 0.769752
 34887/100000: episode: 475, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.215, 10.223], loss: 0.000019, mae: 0.003643, mean_q: 0.770728
 34988/100000: episode: 476, duration: 0.744s, episode steps: 101, steps per second: 136, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.954, 10.146], loss: 0.000016, mae: 0.003237, mean_q: 0.769902
 35089/100000: episode: 477, duration: 0.721s, episode steps: 101, steps per second: 140, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.881, 10.100], loss: 0.000015, mae: 0.002908, mean_q: 0.769587
 35190/100000: episode: 478, duration: 0.749s, episode steps: 101, steps per second: 135, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.626, 10.190], loss: 0.000018, mae: 0.003096, mean_q: 0.769307
 35291/100000: episode: 479, duration: 0.668s, episode steps: 101, steps per second: 151, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.112], loss: 0.000021, mae: 0.003361, mean_q: 0.768775
[Info] 1-TH LEVEL FOUND: 0.8015279173851013, Considering 10/100 traces
 35392/100000: episode: 480, duration: 5.337s, episode steps: 101, steps per second: 19, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.893, 10.128], loss: 0.000016, mae: 0.003083, mean_q: 0.768096
 35468/100000: episode: 481, duration: 0.497s, episode steps: 76, steps per second: 153, episode reward: 0.820, mean reward: 0.011 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.688 [-0.542, 10.142], loss: 0.000027, mae: 0.003264, mean_q: 0.767866
 35544/100000: episode: 482, duration: 0.454s, episode steps: 76, steps per second: 167, episode reward: 0.792, mean reward: 0.010 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.693 [-0.836, 10.195], loss: 0.000024, mae: 0.003520, mean_q: 0.767776
 35619/100000: episode: 483, duration: 0.448s, episode steps: 75, steps per second: 167, episode reward: 0.779, mean reward: 0.010 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.686 [-0.921, 10.229], loss: 0.000020, mae: 0.003384, mean_q: 0.767246
 35715/100000: episode: 484, duration: 0.550s, episode steps: 96, steps per second: 175, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.852, 10.243], loss: 0.000019, mae: 0.003281, mean_q: 0.767538
 35789/100000: episode: 485, duration: 0.469s, episode steps: 74, steps per second: 158, episode reward: 0.780, mean reward: 0.011 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-1.500, 10.103], loss: 0.000010, mae: 0.002508, mean_q: 0.766752
 35866/100000: episode: 486, duration: 0.453s, episode steps: 77, steps per second: 170, episode reward: 0.685, mean reward: 0.009 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.664 [-0.825, 10.100], loss: 0.000014, mae: 0.002710, mean_q: 0.766858
 35959/100000: episode: 487, duration: 0.590s, episode steps: 93, steps per second: 158, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-1.023, 10.335], loss: 0.000018, mae: 0.002684, mean_q: 0.766678
 36036/100000: episode: 488, duration: 0.493s, episode steps: 77, steps per second: 156, episode reward: 0.770, mean reward: 0.010 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-0.781, 10.100], loss: 0.000023, mae: 0.003561, mean_q: 0.766423
 36112/100000: episode: 489, duration: 0.488s, episode steps: 76, steps per second: 156, episode reward: 0.701, mean reward: 0.009 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.683 [-0.525, 10.100], loss: 0.000016, mae: 0.002771, mean_q: 0.765488
 36186/100000: episode: 490, duration: 0.497s, episode steps: 74, steps per second: 149, episode reward: 0.708, mean reward: 0.010 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-1.431, 10.135], loss: 0.000027, mae: 0.003095, mean_q: 0.765680
 36260/100000: episode: 491, duration: 0.477s, episode steps: 74, steps per second: 155, episode reward: 0.752, mean reward: 0.010 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-0.294, 10.186], loss: 0.000032, mae: 0.004091, mean_q: 0.764444
 36334/100000: episode: 492, duration: 0.433s, episode steps: 74, steps per second: 171, episode reward: 0.796, mean reward: 0.011 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.701 [-0.743, 10.230], loss: 0.000029, mae: 0.003570, mean_q: 0.764947
 36410/100000: episode: 493, duration: 0.517s, episode steps: 76, steps per second: 147, episode reward: 0.719, mean reward: 0.009 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.676 [-1.546, 10.100], loss: 0.000024, mae: 0.003694, mean_q: 0.764471
 36486/100000: episode: 494, duration: 0.459s, episode steps: 76, steps per second: 166, episode reward: 0.792, mean reward: 0.010 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.686 [-1.015, 10.214], loss: 0.000024, mae: 0.003361, mean_q: 0.764360
 36562/100000: episode: 495, duration: 0.520s, episode steps: 76, steps per second: 146, episode reward: 0.743, mean reward: 0.010 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.683 [-0.586, 10.108], loss: 0.000023, mae: 0.003355, mean_q: 0.763682
 36636/100000: episode: 496, duration: 0.484s, episode steps: 74, steps per second: 153, episode reward: 0.751, mean reward: 0.010 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.174, 10.175], loss: 0.000019, mae: 0.003124, mean_q: 0.763792
 36713/100000: episode: 497, duration: 0.553s, episode steps: 77, steps per second: 139, episode reward: 0.841, mean reward: 0.011 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.670 [-0.580, 10.100], loss: 0.000014, mae: 0.002962, mean_q: 0.763289
 36787/100000: episode: 498, duration: 0.510s, episode steps: 74, steps per second: 145, episode reward: 0.719, mean reward: 0.010 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.657, 10.100], loss: 0.000041, mae: 0.004927, mean_q: 0.762762
 36862/100000: episode: 499, duration: 0.514s, episode steps: 75, steps per second: 146, episode reward: 0.697, mean reward: 0.009 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.683 [-0.218, 10.100], loss: 0.000029, mae: 0.003777, mean_q: 0.762399
 36955/100000: episode: 500, duration: 0.559s, episode steps: 93, steps per second: 166, episode reward: 0.749, mean reward: 0.008 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.607, 10.282], loss: 0.000015, mae: 0.002929, mean_q: 0.761779
 37029/100000: episode: 501, duration: 0.399s, episode steps: 74, steps per second: 186, episode reward: 0.751, mean reward: 0.010 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.690 [-1.644, 10.221], loss: 0.000018, mae: 0.002750, mean_q: 0.761411
 37105/100000: episode: 502, duration: 0.509s, episode steps: 76, steps per second: 149, episode reward: 0.710, mean reward: 0.009 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-0.264, 10.100], loss: 0.000016, mae: 0.003323, mean_q: 0.760734
 37201/100000: episode: 503, duration: 0.568s, episode steps: 96, steps per second: 169, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.668, 10.185], loss: 0.000017, mae: 0.002668, mean_q: 0.760543
 37277/100000: episode: 504, duration: 0.519s, episode steps: 76, steps per second: 146, episode reward: 0.845, mean reward: 0.011 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.676 [-0.808, 10.319], loss: 0.000020, mae: 0.003396, mean_q: 0.760055
 37353/100000: episode: 505, duration: 0.532s, episode steps: 76, steps per second: 143, episode reward: 0.694, mean reward: 0.009 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.683 [-1.046, 10.100], loss: 0.000008, mae: 0.002018, mean_q: 0.759952
 37427/100000: episode: 506, duration: 0.486s, episode steps: 74, steps per second: 152, episode reward: 0.842, mean reward: 0.011 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.884, 10.433], loss: 0.000015, mae: 0.003210, mean_q: 0.759023
 37520/100000: episode: 507, duration: 0.583s, episode steps: 93, steps per second: 159, episode reward: 0.810, mean reward: 0.009 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-1.250, 10.100], loss: 0.000015, mae: 0.002795, mean_q: 0.759664
 37595/100000: episode: 508, duration: 0.412s, episode steps: 75, steps per second: 182, episode reward: 0.679, mean reward: 0.009 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-0.853, 10.100], loss: 0.000022, mae: 0.003843, mean_q: 0.758589
 37688/100000: episode: 509, duration: 0.551s, episode steps: 93, steps per second: 169, episode reward: 0.747, mean reward: 0.008 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-1.955, 10.263], loss: 0.000023, mae: 0.003169, mean_q: 0.758647
 37762/100000: episode: 510, duration: 0.401s, episode steps: 74, steps per second: 185, episode reward: 0.863, mean reward: 0.012 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.692 [-1.010, 10.102], loss: 0.000031, mae: 0.003321, mean_q: 0.757704
 37838/100000: episode: 511, duration: 0.444s, episode steps: 76, steps per second: 171, episode reward: 0.718, mean reward: 0.009 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-0.175, 10.418], loss: 0.000028, mae: 0.003402, mean_q: 0.757767
 37912/100000: episode: 512, duration: 0.433s, episode steps: 74, steps per second: 171, episode reward: 0.785, mean reward: 0.011 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.737, 10.100], loss: 0.000022, mae: 0.003080, mean_q: 0.757784
 37989/100000: episode: 513, duration: 0.517s, episode steps: 77, steps per second: 149, episode reward: 0.798, mean reward: 0.010 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.664 [-0.411, 10.100], loss: 0.000008, mae: 0.001931, mean_q: 0.757301
 38063/100000: episode: 514, duration: 0.422s, episode steps: 74, steps per second: 175, episode reward: 0.742, mean reward: 0.010 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.724, 10.100], loss: 0.000026, mae: 0.002955, mean_q: 0.757678
 38139/100000: episode: 515, duration: 0.485s, episode steps: 76, steps per second: 157, episode reward: 0.741, mean reward: 0.010 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.672 [-0.395, 10.100], loss: 0.000019, mae: 0.003504, mean_q: 0.756472
 38216/100000: episode: 516, duration: 0.440s, episode steps: 77, steps per second: 175, episode reward: 0.720, mean reward: 0.009 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-0.628, 10.100], loss: 0.000012, mae: 0.002802, mean_q: 0.756851
 38290/100000: episode: 517, duration: 0.428s, episode steps: 74, steps per second: 173, episode reward: 0.772, mean reward: 0.010 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.353, 10.399], loss: 0.000014, mae: 0.002582, mean_q: 0.756042
 38383/100000: episode: 518, duration: 0.525s, episode steps: 93, steps per second: 177, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-0.283, 10.235], loss: 0.000022, mae: 0.003403, mean_q: 0.756347
 38479/100000: episode: 519, duration: 0.573s, episode steps: 96, steps per second: 168, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-1.066, 10.100], loss: 0.000018, mae: 0.002913, mean_q: 0.755842
 38555/100000: episode: 520, duration: 0.443s, episode steps: 76, steps per second: 172, episode reward: 0.814, mean reward: 0.011 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.678 [-0.934, 10.554], loss: 0.000012, mae: 0.002261, mean_q: 0.756382
 38630/100000: episode: 521, duration: 0.431s, episode steps: 75, steps per second: 174, episode reward: 0.744, mean reward: 0.010 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-1.063, 10.348], loss: 0.000023, mae: 0.003969, mean_q: 0.756090
 38706/100000: episode: 522, duration: 0.442s, episode steps: 76, steps per second: 172, episode reward: 0.795, mean reward: 0.010 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-0.788, 10.224], loss: 0.000018, mae: 0.003181, mean_q: 0.755902
 38781/100000: episode: 523, duration: 0.445s, episode steps: 75, steps per second: 168, episode reward: 0.796, mean reward: 0.011 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-1.298, 10.172], loss: 0.000013, mae: 0.002761, mean_q: 0.756380
 38856/100000: episode: 524, duration: 0.434s, episode steps: 75, steps per second: 173, episode reward: 0.762, mean reward: 0.010 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.679 [-1.583, 10.435], loss: 0.000015, mae: 0.002709, mean_q: 0.756410
 38952/100000: episode: 525, duration: 0.600s, episode steps: 96, steps per second: 160, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.601, 10.100], loss: 0.000009, mae: 0.002213, mean_q: 0.756120
 39026/100000: episode: 526, duration: 0.439s, episode steps: 74, steps per second: 169, episode reward: 0.732, mean reward: 0.010 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.701 [-1.568, 10.197], loss: 0.000018, mae: 0.003028, mean_q: 0.756031
 39101/100000: episode: 527, duration: 0.435s, episode steps: 75, steps per second: 172, episode reward: 0.714, mean reward: 0.010 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-0.438, 10.100], loss: 0.000020, mae: 0.003047, mean_q: 0.755880
 39175/100000: episode: 528, duration: 0.475s, episode steps: 74, steps per second: 156, episode reward: 0.750, mean reward: 0.010 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.473, 10.267], loss: 0.000031, mae: 0.004291, mean_q: 0.755365
 39252/100000: episode: 529, duration: 0.471s, episode steps: 77, steps per second: 163, episode reward: 0.736, mean reward: 0.010 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.670 [-1.223, 10.255], loss: 0.000025, mae: 0.003729, mean_q: 0.755803
 39327/100000: episode: 530, duration: 0.478s, episode steps: 75, steps per second: 157, episode reward: 0.697, mean reward: 0.009 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.009, 10.212], loss: 0.000023, mae: 0.003452, mean_q: 0.755943
 39401/100000: episode: 531, duration: 0.684s, episode steps: 74, steps per second: 108, episode reward: 0.852, mean reward: 0.012 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.609, 10.467], loss: 0.000019, mae: 0.003095, mean_q: 0.755861
 39475/100000: episode: 532, duration: 0.472s, episode steps: 74, steps per second: 157, episode reward: 0.862, mean reward: 0.012 [0.000, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.702 [-1.468, 10.212], loss: 0.000014, mae: 0.002319, mean_q: 0.756114
 39549/100000: episode: 533, duration: 0.459s, episode steps: 74, steps per second: 161, episode reward: 0.723, mean reward: 0.010 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.167, 10.161], loss: 0.000027, mae: 0.003043, mean_q: 0.756173
 39624/100000: episode: 534, duration: 0.464s, episode steps: 75, steps per second: 162, episode reward: 0.759, mean reward: 0.010 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.650, 10.100], loss: 0.000015, mae: 0.003197, mean_q: 0.756104
 39698/100000: episode: 535, duration: 0.437s, episode steps: 74, steps per second: 169, episode reward: 0.847, mean reward: 0.011 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-1.162, 10.111], loss: 0.000014, mae: 0.002741, mean_q: 0.756042
 39773/100000: episode: 536, duration: 0.457s, episode steps: 75, steps per second: 164, episode reward: 0.802, mean reward: 0.011 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-0.433, 10.232], loss: 0.000013, mae: 0.002218, mean_q: 0.756215
 39848/100000: episode: 537, duration: 0.446s, episode steps: 75, steps per second: 168, episode reward: 0.687, mean reward: 0.009 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.683 [-1.178, 10.100], loss: 0.000019, mae: 0.003341, mean_q: 0.756425
 39922/100000: episode: 538, duration: 0.423s, episode steps: 74, steps per second: 175, episode reward: 0.718, mean reward: 0.010 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-1.094, 10.100], loss: 0.000018, mae: 0.002893, mean_q: 0.755979
 39997/100000: episode: 539, duration: 0.479s, episode steps: 75, steps per second: 157, episode reward: 0.767, mean reward: 0.010 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.511, 10.126], loss: 0.000017, mae: 0.002971, mean_q: 0.756196
 40073/100000: episode: 540, duration: 0.463s, episode steps: 76, steps per second: 164, episode reward: 0.738, mean reward: 0.010 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.678 [-0.268, 10.180], loss: 0.000020, mae: 0.002908, mean_q: 0.756239
 40148/100000: episode: 541, duration: 0.590s, episode steps: 75, steps per second: 127, episode reward: 0.770, mean reward: 0.010 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.829, 10.407], loss: 0.000019, mae: 0.003116, mean_q: 0.756395
 40244/100000: episode: 542, duration: 0.581s, episode steps: 96, steps per second: 165, episode reward: 0.755, mean reward: 0.008 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-1.014, 10.168], loss: 0.000017, mae: 0.002847, mean_q: 0.756779
 40319/100000: episode: 543, duration: 0.464s, episode steps: 75, steps per second: 162, episode reward: 0.777, mean reward: 0.010 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.210, 10.100], loss: 0.000017, mae: 0.002236, mean_q: 0.756829
 40395/100000: episode: 544, duration: 0.494s, episode steps: 76, steps per second: 154, episode reward: 0.808, mean reward: 0.011 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.670 [-0.298, 10.100], loss: 0.000011, mae: 0.002299, mean_q: 0.756748
 40488/100000: episode: 545, duration: 0.566s, episode steps: 93, steps per second: 164, episode reward: 0.791, mean reward: 0.009 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-1.289, 10.100], loss: 0.000021, mae: 0.003143, mean_q: 0.756934
 40565/100000: episode: 546, duration: 0.503s, episode steps: 77, steps per second: 153, episode reward: 0.768, mean reward: 0.010 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.673 [-0.792, 10.100], loss: 0.000019, mae: 0.003060, mean_q: 0.756726
 40661/100000: episode: 547, duration: 0.550s, episode steps: 96, steps per second: 175, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.777, 10.100], loss: 0.000025, mae: 0.003515, mean_q: 0.756799
 40757/100000: episode: 548, duration: 0.750s, episode steps: 96, steps per second: 128, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.048, 10.143], loss: 0.000018, mae: 0.002940, mean_q: 0.756410
 40850/100000: episode: 549, duration: 0.618s, episode steps: 93, steps per second: 151, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.302, 10.127], loss: 0.000024, mae: 0.003528, mean_q: 0.756569
 40924/100000: episode: 550, duration: 0.533s, episode steps: 74, steps per second: 139, episode reward: 0.783, mean reward: 0.011 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.451, 10.261], loss: 0.000020, mae: 0.003192, mean_q: 0.756863
 40998/100000: episode: 551, duration: 0.474s, episode steps: 74, steps per second: 156, episode reward: 0.762, mean reward: 0.010 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.260, 10.100], loss: 0.000012, mae: 0.002188, mean_q: 0.756757
 41094/100000: episode: 552, duration: 0.601s, episode steps: 96, steps per second: 160, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.662, 10.206], loss: 0.000015, mae: 0.002430, mean_q: 0.756631
 41169/100000: episode: 553, duration: 0.483s, episode steps: 75, steps per second: 155, episode reward: 0.742, mean reward: 0.010 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.682 [-1.223, 10.100], loss: 0.000021, mae: 0.002570, mean_q: 0.756947
 41246/100000: episode: 554, duration: 0.675s, episode steps: 77, steps per second: 114, episode reward: 0.708, mean reward: 0.009 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-1.458, 10.116], loss: 0.000025, mae: 0.003441, mean_q: 0.757195
 41342/100000: episode: 555, duration: 0.669s, episode steps: 96, steps per second: 144, episode reward: 0.728, mean reward: 0.008 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.929, 10.100], loss: 0.000019, mae: 0.002876, mean_q: 0.756296
 41418/100000: episode: 556, duration: 0.476s, episode steps: 76, steps per second: 160, episode reward: 0.659, mean reward: 0.009 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.677 [-0.259, 10.100], loss: 0.000021, mae: 0.003107, mean_q: 0.756295
 41511/100000: episode: 557, duration: 0.544s, episode steps: 93, steps per second: 171, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.345, 10.100], loss: 0.000019, mae: 0.002813, mean_q: 0.756603
 41585/100000: episode: 558, duration: 0.429s, episode steps: 74, steps per second: 172, episode reward: 0.792, mean reward: 0.011 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.621, 10.129], loss: 0.000015, mae: 0.002734, mean_q: 0.756404
 41661/100000: episode: 559, duration: 0.453s, episode steps: 76, steps per second: 168, episode reward: 0.732, mean reward: 0.010 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.677 [-1.345, 10.194], loss: 0.000013, mae: 0.002732, mean_q: 0.756576
 41754/100000: episode: 560, duration: 0.566s, episode steps: 93, steps per second: 164, episode reward: 0.833, mean reward: 0.009 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.296, 10.100], loss: 0.000015, mae: 0.002717, mean_q: 0.756509
 41829/100000: episode: 561, duration: 0.458s, episode steps: 75, steps per second: 164, episode reward: 0.769, mean reward: 0.010 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.680 [-0.295, 10.399], loss: 0.000028, mae: 0.003553, mean_q: 0.756678
 41904/100000: episode: 562, duration: 0.448s, episode steps: 75, steps per second: 167, episode reward: 0.689, mean reward: 0.009 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.693 [-0.369, 10.100], loss: 0.000015, mae: 0.002490, mean_q: 0.756764
 41981/100000: episode: 563, duration: 0.462s, episode steps: 77, steps per second: 167, episode reward: 0.781, mean reward: 0.010 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.672 [-0.642, 10.109], loss: 0.000015, mae: 0.002420, mean_q: 0.756718
 42057/100000: episode: 564, duration: 0.462s, episode steps: 76, steps per second: 165, episode reward: 0.802, mean reward: 0.011 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-1.639, 10.195], loss: 0.000015, mae: 0.002411, mean_q: 0.756754
 42153/100000: episode: 565, duration: 0.592s, episode steps: 96, steps per second: 162, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.136, 10.302], loss: 0.000020, mae: 0.002833, mean_q: 0.756623
 42227/100000: episode: 566, duration: 0.430s, episode steps: 74, steps per second: 172, episode reward: 0.734, mean reward: 0.010 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.714 [-0.910, 10.397], loss: 0.000030, mae: 0.003754, mean_q: 0.756907
 42301/100000: episode: 567, duration: 0.460s, episode steps: 74, steps per second: 161, episode reward: 0.779, mean reward: 0.011 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.794, 10.100], loss: 0.000019, mae: 0.002222, mean_q: 0.756969
 42397/100000: episode: 568, duration: 0.603s, episode steps: 96, steps per second: 159, episode reward: 0.735, mean reward: 0.008 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.330, 10.283], loss: 0.000014, mae: 0.002559, mean_q: 0.756869
 42474/100000: episode: 569, duration: 0.515s, episode steps: 77, steps per second: 150, episode reward: 0.666, mean reward: 0.009 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.679 [-0.402, 10.241], loss: 0.000020, mae: 0.002938, mean_q: 0.756703
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.8015279173851013
1
 42550/100000: episode: 570, duration: 5.200s, episode steps: 76, steps per second: 15, episode reward: 0.733, mean reward: 0.010 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-0.392, 10.100], loss: 0.000014, mae: 0.002353, mean_q: 0.757046
 42651/100000: episode: 571, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.358, 10.154], loss: 0.000033, mae: 0.003433, mean_q: 0.756355
 42752/100000: episode: 572, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.694, 10.266], loss: 0.000022, mae: 0.003074, mean_q: 0.756381
 42853/100000: episode: 573, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.590, 10.100], loss: 0.000021, mae: 0.002653, mean_q: 0.755988
 42954/100000: episode: 574, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.100], loss: 0.000024, mae: 0.003467, mean_q: 0.755781
 43055/100000: episode: 575, duration: 0.645s, episode steps: 101, steps per second: 156, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.476, 10.526], loss: 0.000026, mae: 0.004005, mean_q: 0.755897
 43156/100000: episode: 576, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.884, mean reward: 0.009 [0.000, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.918, 10.100], loss: 0.000018, mae: 0.002471, mean_q: 0.755857
 43257/100000: episode: 577, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.317, 10.100], loss: 0.000026, mae: 0.003692, mean_q: 0.756062
 43358/100000: episode: 578, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.917, 10.415], loss: 0.000023, mae: 0.002989, mean_q: 0.755687
 43459/100000: episode: 579, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.506, 10.100], loss: 0.000027, mae: 0.002953, mean_q: 0.755506
 43560/100000: episode: 580, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.391, 10.100], loss: 0.000011, mae: 0.002154, mean_q: 0.755015
 43661/100000: episode: 581, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.772, 10.251], loss: 0.000013, mae: 0.002498, mean_q: 0.754890
 43762/100000: episode: 582, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.546, 10.100], loss: 0.000031, mae: 0.004516, mean_q: 0.754716
 43863/100000: episode: 583, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.031, 10.309], loss: 0.000020, mae: 0.002715, mean_q: 0.754654
 43964/100000: episode: 584, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.191, 10.100], loss: 0.000018, mae: 0.002299, mean_q: 0.754543
 44065/100000: episode: 585, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.685, 10.383], loss: 0.000025, mae: 0.003488, mean_q: 0.754789
 44166/100000: episode: 586, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.472, 10.100], loss: 0.000026, mae: 0.004042, mean_q: 0.754435
 44267/100000: episode: 587, duration: 0.652s, episode steps: 101, steps per second: 155, episode reward: 0.849, mean reward: 0.008 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.875, 10.486], loss: 0.000026, mae: 0.003643, mean_q: 0.754438
 44368/100000: episode: 588, duration: 1.021s, episode steps: 101, steps per second: 99, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.595, 10.100], loss: 0.000017, mae: 0.002619, mean_q: 0.754345
 44469/100000: episode: 589, duration: 0.637s, episode steps: 101, steps per second: 158, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.970, 10.489], loss: 0.000025, mae: 0.003026, mean_q: 0.754283
 44570/100000: episode: 590, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.241], loss: 0.000018, mae: 0.002334, mean_q: 0.754604
 44671/100000: episode: 591, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.780, 10.164], loss: 0.000018, mae: 0.002472, mean_q: 0.754681
 44772/100000: episode: 592, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.576, 10.100], loss: 0.000015, mae: 0.002474, mean_q: 0.754606
 44873/100000: episode: 593, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.081, 10.245], loss: 0.000017, mae: 0.002457, mean_q: 0.754341
 44974/100000: episode: 594, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.027, 10.100], loss: 0.000016, mae: 0.002958, mean_q: 0.754705
 45075/100000: episode: 595, duration: 0.733s, episode steps: 101, steps per second: 138, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.675, 10.100], loss: 0.000016, mae: 0.002503, mean_q: 0.754599
 45176/100000: episode: 596, duration: 0.658s, episode steps: 101, steps per second: 153, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.291, 10.100], loss: 0.000014, mae: 0.002439, mean_q: 0.754306
 45277/100000: episode: 597, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.146], loss: 0.000019, mae: 0.002591, mean_q: 0.754253
 45378/100000: episode: 598, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.380, 10.379], loss: 0.000023, mae: 0.003068, mean_q: 0.754697
 45479/100000: episode: 599, duration: 0.673s, episode steps: 101, steps per second: 150, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.477, 10.225], loss: 0.000017, mae: 0.002417, mean_q: 0.754598
 45580/100000: episode: 600, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.648, 10.100], loss: 0.000023, mae: 0.003079, mean_q: 0.754647
 45681/100000: episode: 601, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.554, 10.100], loss: 0.000024, mae: 0.003335, mean_q: 0.754271
 45782/100000: episode: 602, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.799, 10.111], loss: 0.000022, mae: 0.003201, mean_q: 0.754094
 45883/100000: episode: 603, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.202, 10.100], loss: 0.000011, mae: 0.002102, mean_q: 0.754111
 45984/100000: episode: 604, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.694, 10.100], loss: 0.000022, mae: 0.002827, mean_q: 0.754292
 46085/100000: episode: 605, duration: 0.651s, episode steps: 101, steps per second: 155, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.322, 10.100], loss: 0.000024, mae: 0.003110, mean_q: 0.753798
 46186/100000: episode: 606, duration: 0.688s, episode steps: 101, steps per second: 147, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.424, 10.142], loss: 0.000014, mae: 0.002490, mean_q: 0.753751
 46287/100000: episode: 607, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.638, 10.100], loss: 0.000018, mae: 0.002748, mean_q: 0.753656
 46388/100000: episode: 608, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.675, 10.244], loss: 0.000038, mae: 0.004754, mean_q: 0.753596
 46489/100000: episode: 609, duration: 0.719s, episode steps: 101, steps per second: 141, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.501, 10.280], loss: 0.000011, mae: 0.001940, mean_q: 0.753560
 46590/100000: episode: 610, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.966, 10.100], loss: 0.000013, mae: 0.002309, mean_q: 0.753661
 46691/100000: episode: 611, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.624, 10.100], loss: 0.000014, mae: 0.002321, mean_q: 0.753550
 46792/100000: episode: 612, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.369, 10.161], loss: 0.000014, mae: 0.002334, mean_q: 0.753612
 46893/100000: episode: 613, duration: 0.881s, episode steps: 101, steps per second: 115, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.805, 10.100], loss: 0.000025, mae: 0.003428, mean_q: 0.753664
 46994/100000: episode: 614, duration: 1.048s, episode steps: 101, steps per second: 96, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.218, 10.100], loss: 0.000032, mae: 0.004069, mean_q: 0.753511
 47095/100000: episode: 615, duration: 0.879s, episode steps: 101, steps per second: 115, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.100], loss: 0.000021, mae: 0.002821, mean_q: 0.753882
 47196/100000: episode: 616, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.854, mean reward: 0.008 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.340, 10.259], loss: 0.000022, mae: 0.002753, mean_q: 0.754091
 47297/100000: episode: 617, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.759, 10.100], loss: 0.000026, mae: 0.003713, mean_q: 0.754016
 47398/100000: episode: 618, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.770, 10.100], loss: 0.000013, mae: 0.002069, mean_q: 0.754359
 47499/100000: episode: 619, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.622, 10.100], loss: 0.000019, mae: 0.002773, mean_q: 0.754565
 47600/100000: episode: 620, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.278, 10.226], loss: 0.000026, mae: 0.003212, mean_q: 0.754803
 47701/100000: episode: 621, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.100], loss: 0.000024, mae: 0.002721, mean_q: 0.754722
 47802/100000: episode: 622, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.381, 10.100], loss: 0.000021, mae: 0.002742, mean_q: 0.755080
 47903/100000: episode: 623, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.774, 10.100], loss: 0.000014, mae: 0.002216, mean_q: 0.755055
 48004/100000: episode: 624, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.153, 10.183], loss: 0.000030, mae: 0.003523, mean_q: 0.755747
 48105/100000: episode: 625, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.157], loss: 0.000022, mae: 0.002884, mean_q: 0.755489
 48206/100000: episode: 626, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.280, 10.178], loss: 0.000017, mae: 0.002661, mean_q: 0.755491
 48307/100000: episode: 627, duration: 0.883s, episode steps: 101, steps per second: 114, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.544, 10.123], loss: 0.000020, mae: 0.002858, mean_q: 0.755324
 48408/100000: episode: 628, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.935, 10.351], loss: 0.000013, mae: 0.002171, mean_q: 0.755677
 48509/100000: episode: 629, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.228, 10.250], loss: 0.000013, mae: 0.002475, mean_q: 0.755902
 48610/100000: episode: 630, duration: 0.723s, episode steps: 101, steps per second: 140, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.275, 10.100], loss: 0.000026, mae: 0.003486, mean_q: 0.756525
 48711/100000: episode: 631, duration: 0.728s, episode steps: 101, steps per second: 139, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.018, 10.367], loss: 0.000015, mae: 0.002502, mean_q: 0.756535
 48812/100000: episode: 632, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.990, 10.100], loss: 0.000010, mae: 0.002146, mean_q: 0.756804
 48913/100000: episode: 633, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.356, 10.100], loss: 0.000015, mae: 0.002358, mean_q: 0.756646
 49014/100000: episode: 634, duration: 0.682s, episode steps: 101, steps per second: 148, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.875, 10.100], loss: 0.000025, mae: 0.003087, mean_q: 0.756709
 49115/100000: episode: 635, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.501, 10.187], loss: 0.000021, mae: 0.002971, mean_q: 0.756724
 49216/100000: episode: 636, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.123, 10.100], loss: 0.000018, mae: 0.002623, mean_q: 0.756739
 49317/100000: episode: 637, duration: 0.656s, episode steps: 101, steps per second: 154, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.759, 10.100], loss: 0.000010, mae: 0.002465, mean_q: 0.756733
 49418/100000: episode: 638, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.403, 10.169], loss: 0.000014, mae: 0.002489, mean_q: 0.756463
 49519/100000: episode: 639, duration: 0.673s, episode steps: 101, steps per second: 150, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.199], loss: 0.000017, mae: 0.002603, mean_q: 0.756437
 49620/100000: episode: 640, duration: 0.684s, episode steps: 101, steps per second: 148, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.866, 10.411], loss: 0.000022, mae: 0.003073, mean_q: 0.756254
 49721/100000: episode: 641, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.384, 10.100], loss: 0.000020, mae: 0.002657, mean_q: 0.756213
 49822/100000: episode: 642, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.135, 10.132], loss: 0.000014, mae: 0.002775, mean_q: 0.756154
 49923/100000: episode: 643, duration: 0.652s, episode steps: 101, steps per second: 155, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.937, 10.100], loss: 0.000018, mae: 0.002528, mean_q: 0.756283
 50024/100000: episode: 644, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.625, 10.106], loss: 0.000015, mae: 0.002420, mean_q: 0.756263
 50125/100000: episode: 645, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.800, 10.181], loss: 0.000010, mae: 0.001746, mean_q: 0.756105
 50226/100000: episode: 646, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.569, 10.100], loss: 0.000019, mae: 0.002879, mean_q: 0.755924
 50327/100000: episode: 647, duration: 0.652s, episode steps: 101, steps per second: 155, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.240, 10.523], loss: 0.000019, mae: 0.003221, mean_q: 0.756211
 50428/100000: episode: 648, duration: 0.686s, episode steps: 101, steps per second: 147, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.151, 10.100], loss: 0.000008, mae: 0.001936, mean_q: 0.755902
 50529/100000: episode: 649, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.815, 10.183], loss: 0.000017, mae: 0.002522, mean_q: 0.756248
 50630/100000: episode: 650, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.100], loss: 0.000012, mae: 0.002013, mean_q: 0.756112
 50731/100000: episode: 651, duration: 0.782s, episode steps: 101, steps per second: 129, episode reward: 0.883, mean reward: 0.009 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.928, 10.100], loss: 0.000015, mae: 0.002439, mean_q: 0.756198
 50832/100000: episode: 652, duration: 0.708s, episode steps: 101, steps per second: 143, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.090, 10.100], loss: 0.000016, mae: 0.002309, mean_q: 0.756405
 50933/100000: episode: 653, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.551, 10.263], loss: 0.000010, mae: 0.002419, mean_q: 0.756157
 51034/100000: episode: 654, duration: 0.657s, episode steps: 101, steps per second: 154, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.305, 10.100], loss: 0.000020, mae: 0.002831, mean_q: 0.756514
 51135/100000: episode: 655, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.394, 10.100], loss: 0.000014, mae: 0.002406, mean_q: 0.756634
 51236/100000: episode: 656, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.197, 10.100], loss: 0.000014, mae: 0.002082, mean_q: 0.756388
 51337/100000: episode: 657, duration: 0.792s, episode steps: 101, steps per second: 128, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.327, 10.100], loss: 0.000015, mae: 0.002466, mean_q: 0.756662
 51438/100000: episode: 658, duration: 0.654s, episode steps: 101, steps per second: 154, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.347, 10.100], loss: 0.000018, mae: 0.002474, mean_q: 0.756406
 51539/100000: episode: 659, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.179, 10.100], loss: 0.000016, mae: 0.002905, mean_q: 0.756369
 51640/100000: episode: 660, duration: 0.659s, episode steps: 101, steps per second: 153, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.415, 10.100], loss: 0.000013, mae: 0.002471, mean_q: 0.756346
 51741/100000: episode: 661, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.714, 10.330], loss: 0.000019, mae: 0.002956, mean_q: 0.756605
 51842/100000: episode: 662, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.800, 10.100], loss: 0.000010, mae: 0.002034, mean_q: 0.756471
 51943/100000: episode: 663, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.304, 10.103], loss: 0.000008, mae: 0.001779, mean_q: 0.756755
 52044/100000: episode: 664, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.304, 10.100], loss: 0.000017, mae: 0.002854, mean_q: 0.756578
 52145/100000: episode: 665, duration: 0.665s, episode steps: 101, steps per second: 152, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.643, 10.298], loss: 0.000013, mae: 0.002032, mean_q: 0.756925
 52246/100000: episode: 666, duration: 0.695s, episode steps: 101, steps per second: 145, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.495, 10.209], loss: 0.000012, mae: 0.001991, mean_q: 0.756783
 52347/100000: episode: 667, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.809, 10.100], loss: 0.000012, mae: 0.002188, mean_q: 0.756718
 52448/100000: episode: 668, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.438, 10.192], loss: 0.000012, mae: 0.002606, mean_q: 0.756522
 52549/100000: episode: 669, duration: 0.711s, episode steps: 101, steps per second: 142, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.188, 10.287], loss: 0.000012, mae: 0.002279, mean_q: 0.756494
[Info] 1-TH LEVEL FOUND: 0.7753348350524902, Considering 10/100 traces
 52650/100000: episode: 670, duration: 5.348s, episode steps: 101, steps per second: 19, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.589, 10.141], loss: 0.000007, mae: 0.001811, mean_q: 0.756412
 52672/100000: episode: 671, duration: 0.147s, episode steps: 22, steps per second: 150, episode reward: 0.677, mean reward: 0.031 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.215, 10.310], loss: 0.000004, mae: 0.002131, mean_q: 0.756454
 52710/100000: episode: 672, duration: 0.224s, episode steps: 38, steps per second: 169, episode reward: 0.785, mean reward: 0.021 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.882, 10.415], loss: 0.000014, mae: 0.001927, mean_q: 0.756540
 52732/100000: episode: 673, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 0.786, mean reward: 0.036 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.859, 10.366], loss: 0.000046, mae: 0.002894, mean_q: 0.756357
 52770/100000: episode: 674, duration: 0.223s, episode steps: 38, steps per second: 170, episode reward: 0.888, mean reward: 0.023 [0.000, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.298, 10.451], loss: 0.000010, mae: 0.002974, mean_q: 0.756611
 52808/100000: episode: 675, duration: 0.224s, episode steps: 38, steps per second: 170, episode reward: 0.835, mean reward: 0.022 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.584, 10.381], loss: 0.000020, mae: 0.002577, mean_q: 0.757370
 52846/100000: episode: 676, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 0.931, mean reward: 0.025 [0.000, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.069, 10.617], loss: 0.000018, mae: 0.002489, mean_q: 0.757106
 52884/100000: episode: 677, duration: 0.252s, episode steps: 38, steps per second: 151, episode reward: 0.779, mean reward: 0.020 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.220, 10.100], loss: 0.000013, mae: 0.002018, mean_q: 0.757396
 52922/100000: episode: 678, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 0.878, mean reward: 0.023 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.357, 10.305], loss: 0.000009, mae: 0.002026, mean_q: 0.757081
 52960/100000: episode: 679, duration: 0.232s, episode steps: 38, steps per second: 164, episode reward: 0.758, mean reward: 0.020 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.731, 10.424], loss: 0.000019, mae: 0.002541, mean_q: 0.757138
 52979/100000: episode: 680, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 0.785, mean reward: 0.041 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.538], loss: 0.000004, mae: 0.001969, mean_q: 0.757058
 53001/100000: episode: 681, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 0.741, mean reward: 0.034 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.370, 10.281], loss: 0.000014, mae: 0.001955, mean_q: 0.756833
 53017/100000: episode: 682, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 0.743, mean reward: 0.046 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-1.004, 10.371], loss: 0.000041, mae: 0.004684, mean_q: 0.757315
 53055/100000: episode: 683, duration: 0.225s, episode steps: 38, steps per second: 169, episode reward: 0.773, mean reward: 0.020 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.557, 10.100], loss: 0.000038, mae: 0.004673, mean_q: 0.757087
 53077/100000: episode: 684, duration: 0.144s, episode steps: 22, steps per second: 153, episode reward: 0.685, mean reward: 0.031 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.064, 10.190], loss: 0.000026, mae: 0.002568, mean_q: 0.757858
 53115/100000: episode: 685, duration: 0.231s, episode steps: 38, steps per second: 165, episode reward: 0.827, mean reward: 0.022 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.314, 10.381], loss: 0.000033, mae: 0.003345, mean_q: 0.757073
 53141/100000: episode: 686, duration: 0.163s, episode steps: 26, steps per second: 159, episode reward: 0.738, mean reward: 0.028 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.671, 10.274], loss: 0.000030, mae: 0.004942, mean_q: 0.756921
 53152/100000: episode: 687, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 0.688, mean reward: 0.063 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.259], loss: 0.000006, mae: 0.002745, mean_q: 0.758173
 53190/100000: episode: 688, duration: 0.231s, episode steps: 38, steps per second: 165, episode reward: 0.825, mean reward: 0.022 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.843, 10.293], loss: 0.000012, mae: 0.002166, mean_q: 0.757261
 53228/100000: episode: 689, duration: 0.241s, episode steps: 38, steps per second: 157, episode reward: 0.723, mean reward: 0.019 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.804, 10.100], loss: 0.000021, mae: 0.003001, mean_q: 0.757228
 53266/100000: episode: 690, duration: 0.229s, episode steps: 38, steps per second: 166, episode reward: 0.788, mean reward: 0.021 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.979, 10.289], loss: 0.000012, mae: 0.002084, mean_q: 0.756689
 53285/100000: episode: 691, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 0.844, mean reward: 0.044 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.566, 10.483], loss: 0.000022, mae: 0.002057, mean_q: 0.757504
 53297/100000: episode: 692, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 0.695, mean reward: 0.058 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.587, 10.381], loss: 0.000045, mae: 0.002127, mean_q: 0.757545
 53318/100000: episode: 693, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 0.787, mean reward: 0.037 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.717, 10.481], loss: 0.000010, mae: 0.002785, mean_q: 0.757496
 53329/100000: episode: 694, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 0.649, mean reward: 0.059 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.228], loss: 0.000027, mae: 0.003209, mean_q: 0.757793
 53348/100000: episode: 695, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 0.724, mean reward: 0.038 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.175, 10.386], loss: 0.000018, mae: 0.003306, mean_q: 0.757046
 53370/100000: episode: 696, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 0.731, mean reward: 0.033 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.228, 10.177], loss: 0.000019, mae: 0.002646, mean_q: 0.756732
 53396/100000: episode: 697, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 0.720, mean reward: 0.028 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.278, 10.100], loss: 0.000011, mae: 0.002466, mean_q: 0.756992
 53417/100000: episode: 698, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 0.734, mean reward: 0.035 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.277, 10.449], loss: 0.000007, mae: 0.001819, mean_q: 0.756752
 53455/100000: episode: 699, duration: 0.231s, episode steps: 38, steps per second: 164, episode reward: 0.842, mean reward: 0.022 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.081, 10.388], loss: 0.000024, mae: 0.003146, mean_q: 0.757640
 53476/100000: episode: 700, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 0.655, mean reward: 0.031 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.378, 10.238], loss: 0.000037, mae: 0.003040, mean_q: 0.758101
 53495/100000: episode: 701, duration: 0.122s, episode steps: 19, steps per second: 155, episode reward: 0.807, mean reward: 0.042 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.502], loss: 0.000018, mae: 0.003186, mean_q: 0.757000
 53514/100000: episode: 702, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 0.878, mean reward: 0.046 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.125, 10.539], loss: 0.000013, mae: 0.002507, mean_q: 0.756943
 53526/100000: episode: 703, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 0.678, mean reward: 0.057 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.282], loss: 0.000014, mae: 0.002688, mean_q: 0.757838
 53538/100000: episode: 704, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 0.718, mean reward: 0.060 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.987, 10.305], loss: 0.000006, mae: 0.002030, mean_q: 0.757601
 53549/100000: episode: 705, duration: 0.065s, episode steps: 11, steps per second: 171, episode reward: 0.775, mean reward: 0.070 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.414, 10.418], loss: 0.000015, mae: 0.001729, mean_q: 0.757813
 53587/100000: episode: 706, duration: 0.236s, episode steps: 38, steps per second: 161, episode reward: 0.871, mean reward: 0.023 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.590, 10.326], loss: 0.000038, mae: 0.003183, mean_q: 0.757709
 53598/100000: episode: 707, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 0.707, mean reward: 0.064 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.316], loss: 0.000048, mae: 0.003196, mean_q: 0.757878
 53610/100000: episode: 708, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 0.720, mean reward: 0.060 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.449], loss: 0.000023, mae: 0.002980, mean_q: 0.757885
 53632/100000: episode: 709, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 0.662, mean reward: 0.030 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.459, 10.122], loss: 0.000005, mae: 0.002240, mean_q: 0.757178
 53670/100000: episode: 710, duration: 0.232s, episode steps: 38, steps per second: 164, episode reward: 0.770, mean reward: 0.020 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.065, 10.403], loss: 0.000020, mae: 0.002902, mean_q: 0.757738
 53692/100000: episode: 711, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 0.870, mean reward: 0.040 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.999, 10.421], loss: 0.000064, mae: 0.006828, mean_q: 0.757696
 53711/100000: episode: 712, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 0.694, mean reward: 0.037 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.379], loss: 0.000046, mae: 0.004835, mean_q: 0.757366
 53749/100000: episode: 713, duration: 0.230s, episode steps: 38, steps per second: 165, episode reward: 0.769, mean reward: 0.020 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.039, 10.165], loss: 0.000025, mae: 0.003103, mean_q: 0.757762
 53775/100000: episode: 714, duration: 0.177s, episode steps: 26, steps per second: 147, episode reward: 0.820, mean reward: 0.032 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.477], loss: 0.000032, mae: 0.003509, mean_q: 0.757375
 53796/100000: episode: 715, duration: 0.138s, episode steps: 21, steps per second: 153, episode reward: 0.660, mean reward: 0.031 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.052, 10.157], loss: 0.000031, mae: 0.002648, mean_q: 0.757388
 53812/100000: episode: 716, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 0.886, mean reward: 0.055 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.512, 10.292], loss: 0.000011, mae: 0.002131, mean_q: 0.757649
 53833/100000: episode: 717, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 0.723, mean reward: 0.034 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.037, 10.418], loss: 0.000013, mae: 0.001364, mean_q: 0.757245
 53859/100000: episode: 718, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 0.748, mean reward: 0.029 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.263], loss: 0.000040, mae: 0.004466, mean_q: 0.757561
 53878/100000: episode: 719, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.706, mean reward: 0.037 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.340], loss: 0.000024, mae: 0.003337, mean_q: 0.756329
 53894/100000: episode: 720, duration: 0.151s, episode steps: 16, steps per second: 106, episode reward: 0.711, mean reward: 0.044 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.432], loss: 0.000072, mae: 0.003734, mean_q: 0.757203
 53916/100000: episode: 721, duration: 0.177s, episode steps: 22, steps per second: 124, episode reward: 0.783, mean reward: 0.036 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.202, 10.413], loss: 0.000024, mae: 0.002892, mean_q: 0.757304
 53932/100000: episode: 722, duration: 0.147s, episode steps: 16, steps per second: 109, episode reward: 0.690, mean reward: 0.043 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.321], loss: 0.000030, mae: 0.003298, mean_q: 0.756936
 53970/100000: episode: 723, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 0.768, mean reward: 0.020 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.060, 10.141], loss: 0.000022, mae: 0.002525, mean_q: 0.757442
 53989/100000: episode: 724, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 0.777, mean reward: 0.041 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.461], loss: 0.000037, mae: 0.002600, mean_q: 0.757088
 54015/100000: episode: 725, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 0.752, mean reward: 0.029 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.231], loss: 0.000020, mae: 0.003689, mean_q: 0.757131
 54037/100000: episode: 726, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 0.723, mean reward: 0.033 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.282, 10.310], loss: 0.000036, mae: 0.003531, mean_q: 0.758064
 54048/100000: episode: 727, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 0.715, mean reward: 0.065 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.446], loss: 0.000054, mae: 0.003904, mean_q: 0.757069
 54086/100000: episode: 728, duration: 0.234s, episode steps: 38, steps per second: 163, episode reward: 0.792, mean reward: 0.021 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.789, 10.308], loss: 0.000066, mae: 0.005472, mean_q: 0.757845
 54107/100000: episode: 729, duration: 0.140s, episode steps: 21, steps per second: 150, episode reward: 0.697, mean reward: 0.033 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.284, 10.158], loss: 0.000083, mae: 0.004415, mean_q: 0.758712
 54133/100000: episode: 730, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 0.762, mean reward: 0.029 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.436], loss: 0.000064, mae: 0.005892, mean_q: 0.758471
 54149/100000: episode: 731, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 0.753, mean reward: 0.047 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.621, 10.462], loss: 0.000052, mae: 0.004325, mean_q: 0.759064
 54171/100000: episode: 732, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 0.798, mean reward: 0.036 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.330, 10.415], loss: 0.000059, mae: 0.003856, mean_q: 0.758412
 54182/100000: episode: 733, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 0.805, mean reward: 0.073 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.242, 10.484], loss: 0.000057, mae: 0.002846, mean_q: 0.757826
 54203/100000: episode: 734, duration: 0.131s, episode steps: 21, steps per second: 161, episode reward: 0.812, mean reward: 0.039 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.471, 10.478], loss: 0.000048, mae: 0.004284, mean_q: 0.757783
 54224/100000: episode: 735, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 0.652, mean reward: 0.031 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.049, 10.264], loss: 0.000053, mae: 0.003472, mean_q: 0.757754
 54243/100000: episode: 736, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 0.696, mean reward: 0.037 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.285], loss: 0.000031, mae: 0.004175, mean_q: 0.757903
 54259/100000: episode: 737, duration: 0.103s, episode steps: 16, steps per second: 155, episode reward: 0.663, mean reward: 0.041 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.485, 10.245], loss: 0.000056, mae: 0.002995, mean_q: 0.758214
 54297/100000: episode: 738, duration: 0.229s, episode steps: 38, steps per second: 166, episode reward: 0.824, mean reward: 0.022 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.218, 10.184], loss: 0.000042, mae: 0.003175, mean_q: 0.758612
 54313/100000: episode: 739, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 0.737, mean reward: 0.046 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.134, 10.371], loss: 0.000023, mae: 0.004190, mean_q: 0.758314
 54324/100000: episode: 740, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 0.662, mean reward: 0.060 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.342], loss: 0.000064, mae: 0.003578, mean_q: 0.758128
 54346/100000: episode: 741, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 0.702, mean reward: 0.032 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.259], loss: 0.000038, mae: 0.003731, mean_q: 0.757628
 54357/100000: episode: 742, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 0.791, mean reward: 0.072 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.194, 10.511], loss: 0.000030, mae: 0.003933, mean_q: 0.756854
 54379/100000: episode: 743, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 0.733, mean reward: 0.033 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.126, 10.349], loss: 0.000048, mae: 0.004390, mean_q: 0.757406
 54401/100000: episode: 744, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 0.735, mean reward: 0.033 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.281, 10.141], loss: 0.000035, mae: 0.003355, mean_q: 0.757785
 54423/100000: episode: 745, duration: 0.144s, episode steps: 22, steps per second: 152, episode reward: 0.714, mean reward: 0.032 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.055, 10.276], loss: 0.000023, mae: 0.003006, mean_q: 0.757862
 54461/100000: episode: 746, duration: 0.228s, episode steps: 38, steps per second: 167, episode reward: 0.823, mean reward: 0.022 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.030, 10.330], loss: 0.000024, mae: 0.002208, mean_q: 0.757756
 54477/100000: episode: 747, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 0.683, mean reward: 0.043 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.159, 10.368], loss: 0.000037, mae: 0.002563, mean_q: 0.757774
 54489/100000: episode: 748, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 0.738, mean reward: 0.061 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.084, 10.333], loss: 0.000023, mae: 0.002031, mean_q: 0.757776
 54511/100000: episode: 749, duration: 0.149s, episode steps: 22, steps per second: 147, episode reward: 0.782, mean reward: 0.036 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.082, 10.369], loss: 0.000061, mae: 0.003010, mean_q: 0.758509
 54527/100000: episode: 750, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 0.767, mean reward: 0.048 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.412], loss: 0.000037, mae: 0.003550, mean_q: 0.757512
 54549/100000: episode: 751, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 0.757, mean reward: 0.034 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.120, 10.301], loss: 0.000056, mae: 0.004540, mean_q: 0.757769
 54561/100000: episode: 752, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 0.742, mean reward: 0.062 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.442], loss: 0.000037, mae: 0.006046, mean_q: 0.757150
 54572/100000: episode: 753, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 0.720, mean reward: 0.065 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.385, 10.334], loss: 0.000038, mae: 0.003419, mean_q: 0.757350
 54588/100000: episode: 754, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 0.722, mean reward: 0.045 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.213], loss: 0.000080, mae: 0.004093, mean_q: 0.757113
 54609/100000: episode: 755, duration: 0.135s, episode steps: 21, steps per second: 156, episode reward: 0.669, mean reward: 0.032 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.337], loss: 0.000060, mae: 0.004345, mean_q: 0.757557
 54647/100000: episode: 756, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 0.794, mean reward: 0.021 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.443, 10.521], loss: 0.000056, mae: 0.005799, mean_q: 0.757940
 54669/100000: episode: 757, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 0.752, mean reward: 0.034 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.225, 10.271], loss: 0.000057, mae: 0.004111, mean_q: 0.757869
 54695/100000: episode: 758, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 0.701, mean reward: 0.027 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.264, 10.275], loss: 0.000032, mae: 0.002636, mean_q: 0.757893
 54711/100000: episode: 759, duration: 0.113s, episode steps: 16, steps per second: 142, episode reward: 0.680, mean reward: 0.042 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.341], loss: 0.000039, mae: 0.003230, mean_q: 0.757805
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7753348350524902
1
 54722/100000: episode: 760, duration: 4.406s, episode steps: 11, steps per second: 2, episode reward: 0.695, mean reward: 0.063 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.461, 10.426], loss: 0.000042, mae: 0.003322, mean_q: 0.757734
 54823/100000: episode: 761, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.313, 10.155], loss: 0.000048, mae: 0.003526, mean_q: 0.757634
 54924/100000: episode: 762, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.266, 10.270], loss: 0.000056, mae: 0.004674, mean_q: 0.757301
 55025/100000: episode: 763, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.136, 10.450], loss: 0.000041, mae: 0.003095, mean_q: 0.757750
 55126/100000: episode: 764, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.976, 10.100], loss: 0.000041, mae: 0.003283, mean_q: 0.757287
 55227/100000: episode: 765, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.168, 10.100], loss: 0.000037, mae: 0.003123, mean_q: 0.757514
 55328/100000: episode: 766, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.726, 10.100], loss: 0.000029, mae: 0.003143, mean_q: 0.756877
 55429/100000: episode: 767, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.042, 10.352], loss: 0.000047, mae: 0.004202, mean_q: 0.756935
 55530/100000: episode: 768, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.387, 10.447], loss: 0.000036, mae: 0.003563, mean_q: 0.756605
 55631/100000: episode: 769, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.351, 10.295], loss: 0.000056, mae: 0.003981, mean_q: 0.756910
 55732/100000: episode: 770, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.705, 10.100], loss: 0.000059, mae: 0.004117, mean_q: 0.756342
 55833/100000: episode: 771, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.670, 10.281], loss: 0.000043, mae: 0.003447, mean_q: 0.756353
 55934/100000: episode: 772, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.250, 10.100], loss: 0.000037, mae: 0.003467, mean_q: 0.756219
 56035/100000: episode: 773, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.620, 10.305], loss: 0.000046, mae: 0.003877, mean_q: 0.755814
 56136/100000: episode: 774, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.613, 10.339], loss: 0.000032, mae: 0.003163, mean_q: 0.755882
 56237/100000: episode: 775, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.295, 10.243], loss: 0.000038, mae: 0.003916, mean_q: 0.755551
 56338/100000: episode: 776, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.825, 10.100], loss: 0.000044, mae: 0.003735, mean_q: 0.755613
 56439/100000: episode: 777, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.144, 10.100], loss: 0.000038, mae: 0.003101, mean_q: 0.755568
 56540/100000: episode: 778, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.059, 10.251], loss: 0.000047, mae: 0.004379, mean_q: 0.755068
 56641/100000: episode: 779, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.577, 10.100], loss: 0.000056, mae: 0.003724, mean_q: 0.754552
 56742/100000: episode: 780, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.954, 10.100], loss: 0.000056, mae: 0.004506, mean_q: 0.754600
 56843/100000: episode: 781, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.986, 10.332], loss: 0.000051, mae: 0.003936, mean_q: 0.754505
 56944/100000: episode: 782, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.305, 10.115], loss: 0.000049, mae: 0.004349, mean_q: 0.754396
 57045/100000: episode: 783, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.143, 10.264], loss: 0.000045, mae: 0.003343, mean_q: 0.754341
 57146/100000: episode: 784, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.564, 10.100], loss: 0.000053, mae: 0.003758, mean_q: 0.754192
 57247/100000: episode: 785, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.564, 10.100], loss: 0.000051, mae: 0.004169, mean_q: 0.753583
 57348/100000: episode: 786, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.695, 10.198], loss: 0.000064, mae: 0.004412, mean_q: 0.754085
 57449/100000: episode: 787, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.886, 10.155], loss: 0.000054, mae: 0.004119, mean_q: 0.753985
 57550/100000: episode: 788, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.352, 10.100], loss: 0.000064, mae: 0.004269, mean_q: 0.753722
 57651/100000: episode: 789, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.186, 10.100], loss: 0.000051, mae: 0.003630, mean_q: 0.753944
 57752/100000: episode: 790, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.978, 10.100], loss: 0.000048, mae: 0.003458, mean_q: 0.753672
 57853/100000: episode: 791, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.885, 10.100], loss: 0.000033, mae: 0.003010, mean_q: 0.753919
 57954/100000: episode: 792, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.884, 10.100], loss: 0.000055, mae: 0.004187, mean_q: 0.753832
 58055/100000: episode: 793, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.822, 10.100], loss: 0.000051, mae: 0.004301, mean_q: 0.753359
 58156/100000: episode: 794, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.635, 10.100], loss: 0.000045, mae: 0.004211, mean_q: 0.752726
 58257/100000: episode: 795, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.164, 10.187], loss: 0.000023, mae: 0.002241, mean_q: 0.752564
 58358/100000: episode: 796, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.724, 10.100], loss: 0.000027, mae: 0.003022, mean_q: 0.752235
 58459/100000: episode: 797, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.850, mean reward: 0.008 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.326, 10.257], loss: 0.000023, mae: 0.002521, mean_q: 0.752115
 58560/100000: episode: 798, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.357, 10.119], loss: 0.000030, mae: 0.002981, mean_q: 0.752056
 58661/100000: episode: 799, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.088, 10.398], loss: 0.000028, mae: 0.003190, mean_q: 0.752059
 58762/100000: episode: 800, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.689, 10.100], loss: 0.000025, mae: 0.002720, mean_q: 0.752168
 58863/100000: episode: 801, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.584, 10.100], loss: 0.000021, mae: 0.002404, mean_q: 0.751637
 58964/100000: episode: 802, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.017, 10.188], loss: 0.000020, mae: 0.002399, mean_q: 0.751735
 59065/100000: episode: 803, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.537, 10.117], loss: 0.000033, mae: 0.003419, mean_q: 0.751698
 59166/100000: episode: 804, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.233, 10.100], loss: 0.000023, mae: 0.002425, mean_q: 0.751774
 59267/100000: episode: 805, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.969, 10.100], loss: 0.000022, mae: 0.002748, mean_q: 0.751366
 59368/100000: episode: 806, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.697, 10.429], loss: 0.000022, mae: 0.002585, mean_q: 0.751378
 59469/100000: episode: 807, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.043, 10.145], loss: 0.000021, mae: 0.002847, mean_q: 0.751606
 59570/100000: episode: 808, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.963, 10.100], loss: 0.000011, mae: 0.002012, mean_q: 0.751460
 59671/100000: episode: 809, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.866, mean reward: 0.009 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.451, 10.282], loss: 0.000015, mae: 0.001853, mean_q: 0.751113
 59772/100000: episode: 810, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.385, 10.100], loss: 0.000012, mae: 0.001532, mean_q: 0.751646
 59873/100000: episode: 811, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.435, 10.265], loss: 0.000011, mae: 0.001878, mean_q: 0.751435
 59974/100000: episode: 812, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.897, 10.100], loss: 0.000010, mae: 0.001950, mean_q: 0.751352
 60075/100000: episode: 813, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.955, 10.248], loss: 0.000012, mae: 0.001874, mean_q: 0.750968
 60176/100000: episode: 814, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.721, 10.106], loss: 0.000013, mae: 0.002099, mean_q: 0.751077
 60277/100000: episode: 815, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.725, 10.100], loss: 0.000014, mae: 0.002160, mean_q: 0.751298
 60378/100000: episode: 816, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.227, 10.100], loss: 0.000017, mae: 0.002418, mean_q: 0.751141
 60479/100000: episode: 817, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.100], loss: 0.000014, mae: 0.002270, mean_q: 0.751248
 60580/100000: episode: 818, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.227, 10.189], loss: 0.000014, mae: 0.002428, mean_q: 0.751191
 60681/100000: episode: 819, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.962, 10.227], loss: 0.000009, mae: 0.001524, mean_q: 0.751118
 60782/100000: episode: 820, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.653, 10.138], loss: 0.000010, mae: 0.001804, mean_q: 0.751221
 60883/100000: episode: 821, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.878, 10.362], loss: 0.000006, mae: 0.001560, mean_q: 0.751175
 60984/100000: episode: 822, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.115, 10.100], loss: 0.000013, mae: 0.001868, mean_q: 0.751253
 61085/100000: episode: 823, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.557, 10.150], loss: 0.000015, mae: 0.002158, mean_q: 0.751323
 61186/100000: episode: 824, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.093, 10.167], loss: 0.000014, mae: 0.002676, mean_q: 0.751123
 61287/100000: episode: 825, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.005, 10.100], loss: 0.000009, mae: 0.002004, mean_q: 0.750942
 61388/100000: episode: 826, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.030, 10.335], loss: 0.000011, mae: 0.002298, mean_q: 0.750838
 61489/100000: episode: 827, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.802, 10.112], loss: 0.000010, mae: 0.001813, mean_q: 0.751046
 61590/100000: episode: 828, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.492, 10.100], loss: 0.000015, mae: 0.002229, mean_q: 0.751085
 61691/100000: episode: 829, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.722, 10.312], loss: 0.000013, mae: 0.002449, mean_q: 0.751084
 61792/100000: episode: 830, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.770, 10.337], loss: 0.000015, mae: 0.002288, mean_q: 0.750974
 61893/100000: episode: 831, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.042, 10.289], loss: 0.000011, mae: 0.001862, mean_q: 0.750886
 61994/100000: episode: 832, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.561, 10.100], loss: 0.000012, mae: 0.002231, mean_q: 0.751134
 62095/100000: episode: 833, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.688, 10.139], loss: 0.000012, mae: 0.002313, mean_q: 0.750853
 62196/100000: episode: 834, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.100], loss: 0.000017, mae: 0.002572, mean_q: 0.751578
 62297/100000: episode: 835, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.932, mean reward: 0.009 [0.000, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.768, 10.109], loss: 0.000012, mae: 0.002109, mean_q: 0.751312
 62398/100000: episode: 836, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.149, 10.100], loss: 0.000019, mae: 0.002221, mean_q: 0.751440
 62499/100000: episode: 837, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.430, 10.106], loss: 0.000016, mae: 0.002502, mean_q: 0.751013
 62600/100000: episode: 838, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.980, 10.231], loss: 0.000013, mae: 0.002245, mean_q: 0.751345
 62701/100000: episode: 839, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.309, 10.100], loss: 0.000014, mae: 0.002080, mean_q: 0.751249
 62802/100000: episode: 840, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.219, 10.121], loss: 0.000013, mae: 0.001890, mean_q: 0.751197
 62903/100000: episode: 841, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.147, 10.114], loss: 0.000026, mae: 0.003327, mean_q: 0.751473
 63004/100000: episode: 842, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.849, mean reward: 0.008 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.856, 10.100], loss: 0.000021, mae: 0.002543, mean_q: 0.751993
 63105/100000: episode: 843, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.474, 10.100], loss: 0.000016, mae: 0.002395, mean_q: 0.751785
 63206/100000: episode: 844, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.852, 10.135], loss: 0.000016, mae: 0.002281, mean_q: 0.751981
 63307/100000: episode: 845, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.192, 10.192], loss: 0.000012, mae: 0.002074, mean_q: 0.751599
 63408/100000: episode: 846, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.836, 10.100], loss: 0.000014, mae: 0.002440, mean_q: 0.751480
 63509/100000: episode: 847, duration: 0.599s, episode steps: 101, steps per second: 168, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.376, 10.100], loss: 0.000011, mae: 0.002006, mean_q: 0.751602
 63610/100000: episode: 848, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.473, 10.287], loss: 0.000020, mae: 0.002888, mean_q: 0.751712
 63711/100000: episode: 849, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.899, mean reward: 0.009 [0.000, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.062, 10.410], loss: 0.000021, mae: 0.002558, mean_q: 0.751670
 63812/100000: episode: 850, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.100], loss: 0.000013, mae: 0.002251, mean_q: 0.751664
 63913/100000: episode: 851, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.695, 10.118], loss: 0.000024, mae: 0.003158, mean_q: 0.751989
 64014/100000: episode: 852, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.953, 10.167], loss: 0.000021, mae: 0.002853, mean_q: 0.751766
 64115/100000: episode: 853, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.128, 10.133], loss: 0.000009, mae: 0.001773, mean_q: 0.751649
 64216/100000: episode: 854, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.184, 10.231], loss: 0.000011, mae: 0.001912, mean_q: 0.751459
 64317/100000: episode: 855, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.528, 10.354], loss: 0.000013, mae: 0.001946, mean_q: 0.751645
 64418/100000: episode: 856, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.581, 10.100], loss: 0.000013, mae: 0.002290, mean_q: 0.751758
 64519/100000: episode: 857, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.100], loss: 0.000020, mae: 0.002471, mean_q: 0.751518
 64620/100000: episode: 858, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.436, 10.100], loss: 0.000018, mae: 0.001944, mean_q: 0.751345
 64721/100000: episode: 859, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.601, 10.152], loss: 0.000019, mae: 0.002393, mean_q: 0.751825
[Info] 1-TH LEVEL FOUND: 0.7668793797492981, Considering 10/100 traces
 64822/100000: episode: 860, duration: 5.146s, episode steps: 101, steps per second: 20, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.988, 10.404], loss: 0.000023, mae: 0.002644, mean_q: 0.751716
 64832/100000: episode: 861, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.715, mean reward: 0.072 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.265], loss: 0.000006, mae: 0.001954, mean_q: 0.752167
 64842/100000: episode: 862, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 0.693, mean reward: 0.069 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.072, 10.287], loss: 0.000004, mae: 0.001463, mean_q: 0.751953
 64866/100000: episode: 863, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 0.846, mean reward: 0.035 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.207, 10.469], loss: 0.000018, mae: 0.001973, mean_q: 0.751812
 64875/100000: episode: 864, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 0.723, mean reward: 0.080 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.449], loss: 0.000012, mae: 0.001622, mean_q: 0.751911
 64877/100000: episode: 865, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.668, mean reward: 0.334 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.258], loss: 0.000001, mae: 0.000902, mean_q: 0.750743
 64887/100000: episode: 866, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 0.840, mean reward: 0.084 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-1.688, 10.456], loss: 0.000029, mae: 0.002586, mean_q: 0.751836
 64897/100000: episode: 867, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 0.770, mean reward: 0.077 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.529], loss: 0.000012, mae: 0.002735, mean_q: 0.751146
 64904/100000: episode: 868, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 0.700, mean reward: 0.100 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.408], loss: 0.000002, mae: 0.001855, mean_q: 0.751833
 64928/100000: episode: 869, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 0.750, mean reward: 0.031 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.542, 10.278], loss: 0.000032, mae: 0.002425, mean_q: 0.752910
 64932/100000: episode: 870, duration: 0.036s, episode steps: 4, steps per second: 112, episode reward: 0.812, mean reward: 0.203 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.578, 10.372], loss: 0.000023, mae: 0.001898, mean_q: 0.751688
 64951/100000: episode: 871, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.915, mean reward: 0.048 [0.000, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.045, 10.516], loss: 0.000010, mae: 0.001914, mean_q: 0.752075
 64958/100000: episode: 872, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 0.714, mean reward: 0.102 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.389], loss: 0.000018, mae: 0.002056, mean_q: 0.751918
 64977/100000: episode: 873, duration: 0.132s, episode steps: 19, steps per second: 144, episode reward: 0.781, mean reward: 0.041 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.454, 10.301], loss: 0.000026, mae: 0.002178, mean_q: 0.752172
 64987/100000: episode: 874, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 0.735, mean reward: 0.073 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.249], loss: 0.000008, mae: 0.002116, mean_q: 0.752825
 64991/100000: episode: 875, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 0.742, mean reward: 0.186 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.203, 10.333], loss: 0.000010, mae: 0.001912, mean_q: 0.751378
 65003/100000: episode: 876, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 0.743, mean reward: 0.062 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.404, 10.487], loss: 0.000016, mae: 0.002403, mean_q: 0.752505
 65005/100000: episode: 877, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 0.663, mean reward: 0.331 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.253], loss: 0.000003, mae: 0.002114, mean_q: 0.749249
 65014/100000: episode: 878, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 0.709, mean reward: 0.079 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.401], loss: 0.000048, mae: 0.002315, mean_q: 0.753659
 65018/100000: episode: 879, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 0.698, mean reward: 0.175 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.359], loss: 0.000008, mae: 0.001522, mean_q: 0.752073
 65030/100000: episode: 880, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 0.953, mean reward: 0.079 [0.000, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.228, 10.382], loss: 0.000032, mae: 0.002816, mean_q: 0.752581
 65049/100000: episode: 881, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 0.741, mean reward: 0.039 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.316, 10.314], loss: 0.000050, mae: 0.004584, mean_q: 0.753026
 65061/100000: episode: 882, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 0.692, mean reward: 0.058 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.407, 10.268], loss: 0.000050, mae: 0.002879, mean_q: 0.752115
 65085/100000: episode: 883, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 0.806, mean reward: 0.034 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.318, 10.458], loss: 0.000038, mae: 0.002465, mean_q: 0.752918
 65095/100000: episode: 884, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 0.781, mean reward: 0.078 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.387, 10.496], loss: 0.000012, mae: 0.001856, mean_q: 0.752786
 65105/100000: episode: 885, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 0.725, mean reward: 0.073 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.476], loss: 0.000006, mae: 0.001368, mean_q: 0.752748
 65117/100000: episode: 886, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 0.864, mean reward: 0.072 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.274, 10.310], loss: 0.000028, mae: 0.002706, mean_q: 0.753183
 65136/100000: episode: 887, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 0.806, mean reward: 0.042 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.082, 10.342], loss: 0.000017, mae: 0.002298, mean_q: 0.752702
 65143/100000: episode: 888, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 0.737, mean reward: 0.105 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.478], loss: 0.000005, mae: 0.001699, mean_q: 0.753897
 65162/100000: episode: 889, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 0.752, mean reward: 0.040 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.255, 10.501], loss: 0.000020, mae: 0.002060, mean_q: 0.753006
 65164/100000: episode: 890, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 0.726, mean reward: 0.363 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.237], loss: 0.000005, mae: 0.002777, mean_q: 0.750218
 65171/100000: episode: 891, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 0.695, mean reward: 0.099 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.590, 10.396], loss: 0.000040, mae: 0.002997, mean_q: 0.751994
 65190/100000: episode: 892, duration: 0.118s, episode steps: 19, steps per second: 162, episode reward: 0.870, mean reward: 0.046 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.489], loss: 0.000044, mae: 0.004375, mean_q: 0.752854
 65214/100000: episode: 893, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 0.791, mean reward: 0.033 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.243], loss: 0.000018, mae: 0.003582, mean_q: 0.753620
 65221/100000: episode: 894, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.719, mean reward: 0.103 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.441], loss: 0.000012, mae: 0.002596, mean_q: 0.752850
 65228/100000: episode: 895, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 0.734, mean reward: 0.105 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.223, 10.426], loss: 0.000009, mae: 0.002833, mean_q: 0.752775
 65247/100000: episode: 896, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 0.708, mean reward: 0.037 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.206, 10.353], loss: 0.000009, mae: 0.001879, mean_q: 0.753344
 65251/100000: episode: 897, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 0.713, mean reward: 0.178 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.325], loss: 0.000013, mae: 0.001569, mean_q: 0.753455
 65255/100000: episode: 898, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 0.769, mean reward: 0.192 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.250, 10.355], loss: 0.000042, mae: 0.002085, mean_q: 0.752224
 65262/100000: episode: 899, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.729, mean reward: 0.104 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.465], loss: 0.000024, mae: 0.002382, mean_q: 0.752869
 65264/100000: episode: 900, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.673, mean reward: 0.337 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.261], loss: 0.000085, mae: 0.003203, mean_q: 0.753810
 65283/100000: episode: 901, duration: 0.124s, episode steps: 19, steps per second: 153, episode reward: 0.850, mean reward: 0.045 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.310, 10.504], loss: 0.000020, mae: 0.002040, mean_q: 0.753343
 65285/100000: episode: 902, duration: 0.021s, episode steps: 2, steps per second: 95, episode reward: 0.677, mean reward: 0.339 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.193], loss: 0.000019, mae: 0.005594, mean_q: 0.747755
 65287/100000: episode: 903, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.648, mean reward: 0.324 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.346], loss: 0.000081, mae: 0.007749, mean_q: 0.760005
 65291/100000: episode: 904, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.743, mean reward: 0.186 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.289], loss: 0.000025, mae: 0.003773, mean_q: 0.751135
 65298/100000: episode: 905, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 0.753, mean reward: 0.108 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.578, 10.455], loss: 0.000059, mae: 0.004209, mean_q: 0.754989
 65317/100000: episode: 906, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 0.859, mean reward: 0.045 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.760, 10.634], loss: 0.000041, mae: 0.004437, mean_q: 0.753298
 65319/100000: episode: 907, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.798, mean reward: 0.399 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.309], loss: 0.000012, mae: 0.004626, mean_q: 0.758156
 65329/100000: episode: 908, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.752, mean reward: 0.075 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.287], loss: 0.000054, mae: 0.004900, mean_q: 0.753065
 65331/100000: episode: 909, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.783, mean reward: 0.392 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.313], loss: 0.000002, mae: 0.001857, mean_q: 0.751921
 65335/100000: episode: 910, duration: 0.033s, episode steps: 4, steps per second: 122, episode reward: 0.683, mean reward: 0.171 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.381], loss: 0.000113, mae: 0.004871, mean_q: 0.754058
 65359/100000: episode: 911, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 0.791, mean reward: 0.033 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.531, 10.256], loss: 0.000029, mae: 0.003281, mean_q: 0.753681
 65363/100000: episode: 912, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.649, mean reward: 0.162 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.260], loss: 0.000070, mae: 0.003806, mean_q: 0.755445
 65387/100000: episode: 913, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 0.846, mean reward: 0.035 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.063, 10.459], loss: 0.000053, mae: 0.004374, mean_q: 0.752909
 65394/100000: episode: 914, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 0.793, mean reward: 0.113 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.490], loss: 0.000037, mae: 0.004990, mean_q: 0.753501
 65396/100000: episode: 915, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.626, mean reward: 0.313 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.309], loss: 0.000013, mae: 0.004346, mean_q: 0.749901
 65408/100000: episode: 916, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 0.676, mean reward: 0.056 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.370], loss: 0.000032, mae: 0.003531, mean_q: 0.754500
 65410/100000: episode: 917, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.729, mean reward: 0.364 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.234], loss: 0.000027, mae: 0.003106, mean_q: 0.753400
 65429/100000: episode: 918, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 0.760, mean reward: 0.040 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.864, 10.285], loss: 0.000043, mae: 0.002841, mean_q: 0.753974
 65439/100000: episode: 919, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.731, mean reward: 0.073 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.192, 10.280], loss: 0.000027, mae: 0.003369, mean_q: 0.753837
 65451/100000: episode: 920, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 0.778, mean reward: 0.065 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.359, 10.506], loss: 0.000004, mae: 0.001164, mean_q: 0.753204
 65470/100000: episode: 921, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.751, mean reward: 0.040 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.141, 10.437], loss: 0.000073, mae: 0.004079, mean_q: 0.753495
 65477/100000: episode: 922, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 0.726, mean reward: 0.104 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.061, 10.359], loss: 0.000035, mae: 0.004216, mean_q: 0.752856
 65481/100000: episode: 923, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 0.659, mean reward: 0.165 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.367], loss: 0.000043, mae: 0.004178, mean_q: 0.751525
 65488/100000: episode: 924, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 0.755, mean reward: 0.108 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.107, 10.417], loss: 0.000014, mae: 0.003552, mean_q: 0.754804
 65498/100000: episode: 925, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.749, mean reward: 0.075 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.273], loss: 0.000067, mae: 0.005137, mean_q: 0.751478
 65505/100000: episode: 926, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 0.751, mean reward: 0.107 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.512], loss: 0.000055, mae: 0.003928, mean_q: 0.755285
 65524/100000: episode: 927, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 0.842, mean reward: 0.044 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.687, 10.520], loss: 0.000026, mae: 0.002523, mean_q: 0.754307
 65533/100000: episode: 928, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 0.720, mean reward: 0.080 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.372], loss: 0.000063, mae: 0.004520, mean_q: 0.753760
 65543/100000: episode: 929, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.702, mean reward: 0.070 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.423, 10.312], loss: 0.000011, mae: 0.003748, mean_q: 0.752677
 65555/100000: episode: 930, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 0.754, mean reward: 0.063 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.404], loss: 0.000019, mae: 0.003323, mean_q: 0.754912
 65574/100000: episode: 931, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 0.777, mean reward: 0.041 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.491, 10.454], loss: 0.000029, mae: 0.003359, mean_q: 0.754057
 65578/100000: episode: 932, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: 0.632, mean reward: 0.158 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.247], loss: 0.000047, mae: 0.005204, mean_q: 0.750471
 65597/100000: episode: 933, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 0.737, mean reward: 0.039 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-1.115, 10.380], loss: 0.000084, mae: 0.006253, mean_q: 0.753980
 65609/100000: episode: 934, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 0.727, mean reward: 0.061 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.395], loss: 0.000082, mae: 0.004355, mean_q: 0.754275
 65611/100000: episode: 935, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.795, mean reward: 0.397 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.190], loss: 0.000003, mae: 0.001613, mean_q: 0.753019
 65635/100000: episode: 936, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 0.746, mean reward: 0.031 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.278], loss: 0.000044, mae: 0.003366, mean_q: 0.753573
 65642/100000: episode: 937, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 0.733, mean reward: 0.105 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.409, 10.456], loss: 0.000026, mae: 0.002346, mean_q: 0.753298
 65644/100000: episode: 938, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.694, mean reward: 0.347 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.216], loss: 0.000136, mae: 0.004730, mean_q: 0.755584
 65663/100000: episode: 939, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 0.704, mean reward: 0.037 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.306], loss: 0.000026, mae: 0.001951, mean_q: 0.753601
 65670/100000: episode: 940, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 0.724, mean reward: 0.103 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.443], loss: 0.000140, mae: 0.003865, mean_q: 0.754032
 65679/100000: episode: 941, duration: 0.069s, episode steps: 9, steps per second: 131, episode reward: 0.670, mean reward: 0.074 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.346], loss: 0.000011, mae: 0.004129, mean_q: 0.754338
 65683/100000: episode: 942, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 0.655, mean reward: 0.164 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.339], loss: 0.000063, mae: 0.004691, mean_q: 0.751407
 65687/100000: episode: 943, duration: 0.036s, episode steps: 4, steps per second: 113, episode reward: 0.684, mean reward: 0.171 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.381], loss: 0.000033, mae: 0.004056, mean_q: 0.753904
 65697/100000: episode: 944, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 0.655, mean reward: 0.066 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.332], loss: 0.000027, mae: 0.003144, mean_q: 0.754248
 65699/100000: episode: 945, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.802, mean reward: 0.401 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.233], loss: 0.000033, mae: 0.002968, mean_q: 0.754641
 65723/100000: episode: 946, duration: 0.158s, episode steps: 24, steps per second: 152, episode reward: 0.768, mean reward: 0.032 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.357], loss: 0.000071, mae: 0.004694, mean_q: 0.753821
 65747/100000: episode: 947, duration: 0.163s, episode steps: 24, steps per second: 148, episode reward: 0.758, mean reward: 0.032 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.385], loss: 0.000040, mae: 0.003359, mean_q: 0.753934
 65766/100000: episode: 948, duration: 0.133s, episode steps: 19, steps per second: 143, episode reward: 0.910, mean reward: 0.048 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.160, 10.623], loss: 0.000027, mae: 0.003306, mean_q: 0.752922
 65768/100000: episode: 949, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.631, mean reward: 0.315 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.337], loss: 0.000006, mae: 0.003340, mean_q: 0.757900
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7668793797492981
1
 65787/100000: episode: 950, duration: 4.467s, episode steps: 19, steps per second: 4, episode reward: 0.743, mean reward: 0.039 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.456, 10.377], loss: 0.000020, mae: 0.001994, mean_q: 0.753014
 65888/100000: episode: 951, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.164, 10.279], loss: 0.000077, mae: 0.004751, mean_q: 0.753060
 65989/100000: episode: 952, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.025, 10.183], loss: 0.000076, mae: 0.004202, mean_q: 0.753784
 66090/100000: episode: 953, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.276, 10.100], loss: 0.000065, mae: 0.004755, mean_q: 0.752857
 66191/100000: episode: 954, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.107, 10.255], loss: 0.000036, mae: 0.002866, mean_q: 0.752210
 66292/100000: episode: 955, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.214], loss: 0.000042, mae: 0.003304, mean_q: 0.752063
 66393/100000: episode: 956, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.049, 10.100], loss: 0.000068, mae: 0.003919, mean_q: 0.752215
 66494/100000: episode: 957, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.858, 10.121], loss: 0.000073, mae: 0.005035, mean_q: 0.751587
 66595/100000: episode: 958, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.262, 10.177], loss: 0.000055, mae: 0.003596, mean_q: 0.751942
 66696/100000: episode: 959, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.713, 10.100], loss: 0.000052, mae: 0.003498, mean_q: 0.751645
 66797/100000: episode: 960, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.552, 10.100], loss: 0.000051, mae: 0.003132, mean_q: 0.751277
 66898/100000: episode: 961, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.946, 10.100], loss: 0.000050, mae: 0.003873, mean_q: 0.751376
 66999/100000: episode: 962, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.604, 10.124], loss: 0.000048, mae: 0.003339, mean_q: 0.751253
 67100/100000: episode: 963, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.349, 10.100], loss: 0.000057, mae: 0.003264, mean_q: 0.751433
 67201/100000: episode: 964, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.312, 10.100], loss: 0.000087, mae: 0.004922, mean_q: 0.750897
 67302/100000: episode: 965, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-2.264, 10.100], loss: 0.000043, mae: 0.003019, mean_q: 0.750817
 67403/100000: episode: 966, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.735, 10.100], loss: 0.000051, mae: 0.003655, mean_q: 0.750296
 67504/100000: episode: 967, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.821, 10.100], loss: 0.000044, mae: 0.002823, mean_q: 0.750828
 67605/100000: episode: 968, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.602, 10.100], loss: 0.000056, mae: 0.004191, mean_q: 0.750648
 67706/100000: episode: 969, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.494, 10.100], loss: 0.000066, mae: 0.004768, mean_q: 0.750546
 67807/100000: episode: 970, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.549, 10.243], loss: 0.000052, mae: 0.003568, mean_q: 0.749665
 67908/100000: episode: 971, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.506, 10.151], loss: 0.000048, mae: 0.003290, mean_q: 0.749880
 68009/100000: episode: 972, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.066, 10.102], loss: 0.000040, mae: 0.003122, mean_q: 0.749461
 68110/100000: episode: 973, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.750, 10.209], loss: 0.000053, mae: 0.003834, mean_q: 0.749693
 68211/100000: episode: 974, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.624, 10.100], loss: 0.000049, mae: 0.003266, mean_q: 0.749365
 68312/100000: episode: 975, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.478, 10.100], loss: 0.000046, mae: 0.003842, mean_q: 0.749310
 68413/100000: episode: 976, duration: 0.629s, episode steps: 101, steps per second: 160, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.841, 10.243], loss: 0.000034, mae: 0.002676, mean_q: 0.749171
 68514/100000: episode: 977, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.847, 10.427], loss: 0.000053, mae: 0.003884, mean_q: 0.749652
 68615/100000: episode: 978, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.998, 10.283], loss: 0.000047, mae: 0.003573, mean_q: 0.749592
 68716/100000: episode: 979, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.100], loss: 0.000054, mae: 0.003040, mean_q: 0.749886
 68817/100000: episode: 980, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.226, 10.100], loss: 0.000046, mae: 0.003156, mean_q: 0.750295
 68918/100000: episode: 981, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.220, 10.375], loss: 0.000042, mae: 0.003156, mean_q: 0.750480
 69019/100000: episode: 982, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.171, 10.100], loss: 0.000039, mae: 0.002942, mean_q: 0.750579
 69120/100000: episode: 983, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.209, 10.523], loss: 0.000054, mae: 0.003867, mean_q: 0.750098
 69221/100000: episode: 984, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.717, 10.156], loss: 0.000038, mae: 0.003451, mean_q: 0.749575
 69322/100000: episode: 985, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.934, 10.100], loss: 0.000070, mae: 0.004444, mean_q: 0.750230
 69423/100000: episode: 986, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.741, 10.102], loss: 0.000062, mae: 0.004240, mean_q: 0.750175
 69524/100000: episode: 987, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.423, 10.101], loss: 0.000052, mae: 0.003490, mean_q: 0.750109
 69625/100000: episode: 988, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.680, 10.100], loss: 0.000039, mae: 0.003325, mean_q: 0.750213
 69726/100000: episode: 989, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.583, 10.100], loss: 0.000062, mae: 0.003062, mean_q: 0.750326
 69827/100000: episode: 990, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.641, 10.293], loss: 0.000057, mae: 0.003739, mean_q: 0.750312
 69928/100000: episode: 991, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.100], loss: 0.000046, mae: 0.003205, mean_q: 0.749877
 70029/100000: episode: 992, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.836, 10.339], loss: 0.000045, mae: 0.003497, mean_q: 0.749864
 70130/100000: episode: 993, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.162, 10.100], loss: 0.000042, mae: 0.003161, mean_q: 0.749639
 70231/100000: episode: 994, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.714, 10.190], loss: 0.000039, mae: 0.002998, mean_q: 0.749631
 70332/100000: episode: 995, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.067, 10.340], loss: 0.000026, mae: 0.002613, mean_q: 0.749641
 70433/100000: episode: 996, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.332, 10.100], loss: 0.000016, mae: 0.001907, mean_q: 0.749314
 70534/100000: episode: 997, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.663, mean reward: 0.007 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.340, 10.125], loss: 0.000010, mae: 0.001319, mean_q: 0.749551
 70635/100000: episode: 998, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.129, 10.375], loss: 0.000016, mae: 0.001777, mean_q: 0.749656
 70736/100000: episode: 999, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.118, 10.252], loss: 0.000012, mae: 0.001749, mean_q: 0.749532
 70837/100000: episode: 1000, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.824, 10.133], loss: 0.000006, mae: 0.001214, mean_q: 0.749639
 70938/100000: episode: 1001, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.976, 10.100], loss: 0.000009, mae: 0.001625, mean_q: 0.749840
 71039/100000: episode: 1002, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.679, 10.100], loss: 0.000008, mae: 0.001468, mean_q: 0.750034
 71140/100000: episode: 1003, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.660, mean reward: 0.007 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.298, 10.193], loss: 0.000008, mae: 0.001462, mean_q: 0.749987
 71241/100000: episode: 1004, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.101, 10.100], loss: 0.000015, mae: 0.002022, mean_q: 0.750024
 71342/100000: episode: 1005, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.139], loss: 0.000020, mae: 0.002139, mean_q: 0.749923
 71443/100000: episode: 1006, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.805, 10.100], loss: 0.000013, mae: 0.001624, mean_q: 0.749851
 71544/100000: episode: 1007, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.829, 10.165], loss: 0.000011, mae: 0.001591, mean_q: 0.750182
 71645/100000: episode: 1008, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.506, 10.226], loss: 0.000016, mae: 0.002111, mean_q: 0.749988
 71746/100000: episode: 1009, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.039, 10.312], loss: 0.000007, mae: 0.001441, mean_q: 0.750269
 71847/100000: episode: 1010, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.048, 10.100], loss: 0.000006, mae: 0.001077, mean_q: 0.750256
 71948/100000: episode: 1011, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.391, 10.332], loss: 0.000016, mae: 0.001988, mean_q: 0.750212
 72049/100000: episode: 1012, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.384, 10.286], loss: 0.000006, mae: 0.001310, mean_q: 0.750118
 72150/100000: episode: 1013, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.700, 10.278], loss: 0.000019, mae: 0.002634, mean_q: 0.750017
 72251/100000: episode: 1014, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.305, 10.100], loss: 0.000009, mae: 0.001608, mean_q: 0.750443
 72352/100000: episode: 1015, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.100], loss: 0.000011, mae: 0.001509, mean_q: 0.750329
 72453/100000: episode: 1016, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.726, 10.100], loss: 0.000017, mae: 0.002319, mean_q: 0.750425
 72554/100000: episode: 1017, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.676, 10.160], loss: 0.000013, mae: 0.001993, mean_q: 0.750544
 72655/100000: episode: 1018, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.165], loss: 0.000009, mae: 0.001708, mean_q: 0.750446
 72756/100000: episode: 1019, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.828, 10.296], loss: 0.000016, mae: 0.002058, mean_q: 0.750676
 72857/100000: episode: 1020, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.494, 10.100], loss: 0.000009, mae: 0.001607, mean_q: 0.750767
 72958/100000: episode: 1021, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.914, 10.183], loss: 0.000009, mae: 0.001655, mean_q: 0.750818
 73059/100000: episode: 1022, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.271, 10.100], loss: 0.000016, mae: 0.002240, mean_q: 0.750704
 73160/100000: episode: 1023, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.969, 10.163], loss: 0.000007, mae: 0.001163, mean_q: 0.750697
 73261/100000: episode: 1024, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.539, 10.126], loss: 0.000013, mae: 0.001509, mean_q: 0.750977
 73362/100000: episode: 1025, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.575, 10.132], loss: 0.000015, mae: 0.002470, mean_q: 0.750949
 73463/100000: episode: 1026, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.943, 10.255], loss: 0.000011, mae: 0.001545, mean_q: 0.751021
 73564/100000: episode: 1027, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.185, 10.100], loss: 0.000020, mae: 0.002882, mean_q: 0.750947
 73665/100000: episode: 1028, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.871, 10.102], loss: 0.000013, mae: 0.002062, mean_q: 0.750859
 73766/100000: episode: 1029, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.100, 10.204], loss: 0.000011, mae: 0.001893, mean_q: 0.750941
 73867/100000: episode: 1030, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.325, 10.240], loss: 0.000012, mae: 0.001821, mean_q: 0.750489
 73968/100000: episode: 1031, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.735, 10.195], loss: 0.000013, mae: 0.001798, mean_q: 0.750641
 74069/100000: episode: 1032, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.339, 10.150], loss: 0.000016, mae: 0.002284, mean_q: 0.750882
 74170/100000: episode: 1033, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.914, 10.100], loss: 0.000008, mae: 0.001639, mean_q: 0.750751
 74271/100000: episode: 1034, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.100], loss: 0.000009, mae: 0.001539, mean_q: 0.750496
 74372/100000: episode: 1035, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.100], loss: 0.000011, mae: 0.001802, mean_q: 0.750568
 74473/100000: episode: 1036, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.819, 10.120], loss: 0.000012, mae: 0.001656, mean_q: 0.750335
 74574/100000: episode: 1037, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.601, 10.180], loss: 0.000013, mae: 0.002053, mean_q: 0.750253
 74675/100000: episode: 1038, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.222], loss: 0.000009, mae: 0.001578, mean_q: 0.750267
 74776/100000: episode: 1039, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.395, 10.123], loss: 0.000014, mae: 0.002068, mean_q: 0.750224
 74877/100000: episode: 1040, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.936, 10.355], loss: 0.000010, mae: 0.001668, mean_q: 0.750154
 74978/100000: episode: 1041, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.142, 10.100], loss: 0.000012, mae: 0.001989, mean_q: 0.749824
 75079/100000: episode: 1042, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.370, 10.129], loss: 0.000014, mae: 0.002106, mean_q: 0.749925
 75180/100000: episode: 1043, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.800, 10.219], loss: 0.000013, mae: 0.002093, mean_q: 0.749667
 75281/100000: episode: 1044, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.355, 10.100], loss: 0.000020, mae: 0.002744, mean_q: 0.749749
 75382/100000: episode: 1045, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.813, 10.100], loss: 0.000014, mae: 0.002020, mean_q: 0.749844
 75483/100000: episode: 1046, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.791, 10.135], loss: 0.000011, mae: 0.001864, mean_q: 0.749875
 75584/100000: episode: 1047, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.770, 10.100], loss: 0.000012, mae: 0.001521, mean_q: 0.749565
 75685/100000: episode: 1048, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.752, 10.242], loss: 0.000014, mae: 0.001721, mean_q: 0.749708
 75786/100000: episode: 1049, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.041, 10.100], loss: 0.000016, mae: 0.001778, mean_q: 0.749435
[Info] 1-TH LEVEL FOUND: 0.7573659420013428, Considering 10/100 traces
 75887/100000: episode: 1050, duration: 5.193s, episode steps: 101, steps per second: 19, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.756, 10.246], loss: 0.000011, mae: 0.001477, mean_q: 0.749477
 75919/100000: episode: 1051, duration: 0.205s, episode steps: 32, steps per second: 156, episode reward: 0.658, mean reward: 0.021 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.014, 10.104], loss: 0.000018, mae: 0.002347, mean_q: 0.749325
 75954/100000: episode: 1052, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 0.742, mean reward: 0.021 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.863, 10.309], loss: 0.000018, mae: 0.002894, mean_q: 0.749638
 75990/100000: episode: 1053, duration: 0.226s, episode steps: 36, steps per second: 159, episode reward: 0.839, mean reward: 0.023 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.182, 10.153], loss: 0.000004, mae: 0.001555, mean_q: 0.749371
 76026/100000: episode: 1054, duration: 0.212s, episode steps: 36, steps per second: 169, episode reward: 0.844, mean reward: 0.023 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.582, 10.185], loss: 0.000019, mae: 0.002333, mean_q: 0.749732
 76058/100000: episode: 1055, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 0.718, mean reward: 0.022 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.345, 10.343], loss: 0.000015, mae: 0.002920, mean_q: 0.749836
 76094/100000: episode: 1056, duration: 0.226s, episode steps: 36, steps per second: 159, episode reward: 0.743, mean reward: 0.021 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.339, 10.100], loss: 0.000016, mae: 0.001974, mean_q: 0.749693
 76130/100000: episode: 1057, duration: 0.219s, episode steps: 36, steps per second: 164, episode reward: 0.775, mean reward: 0.022 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.629, 10.201], loss: 0.000018, mae: 0.001861, mean_q: 0.749724
 76166/100000: episode: 1058, duration: 0.246s, episode steps: 36, steps per second: 146, episode reward: 0.746, mean reward: 0.021 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.922, 10.362], loss: 0.000011, mae: 0.001607, mean_q: 0.749817
 76202/100000: episode: 1059, duration: 0.220s, episode steps: 36, steps per second: 164, episode reward: 0.824, mean reward: 0.023 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.381, 10.511], loss: 0.000016, mae: 0.001981, mean_q: 0.749619
 76234/100000: episode: 1060, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 0.680, mean reward: 0.021 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.220], loss: 0.000013, mae: 0.001748, mean_q: 0.750015
 76269/100000: episode: 1061, duration: 0.214s, episode steps: 35, steps per second: 164, episode reward: 0.755, mean reward: 0.022 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.417, 10.387], loss: 0.000014, mae: 0.002098, mean_q: 0.749710
 76303/100000: episode: 1062, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 0.822, mean reward: 0.024 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.749, 10.155], loss: 0.000016, mae: 0.002234, mean_q: 0.749410
 76338/100000: episode: 1063, duration: 0.225s, episode steps: 35, steps per second: 155, episode reward: 0.789, mean reward: 0.023 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.886, 10.399], loss: 0.000019, mae: 0.002717, mean_q: 0.750018
 76373/100000: episode: 1064, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 0.763, mean reward: 0.022 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.247, 10.100], loss: 0.000009, mae: 0.001406, mean_q: 0.749601
 76408/100000: episode: 1065, duration: 0.213s, episode steps: 35, steps per second: 165, episode reward: 0.738, mean reward: 0.021 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.160, 10.245], loss: 0.000013, mae: 0.002486, mean_q: 0.749934
 76443/100000: episode: 1066, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 0.712, mean reward: 0.020 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.647, 10.265], loss: 0.000027, mae: 0.002421, mean_q: 0.750063
 76475/100000: episode: 1067, duration: 0.204s, episode steps: 32, steps per second: 157, episode reward: 0.688, mean reward: 0.021 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.529, 10.109], loss: 0.000005, mae: 0.001249, mean_q: 0.750104
 76507/100000: episode: 1068, duration: 0.192s, episode steps: 32, steps per second: 167, episode reward: 0.677, mean reward: 0.021 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.554, 10.140], loss: 0.000015, mae: 0.002477, mean_q: 0.749700
 76543/100000: episode: 1069, duration: 0.223s, episode steps: 36, steps per second: 161, episode reward: 0.833, mean reward: 0.023 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.035, 10.401], loss: 0.000021, mae: 0.002456, mean_q: 0.749879
 76578/100000: episode: 1070, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 0.714, mean reward: 0.020 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.423, 10.347], loss: 0.000013, mae: 0.002452, mean_q: 0.749647
 76613/100000: episode: 1071, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: 0.714, mean reward: 0.020 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.022, 10.100], loss: 0.000019, mae: 0.001721, mean_q: 0.749738
 76648/100000: episode: 1072, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 0.716, mean reward: 0.020 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-2.135, 10.100], loss: 0.000008, mae: 0.001550, mean_q: 0.750093
 76680/100000: episode: 1073, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 0.770, mean reward: 0.024 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.708, 10.233], loss: 0.000005, mae: 0.001525, mean_q: 0.749564
 76716/100000: episode: 1074, duration: 0.210s, episode steps: 36, steps per second: 171, episode reward: 0.796, mean reward: 0.022 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.124, 10.288], loss: 0.000015, mae: 0.001699, mean_q: 0.749731
 76751/100000: episode: 1075, duration: 0.206s, episode steps: 35, steps per second: 170, episode reward: 0.741, mean reward: 0.021 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.459, 10.100], loss: 0.000019, mae: 0.002561, mean_q: 0.749672
 76786/100000: episode: 1076, duration: 0.225s, episode steps: 35, steps per second: 155, episode reward: 0.675, mean reward: 0.019 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.108, 10.185], loss: 0.000017, mae: 0.002138, mean_q: 0.749594
 76821/100000: episode: 1077, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 0.828, mean reward: 0.024 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.586, 10.100], loss: 0.000015, mae: 0.001858, mean_q: 0.749785
 76853/100000: episode: 1078, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 0.758, mean reward: 0.024 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.648, 10.280], loss: 0.000007, mae: 0.001048, mean_q: 0.749795
 76889/100000: episode: 1079, duration: 0.226s, episode steps: 36, steps per second: 159, episode reward: 0.786, mean reward: 0.022 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.497, 10.460], loss: 0.000015, mae: 0.002040, mean_q: 0.749515
 76925/100000: episode: 1080, duration: 0.219s, episode steps: 36, steps per second: 164, episode reward: 0.655, mean reward: 0.018 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.910, 10.165], loss: 0.000016, mae: 0.001932, mean_q: 0.749749
 76960/100000: episode: 1081, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 0.760, mean reward: 0.022 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.516, 10.146], loss: 0.000007, mae: 0.001293, mean_q: 0.749669
 76992/100000: episode: 1082, duration: 0.201s, episode steps: 32, steps per second: 159, episode reward: 0.708, mean reward: 0.022 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.335], loss: 0.000019, mae: 0.002405, mean_q: 0.749541
 77024/100000: episode: 1083, duration: 0.198s, episode steps: 32, steps per second: 162, episode reward: 0.807, mean reward: 0.025 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.618, 10.359], loss: 0.000013, mae: 0.002047, mean_q: 0.749540
 77058/100000: episode: 1084, duration: 0.212s, episode steps: 34, steps per second: 161, episode reward: 0.708, mean reward: 0.021 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.224, 10.100], loss: 0.000010, mae: 0.001496, mean_q: 0.749350
 77092/100000: episode: 1085, duration: 0.196s, episode steps: 34, steps per second: 174, episode reward: 0.859, mean reward: 0.025 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.263, 10.381], loss: 0.000015, mae: 0.001679, mean_q: 0.749175
 77127/100000: episode: 1086, duration: 0.222s, episode steps: 35, steps per second: 158, episode reward: 0.829, mean reward: 0.024 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.392, 10.215], loss: 0.000005, mae: 0.001128, mean_q: 0.749271
 77155/100000: episode: 1087, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 0.802, mean reward: 0.029 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.350, 10.310], loss: 0.000017, mae: 0.001930, mean_q: 0.749464
 77191/100000: episode: 1088, duration: 0.211s, episode steps: 36, steps per second: 170, episode reward: 0.851, mean reward: 0.024 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.069, 10.580], loss: 0.000013, mae: 0.001562, mean_q: 0.749554
 77223/100000: episode: 1089, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 0.684, mean reward: 0.021 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.193, 10.100], loss: 0.000028, mae: 0.002403, mean_q: 0.750110
 77255/100000: episode: 1090, duration: 0.183s, episode steps: 32, steps per second: 174, episode reward: 0.881, mean reward: 0.028 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.968, 10.307], loss: 0.000036, mae: 0.004006, mean_q: 0.749773
 77291/100000: episode: 1091, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 0.878, mean reward: 0.024 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.040, 10.475], loss: 0.000015, mae: 0.001506, mean_q: 0.749479
 77323/100000: episode: 1092, duration: 0.214s, episode steps: 32, steps per second: 149, episode reward: 0.716, mean reward: 0.022 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.697, 10.207], loss: 0.000043, mae: 0.002997, mean_q: 0.749619
 77351/100000: episode: 1093, duration: 0.189s, episode steps: 28, steps per second: 148, episode reward: 0.713, mean reward: 0.025 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.152], loss: 0.000020, mae: 0.003020, mean_q: 0.749986
 77387/100000: episode: 1094, duration: 0.219s, episode steps: 36, steps per second: 164, episode reward: 0.832, mean reward: 0.023 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.120, 10.325], loss: 0.000007, mae: 0.001273, mean_q: 0.750124
 77422/100000: episode: 1095, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 0.760, mean reward: 0.022 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.352], loss: 0.000040, mae: 0.002376, mean_q: 0.750154
 77458/100000: episode: 1096, duration: 0.210s, episode steps: 36, steps per second: 171, episode reward: 0.745, mean reward: 0.021 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.427, 10.282], loss: 0.000016, mae: 0.002384, mean_q: 0.749601
 77490/100000: episode: 1097, duration: 0.212s, episode steps: 32, steps per second: 151, episode reward: 0.724, mean reward: 0.023 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.251, 10.100], loss: 0.000015, mae: 0.002053, mean_q: 0.750186
 77522/100000: episode: 1098, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 0.806, mean reward: 0.025 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.354, 10.343], loss: 0.000028, mae: 0.002774, mean_q: 0.749945
 77558/100000: episode: 1099, duration: 0.204s, episode steps: 36, steps per second: 177, episode reward: 0.773, mean reward: 0.021 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.719, 10.429], loss: 0.000020, mae: 0.002463, mean_q: 0.749424
 77590/100000: episode: 1100, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 0.768, mean reward: 0.024 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.596, 10.325], loss: 0.000046, mae: 0.004299, mean_q: 0.749883
 77626/100000: episode: 1101, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 0.791, mean reward: 0.022 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.338, 10.384], loss: 0.000032, mae: 0.002180, mean_q: 0.750365
 77660/100000: episode: 1102, duration: 0.206s, episode steps: 34, steps per second: 165, episode reward: 0.751, mean reward: 0.022 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.448, 10.100], loss: 0.000020, mae: 0.001895, mean_q: 0.750037
 77695/100000: episode: 1103, duration: 0.222s, episode steps: 35, steps per second: 158, episode reward: 0.671, mean reward: 0.019 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.346], loss: 0.000033, mae: 0.002841, mean_q: 0.750538
 77723/100000: episode: 1104, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 0.734, mean reward: 0.026 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.052, 10.306], loss: 0.000020, mae: 0.001796, mean_q: 0.749905
 77751/100000: episode: 1105, duration: 0.168s, episode steps: 28, steps per second: 166, episode reward: 0.709, mean reward: 0.025 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.521, 10.100], loss: 0.000016, mae: 0.001370, mean_q: 0.750202
 77785/100000: episode: 1106, duration: 0.210s, episode steps: 34, steps per second: 162, episode reward: 0.732, mean reward: 0.022 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.089, 10.333], loss: 0.000017, mae: 0.001783, mean_q: 0.750143
 77819/100000: episode: 1107, duration: 0.209s, episode steps: 34, steps per second: 163, episode reward: 0.733, mean reward: 0.022 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.208, 10.248], loss: 0.000020, mae: 0.002413, mean_q: 0.750201
 77847/100000: episode: 1108, duration: 0.175s, episode steps: 28, steps per second: 160, episode reward: 0.749, mean reward: 0.027 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.373, 10.377], loss: 0.000033, mae: 0.003032, mean_q: 0.750276
 77882/100000: episode: 1109, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 0.740, mean reward: 0.021 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.360, 10.290], loss: 0.000030, mae: 0.001879, mean_q: 0.750485
 77914/100000: episode: 1110, duration: 0.198s, episode steps: 32, steps per second: 162, episode reward: 0.737, mean reward: 0.023 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.196, 10.100], loss: 0.000012, mae: 0.001673, mean_q: 0.750273
 77948/100000: episode: 1111, duration: 0.194s, episode steps: 34, steps per second: 176, episode reward: 0.709, mean reward: 0.021 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.347, 10.100], loss: 0.000033, mae: 0.002602, mean_q: 0.750002
 77984/100000: episode: 1112, duration: 0.219s, episode steps: 36, steps per second: 165, episode reward: 0.778, mean reward: 0.022 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.337, 10.287], loss: 0.000030, mae: 0.002499, mean_q: 0.750348
 78016/100000: episode: 1113, duration: 0.189s, episode steps: 32, steps per second: 169, episode reward: 0.692, mean reward: 0.022 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.523, 10.100], loss: 0.000041, mae: 0.002293, mean_q: 0.750727
 78051/100000: episode: 1114, duration: 0.223s, episode steps: 35, steps per second: 157, episode reward: 0.740, mean reward: 0.021 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.301, 10.288], loss: 0.000031, mae: 0.003173, mean_q: 0.749989
 78083/100000: episode: 1115, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 0.690, mean reward: 0.022 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.702, 10.100], loss: 0.000016, mae: 0.001760, mean_q: 0.750628
 78115/100000: episode: 1116, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 0.791, mean reward: 0.025 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.196], loss: 0.000039, mae: 0.003297, mean_q: 0.750593
 78150/100000: episode: 1117, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 0.722, mean reward: 0.021 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.170, 10.100], loss: 0.000030, mae: 0.003061, mean_q: 0.750461
 78185/100000: episode: 1118, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 0.727, mean reward: 0.021 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.438, 10.186], loss: 0.000028, mae: 0.002864, mean_q: 0.750281
 78217/100000: episode: 1119, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 0.753, mean reward: 0.024 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.312, 10.400], loss: 0.000034, mae: 0.002616, mean_q: 0.750355
 78251/100000: episode: 1120, duration: 0.209s, episode steps: 34, steps per second: 163, episode reward: 0.723, mean reward: 0.021 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.606, 10.254], loss: 0.000020, mae: 0.001737, mean_q: 0.750366
 78286/100000: episode: 1121, duration: 0.215s, episode steps: 35, steps per second: 163, episode reward: 0.670, mean reward: 0.019 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.035, 10.117], loss: 0.000024, mae: 0.002396, mean_q: 0.750031
 78314/100000: episode: 1122, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 0.723, mean reward: 0.026 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.334, 10.132], loss: 0.000014, mae: 0.002084, mean_q: 0.750425
 78346/100000: episode: 1123, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 0.670, mean reward: 0.021 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.100, 10.132], loss: 0.000028, mae: 0.002575, mean_q: 0.749951
 78381/100000: episode: 1124, duration: 0.193s, episode steps: 35, steps per second: 182, episode reward: 0.661, mean reward: 0.019 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.979, 10.116], loss: 0.000029, mae: 0.002783, mean_q: 0.749418
 78413/100000: episode: 1125, duration: 0.193s, episode steps: 32, steps per second: 166, episode reward: 0.834, mean reward: 0.026 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.166, 10.401], loss: 0.000021, mae: 0.003483, mean_q: 0.749846
 78448/100000: episode: 1126, duration: 0.218s, episode steps: 35, steps per second: 161, episode reward: 0.783, mean reward: 0.022 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.285, 10.100], loss: 0.000018, mae: 0.001711, mean_q: 0.750203
 78476/100000: episode: 1127, duration: 0.188s, episode steps: 28, steps per second: 149, episode reward: 0.710, mean reward: 0.025 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.319], loss: 0.000042, mae: 0.002894, mean_q: 0.750201
 78512/100000: episode: 1128, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 0.692, mean reward: 0.019 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.211, 10.100], loss: 0.000031, mae: 0.002801, mean_q: 0.750158
 78547/100000: episode: 1129, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 0.736, mean reward: 0.021 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.577, 10.349], loss: 0.000032, mae: 0.002945, mean_q: 0.749887
 78575/100000: episode: 1130, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 0.746, mean reward: 0.027 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.414, 10.364], loss: 0.000022, mae: 0.002585, mean_q: 0.750652
 78610/100000: episode: 1131, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 0.824, mean reward: 0.024 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.613, 10.531], loss: 0.000029, mae: 0.002061, mean_q: 0.750037
 78642/100000: episode: 1132, duration: 0.199s, episode steps: 32, steps per second: 161, episode reward: 0.728, mean reward: 0.023 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.687, 10.119], loss: 0.000029, mae: 0.002630, mean_q: 0.749352
 78674/100000: episode: 1133, duration: 0.193s, episode steps: 32, steps per second: 165, episode reward: 0.777, mean reward: 0.024 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.089, 10.479], loss: 0.000044, mae: 0.003573, mean_q: 0.750440
 78709/100000: episode: 1134, duration: 0.229s, episode steps: 35, steps per second: 153, episode reward: 0.772, mean reward: 0.022 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.364, 10.192], loss: 0.000034, mae: 0.002482, mean_q: 0.749963
 78741/100000: episode: 1135, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 0.750, mean reward: 0.023 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.323], loss: 0.000023, mae: 0.002891, mean_q: 0.749492
 78776/100000: episode: 1136, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 0.739, mean reward: 0.021 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.101, 10.378], loss: 0.000033, mae: 0.003702, mean_q: 0.750240
 78808/100000: episode: 1137, duration: 0.199s, episode steps: 32, steps per second: 161, episode reward: 0.699, mean reward: 0.022 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.035, 10.182], loss: 0.000038, mae: 0.003430, mean_q: 0.749621
 78844/100000: episode: 1138, duration: 0.221s, episode steps: 36, steps per second: 163, episode reward: 0.830, mean reward: 0.023 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.423, 10.295], loss: 0.000034, mae: 0.002668, mean_q: 0.750276
 78876/100000: episode: 1139, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 0.725, mean reward: 0.023 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.521, 10.228], loss: 0.000023, mae: 0.002384, mean_q: 0.749896
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7573659420013428
1
 78912/100000: episode: 1140, duration: 4.512s, episode steps: 36, steps per second: 8, episode reward: 0.751, mean reward: 0.021 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.623, 10.402], loss: 0.000044, mae: 0.002801, mean_q: 0.749787
 79013/100000: episode: 1141, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.679, 10.282], loss: 0.000022, mae: 0.002687, mean_q: 0.749690
 79114/100000: episode: 1142, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.870, mean reward: 0.009 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.731, 10.107], loss: 0.000028, mae: 0.002084, mean_q: 0.749578
 79215/100000: episode: 1143, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.956, 10.141], loss: 0.000034, mae: 0.002841, mean_q: 0.749320
 79316/100000: episode: 1144, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.101, 10.100], loss: 0.000035, mae: 0.002978, mean_q: 0.750026
 79417/100000: episode: 1145, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.415, 10.167], loss: 0.000034, mae: 0.003298, mean_q: 0.749665
 79518/100000: episode: 1146, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.385, 10.100], loss: 0.000034, mae: 0.002757, mean_q: 0.749770
 79619/100000: episode: 1147, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.346, 10.173], loss: 0.000030, mae: 0.002758, mean_q: 0.750130
 79720/100000: episode: 1148, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.523, 10.102], loss: 0.000042, mae: 0.003162, mean_q: 0.750196
 79821/100000: episode: 1149, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.554, 10.161], loss: 0.000036, mae: 0.003181, mean_q: 0.749985
 79922/100000: episode: 1150, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.991, 10.252], loss: 0.000018, mae: 0.002022, mean_q: 0.750237
 80023/100000: episode: 1151, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.100], loss: 0.000040, mae: 0.003493, mean_q: 0.750564
 80124/100000: episode: 1152, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.294, 10.100], loss: 0.000030, mae: 0.002485, mean_q: 0.750401
 80225/100000: episode: 1153, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.880, mean reward: 0.009 [0.000, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.100], loss: 0.000026, mae: 0.002251, mean_q: 0.750476
 80326/100000: episode: 1154, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.472, 10.107], loss: 0.000035, mae: 0.003333, mean_q: 0.750509
 80427/100000: episode: 1155, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.472, 10.463], loss: 0.000038, mae: 0.002807, mean_q: 0.750583
 80528/100000: episode: 1156, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.752, 10.100], loss: 0.000042, mae: 0.003103, mean_q: 0.750764
 80629/100000: episode: 1157, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.826, 10.114], loss: 0.000036, mae: 0.003226, mean_q: 0.750832
 80730/100000: episode: 1158, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.658, mean reward: 0.007 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.751, 10.140], loss: 0.000032, mae: 0.002439, mean_q: 0.751327
 80831/100000: episode: 1159, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.069, 10.100], loss: 0.000027, mae: 0.002420, mean_q: 0.751289
 80932/100000: episode: 1160, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.649, 10.100], loss: 0.000029, mae: 0.002814, mean_q: 0.751377
 81033/100000: episode: 1161, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.599, 10.209], loss: 0.000034, mae: 0.002523, mean_q: 0.751669
 81134/100000: episode: 1162, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.517, 10.100], loss: 0.000028, mae: 0.002568, mean_q: 0.751554
 81235/100000: episode: 1163, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.365, 10.175], loss: 0.000031, mae: 0.002404, mean_q: 0.751508
 81336/100000: episode: 1164, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.627, 10.249], loss: 0.000025, mae: 0.002470, mean_q: 0.751535
 81437/100000: episode: 1165, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.022, 10.100], loss: 0.000034, mae: 0.002448, mean_q: 0.751864
 81538/100000: episode: 1166, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.580, 10.354], loss: 0.000020, mae: 0.001912, mean_q: 0.751730
 81639/100000: episode: 1167, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.242, 10.321], loss: 0.000018, mae: 0.001810, mean_q: 0.752098
 81740/100000: episode: 1168, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.594, 10.407], loss: 0.000022, mae: 0.002018, mean_q: 0.752111
 81841/100000: episode: 1169, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.516, 10.100], loss: 0.000032, mae: 0.002161, mean_q: 0.752333
 81942/100000: episode: 1170, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.914, 10.179], loss: 0.000023, mae: 0.002264, mean_q: 0.752265
 82043/100000: episode: 1171, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.614, 10.100], loss: 0.000034, mae: 0.002957, mean_q: 0.752196
 82144/100000: episode: 1172, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.834, 10.136], loss: 0.000033, mae: 0.003167, mean_q: 0.752019
 82245/100000: episode: 1173, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.015, 10.208], loss: 0.000013, mae: 0.002105, mean_q: 0.751622
 82346/100000: episode: 1174, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.719, 10.100], loss: 0.000015, mae: 0.001791, mean_q: 0.751677
 82447/100000: episode: 1175, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.608, 10.100], loss: 0.000024, mae: 0.002056, mean_q: 0.751657
 82548/100000: episode: 1176, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.456, 10.233], loss: 0.000018, mae: 0.002042, mean_q: 0.751483
 82649/100000: episode: 1177, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.420, 10.218], loss: 0.000014, mae: 0.001829, mean_q: 0.751289
 82750/100000: episode: 1178, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.460, 10.428], loss: 0.000015, mae: 0.002193, mean_q: 0.750845
 82851/100000: episode: 1179, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.838, 10.100], loss: 0.000012, mae: 0.001694, mean_q: 0.750434
 82952/100000: episode: 1180, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.930, 10.273], loss: 0.000018, mae: 0.002454, mean_q: 0.750416
 83053/100000: episode: 1181, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.824, 10.158], loss: 0.000015, mae: 0.001937, mean_q: 0.750240
 83154/100000: episode: 1182, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.596, 10.100], loss: 0.000015, mae: 0.001841, mean_q: 0.750303
 83255/100000: episode: 1183, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.045, 10.177], loss: 0.000019, mae: 0.002293, mean_q: 0.750278
 83356/100000: episode: 1184, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.802, 10.100], loss: 0.000008, mae: 0.001166, mean_q: 0.750392
 83457/100000: episode: 1185, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.253, 10.100], loss: 0.000016, mae: 0.001875, mean_q: 0.749982
 83558/100000: episode: 1186, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.642, 10.191], loss: 0.000021, mae: 0.002427, mean_q: 0.749968
 83659/100000: episode: 1187, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.217, 10.350], loss: 0.000017, mae: 0.001892, mean_q: 0.750038
 83760/100000: episode: 1188, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.516, 10.100], loss: 0.000009, mae: 0.001326, mean_q: 0.749868
 83861/100000: episode: 1189, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.708, 10.148], loss: 0.000005, mae: 0.000903, mean_q: 0.749679
 83962/100000: episode: 1190, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.668, 10.100], loss: 0.000015, mae: 0.001826, mean_q: 0.749704
 84063/100000: episode: 1191, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.391, 10.177], loss: 0.000007, mae: 0.001234, mean_q: 0.749589
 84164/100000: episode: 1192, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.244, 10.100], loss: 0.000009, mae: 0.001587, mean_q: 0.749190
 84265/100000: episode: 1193, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.148, 10.354], loss: 0.000013, mae: 0.001695, mean_q: 0.749077
 84366/100000: episode: 1194, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.437, 10.229], loss: 0.000009, mae: 0.001407, mean_q: 0.748937
 84467/100000: episode: 1195, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.324, 10.100], loss: 0.000010, mae: 0.001648, mean_q: 0.748812
 84568/100000: episode: 1196, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.228], loss: 0.000013, mae: 0.001932, mean_q: 0.748561
 84669/100000: episode: 1197, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.611, 10.168], loss: 0.000016, mae: 0.001935, mean_q: 0.748486
 84770/100000: episode: 1198, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.091, 10.383], loss: 0.000013, mae: 0.001738, mean_q: 0.748389
 84871/100000: episode: 1199, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.846, mean reward: 0.008 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.061, 10.100], loss: 0.000010, mae: 0.001530, mean_q: 0.748268
 84972/100000: episode: 1200, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.236], loss: 0.000009, mae: 0.001748, mean_q: 0.748133
 85073/100000: episode: 1201, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.657, 10.100], loss: 0.000009, mae: 0.001346, mean_q: 0.748223
 85174/100000: episode: 1202, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-1.099, 10.449], loss: 0.000015, mae: 0.001677, mean_q: 0.748084
 85275/100000: episode: 1203, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.516, 10.100], loss: 0.000008, mae: 0.001630, mean_q: 0.748137
 85376/100000: episode: 1204, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.624, 10.415], loss: 0.000014, mae: 0.001822, mean_q: 0.747845
 85477/100000: episode: 1205, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.342, 10.185], loss: 0.000011, mae: 0.001536, mean_q: 0.748011
 85578/100000: episode: 1206, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.481, 10.100], loss: 0.000015, mae: 0.002059, mean_q: 0.747876
 85679/100000: episode: 1207, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.544, 10.100], loss: 0.000006, mae: 0.001373, mean_q: 0.747825
 85780/100000: episode: 1208, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.109, 10.223], loss: 0.000014, mae: 0.001836, mean_q: 0.747978
 85881/100000: episode: 1209, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.106, 10.100], loss: 0.000009, mae: 0.001224, mean_q: 0.747869
 85982/100000: episode: 1210, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.269, 10.390], loss: 0.000010, mae: 0.001719, mean_q: 0.747799
 86083/100000: episode: 1211, duration: 0.641s, episode steps: 101, steps per second: 157, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.600, 10.100], loss: 0.000015, mae: 0.002097, mean_q: 0.748097
 86184/100000: episode: 1212, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.100], loss: 0.000011, mae: 0.001678, mean_q: 0.748127
 86285/100000: episode: 1213, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.079, 10.269], loss: 0.000014, mae: 0.001993, mean_q: 0.748167
 86386/100000: episode: 1214, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.854, mean reward: 0.008 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.100], loss: 0.000009, mae: 0.001288, mean_q: 0.748098
 86487/100000: episode: 1215, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.845, 10.100], loss: 0.000016, mae: 0.001618, mean_q: 0.748243
 86588/100000: episode: 1216, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.431, 10.100], loss: 0.000005, mae: 0.001143, mean_q: 0.748255
 86689/100000: episode: 1217, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.966, mean reward: 0.010 [0.000, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.088, 10.262], loss: 0.000007, mae: 0.001230, mean_q: 0.748250
 86790/100000: episode: 1218, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.460, 10.126], loss: 0.000029, mae: 0.002306, mean_q: 0.748565
 86891/100000: episode: 1219, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.925, 10.196], loss: 0.000013, mae: 0.002091, mean_q: 0.748535
 86992/100000: episode: 1220, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.022, 10.337], loss: 0.000018, mae: 0.001716, mean_q: 0.748784
 87093/100000: episode: 1221, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.150, 10.100], loss: 0.000010, mae: 0.001303, mean_q: 0.748843
 87194/100000: episode: 1222, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.829, 10.100], loss: 0.000018, mae: 0.002113, mean_q: 0.749194
 87295/100000: episode: 1223, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.227], loss: 0.000013, mae: 0.001553, mean_q: 0.749107
 87396/100000: episode: 1224, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.051, 10.100], loss: 0.000019, mae: 0.002253, mean_q: 0.749524
 87497/100000: episode: 1225, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.000, 10.100], loss: 0.000009, mae: 0.001536, mean_q: 0.749470
 87598/100000: episode: 1226, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.119, 10.138], loss: 0.000013, mae: 0.001385, mean_q: 0.749605
 87699/100000: episode: 1227, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.047, 10.384], loss: 0.000014, mae: 0.002138, mean_q: 0.749892
 87800/100000: episode: 1228, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.511, 10.204], loss: 0.000019, mae: 0.002690, mean_q: 0.750023
 87901/100000: episode: 1229, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.505, 10.312], loss: 0.000018, mae: 0.001734, mean_q: 0.750408
 88002/100000: episode: 1230, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.039, 10.208], loss: 0.000009, mae: 0.001333, mean_q: 0.750363
 88103/100000: episode: 1231, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.980, 10.100], loss: 0.000019, mae: 0.001927, mean_q: 0.750625
 88204/100000: episode: 1232, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.494, 10.519], loss: 0.000018, mae: 0.002063, mean_q: 0.750913
 88305/100000: episode: 1233, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.864, mean reward: 0.009 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.907, 10.100], loss: 0.000011, mae: 0.001885, mean_q: 0.751040
 88406/100000: episode: 1234, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.189, 10.298], loss: 0.000019, mae: 0.002044, mean_q: 0.751400
 88507/100000: episode: 1235, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.613, 10.353], loss: 0.000015, mae: 0.001865, mean_q: 0.751709
 88608/100000: episode: 1236, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.675, 10.100], loss: 0.000012, mae: 0.001697, mean_q: 0.751860
 88709/100000: episode: 1237, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.198, 10.637], loss: 0.000009, mae: 0.001591, mean_q: 0.751874
 88810/100000: episode: 1238, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.832, 10.100], loss: 0.000019, mae: 0.002210, mean_q: 0.752202
 88911/100000: episode: 1239, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.350, 10.248], loss: 0.000011, mae: 0.001338, mean_q: 0.752676
[Info] 1-TH LEVEL FOUND: 0.7784573435783386, Considering 10/100 traces
 89012/100000: episode: 1240, duration: 5.130s, episode steps: 101, steps per second: 20, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.494, 10.198], loss: 0.000012, mae: 0.001820, mean_q: 0.752560
 89013/100000: episode: 1241, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.639, mean reward: 0.639 [0.639, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.439 [-0.070, 10.314], loss: 0.000000, mae: 0.000752, mean_q: 0.753739
 89014/100000: episode: 1242, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.708, mean reward: 0.708 [0.708, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.070, 10.359], loss: 0.000049, mae: 0.002145, mean_q: 0.752611
 89016/100000: episode: 1243, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.704, mean reward: 0.352 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.429], loss: 0.000001, mae: 0.000896, mean_q: 0.753701
 89017/100000: episode: 1244, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.611, mean reward: 0.611 [0.611, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.070, 10.292], loss: 0.000040, mae: 0.002649, mean_q: 0.754483
 89018/100000: episode: 1245, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.327], loss: 0.000001, mae: 0.000848, mean_q: 0.752578
 89019/100000: episode: 1246, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.619, mean reward: 0.619 [0.619, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.070, 10.313], loss: 0.000000, mae: 0.000733, mean_q: 0.753083
 89020/100000: episode: 1247, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.070, 10.344], loss: 0.000000, mae: 0.000782, mean_q: 0.751811
 89022/100000: episode: 1248, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.775, mean reward: 0.388 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.486], loss: 0.000000, mae: 0.000529, mean_q: 0.752126
 89023/100000: episode: 1249, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 0.653, mean reward: 0.653 [0.653, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.070, 10.200], loss: 0.000196, mae: 0.005787, mean_q: 0.757690
 89027/100000: episode: 1250, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 0.730, mean reward: 0.182 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.127, 10.428], loss: 0.000082, mae: 0.003752, mean_q: 0.752048
 89028/100000: episode: 1251, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.660, mean reward: 0.660 [0.660, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.070, 10.295], loss: 0.000000, mae: 0.000630, mean_q: 0.752126
 89032/100000: episode: 1252, duration: 0.031s, episode steps: 4, steps per second: 130, episode reward: 0.773, mean reward: 0.193 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.538], loss: 0.000034, mae: 0.002470, mean_q: 0.756678
 89036/100000: episode: 1253, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 0.745, mean reward: 0.186 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.502], loss: 0.000228, mae: 0.006550, mean_q: 0.750451
 89037/100000: episode: 1254, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.340], loss: 0.000034, mae: 0.002903, mean_q: 0.751794
 89039/100000: episode: 1255, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.737, mean reward: 0.369 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.390], loss: 0.000005, mae: 0.002413, mean_q: 0.749484
 89040/100000: episode: 1256, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.731, mean reward: 0.731 [0.731, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.309, 10.218], loss: 0.000002, mae: 0.001670, mean_q: 0.750064
 89044/100000: episode: 1257, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 0.767, mean reward: 0.192 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.448], loss: 0.000019, mae: 0.003788, mean_q: 0.755133
 89045/100000: episode: 1258, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.674, mean reward: 0.674 [0.674, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.070, 10.212], loss: 0.000010, mae: 0.003818, mean_q: 0.748018
 89046/100000: episode: 1259, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.045, 10.315], loss: 0.000008, mae: 0.003409, mean_q: 0.754100
 89047/100000: episode: 1260, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.644, mean reward: 0.644 [0.644, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.312], loss: 0.000006, mae: 0.003002, mean_q: 0.757293
 89048/100000: episode: 1261, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.070, 10.282], loss: 0.000483, mae: 0.009628, mean_q: 0.757411
 89049/100000: episode: 1262, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.606, mean reward: 0.606 [0.606, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.070, 10.291], loss: 0.000033, mae: 0.007812, mean_q: 0.745767
 89050/100000: episode: 1263, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.645, mean reward: 0.645 [0.645, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.070, 10.311], loss: 0.000027, mae: 0.007040, mean_q: 0.743495
 89051/100000: episode: 1264, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.070, 10.277], loss: 0.000006, mae: 0.002291, mean_q: 0.753679
 89052/100000: episode: 1265, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.594, mean reward: 0.594 [0.594, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.070, 10.243], loss: 0.000205, mae: 0.009545, mean_q: 0.761112
 89053/100000: episode: 1266, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.614, mean reward: 0.614 [0.614, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.246], loss: 0.000022, mae: 0.006270, mean_q: 0.759164
 89054/100000: episode: 1267, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.588, mean reward: 0.588 [0.588, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.070, 10.246], loss: 0.000046, mae: 0.004623, mean_q: 0.751851
 89055/100000: episode: 1268, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.070, 10.242], loss: 0.000124, mae: 0.007542, mean_q: 0.747845
 89056/100000: episode: 1269, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.568, mean reward: 0.568 [0.568, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.070, 10.200], loss: 0.000003, mae: 0.002136, mean_q: 0.752896
 89057/100000: episode: 1270, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.722, mean reward: 0.722 [0.722, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.070, 10.370], loss: 0.000557, mae: 0.010903, mean_q: 0.757576
 89058/100000: episode: 1271, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.070, 10.309], loss: 0.000011, mae: 0.004398, mean_q: 0.750433
 89059/100000: episode: 1272, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.070, 10.260], loss: 0.000298, mae: 0.011548, mean_q: 0.744152
 89060/100000: episode: 1273, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.692, mean reward: 0.692 [0.692, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.435 [-0.035, 10.341], loss: 0.000007, mae: 0.003251, mean_q: 0.749146
 89061/100000: episode: 1274, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.606, mean reward: 0.606 [0.606, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.070, 10.280], loss: 0.000039, mae: 0.006373, mean_q: 0.757714
 89062/100000: episode: 1275, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.058, 10.272], loss: 0.000024, mae: 0.006762, mean_q: 0.760053
 89063/100000: episode: 1276, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.035, 10.401], loss: 0.000300, mae: 0.005407, mean_q: 0.751462
 89067/100000: episode: 1277, duration: 0.035s, episode steps: 4, steps per second: 113, episode reward: 0.793, mean reward: 0.198 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.543], loss: 0.000046, mae: 0.008316, mean_q: 0.744306
 89068/100000: episode: 1278, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.655, mean reward: 0.655 [0.655, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.070, 10.243], loss: 0.000034, mae: 0.008021, mean_q: 0.760902
 89069/100000: episode: 1279, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.577, mean reward: 0.577 [0.577, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.070, 10.212], loss: 0.000016, mae: 0.005274, mean_q: 0.758773
 89070/100000: episode: 1280, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.035, 10.289], loss: 0.000008, mae: 0.003364, mean_q: 0.750406
 89071/100000: episode: 1281, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.070, 10.376], loss: 0.000022, mae: 0.005969, mean_q: 0.748274
 89073/100000: episode: 1282, duration: 0.021s, episode steps: 2, steps per second: 95, episode reward: 0.726, mean reward: 0.363 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.474], loss: 0.000067, mae: 0.003228, mean_q: 0.753886
 89075/100000: episode: 1283, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.745, mean reward: 0.372 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.493], loss: 0.000016, mae: 0.002323, mean_q: 0.751213
 89076/100000: episode: 1284, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.638, mean reward: 0.638 [0.638, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.070, 10.269], loss: 0.000008, mae: 0.003530, mean_q: 0.749710
 89077/100000: episode: 1285, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.290], loss: 0.000094, mae: 0.003861, mean_q: 0.753389
 89079/100000: episode: 1286, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.716, mean reward: 0.358 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.438], loss: 0.000492, mae: 0.007456, mean_q: 0.751724
 89080/100000: episode: 1287, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.315], loss: 0.000192, mae: 0.004970, mean_q: 0.751561
 89081/100000: episode: 1288, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.363], loss: 0.000029, mae: 0.002584, mean_q: 0.750158
 89082/100000: episode: 1289, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.629, mean reward: 0.629 [0.629, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.300], loss: 0.000004, mae: 0.001966, mean_q: 0.752725
 89086/100000: episode: 1290, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 0.684, mean reward: 0.171 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.409], loss: 0.000065, mae: 0.003651, mean_q: 0.754092
 89087/100000: episode: 1291, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.690, mean reward: 0.690 [0.690, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.070, 10.252], loss: 0.000262, mae: 0.005477, mean_q: 0.750821
 89088/100000: episode: 1292, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.601, mean reward: 0.601 [0.601, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.159, 10.284], loss: 0.000035, mae: 0.004745, mean_q: 0.751769
 89089/100000: episode: 1293, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.070, 10.308], loss: 0.000107, mae: 0.005213, mean_q: 0.750591
 89093/100000: episode: 1294, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.777, mean reward: 0.194 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.530], loss: 0.000109, mae: 0.003549, mean_q: 0.750015
 89094/100000: episode: 1295, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.631, mean reward: 0.631 [0.631, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.070, 10.242], loss: 0.000666, mae: 0.009554, mean_q: 0.752670
 89095/100000: episode: 1296, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.350], loss: 0.000023, mae: 0.006442, mean_q: 0.757870
 89099/100000: episode: 1297, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.740, mean reward: 0.185 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.456], loss: 0.000211, mae: 0.008909, mean_q: 0.749466
 89100/100000: episode: 1298, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.373, 10.356], loss: 0.000264, mae: 0.011900, mean_q: 0.745388
 89104/100000: episode: 1299, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 0.720, mean reward: 0.180 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.437], loss: 0.000013, mae: 0.004376, mean_q: 0.757090
 89105/100000: episode: 1300, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.640, mean reward: 0.640 [0.640, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.352], loss: 0.000198, mae: 0.005637, mean_q: 0.753447
 89109/100000: episode: 1301, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 0.768, mean reward: 0.192 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.499], loss: 0.000088, mae: 0.009010, mean_q: 0.743425
 89113/100000: episode: 1302, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 0.751, mean reward: 0.188 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.583, 10.509], loss: 0.000046, mae: 0.005901, mean_q: 0.756893
 89117/100000: episode: 1303, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 0.780, mean reward: 0.195 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.533], loss: 0.000054, mae: 0.003511, mean_q: 0.751483
 89118/100000: episode: 1304, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.619, mean reward: 0.619 [0.619, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.244], loss: 0.000239, mae: 0.007862, mean_q: 0.755589
 89119/100000: episode: 1305, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.705, mean reward: 0.705 [0.705, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.035, 10.409], loss: 0.000007, mae: 0.003235, mean_q: 0.750873
 89120/100000: episode: 1306, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.715, mean reward: 0.715 [0.715, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.070, 10.248], loss: 0.000123, mae: 0.007902, mean_q: 0.746436
 89122/100000: episode: 1307, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.700, mean reward: 0.350 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.426], loss: 0.000121, mae: 0.004865, mean_q: 0.751272
 89123/100000: episode: 1308, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.612, mean reward: 0.612 [0.612, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.070, 10.272], loss: 0.000038, mae: 0.003501, mean_q: 0.749977
 89124/100000: episode: 1309, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.569, mean reward: 0.569 [0.569, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.666, 10.233], loss: 0.000030, mae: 0.004891, mean_q: 0.755285
 89125/100000: episode: 1310, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.070, 10.214], loss: 0.000004, mae: 0.002419, mean_q: 0.753895
 89127/100000: episode: 1311, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.755, mean reward: 0.378 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.035, 10.492], loss: 0.000005, mae: 0.002192, mean_q: 0.749362
 89129/100000: episode: 1312, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 0.774, mean reward: 0.387 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.414], loss: 0.000012, mae: 0.003103, mean_q: 0.748874
 89130/100000: episode: 1313, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.636, mean reward: 0.636 [0.636, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.070, 10.301], loss: 0.000004, mae: 0.002208, mean_q: 0.746754
 89134/100000: episode: 1314, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 0.782, mean reward: 0.196 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.193, 10.538], loss: 0.000043, mae: 0.003467, mean_q: 0.753153
 89136/100000: episode: 1315, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.708, mean reward: 0.354 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.452], loss: 0.000046, mae: 0.005181, mean_q: 0.749294
 89137/100000: episode: 1316, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.612, mean reward: 0.612 [0.612, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.070, 10.259], loss: 0.000345, mae: 0.014189, mean_q: 0.745774
 89138/100000: episode: 1317, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.693, mean reward: 0.693 [0.693, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.313], loss: 0.000157, mae: 0.006683, mean_q: 0.746663
 89139/100000: episode: 1318, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.670, mean reward: 0.670 [0.670, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.070, 10.267], loss: 0.000743, mae: 0.010259, mean_q: 0.754179
 89140/100000: episode: 1319, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.085, 10.298], loss: 0.000369, mae: 0.011600, mean_q: 0.755475
 89141/100000: episode: 1320, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.401], loss: 0.000006, mae: 0.002569, mean_q: 0.750742
 89142/100000: episode: 1321, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.567, mean reward: 0.567 [0.567, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.229], loss: 0.000014, mae: 0.004295, mean_q: 0.747849
 89146/100000: episode: 1322, duration: 0.032s, episode steps: 4, steps per second: 123, episode reward: 0.803, mean reward: 0.201 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.483], loss: 0.000064, mae: 0.003326, mean_q: 0.751408
 89148/100000: episode: 1323, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.725, mean reward: 0.363 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.457], loss: 0.000286, mae: 0.007867, mean_q: 0.748487
 89149/100000: episode: 1324, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.643, mean reward: 0.643 [0.643, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.224, 10.350], loss: 0.000037, mae: 0.007635, mean_q: 0.743811
 89153/100000: episode: 1325, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 0.804, mean reward: 0.201 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.571, 10.342], loss: 0.000089, mae: 0.006626, mean_q: 0.753606
 89154/100000: episode: 1326, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.070, 10.209], loss: 0.000121, mae: 0.006646, mean_q: 0.755283
 89158/100000: episode: 1327, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 0.716, mean reward: 0.179 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.447], loss: 0.000084, mae: 0.006812, mean_q: 0.745431
 89159/100000: episode: 1328, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.596, mean reward: 0.596 [0.596, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.070, 10.265], loss: 0.000150, mae: 0.006387, mean_q: 0.754018
 89160/100000: episode: 1329, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.580, mean reward: 0.580 [0.580, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.070, 10.241], loss: 0.000267, mae: 0.006414, mean_q: 0.751727
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7784573435783386
1
 89164/100000: episode: 1330, duration: 4.443s, episode steps: 4, steps per second: 1, episode reward: 0.754, mean reward: 0.188 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.515], loss: 0.000122, mae: 0.007648, mean_q: 0.745184
 89265/100000: episode: 1331, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.447, 10.100], loss: 0.000113, mae: 0.005863, mean_q: 0.749262
 89366/100000: episode: 1332, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.955, 10.168], loss: 0.000098, mae: 0.005599, mean_q: 0.748018
 89467/100000: episode: 1333, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.115, 10.100], loss: 0.000105, mae: 0.005552, mean_q: 0.747610
 89568/100000: episode: 1334, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.862, mean reward: 0.009 [0.000, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.247, 10.109], loss: 0.000098, mae: 0.004912, mean_q: 0.746677
 89669/100000: episode: 1335, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.790, 10.100], loss: 0.000106, mae: 0.005020, mean_q: 0.746276
 89770/100000: episode: 1336, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.993, 10.100], loss: 0.000092, mae: 0.005923, mean_q: 0.745191
 89871/100000: episode: 1337, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.100], loss: 0.000093, mae: 0.003590, mean_q: 0.744803
 89972/100000: episode: 1338, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.844, 10.395], loss: 0.000082, mae: 0.004713, mean_q: 0.744575
 90073/100000: episode: 1339, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.777, 10.100], loss: 0.000116, mae: 0.005738, mean_q: 0.745348
 90174/100000: episode: 1340, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.138, 10.100], loss: 0.000066, mae: 0.003514, mean_q: 0.744297
 90275/100000: episode: 1341, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.150, 10.204], loss: 0.000074, mae: 0.003886, mean_q: 0.744380
 90376/100000: episode: 1342, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.115, 10.100], loss: 0.000075, mae: 0.004817, mean_q: 0.743829
 90477/100000: episode: 1343, duration: 0.626s, episode steps: 101, steps per second: 161, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.610, 10.350], loss: 0.000085, mae: 0.004517, mean_q: 0.744351
 90578/100000: episode: 1344, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.725, 10.246], loss: 0.000072, mae: 0.004109, mean_q: 0.744605
 90679/100000: episode: 1345, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.022, 10.365], loss: 0.000062, mae: 0.003721, mean_q: 0.744645
 90780/100000: episode: 1346, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.954, 10.144], loss: 0.000054, mae: 0.003571, mean_q: 0.744470
 90881/100000: episode: 1347, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.836, 10.100], loss: 0.000080, mae: 0.004087, mean_q: 0.743537
 90982/100000: episode: 1348, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.265, 10.187], loss: 0.000073, mae: 0.003995, mean_q: 0.744092
 91083/100000: episode: 1349, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.536, 10.100], loss: 0.000081, mae: 0.004428, mean_q: 0.744399
 91184/100000: episode: 1350, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.337, 10.293], loss: 0.000087, mae: 0.003765, mean_q: 0.744739
 91285/100000: episode: 1351, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.681, 10.100], loss: 0.000082, mae: 0.004594, mean_q: 0.744573
 91386/100000: episode: 1352, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.176, 10.206], loss: 0.000078, mae: 0.004678, mean_q: 0.743990
 91487/100000: episode: 1353, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.389, 10.100], loss: 0.000067, mae: 0.003456, mean_q: 0.744164
 91588/100000: episode: 1354, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.671, 10.276], loss: 0.000068, mae: 0.003674, mean_q: 0.743758
 91689/100000: episode: 1355, duration: 0.677s, episode steps: 101, steps per second: 149, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.195, 10.228], loss: 0.000065, mae: 0.003704, mean_q: 0.743606
 91790/100000: episode: 1356, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.733, 10.167], loss: 0.000062, mae: 0.004020, mean_q: 0.744326
 91891/100000: episode: 1357, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.152, 10.244], loss: 0.000045, mae: 0.002983, mean_q: 0.744639
 91992/100000: episode: 1358, duration: 0.653s, episode steps: 101, steps per second: 155, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.100], loss: 0.000053, mae: 0.003295, mean_q: 0.744112
 92093/100000: episode: 1359, duration: 0.625s, episode steps: 101, steps per second: 161, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.405, 10.100], loss: 0.000077, mae: 0.003855, mean_q: 0.743797
 92194/100000: episode: 1360, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.550, 10.100], loss: 0.000062, mae: 0.003662, mean_q: 0.744118
 92295/100000: episode: 1361, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.100], loss: 0.000079, mae: 0.003939, mean_q: 0.744049
 92396/100000: episode: 1362, duration: 0.632s, episode steps: 101, steps per second: 160, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.558, 10.100], loss: 0.000051, mae: 0.003268, mean_q: 0.744746
 92497/100000: episode: 1363, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.411, 10.100], loss: 0.000076, mae: 0.004596, mean_q: 0.744332
 92598/100000: episode: 1364, duration: 0.653s, episode steps: 101, steps per second: 155, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.280, 10.100], loss: 0.000072, mae: 0.004223, mean_q: 0.744064
 92699/100000: episode: 1365, duration: 0.650s, episode steps: 101, steps per second: 155, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.115, 10.202], loss: 0.000049, mae: 0.003361, mean_q: 0.744033
 92800/100000: episode: 1366, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.029, 10.147], loss: 0.000063, mae: 0.003451, mean_q: 0.744089
 92901/100000: episode: 1367, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.808, 10.277], loss: 0.000046, mae: 0.003306, mean_q: 0.744514
 93002/100000: episode: 1368, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.051, 10.217], loss: 0.000055, mae: 0.003139, mean_q: 0.744301
 93103/100000: episode: 1369, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.298, 10.109], loss: 0.000052, mae: 0.003050, mean_q: 0.744118
 93204/100000: episode: 1370, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.975, 10.255], loss: 0.000065, mae: 0.004416, mean_q: 0.744266
 93305/100000: episode: 1371, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.193, 10.100], loss: 0.000057, mae: 0.003978, mean_q: 0.743947
 93406/100000: episode: 1372, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.581, 10.284], loss: 0.000051, mae: 0.003639, mean_q: 0.744191
 93507/100000: episode: 1373, duration: 0.618s, episode steps: 101, steps per second: 164, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.071, 10.100], loss: 0.000062, mae: 0.003792, mean_q: 0.744471
 93608/100000: episode: 1374, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.100], loss: 0.000048, mae: 0.003462, mean_q: 0.744302
 93709/100000: episode: 1375, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.271, 10.212], loss: 0.000057, mae: 0.003849, mean_q: 0.743997
 93810/100000: episode: 1376, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.138, 10.231], loss: 0.000048, mae: 0.003238, mean_q: 0.744299
 93911/100000: episode: 1377, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.593, 10.100], loss: 0.000044, mae: 0.003268, mean_q: 0.744111
 94012/100000: episode: 1378, duration: 0.737s, episode steps: 101, steps per second: 137, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.496, 10.167], loss: 0.000028, mae: 0.003156, mean_q: 0.745453
 94113/100000: episode: 1379, duration: 0.692s, episode steps: 101, steps per second: 146, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.009, 10.100], loss: 0.000035, mae: 0.002940, mean_q: 0.745711
 94214/100000: episode: 1380, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.314, 10.100], loss: 0.000024, mae: 0.002338, mean_q: 0.745829
 94315/100000: episode: 1381, duration: 0.723s, episode steps: 101, steps per second: 140, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.577, 10.110], loss: 0.000017, mae: 0.002151, mean_q: 0.746022
 94416/100000: episode: 1382, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.875, mean reward: 0.009 [0.000, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.049, 10.100], loss: 0.000021, mae: 0.002271, mean_q: 0.746173
 94517/100000: episode: 1383, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.974, 10.248], loss: 0.000024, mae: 0.002151, mean_q: 0.746537
 94618/100000: episode: 1384, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.579, 10.100], loss: 0.000009, mae: 0.001536, mean_q: 0.746336
 94719/100000: episode: 1385, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.574, 10.272], loss: 0.000009, mae: 0.001474, mean_q: 0.746136
 94820/100000: episode: 1386, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.835, 10.119], loss: 0.000010, mae: 0.001730, mean_q: 0.746122
 94921/100000: episode: 1387, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.819, 10.449], loss: 0.000013, mae: 0.001633, mean_q: 0.745893
 95022/100000: episode: 1388, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.849, mean reward: 0.008 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.730, 10.100], loss: 0.000007, mae: 0.001419, mean_q: 0.746068
 95123/100000: episode: 1389, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.070, 10.278], loss: 0.000012, mae: 0.001483, mean_q: 0.746029
 95224/100000: episode: 1390, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.569, 10.355], loss: 0.000012, mae: 0.001571, mean_q: 0.745878
 95325/100000: episode: 1391, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.100], loss: 0.000008, mae: 0.001376, mean_q: 0.745908
 95426/100000: episode: 1392, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.028, 10.462], loss: 0.000009, mae: 0.001627, mean_q: 0.745955
 95527/100000: episode: 1393, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.601, 10.100], loss: 0.000008, mae: 0.001558, mean_q: 0.745902
 95628/100000: episode: 1394, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.682, 10.226], loss: 0.000009, mae: 0.001571, mean_q: 0.746118
 95729/100000: episode: 1395, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.169, 10.101], loss: 0.000009, mae: 0.001453, mean_q: 0.746034
 95830/100000: episode: 1396, duration: 0.723s, episode steps: 101, steps per second: 140, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.876, 10.100], loss: 0.000009, mae: 0.001403, mean_q: 0.746058
 95931/100000: episode: 1397, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.850, mean reward: 0.008 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.932, 10.351], loss: 0.000005, mae: 0.001126, mean_q: 0.746159
 96032/100000: episode: 1398, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.100], loss: 0.000005, mae: 0.001207, mean_q: 0.746117
 96133/100000: episode: 1399, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.136, 10.100], loss: 0.000009, mae: 0.001422, mean_q: 0.746238
 96234/100000: episode: 1400, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.945, 10.100], loss: 0.000012, mae: 0.001685, mean_q: 0.745925
 96335/100000: episode: 1401, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.785, 10.100], loss: 0.000011, mae: 0.001538, mean_q: 0.746122
 96436/100000: episode: 1402, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.987, 10.100], loss: 0.000007, mae: 0.001368, mean_q: 0.746194
 96537/100000: episode: 1403, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.912, 10.159], loss: 0.000006, mae: 0.001350, mean_q: 0.745952
 96638/100000: episode: 1404, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.414, 10.122], loss: 0.000012, mae: 0.001501, mean_q: 0.746050
 96739/100000: episode: 1405, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.359, 10.218], loss: 0.000007, mae: 0.001373, mean_q: 0.746019
 96840/100000: episode: 1406, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.353, 10.100], loss: 0.000016, mae: 0.001787, mean_q: 0.746091
 96941/100000: episode: 1407, duration: 0.668s, episode steps: 101, steps per second: 151, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.919, 10.131], loss: 0.000004, mae: 0.001014, mean_q: 0.746024
 97042/100000: episode: 1408, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.307, 10.272], loss: 0.000010, mae: 0.001643, mean_q: 0.745912
 97143/100000: episode: 1409, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.928, 10.152], loss: 0.000009, mae: 0.001254, mean_q: 0.746095
 97244/100000: episode: 1410, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.324, 10.194], loss: 0.000008, mae: 0.001509, mean_q: 0.746102
 97345/100000: episode: 1411, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.152, 10.100], loss: 0.000011, mae: 0.001498, mean_q: 0.746455
 97446/100000: episode: 1412, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.915, 10.219], loss: 0.000010, mae: 0.001483, mean_q: 0.746540
 97547/100000: episode: 1413, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.711, 10.411], loss: 0.000006, mae: 0.001289, mean_q: 0.746292
 97648/100000: episode: 1414, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.524, 10.184], loss: 0.000013, mae: 0.001657, mean_q: 0.746213
 97749/100000: episode: 1415, duration: 0.740s, episode steps: 101, steps per second: 136, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.029, 10.225], loss: 0.000006, mae: 0.001068, mean_q: 0.746300
 97850/100000: episode: 1416, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.187, 10.100], loss: 0.000017, mae: 0.002333, mean_q: 0.746397
 97951/100000: episode: 1417, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.124], loss: 0.000012, mae: 0.001429, mean_q: 0.746585
 98052/100000: episode: 1418, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.111, 10.117], loss: 0.000006, mae: 0.001129, mean_q: 0.746676
 98153/100000: episode: 1419, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.407, 10.100], loss: 0.000013, mae: 0.001962, mean_q: 0.746635
 98254/100000: episode: 1420, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.972, 10.100], loss: 0.000018, mae: 0.001910, mean_q: 0.746701
 98355/100000: episode: 1421, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.004, 10.154], loss: 0.000009, mae: 0.001305, mean_q: 0.746664
 98456/100000: episode: 1422, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.669, 10.219], loss: 0.000009, mae: 0.001422, mean_q: 0.747076
 98557/100000: episode: 1423, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.207], loss: 0.000007, mae: 0.001214, mean_q: 0.747156
 98658/100000: episode: 1424, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.001, 10.246], loss: 0.000011, mae: 0.001691, mean_q: 0.746872
 98759/100000: episode: 1425, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.100], loss: 0.000009, mae: 0.001433, mean_q: 0.747148
 98860/100000: episode: 1426, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.782, 10.100], loss: 0.000007, mae: 0.001118, mean_q: 0.747075
 98961/100000: episode: 1427, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.027, 10.100], loss: 0.000011, mae: 0.001381, mean_q: 0.747371
 99062/100000: episode: 1428, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.660, 10.100], loss: 0.000013, mae: 0.001522, mean_q: 0.747416
 99163/100000: episode: 1429, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.135, 10.168], loss: 0.000012, mae: 0.001555, mean_q: 0.747545
[Info] 1-TH LEVEL FOUND: 0.7668312788009644, Considering 10/100 traces
 99264/100000: episode: 1430, duration: 5.206s, episode steps: 101, steps per second: 19, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.912, 10.100], loss: 0.000011, mae: 0.001505, mean_q: 0.747633
 99265/100000: episode: 1431, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.683, mean reward: 0.683 [0.683, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.070, 10.401], loss: 0.000003, mae: 0.002196, mean_q: 0.751791
 99266/100000: episode: 1432, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.753, mean reward: 0.753 [0.753, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.070, 10.410], loss: 0.000004, mae: 0.002463, mean_q: 0.751783
 99267/100000: episode: 1433, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.722, mean reward: 0.722 [0.722, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.070, 10.200], loss: 0.000001, mae: 0.001018, mean_q: 0.747060
 99268/100000: episode: 1434, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.592, mean reward: 0.592 [0.592, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.070, 10.246], loss: 0.000003, mae: 0.002306, mean_q: 0.747276
 99269/100000: episode: 1435, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.070, 10.373], loss: 0.000000, mae: 0.000661, mean_q: 0.746427
 99270/100000: episode: 1436, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.583, mean reward: 0.583 [0.583, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.070, 10.228], loss: 0.000001, mae: 0.001155, mean_q: 0.747784
 99271/100000: episode: 1437, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.070, 10.401], loss: 0.000001, mae: 0.001070, mean_q: 0.747805
 99272/100000: episode: 1438, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.662, mean reward: 0.662 [0.662, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.070, 10.366], loss: 0.000000, mae: 0.000699, mean_q: 0.748224
 99273/100000: episode: 1439, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.555, mean reward: 0.555 [0.555, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.070, 10.200], loss: 0.000071, mae: 0.003191, mean_q: 0.746991
 99274/100000: episode: 1440, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.662, mean reward: 0.662 [0.662, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.070, 10.367], loss: 0.000001, mae: 0.001140, mean_q: 0.748603
 99275/100000: episode: 1441, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.200], loss: 0.000181, mae: 0.004839, mean_q: 0.749312
 99276/100000: episode: 1442, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.643, mean reward: 0.643 [0.643, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.070, 10.338], loss: 0.000003, mae: 0.002454, mean_q: 0.745163
 99277/100000: episode: 1443, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.662, mean reward: 0.662 [0.662, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.070, 10.200], loss: 0.000022, mae: 0.003051, mean_q: 0.748528
 99278/100000: episode: 1444, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.718, mean reward: 0.718 [0.718, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.070, 10.418], loss: 0.000017, mae: 0.002801, mean_q: 0.751752
 99279/100000: episode: 1445, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.703, mean reward: 0.703 [0.703, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.070, 10.432], loss: 0.000004, mae: 0.002870, mean_q: 0.749719
 99280/100000: episode: 1446, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.372], loss: 0.000000, mae: 0.000256, mean_q: 0.747515
 99281/100000: episode: 1447, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.412], loss: 0.000004, mae: 0.002809, mean_q: 0.745388
 99282/100000: episode: 1448, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.721, mean reward: 0.721 [0.721, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.070, 10.458], loss: 0.000468, mae: 0.006835, mean_q: 0.748747
 99283/100000: episode: 1449, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.070, 10.358], loss: 0.000002, mae: 0.001579, mean_q: 0.743345
 99284/100000: episode: 1450, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.749, mean reward: 0.749 [0.749, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.070, 10.496], loss: 0.000001, mae: 0.000984, mean_q: 0.748882
 99285/100000: episode: 1451, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.715, mean reward: 0.715 [0.715, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.070, 10.457], loss: 0.000065, mae: 0.004662, mean_q: 0.750522
 99286/100000: episode: 1452, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.603, mean reward: 0.603 [0.603, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.070, 10.282], loss: 0.000002, mae: 0.001543, mean_q: 0.747489
 99287/100000: episode: 1453, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.663, mean reward: 0.663 [0.663, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.070, 10.349], loss: 0.000002, mae: 0.001525, mean_q: 0.746082
 99288/100000: episode: 1454, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.070, 10.250], loss: 0.000001, mae: 0.001384, mean_q: 0.747922
 99289/100000: episode: 1455, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.185, 10.200], loss: 0.000003, mae: 0.002051, mean_q: 0.750499
 99290/100000: episode: 1456, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.568, mean reward: 0.568 [0.568, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.070, 10.204], loss: 0.000001, mae: 0.000846, mean_q: 0.747273
 99291/100000: episode: 1457, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.559, mean reward: 0.559 [0.559, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.070, 10.206], loss: 0.000006, mae: 0.002641, mean_q: 0.747006
 99292/100000: episode: 1458, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.699, mean reward: 0.699 [0.699, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.070, 10.434], loss: 0.000126, mae: 0.003612, mean_q: 0.748757
 99293/100000: episode: 1459, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.070, 10.200], loss: 0.000186, mae: 0.006883, mean_q: 0.751628
 99294/100000: episode: 1460, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.670, mean reward: 0.670 [0.670, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.070, 10.389], loss: 0.000035, mae: 0.004944, mean_q: 0.744484
 99295/100000: episode: 1461, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.708, mean reward: 0.708 [0.708, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.070, 10.200], loss: 0.000043, mae: 0.009223, mean_q: 0.737532
 99296/100000: episode: 1462, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.070, 10.271], loss: 0.000158, mae: 0.007409, mean_q: 0.743722
 99297/100000: episode: 1463, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.642, mean reward: 0.642 [0.642, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.070, 10.349], loss: 0.000018, mae: 0.005292, mean_q: 0.752896
 99298/100000: episode: 1464, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.628, mean reward: 0.628 [0.628, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.070, 10.230], loss: 0.000022, mae: 0.005838, mean_q: 0.754155
 99299/100000: episode: 1465, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.731, mean reward: 0.731 [0.731, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.468], loss: 0.000006, mae: 0.002364, mean_q: 0.746783
 99300/100000: episode: 1466, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.070, 10.345], loss: 0.000018, mae: 0.005426, mean_q: 0.742977
 99301/100000: episode: 1467, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: 0.706, mean reward: 0.706 [0.706, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.443], loss: 0.000084, mae: 0.004989, mean_q: 0.745151
 99302/100000: episode: 1468, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.070, 10.369], loss: 0.000003, mae: 0.001995, mean_q: 0.747824
 99303/100000: episode: 1469, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.538, mean reward: 0.538 [0.538, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.070, 10.200], loss: 0.000004, mae: 0.002468, mean_q: 0.746917
 99304/100000: episode: 1470, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.683, mean reward: 0.683 [0.683, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.070, 10.390], loss: 0.000004, mae: 0.001792, mean_q: 0.747567
 99305/100000: episode: 1471, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.070, 10.340], loss: 0.000001, mae: 0.001268, mean_q: 0.745233
 99306/100000: episode: 1472, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.703, mean reward: 0.703 [0.703, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.070, 10.406], loss: 0.000004, mae: 0.001779, mean_q: 0.746594
 99307/100000: episode: 1473, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.622, mean reward: 0.622 [0.622, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.070, 10.305], loss: 0.000018, mae: 0.002274, mean_q: 0.746119
 99308/100000: episode: 1474, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.729, mean reward: 0.729 [0.729, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.451, 10.262], loss: 0.000006, mae: 0.002309, mean_q: 0.747797
 99309/100000: episode: 1475, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.618, mean reward: 0.618 [0.618, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.070, 10.200], loss: 0.000123, mae: 0.005077, mean_q: 0.748238
 99310/100000: episode: 1476, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.572, mean reward: 0.572 [0.572, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.070, 10.205], loss: 0.000001, mae: 0.000966, mean_q: 0.748206
 99311/100000: episode: 1477, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.742, mean reward: 0.742 [0.742, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.451 [-0.070, 10.421], loss: 0.000003, mae: 0.001354, mean_q: 0.747507
 99312/100000: episode: 1478, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.625, mean reward: 0.625 [0.625, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.070, 10.219], loss: 0.000124, mae: 0.004953, mean_q: 0.748995
 99313/100000: episode: 1479, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.070, 10.295], loss: 0.000002, mae: 0.001141, mean_q: 0.746449
 99314/100000: episode: 1480, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.249], loss: 0.000005, mae: 0.002841, mean_q: 0.744098
 99315/100000: episode: 1481, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.070, 10.390], loss: 0.000009, mae: 0.002715, mean_q: 0.745562
 99316/100000: episode: 1482, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.388], loss: 0.000003, mae: 0.001878, mean_q: 0.748159
 99317/100000: episode: 1483, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.614, mean reward: 0.614 [0.614, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.070, 10.269], loss: 0.000006, mae: 0.003201, mean_q: 0.749035
 99318/100000: episode: 1484, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.629, mean reward: 0.629 [0.629, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.324], loss: 0.000051, mae: 0.003624, mean_q: 0.747164
 99319/100000: episode: 1485, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.668, mean reward: 0.668 [0.668, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.070, 10.379], loss: 0.000003, mae: 0.001342, mean_q: 0.745981
 99320/100000: episode: 1486, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.711, mean reward: 0.711 [0.711, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.070, 10.446], loss: 0.000004, mae: 0.002551, mean_q: 0.745147
 99321/100000: episode: 1487, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.070, 10.371], loss: 0.000003, mae: 0.001778, mean_q: 0.745149
 99322/100000: episode: 1488, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.589, mean reward: 0.589 [0.589, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.070, 10.256], loss: 0.000256, mae: 0.005709, mean_q: 0.748301
 99323/100000: episode: 1489, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.070, 10.320], loss: 0.000164, mae: 0.004391, mean_q: 0.746110
 99324/100000: episode: 1490, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.705, mean reward: 0.705 [0.705, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.070, 10.309], loss: 0.000096, mae: 0.004997, mean_q: 0.743692
 99325/100000: episode: 1491, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: 0.640, mean reward: 0.640 [0.640, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.346], loss: 0.000018, mae: 0.003085, mean_q: 0.745109
 99326/100000: episode: 1492, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.769, mean reward: 0.769 [0.769, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.070, 10.346], loss: 0.000020, mae: 0.005679, mean_q: 0.748932
 99327/100000: episode: 1493, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.626, mean reward: 0.626 [0.626, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.070, 10.324], loss: 0.000010, mae: 0.004288, mean_q: 0.749414
 99328/100000: episode: 1494, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.070, 10.350], loss: 0.000049, mae: 0.007575, mean_q: 0.740102
 99329/100000: episode: 1495, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.770, mean reward: 0.770 [0.770, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.070, 10.326], loss: 0.000076, mae: 0.009739, mean_q: 0.738253
 99330/100000: episode: 1496, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.070, 10.453], loss: 0.000016, mae: 0.003961, mean_q: 0.747928
 99331/100000: episode: 1497, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.791, mean reward: 0.791 [0.791, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.070, 10.377], loss: 0.000340, mae: 0.013078, mean_q: 0.753848
 99332/100000: episode: 1498, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.861, mean reward: 0.861 [0.861, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.070, 10.371], loss: 0.000139, mae: 0.007240, mean_q: 0.747421
 99333/100000: episode: 1499, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.660, mean reward: 0.660 [0.660, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.070, 10.377], loss: 0.000491, mae: 0.012239, mean_q: 0.739694
 99334/100000: episode: 1500, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.718, mean reward: 0.718 [0.718, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.422], loss: 0.000051, mae: 0.008872, mean_q: 0.737374
 99335/100000: episode: 1501, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.580, mean reward: 0.580 [0.580, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.070, 10.200], loss: 0.000041, mae: 0.003788, mean_q: 0.747012
 99336/100000: episode: 1502, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.070, 10.436], loss: 0.000033, mae: 0.007927, mean_q: 0.753820
 99337/100000: episode: 1503, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.625, mean reward: 0.625 [0.625, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.070, 10.249], loss: 0.000018, mae: 0.005849, mean_q: 0.752169
 99338/100000: episode: 1504, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.566, mean reward: 0.566 [0.566, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.228, 10.200], loss: 0.000006, mae: 0.003265, mean_q: 0.744510
 99339/100000: episode: 1505, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.592, mean reward: 0.592 [0.592, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.070, 10.267], loss: 0.000053, mae: 0.009814, mean_q: 0.737930
 99340/100000: episode: 1506, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.837, mean reward: 0.837 [0.837, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.479 [-0.070, 10.441], loss: 0.000033, mae: 0.006534, mean_q: 0.741033
 99341/100000: episode: 1507, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.704, mean reward: 0.704 [0.704, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.070, 10.422], loss: 0.000016, mae: 0.005501, mean_q: 0.749384
 99342/100000: episode: 1508, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.720, mean reward: 0.720 [0.720, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.070, 10.460], loss: 0.000032, mae: 0.007939, mean_q: 0.755037
 99343/100000: episode: 1509, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.070, 10.279], loss: 0.000370, mae: 0.007605, mean_q: 0.747909
 99344/100000: episode: 1510, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.645, mean reward: 0.645 [0.645, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.342], loss: 0.000018, mae: 0.005885, mean_q: 0.741017
 99345/100000: episode: 1511, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.719, mean reward: 0.719 [0.719, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.403 [-0.070, 10.358], loss: 0.000053, mae: 0.009156, mean_q: 0.739350
 99346/100000: episode: 1512, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.070, 10.236], loss: 0.000023, mae: 0.003997, mean_q: 0.741587
 99347/100000: episode: 1513, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.708, mean reward: 0.708 [0.708, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.070, 10.438], loss: 0.000019, mae: 0.004398, mean_q: 0.746852
 99348/100000: episode: 1514, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.070, 10.222], loss: 0.000027, mae: 0.006800, mean_q: 0.750304
 99349/100000: episode: 1515, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.762, mean reward: 0.762 [0.762, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.070, 10.469], loss: 0.000285, mae: 0.007575, mean_q: 0.748792
 99350/100000: episode: 1516, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.884, mean reward: 0.884 [0.884, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.070, 10.374], loss: 0.000063, mae: 0.008132, mean_q: 0.738937
 99351/100000: episode: 1517, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.589, mean reward: 0.589 [0.589, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.070, 10.217], loss: 0.000100, mae: 0.009354, mean_q: 0.734860
 99352/100000: episode: 1518, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.668, mean reward: 0.668 [0.668, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.070, 10.299], loss: 0.000221, mae: 0.008537, mean_q: 0.738957
 99353/100000: episode: 1519, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.636, mean reward: 0.636 [0.636, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.070, 10.295], loss: 0.000284, mae: 0.013761, mean_q: 0.736859
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7668312788009644
1
 99354/100000: episode: 1520, duration: 4.554s, episode steps: 1, steps per second: 0, episode reward: 0.610, mean reward: 0.610 [0.610, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.070, 10.268], loss: 0.000131, mae: 0.011347, mean_q: 0.751168
 99455/100000: episode: 1521, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.637, 10.152], loss: 0.000098, mae: 0.005518, mean_q: 0.744343
 99556/100000: episode: 1522, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.100], loss: 0.000062, mae: 0.003791, mean_q: 0.743759
 99657/100000: episode: 1523, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.763, 10.100], loss: 0.000071, mae: 0.004030, mean_q: 0.743312
 99758/100000: episode: 1524, duration: 0.670s, episode steps: 101, steps per second: 151, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.567, 10.234], loss: 0.000072, mae: 0.004458, mean_q: 0.742390
 99859/100000: episode: 1525, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.961, 10.264], loss: 0.000099, mae: 0.005245, mean_q: 0.742602
 99960/100000: episode: 1526, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.470, 10.100], loss: 0.000041, mae: 0.003327, mean_q: 0.742758
done, took 652.068 seconds
[Info] End Importance Splitting. Falsification occurred 1 times.
