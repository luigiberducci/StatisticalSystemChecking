Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_14_42_05_CET/4, out/localization/mer27nov2019_14_42_05_CET/4/levels, out/localization/mer27nov2019_14_42_05_CET/4/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.174s, episode steps: 101, steps per second: 579, episode reward: 0.864, mean reward: 0.009 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.715, 10.397], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.066s, episode steps: 101, steps per second: 1529, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.067s, episode steps: 101, steps per second: 1508, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.137], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.067s, episode steps: 101, steps per second: 1500, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.952, 10.100], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.072s, episode steps: 101, steps per second: 1401, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.706, 10.148], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.067s, episode steps: 101, steps per second: 1515, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.146, 10.278], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.068s, episode steps: 101, steps per second: 1494, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.171], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.066s, episode steps: 101, steps per second: 1532, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.033, 10.100], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.066s, episode steps: 101, steps per second: 1536, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.735, 10.100], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.071s, episode steps: 101, steps per second: 1427, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.626, 10.120], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.080s, episode steps: 101, steps per second: 1269, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.020, 10.278], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.065s, episode steps: 101, steps per second: 1552, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.338, 10.233], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.065s, episode steps: 101, steps per second: 1554, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.606, 10.267], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.073s, episode steps: 101, steps per second: 1375, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.985, 10.448], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.065s, episode steps: 101, steps per second: 1553, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.770, 10.277], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.065s, episode steps: 101, steps per second: 1547, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.687, 10.100], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.065s, episode steps: 101, steps per second: 1546, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.905, 10.329], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.066s, episode steps: 101, steps per second: 1533, episode reward: 0.816, mean reward: 0.008 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.100], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.066s, episode steps: 101, steps per second: 1534, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.608, 10.196], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.065s, episode steps: 101, steps per second: 1546, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.610, 10.265], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.065s, episode steps: 101, steps per second: 1550, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.709, 10.100], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.065s, episode steps: 101, steps per second: 1545, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.455, 10.100], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.066s, episode steps: 101, steps per second: 1524, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.330, 10.100], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.072s, episode steps: 101, steps per second: 1394, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.919, 10.152], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.065s, episode steps: 101, steps per second: 1549, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.647, 10.133], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.065s, episode steps: 101, steps per second: 1546, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.978, 10.100], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.071s, episode steps: 101, steps per second: 1425, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.726, 10.218], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.069s, episode steps: 101, steps per second: 1465, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.753, 10.100], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.066s, episode steps: 101, steps per second: 1535, episode reward: 0.931, mean reward: 0.009 [0.000, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.233, 10.508], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.071s, episode steps: 101, steps per second: 1418, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-2.232, 10.165], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.066s, episode steps: 101, steps per second: 1530, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.987, 10.100], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.066s, episode steps: 101, steps per second: 1539, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.659, 10.282], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.065s, episode steps: 101, steps per second: 1549, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.484, 10.128], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.065s, episode steps: 101, steps per second: 1549, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.207, 10.250], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.065s, episode steps: 101, steps per second: 1555, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.271, 10.100], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.065s, episode steps: 101, steps per second: 1557, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.707, 10.100], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.074s, episode steps: 101, steps per second: 1370, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.524, 10.100], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.079s, episode steps: 101, steps per second: 1278, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.026, 10.100], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.071s, episode steps: 101, steps per second: 1417, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.512, 10.220], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.082s, episode steps: 101, steps per second: 1232, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.585, 10.135], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.088s, episode steps: 101, steps per second: 1150, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.529, 10.100], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.065s, episode steps: 101, steps per second: 1550, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.793, 10.242], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.065s, episode steps: 101, steps per second: 1547, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.970, 10.100], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.090s, episode steps: 101, steps per second: 1126, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.332, 10.100], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.065s, episode steps: 101, steps per second: 1549, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.822, 10.100], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.065s, episode steps: 101, steps per second: 1547, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.907, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.065s, episode steps: 101, steps per second: 1546, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.420, 10.100], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.075s, episode steps: 101, steps per second: 1344, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.738, 10.100], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.065s, episode steps: 101, steps per second: 1544, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.612, 10.272], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 1.008s, episode steps: 101, steps per second: 100, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.665, 10.245], loss: 0.020016, mae: 0.071914, mean_q: 1.288653
  5151/100000: episode: 51, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.665, mean reward: 0.007 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.770, 10.100], loss: 0.013192, mae: 0.055037, mean_q: 1.302058
  5252/100000: episode: 52, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.100], loss: 0.010456, mae: 0.050487, mean_q: 1.294016
  5353/100000: episode: 53, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.885, mean reward: 0.009 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.341, 10.100], loss: 0.008341, mae: 0.044220, mean_q: 1.339444
  5454/100000: episode: 54, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.866, 10.186], loss: 0.007547, mae: 0.043500, mean_q: 1.325948
  5555/100000: episode: 55, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.507, 10.331], loss: 0.006064, mae: 0.036271, mean_q: 1.326708
  5656/100000: episode: 56, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.118, 10.100], loss: 0.003767, mae: 0.027849, mean_q: 1.327990
  5757/100000: episode: 57, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.871, 10.100], loss: 0.003759, mae: 0.031357, mean_q: 1.352255
  5858/100000: episode: 58, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.506, 10.271], loss: 0.003289, mae: 0.037999, mean_q: 1.323540
  5959/100000: episode: 59, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.167, 10.107], loss: 0.001788, mae: 0.024703, mean_q: 1.345783
  6060/100000: episode: 60, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.898, 10.405], loss: 0.001369, mae: 0.027728, mean_q: 1.351783
  6161/100000: episode: 61, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.190, 10.170], loss: 0.000444, mae: 0.016585, mean_q: 1.327121
  6262/100000: episode: 62, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.638, 10.100], loss: 0.000378, mae: 0.015666, mean_q: 1.336769
  6363/100000: episode: 63, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.066, 10.311], loss: 0.000283, mae: 0.014524, mean_q: 1.357727
  6464/100000: episode: 64, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.591, 10.100], loss: 0.000269, mae: 0.014635, mean_q: 1.357970
  6565/100000: episode: 65, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.497, 10.264], loss: 0.000220, mae: 0.014343, mean_q: 1.358706
  6666/100000: episode: 66, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.358, 10.100], loss: 0.000435, mae: 0.016330, mean_q: 1.380234
  6767/100000: episode: 67, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.977, 10.130], loss: 0.000391, mae: 0.015735, mean_q: 1.385985
  6868/100000: episode: 68, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.825, 10.100], loss: 0.000393, mae: 0.016204, mean_q: 1.371897
  6969/100000: episode: 69, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.800, 10.100], loss: 0.000276, mae: 0.014929, mean_q: 1.380813
  7070/100000: episode: 70, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.524, 10.304], loss: 0.000305, mae: 0.015025, mean_q: 1.373159
  7171/100000: episode: 71, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.494, 10.486], loss: 0.000287, mae: 0.015410, mean_q: 1.400191
  7272/100000: episode: 72, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.390, 10.358], loss: 0.000259, mae: 0.015603, mean_q: 1.376482
  7373/100000: episode: 73, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.669, mean reward: 0.007 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.593, 10.240], loss: 0.000243, mae: 0.015648, mean_q: 1.388003
  7474/100000: episode: 74, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.450, 10.100], loss: 0.000196, mae: 0.014501, mean_q: 1.388300
  7575/100000: episode: 75, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.571, 10.100], loss: 0.000217, mae: 0.014786, mean_q: 1.375260
  7676/100000: episode: 76, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.159, 10.100], loss: 0.000242, mae: 0.015874, mean_q: 1.380862
  7777/100000: episode: 77, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.822, 10.114], loss: 0.000243, mae: 0.015286, mean_q: 1.399749
  7878/100000: episode: 78, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.097, 10.202], loss: 0.000240, mae: 0.015335, mean_q: 1.392472
  7979/100000: episode: 79, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.620, 10.100], loss: 0.000296, mae: 0.016078, mean_q: 1.391517
  8080/100000: episode: 80, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.100], loss: 0.000297, mae: 0.016364, mean_q: 1.394102
  8181/100000: episode: 81, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.937, 10.378], loss: 0.000328, mae: 0.016905, mean_q: 1.389591
  8282/100000: episode: 82, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.859, 10.299], loss: 0.000278, mae: 0.016269, mean_q: 1.399220
  8383/100000: episode: 83, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.844, 10.288], loss: 0.000291, mae: 0.016644, mean_q: 1.419823
  8484/100000: episode: 84, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.460, 10.212], loss: 0.000286, mae: 0.015701, mean_q: 1.395428
  8585/100000: episode: 85, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.571, 10.262], loss: 0.000331, mae: 0.017275, mean_q: 1.406485
  8686/100000: episode: 86, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.057, 10.184], loss: 0.000227, mae: 0.015454, mean_q: 1.404798
  8787/100000: episode: 87, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.354, 10.141], loss: 0.000286, mae: 0.016482, mean_q: 1.377840
  8888/100000: episode: 88, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.438, 10.100], loss: 0.000259, mae: 0.015911, mean_q: 1.385830
  8989/100000: episode: 89, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.621, 10.121], loss: 0.000343, mae: 0.018236, mean_q: 1.395988
  9090/100000: episode: 90, duration: 0.539s, episode steps: 101, steps per second: 188, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.100], loss: 0.000248, mae: 0.015743, mean_q: 1.401677
  9191/100000: episode: 91, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.216], loss: 0.000272, mae: 0.016840, mean_q: 1.367375
  9292/100000: episode: 92, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.133, 10.341], loss: 0.000251, mae: 0.016398, mean_q: 1.393077
  9393/100000: episode: 93, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.233, 10.131], loss: 0.000210, mae: 0.014609, mean_q: 1.393903
  9494/100000: episode: 94, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.143], loss: 0.000198, mae: 0.014021, mean_q: 1.388875
  9595/100000: episode: 95, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.866, mean reward: 0.009 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.273, 10.100], loss: 0.000197, mae: 0.014659, mean_q: 1.364468
  9696/100000: episode: 96, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.776, 10.507], loss: 0.000210, mae: 0.015148, mean_q: 1.378138
  9797/100000: episode: 97, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.910, 10.100], loss: 0.000226, mae: 0.015811, mean_q: 1.371957
  9898/100000: episode: 98, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.273, 10.371], loss: 0.000228, mae: 0.015673, mean_q: 1.359607
  9999/100000: episode: 99, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.688, 10.100], loss: 0.000238, mae: 0.015981, mean_q: 1.355300
 10100/100000: episode: 100, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.533, 10.100], loss: 0.000249, mae: 0.016096, mean_q: 1.360893
 10201/100000: episode: 101, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.710, 10.176], loss: 0.000233, mae: 0.016152, mean_q: 1.346579
 10302/100000: episode: 102, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.466, 10.172], loss: 0.000185, mae: 0.013968, mean_q: 1.362588
 10403/100000: episode: 103, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.311], loss: 0.000192, mae: 0.014148, mean_q: 1.332976
 10504/100000: episode: 104, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.072, 10.100], loss: 0.000222, mae: 0.015159, mean_q: 1.326630
 10605/100000: episode: 105, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.513, 10.265], loss: 0.000204, mae: 0.014379, mean_q: 1.309377
 10706/100000: episode: 106, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.576, 10.166], loss: 0.000191, mae: 0.014382, mean_q: 1.330114
 10807/100000: episode: 107, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.585, 10.172], loss: 0.000186, mae: 0.013762, mean_q: 1.311137
 10908/100000: episode: 108, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.877, 10.100], loss: 0.000203, mae: 0.014244, mean_q: 1.306897
 11009/100000: episode: 109, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.824, 10.107], loss: 0.000203, mae: 0.014580, mean_q: 1.314350
 11110/100000: episode: 110, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.628, 10.100], loss: 0.000275, mae: 0.016585, mean_q: 1.317670
 11211/100000: episode: 111, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.080, 10.100], loss: 0.000199, mae: 0.014003, mean_q: 1.265209
 11312/100000: episode: 112, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.516, 10.100], loss: 0.000191, mae: 0.014043, mean_q: 1.259931
 11413/100000: episode: 113, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.416, 10.298], loss: 0.000233, mae: 0.015790, mean_q: 1.263528
 11514/100000: episode: 114, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.153], loss: 0.000203, mae: 0.013800, mean_q: 1.246392
 11615/100000: episode: 115, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.132, 10.236], loss: 0.000195, mae: 0.014029, mean_q: 1.237483
 11716/100000: episode: 116, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.560, 10.100], loss: 0.000186, mae: 0.013438, mean_q: 1.235668
 11817/100000: episode: 117, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.473, 10.100], loss: 0.000169, mae: 0.012955, mean_q: 1.217991
 11918/100000: episode: 118, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.571, 10.298], loss: 0.000222, mae: 0.015349, mean_q: 1.210253
 12019/100000: episode: 119, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.078, 10.100], loss: 0.000260, mae: 0.016119, mean_q: 1.214589
 12120/100000: episode: 120, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.416, 10.100], loss: 0.000227, mae: 0.015297, mean_q: 1.192272
 12221/100000: episode: 121, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.111, 10.281], loss: 0.000186, mae: 0.014024, mean_q: 1.177505
 12322/100000: episode: 122, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.210], loss: 0.000147, mae: 0.012002, mean_q: 1.156524
 12423/100000: episode: 123, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.342, 10.100], loss: 0.000175, mae: 0.013660, mean_q: 1.140922
 12524/100000: episode: 124, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.982, 10.100], loss: 0.000162, mae: 0.012763, mean_q: 1.146753
 12625/100000: episode: 125, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.813, 10.267], loss: 0.000156, mae: 0.012455, mean_q: 1.135126
 12726/100000: episode: 126, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.376, 10.100], loss: 0.000133, mae: 0.011231, mean_q: 1.111845
 12827/100000: episode: 127, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.159, 10.179], loss: 0.000133, mae: 0.011605, mean_q: 1.115461
 12928/100000: episode: 128, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.468, 10.392], loss: 0.000142, mae: 0.011904, mean_q: 1.093153
 13029/100000: episode: 129, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.975, 10.100], loss: 0.000130, mae: 0.011147, mean_q: 1.076405
 13130/100000: episode: 130, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.874, 10.103], loss: 0.000138, mae: 0.011896, mean_q: 1.054581
 13231/100000: episode: 131, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.389, 10.178], loss: 0.000138, mae: 0.011492, mean_q: 1.050371
 13332/100000: episode: 132, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.954, 10.173], loss: 0.000121, mae: 0.010749, mean_q: 1.042144
 13433/100000: episode: 133, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.736, 10.100], loss: 0.000118, mae: 0.010643, mean_q: 1.028477
 13534/100000: episode: 134, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.877, 10.447], loss: 0.000125, mae: 0.010597, mean_q: 1.016954
 13635/100000: episode: 135, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.601, 10.113], loss: 0.000118, mae: 0.010183, mean_q: 0.999287
 13736/100000: episode: 136, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.207], loss: 0.000132, mae: 0.011066, mean_q: 0.999715
 13837/100000: episode: 137, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.161, 10.154], loss: 0.000105, mae: 0.009772, mean_q: 0.966706
 13938/100000: episode: 138, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.221, 10.241], loss: 0.000089, mae: 0.008770, mean_q: 0.962804
 14039/100000: episode: 139, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.875, mean reward: 0.009 [0.000, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.064, 10.100], loss: 0.000111, mae: 0.010048, mean_q: 0.939294
 14140/100000: episode: 140, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.117, 10.100], loss: 0.000097, mae: 0.009166, mean_q: 0.924719
 14241/100000: episode: 141, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.129, 10.212], loss: 0.000105, mae: 0.009405, mean_q: 0.925337
 14342/100000: episode: 142, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.203, 10.135], loss: 0.000110, mae: 0.010398, mean_q: 0.896822
 14443/100000: episode: 143, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.946, 10.100], loss: 0.000099, mae: 0.009557, mean_q: 0.892142
 14544/100000: episode: 144, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.902, 10.212], loss: 0.000097, mae: 0.009360, mean_q: 0.887973
 14645/100000: episode: 145, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.929, 10.116], loss: 0.000084, mae: 0.008603, mean_q: 0.869584
 14746/100000: episode: 146, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.537, 10.348], loss: 0.000101, mae: 0.009499, mean_q: 0.854925
 14847/100000: episode: 147, duration: 0.629s, episode steps: 101, steps per second: 160, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.960, 10.140], loss: 0.000094, mae: 0.008605, mean_q: 0.838641
 14948/100000: episode: 148, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.313, 10.106], loss: 0.000071, mae: 0.008188, mean_q: 0.839520
 15049/100000: episode: 149, duration: 0.662s, episode steps: 101, steps per second: 153, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.591, 10.100], loss: 0.000088, mae: 0.008549, mean_q: 0.824013
 15150/100000: episode: 150, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: 0.664, mean reward: 0.007 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.189, 10.100], loss: 0.000069, mae: 0.007683, mean_q: 0.824124
 15251/100000: episode: 151, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.979, 10.100], loss: 0.000066, mae: 0.007963, mean_q: 0.808972
 15352/100000: episode: 152, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.016, 10.457], loss: 0.000064, mae: 0.007620, mean_q: 0.796428
 15453/100000: episode: 153, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.019, 10.109], loss: 0.000075, mae: 0.007803, mean_q: 0.785923
 15554/100000: episode: 154, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.800, 10.194], loss: 0.000058, mae: 0.007177, mean_q: 0.781677
 15655/100000: episode: 155, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.058, 10.219], loss: 0.000075, mae: 0.007761, mean_q: 0.772300
 15756/100000: episode: 156, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.789, 10.207], loss: 0.000066, mae: 0.007439, mean_q: 0.766759
 15857/100000: episode: 157, duration: 0.673s, episode steps: 101, steps per second: 150, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.925, 10.100], loss: 0.000053, mae: 0.006440, mean_q: 0.758925
 15958/100000: episode: 158, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.403, 10.211], loss: 0.000054, mae: 0.006930, mean_q: 0.755981
 16059/100000: episode: 159, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.877, mean reward: 0.009 [0.000, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.409, 10.370], loss: 0.000045, mae: 0.006138, mean_q: 0.752450
 16160/100000: episode: 160, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.578, 10.278], loss: 0.000054, mae: 0.007009, mean_q: 0.747713
 16261/100000: episode: 161, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.417, 10.378], loss: 0.000059, mae: 0.006817, mean_q: 0.744903
 16362/100000: episode: 162, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.975, 10.100], loss: 0.000040, mae: 0.005833, mean_q: 0.743756
 16463/100000: episode: 163, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.953, 10.321], loss: 0.000039, mae: 0.005938, mean_q: 0.743050
 16564/100000: episode: 164, duration: 0.631s, episode steps: 101, steps per second: 160, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.100], loss: 0.000052, mae: 0.006446, mean_q: 0.741420
 16665/100000: episode: 165, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.075, 10.100], loss: 0.000058, mae: 0.007172, mean_q: 0.741038
 16766/100000: episode: 166, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.602, 10.104], loss: 0.000037, mae: 0.005336, mean_q: 0.740169
 16867/100000: episode: 167, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.765, 10.100], loss: 0.000035, mae: 0.005612, mean_q: 0.741633
 16968/100000: episode: 168, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.863, 10.309], loss: 0.000043, mae: 0.005780, mean_q: 0.741534
 17069/100000: episode: 169, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.352, 10.100], loss: 0.000037, mae: 0.005769, mean_q: 0.743154
 17170/100000: episode: 170, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.263, 10.493], loss: 0.000030, mae: 0.005276, mean_q: 0.743356
 17271/100000: episode: 171, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.387, 10.407], loss: 0.000031, mae: 0.005513, mean_q: 0.744628
 17372/100000: episode: 172, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.217, 10.128], loss: 0.000030, mae: 0.004894, mean_q: 0.744558
 17473/100000: episode: 173, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.653, mean reward: 0.006 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.030, 10.257], loss: 0.000025, mae: 0.004545, mean_q: 0.746627
 17574/100000: episode: 174, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.538, 10.258], loss: 0.000034, mae: 0.005679, mean_q: 0.747002
 17675/100000: episode: 175, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.992, 10.374], loss: 0.000029, mae: 0.004937, mean_q: 0.747437
 17776/100000: episode: 176, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.618, 10.314], loss: 0.000040, mae: 0.005378, mean_q: 0.748907
 17877/100000: episode: 177, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.025, 10.100], loss: 0.000026, mae: 0.004866, mean_q: 0.749678
 17978/100000: episode: 178, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.098, 10.229], loss: 0.000066, mae: 0.007890, mean_q: 0.749298
 18079/100000: episode: 179, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.806, 10.100], loss: 0.000032, mae: 0.004906, mean_q: 0.750484
 18180/100000: episode: 180, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.036, 10.188], loss: 0.000046, mae: 0.006307, mean_q: 0.749468
 18281/100000: episode: 181, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.554, 10.100], loss: 0.000030, mae: 0.004694, mean_q: 0.750729
 18382/100000: episode: 182, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.678, 10.336], loss: 0.000032, mae: 0.005057, mean_q: 0.750348
 18483/100000: episode: 183, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.935, 10.100], loss: 0.000029, mae: 0.004956, mean_q: 0.750732
 18584/100000: episode: 184, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.733, 10.100], loss: 0.000041, mae: 0.005827, mean_q: 0.751195
 18685/100000: episode: 185, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.888, 10.100], loss: 0.000038, mae: 0.005952, mean_q: 0.751772
 18786/100000: episode: 186, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.525, 10.360], loss: 0.000034, mae: 0.005344, mean_q: 0.751572
 18887/100000: episode: 187, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.322, 10.100], loss: 0.000029, mae: 0.004871, mean_q: 0.751228
 18988/100000: episode: 188, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.344, 10.201], loss: 0.000037, mae: 0.005381, mean_q: 0.751253
 19089/100000: episode: 189, duration: 0.734s, episode steps: 101, steps per second: 138, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.637, 10.143], loss: 0.000035, mae: 0.004859, mean_q: 0.751363
 19190/100000: episode: 190, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.144, 10.100], loss: 0.000034, mae: 0.005397, mean_q: 0.751231
 19291/100000: episode: 191, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.430, 10.255], loss: 0.000032, mae: 0.004797, mean_q: 0.750887
 19392/100000: episode: 192, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.822, 10.210], loss: 0.000032, mae: 0.005040, mean_q: 0.751334
 19493/100000: episode: 193, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.827, 10.100], loss: 0.000029, mae: 0.004700, mean_q: 0.751402
 19594/100000: episode: 194, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.789, 10.100], loss: 0.000030, mae: 0.004748, mean_q: 0.749837
 19695/100000: episode: 195, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.596, 10.266], loss: 0.000034, mae: 0.005463, mean_q: 0.749704
 19796/100000: episode: 196, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.645, 10.264], loss: 0.000039, mae: 0.005373, mean_q: 0.749246
 19897/100000: episode: 197, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.802, 10.100], loss: 0.000023, mae: 0.004515, mean_q: 0.749167
 19998/100000: episode: 198, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.868, 10.100], loss: 0.000028, mae: 0.004647, mean_q: 0.748264
 20099/100000: episode: 199, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.915, 10.100], loss: 0.000022, mae: 0.004266, mean_q: 0.747871
[Info] 1-TH LEVEL FOUND: 0.7928426861763, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.478s, episode steps: 101, steps per second: 18, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.340, 10.100], loss: 0.000033, mae: 0.004728, mean_q: 0.747243
 20298/100000: episode: 201, duration: 0.576s, episode steps: 98, steps per second: 170, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.441, 10.100], loss: 0.000032, mae: 0.005311, mean_q: 0.747375
 20398/100000: episode: 202, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.529, 10.100], loss: 0.000029, mae: 0.004498, mean_q: 0.747314
 20498/100000: episode: 203, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.974, 10.193], loss: 0.000034, mae: 0.004984, mean_q: 0.747248
 20597/100000: episode: 204, duration: 0.607s, episode steps: 99, steps per second: 163, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.003, 10.100], loss: 0.000031, mae: 0.004989, mean_q: 0.747078
 20696/100000: episode: 205, duration: 0.579s, episode steps: 99, steps per second: 171, episode reward: 0.744, mean reward: 0.008 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.659, 10.140], loss: 0.000040, mae: 0.005774, mean_q: 0.746618
 20794/100000: episode: 206, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.647, 10.105], loss: 0.000031, mae: 0.004928, mean_q: 0.746320
 20893/100000: episode: 207, duration: 0.538s, episode steps: 99, steps per second: 184, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.043, 10.100], loss: 0.000036, mae: 0.005342, mean_q: 0.746647
 20992/100000: episode: 208, duration: 0.550s, episode steps: 99, steps per second: 180, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.984, 10.271], loss: 0.000027, mae: 0.004560, mean_q: 0.746443
 21090/100000: episode: 209, duration: 0.544s, episode steps: 98, steps per second: 180, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.591, 10.170], loss: 0.000054, mae: 0.006878, mean_q: 0.746469
 21188/100000: episode: 210, duration: 0.527s, episode steps: 98, steps per second: 186, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.136, 10.177], loss: 0.000030, mae: 0.004247, mean_q: 0.746373
 21288/100000: episode: 211, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.104, 10.100], loss: 0.000029, mae: 0.004640, mean_q: 0.746383
 21387/100000: episode: 212, duration: 0.525s, episode steps: 99, steps per second: 188, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.609, 10.100], loss: 0.000024, mae: 0.004217, mean_q: 0.746704
 21485/100000: episode: 213, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 0.736, mean reward: 0.008 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.234, 10.166], loss: 0.000027, mae: 0.004649, mean_q: 0.746907
 21584/100000: episode: 214, duration: 0.567s, episode steps: 99, steps per second: 175, episode reward: 0.752, mean reward: 0.008 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.560, 10.100], loss: 0.000022, mae: 0.004222, mean_q: 0.747311
 21682/100000: episode: 215, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.780, 10.135], loss: 0.000026, mae: 0.004474, mean_q: 0.747544
 21782/100000: episode: 216, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.005, 10.333], loss: 0.000034, mae: 0.004829, mean_q: 0.747810
 21880/100000: episode: 217, duration: 0.539s, episode steps: 98, steps per second: 182, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-1.018, 10.275], loss: 0.000026, mae: 0.004328, mean_q: 0.747771
 21978/100000: episode: 218, duration: 0.544s, episode steps: 98, steps per second: 180, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.909, 10.195], loss: 0.000024, mae: 0.004191, mean_q: 0.748454
 22077/100000: episode: 219, duration: 0.533s, episode steps: 99, steps per second: 186, episode reward: 0.881, mean reward: 0.009 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.793, 10.401], loss: 0.000023, mae: 0.003841, mean_q: 0.748749
 22177/100000: episode: 220, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.800, 10.173], loss: 0.000016, mae: 0.003853, mean_q: 0.748445
 22277/100000: episode: 221, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.357, 10.181], loss: 0.000027, mae: 0.004506, mean_q: 0.749116
 22376/100000: episode: 222, duration: 0.555s, episode steps: 99, steps per second: 178, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.800, 10.311], loss: 0.000027, mae: 0.004090, mean_q: 0.750104
 22475/100000: episode: 223, duration: 0.549s, episode steps: 99, steps per second: 180, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.345, 10.350], loss: 0.000033, mae: 0.004797, mean_q: 0.749955
 22575/100000: episode: 224, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 0.753, mean reward: 0.008 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.779, 10.174], loss: 0.000019, mae: 0.003646, mean_q: 0.749983
 22673/100000: episode: 225, duration: 0.548s, episode steps: 98, steps per second: 179, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.586, 10.283], loss: 0.000023, mae: 0.003831, mean_q: 0.750852
 22773/100000: episode: 226, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.325, 10.272], loss: 0.000029, mae: 0.004767, mean_q: 0.750593
 22873/100000: episode: 227, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.800, 10.204], loss: 0.000018, mae: 0.003804, mean_q: 0.751096
 22973/100000: episode: 228, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.470, 10.100], loss: 0.000023, mae: 0.004128, mean_q: 0.751839
 23072/100000: episode: 229, duration: 0.578s, episode steps: 99, steps per second: 171, episode reward: 0.756, mean reward: 0.008 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.447, 10.363], loss: 0.000028, mae: 0.004642, mean_q: 0.752049
 23171/100000: episode: 230, duration: 0.550s, episode steps: 99, steps per second: 180, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.708, 10.100], loss: 0.000015, mae: 0.003441, mean_q: 0.751849
 23271/100000: episode: 231, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.537, 10.100], loss: 0.000021, mae: 0.003803, mean_q: 0.752744
 23370/100000: episode: 232, duration: 0.551s, episode steps: 99, steps per second: 180, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.601, 10.100], loss: 0.000019, mae: 0.003730, mean_q: 0.752779
 23468/100000: episode: 233, duration: 0.565s, episode steps: 98, steps per second: 173, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-2.316, 10.100], loss: 0.000035, mae: 0.005669, mean_q: 0.753180
 23568/100000: episode: 234, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.525, 10.386], loss: 0.000029, mae: 0.004892, mean_q: 0.753360
 23667/100000: episode: 235, duration: 0.579s, episode steps: 99, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-1.152, 10.224], loss: 0.000023, mae: 0.004031, mean_q: 0.753659
 23766/100000: episode: 236, duration: 0.533s, episode steps: 99, steps per second: 186, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.567, 10.100], loss: 0.000022, mae: 0.003900, mean_q: 0.753766
 23865/100000: episode: 237, duration: 0.541s, episode steps: 99, steps per second: 183, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.866, 10.100], loss: 0.000016, mae: 0.003048, mean_q: 0.754507
 23963/100000: episode: 238, duration: 0.533s, episode steps: 98, steps per second: 184, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.967, 10.100], loss: 0.000018, mae: 0.003704, mean_q: 0.754465
 24061/100000: episode: 239, duration: 0.556s, episode steps: 98, steps per second: 176, episode reward: 0.755, mean reward: 0.008 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.827, 10.100], loss: 0.000026, mae: 0.004085, mean_q: 0.755062
 24159/100000: episode: 240, duration: 0.561s, episode steps: 98, steps per second: 175, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.534, 10.100], loss: 0.000016, mae: 0.003473, mean_q: 0.754803
 24259/100000: episode: 241, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.509, 10.184], loss: 0.000028, mae: 0.004880, mean_q: 0.755394
 24359/100000: episode: 242, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.950, 10.135], loss: 0.000018, mae: 0.003668, mean_q: 0.755562
 24459/100000: episode: 243, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.727, 10.361], loss: 0.000010, mae: 0.003013, mean_q: 0.756374
 24559/100000: episode: 244, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.390, 10.247], loss: 0.000021, mae: 0.003942, mean_q: 0.755949
 24658/100000: episode: 245, duration: 0.585s, episode steps: 99, steps per second: 169, episode reward: 0.858, mean reward: 0.009 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-1.466, 10.344], loss: 0.000020, mae: 0.003664, mean_q: 0.757587
 24757/100000: episode: 246, duration: 0.546s, episode steps: 99, steps per second: 181, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.619, 10.181], loss: 0.000017, mae: 0.003087, mean_q: 0.757468
 24856/100000: episode: 247, duration: 0.569s, episode steps: 99, steps per second: 174, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.352, 10.100], loss: 0.000021, mae: 0.003990, mean_q: 0.757658
 24956/100000: episode: 248, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.175, 10.269], loss: 0.000013, mae: 0.002917, mean_q: 0.758525
 25055/100000: episode: 249, duration: 0.548s, episode steps: 99, steps per second: 181, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.772, 10.403], loss: 0.000021, mae: 0.004055, mean_q: 0.758141
 25153/100000: episode: 250, duration: 0.565s, episode steps: 98, steps per second: 173, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.904, 10.107], loss: 0.000021, mae: 0.003997, mean_q: 0.758634
 25251/100000: episode: 251, duration: 0.571s, episode steps: 98, steps per second: 172, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.883, 10.207], loss: 0.000024, mae: 0.003773, mean_q: 0.759145
 25351/100000: episode: 252, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.535, 10.103], loss: 0.000023, mae: 0.003559, mean_q: 0.758777
 25451/100000: episode: 253, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.859, 10.100], loss: 0.000015, mae: 0.003061, mean_q: 0.759342
 25551/100000: episode: 254, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.018, 10.100], loss: 0.000026, mae: 0.003929, mean_q: 0.758663
 25651/100000: episode: 255, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 0.751, mean reward: 0.008 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.666, 10.154], loss: 0.000019, mae: 0.003663, mean_q: 0.758388
 25751/100000: episode: 256, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.049, 10.100], loss: 0.000019, mae: 0.003176, mean_q: 0.759452
 25849/100000: episode: 257, duration: 0.532s, episode steps: 98, steps per second: 184, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.814, 10.122], loss: 0.000025, mae: 0.004351, mean_q: 0.759100
 25949/100000: episode: 258, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.211, 10.201], loss: 0.000031, mae: 0.004465, mean_q: 0.759178
 26049/100000: episode: 259, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.861, 10.100], loss: 0.000016, mae: 0.003603, mean_q: 0.759191
 26148/100000: episode: 260, duration: 0.530s, episode steps: 99, steps per second: 187, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-1.247, 10.393], loss: 0.000018, mae: 0.003166, mean_q: 0.759279
 26247/100000: episode: 261, duration: 0.575s, episode steps: 99, steps per second: 172, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.367, 10.100], loss: 0.000010, mae: 0.002876, mean_q: 0.758405
 26347/100000: episode: 262, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.789, 10.424], loss: 0.000019, mae: 0.003199, mean_q: 0.758521
 26447/100000: episode: 263, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.261, 10.100], loss: 0.000014, mae: 0.003188, mean_q: 0.759210
 26547/100000: episode: 264, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.281, 10.100], loss: 0.000014, mae: 0.002960, mean_q: 0.759159
 26646/100000: episode: 265, duration: 0.612s, episode steps: 99, steps per second: 162, episode reward: 0.969, mean reward: 0.010 [0.000, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.032, 10.162], loss: 0.000036, mae: 0.005077, mean_q: 0.758808
 26745/100000: episode: 266, duration: 0.564s, episode steps: 99, steps per second: 176, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.344, 10.100], loss: 0.000017, mae: 0.003774, mean_q: 0.758790
 26845/100000: episode: 267, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.448, 10.117], loss: 0.000022, mae: 0.003471, mean_q: 0.758120
 26945/100000: episode: 268, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.716, 10.248], loss: 0.000050, mae: 0.004819, mean_q: 0.759442
 27045/100000: episode: 269, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.332, 10.255], loss: 0.000049, mae: 0.005463, mean_q: 0.759602
 27143/100000: episode: 270, duration: 0.579s, episode steps: 98, steps per second: 169, episode reward: 0.755, mean reward: 0.008 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.676, 10.160], loss: 0.000023, mae: 0.003558, mean_q: 0.759524
 27241/100000: episode: 271, duration: 0.540s, episode steps: 98, steps per second: 181, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.156, 10.125], loss: 0.000037, mae: 0.004452, mean_q: 0.759927
 27341/100000: episode: 272, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 0.756, mean reward: 0.008 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.980, 10.373], loss: 0.000035, mae: 0.003452, mean_q: 0.760318
 27441/100000: episode: 273, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.627, 10.298], loss: 0.000037, mae: 0.004685, mean_q: 0.760273
 27541/100000: episode: 274, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.182, 10.100], loss: 0.000023, mae: 0.003233, mean_q: 0.760556
 27640/100000: episode: 275, duration: 0.539s, episode steps: 99, steps per second: 184, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.605, 10.100], loss: 0.000013, mae: 0.002758, mean_q: 0.760217
 27740/100000: episode: 276, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.389, 10.129], loss: 0.000037, mae: 0.003974, mean_q: 0.760723
 27840/100000: episode: 277, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.559, 10.264], loss: 0.000015, mae: 0.002930, mean_q: 0.760652
 27938/100000: episode: 278, duration: 0.570s, episode steps: 98, steps per second: 172, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.870, 10.181], loss: 0.000026, mae: 0.003529, mean_q: 0.761325
 28037/100000: episode: 279, duration: 0.556s, episode steps: 99, steps per second: 178, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.404, 10.100], loss: 0.000020, mae: 0.003230, mean_q: 0.761113
 28135/100000: episode: 280, duration: 0.527s, episode steps: 98, steps per second: 186, episode reward: 0.742, mean reward: 0.008 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.885, 10.100], loss: 0.000017, mae: 0.003003, mean_q: 0.760997
 28233/100000: episode: 281, duration: 0.580s, episode steps: 98, steps per second: 169, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.209, 10.100], loss: 0.000014, mae: 0.002576, mean_q: 0.761579
 28331/100000: episode: 282, duration: 0.536s, episode steps: 98, steps per second: 183, episode reward: 0.669, mean reward: 0.007 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.052, 10.100], loss: 0.000014, mae: 0.002520, mean_q: 0.761044
 28429/100000: episode: 283, duration: 0.564s, episode steps: 98, steps per second: 174, episode reward: 0.741, mean reward: 0.008 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.491, 10.165], loss: 0.000025, mae: 0.004365, mean_q: 0.761438
 28527/100000: episode: 284, duration: 0.513s, episode steps: 98, steps per second: 191, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.876, 10.353], loss: 0.000047, mae: 0.004714, mean_q: 0.762012
 28625/100000: episode: 285, duration: 0.550s, episode steps: 98, steps per second: 178, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.593, 10.100], loss: 0.000024, mae: 0.003857, mean_q: 0.761855
 28723/100000: episode: 286, duration: 0.590s, episode steps: 98, steps per second: 166, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.966, 10.100], loss: 0.000020, mae: 0.003042, mean_q: 0.761903
 28823/100000: episode: 287, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.005, 10.200], loss: 0.000023, mae: 0.003659, mean_q: 0.761766
 28923/100000: episode: 288, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.647, 10.100], loss: 0.000016, mae: 0.003126, mean_q: 0.761301
 29022/100000: episode: 289, duration: 0.567s, episode steps: 99, steps per second: 175, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.459, 10.422], loss: 0.000031, mae: 0.003804, mean_q: 0.761550
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7928426861763
1
 29122/100000: episode: 290, duration: 4.776s, episode steps: 100, steps per second: 21, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.507, 10.100], loss: 0.000029, mae: 0.003475, mean_q: 0.761749
 29223/100000: episode: 291, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.584, 10.238], loss: 0.000017, mae: 0.002709, mean_q: 0.761571
 29324/100000: episode: 292, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.798, 10.100], loss: 0.000021, mae: 0.002895, mean_q: 0.761813
 29425/100000: episode: 293, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.729, 10.190], loss: 0.000023, mae: 0.002996, mean_q: 0.762124
 29526/100000: episode: 294, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.385, 10.173], loss: 0.000023, mae: 0.003757, mean_q: 0.761618
 29627/100000: episode: 295, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.630, 10.412], loss: 0.000034, mae: 0.003930, mean_q: 0.761383
 29728/100000: episode: 296, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.827, 10.100], loss: 0.000022, mae: 0.003534, mean_q: 0.761492
 29829/100000: episode: 297, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.684, 10.100], loss: 0.000015, mae: 0.002534, mean_q: 0.761481
 29930/100000: episode: 298, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.623, 10.255], loss: 0.000016, mae: 0.003353, mean_q: 0.761326
 30031/100000: episode: 299, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.015, 10.100], loss: 0.000018, mae: 0.003272, mean_q: 0.760715
 30132/100000: episode: 300, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.248, 10.100], loss: 0.000021, mae: 0.003302, mean_q: 0.760967
 30233/100000: episode: 301, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.986, 10.190], loss: 0.000015, mae: 0.003141, mean_q: 0.760502
 30334/100000: episode: 302, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.688, 10.100], loss: 0.000016, mae: 0.002908, mean_q: 0.760768
 30435/100000: episode: 303, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.554, 10.139], loss: 0.000026, mae: 0.003509, mean_q: 0.760889
 30536/100000: episode: 304, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.837, 10.270], loss: 0.000024, mae: 0.003747, mean_q: 0.760588
 30637/100000: episode: 305, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.705, 10.100], loss: 0.000024, mae: 0.003377, mean_q: 0.760058
 30738/100000: episode: 306, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.809, 10.106], loss: 0.000018, mae: 0.003414, mean_q: 0.759827
 30839/100000: episode: 307, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.574, 10.100], loss: 0.000011, mae: 0.002565, mean_q: 0.760143
 30940/100000: episode: 308, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.456, 10.180], loss: 0.000019, mae: 0.002670, mean_q: 0.760219
 31041/100000: episode: 309, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.100], loss: 0.000021, mae: 0.003060, mean_q: 0.759748
 31142/100000: episode: 310, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.798, 10.100], loss: 0.000028, mae: 0.003354, mean_q: 0.759473
 31243/100000: episode: 311, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.731, 10.100], loss: 0.000036, mae: 0.003845, mean_q: 0.759617
 31344/100000: episode: 312, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.144, 10.136], loss: 0.000013, mae: 0.002723, mean_q: 0.759667
 31445/100000: episode: 313, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.556, 10.100], loss: 0.000016, mae: 0.003450, mean_q: 0.759637
 31546/100000: episode: 314, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.005, 10.100], loss: 0.000022, mae: 0.002885, mean_q: 0.759422
 31647/100000: episode: 315, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.571, 10.302], loss: 0.000029, mae: 0.003962, mean_q: 0.758985
 31748/100000: episode: 316, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.994, 10.230], loss: 0.000018, mae: 0.002962, mean_q: 0.758967
 31849/100000: episode: 317, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.592, 10.100], loss: 0.000016, mae: 0.002978, mean_q: 0.758761
 31950/100000: episode: 318, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.921, 10.149], loss: 0.000022, mae: 0.003121, mean_q: 0.759009
 32051/100000: episode: 319, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.312, 10.211], loss: 0.000015, mae: 0.002808, mean_q: 0.758589
 32152/100000: episode: 320, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.682, 10.100], loss: 0.000021, mae: 0.003520, mean_q: 0.758585
 32253/100000: episode: 321, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.804, 10.100], loss: 0.000020, mae: 0.002621, mean_q: 0.758089
 32354/100000: episode: 322, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.958, 10.100], loss: 0.000015, mae: 0.002689, mean_q: 0.757882
 32455/100000: episode: 323, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.035, 10.212], loss: 0.000015, mae: 0.002925, mean_q: 0.757514
 32556/100000: episode: 324, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.787, 10.176], loss: 0.000013, mae: 0.002658, mean_q: 0.757186
 32657/100000: episode: 325, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.019, 10.100], loss: 0.000015, mae: 0.002695, mean_q: 0.757189
 32758/100000: episode: 326, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.448, 10.100], loss: 0.000019, mae: 0.003128, mean_q: 0.756861
 32859/100000: episode: 327, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.510, 10.435], loss: 0.000023, mae: 0.003575, mean_q: 0.756336
 32960/100000: episode: 328, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.459, 10.100], loss: 0.000017, mae: 0.003028, mean_q: 0.755974
 33061/100000: episode: 329, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.420, 10.126], loss: 0.000010, mae: 0.002555, mean_q: 0.755991
 33162/100000: episode: 330, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.700, 10.243], loss: 0.000017, mae: 0.002725, mean_q: 0.755665
 33263/100000: episode: 331, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.361, 10.223], loss: 0.000023, mae: 0.003342, mean_q: 0.755276
 33364/100000: episode: 332, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.466, 10.100], loss: 0.000014, mae: 0.002598, mean_q: 0.754858
 33465/100000: episode: 333, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.454, 10.100], loss: 0.000016, mae: 0.002568, mean_q: 0.754835
 33566/100000: episode: 334, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.141, 10.100], loss: 0.000017, mae: 0.002600, mean_q: 0.754775
 33667/100000: episode: 335, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.333, 10.265], loss: 0.000014, mae: 0.003031, mean_q: 0.754269
 33768/100000: episode: 336, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.843, 10.232], loss: 0.000015, mae: 0.002719, mean_q: 0.754606
 33869/100000: episode: 337, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.618, 10.100], loss: 0.000015, mae: 0.002671, mean_q: 0.754305
 33970/100000: episode: 338, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.827, 10.191], loss: 0.000018, mae: 0.003186, mean_q: 0.754025
 34071/100000: episode: 339, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.625, 10.100], loss: 0.000014, mae: 0.002732, mean_q: 0.754137
 34172/100000: episode: 340, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.197, 10.262], loss: 0.000017, mae: 0.003070, mean_q: 0.753918
 34273/100000: episode: 341, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.440, 10.100], loss: 0.000012, mae: 0.002427, mean_q: 0.753951
 34374/100000: episode: 342, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.528, 10.223], loss: 0.000011, mae: 0.002505, mean_q: 0.753389
 34475/100000: episode: 343, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.240, 10.171], loss: 0.000015, mae: 0.002561, mean_q: 0.752849
 34576/100000: episode: 344, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.373, 10.100], loss: 0.000014, mae: 0.002643, mean_q: 0.753108
 34677/100000: episode: 345, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.093, 10.193], loss: 0.000009, mae: 0.002311, mean_q: 0.752781
 34778/100000: episode: 346, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.607, 10.100], loss: 0.000016, mae: 0.002851, mean_q: 0.752649
 34879/100000: episode: 347, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.100], loss: 0.000019, mae: 0.003087, mean_q: 0.752465
 34980/100000: episode: 348, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.724, 10.156], loss: 0.000028, mae: 0.004039, mean_q: 0.752385
 35081/100000: episode: 349, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.060, 10.144], loss: 0.000015, mae: 0.003156, mean_q: 0.752290
 35182/100000: episode: 350, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.192, 10.100], loss: 0.000014, mae: 0.002567, mean_q: 0.751944
 35283/100000: episode: 351, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.521, 10.100], loss: 0.000020, mae: 0.003274, mean_q: 0.751788
 35384/100000: episode: 352, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.021, 10.118], loss: 0.000012, mae: 0.002703, mean_q: 0.751607
 35485/100000: episode: 353, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.217, 10.405], loss: 0.000010, mae: 0.002112, mean_q: 0.751648
 35586/100000: episode: 354, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.698, 10.100], loss: 0.000010, mae: 0.002345, mean_q: 0.751100
 35687/100000: episode: 355, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.617, 10.399], loss: 0.000008, mae: 0.002087, mean_q: 0.751308
 35788/100000: episode: 356, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.643, 10.100], loss: 0.000014, mae: 0.002458, mean_q: 0.751177
 35889/100000: episode: 357, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.290, 10.185], loss: 0.000012, mae: 0.002448, mean_q: 0.750993
 35990/100000: episode: 358, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.100], loss: 0.000019, mae: 0.003161, mean_q: 0.750990
 36091/100000: episode: 359, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.372, 10.100], loss: 0.000014, mae: 0.002701, mean_q: 0.750882
 36192/100000: episode: 360, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.930, mean reward: 0.009 [0.000, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.225, 10.100], loss: 0.000015, mae: 0.002614, mean_q: 0.750953
 36293/100000: episode: 361, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.542], loss: 0.000011, mae: 0.002553, mean_q: 0.750958
 36394/100000: episode: 362, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.752, 10.287], loss: 0.000023, mae: 0.003346, mean_q: 0.751368
 36495/100000: episode: 363, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.682, 10.262], loss: 0.000017, mae: 0.003115, mean_q: 0.751100
 36596/100000: episode: 364, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.951, 10.100], loss: 0.000017, mae: 0.002950, mean_q: 0.750855
 36697/100000: episode: 365, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.796, 10.100], loss: 0.000015, mae: 0.002631, mean_q: 0.750987
 36798/100000: episode: 366, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.033, 10.100], loss: 0.000012, mae: 0.002297, mean_q: 0.751099
 36899/100000: episode: 367, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.022, 10.100], loss: 0.000022, mae: 0.002987, mean_q: 0.751186
 37000/100000: episode: 368, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.458, 10.225], loss: 0.000022, mae: 0.003459, mean_q: 0.751205
 37101/100000: episode: 369, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.788, 10.102], loss: 0.000016, mae: 0.002791, mean_q: 0.750873
 37202/100000: episode: 370, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.549, 10.136], loss: 0.000009, mae: 0.001971, mean_q: 0.751327
 37303/100000: episode: 371, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.889, mean reward: 0.009 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.868, 10.225], loss: 0.000021, mae: 0.003232, mean_q: 0.751486
 37404/100000: episode: 372, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.890, mean reward: 0.009 [0.000, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.509, 10.277], loss: 0.000009, mae: 0.002316, mean_q: 0.751104
 37505/100000: episode: 373, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.100], loss: 0.000018, mae: 0.002950, mean_q: 0.751167
 37606/100000: episode: 374, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.932, 10.173], loss: 0.000019, mae: 0.003405, mean_q: 0.751141
 37707/100000: episode: 375, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.330, 10.149], loss: 0.000019, mae: 0.002933, mean_q: 0.751525
 37808/100000: episode: 376, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.883, 10.100], loss: 0.000020, mae: 0.003116, mean_q: 0.751830
 37909/100000: episode: 377, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.611, 10.100], loss: 0.000013, mae: 0.002203, mean_q: 0.751531
 38010/100000: episode: 378, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.709, 10.100], loss: 0.000025, mae: 0.003296, mean_q: 0.751872
 38111/100000: episode: 379, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.811, 10.100], loss: 0.000026, mae: 0.003364, mean_q: 0.751752
 38212/100000: episode: 380, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.794, 10.123], loss: 0.000021, mae: 0.003337, mean_q: 0.751871
 38313/100000: episode: 381, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.388, 10.100], loss: 0.000022, mae: 0.003279, mean_q: 0.751692
 38414/100000: episode: 382, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.589, 10.100], loss: 0.000024, mae: 0.002845, mean_q: 0.752313
 38515/100000: episode: 383, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.915, 10.100], loss: 0.000018, mae: 0.002932, mean_q: 0.751877
 38616/100000: episode: 384, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.802, 10.100], loss: 0.000015, mae: 0.002881, mean_q: 0.752842
 38717/100000: episode: 385, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.906, 10.257], loss: 0.000033, mae: 0.003603, mean_q: 0.752771
 38818/100000: episode: 386, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.478, 10.241], loss: 0.000024, mae: 0.003083, mean_q: 0.753230
 38919/100000: episode: 387, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.934, 10.100], loss: 0.000023, mae: 0.002856, mean_q: 0.752942
 39020/100000: episode: 388, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.089, 10.100], loss: 0.000031, mae: 0.003470, mean_q: 0.752941
 39121/100000: episode: 389, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.169, 10.146], loss: 0.000032, mae: 0.003468, mean_q: 0.753430
[Info] 1-TH LEVEL FOUND: 0.7718771696090698, Considering 10/100 traces
 39222/100000: episode: 390, duration: 4.949s, episode steps: 101, steps per second: 20, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.339, 10.134], loss: 0.000021, mae: 0.002688, mean_q: 0.753121
 39269/100000: episode: 391, duration: 0.268s, episode steps: 47, steps per second: 175, episode reward: 0.753, mean reward: 0.016 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.145, 10.191], loss: 0.000025, mae: 0.003420, mean_q: 0.753810
 39270/100000: episode: 392, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.720, mean reward: 0.720 [0.720, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.070, 10.459], loss: 0.000003, mae: 0.001702, mean_q: 0.752999
 39273/100000: episode: 393, duration: 0.032s, episode steps: 3, steps per second: 95, episode reward: 0.778, mean reward: 0.259 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.513], loss: 0.000003, mae: 0.001700, mean_q: 0.752547
 39274/100000: episode: 394, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.538, mean reward: 0.538 [0.538, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.070, 10.200], loss: 0.000153, mae: 0.004470, mean_q: 0.754294
 39276/100000: episode: 395, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.655, mean reward: 0.328 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-2.390, 10.148], loss: 0.000005, mae: 0.002404, mean_q: 0.749043
 39277/100000: episode: 396, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.238], loss: 0.000004, mae: 0.002556, mean_q: 0.754401
 39278/100000: episode: 397, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.757, mean reward: 0.757 [0.757, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.070, 10.465], loss: 0.000004, mae: 0.002286, mean_q: 0.757789
 39279/100000: episode: 398, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.563, mean reward: 0.563 [0.563, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.070, 10.200], loss: 0.000005, mae: 0.002303, mean_q: 0.751060
 39281/100000: episode: 399, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.550, mean reward: 0.275 [0.000, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.137], loss: 0.000002, mae: 0.001487, mean_q: 0.753325
 39282/100000: episode: 400, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.538, mean reward: 0.538 [0.538, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.498, 10.200], loss: 0.000210, mae: 0.005164, mean_q: 0.754771
 39329/100000: episode: 401, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 0.832, mean reward: 0.018 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.966, 10.100], loss: 0.000068, mae: 0.005250, mean_q: 0.754104
 39332/100000: episode: 402, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 0.745, mean reward: 0.248 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.426], loss: 0.000010, mae: 0.003833, mean_q: 0.751326
 39334/100000: episode: 403, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.544, mean reward: 0.272 [0.000, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.184], loss: 0.000009, mae: 0.003026, mean_q: 0.755307
 39335/100000: episode: 404, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.779, mean reward: 0.779 [0.779, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.249], loss: 0.000044, mae: 0.008776, mean_q: 0.745417
 39396/100000: episode: 405, duration: 0.337s, episode steps: 61, steps per second: 181, episode reward: 0.864, mean reward: 0.014 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.783, 10.100], loss: 0.000065, mae: 0.006289, mean_q: 0.753822
 39397/100000: episode: 406, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.581, mean reward: 0.581 [0.581, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.235], loss: 0.000004, mae: 0.001899, mean_q: 0.752987
 39398/100000: episode: 407, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.706, mean reward: 0.706 [0.706, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.070, 10.443], loss: 0.000009, mae: 0.003732, mean_q: 0.756692
 39399/100000: episode: 408, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.610, mean reward: 0.610 [0.610, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.070, 10.284], loss: 0.000007, mae: 0.003266, mean_q: 0.754777
 39460/100000: episode: 409, duration: 0.354s, episode steps: 61, steps per second: 172, episode reward: 0.742, mean reward: 0.012 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.309, 10.100], loss: 0.000085, mae: 0.004510, mean_q: 0.753909
 39507/100000: episode: 410, duration: 0.268s, episode steps: 47, steps per second: 176, episode reward: 0.791, mean reward: 0.017 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.044, 10.100], loss: 0.000066, mae: 0.006394, mean_q: 0.753861
 39509/100000: episode: 411, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 0.602, mean reward: 0.301 [0.000, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.171], loss: 0.000097, mae: 0.006922, mean_q: 0.760277
 39510/100000: episode: 412, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.207], loss: 0.000049, mae: 0.008812, mean_q: 0.745442
 39511/100000: episode: 413, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.537, mean reward: 0.537 [0.537, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.070, 10.200], loss: 0.000132, mae: 0.007209, mean_q: 0.750270
 39512/100000: episode: 414, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.598, mean reward: 0.598 [0.598, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.631, 10.218], loss: 0.000011, mae: 0.003255, mean_q: 0.754568
 39513/100000: episode: 415, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.529, mean reward: 0.529 [0.529, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.200], loss: 0.000009, mae: 0.003880, mean_q: 0.756159
 39514/100000: episode: 416, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.653, mean reward: 0.653 [0.653, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.035, 10.277], loss: 0.000006, mae: 0.002163, mean_q: 0.753687
 39515/100000: episode: 417, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.621, mean reward: 0.621 [0.621, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.035, 10.264], loss: 0.000008, mae: 0.003275, mean_q: 0.751209
 39562/100000: episode: 418, duration: 0.271s, episode steps: 47, steps per second: 174, episode reward: 0.730, mean reward: 0.016 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.120, 10.219], loss: 0.000073, mae: 0.004265, mean_q: 0.753483
 39563/100000: episode: 419, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.594, mean reward: 0.594 [0.594, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.226], loss: 0.000294, mae: 0.009552, mean_q: 0.752065
 39564/100000: episode: 420, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.687, mean reward: 0.687 [0.687, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.340], loss: 0.000033, mae: 0.006446, mean_q: 0.747929
 39565/100000: episode: 421, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.635, mean reward: 0.635 [0.635, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.301, 10.170], loss: 0.000009, mae: 0.003182, mean_q: 0.752447
 39612/100000: episode: 422, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 0.790, mean reward: 0.017 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.238, 10.100], loss: 0.000132, mae: 0.007046, mean_q: 0.753028
 39613/100000: episode: 423, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.706, mean reward: 0.706 [0.706, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.374], loss: 0.000115, mae: 0.011757, mean_q: 0.744523
 39674/100000: episode: 424, duration: 0.345s, episode steps: 61, steps per second: 177, episode reward: 0.745, mean reward: 0.012 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.402, 10.100], loss: 0.000055, mae: 0.005143, mean_q: 0.753644
 39676/100000: episode: 425, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.601, mean reward: 0.300 [0.000, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.185], loss: 0.000227, mae: 0.006605, mean_q: 0.754857
 39677/100000: episode: 426, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.560, mean reward: 0.560 [0.560, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.134, 10.184], loss: 0.000028, mae: 0.005011, mean_q: 0.747005
 39678/100000: episode: 427, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.573, mean reward: 0.573 [0.573, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.200], loss: 0.000252, mae: 0.008506, mean_q: 0.746982
 39679/100000: episode: 428, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.594, mean reward: 0.594 [0.594, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.245], loss: 0.000078, mae: 0.005658, mean_q: 0.753250
 39680/100000: episode: 429, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.568, mean reward: 0.568 [0.568, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.220], loss: 0.000015, mae: 0.004950, mean_q: 0.756695
 39681/100000: episode: 430, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.583, mean reward: 0.583 [0.583, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.248], loss: 0.000010, mae: 0.003484, mean_q: 0.754804
 39684/100000: episode: 431, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 0.768, mean reward: 0.256 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.502], loss: 0.000018, mae: 0.004897, mean_q: 0.750796
 39685/100000: episode: 432, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.170], loss: 0.000004, mae: 0.002199, mean_q: 0.751952
 39686/100000: episode: 433, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.730, mean reward: 0.730 [0.730, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.070, 10.442], loss: 0.000042, mae: 0.006318, mean_q: 0.761136
 39687/100000: episode: 434, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.578, mean reward: 0.578 [0.578, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.070, 10.205], loss: 0.000035, mae: 0.002974, mean_q: 0.752649
 39690/100000: episode: 435, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.823, mean reward: 0.274 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.462], loss: 0.000140, mae: 0.006429, mean_q: 0.748367
 39691/100000: episode: 436, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.544, mean reward: 0.544 [0.544, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.070, 10.200], loss: 0.000002, mae: 0.001768, mean_q: 0.750761
 39692/100000: episode: 437, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.559, mean reward: 0.559 [0.559, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.070, 10.204], loss: 0.000004, mae: 0.002241, mean_q: 0.751283
 39693/100000: episode: 438, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.606, mean reward: 0.606 [0.606, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.070, 10.201], loss: 0.000019, mae: 0.003418, mean_q: 0.751791
 39754/100000: episode: 439, duration: 0.363s, episode steps: 61, steps per second: 168, episode reward: 0.746, mean reward: 0.012 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-0.496, 10.141], loss: 0.000155, mae: 0.006880, mean_q: 0.752528
 39801/100000: episode: 440, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 0.855, mean reward: 0.018 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.037, 10.100], loss: 0.000144, mae: 0.007838, mean_q: 0.752129
 39802/100000: episode: 441, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.568, mean reward: 0.568 [0.568, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.070, 10.223], loss: 0.000021, mae: 0.005310, mean_q: 0.761755
 39804/100000: episode: 442, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.527, mean reward: 0.264 [0.000, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.131], loss: 0.000005, mae: 0.002381, mean_q: 0.755315
 39805/100000: episode: 443, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.623, mean reward: 0.623 [0.623, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.264], loss: 0.000021, mae: 0.004132, mean_q: 0.752027
 39806/100000: episode: 444, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.564, mean reward: 0.564 [0.564, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.070, 10.209], loss: 0.000016, mae: 0.003699, mean_q: 0.753462
 39807/100000: episode: 445, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.571, mean reward: 0.571 [0.571, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.070, 10.200], loss: 0.000017, mae: 0.005338, mean_q: 0.758951
 39808/100000: episode: 446, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.240], loss: 0.000693, mae: 0.009158, mean_q: 0.756181
 39810/100000: episode: 447, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.622, mean reward: 0.311 [0.000, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.183], loss: 0.000355, mae: 0.013866, mean_q: 0.741984
 39811/100000: episode: 448, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.560, mean reward: 0.560 [0.560, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.070, 10.200], loss: 0.000010, mae: 0.003048, mean_q: 0.751293
 39812/100000: episode: 449, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.070, 10.247], loss: 0.000041, mae: 0.008641, mean_q: 0.759749
 39813/100000: episode: 450, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.574, mean reward: 0.574 [0.574, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.221], loss: 0.000028, mae: 0.006653, mean_q: 0.758157
 39874/100000: episode: 451, duration: 0.364s, episode steps: 61, steps per second: 168, episode reward: 0.797, mean reward: 0.013 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-1.082, 10.100], loss: 0.000169, mae: 0.008183, mean_q: 0.750335
 39877/100000: episode: 452, duration: 0.024s, episode steps: 3, steps per second: 128, episode reward: 0.814, mean reward: 0.271 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.045, 10.550], loss: 0.000168, mae: 0.009255, mean_q: 0.743777
 39878/100000: episode: 453, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.528, mean reward: 0.528 [0.528, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.272, 10.200], loss: 0.000025, mae: 0.004130, mean_q: 0.747036
 39925/100000: episode: 454, duration: 0.256s, episode steps: 47, steps per second: 184, episode reward: 0.711, mean reward: 0.015 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.752, 10.100], loss: 0.000129, mae: 0.006644, mean_q: 0.751401
 39926/100000: episode: 455, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.562, mean reward: 0.562 [0.562, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.070, 10.213], loss: 0.000022, mae: 0.005800, mean_q: 0.748370
 39929/100000: episode: 456, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.816, mean reward: 0.272 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.088, 10.582], loss: 0.000029, mae: 0.005094, mean_q: 0.750210
 39976/100000: episode: 457, duration: 0.290s, episode steps: 47, steps per second: 162, episode reward: 0.844, mean reward: 0.018 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.369, 10.248], loss: 0.000141, mae: 0.006798, mean_q: 0.749757
 39977/100000: episode: 458, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.287], loss: 0.000034, mae: 0.006063, mean_q: 0.744176
 40024/100000: episode: 459, duration: 0.290s, episode steps: 47, steps per second: 162, episode reward: 0.798, mean reward: 0.017 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.018, 10.100], loss: 0.000171, mae: 0.008519, mean_q: 0.750002
 40071/100000: episode: 460, duration: 0.299s, episode steps: 47, steps per second: 157, episode reward: 0.740, mean reward: 0.016 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.765, 10.100], loss: 0.000106, mae: 0.005304, mean_q: 0.750390
 40072/100000: episode: 461, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.575, mean reward: 0.575 [0.575, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.216], loss: 0.000013, mae: 0.003897, mean_q: 0.745156
 40074/100000: episode: 462, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.565, mean reward: 0.282 [0.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.198], loss: 0.000315, mae: 0.007101, mean_q: 0.748913
 40075/100000: episode: 463, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.576, mean reward: 0.576 [0.576, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.243], loss: 0.000011, mae: 0.004411, mean_q: 0.747773
 40078/100000: episode: 464, duration: 0.026s, episode steps: 3, steps per second: 117, episode reward: 0.809, mean reward: 0.270 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.442], loss: 0.000166, mae: 0.005020, mean_q: 0.754613
 40081/100000: episode: 465, duration: 0.025s, episode steps: 3, steps per second: 118, episode reward: 0.767, mean reward: 0.256 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.510], loss: 0.000222, mae: 0.007221, mean_q: 0.750480
 40083/100000: episode: 466, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.613, mean reward: 0.306 [0.000, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.265], loss: 0.000180, mae: 0.006677, mean_q: 0.746230
 40084/100000: episode: 467, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.725, mean reward: 0.725 [0.725, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.448], loss: 0.000006, mae: 0.002179, mean_q: 0.744959
 40085/100000: episode: 468, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.726, mean reward: 0.726 [0.726, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.035, 10.442], loss: 0.000004, mae: 0.002431, mean_q: 0.754097
 40132/100000: episode: 469, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 0.820, mean reward: 0.017 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.144, 10.100], loss: 0.000171, mae: 0.009089, mean_q: 0.748288
 40179/100000: episode: 470, duration: 0.252s, episode steps: 47, steps per second: 186, episode reward: 0.785, mean reward: 0.017 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.845, 10.487], loss: 0.000209, mae: 0.008412, mean_q: 0.747370
 40180/100000: episode: 471, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.592, mean reward: 0.592 [0.592, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.070, 10.265], loss: 0.000005, mae: 0.002571, mean_q: 0.752125
 40181/100000: episode: 472, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.687, mean reward: 0.687 [0.687, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.070, 10.409], loss: 0.000504, mae: 0.015695, mean_q: 0.734825
 40182/100000: episode: 473, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.581, mean reward: 0.581 [0.581, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.070, 10.212], loss: 0.000004, mae: 0.002289, mean_q: 0.749879
 40229/100000: episode: 474, duration: 0.273s, episode steps: 47, steps per second: 172, episode reward: 0.789, mean reward: 0.017 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.366, 10.100], loss: 0.000138, mae: 0.004813, mean_q: 0.748257
 40230/100000: episode: 475, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.579, mean reward: 0.579 [0.579, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.070, 10.200], loss: 0.000312, mae: 0.008604, mean_q: 0.744383
 40277/100000: episode: 476, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 0.751, mean reward: 0.016 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.185, 10.100], loss: 0.000138, mae: 0.007280, mean_q: 0.747875
 40280/100000: episode: 477, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 0.812, mean reward: 0.271 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.370, 10.502], loss: 0.000107, mae: 0.007597, mean_q: 0.741765
 40327/100000: episode: 478, duration: 0.265s, episode steps: 47, steps per second: 177, episode reward: 0.787, mean reward: 0.017 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.739, 10.100], loss: 0.000236, mae: 0.009035, mean_q: 0.746973
 40328/100000: episode: 479, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.565, mean reward: 0.565 [0.565, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.070, 10.200], loss: 0.000188, mae: 0.010426, mean_q: 0.742407
[Info] 2-TH LEVEL FOUND: 0.7846424579620361, Considering 17/100 traces
 40329/100000: episode: 480, duration: 4.412s, episode steps: 1, steps per second: 0, episode reward: 0.701, mean reward: 0.701 [0.701, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.320], loss: 0.000260, mae: 0.007845, mean_q: 0.743182
 40376/100000: episode: 481, duration: 0.278s, episode steps: 47, steps per second: 169, episode reward: 0.744, mean reward: 0.016 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.733, 10.100], loss: 0.000194, mae: 0.008857, mean_q: 0.746914
 40423/100000: episode: 482, duration: 0.278s, episode steps: 47, steps per second: 169, episode reward: 0.796, mean reward: 0.017 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-1.054, 10.100], loss: 0.000161, mae: 0.006768, mean_q: 0.747723
 40470/100000: episode: 483, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 0.911, mean reward: 0.019 [0.000, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.617, 10.100], loss: 0.000130, mae: 0.008325, mean_q: 0.746038
 40525/100000: episode: 484, duration: 0.304s, episode steps: 55, steps per second: 181, episode reward: 0.718, mean reward: 0.013 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.382, 10.100], loss: 0.000211, mae: 0.010243, mean_q: 0.745084
 40572/100000: episode: 485, duration: 0.286s, episode steps: 47, steps per second: 165, episode reward: 0.767, mean reward: 0.016 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.320, 10.100], loss: 0.000123, mae: 0.004940, mean_q: 0.744581
 40619/100000: episode: 486, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 0.736, mean reward: 0.016 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.696, 10.100], loss: 0.000118, mae: 0.006071, mean_q: 0.744239
 40666/100000: episode: 487, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 0.789, mean reward: 0.017 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.452, 10.100], loss: 0.000100, mae: 0.004930, mean_q: 0.744113
 40713/100000: episode: 488, duration: 0.254s, episode steps: 47, steps per second: 185, episode reward: 0.778, mean reward: 0.017 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.233, 10.100], loss: 0.000170, mae: 0.008324, mean_q: 0.742549
 40760/100000: episode: 489, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 0.779, mean reward: 0.017 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.780, 10.135], loss: 0.000240, mae: 0.009460, mean_q: 0.743538
 40807/100000: episode: 490, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 0.758, mean reward: 0.016 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.358, 10.100], loss: 0.000143, mae: 0.005654, mean_q: 0.743705
 40854/100000: episode: 491, duration: 0.268s, episode steps: 47, steps per second: 176, episode reward: 0.762, mean reward: 0.016 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.510, 10.100], loss: 0.000149, mae: 0.006715, mean_q: 0.744549
 40901/100000: episode: 492, duration: 0.271s, episode steps: 47, steps per second: 173, episode reward: 0.813, mean reward: 0.017 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.119, 10.100], loss: 0.000102, mae: 0.005555, mean_q: 0.743086
 40948/100000: episode: 493, duration: 0.279s, episode steps: 47, steps per second: 169, episode reward: 0.724, mean reward: 0.015 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.677, 10.100], loss: 0.000155, mae: 0.006333, mean_q: 0.741057
 40995/100000: episode: 494, duration: 0.269s, episode steps: 47, steps per second: 175, episode reward: 0.804, mean reward: 0.017 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.817, 10.100], loss: 0.000201, mae: 0.006993, mean_q: 0.742489
 41042/100000: episode: 495, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 0.824, mean reward: 0.018 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.421, 10.100], loss: 0.000145, mae: 0.007257, mean_q: 0.741408
 41089/100000: episode: 496, duration: 0.285s, episode steps: 47, steps per second: 165, episode reward: 0.748, mean reward: 0.016 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.894, 10.100], loss: 0.000156, mae: 0.008641, mean_q: 0.741318
 41136/100000: episode: 497, duration: 0.281s, episode steps: 47, steps per second: 167, episode reward: 0.777, mean reward: 0.017 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.625, 10.100], loss: 0.000259, mae: 0.008931, mean_q: 0.741640
 41183/100000: episode: 498, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 0.743, mean reward: 0.016 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.046, 10.100], loss: 0.000180, mae: 0.006172, mean_q: 0.740190
 41230/100000: episode: 499, duration: 0.252s, episode steps: 47, steps per second: 186, episode reward: 0.806, mean reward: 0.017 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.644, 10.100], loss: 0.000169, mae: 0.006003, mean_q: 0.741611
 41277/100000: episode: 500, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 0.801, mean reward: 0.017 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.273, 10.100], loss: 0.000161, mae: 0.005930, mean_q: 0.740607
 41324/100000: episode: 501, duration: 0.277s, episode steps: 47, steps per second: 169, episode reward: 0.865, mean reward: 0.018 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.775, 10.100], loss: 0.000161, mae: 0.006523, mean_q: 0.740035
 41371/100000: episode: 502, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 0.765, mean reward: 0.016 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.286, 10.122], loss: 0.000195, mae: 0.005891, mean_q: 0.739057
 41418/100000: episode: 503, duration: 0.288s, episode steps: 47, steps per second: 163, episode reward: 0.781, mean reward: 0.017 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.235, 10.131], loss: 0.000173, mae: 0.005939, mean_q: 0.739906
 41465/100000: episode: 504, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 0.881, mean reward: 0.019 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.106, 10.100], loss: 0.000131, mae: 0.007265, mean_q: 0.738973
 41512/100000: episode: 505, duration: 0.255s, episode steps: 47, steps per second: 185, episode reward: 0.777, mean reward: 0.017 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.201, 10.228], loss: 0.000124, mae: 0.005062, mean_q: 0.739084
 41559/100000: episode: 506, duration: 0.269s, episode steps: 47, steps per second: 175, episode reward: 0.797, mean reward: 0.017 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.766, 10.100], loss: 0.000125, mae: 0.005491, mean_q: 0.737204
 41614/100000: episode: 507, duration: 0.299s, episode steps: 55, steps per second: 184, episode reward: 0.792, mean reward: 0.014 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.854, 10.316], loss: 0.000134, mae: 0.005699, mean_q: 0.737219
 41661/100000: episode: 508, duration: 0.267s, episode steps: 47, steps per second: 176, episode reward: 0.774, mean reward: 0.016 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.541, 10.100], loss: 0.000143, mae: 0.005712, mean_q: 0.736304
 41708/100000: episode: 509, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 0.756, mean reward: 0.016 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.356, 10.100], loss: 0.000181, mae: 0.007471, mean_q: 0.736769
 41755/100000: episode: 510, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 0.796, mean reward: 0.017 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.064, 10.100], loss: 0.000140, mae: 0.006716, mean_q: 0.736792
 41802/100000: episode: 511, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 0.878, mean reward: 0.019 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.272, 10.123], loss: 0.000139, mae: 0.006011, mean_q: 0.737127
 41849/100000: episode: 512, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 0.797, mean reward: 0.017 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.290, 10.100], loss: 0.000161, mae: 0.006047, mean_q: 0.736191
 41896/100000: episode: 513, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 0.761, mean reward: 0.016 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.430, 10.387], loss: 0.000142, mae: 0.005796, mean_q: 0.735694
 41943/100000: episode: 514, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 0.756, mean reward: 0.016 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.102, 10.100], loss: 0.000131, mae: 0.005277, mean_q: 0.735899
 41990/100000: episode: 515, duration: 0.272s, episode steps: 47, steps per second: 173, episode reward: 0.930, mean reward: 0.020 [0.000, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.752, 10.100], loss: 0.000166, mae: 0.005593, mean_q: 0.735873
 42037/100000: episode: 516, duration: 0.279s, episode steps: 47, steps per second: 168, episode reward: 0.768, mean reward: 0.016 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.705, 10.100], loss: 0.000115, mae: 0.004497, mean_q: 0.734535
 42084/100000: episode: 517, duration: 0.284s, episode steps: 47, steps per second: 166, episode reward: 0.784, mean reward: 0.017 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.924, 10.101], loss: 0.000138, mae: 0.005340, mean_q: 0.734605
 42131/100000: episode: 518, duration: 0.262s, episode steps: 47, steps per second: 179, episode reward: 0.754, mean reward: 0.016 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.068, 10.100], loss: 0.000208, mae: 0.007357, mean_q: 0.733509
 42178/100000: episode: 519, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 0.739, mean reward: 0.016 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.389, 10.306], loss: 0.000146, mae: 0.006000, mean_q: 0.734209
 42225/100000: episode: 520, duration: 0.263s, episode steps: 47, steps per second: 178, episode reward: 0.744, mean reward: 0.016 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.499, 10.100], loss: 0.000164, mae: 0.006236, mean_q: 0.733962
 42272/100000: episode: 521, duration: 0.288s, episode steps: 47, steps per second: 163, episode reward: 0.825, mean reward: 0.018 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.940, 10.100], loss: 0.000151, mae: 0.006245, mean_q: 0.732961
 42319/100000: episode: 522, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 0.776, mean reward: 0.017 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.081, 10.100], loss: 0.000135, mae: 0.005182, mean_q: 0.731461
 42366/100000: episode: 523, duration: 0.273s, episode steps: 47, steps per second: 172, episode reward: 0.858, mean reward: 0.018 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.706, 10.100], loss: 0.000166, mae: 0.005094, mean_q: 0.732374
 42413/100000: episode: 524, duration: 0.307s, episode steps: 47, steps per second: 153, episode reward: 0.847, mean reward: 0.018 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.833, 10.115], loss: 0.000229, mae: 0.007437, mean_q: 0.731255
 42460/100000: episode: 525, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 0.783, mean reward: 0.017 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.708, 10.100], loss: 0.000160, mae: 0.006804, mean_q: 0.731382
 42507/100000: episode: 526, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 0.819, mean reward: 0.017 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.661, 10.150], loss: 0.000124, mae: 0.006147, mean_q: 0.732258
 42554/100000: episode: 527, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 0.813, mean reward: 0.017 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.393, 10.100], loss: 0.000142, mae: 0.005209, mean_q: 0.730142
 42601/100000: episode: 528, duration: 0.268s, episode steps: 47, steps per second: 175, episode reward: 0.765, mean reward: 0.016 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.611, 10.235], loss: 0.000169, mae: 0.008097, mean_q: 0.730044
 42648/100000: episode: 529, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 0.848, mean reward: 0.018 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.428, 10.100], loss: 0.000125, mae: 0.004781, mean_q: 0.730289
 42695/100000: episode: 530, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 0.814, mean reward: 0.017 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.233, 10.100], loss: 0.000136, mae: 0.005144, mean_q: 0.728897
 42742/100000: episode: 531, duration: 0.278s, episode steps: 47, steps per second: 169, episode reward: 0.821, mean reward: 0.017 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.741, 10.100], loss: 0.000146, mae: 0.006069, mean_q: 0.729239
 42789/100000: episode: 532, duration: 0.287s, episode steps: 47, steps per second: 164, episode reward: 0.757, mean reward: 0.016 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.260, 10.100], loss: 0.000181, mae: 0.007622, mean_q: 0.727711
 42836/100000: episode: 533, duration: 0.272s, episode steps: 47, steps per second: 173, episode reward: 0.871, mean reward: 0.019 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.815, 10.100], loss: 0.000136, mae: 0.005579, mean_q: 0.727639
 42883/100000: episode: 534, duration: 0.268s, episode steps: 47, steps per second: 175, episode reward: 0.757, mean reward: 0.016 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.423, 10.100], loss: 0.000192, mae: 0.006393, mean_q: 0.727661
 42930/100000: episode: 535, duration: 0.251s, episode steps: 47, steps per second: 188, episode reward: 0.801, mean reward: 0.017 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.315, 10.149], loss: 0.000173, mae: 0.006162, mean_q: 0.726963
 42977/100000: episode: 536, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 0.742, mean reward: 0.016 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.268, 10.138], loss: 0.000131, mae: 0.005136, mean_q: 0.726817
 43024/100000: episode: 537, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 0.738, mean reward: 0.016 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.768, 10.280], loss: 0.000167, mae: 0.006029, mean_q: 0.725743
 43071/100000: episode: 538, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 0.735, mean reward: 0.016 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.304, 10.110], loss: 0.000161, mae: 0.006011, mean_q: 0.726543
 43118/100000: episode: 539, duration: 0.262s, episode steps: 47, steps per second: 179, episode reward: 0.744, mean reward: 0.016 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.559, 10.249], loss: 0.000176, mae: 0.005370, mean_q: 0.726072
 43165/100000: episode: 540, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 0.791, mean reward: 0.017 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.608, 10.167], loss: 0.000244, mae: 0.008942, mean_q: 0.723546
 43212/100000: episode: 541, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 0.781, mean reward: 0.017 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.518, 10.123], loss: 0.000147, mae: 0.006601, mean_q: 0.723658
 43259/100000: episode: 542, duration: 0.272s, episode steps: 47, steps per second: 173, episode reward: 0.809, mean reward: 0.017 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.344, 10.100], loss: 0.000148, mae: 0.006132, mean_q: 0.723789
 43306/100000: episode: 543, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 0.772, mean reward: 0.016 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.263, 10.100], loss: 0.000168, mae: 0.006644, mean_q: 0.723505
 43353/100000: episode: 544, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 0.804, mean reward: 0.017 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.245, 10.100], loss: 0.000153, mae: 0.005521, mean_q: 0.723228
 43400/100000: episode: 545, duration: 0.285s, episode steps: 47, steps per second: 165, episode reward: 0.812, mean reward: 0.017 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.712, 10.100], loss: 0.000124, mae: 0.005482, mean_q: 0.723228
 43447/100000: episode: 546, duration: 0.273s, episode steps: 47, steps per second: 172, episode reward: 0.772, mean reward: 0.016 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.314, 10.100], loss: 0.000174, mae: 0.007771, mean_q: 0.723567
 43494/100000: episode: 547, duration: 0.296s, episode steps: 47, steps per second: 159, episode reward: 0.744, mean reward: 0.016 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.195, 10.100], loss: 0.000172, mae: 0.006548, mean_q: 0.721455
 43541/100000: episode: 548, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 0.756, mean reward: 0.016 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.511, 10.100], loss: 0.000199, mae: 0.007099, mean_q: 0.721472
 43588/100000: episode: 549, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 0.787, mean reward: 0.017 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.137, 10.100], loss: 0.000159, mae: 0.006914, mean_q: 0.723087
 43635/100000: episode: 550, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 0.792, mean reward: 0.017 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.708, 10.100], loss: 0.000196, mae: 0.006230, mean_q: 0.721198
 43682/100000: episode: 551, duration: 0.252s, episode steps: 47, steps per second: 187, episode reward: 0.746, mean reward: 0.016 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.089, 10.117], loss: 0.000191, mae: 0.008071, mean_q: 0.722362
 43729/100000: episode: 552, duration: 0.256s, episode steps: 47, steps per second: 184, episode reward: 0.755, mean reward: 0.016 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.668, 10.100], loss: 0.000201, mae: 0.007453, mean_q: 0.722066
 43776/100000: episode: 553, duration: 0.277s, episode steps: 47, steps per second: 169, episode reward: 0.816, mean reward: 0.017 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.309, 10.100], loss: 0.000213, mae: 0.006958, mean_q: 0.720294
 43823/100000: episode: 554, duration: 0.301s, episode steps: 47, steps per second: 156, episode reward: 0.815, mean reward: 0.017 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.427, 10.100], loss: 0.000173, mae: 0.006928, mean_q: 0.720047
 43870/100000: episode: 555, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 0.798, mean reward: 0.017 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.745, 10.124], loss: 0.000182, mae: 0.007466, mean_q: 0.719304
 43917/100000: episode: 556, duration: 0.284s, episode steps: 47, steps per second: 166, episode reward: 0.766, mean reward: 0.016 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.199, 10.140], loss: 0.000144, mae: 0.006446, mean_q: 0.719982
 43964/100000: episode: 557, duration: 0.253s, episode steps: 47, steps per second: 185, episode reward: 0.742, mean reward: 0.016 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.598, 10.100], loss: 0.000121, mae: 0.005355, mean_q: 0.720589
 44011/100000: episode: 558, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 0.738, mean reward: 0.016 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.465, 10.100], loss: 0.000194, mae: 0.006707, mean_q: 0.719073
 44058/100000: episode: 559, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 0.762, mean reward: 0.016 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.489, 10.100], loss: 0.000142, mae: 0.005430, mean_q: 0.719857
 44105/100000: episode: 560, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 0.802, mean reward: 0.017 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.530, 10.100], loss: 0.000216, mae: 0.009858, mean_q: 0.719668
 44160/100000: episode: 561, duration: 0.318s, episode steps: 55, steps per second: 173, episode reward: 0.771, mean reward: 0.014 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-1.441, 10.199], loss: 0.000155, mae: 0.007563, mean_q: 0.718510
 44207/100000: episode: 562, duration: 0.271s, episode steps: 47, steps per second: 173, episode reward: 0.775, mean reward: 0.016 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.326, 10.100], loss: 0.000138, mae: 0.005807, mean_q: 0.719467
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7846424579620361
2
 44254/100000: episode: 563, duration: 4.465s, episode steps: 47, steps per second: 11, episode reward: 0.763, mean reward: 0.016 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.296, 10.156], loss: 0.000123, mae: 0.005212, mean_q: 0.719475
 44355/100000: episode: 564, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.399], loss: 0.000113, mae: 0.005084, mean_q: 0.719741
 44456/100000: episode: 565, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.220, 10.615], loss: 0.000117, mae: 0.004718, mean_q: 0.721369
 44557/100000: episode: 566, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.579, 10.170], loss: 0.000113, mae: 0.005298, mean_q: 0.720974
 44658/100000: episode: 567, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.658, 10.228], loss: 0.000097, mae: 0.004009, mean_q: 0.721846
 44759/100000: episode: 568, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.564, 10.100], loss: 0.000078, mae: 0.004869, mean_q: 0.723284
 44860/100000: episode: 569, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.813, 10.262], loss: 0.000067, mae: 0.003990, mean_q: 0.724875
 44961/100000: episode: 570, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.770, 10.280], loss: 0.000055, mae: 0.003874, mean_q: 0.724949
 45062/100000: episode: 571, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.834, 10.168], loss: 0.000027, mae: 0.002767, mean_q: 0.727173
 45163/100000: episode: 572, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.100], loss: 0.000026, mae: 0.002474, mean_q: 0.727737
 45264/100000: episode: 573, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.013, 10.285], loss: 0.000020, mae: 0.002531, mean_q: 0.729339
 45365/100000: episode: 574, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.299, 10.100], loss: 0.000019, mae: 0.002222, mean_q: 0.730932
 45466/100000: episode: 575, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.738, 10.100], loss: 0.000026, mae: 0.002655, mean_q: 0.731928
 45567/100000: episode: 576, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.782, 10.139], loss: 0.000019, mae: 0.002307, mean_q: 0.733254
 45668/100000: episode: 577, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.885, 10.121], loss: 0.000015, mae: 0.002234, mean_q: 0.734070
 45769/100000: episode: 578, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.336], loss: 0.000012, mae: 0.002309, mean_q: 0.734981
 45870/100000: episode: 579, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.812, 10.130], loss: 0.000016, mae: 0.002453, mean_q: 0.736089
 45971/100000: episode: 580, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.202, 10.233], loss: 0.000021, mae: 0.002690, mean_q: 0.736595
 46072/100000: episode: 581, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.843, 10.174], loss: 0.000027, mae: 0.003727, mean_q: 0.736791
 46173/100000: episode: 582, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.310, 10.269], loss: 0.000024, mae: 0.003036, mean_q: 0.738942
 46274/100000: episode: 583, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.285, 10.100], loss: 0.000018, mae: 0.002526, mean_q: 0.739210
 46375/100000: episode: 584, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.305], loss: 0.000021, mae: 0.003153, mean_q: 0.739784
 46476/100000: episode: 585, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.898, 10.178], loss: 0.000024, mae: 0.003040, mean_q: 0.739215
 46577/100000: episode: 586, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.115], loss: 0.000024, mae: 0.003132, mean_q: 0.740325
 46678/100000: episode: 587, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.601, 10.123], loss: 0.000023, mae: 0.002517, mean_q: 0.741124
 46779/100000: episode: 588, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.657, 10.359], loss: 0.000027, mae: 0.003240, mean_q: 0.741392
 46880/100000: episode: 589, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.630, 10.422], loss: 0.000016, mae: 0.002524, mean_q: 0.740941
 46981/100000: episode: 590, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.290, 10.246], loss: 0.000026, mae: 0.002833, mean_q: 0.740844
 47082/100000: episode: 591, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.211, 10.100], loss: 0.000019, mae: 0.002335, mean_q: 0.740866
 47183/100000: episode: 592, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.906, 10.120], loss: 0.000012, mae: 0.002231, mean_q: 0.742190
 47284/100000: episode: 593, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.434, 10.122], loss: 0.000011, mae: 0.002091, mean_q: 0.740797
 47385/100000: episode: 594, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.767, 10.168], loss: 0.000017, mae: 0.002552, mean_q: 0.741452
 47486/100000: episode: 595, duration: 0.529s, episode steps: 101, steps per second: 191, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.883, 10.100], loss: 0.000018, mae: 0.002498, mean_q: 0.741596
 47587/100000: episode: 596, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.659, mean reward: 0.007 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.160, 10.100], loss: 0.000021, mae: 0.003128, mean_q: 0.740981
 47688/100000: episode: 597, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.792, 10.237], loss: 0.000020, mae: 0.002872, mean_q: 0.741214
 47789/100000: episode: 598, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.333, 10.214], loss: 0.000019, mae: 0.002322, mean_q: 0.741399
 47890/100000: episode: 599, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.083, 10.386], loss: 0.000014, mae: 0.002321, mean_q: 0.740567
 47991/100000: episode: 600, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.903, 10.100], loss: 0.000012, mae: 0.002395, mean_q: 0.740016
 48092/100000: episode: 601, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.486, 10.100], loss: 0.000013, mae: 0.002434, mean_q: 0.739938
 48193/100000: episode: 602, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.147, 10.158], loss: 0.000011, mae: 0.001827, mean_q: 0.739311
 48294/100000: episode: 603, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.177, 10.158], loss: 0.000017, mae: 0.002555, mean_q: 0.739605
 48395/100000: episode: 604, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.746, 10.100], loss: 0.000012, mae: 0.002005, mean_q: 0.738828
 48496/100000: episode: 605, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.330, 10.293], loss: 0.000019, mae: 0.002498, mean_q: 0.738952
 48597/100000: episode: 606, duration: 0.530s, episode steps: 101, steps per second: 190, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.444, 10.244], loss: 0.000017, mae: 0.002375, mean_q: 0.738461
 48698/100000: episode: 607, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.449, 10.271], loss: 0.000012, mae: 0.002243, mean_q: 0.737514
 48799/100000: episode: 608, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.184, 10.173], loss: 0.000012, mae: 0.001946, mean_q: 0.737574
 48900/100000: episode: 609, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.596, 10.100], loss: 0.000009, mae: 0.001869, mean_q: 0.737304
 49001/100000: episode: 610, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.339, 10.132], loss: 0.000010, mae: 0.001721, mean_q: 0.737279
 49102/100000: episode: 611, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.178, 10.100], loss: 0.000010, mae: 0.001818, mean_q: 0.736141
 49203/100000: episode: 612, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.333, 10.100], loss: 0.000018, mae: 0.002748, mean_q: 0.736174
 49304/100000: episode: 613, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.870, mean reward: 0.009 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.242, 10.100], loss: 0.000012, mae: 0.001786, mean_q: 0.736164
 49405/100000: episode: 614, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.448, 10.145], loss: 0.000016, mae: 0.001960, mean_q: 0.735774
 49506/100000: episode: 615, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.894, 10.172], loss: 0.000011, mae: 0.001733, mean_q: 0.735806
 49607/100000: episode: 616, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.330, 10.216], loss: 0.000016, mae: 0.002039, mean_q: 0.737101
 49708/100000: episode: 617, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.546, 10.228], loss: 0.000010, mae: 0.002272, mean_q: 0.736804
 49809/100000: episode: 618, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.634, 10.424], loss: 0.000007, mae: 0.001349, mean_q: 0.737741
 49910/100000: episode: 619, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.757, 10.372], loss: 0.000009, mae: 0.001757, mean_q: 0.736984
 50011/100000: episode: 620, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.969, 10.187], loss: 0.000006, mae: 0.001318, mean_q: 0.738760
 50112/100000: episode: 621, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.950, mean reward: 0.009 [0.000, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.309, 10.143], loss: 0.000014, mae: 0.002154, mean_q: 0.738345
 50213/100000: episode: 622, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.377, 10.127], loss: 0.000018, mae: 0.002008, mean_q: 0.739165
 50314/100000: episode: 623, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.856, 10.174], loss: 0.000006, mae: 0.001293, mean_q: 0.738675
 50415/100000: episode: 624, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.485, 10.202], loss: 0.000044, mae: 0.002840, mean_q: 0.739620
 50516/100000: episode: 625, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.293, 10.100], loss: 0.000018, mae: 0.002404, mean_q: 0.740670
 50617/100000: episode: 626, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.073, 10.250], loss: 0.000012, mae: 0.002019, mean_q: 0.740543
 50718/100000: episode: 627, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.610, 10.135], loss: 0.000007, mae: 0.001523, mean_q: 0.740670
 50819/100000: episode: 628, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.823, 10.187], loss: 0.000010, mae: 0.001671, mean_q: 0.741186
 50920/100000: episode: 629, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.121, 10.100], loss: 0.000026, mae: 0.002572, mean_q: 0.742373
 51021/100000: episode: 630, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.750, 10.243], loss: 0.000007, mae: 0.001232, mean_q: 0.742073
 51122/100000: episode: 631, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.059, 10.115], loss: 0.000009, mae: 0.001703, mean_q: 0.743093
 51223/100000: episode: 632, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.466, 10.103], loss: 0.000023, mae: 0.002127, mean_q: 0.743742
 51324/100000: episode: 633, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.975, 10.236], loss: 0.000016, mae: 0.002022, mean_q: 0.743525
 51425/100000: episode: 634, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.675, 10.100], loss: 0.000013, mae: 0.001630, mean_q: 0.745301
 51526/100000: episode: 635, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.284, 10.184], loss: 0.000007, mae: 0.001458, mean_q: 0.744843
 51627/100000: episode: 636, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.773, 10.100], loss: 0.000024, mae: 0.002740, mean_q: 0.745403
 51728/100000: episode: 637, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.821, 10.207], loss: 0.000036, mae: 0.003111, mean_q: 0.746334
 51829/100000: episode: 638, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.100], loss: 0.000023, mae: 0.002012, mean_q: 0.746668
 51930/100000: episode: 639, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.667, 10.223], loss: 0.000028, mae: 0.002200, mean_q: 0.747718
 52031/100000: episode: 640, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.293, 10.100], loss: 0.000016, mae: 0.002042, mean_q: 0.748386
 52132/100000: episode: 641, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.879, mean reward: 0.009 [0.000, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.977, 10.319], loss: 0.000009, mae: 0.001772, mean_q: 0.748119
 52233/100000: episode: 642, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.415, 10.273], loss: 0.000014, mae: 0.001668, mean_q: 0.748455
 52334/100000: episode: 643, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.862, mean reward: 0.009 [0.000, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.015, 10.255], loss: 0.000009, mae: 0.001742, mean_q: 0.749021
 52435/100000: episode: 644, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.389, 10.100], loss: 0.000021, mae: 0.002014, mean_q: 0.749387
 52536/100000: episode: 645, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.100], loss: 0.000034, mae: 0.002858, mean_q: 0.750513
 52637/100000: episode: 646, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.100], loss: 0.000024, mae: 0.002558, mean_q: 0.750658
 52738/100000: episode: 647, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.799, 10.198], loss: 0.000018, mae: 0.002062, mean_q: 0.750777
 52839/100000: episode: 648, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.519, 10.348], loss: 0.000019, mae: 0.002071, mean_q: 0.751877
 52940/100000: episode: 649, duration: 0.530s, episode steps: 101, steps per second: 190, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.452, 10.149], loss: 0.000009, mae: 0.001717, mean_q: 0.751797
 53041/100000: episode: 650, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.913, 10.138], loss: 0.000010, mae: 0.001724, mean_q: 0.751642
 53142/100000: episode: 651, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.811, 10.100], loss: 0.000013, mae: 0.001540, mean_q: 0.752281
 53243/100000: episode: 652, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.269, 10.348], loss: 0.000022, mae: 0.002375, mean_q: 0.752726
 53344/100000: episode: 653, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.445, 10.100], loss: 0.000011, mae: 0.001703, mean_q: 0.753143
 53445/100000: episode: 654, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.892, mean reward: 0.009 [0.000, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.257, 10.323], loss: 0.000022, mae: 0.002583, mean_q: 0.753800
 53546/100000: episode: 655, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.214], loss: 0.000014, mae: 0.001735, mean_q: 0.753730
 53647/100000: episode: 656, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-2.764, 10.211], loss: 0.000023, mae: 0.002284, mean_q: 0.754708
 53748/100000: episode: 657, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.897, mean reward: 0.009 [0.000, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.515, 10.100], loss: 0.000012, mae: 0.001775, mean_q: 0.755071
 53849/100000: episode: 658, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.372, 10.104], loss: 0.000024, mae: 0.002256, mean_q: 0.755404
 53950/100000: episode: 659, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.222], loss: 0.000032, mae: 0.002406, mean_q: 0.755656
 54051/100000: episode: 660, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.278, 10.144], loss: 0.000012, mae: 0.001910, mean_q: 0.756167
 54152/100000: episode: 661, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.734, 10.100], loss: 0.000013, mae: 0.001706, mean_q: 0.756859
 54253/100000: episode: 662, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.741, 10.174], loss: 0.000022, mae: 0.002577, mean_q: 0.756555
[Info] 1-TH LEVEL FOUND: 0.7711596488952637, Considering 10/100 traces
 54354/100000: episode: 663, duration: 4.944s, episode steps: 101, steps per second: 20, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.302, 10.100], loss: 0.000015, mae: 0.001920, mean_q: 0.757152
 54356/100000: episode: 664, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.569, mean reward: 0.284 [0.000, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.215, 10.100], loss: 0.000054, mae: 0.002828, mean_q: 0.756573
 54357/100000: episode: 665, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.631, mean reward: 0.631 [0.631, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.255, 10.100], loss: 0.000002, mae: 0.001407, mean_q: 0.754214
 54358/100000: episode: 666, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.599, mean reward: 0.599 [0.599, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.265, 10.200], loss: 0.000002, mae: 0.001472, mean_q: 0.759692
 54359/100000: episode: 667, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.558, mean reward: 0.558 [0.558, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.299, 10.200], loss: 0.000047, mae: 0.003202, mean_q: 0.756116
 54363/100000: episode: 668, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.750, mean reward: 0.188 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.188, 10.100], loss: 0.000025, mae: 0.002333, mean_q: 0.754829
 54364/100000: episode: 669, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.519, mean reward: 0.519 [0.519, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.287, 10.200], loss: 0.000006, mae: 0.003106, mean_q: 0.751350
 54369/100000: episode: 670, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 0.679, mean reward: 0.136 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.341, 10.100], loss: 0.000012, mae: 0.003025, mean_q: 0.757570
 54372/100000: episode: 671, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 0.680, mean reward: 0.227 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.291, 10.100], loss: 0.000342, mae: 0.007923, mean_q: 0.753858
 54376/100000: episode: 672, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 0.661, mean reward: 0.165 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.217, 10.100], loss: 0.000014, mae: 0.004599, mean_q: 0.760412
 54377/100000: episode: 673, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.564, mean reward: 0.564 [0.564, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.240, 10.100], loss: 0.000002, mae: 0.001810, mean_q: 0.758703
 54382/100000: episode: 674, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 0.692, mean reward: 0.138 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.330, 10.100], loss: 0.000245, mae: 0.005869, mean_q: 0.753685
 54384/100000: episode: 675, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.554, mean reward: 0.277 [0.000, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.208, 10.100], loss: 0.000022, mae: 0.005070, mean_q: 0.760971
 54385/100000: episode: 676, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.639, mean reward: 0.639 [0.639, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.217, 10.100], loss: 0.000950, mae: 0.010043, mean_q: 0.754514
 54386/100000: episode: 677, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.631, mean reward: 0.631 [0.631, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.201, 10.100], loss: 0.000052, mae: 0.010093, mean_q: 0.745890
 54390/100000: episode: 678, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.687, mean reward: 0.172 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.218, 10.100], loss: 0.000098, mae: 0.005952, mean_q: 0.756121
 54400/100000: episode: 679, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 0.847, mean reward: 0.085 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.424, 10.100], loss: 0.000112, mae: 0.005650, mean_q: 0.755689
 54410/100000: episode: 680, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 0.821, mean reward: 0.082 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.359, 10.100], loss: 0.000014, mae: 0.002748, mean_q: 0.756904
 54420/100000: episode: 681, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.827, mean reward: 0.083 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.558, 10.100], loss: 0.000123, mae: 0.005587, mean_q: 0.755605
 54421/100000: episode: 682, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.203, 10.100], loss: 0.000008, mae: 0.002455, mean_q: 0.754728
 54422/100000: episode: 683, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.618, mean reward: 0.618 [0.618, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.202, 10.100], loss: 0.000059, mae: 0.007486, mean_q: 0.751886
 54426/100000: episode: 684, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.626, mean reward: 0.156 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.282, 10.100], loss: 0.000177, mae: 0.005668, mean_q: 0.757379
 54427/100000: episode: 685, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.681, mean reward: 0.681 [0.681, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.304, 10.100], loss: 0.000004, mae: 0.002146, mean_q: 0.753965
 54431/100000: episode: 686, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 0.683, mean reward: 0.171 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.324, 10.100], loss: 0.000012, mae: 0.003912, mean_q: 0.752411
 54435/100000: episode: 687, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.722, mean reward: 0.181 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.174, 10.100], loss: 0.000031, mae: 0.003825, mean_q: 0.759198
 54436/100000: episode: 688, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.580, mean reward: 0.580 [0.580, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.243, 10.100], loss: 0.000034, mae: 0.005367, mean_q: 0.753397
 54437/100000: episode: 689, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.608, mean reward: 0.608 [0.608, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.255, 10.100], loss: 0.000003, mae: 0.001532, mean_q: 0.756638
 54438/100000: episode: 690, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.286, 10.100], loss: 0.000007, mae: 0.003493, mean_q: 0.759843
 54440/100000: episode: 691, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.606, mean reward: 0.303 [0.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.763, 10.100], loss: 0.000144, mae: 0.004150, mean_q: 0.757069
 54450/100000: episode: 692, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 0.781, mean reward: 0.078 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.261, 10.100], loss: 0.000011, mae: 0.003113, mean_q: 0.755786
 54454/100000: episode: 693, duration: 0.028s, episode steps: 4, steps per second: 140, episode reward: 0.904, mean reward: 0.226 [0.000, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.260, 10.100], loss: 0.000259, mae: 0.007708, mean_q: 0.751769
 54464/100000: episode: 694, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 0.800, mean reward: 0.080 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.456, 10.100], loss: 0.000051, mae: 0.003583, mean_q: 0.756611
 54467/100000: episode: 695, duration: 0.030s, episode steps: 3, steps per second: 99, episode reward: 0.683, mean reward: 0.228 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.287, 10.100], loss: 0.000348, mae: 0.007990, mean_q: 0.755052
 54470/100000: episode: 696, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 0.626, mean reward: 0.209 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.277, 10.100], loss: 0.000019, mae: 0.004653, mean_q: 0.752595
 54471/100000: episode: 697, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.622, mean reward: 0.622 [0.622, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.252, 10.200], loss: 0.000588, mae: 0.013173, mean_q: 0.766044
 54474/100000: episode: 698, duration: 0.026s, episode steps: 3, steps per second: 114, episode reward: 0.609, mean reward: 0.203 [0.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.189, 10.100], loss: 0.000012, mae: 0.003730, mean_q: 0.753077
 54475/100000: episode: 699, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.554, mean reward: 0.554 [0.554, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.284, 10.200], loss: 0.000100, mae: 0.004332, mean_q: 0.756047
 54479/100000: episode: 700, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 0.625, mean reward: 0.156 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.273, 10.100], loss: 0.000276, mae: 0.007698, mean_q: 0.759641
 54480/100000: episode: 701, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.547, mean reward: 0.547 [0.547, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.261, 10.200], loss: 0.000062, mae: 0.010868, mean_q: 0.747804
 54481/100000: episode: 702, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.607, mean reward: 0.607 [0.607, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.247, 10.100], loss: 0.000512, mae: 0.012866, mean_q: 0.747914
 54485/100000: episode: 703, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.695, mean reward: 0.174 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.212, 10.100], loss: 0.000085, mae: 0.004848, mean_q: 0.758524
 54495/100000: episode: 704, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.760, mean reward: 0.076 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.262, 10.100], loss: 0.000026, mae: 0.003681, mean_q: 0.756263
 54499/100000: episode: 705, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 0.682, mean reward: 0.171 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.274, 10.100], loss: 0.000058, mae: 0.003654, mean_q: 0.753081
 54500/100000: episode: 706, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.203, 10.100], loss: 0.000011, mae: 0.003905, mean_q: 0.752353
 54503/100000: episode: 707, duration: 0.030s, episode steps: 3, steps per second: 99, episode reward: 0.635, mean reward: 0.212 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.253, 10.100], loss: 0.000033, mae: 0.003537, mean_q: 0.755994
 54504/100000: episode: 708, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.627, mean reward: 0.627 [0.627, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.275, 10.100], loss: 0.000015, mae: 0.005086, mean_q: 0.761477
 54509/100000: episode: 709, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 0.679, mean reward: 0.136 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.270, 10.100], loss: 0.000010, mae: 0.002913, mean_q: 0.753217
 54510/100000: episode: 710, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.545, mean reward: 0.545 [0.545, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.231, 10.100], loss: 0.000001, mae: 0.001365, mean_q: 0.755096
 54514/100000: episode: 711, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.639, mean reward: 0.160 [0.000, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.259, 10.100], loss: 0.000048, mae: 0.003480, mean_q: 0.755936
 54516/100000: episode: 712, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.574, mean reward: 0.287 [0.000, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.251, 10.100], loss: 0.000027, mae: 0.003600, mean_q: 0.751989
 54520/100000: episode: 713, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 0.714, mean reward: 0.179 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.265, 10.100], loss: 0.000112, mae: 0.005501, mean_q: 0.757829
 54524/100000: episode: 714, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 0.727, mean reward: 0.182 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.184, 10.100], loss: 0.000093, mae: 0.006011, mean_q: 0.751335
 54528/100000: episode: 715, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 0.683, mean reward: 0.171 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.240, 10.100], loss: 0.000250, mae: 0.007081, mean_q: 0.756109
 54532/100000: episode: 716, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 0.706, mean reward: 0.176 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.255, 10.100], loss: 0.000014, mae: 0.004242, mean_q: 0.752661
 54536/100000: episode: 717, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 0.657, mean reward: 0.164 [0.000, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.272, 10.100], loss: 0.000014, mae: 0.004600, mean_q: 0.759793
 54537/100000: episode: 718, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.530, mean reward: 0.530 [0.530, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.275, 10.200], loss: 0.001049, mae: 0.016225, mean_q: 0.751062
 54538/100000: episode: 719, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.233, 10.100], loss: 0.000289, mae: 0.011112, mean_q: 0.747661
 54548/100000: episode: 720, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 0.879, mean reward: 0.088 [0.000, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.444, 10.100], loss: 0.000095, mae: 0.005067, mean_q: 0.755066
 54558/100000: episode: 721, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 0.805, mean reward: 0.080 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.697, 10.100], loss: 0.000025, mae: 0.004500, mean_q: 0.755937
 54559/100000: episode: 722, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.211, 10.100], loss: 0.000743, mae: 0.014302, mean_q: 0.748729
 54561/100000: episode: 723, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 0.603, mean reward: 0.302 [0.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.173, 10.100], loss: 0.000209, mae: 0.008522, mean_q: 0.750268
 54571/100000: episode: 724, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 0.796, mean reward: 0.080 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.168, 10.100], loss: 0.000108, mae: 0.005044, mean_q: 0.755043
 54575/100000: episode: 725, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 0.666, mean reward: 0.167 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.210, 10.100], loss: 0.000082, mae: 0.004793, mean_q: 0.755003
 54579/100000: episode: 726, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 0.650, mean reward: 0.163 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.299, 10.100], loss: 0.000177, mae: 0.005722, mean_q: 0.752159
 54589/100000: episode: 727, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 0.854, mean reward: 0.085 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.385, 10.100], loss: 0.000150, mae: 0.006066, mean_q: 0.752628
 54592/100000: episode: 728, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 0.655, mean reward: 0.218 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.258, 10.100], loss: 0.000021, mae: 0.003413, mean_q: 0.753981
 54593/100000: episode: 729, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.555, mean reward: 0.555 [0.555, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.267, 10.200], loss: 0.000371, mae: 0.008871, mean_q: 0.750285
 54597/100000: episode: 730, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.625, mean reward: 0.156 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.271, 10.100], loss: 0.000180, mae: 0.010797, mean_q: 0.761198
 54601/100000: episode: 731, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 0.674, mean reward: 0.168 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.454, 10.100], loss: 0.000294, mae: 0.006910, mean_q: 0.755336
 54605/100000: episode: 732, duration: 0.031s, episode steps: 4, steps per second: 131, episode reward: 0.745, mean reward: 0.186 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.231, 10.100], loss: 0.000293, mae: 0.011735, mean_q: 0.746783
 54609/100000: episode: 733, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.633, mean reward: 0.158 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.289, 10.100], loss: 0.000141, mae: 0.006966, mean_q: 0.760522
 54612/100000: episode: 734, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.642, mean reward: 0.214 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.252, 10.100], loss: 0.000030, mae: 0.006346, mean_q: 0.749235
 54614/100000: episode: 735, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.523, mean reward: 0.261 [0.000, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.272, 10.100], loss: 0.000034, mae: 0.006376, mean_q: 0.748417
 54615/100000: episode: 736, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.641, mean reward: 0.641 [0.641, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.252, 10.100], loss: 0.000072, mae: 0.006103, mean_q: 0.754193
 54625/100000: episode: 737, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.777, mean reward: 0.078 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.697, 10.100], loss: 0.000171, mae: 0.007884, mean_q: 0.754181
 54629/100000: episode: 738, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 0.692, mean reward: 0.173 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.193, 10.100], loss: 0.000008, mae: 0.003151, mean_q: 0.753783
 54632/100000: episode: 739, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.623, mean reward: 0.208 [0.000, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.320, 10.100], loss: 0.000053, mae: 0.003308, mean_q: 0.754597
 54636/100000: episode: 740, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.778, mean reward: 0.195 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.852, 10.100], loss: 0.000058, mae: 0.003707, mean_q: 0.751606
 54639/100000: episode: 741, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 0.628, mean reward: 0.209 [0.000, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.318, 10.100], loss: 0.000143, mae: 0.006156, mean_q: 0.757128
 54641/100000: episode: 742, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.539, mean reward: 0.269 [0.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.176, 10.100], loss: 0.000395, mae: 0.008293, mean_q: 0.757315
 54646/100000: episode: 743, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 0.761, mean reward: 0.152 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.322, 10.100], loss: 0.000185, mae: 0.010991, mean_q: 0.743922
 54647/100000: episode: 744, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.250, 10.100], loss: 0.000008, mae: 0.003571, mean_q: 0.757658
 54657/100000: episode: 745, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 0.791, mean reward: 0.079 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.454, 10.100], loss: 0.000047, mae: 0.007170, mean_q: 0.754206
 54659/100000: episode: 746, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.607, mean reward: 0.304 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.209, 10.100], loss: 0.000133, mae: 0.007990, mean_q: 0.760172
 54660/100000: episode: 747, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.562, mean reward: 0.562 [0.562, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.274, 10.200], loss: 0.000009, mae: 0.004012, mean_q: 0.759232
 54664/100000: episode: 748, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 0.662, mean reward: 0.165 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.204, 10.100], loss: 0.000031, mae: 0.003941, mean_q: 0.750272
 54667/100000: episode: 749, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 0.619, mean reward: 0.206 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.254, 10.100], loss: 0.000187, mae: 0.004696, mean_q: 0.753407
 54669/100000: episode: 750, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.546, mean reward: 0.273 [0.000, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.174, 10.100], loss: 0.000091, mae: 0.005103, mean_q: 0.750639
 54679/100000: episode: 751, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.764, mean reward: 0.076 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.312, 10.100], loss: 0.000063, mae: 0.003153, mean_q: 0.754405
 54684/100000: episode: 752, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 0.725, mean reward: 0.145 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.668, 10.100], loss: 0.000070, mae: 0.003415, mean_q: 0.753285
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7711596488952637
1
 54685/100000: episode: 753, duration: 4.201s, episode steps: 1, steps per second: 0, episode reward: 0.552, mean reward: 0.552 [0.552, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.247, 10.200], loss: 0.000101, mae: 0.007171, mean_q: 0.750568
 54786/100000: episode: 754, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.028, 10.400], loss: 0.000148, mae: 0.005956, mean_q: 0.752609
 54887/100000: episode: 755, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.755, 10.119], loss: 0.000117, mae: 0.004960, mean_q: 0.751612
 54988/100000: episode: 756, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.342, 10.282], loss: 0.000127, mae: 0.004839, mean_q: 0.750926
 55089/100000: episode: 757, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.518, 10.180], loss: 0.000123, mae: 0.005602, mean_q: 0.749639
 55190/100000: episode: 758, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.165, 10.100], loss: 0.000147, mae: 0.005463, mean_q: 0.749417
 55291/100000: episode: 759, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.917, 10.100], loss: 0.000089, mae: 0.005155, mean_q: 0.749053
 55392/100000: episode: 760, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.475, 10.290], loss: 0.000129, mae: 0.005458, mean_q: 0.747535
 55493/100000: episode: 761, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.330, 10.199], loss: 0.000114, mae: 0.004692, mean_q: 0.747143
 55594/100000: episode: 762, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.357, 10.125], loss: 0.000097, mae: 0.004129, mean_q: 0.745846
 55695/100000: episode: 763, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.438, 10.158], loss: 0.000122, mae: 0.005829, mean_q: 0.747027
 55796/100000: episode: 764, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.120, 10.233], loss: 0.000130, mae: 0.006073, mean_q: 0.746520
 55897/100000: episode: 765, duration: 0.550s, episode steps: 101, steps per second: 183, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.858, 10.100], loss: 0.000112, mae: 0.005287, mean_q: 0.746509
 55998/100000: episode: 766, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.879, 10.100], loss: 0.000097, mae: 0.003974, mean_q: 0.745158
 56099/100000: episode: 767, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.883, 10.156], loss: 0.000113, mae: 0.004315, mean_q: 0.745651
 56200/100000: episode: 768, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.171, 10.278], loss: 0.000093, mae: 0.003890, mean_q: 0.744744
 56301/100000: episode: 769, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.004, 10.100], loss: 0.000123, mae: 0.005671, mean_q: 0.744566
 56402/100000: episode: 770, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.552, 10.100], loss: 0.000094, mae: 0.003904, mean_q: 0.743703
 56503/100000: episode: 771, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.429, 10.227], loss: 0.000150, mae: 0.005624, mean_q: 0.743811
 56604/100000: episode: 772, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.531, 10.118], loss: 0.000124, mae: 0.004315, mean_q: 0.742391
 56705/100000: episode: 773, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.712, 10.128], loss: 0.000123, mae: 0.005038, mean_q: 0.741806
 56806/100000: episode: 774, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.487, 10.100], loss: 0.000141, mae: 0.004246, mean_q: 0.741968
 56907/100000: episode: 775, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.405, 10.135], loss: 0.000121, mae: 0.005825, mean_q: 0.741924
 57008/100000: episode: 776, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.947, 10.166], loss: 0.000095, mae: 0.004387, mean_q: 0.740430
 57109/100000: episode: 777, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.437, 10.100], loss: 0.000095, mae: 0.004334, mean_q: 0.740726
 57210/100000: episode: 778, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.906, 10.158], loss: 0.000090, mae: 0.003761, mean_q: 0.739105
 57311/100000: episode: 779, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.521, 10.240], loss: 0.000117, mae: 0.004310, mean_q: 0.738465
 57412/100000: episode: 780, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.577, 10.100], loss: 0.000101, mae: 0.004266, mean_q: 0.738168
 57513/100000: episode: 781, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.457, 10.285], loss: 0.000119, mae: 0.004859, mean_q: 0.737471
 57614/100000: episode: 782, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.880, 10.100], loss: 0.000098, mae: 0.003592, mean_q: 0.737372
 57715/100000: episode: 783, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.895, mean reward: 0.009 [0.000, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.910, 10.340], loss: 0.000099, mae: 0.004101, mean_q: 0.736386
 57816/100000: episode: 784, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.719, 10.100], loss: 0.000140, mae: 0.004900, mean_q: 0.735869
 57917/100000: episode: 785, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.630, 10.100], loss: 0.000123, mae: 0.005027, mean_q: 0.735443
 58018/100000: episode: 786, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.796, 10.162], loss: 0.000119, mae: 0.005510, mean_q: 0.734202
 58119/100000: episode: 787, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.849, mean reward: 0.008 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.570, 10.234], loss: 0.000081, mae: 0.003552, mean_q: 0.734324
 58220/100000: episode: 788, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.885, 10.100], loss: 0.000130, mae: 0.004943, mean_q: 0.734104
 58321/100000: episode: 789, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.548, 10.231], loss: 0.000122, mae: 0.004888, mean_q: 0.733850
 58422/100000: episode: 790, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.217, 10.181], loss: 0.000105, mae: 0.003963, mean_q: 0.733850
 58523/100000: episode: 791, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.338, 10.289], loss: 0.000114, mae: 0.004434, mean_q: 0.734001
 58624/100000: episode: 792, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.456, 10.100], loss: 0.000115, mae: 0.004527, mean_q: 0.733064
 58725/100000: episode: 793, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.863, 10.461], loss: 0.000087, mae: 0.003870, mean_q: 0.731038
 58826/100000: episode: 794, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.127, 10.100], loss: 0.000080, mae: 0.004392, mean_q: 0.730865
 58927/100000: episode: 795, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.366, 10.258], loss: 0.000107, mae: 0.004092, mean_q: 0.730069
 59028/100000: episode: 796, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.457, 10.254], loss: 0.000075, mae: 0.003942, mean_q: 0.729384
 59129/100000: episode: 797, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.374, 10.388], loss: 0.000107, mae: 0.004369, mean_q: 0.728282
 59230/100000: episode: 798, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.949, 10.130], loss: 0.000119, mae: 0.004852, mean_q: 0.729564
 59331/100000: episode: 799, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.244, 10.226], loss: 0.000108, mae: 0.004855, mean_q: 0.729960
[Info] FALSIFICATION!
 59432/100000: episode: 800, duration: 0.941s, episode steps: 101, steps per second: 107, episode reward: 1.025, mean reward: 0.010 [0.000, 1.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.696, 10.388], loss: 0.000097, mae: 0.004016, mean_q: 0.729526
 59533/100000: episode: 801, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.797, 10.425], loss: 0.000051, mae: 0.002735, mean_q: 0.730097
 59634/100000: episode: 802, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.566, 10.296], loss: 0.000045, mae: 0.003243, mean_q: 0.730974
 59735/100000: episode: 803, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.027, 10.154], loss: 0.000017, mae: 0.001630, mean_q: 0.731046
 59836/100000: episode: 804, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.360, 10.224], loss: 0.000031, mae: 0.002674, mean_q: 0.730392
 59937/100000: episode: 805, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.350, 10.100], loss: 0.000019, mae: 0.001717, mean_q: 0.731313
 60038/100000: episode: 806, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.591, 10.100], loss: 0.000049, mae: 0.002280, mean_q: 0.730957
 60139/100000: episode: 807, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.387, 10.100], loss: 0.000027, mae: 0.001919, mean_q: 0.731445
 60240/100000: episode: 808, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.855, 10.100], loss: 0.000040, mae: 0.002617, mean_q: 0.731209
 60341/100000: episode: 809, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.370, 10.294], loss: 0.000030, mae: 0.002111, mean_q: 0.731572
 60442/100000: episode: 810, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.911, 10.198], loss: 0.000021, mae: 0.001921, mean_q: 0.731311
 60543/100000: episode: 811, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.234, 10.100], loss: 0.000014, mae: 0.001785, mean_q: 0.731322
 60644/100000: episode: 812, duration: 0.536s, episode steps: 101, steps per second: 189, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.404, 10.192], loss: 0.000036, mae: 0.002747, mean_q: 0.731057
 60745/100000: episode: 813, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.100], loss: 0.000022, mae: 0.002112, mean_q: 0.732029
 60846/100000: episode: 814, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.772, 10.100], loss: 0.000026, mae: 0.002219, mean_q: 0.732085
 60947/100000: episode: 815, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.874, 10.100], loss: 0.000015, mae: 0.001959, mean_q: 0.731806
 61048/100000: episode: 816, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.647, 10.223], loss: 0.000020, mae: 0.001837, mean_q: 0.731858
 61149/100000: episode: 817, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.800, 10.117], loss: 0.000005, mae: 0.001466, mean_q: 0.731987
 61250/100000: episode: 818, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.508, 10.183], loss: 0.000005, mae: 0.001093, mean_q: 0.732104
 61351/100000: episode: 819, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.775, 10.153], loss: 0.000023, mae: 0.001967, mean_q: 0.732461
 61452/100000: episode: 820, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.912, 10.144], loss: 0.000026, mae: 0.001994, mean_q: 0.732001
 61553/100000: episode: 821, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.909, 10.201], loss: 0.000028, mae: 0.001990, mean_q: 0.732776
 61654/100000: episode: 822, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.537, 10.100], loss: 0.000028, mae: 0.002401, mean_q: 0.732654
 61755/100000: episode: 823, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.105, 10.133], loss: 0.000006, mae: 0.001360, mean_q: 0.732368
 61856/100000: episode: 824, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.107], loss: 0.000012, mae: 0.002005, mean_q: 0.732982
 61957/100000: episode: 825, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.531, 10.100], loss: 0.000024, mae: 0.001906, mean_q: 0.732749
 62058/100000: episode: 826, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.210, 10.100], loss: 0.000012, mae: 0.001821, mean_q: 0.732747
 62159/100000: episode: 827, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.218, 10.100], loss: 0.000012, mae: 0.001564, mean_q: 0.732927
 62260/100000: episode: 828, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.775, 10.100], loss: 0.000025, mae: 0.002144, mean_q: 0.732586
 62361/100000: episode: 829, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.947, 10.100], loss: 0.000014, mae: 0.001661, mean_q: 0.732812
 62462/100000: episode: 830, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.782, 10.121], loss: 0.000023, mae: 0.001874, mean_q: 0.733392
 62563/100000: episode: 831, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.339, 10.100], loss: 0.000014, mae: 0.001783, mean_q: 0.733557
 62664/100000: episode: 832, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.267, 10.392], loss: 0.000017, mae: 0.001598, mean_q: 0.734400
 62765/100000: episode: 833, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.072, 10.247], loss: 0.000018, mae: 0.001595, mean_q: 0.734034
 62866/100000: episode: 834, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.532, 10.146], loss: 0.000027, mae: 0.002871, mean_q: 0.733704
 62967/100000: episode: 835, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.547, 10.100], loss: 0.000022, mae: 0.002440, mean_q: 0.734540
 63068/100000: episode: 836, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.094, 10.100], loss: 0.000012, mae: 0.001711, mean_q: 0.734374
 63169/100000: episode: 837, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.180, 10.100], loss: 0.000044, mae: 0.002678, mean_q: 0.734788
 63270/100000: episode: 838, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.292, 10.100], loss: 0.000021, mae: 0.002205, mean_q: 0.735201
 63371/100000: episode: 839, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.411, 10.100], loss: 0.000018, mae: 0.001727, mean_q: 0.736088
 63472/100000: episode: 840, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.925, 10.100], loss: 0.000011, mae: 0.001598, mean_q: 0.735554
 63573/100000: episode: 841, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.215, 10.152], loss: 0.000011, mae: 0.001580, mean_q: 0.735529
 63674/100000: episode: 842, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.092, 10.100], loss: 0.000047, mae: 0.003655, mean_q: 0.736050
 63775/100000: episode: 843, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.969, 10.100], loss: 0.000022, mae: 0.002493, mean_q: 0.736022
 63876/100000: episode: 844, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.460, 10.157], loss: 0.000013, mae: 0.001705, mean_q: 0.736260
 63977/100000: episode: 845, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.216, 10.100], loss: 0.000023, mae: 0.001565, mean_q: 0.737827
 64078/100000: episode: 846, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.368, 10.262], loss: 0.000042, mae: 0.002565, mean_q: 0.737579
 64179/100000: episode: 847, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.777, 10.107], loss: 0.000028, mae: 0.002366, mean_q: 0.737305
 64280/100000: episode: 848, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.419, 10.110], loss: 0.000025, mae: 0.002042, mean_q: 0.737318
 64381/100000: episode: 849, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.528, 10.100], loss: 0.000044, mae: 0.003250, mean_q: 0.738029
 64482/100000: episode: 850, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.126, 10.100], loss: 0.000022, mae: 0.002717, mean_q: 0.737257
 64583/100000: episode: 851, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.470, 10.407], loss: 0.000017, mae: 0.001470, mean_q: 0.737105
 64684/100000: episode: 852, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.717, 10.100], loss: 0.000016, mae: 0.001686, mean_q: 0.737591
[Info] Complete ISplit Iteration
[Info] Levels: [0.7693565]
[Info] Cond. Prob: [0.01]
[Info] Error Prob: 0.01

 64785/100000: episode: 853, duration: 5.082s, episode steps: 101, steps per second: 20, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.558, 10.198], loss: 0.000022, mae: 0.002370, mean_q: 0.737389
 64886/100000: episode: 854, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.375, 10.141], loss: 0.000014, mae: 0.001970, mean_q: 0.737411
 64987/100000: episode: 855, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.942, 10.100], loss: 0.000019, mae: 0.002067, mean_q: 0.737444
 65088/100000: episode: 856, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.464, 10.100], loss: 0.000018, mae: 0.001510, mean_q: 0.737836
 65189/100000: episode: 857, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.677, 10.100], loss: 0.000007, mae: 0.001046, mean_q: 0.737637
 65290/100000: episode: 858, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.810, 10.100], loss: 0.000012, mae: 0.001457, mean_q: 0.738458
 65391/100000: episode: 859, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.417, 10.100], loss: 0.000018, mae: 0.001860, mean_q: 0.738602
 65492/100000: episode: 860, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.082, 10.277], loss: 0.000019, mae: 0.002050, mean_q: 0.738562
 65593/100000: episode: 861, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.852, 10.240], loss: 0.000007, mae: 0.001005, mean_q: 0.739171
 65694/100000: episode: 862, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.014, 10.221], loss: 0.000012, mae: 0.001345, mean_q: 0.739754
 65795/100000: episode: 863, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.357, 10.186], loss: 0.000014, mae: 0.001509, mean_q: 0.739539
 65896/100000: episode: 864, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.774, 10.111], loss: 0.000011, mae: 0.001401, mean_q: 0.739887
 65997/100000: episode: 865, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.687, 10.150], loss: 0.000015, mae: 0.001622, mean_q: 0.740098
 66098/100000: episode: 866, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.052, 10.397], loss: 0.000013, mae: 0.001607, mean_q: 0.740502
 66199/100000: episode: 867, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.003, 10.322], loss: 0.000015, mae: 0.001899, mean_q: 0.740435
 66300/100000: episode: 868, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.520, 10.109], loss: 0.000011, mae: 0.001484, mean_q: 0.741321
 66401/100000: episode: 869, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.009, 10.100], loss: 0.000014, mae: 0.001518, mean_q: 0.741603
 66502/100000: episode: 870, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.261, 10.395], loss: 0.000018, mae: 0.002502, mean_q: 0.742491
 66603/100000: episode: 871, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.680, 10.170], loss: 0.000013, mae: 0.001811, mean_q: 0.741632
 66704/100000: episode: 872, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.494, 10.100], loss: 0.000010, mae: 0.001412, mean_q: 0.743306
 66805/100000: episode: 873, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.037, 10.144], loss: 0.000012, mae: 0.001348, mean_q: 0.743082
 66906/100000: episode: 874, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.854, 10.243], loss: 0.000013, mae: 0.001626, mean_q: 0.743132
 67007/100000: episode: 875, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.300, 10.100], loss: 0.000013, mae: 0.001798, mean_q: 0.744002
 67108/100000: episode: 876, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.777, 10.380], loss: 0.000019, mae: 0.001810, mean_q: 0.744317
 67209/100000: episode: 877, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.736, 10.100], loss: 0.000013, mae: 0.001468, mean_q: 0.744501
 67310/100000: episode: 878, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.141, 10.256], loss: 0.000015, mae: 0.001430, mean_q: 0.745258
 67411/100000: episode: 879, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.278, 10.100], loss: 0.000013, mae: 0.001755, mean_q: 0.745721
 67512/100000: episode: 880, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.654, 10.189], loss: 0.000015, mae: 0.002018, mean_q: 0.746089
 67613/100000: episode: 881, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.971, 10.405], loss: 0.000011, mae: 0.001589, mean_q: 0.745920
 67714/100000: episode: 882, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.529, 10.100], loss: 0.000017, mae: 0.002077, mean_q: 0.746344
 67815/100000: episode: 883, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.887, mean reward: 0.009 [0.000, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.703, 10.150], loss: 0.000014, mae: 0.001824, mean_q: 0.746529
 67916/100000: episode: 884, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.100], loss: 0.000014, mae: 0.001726, mean_q: 0.747444
 68017/100000: episode: 885, duration: 0.789s, episode steps: 101, steps per second: 128, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.674, 10.490], loss: 0.000011, mae: 0.001685, mean_q: 0.747560
 68118/100000: episode: 886, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.576, 10.308], loss: 0.000019, mae: 0.002097, mean_q: 0.748426
 68219/100000: episode: 887, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.851, 10.223], loss: 0.000020, mae: 0.001847, mean_q: 0.747916
 68320/100000: episode: 888, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.717, 10.152], loss: 0.000015, mae: 0.001832, mean_q: 0.749097
 68421/100000: episode: 889, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.625, 10.147], loss: 0.000016, mae: 0.001736, mean_q: 0.749033
 68522/100000: episode: 890, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.316, 10.100], loss: 0.000019, mae: 0.002208, mean_q: 0.749921
 68623/100000: episode: 891, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.682, 10.413], loss: 0.000015, mae: 0.001361, mean_q: 0.750654
 68724/100000: episode: 892, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.438, 10.100], loss: 0.000012, mae: 0.001565, mean_q: 0.750610
 68825/100000: episode: 893, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.208, 10.100], loss: 0.000013, mae: 0.001579, mean_q: 0.750912
 68926/100000: episode: 894, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.472, 10.300], loss: 0.000014, mae: 0.001840, mean_q: 0.751212
 69027/100000: episode: 895, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.628, 10.100], loss: 0.000015, mae: 0.001831, mean_q: 0.751259
 69128/100000: episode: 896, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.949, 10.100], loss: 0.000017, mae: 0.001937, mean_q: 0.751921
 69229/100000: episode: 897, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.056, 10.100], loss: 0.000011, mae: 0.001604, mean_q: 0.751628
 69330/100000: episode: 898, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.251, 10.100], loss: 0.000017, mae: 0.001879, mean_q: 0.752765
 69431/100000: episode: 899, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.087, 10.115], loss: 0.000011, mae: 0.001835, mean_q: 0.752437
 69532/100000: episode: 900, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.916, 10.127], loss: 0.000013, mae: 0.001618, mean_q: 0.753118
 69633/100000: episode: 901, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.533, 10.398], loss: 0.000012, mae: 0.001937, mean_q: 0.753441
 69734/100000: episode: 902, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.769, 10.100], loss: 0.000022, mae: 0.002446, mean_q: 0.753317
 69835/100000: episode: 903, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.296, 10.388], loss: 0.000014, mae: 0.001864, mean_q: 0.753938
 69936/100000: episode: 904, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.100], loss: 0.000014, mae: 0.001574, mean_q: 0.754391
 70037/100000: episode: 905, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.598, 10.100], loss: 0.000015, mae: 0.001947, mean_q: 0.754688
 70138/100000: episode: 906, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.511, 10.102], loss: 0.000012, mae: 0.001355, mean_q: 0.755007
 70239/100000: episode: 907, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.640, 10.237], loss: 0.000010, mae: 0.001112, mean_q: 0.754872
 70340/100000: episode: 908, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.827, 10.243], loss: 0.000013, mae: 0.001340, mean_q: 0.755306
 70441/100000: episode: 909, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.896, 10.100], loss: 0.000017, mae: 0.001710, mean_q: 0.755709
 70542/100000: episode: 910, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.477, 10.206], loss: 0.000013, mae: 0.001549, mean_q: 0.755770
 70643/100000: episode: 911, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.988, 10.100], loss: 0.000009, mae: 0.001320, mean_q: 0.756215
 70744/100000: episode: 912, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.455, 10.100], loss: 0.000013, mae: 0.001598, mean_q: 0.756278
 70845/100000: episode: 913, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.251, 10.578], loss: 0.000010, mae: 0.001503, mean_q: 0.756237
 70946/100000: episode: 914, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.415, 10.100], loss: 0.000013, mae: 0.001760, mean_q: 0.756701
 71047/100000: episode: 915, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.100], loss: 0.000012, mae: 0.001321, mean_q: 0.756805
 71148/100000: episode: 916, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.887, mean reward: 0.009 [0.000, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.193, 10.182], loss: 0.000016, mae: 0.001445, mean_q: 0.757026
 71249/100000: episode: 917, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.127, 10.100], loss: 0.000031, mae: 0.003052, mean_q: 0.757057
 71350/100000: episode: 918, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.945, 10.472], loss: 0.000016, mae: 0.001465, mean_q: 0.757388
 71451/100000: episode: 919, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.043, 10.125], loss: 0.000014, mae: 0.001698, mean_q: 0.757306
 71552/100000: episode: 920, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.413, 10.100], loss: 0.000015, mae: 0.001416, mean_q: 0.757303
 71653/100000: episode: 921, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.149, 10.100], loss: 0.000016, mae: 0.001336, mean_q: 0.757385
 71754/100000: episode: 922, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.050, 10.100], loss: 0.000012, mae: 0.001131, mean_q: 0.757670
 71855/100000: episode: 923, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.188, 10.257], loss: 0.000016, mae: 0.001402, mean_q: 0.757783
 71956/100000: episode: 924, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.715, 10.100], loss: 0.000010, mae: 0.001110, mean_q: 0.757718
 72057/100000: episode: 925, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.238, 10.100], loss: 0.000011, mae: 0.001262, mean_q: 0.757853
 72158/100000: episode: 926, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.065, 10.205], loss: 0.000023, mae: 0.001781, mean_q: 0.757916
 72259/100000: episode: 927, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.548, 10.100], loss: 0.000012, mae: 0.001430, mean_q: 0.758194
 72360/100000: episode: 928, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.268, 10.100], loss: 0.000006, mae: 0.001076, mean_q: 0.757994
 72461/100000: episode: 929, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.100], loss: 0.000016, mae: 0.002016, mean_q: 0.758267
 72562/100000: episode: 930, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.689, 10.256], loss: 0.000012, mae: 0.001077, mean_q: 0.758553
 72663/100000: episode: 931, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.129, 10.202], loss: 0.000015, mae: 0.001648, mean_q: 0.758371
 72764/100000: episode: 932, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.154, 10.100], loss: 0.000013, mae: 0.001511, mean_q: 0.758460
 72865/100000: episode: 933, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.969, 10.169], loss: 0.000017, mae: 0.001397, mean_q: 0.758651
 72966/100000: episode: 934, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.900, 10.357], loss: 0.000023, mae: 0.002565, mean_q: 0.758638
 73067/100000: episode: 935, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.111, 10.100], loss: 0.000011, mae: 0.001154, mean_q: 0.759284
 73168/100000: episode: 936, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.599, 10.100], loss: 0.000011, mae: 0.001426, mean_q: 0.759053
 73269/100000: episode: 937, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.679, 10.228], loss: 0.000009, mae: 0.001120, mean_q: 0.759339
 73370/100000: episode: 938, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.668, mean reward: 0.007 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.571, 10.171], loss: 0.000014, mae: 0.001553, mean_q: 0.759179
 73471/100000: episode: 939, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.652, 10.100], loss: 0.000022, mae: 0.001765, mean_q: 0.759519
 73572/100000: episode: 940, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.745, 10.341], loss: 0.000011, mae: 0.001103, mean_q: 0.759294
 73673/100000: episode: 941, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.967, mean reward: 0.010 [0.000, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.659, 10.100], loss: 0.000019, mae: 0.001303, mean_q: 0.759347
 73774/100000: episode: 942, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.981, 10.260], loss: 0.000026, mae: 0.001705, mean_q: 0.759375
 73875/100000: episode: 943, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.126], loss: 0.000011, mae: 0.001269, mean_q: 0.759340
 73976/100000: episode: 944, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.712, 10.313], loss: 0.000011, mae: 0.001386, mean_q: 0.759750
 74077/100000: episode: 945, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.392, 10.236], loss: 0.000014, mae: 0.001555, mean_q: 0.760464
 74178/100000: episode: 946, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.806, 10.177], loss: 0.000010, mae: 0.001218, mean_q: 0.760224
 74279/100000: episode: 947, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.995, 10.497], loss: 0.000012, mae: 0.001894, mean_q: 0.760199
 74380/100000: episode: 948, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.892, 10.130], loss: 0.000022, mae: 0.001998, mean_q: 0.760427
 74481/100000: episode: 949, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.922, 10.100], loss: 0.000014, mae: 0.001210, mean_q: 0.760544
 74582/100000: episode: 950, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.653, 10.390], loss: 0.000016, mae: 0.001452, mean_q: 0.760400
 74683/100000: episode: 951, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.330, 10.100], loss: 0.000018, mae: 0.001500, mean_q: 0.760684
 74784/100000: episode: 952, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.746, 10.191], loss: 0.000024, mae: 0.001354, mean_q: 0.760701
[Info] 1-TH LEVEL FOUND: 0.7649322748184204, Considering 10/100 traces
 74885/100000: episode: 953, duration: 4.992s, episode steps: 101, steps per second: 20, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.970, 10.100], loss: 0.000007, mae: 0.000807, mean_q: 0.760720
 74936/100000: episode: 954, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 0.695, mean reward: 0.014 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.540, 10.130], loss: 0.000006, mae: 0.000983, mean_q: 0.760376
 74990/100000: episode: 955, duration: 0.321s, episode steps: 54, steps per second: 168, episode reward: 0.787, mean reward: 0.015 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.341, 10.544], loss: 0.000031, mae: 0.002298, mean_q: 0.760819
 75041/100000: episode: 956, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 0.700, mean reward: 0.014 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.097, 10.187], loss: 0.000023, mae: 0.001563, mean_q: 0.760884
 75092/100000: episode: 957, duration: 0.306s, episode steps: 51, steps per second: 167, episode reward: 0.725, mean reward: 0.014 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.801, 10.239], loss: 0.000012, mae: 0.001391, mean_q: 0.760546
 75143/100000: episode: 958, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 0.728, mean reward: 0.014 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.460, 10.100], loss: 0.000024, mae: 0.001341, mean_q: 0.760657
 75194/100000: episode: 959, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 0.710, mean reward: 0.014 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.369, 10.100], loss: 0.000022, mae: 0.001553, mean_q: 0.760520
 75247/100000: episode: 960, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 0.754, mean reward: 0.014 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.336, 10.198], loss: 0.000014, mae: 0.001348, mean_q: 0.760524
 75298/100000: episode: 961, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 0.716, mean reward: 0.014 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.444, 10.265], loss: 0.000015, mae: 0.001195, mean_q: 0.760340
 75351/100000: episode: 962, duration: 0.323s, episode steps: 53, steps per second: 164, episode reward: 0.840, mean reward: 0.016 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.344, 10.530], loss: 0.000026, mae: 0.001578, mean_q: 0.760626
 75405/100000: episode: 963, duration: 0.291s, episode steps: 54, steps per second: 185, episode reward: 0.814, mean reward: 0.015 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.693, 10.455], loss: 0.000013, mae: 0.001704, mean_q: 0.760222
 75456/100000: episode: 964, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 0.711, mean reward: 0.014 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.237, 10.335], loss: 0.000005, mae: 0.001250, mean_q: 0.760466
 75508/100000: episode: 965, duration: 0.299s, episode steps: 52, steps per second: 174, episode reward: 0.812, mean reward: 0.016 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.116, 10.211], loss: 0.000042, mae: 0.001914, mean_q: 0.760175
 75560/100000: episode: 966, duration: 0.307s, episode steps: 52, steps per second: 169, episode reward: 0.737, mean reward: 0.014 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.973, 10.151], loss: 0.000005, mae: 0.001084, mean_q: 0.760022
 75613/100000: episode: 967, duration: 0.299s, episode steps: 53, steps per second: 177, episode reward: 0.694, mean reward: 0.013 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-2.120, 10.347], loss: 0.000016, mae: 0.001286, mean_q: 0.760383
 75665/100000: episode: 968, duration: 0.298s, episode steps: 52, steps per second: 175, episode reward: 0.687, mean reward: 0.013 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.305, 10.100], loss: 0.000014, mae: 0.001456, mean_q: 0.760007
 75716/100000: episode: 969, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.744, mean reward: 0.015 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.510, 10.100], loss: 0.000011, mae: 0.001438, mean_q: 0.759964
 75770/100000: episode: 970, duration: 0.279s, episode steps: 54, steps per second: 193, episode reward: 0.826, mean reward: 0.015 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.629, 10.110], loss: 0.000008, mae: 0.001014, mean_q: 0.759553
 75824/100000: episode: 971, duration: 0.314s, episode steps: 54, steps per second: 172, episode reward: 0.725, mean reward: 0.013 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.275, 10.100], loss: 0.000011, mae: 0.001243, mean_q: 0.759471
 75875/100000: episode: 972, duration: 0.305s, episode steps: 51, steps per second: 167, episode reward: 0.713, mean reward: 0.014 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.569, 10.290], loss: 0.000028, mae: 0.002029, mean_q: 0.759729
 75926/100000: episode: 973, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 0.748, mean reward: 0.015 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.413, 10.383], loss: 0.000010, mae: 0.001475, mean_q: 0.759306
 75977/100000: episode: 974, duration: 0.306s, episode steps: 51, steps per second: 167, episode reward: 0.854, mean reward: 0.017 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.552, 10.211], loss: 0.000009, mae: 0.001012, mean_q: 0.759342
 76030/100000: episode: 975, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 0.729, mean reward: 0.014 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.873, 10.405], loss: 0.000017, mae: 0.001974, mean_q: 0.759278
 76081/100000: episode: 976, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 0.699, mean reward: 0.014 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.586, 10.129], loss: 0.000011, mae: 0.001719, mean_q: 0.759040
 76133/100000: episode: 977, duration: 0.309s, episode steps: 52, steps per second: 168, episode reward: 0.756, mean reward: 0.015 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.356, 10.233], loss: 0.000010, mae: 0.001529, mean_q: 0.758827
 76185/100000: episode: 978, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 0.814, mean reward: 0.016 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.975, 10.166], loss: 0.000020, mae: 0.001482, mean_q: 0.759017
 76238/100000: episode: 979, duration: 0.283s, episode steps: 53, steps per second: 187, episode reward: 0.746, mean reward: 0.014 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.439, 10.100], loss: 0.000015, mae: 0.002232, mean_q: 0.758830
 76291/100000: episode: 980, duration: 0.286s, episode steps: 53, steps per second: 185, episode reward: 0.708, mean reward: 0.013 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.414, 10.181], loss: 0.000028, mae: 0.001691, mean_q: 0.758638
 76342/100000: episode: 981, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.639, mean reward: 0.013 [0.000, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.220, 10.178], loss: 0.000029, mae: 0.002096, mean_q: 0.759021
 76393/100000: episode: 982, duration: 0.306s, episode steps: 51, steps per second: 167, episode reward: 0.737, mean reward: 0.014 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.224, 10.150], loss: 0.000026, mae: 0.001751, mean_q: 0.758669
 76444/100000: episode: 983, duration: 0.311s, episode steps: 51, steps per second: 164, episode reward: 0.714, mean reward: 0.014 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.271, 10.398], loss: 0.000029, mae: 0.001856, mean_q: 0.758693
 76495/100000: episode: 984, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 0.790, mean reward: 0.015 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.420, 10.100], loss: 0.000030, mae: 0.001851, mean_q: 0.758431
 76546/100000: episode: 985, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.759, mean reward: 0.015 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.465, 10.195], loss: 0.000022, mae: 0.001685, mean_q: 0.758487
 76598/100000: episode: 986, duration: 0.310s, episode steps: 52, steps per second: 168, episode reward: 0.715, mean reward: 0.014 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.281, 10.208], loss: 0.000007, mae: 0.001253, mean_q: 0.758364
 76650/100000: episode: 987, duration: 0.320s, episode steps: 52, steps per second: 162, episode reward: 0.744, mean reward: 0.014 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.389, 10.397], loss: 0.000019, mae: 0.001509, mean_q: 0.758428
 76704/100000: episode: 988, duration: 0.299s, episode steps: 54, steps per second: 180, episode reward: 0.749, mean reward: 0.014 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.601, 10.269], loss: 0.000017, mae: 0.001762, mean_q: 0.758086
 76755/100000: episode: 989, duration: 0.304s, episode steps: 51, steps per second: 168, episode reward: 0.861, mean reward: 0.017 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.708, 10.100], loss: 0.000060, mae: 0.003042, mean_q: 0.758647
 76806/100000: episode: 990, duration: 0.313s, episode steps: 51, steps per second: 163, episode reward: 0.681, mean reward: 0.013 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.391, 10.100], loss: 0.000019, mae: 0.001642, mean_q: 0.758532
 76857/100000: episode: 991, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.820, mean reward: 0.016 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.941, 10.287], loss: 0.000011, mae: 0.001274, mean_q: 0.758394
 76909/100000: episode: 992, duration: 0.302s, episode steps: 52, steps per second: 172, episode reward: 0.767, mean reward: 0.015 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.277, 10.468], loss: 0.000017, mae: 0.001204, mean_q: 0.758290
 76960/100000: episode: 993, duration: 0.310s, episode steps: 51, steps per second: 164, episode reward: 0.720, mean reward: 0.014 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.007, 10.100], loss: 0.000015, mae: 0.001698, mean_q: 0.757826
 77011/100000: episode: 994, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 0.673, mean reward: 0.013 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.832, 10.303], loss: 0.000045, mae: 0.002575, mean_q: 0.758011
 77063/100000: episode: 995, duration: 0.309s, episode steps: 52, steps per second: 168, episode reward: 0.741, mean reward: 0.014 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.521, 10.269], loss: 0.000018, mae: 0.001823, mean_q: 0.757539
 77114/100000: episode: 996, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 0.712, mean reward: 0.014 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.340, 10.275], loss: 0.000019, mae: 0.001721, mean_q: 0.757429
 77165/100000: episode: 997, duration: 0.294s, episode steps: 51, steps per second: 174, episode reward: 0.725, mean reward: 0.014 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.623, 10.225], loss: 0.000014, mae: 0.001299, mean_q: 0.757219
 77216/100000: episode: 998, duration: 0.304s, episode steps: 51, steps per second: 168, episode reward: 0.715, mean reward: 0.014 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.915, 10.149], loss: 0.000007, mae: 0.001119, mean_q: 0.757190
 77269/100000: episode: 999, duration: 0.339s, episode steps: 53, steps per second: 156, episode reward: 0.833, mean reward: 0.016 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-1.130, 10.345], loss: 0.000015, mae: 0.001085, mean_q: 0.757204
 77321/100000: episode: 1000, duration: 0.300s, episode steps: 52, steps per second: 173, episode reward: 0.816, mean reward: 0.016 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.444, 10.200], loss: 0.000018, mae: 0.001304, mean_q: 0.756819
 77374/100000: episode: 1001, duration: 0.294s, episode steps: 53, steps per second: 181, episode reward: 0.831, mean reward: 0.016 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.358, 10.320], loss: 0.000017, mae: 0.001144, mean_q: 0.757033
 77428/100000: episode: 1002, duration: 0.327s, episode steps: 54, steps per second: 165, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.329, 10.255], loss: 0.000026, mae: 0.001752, mean_q: 0.756837
 77479/100000: episode: 1003, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 0.760, mean reward: 0.015 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.284, 10.100], loss: 0.000009, mae: 0.001065, mean_q: 0.756808
 77530/100000: episode: 1004, duration: 0.315s, episode steps: 51, steps per second: 162, episode reward: 0.702, mean reward: 0.014 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.218, 10.295], loss: 0.000025, mae: 0.001587, mean_q: 0.756714
 77581/100000: episode: 1005, duration: 0.284s, episode steps: 51, steps per second: 180, episode reward: 0.756, mean reward: 0.015 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.246, 10.271], loss: 0.000038, mae: 0.003097, mean_q: 0.756602
 77632/100000: episode: 1006, duration: 0.310s, episode steps: 51, steps per second: 164, episode reward: 0.701, mean reward: 0.014 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.457, 10.340], loss: 0.000036, mae: 0.003423, mean_q: 0.756316
 77686/100000: episode: 1007, duration: 0.326s, episode steps: 54, steps per second: 166, episode reward: 0.871, mean reward: 0.016 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.879, 10.220], loss: 0.000028, mae: 0.002107, mean_q: 0.756569
 77737/100000: episode: 1008, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 0.776, mean reward: 0.015 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.355, 10.100], loss: 0.000013, mae: 0.001323, mean_q: 0.755971
 77788/100000: episode: 1009, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.684, mean reward: 0.013 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.229, 10.100], loss: 0.000029, mae: 0.001713, mean_q: 0.755909
 77842/100000: episode: 1010, duration: 0.309s, episode steps: 54, steps per second: 174, episode reward: 0.749, mean reward: 0.014 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-1.001, 10.246], loss: 0.000011, mae: 0.001032, mean_q: 0.755606
 77894/100000: episode: 1011, duration: 0.307s, episode steps: 52, steps per second: 169, episode reward: 0.762, mean reward: 0.015 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.265, 10.230], loss: 0.000015, mae: 0.001053, mean_q: 0.755463
 77947/100000: episode: 1012, duration: 0.316s, episode steps: 53, steps per second: 168, episode reward: 0.718, mean reward: 0.014 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.510, 10.227], loss: 0.000014, mae: 0.001153, mean_q: 0.754984
 78000/100000: episode: 1013, duration: 0.300s, episode steps: 53, steps per second: 176, episode reward: 0.741, mean reward: 0.014 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.972, 10.129], loss: 0.000022, mae: 0.001773, mean_q: 0.754624
 78053/100000: episode: 1014, duration: 0.322s, episode steps: 53, steps per second: 165, episode reward: 0.746, mean reward: 0.014 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.367, 10.336], loss: 0.000017, mae: 0.001993, mean_q: 0.754559
 78104/100000: episode: 1015, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.679, mean reward: 0.013 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.845, 10.204], loss: 0.000013, mae: 0.001224, mean_q: 0.754731
 78155/100000: episode: 1016, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.771, mean reward: 0.015 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.443, 10.100], loss: 0.000023, mae: 0.001532, mean_q: 0.754401
 78206/100000: episode: 1017, duration: 0.301s, episode steps: 51, steps per second: 170, episode reward: 0.750, mean reward: 0.015 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.347, 10.448], loss: 0.000032, mae: 0.001999, mean_q: 0.754027
 78257/100000: episode: 1018, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 0.779, mean reward: 0.015 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.228, 10.322], loss: 0.000017, mae: 0.001045, mean_q: 0.753991
 78308/100000: episode: 1019, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 0.726, mean reward: 0.014 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-1.303, 10.415], loss: 0.000013, mae: 0.001089, mean_q: 0.753621
 78359/100000: episode: 1020, duration: 0.302s, episode steps: 51, steps per second: 169, episode reward: 0.764, mean reward: 0.015 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.534, 10.372], loss: 0.000018, mae: 0.001871, mean_q: 0.753189
 78410/100000: episode: 1021, duration: 0.314s, episode steps: 51, steps per second: 163, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.232, 10.252], loss: 0.000044, mae: 0.002251, mean_q: 0.753778
 78461/100000: episode: 1022, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 0.767, mean reward: 0.015 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.394, 10.171], loss: 0.000017, mae: 0.001422, mean_q: 0.753588
 78514/100000: episode: 1023, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 0.733, mean reward: 0.014 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.340, 10.360], loss: 0.000022, mae: 0.001872, mean_q: 0.753094
 78565/100000: episode: 1024, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 0.741, mean reward: 0.015 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.953, 10.207], loss: 0.000023, mae: 0.001860, mean_q: 0.753114
 78617/100000: episode: 1025, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 0.665, mean reward: 0.013 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.492, 10.190], loss: 0.000016, mae: 0.001359, mean_q: 0.752491
 78670/100000: episode: 1026, duration: 0.287s, episode steps: 53, steps per second: 184, episode reward: 0.844, mean reward: 0.016 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.774, 10.393], loss: 0.000017, mae: 0.001410, mean_q: 0.752502
 78724/100000: episode: 1027, duration: 0.307s, episode steps: 54, steps per second: 176, episode reward: 0.838, mean reward: 0.016 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.335, 10.100], loss: 0.000016, mae: 0.001567, mean_q: 0.752441
 78775/100000: episode: 1028, duration: 0.292s, episode steps: 51, steps per second: 174, episode reward: 0.677, mean reward: 0.013 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.961, 10.191], loss: 0.000022, mae: 0.002008, mean_q: 0.752234
 78829/100000: episode: 1029, duration: 0.326s, episode steps: 54, steps per second: 166, episode reward: 0.766, mean reward: 0.014 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.875, 10.160], loss: 0.000022, mae: 0.001958, mean_q: 0.752052
 78880/100000: episode: 1030, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 0.772, mean reward: 0.015 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.780, 10.184], loss: 0.000013, mae: 0.001618, mean_q: 0.751704
 78933/100000: episode: 1031, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 0.741, mean reward: 0.014 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-1.245, 10.213], loss: 0.000020, mae: 0.001518, mean_q: 0.751837
 78984/100000: episode: 1032, duration: 0.296s, episode steps: 51, steps per second: 173, episode reward: 0.756, mean reward: 0.015 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.318, 10.100], loss: 0.000019, mae: 0.001249, mean_q: 0.751423
 79037/100000: episode: 1033, duration: 0.302s, episode steps: 53, steps per second: 175, episode reward: 0.742, mean reward: 0.014 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-1.268, 10.100], loss: 0.000021, mae: 0.001591, mean_q: 0.751097
 79088/100000: episode: 1034, duration: 0.283s, episode steps: 51, steps per second: 181, episode reward: 0.690, mean reward: 0.014 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.812, 10.154], loss: 0.000030, mae: 0.001985, mean_q: 0.751411
 79140/100000: episode: 1035, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 0.680, mean reward: 0.013 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.144, 10.307], loss: 0.000020, mae: 0.002094, mean_q: 0.751036
 79191/100000: episode: 1036, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.739, mean reward: 0.014 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.401, 10.100], loss: 0.000018, mae: 0.001601, mean_q: 0.750668
 79242/100000: episode: 1037, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.861, mean reward: 0.017 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.335, 10.557], loss: 0.000014, mae: 0.001435, mean_q: 0.750647
 79295/100000: episode: 1038, duration: 0.288s, episode steps: 53, steps per second: 184, episode reward: 0.771, mean reward: 0.015 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.586, 10.355], loss: 0.000016, mae: 0.001505, mean_q: 0.750506
 79346/100000: episode: 1039, duration: 0.291s, episode steps: 51, steps per second: 176, episode reward: 0.670, mean reward: 0.013 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.979, 10.222], loss: 0.000025, mae: 0.001947, mean_q: 0.750484
 79399/100000: episode: 1040, duration: 0.296s, episode steps: 53, steps per second: 179, episode reward: 0.783, mean reward: 0.015 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.608, 10.313], loss: 0.000025, mae: 0.001914, mean_q: 0.750282
 79450/100000: episode: 1041, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 0.751, mean reward: 0.015 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.216, 10.100], loss: 0.000024, mae: 0.001491, mean_q: 0.749928
 79503/100000: episode: 1042, duration: 0.298s, episode steps: 53, steps per second: 178, episode reward: 0.810, mean reward: 0.015 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.579, 10.228], loss: 0.000030, mae: 0.002083, mean_q: 0.750121
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7649322748184204
1
 79556/100000: episode: 1043, duration: 4.491s, episode steps: 53, steps per second: 12, episode reward: 0.758, mean reward: 0.014 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.581, 10.323], loss: 0.000023, mae: 0.002169, mean_q: 0.749897
 79657/100000: episode: 1044, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.923, 10.352], loss: 0.000034, mae: 0.002255, mean_q: 0.750025
 79758/100000: episode: 1045, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.985, 10.100], loss: 0.000015, mae: 0.001473, mean_q: 0.749664
 79859/100000: episode: 1046, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.151, 10.100], loss: 0.000022, mae: 0.001353, mean_q: 0.749558
 79960/100000: episode: 1047, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.279], loss: 0.000023, mae: 0.001714, mean_q: 0.749307
 80061/100000: episode: 1048, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.627, 10.383], loss: 0.000014, mae: 0.001888, mean_q: 0.749159
 80162/100000: episode: 1049, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.607, 10.442], loss: 0.000026, mae: 0.001873, mean_q: 0.749704
 80263/100000: episode: 1050, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.487, 10.100], loss: 0.000017, mae: 0.001442, mean_q: 0.749447
 80364/100000: episode: 1051, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.578, 10.100], loss: 0.000017, mae: 0.001446, mean_q: 0.749760
 80465/100000: episode: 1052, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.618, 10.230], loss: 0.000024, mae: 0.001810, mean_q: 0.749480
 80566/100000: episode: 1053, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.678, 10.100], loss: 0.000025, mae: 0.002148, mean_q: 0.749813
 80667/100000: episode: 1054, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.393, 10.129], loss: 0.000024, mae: 0.002238, mean_q: 0.749751
 80768/100000: episode: 1055, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.050, 10.100], loss: 0.000021, mae: 0.001815, mean_q: 0.749987
 80869/100000: episode: 1056, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.645, 10.208], loss: 0.000023, mae: 0.001645, mean_q: 0.750125
 80970/100000: episode: 1057, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.882, mean reward: 0.009 [0.000, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.540, 10.486], loss: 0.000031, mae: 0.002009, mean_q: 0.750326
 81071/100000: episode: 1058, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.816, 10.100], loss: 0.000023, mae: 0.001667, mean_q: 0.750087
 81172/100000: episode: 1059, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.406, 10.218], loss: 0.000019, mae: 0.001095, mean_q: 0.750051
 81273/100000: episode: 1060, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.569, 10.326], loss: 0.000028, mae: 0.001783, mean_q: 0.750286
 81374/100000: episode: 1061, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.928, 10.221], loss: 0.000020, mae: 0.001633, mean_q: 0.750503
 81475/100000: episode: 1062, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.232, 10.100], loss: 0.000027, mae: 0.002448, mean_q: 0.750802
 81576/100000: episode: 1063, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.231, 10.100], loss: 0.000025, mae: 0.001691, mean_q: 0.750902
 81677/100000: episode: 1064, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.498, 10.304], loss: 0.000025, mae: 0.001746, mean_q: 0.750789
 81778/100000: episode: 1065, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.235, 10.100], loss: 0.000022, mae: 0.001627, mean_q: 0.750635
 81879/100000: episode: 1066, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.346, 10.531], loss: 0.000021, mae: 0.002149, mean_q: 0.750935
 81980/100000: episode: 1067, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.017, 10.146], loss: 0.000024, mae: 0.002198, mean_q: 0.751202
 82081/100000: episode: 1068, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.178], loss: 0.000022, mae: 0.001187, mean_q: 0.751500
 82182/100000: episode: 1069, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.047, 10.151], loss: 0.000018, mae: 0.001324, mean_q: 0.751441
 82283/100000: episode: 1070, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.799, 10.100], loss: 0.000022, mae: 0.001830, mean_q: 0.751774
 82384/100000: episode: 1071, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.856, 10.312], loss: 0.000013, mae: 0.001286, mean_q: 0.751873
 82485/100000: episode: 1072, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.669, 10.100], loss: 0.000016, mae: 0.001275, mean_q: 0.751963
 82586/100000: episode: 1073, duration: 0.536s, episode steps: 101, steps per second: 189, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.372, 10.181], loss: 0.000010, mae: 0.001143, mean_q: 0.751932
 82687/100000: episode: 1074, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.433, 10.267], loss: 0.000010, mae: 0.001327, mean_q: 0.751677
 82788/100000: episode: 1075, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.001, 10.235], loss: 0.000013, mae: 0.001210, mean_q: 0.751657
 82889/100000: episode: 1076, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.823, 10.109], loss: 0.000016, mae: 0.001428, mean_q: 0.751805
 82990/100000: episode: 1077, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.493, 10.185], loss: 0.000019, mae: 0.001889, mean_q: 0.752075
 83091/100000: episode: 1078, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.681, 10.338], loss: 0.000018, mae: 0.001690, mean_q: 0.752058
 83192/100000: episode: 1079, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.859, 10.100], loss: 0.000013, mae: 0.001328, mean_q: 0.752052
 83293/100000: episode: 1080, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.996, 10.100], loss: 0.000019, mae: 0.001412, mean_q: 0.752194
 83394/100000: episode: 1081, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.665, mean reward: 0.007 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.554, 10.100], loss: 0.000009, mae: 0.001059, mean_q: 0.752320
 83495/100000: episode: 1082, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.198, 10.153], loss: 0.000016, mae: 0.001759, mean_q: 0.752231
 83596/100000: episode: 1083, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.319], loss: 0.000017, mae: 0.001970, mean_q: 0.752034
 83697/100000: episode: 1084, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.940, mean reward: 0.009 [0.000, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.035, 10.567], loss: 0.000025, mae: 0.001857, mean_q: 0.752345
 83798/100000: episode: 1085, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.866, 10.100], loss: 0.000019, mae: 0.001534, mean_q: 0.752696
 83899/100000: episode: 1086, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.794, 10.369], loss: 0.000010, mae: 0.001153, mean_q: 0.752697
 84000/100000: episode: 1087, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.126], loss: 0.000017, mae: 0.001035, mean_q: 0.752841
 84101/100000: episode: 1088, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.120, 10.171], loss: 0.000022, mae: 0.001287, mean_q: 0.753037
 84202/100000: episode: 1089, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.889, 10.100], loss: 0.000010, mae: 0.001217, mean_q: 0.753347
 84303/100000: episode: 1090, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.268, 10.100], loss: 0.000024, mae: 0.001795, mean_q: 0.753445
 84404/100000: episode: 1091, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.880, 10.100], loss: 0.000013, mae: 0.001186, mean_q: 0.753687
 84505/100000: episode: 1092, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.305, 10.100], loss: 0.000024, mae: 0.001615, mean_q: 0.753985
 84606/100000: episode: 1093, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.790, 10.151], loss: 0.000018, mae: 0.001180, mean_q: 0.754111
 84707/100000: episode: 1094, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.685, 10.150], loss: 0.000013, mae: 0.001700, mean_q: 0.754076
 84808/100000: episode: 1095, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.795, 10.113], loss: 0.000016, mae: 0.001152, mean_q: 0.754206
 84909/100000: episode: 1096, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.458, 10.100], loss: 0.000018, mae: 0.001653, mean_q: 0.754356
 85010/100000: episode: 1097, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.950, mean reward: 0.009 [0.000, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.637, 10.699], loss: 0.000018, mae: 0.001264, mean_q: 0.754756
 85111/100000: episode: 1098, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.475, 10.457], loss: 0.000018, mae: 0.001424, mean_q: 0.754742
 85212/100000: episode: 1099, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.051, 10.100], loss: 0.000015, mae: 0.001516, mean_q: 0.754911
 85313/100000: episode: 1100, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.238, 10.100], loss: 0.000027, mae: 0.001735, mean_q: 0.755097
 85414/100000: episode: 1101, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.657, 10.269], loss: 0.000029, mae: 0.001874, mean_q: 0.755524
 85515/100000: episode: 1102, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.844, 10.173], loss: 0.000025, mae: 0.001692, mean_q: 0.755452
 85616/100000: episode: 1103, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.598, 10.100], loss: 0.000024, mae: 0.001413, mean_q: 0.755819
 85717/100000: episode: 1104, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.639, 10.123], loss: 0.000020, mae: 0.001665, mean_q: 0.755966
 85818/100000: episode: 1105, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.209, 10.100], loss: 0.000016, mae: 0.001306, mean_q: 0.756057
 85919/100000: episode: 1106, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.597, 10.149], loss: 0.000023, mae: 0.001662, mean_q: 0.756451
 86020/100000: episode: 1107, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.409, 10.193], loss: 0.000013, mae: 0.001574, mean_q: 0.756447
 86121/100000: episode: 1108, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.046, 10.289], loss: 0.000025, mae: 0.001724, mean_q: 0.756518
 86222/100000: episode: 1109, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.281, 10.164], loss: 0.000022, mae: 0.001695, mean_q: 0.756743
 86323/100000: episode: 1110, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.314, 10.100], loss: 0.000009, mae: 0.001127, mean_q: 0.756996
 86424/100000: episode: 1111, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.481, 10.100], loss: 0.000015, mae: 0.001590, mean_q: 0.757041
 86525/100000: episode: 1112, duration: 0.735s, episode steps: 101, steps per second: 137, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.602, 10.245], loss: 0.000016, mae: 0.001395, mean_q: 0.756894
 86626/100000: episode: 1113, duration: 0.758s, episode steps: 101, steps per second: 133, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.174, 10.100], loss: 0.000014, mae: 0.001139, mean_q: 0.756898
 86727/100000: episode: 1114, duration: 0.896s, episode steps: 101, steps per second: 113, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-3.006, 10.100], loss: 0.000006, mae: 0.000672, mean_q: 0.757110
 86828/100000: episode: 1115, duration: 1.043s, episode steps: 101, steps per second: 97, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.136, 10.395], loss: 0.000010, mae: 0.000991, mean_q: 0.756939
 86929/100000: episode: 1116, duration: 0.963s, episode steps: 101, steps per second: 105, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.036, 10.100], loss: 0.000013, mae: 0.001092, mean_q: 0.757223
 87030/100000: episode: 1117, duration: 1.310s, episode steps: 101, steps per second: 77, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.642, 10.100], loss: 0.000008, mae: 0.001103, mean_q: 0.757042
 87131/100000: episode: 1118, duration: 0.993s, episode steps: 101, steps per second: 102, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.913, 10.100], loss: 0.000016, mae: 0.001072, mean_q: 0.757129
 87232/100000: episode: 1119, duration: 0.817s, episode steps: 101, steps per second: 124, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.475, 10.100], loss: 0.000024, mae: 0.001723, mean_q: 0.757266
 87333/100000: episode: 1120, duration: 0.996s, episode steps: 101, steps per second: 101, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.205], loss: 0.000021, mae: 0.001966, mean_q: 0.757177
 87434/100000: episode: 1121, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.723, 10.232], loss: 0.000010, mae: 0.000965, mean_q: 0.757010
 87535/100000: episode: 1122, duration: 1.266s, episode steps: 101, steps per second: 80, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.219, 10.214], loss: 0.000017, mae: 0.001129, mean_q: 0.757263
 87636/100000: episode: 1123, duration: 0.736s, episode steps: 101, steps per second: 137, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.100], loss: 0.000009, mae: 0.000943, mean_q: 0.757145
 87737/100000: episode: 1124, duration: 0.733s, episode steps: 101, steps per second: 138, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.272, 10.446], loss: 0.000022, mae: 0.001538, mean_q: 0.757286
 87838/100000: episode: 1125, duration: 0.744s, episode steps: 101, steps per second: 136, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.801, 10.234], loss: 0.000014, mae: 0.001105, mean_q: 0.757110
 87939/100000: episode: 1126, duration: 0.895s, episode steps: 101, steps per second: 113, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.119, 10.100], loss: 0.000011, mae: 0.001171, mean_q: 0.757183
 88040/100000: episode: 1127, duration: 1.183s, episode steps: 101, steps per second: 85, episode reward: 0.870, mean reward: 0.009 [0.000, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.100], loss: 0.000027, mae: 0.001616, mean_q: 0.757430
 88141/100000: episode: 1128, duration: 1.196s, episode steps: 101, steps per second: 84, episode reward: 0.931, mean reward: 0.009 [0.000, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.206, 10.170], loss: 0.000009, mae: 0.001235, mean_q: 0.757770
 88242/100000: episode: 1129, duration: 1.014s, episode steps: 101, steps per second: 100, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.046, 10.288], loss: 0.000027, mae: 0.002087, mean_q: 0.757999
 88343/100000: episode: 1130, duration: 0.702s, episode steps: 101, steps per second: 144, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.352, 10.287], loss: 0.000014, mae: 0.001229, mean_q: 0.757955
 88444/100000: episode: 1131, duration: 1.089s, episode steps: 101, steps per second: 93, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.577, 10.100], loss: 0.000023, mae: 0.001407, mean_q: 0.758228
 88545/100000: episode: 1132, duration: 0.735s, episode steps: 101, steps per second: 137, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.623, 10.100], loss: 0.000014, mae: 0.001221, mean_q: 0.757969
 88646/100000: episode: 1133, duration: 0.969s, episode steps: 101, steps per second: 104, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.016, 10.145], loss: 0.000011, mae: 0.001197, mean_q: 0.758018
 88747/100000: episode: 1134, duration: 0.698s, episode steps: 101, steps per second: 145, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.302, 10.193], loss: 0.000010, mae: 0.001200, mean_q: 0.758084
 88848/100000: episode: 1135, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.661, 10.100], loss: 0.000012, mae: 0.001154, mean_q: 0.758079
 88949/100000: episode: 1136, duration: 0.966s, episode steps: 101, steps per second: 105, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.454, 10.100], loss: 0.000018, mae: 0.001163, mean_q: 0.758254
 89050/100000: episode: 1137, duration: 0.835s, episode steps: 101, steps per second: 121, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.125, 10.100], loss: 0.000016, mae: 0.001408, mean_q: 0.758499
 89151/100000: episode: 1138, duration: 1.111s, episode steps: 101, steps per second: 91, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.200, 10.100], loss: 0.000019, mae: 0.001530, mean_q: 0.758559
 89252/100000: episode: 1139, duration: 0.769s, episode steps: 101, steps per second: 131, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.994, 10.228], loss: 0.000017, mae: 0.001440, mean_q: 0.758785
 89353/100000: episode: 1140, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.213, 10.100], loss: 0.000013, mae: 0.001281, mean_q: 0.758983
 89454/100000: episode: 1141, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.628, 10.247], loss: 0.000020, mae: 0.001493, mean_q: 0.759571
 89555/100000: episode: 1142, duration: 0.691s, episode steps: 101, steps per second: 146, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.005, 10.110], loss: 0.000021, mae: 0.001636, mean_q: 0.759596
[Info] 1-TH LEVEL FOUND: 0.7616432309150696, Considering 10/100 traces
 89656/100000: episode: 1143, duration: 6.381s, episode steps: 101, steps per second: 16, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.090, 10.223], loss: 0.000014, mae: 0.001446, mean_q: 0.759572
 89658/100000: episode: 1144, duration: 0.041s, episode steps: 2, steps per second: 49, episode reward: 0.917, mean reward: 0.458 [0.000, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.654], loss: 0.000000, mae: 0.000409, mean_q: 0.759821
 89659/100000: episode: 1145, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.631, mean reward: 0.631 [0.631, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.116, 10.200], loss: 0.000000, mae: 0.000427, mean_q: 0.760132
 89660/100000: episode: 1146, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.070, 10.200], loss: 0.000000, mae: 0.000170, mean_q: 0.759110
 89661/100000: episode: 1147, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.574, mean reward: 0.574 [0.574, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.200], loss: 0.000027, mae: 0.001366, mean_q: 0.759575
 89662/100000: episode: 1148, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.106, 10.200], loss: 0.000033, mae: 0.002224, mean_q: 0.759089
 89663/100000: episode: 1149, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.679, mean reward: 0.679 [0.679, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.084, 10.200], loss: 0.000000, mae: 0.000124, mean_q: 0.759607
 89664/100000: episode: 1150, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.570, mean reward: 0.570 [0.570, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.070, 10.228], loss: 0.000012, mae: 0.001329, mean_q: 0.760036
 89665/100000: episode: 1151, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.544, mean reward: 0.544 [0.544, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.439, 10.200], loss: 0.000000, mae: 0.000116, mean_q: 0.759641
 89666/100000: episode: 1152, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.532, mean reward: 0.532 [0.532, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.070, 10.200], loss: 0.000000, mae: 0.000311, mean_q: 0.759653
 89667/100000: episode: 1153, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.562, mean reward: 0.562 [0.562, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.070, 10.200], loss: 0.000000, mae: 0.000306, mean_q: 0.759260
 89668/100000: episode: 1154, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.622, mean reward: 0.622 [0.622, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.131, 10.242], loss: 0.000000, mae: 0.000080, mean_q: 0.759795
 89669/100000: episode: 1155, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.650, mean reward: 0.650 [0.650, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.359], loss: 0.000000, mae: 0.000329, mean_q: 0.759324
 89670/100000: episode: 1156, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.602, mean reward: 0.602 [0.602, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.070, 10.286], loss: 0.000017, mae: 0.003733, mean_q: 0.756697
 89671/100000: episode: 1157, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.592, mean reward: 0.592 [0.592, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.070, 10.200], loss: 0.000007, mae: 0.003808, mean_q: 0.755624
 89672/100000: episode: 1158, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.070, 10.264], loss: 0.000003, mae: 0.002361, mean_q: 0.756963
 89673/100000: episode: 1159, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.714, mean reward: 0.714 [0.714, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.109, 10.206], loss: 0.000000, mae: 0.000553, mean_q: 0.759445
 89674/100000: episode: 1160, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.126, 10.242], loss: 0.000002, mae: 0.001766, mean_q: 0.761520
 89675/100000: episode: 1161, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.610, mean reward: 0.610 [0.610, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.070, 10.288], loss: 0.000002, mae: 0.001755, mean_q: 0.761302
 89676/100000: episode: 1162, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.619, mean reward: 0.619 [0.619, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.077, 10.234], loss: 0.000000, mae: 0.000624, mean_q: 0.759705
 89677/100000: episode: 1163, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.585, mean reward: 0.585 [0.585, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.070, 10.242], loss: 0.000001, mae: 0.001105, mean_q: 0.758152
 89678/100000: episode: 1164, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.544, mean reward: 0.544 [0.544, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.070, 10.200], loss: 0.000001, mae: 0.001408, mean_q: 0.758070
 89679/100000: episode: 1165, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.602, mean reward: 0.602 [0.602, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.070, 10.200], loss: 0.000000, mae: 0.000640, mean_q: 0.759036
 89680/100000: episode: 1166, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.808, mean reward: 0.808 [0.808, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.106, 10.251], loss: 0.000000, mae: 0.000736, mean_q: 0.758903
 89681/100000: episode: 1167, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.070, 10.210], loss: 0.000001, mae: 0.001444, mean_q: 0.758262
 89683/100000: episode: 1168, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.865, mean reward: 0.433 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.645], loss: 0.000308, mae: 0.003717, mean_q: 0.759368
 89684/100000: episode: 1169, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.635, mean reward: 0.635 [0.635, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.070, 10.270], loss: 0.000001, mae: 0.001107, mean_q: 0.757959
 89685/100000: episode: 1170, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.070, 10.219], loss: 0.000382, mae: 0.006241, mean_q: 0.758006
 89686/100000: episode: 1171, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.381], loss: 0.000003, mae: 0.002429, mean_q: 0.756943
 89688/100000: episode: 1172, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.774, mean reward: 0.387 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.541], loss: 0.000204, mae: 0.003794, mean_q: 0.758671
 89689/100000: episode: 1173, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.610, mean reward: 0.610 [0.610, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.070, 10.290], loss: 0.000000, mae: 0.000553, mean_q: 0.759166
 89690/100000: episode: 1174, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.575, mean reward: 0.575 [0.575, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.070, 10.226], loss: 0.000000, mae: 0.000569, mean_q: 0.759248
 89691/100000: episode: 1175, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.592, mean reward: 0.592 [0.592, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.223, 10.226], loss: 0.000013, mae: 0.001599, mean_q: 0.759603
 89692/100000: episode: 1176, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.588, mean reward: 0.588 [0.588, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.107, 10.237], loss: 0.000009, mae: 0.001094, mean_q: 0.759575
 89694/100000: episode: 1177, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 0.731, mean reward: 0.366 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.485], loss: 0.000004, mae: 0.000700, mean_q: 0.759419
 89695/100000: episode: 1178, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.435], loss: 0.000000, mae: 0.000259, mean_q: 0.759425
 89696/100000: episode: 1179, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.701, mean reward: 0.701 [0.701, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.146, 10.200], loss: 0.000001, mae: 0.000971, mean_q: 0.758134
 89697/100000: episode: 1180, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.544, mean reward: 0.544 [0.544, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.070, 10.200], loss: 0.000301, mae: 0.006326, mean_q: 0.757099
 89698/100000: episode: 1181, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.070, 10.284], loss: 0.000004, mae: 0.002863, mean_q: 0.756104
 89699/100000: episode: 1182, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.599, mean reward: 0.599 [0.599, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.162, 10.200], loss: 0.000001, mae: 0.001181, mean_q: 0.758132
 89700/100000: episode: 1183, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.529, mean reward: 0.529 [0.529, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.070, 10.200], loss: 0.000055, mae: 0.003261, mean_q: 0.760361
 89701/100000: episode: 1184, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.549, mean reward: 0.549 [0.549, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.108, 10.200], loss: 0.000258, mae: 0.006652, mean_q: 0.762110
 89702/100000: episode: 1185, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.538, mean reward: 0.538 [0.538, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.114, 10.200], loss: 0.000003, mae: 0.002222, mean_q: 0.756967
 89704/100000: episode: 1186, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.817, mean reward: 0.409 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.590], loss: 0.000037, mae: 0.008593, mean_q: 0.750589
 89705/100000: episode: 1187, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.616, mean reward: 0.616 [0.616, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.070, 10.262], loss: 0.000015, mae: 0.005187, mean_q: 0.753884
 89706/100000: episode: 1188, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.070, 10.321], loss: 0.000507, mae: 0.007692, mean_q: 0.760215
 89707/100000: episode: 1189, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.688, 10.422], loss: 0.000766, mae: 0.011528, mean_q: 0.762813
 89708/100000: episode: 1190, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.615, mean reward: 0.615 [0.615, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.070, 10.211], loss: 0.000000, mae: 0.000491, mean_q: 0.759216
 89709/100000: episode: 1191, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.070, 10.295], loss: 0.000044, mae: 0.003756, mean_q: 0.756531
 89710/100000: episode: 1192, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.652, mean reward: 0.652 [0.652, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.070, 10.283], loss: 0.000015, mae: 0.003942, mean_q: 0.755535
 89711/100000: episode: 1193, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.531, mean reward: 0.531 [0.531, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.070, 10.200], loss: 0.000227, mae: 0.008824, mean_q: 0.755491
 89712/100000: episode: 1194, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.070, 10.200], loss: 0.000006, mae: 0.003406, mean_q: 0.755502
 89713/100000: episode: 1195, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.575, mean reward: 0.575 [0.575, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.070, 10.200], loss: 0.000000, mae: 0.000622, mean_q: 0.758507
 89714/100000: episode: 1196, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.558, mean reward: 0.558 [0.558, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.070, 10.208], loss: 0.000548, mae: 0.008168, mean_q: 0.761010
 89715/100000: episode: 1197, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.569, mean reward: 0.569 [0.569, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.070, 10.221], loss: 0.000001, mae: 0.001147, mean_q: 0.757939
 89716/100000: episode: 1198, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.561, mean reward: 0.561 [0.561, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.070, 10.200], loss: 0.000004, mae: 0.002703, mean_q: 0.756109
 89717/100000: episode: 1199, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.617, mean reward: 0.617 [0.617, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.070, 10.244], loss: 0.000002, mae: 0.001562, mean_q: 0.757095
 89718/100000: episode: 1200, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.070, 10.350], loss: 0.000011, mae: 0.001802, mean_q: 0.757961
 89719/100000: episode: 1201, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.614, mean reward: 0.614 [0.614, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.133, 10.260], loss: 0.000002, mae: 0.001835, mean_q: 0.756985
 89720/100000: episode: 1202, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.553, mean reward: 0.553 [0.553, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.070, 10.200], loss: 0.000052, mae: 0.004875, mean_q: 0.755464
 89721/100000: episode: 1203, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.716, mean reward: 0.716 [0.716, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.070, 10.214], loss: 0.000052, mae: 0.005610, mean_q: 0.754421
 89722/100000: episode: 1204, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.722, mean reward: 0.722 [0.722, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.134, 10.271], loss: 0.000003, mae: 0.002048, mean_q: 0.756646
 89723/100000: episode: 1205, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.070, 10.200], loss: 0.000003, mae: 0.002183, mean_q: 0.760668
 89724/100000: episode: 1206, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.509, mean reward: 0.509 [0.509, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.070, 10.200], loss: 0.000010, mae: 0.004427, mean_q: 0.763068
 89725/100000: episode: 1207, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.694, mean reward: 0.694 [0.694, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.433], loss: 0.000015, mae: 0.005355, mean_q: 0.764175
 89726/100000: episode: 1208, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.625, mean reward: 0.625 [0.625, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.200], loss: 0.000580, mae: 0.010638, mean_q: 0.761484
 89727/100000: episode: 1209, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.070, 10.200], loss: 0.000003, mae: 0.002482, mean_q: 0.756498
 89728/100000: episode: 1210, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.749, mean reward: 0.749 [0.749, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.506], loss: 0.000018, mae: 0.005875, mean_q: 0.753079
 89729/100000: episode: 1211, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.743, mean reward: 0.743 [0.743, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.070, 10.223], loss: 0.000014, mae: 0.003548, mean_q: 0.755883
 89730/100000: episode: 1212, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.417], loss: 0.000700, mae: 0.008712, mean_q: 0.760325
 89731/100000: episode: 1213, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.600, mean reward: 0.600 [0.600, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.070, 10.282], loss: 0.000001, mae: 0.001483, mean_q: 0.760305
 89732/100000: episode: 1214, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.575, mean reward: 0.575 [0.575, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.115, 10.229], loss: 0.000425, mae: 0.006562, mean_q: 0.757395
 89733/100000: episode: 1215, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.584, mean reward: 0.584 [0.584, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.070, 10.200], loss: 0.000500, mae: 0.011069, mean_q: 0.752992
 89734/100000: episode: 1216, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.531, mean reward: 0.531 [0.531, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.070, 10.200], loss: 0.000033, mae: 0.007427, mean_q: 0.751686
 89736/100000: episode: 1217, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.737, mean reward: 0.368 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.586, 10.484], loss: 0.000399, mae: 0.004957, mean_q: 0.757005
 89737/100000: episode: 1218, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.583, mean reward: 0.583 [0.583, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.070, 10.254], loss: 0.000001, mae: 0.001253, mean_q: 0.759547
 89738/100000: episode: 1219, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.583, mean reward: 0.583 [0.583, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.200], loss: 0.000001, mae: 0.001287, mean_q: 0.759393
 89739/100000: episode: 1220, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.607, mean reward: 0.607 [0.607, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.070, 10.200], loss: 0.000512, mae: 0.008175, mean_q: 0.758965
 89740/100000: episode: 1221, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.577, mean reward: 0.577 [0.577, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.070, 10.200], loss: 0.000002, mae: 0.001841, mean_q: 0.756624
 89741/100000: episode: 1222, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.817, mean reward: 0.817 [0.817, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.070, 10.200], loss: 0.000257, mae: 0.009775, mean_q: 0.753091
 89742/100000: episode: 1223, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.101, 10.200], loss: 0.000047, mae: 0.008568, mean_q: 0.750418
 89743/100000: episode: 1224, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.528, mean reward: 0.528 [0.528, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.070, 10.200], loss: 0.000019, mae: 0.005589, mean_q: 0.752630
 89744/100000: episode: 1225, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.523, mean reward: 0.523 [0.523, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.070, 10.200], loss: 0.000018, mae: 0.002818, mean_q: 0.759305
 89746/100000: episode: 1226, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.757, mean reward: 0.379 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.520], loss: 0.000015, mae: 0.005246, mean_q: 0.763509
 89747/100000: episode: 1227, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.615, mean reward: 0.615 [0.615, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.070, 10.300], loss: 0.000005, mae: 0.002760, mean_q: 0.760087
 89748/100000: episode: 1228, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.735, mean reward: 0.735 [0.735, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.070, 10.200], loss: 0.000004, mae: 0.001835, mean_q: 0.756798
 89749/100000: episode: 1229, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.661, mean reward: 0.661 [0.661, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.070, 10.202], loss: 0.000011, mae: 0.003976, mean_q: 0.754400
 89751/100000: episode: 1230, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.844, mean reward: 0.422 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.620], loss: 0.000503, mae: 0.007324, mean_q: 0.756404
 89752/100000: episode: 1231, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.599, mean reward: 0.599 [0.599, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.070, 10.248], loss: 0.000006, mae: 0.002946, mean_q: 0.755426
 89754/100000: episode: 1232, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.708, mean reward: 0.354 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.441], loss: 0.000125, mae: 0.004736, mean_q: 0.755285
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7616432309150696
1
 89755/100000: episode: 1233, duration: 4.346s, episode steps: 1, steps per second: 0, episode reward: 0.611, mean reward: 0.611 [0.611, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.070, 10.226], loss: 0.000012, mae: 0.002915, mean_q: 0.755635
 89856/100000: episode: 1234, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.824, 10.100], loss: 0.000200, mae: 0.006948, mean_q: 0.756245
 89957/100000: episode: 1235, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.522, 10.278], loss: 0.000153, mae: 0.006100, mean_q: 0.755418
 90058/100000: episode: 1236, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.913, 10.100], loss: 0.000151, mae: 0.006414, mean_q: 0.754886
 90159/100000: episode: 1237, duration: 0.629s, episode steps: 101, steps per second: 160, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.629, 10.100], loss: 0.000144, mae: 0.006684, mean_q: 0.753837
 90260/100000: episode: 1238, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.705, 10.400], loss: 0.000185, mae: 0.008037, mean_q: 0.753028
 90361/100000: episode: 1239, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.379, 10.205], loss: 0.000142, mae: 0.007069, mean_q: 0.752846
 90462/100000: episode: 1240, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.332, 10.100], loss: 0.000137, mae: 0.005867, mean_q: 0.754498
 90563/100000: episode: 1241, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.656, 10.100], loss: 0.000132, mae: 0.006720, mean_q: 0.752295
 90664/100000: episode: 1242, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.991, 10.238], loss: 0.000123, mae: 0.005711, mean_q: 0.753089
 90765/100000: episode: 1243, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.289, 10.287], loss: 0.000119, mae: 0.005218, mean_q: 0.752730
 90866/100000: episode: 1244, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.161, 10.100], loss: 0.000138, mae: 0.005298, mean_q: 0.751836
 90967/100000: episode: 1245, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.476, 10.160], loss: 0.000158, mae: 0.006089, mean_q: 0.751549
 91068/100000: episode: 1246, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.830, 10.100], loss: 0.000121, mae: 0.005218, mean_q: 0.750855
 91169/100000: episode: 1247, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.853, 10.181], loss: 0.000136, mae: 0.005739, mean_q: 0.750405
 91270/100000: episode: 1248, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.072, 10.100], loss: 0.000143, mae: 0.005962, mean_q: 0.751728
 91371/100000: episode: 1249, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.345, 10.511], loss: 0.000095, mae: 0.004973, mean_q: 0.750320
 91472/100000: episode: 1250, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.516, 10.425], loss: 0.000135, mae: 0.005362, mean_q: 0.750194
 91573/100000: episode: 1251, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.130, 10.100], loss: 0.000116, mae: 0.005135, mean_q: 0.749041
 91674/100000: episode: 1252, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.100], loss: 0.000161, mae: 0.005950, mean_q: 0.748416
 91775/100000: episode: 1253, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.100], loss: 0.000139, mae: 0.005335, mean_q: 0.749357
 91876/100000: episode: 1254, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.700, 10.287], loss: 0.000121, mae: 0.005118, mean_q: 0.747781
 91977/100000: episode: 1255, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.841, mean reward: 0.008 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.313, 10.226], loss: 0.000112, mae: 0.005150, mean_q: 0.747854
 92078/100000: episode: 1256, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.287, 10.320], loss: 0.000127, mae: 0.005742, mean_q: 0.747850
 92179/100000: episode: 1257, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.084, 10.130], loss: 0.000119, mae: 0.005220, mean_q: 0.747206
 92280/100000: episode: 1258, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.693, 10.317], loss: 0.000129, mae: 0.005072, mean_q: 0.746219
 92381/100000: episode: 1259, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.599, 10.100], loss: 0.000138, mae: 0.005483, mean_q: 0.746174
 92482/100000: episode: 1260, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.912, 10.171], loss: 0.000148, mae: 0.005979, mean_q: 0.745691
 92583/100000: episode: 1261, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.069, 10.100], loss: 0.000170, mae: 0.007248, mean_q: 0.745366
 92684/100000: episode: 1262, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.220, 10.159], loss: 0.000175, mae: 0.006368, mean_q: 0.744827
 92785/100000: episode: 1263, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.123, 10.100], loss: 0.000136, mae: 0.005438, mean_q: 0.745203
 92886/100000: episode: 1264, duration: 0.736s, episode steps: 101, steps per second: 137, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.036, 10.100], loss: 0.000142, mae: 0.005406, mean_q: 0.744460
 92987/100000: episode: 1265, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.744, 10.166], loss: 0.000144, mae: 0.005677, mean_q: 0.743645
 93088/100000: episode: 1266, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.515, 10.224], loss: 0.000123, mae: 0.005135, mean_q: 0.743472
 93189/100000: episode: 1267, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.730, 10.113], loss: 0.000145, mae: 0.005469, mean_q: 0.743987
 93290/100000: episode: 1268, duration: 0.547s, episode steps: 101, steps per second: 184, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.809, 10.131], loss: 0.000122, mae: 0.005092, mean_q: 0.742639
 93391/100000: episode: 1269, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.516, 10.100], loss: 0.000112, mae: 0.005026, mean_q: 0.741855
 93492/100000: episode: 1270, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.259, 10.490], loss: 0.000141, mae: 0.005777, mean_q: 0.741906
 93593/100000: episode: 1271, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.811, 10.313], loss: 0.000119, mae: 0.005062, mean_q: 0.742110
 93694/100000: episode: 1272, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.975, 10.100], loss: 0.000112, mae: 0.004444, mean_q: 0.742677
 93795/100000: episode: 1273, duration: 0.525s, episode steps: 101, steps per second: 193, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.136, 10.287], loss: 0.000100, mae: 0.004920, mean_q: 0.740972
 93896/100000: episode: 1274, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.882, 10.273], loss: 0.000128, mae: 0.005105, mean_q: 0.739964
 93997/100000: episode: 1275, duration: 0.533s, episode steps: 101, steps per second: 189, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.100], loss: 0.000131, mae: 0.004597, mean_q: 0.739552
 94098/100000: episode: 1276, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.284, 10.100], loss: 0.000092, mae: 0.004491, mean_q: 0.740325
 94199/100000: episode: 1277, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.030, 10.152], loss: 0.000109, mae: 0.004509, mean_q: 0.738422
 94300/100000: episode: 1278, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.200, 10.135], loss: 0.000110, mae: 0.004618, mean_q: 0.737460
 94401/100000: episode: 1279, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.159], loss: 0.000140, mae: 0.004974, mean_q: 0.738267
 94502/100000: episode: 1280, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.089, 10.100], loss: 0.000068, mae: 0.004087, mean_q: 0.738067
 94603/100000: episode: 1281, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.014, 10.100], loss: 0.000088, mae: 0.004244, mean_q: 0.736769
 94704/100000: episode: 1282, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.876, mean reward: 0.009 [0.000, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.896, 10.398], loss: 0.000072, mae: 0.003503, mean_q: 0.737531
 94805/100000: episode: 1283, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.903, 10.341], loss: 0.000067, mae: 0.003739, mean_q: 0.738995
 94906/100000: episode: 1284, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.160, 10.100], loss: 0.000024, mae: 0.002396, mean_q: 0.737821
 95007/100000: episode: 1285, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.277, 10.499], loss: 0.000040, mae: 0.002738, mean_q: 0.737860
 95108/100000: episode: 1286, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.595, 10.100], loss: 0.000028, mae: 0.002183, mean_q: 0.738338
 95209/100000: episode: 1287, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.731, 10.215], loss: 0.000044, mae: 0.002936, mean_q: 0.737991
 95310/100000: episode: 1288, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.037, 10.125], loss: 0.000047, mae: 0.002990, mean_q: 0.738761
 95411/100000: episode: 1289, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.424, 10.100], loss: 0.000027, mae: 0.002455, mean_q: 0.738351
 95512/100000: episode: 1290, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.910, 10.100], loss: 0.000040, mae: 0.003040, mean_q: 0.738577
 95613/100000: episode: 1291, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.755, 10.100], loss: 0.000019, mae: 0.001926, mean_q: 0.738941
 95714/100000: episode: 1292, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.997, 10.311], loss: 0.000033, mae: 0.002385, mean_q: 0.738893
 95815/100000: episode: 1293, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.299, 10.100], loss: 0.000020, mae: 0.001636, mean_q: 0.738823
 95916/100000: episode: 1294, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.700, 10.100], loss: 0.000019, mae: 0.001719, mean_q: 0.739516
 96017/100000: episode: 1295, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.100], loss: 0.000019, mae: 0.001843, mean_q: 0.739429
 96118/100000: episode: 1296, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.100, 10.171], loss: 0.000016, mae: 0.002078, mean_q: 0.739247
 96219/100000: episode: 1297, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.863, 10.100], loss: 0.000017, mae: 0.002244, mean_q: 0.739427
 96320/100000: episode: 1298, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.986, 10.304], loss: 0.000012, mae: 0.001618, mean_q: 0.738814
 96421/100000: episode: 1299, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.640, 10.100], loss: 0.000017, mae: 0.002222, mean_q: 0.738679
 96522/100000: episode: 1300, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.394, 10.100], loss: 0.000020, mae: 0.002075, mean_q: 0.739140
 96623/100000: episode: 1301, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.267, 10.100], loss: 0.000011, mae: 0.001233, mean_q: 0.738211
 96724/100000: episode: 1302, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.033, 10.100], loss: 0.000012, mae: 0.001493, mean_q: 0.738263
 96825/100000: episode: 1303, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.854, mean reward: 0.008 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.283, 10.100], loss: 0.000010, mae: 0.001001, mean_q: 0.738612
 96926/100000: episode: 1304, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.979, 10.100], loss: 0.000010, mae: 0.001227, mean_q: 0.738126
 97027/100000: episode: 1305, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.628, 10.120], loss: 0.000019, mae: 0.001652, mean_q: 0.738293
 97128/100000: episode: 1306, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.860, mean reward: 0.009 [0.000, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.341, 10.348], loss: 0.000011, mae: 0.001538, mean_q: 0.738415
 97229/100000: episode: 1307, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.992, 10.211], loss: 0.000015, mae: 0.001695, mean_q: 0.738337
 97330/100000: episode: 1308, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.991, 10.127], loss: 0.000017, mae: 0.001830, mean_q: 0.738306
 97431/100000: episode: 1309, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.810, 10.373], loss: 0.000016, mae: 0.001755, mean_q: 0.737922
 97532/100000: episode: 1310, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.644, 10.100], loss: 0.000015, mae: 0.001497, mean_q: 0.738002
 97633/100000: episode: 1311, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.300, 10.100], loss: 0.000015, mae: 0.001601, mean_q: 0.738177
 97734/100000: episode: 1312, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.027, 10.100], loss: 0.000022, mae: 0.001758, mean_q: 0.738469
 97835/100000: episode: 1313, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.379, 10.100], loss: 0.000014, mae: 0.002045, mean_q: 0.737992
 97936/100000: episode: 1314, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.592, 10.100], loss: 0.000019, mae: 0.001474, mean_q: 0.737540
 98037/100000: episode: 1315, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.589, 10.100], loss: 0.000012, mae: 0.001182, mean_q: 0.737704
 98138/100000: episode: 1316, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.469, 10.100], loss: 0.000014, mae: 0.001378, mean_q: 0.738585
 98239/100000: episode: 1317, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.682, 10.100], loss: 0.000016, mae: 0.001686, mean_q: 0.738294
 98340/100000: episode: 1318, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.438, 10.100], loss: 0.000010, mae: 0.001125, mean_q: 0.738285
 98441/100000: episode: 1319, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.377, 10.322], loss: 0.000012, mae: 0.001439, mean_q: 0.738608
 98542/100000: episode: 1320, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.012, 10.100], loss: 0.000017, mae: 0.001678, mean_q: 0.738275
 98643/100000: episode: 1321, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.257, 10.100], loss: 0.000014, mae: 0.001283, mean_q: 0.738943
 98744/100000: episode: 1322, duration: 0.533s, episode steps: 101, steps per second: 189, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.245, 10.152], loss: 0.000016, mae: 0.001812, mean_q: 0.738575
 98845/100000: episode: 1323, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.934, 10.100], loss: 0.000010, mae: 0.001121, mean_q: 0.738537
 98946/100000: episode: 1324, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.127, 10.273], loss: 0.000015, mae: 0.001384, mean_q: 0.739160
 99047/100000: episode: 1325, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.253, 10.240], loss: 0.000014, mae: 0.001521, mean_q: 0.738469
 99148/100000: episode: 1326, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.581, 10.152], loss: 0.000013, mae: 0.001167, mean_q: 0.739007
 99249/100000: episode: 1327, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.915, 10.100], loss: 0.000014, mae: 0.001359, mean_q: 0.739299
 99350/100000: episode: 1328, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.008, 10.343], loss: 0.000023, mae: 0.002314, mean_q: 0.739831
 99451/100000: episode: 1329, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.281, 10.100], loss: 0.000009, mae: 0.001162, mean_q: 0.739322
 99552/100000: episode: 1330, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.034, 10.197], loss: 0.000014, mae: 0.001345, mean_q: 0.740216
 99653/100000: episode: 1331, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.100], loss: 0.000017, mae: 0.001743, mean_q: 0.739951
 99754/100000: episode: 1332, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.933, 10.100], loss: 0.000015, mae: 0.001494, mean_q: 0.740366
[Info] 1-TH LEVEL FOUND: 0.7757678627967834, Considering 10/100 traces
 99855/100000: episode: 1333, duration: 5.031s, episode steps: 101, steps per second: 20, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.201, 10.289], loss: 0.000010, mae: 0.001115, mean_q: 0.740256
 99859/100000: episode: 1334, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 0.665, mean reward: 0.166 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.240], loss: 0.000030, mae: 0.001401, mean_q: 0.740533
 99865/100000: episode: 1335, duration: 0.037s, episode steps: 6, steps per second: 160, episode reward: 0.661, mean reward: 0.110 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.619, 10.100], loss: 0.000010, mae: 0.001302, mean_q: 0.742116
 99870/100000: episode: 1336, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 0.618, mean reward: 0.124 [0.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.109, 10.100], loss: 0.000008, mae: 0.001173, mean_q: 0.740858
 99871/100000: episode: 1337, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.603, mean reward: 0.603 [0.603, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.269], loss: 0.000000, mae: 0.000435, mean_q: 0.741333
 99873/100000: episode: 1338, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.565, mean reward: 0.282 [0.000, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.182], loss: 0.000006, mae: 0.000991, mean_q: 0.736981
 99880/100000: episode: 1339, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 0.611, mean reward: 0.087 [0.000, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.278, 10.100], loss: 0.000032, mae: 0.002037, mean_q: 0.741809
 99881/100000: episode: 1340, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.347], loss: 0.000004, mae: 0.001559, mean_q: 0.739134
 99887/100000: episode: 1341, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 0.704, mean reward: 0.117 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.171, 10.203], loss: 0.000005, mae: 0.001056, mean_q: 0.738779
 99891/100000: episode: 1342, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.606, mean reward: 0.152 [0.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.190], loss: 0.000003, mae: 0.001797, mean_q: 0.741686
 99895/100000: episode: 1343, duration: 0.037s, episode steps: 4, steps per second: 109, episode reward: 0.621, mean reward: 0.155 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.084, 10.168], loss: 0.000043, mae: 0.002774, mean_q: 0.740236
 99900/100000: episode: 1344, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 0.584, mean reward: 0.117 [0.000, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.100], loss: 0.000022, mae: 0.002689, mean_q: 0.739054
 99907/100000: episode: 1345, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 0.702, mean reward: 0.100 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.235, 10.100], loss: 0.000036, mae: 0.002674, mean_q: 0.739829
 99914/100000: episode: 1346, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 0.641, mean reward: 0.092 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.191, 10.100], loss: 0.000018, mae: 0.001596, mean_q: 0.741026
 99915/100000: episode: 1347, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.370], loss: 0.000001, mae: 0.001039, mean_q: 0.743745
 99921/100000: episode: 1348, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 0.662, mean reward: 0.110 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.132, 10.100], loss: 0.000019, mae: 0.001282, mean_q: 0.738096
 99927/100000: episode: 1349, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 0.622, mean reward: 0.104 [0.000, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.129, 10.100], loss: 0.000005, mae: 0.000752, mean_q: 0.739831
 99934/100000: episode: 1350, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 0.636, mean reward: 0.091 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.222, 10.100], loss: 0.000073, mae: 0.002059, mean_q: 0.740725
 99936/100000: episode: 1351, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.598, mean reward: 0.299 [0.000, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.178], loss: 0.000004, mae: 0.001411, mean_q: 0.737839
 99941/100000: episode: 1352, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 0.600, mean reward: 0.120 [0.000, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.111, 10.100], loss: 0.000002, mae: 0.001184, mean_q: 0.739051
 99946/100000: episode: 1353, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 0.583, mean reward: 0.117 [0.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.075, 10.100], loss: 0.000070, mae: 0.001918, mean_q: 0.741253
 99950/100000: episode: 1354, duration: 0.032s, episode steps: 4, steps per second: 123, episode reward: 0.668, mean reward: 0.167 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.130], loss: 0.000161, mae: 0.003171, mean_q: 0.741652
 99955/100000: episode: 1355, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 0.572, mean reward: 0.114 [0.000, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.062, 10.100], loss: 0.000022, mae: 0.003239, mean_q: 0.737895
 99960/100000: episode: 1356, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 0.587, mean reward: 0.117 [0.000, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.108, 10.100], loss: 0.000059, mae: 0.003289, mean_q: 0.738775
 99967/100000: episode: 1357, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 0.671, mean reward: 0.096 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.209, 10.100], loss: 0.000122, mae: 0.003485, mean_q: 0.739302
 99974/100000: episode: 1358, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 0.627, mean reward: 0.090 [0.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.194, 10.100], loss: 0.000007, mae: 0.001464, mean_q: 0.740447
 99979/100000: episode: 1359, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.625, mean reward: 0.125 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.100], loss: 0.000086, mae: 0.002627, mean_q: 0.739875
 99981/100000: episode: 1360, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.604, mean reward: 0.302 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.202], loss: 0.000358, mae: 0.006678, mean_q: 0.738738
 99986/100000: episode: 1361, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 0.619, mean reward: 0.124 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.165, 10.100], loss: 0.000189, mae: 0.007172, mean_q: 0.736419
 99988/100000: episode: 1362, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.570, mean reward: 0.285 [0.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.234], loss: 0.000054, mae: 0.004939, mean_q: 0.736870
 99992/100000: episode: 1363, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 0.663, mean reward: 0.166 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.301, 10.202], loss: 0.000112, mae: 0.003628, mean_q: 0.738905
 99998/100000: episode: 1364, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 0.651, mean reward: 0.108 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.118, 10.140], loss: 0.000047, mae: 0.002888, mean_q: 0.741234
done, took 612.719 seconds
[Info] End Importance Splitting. Falsification occurred 1 times.
