Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_14_42_05_CET/1, out/localization/mer27nov2019_14_42_05_CET/1/levels, out/localization/mer27nov2019_14_42_05_CET/1/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.191s, episode steps: 101, steps per second: 529, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.579, 10.100], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.065s, episode steps: 101, steps per second: 1547, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.602, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.120s, episode steps: 101, steps per second: 845, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.740, 10.100], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.077s, episode steps: 101, steps per second: 1319, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.533, 10.357], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.071s, episode steps: 101, steps per second: 1414, episode reward: 0.858, mean reward: 0.008 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.343, 10.100], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.071s, episode steps: 101, steps per second: 1413, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.414, 10.194], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.071s, episode steps: 101, steps per second: 1418, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.862, 10.215], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.109s, episode steps: 101, steps per second: 929, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.236], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.102s, episode steps: 101, steps per second: 987, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.921, 10.100], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.131s, episode steps: 101, steps per second: 772, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.974, 10.288], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.120s, episode steps: 101, steps per second: 845, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.966, 10.260], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.125s, episode steps: 101, steps per second: 805, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.211, 10.100], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.064s, episode steps: 101, steps per second: 1571, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.771, 10.389], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.064s, episode steps: 101, steps per second: 1583, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.516, 10.100], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.064s, episode steps: 101, steps per second: 1582, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.592, 10.224], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.077s, episode steps: 101, steps per second: 1307, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.100], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.064s, episode steps: 101, steps per second: 1574, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.100], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.069s, episode steps: 101, steps per second: 1464, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.915, 10.204], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.068s, episode steps: 101, steps per second: 1493, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.989, 10.123], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.079s, episode steps: 101, steps per second: 1281, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.100], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.078s, episode steps: 101, steps per second: 1293, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.135, 10.100], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.071s, episode steps: 101, steps per second: 1426, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.248, 10.207], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.064s, episode steps: 101, steps per second: 1574, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.749, 10.239], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.064s, episode steps: 101, steps per second: 1574, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.594, 10.229], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.083s, episode steps: 101, steps per second: 1219, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.405, 10.100], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.084s, episode steps: 101, steps per second: 1208, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.348, 10.100], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.082s, episode steps: 101, steps per second: 1238, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.893, 10.100], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.066s, episode steps: 101, steps per second: 1530, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.558, 10.100], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.069s, episode steps: 101, steps per second: 1454, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.470, 10.100], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.064s, episode steps: 101, steps per second: 1577, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.291, 10.100], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.069s, episode steps: 101, steps per second: 1460, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.009, 10.100], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.064s, episode steps: 101, steps per second: 1581, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.276, 10.187], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.080s, episode steps: 101, steps per second: 1263, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.792, 10.100], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.064s, episode steps: 101, steps per second: 1569, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.957, 10.100], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.075s, episode steps: 101, steps per second: 1354, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.698, 10.100], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.075s, episode steps: 101, steps per second: 1342, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.831, 10.198], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.102s, episode steps: 101, steps per second: 989, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.018, 10.100], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.127s, episode steps: 101, steps per second: 795, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.099, 10.100], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.122s, episode steps: 101, steps per second: 826, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.381, 10.100], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.139s, episode steps: 101, steps per second: 728, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.582, 10.100], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.084s, episode steps: 101, steps per second: 1206, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.439, 10.178], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.064s, episode steps: 101, steps per second: 1575, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.100], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.065s, episode steps: 101, steps per second: 1565, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.271, 10.105], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.071s, episode steps: 101, steps per second: 1418, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.817, 10.100], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.070s, episode steps: 101, steps per second: 1442, episode reward: 0.885, mean reward: 0.009 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.468, 10.185], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.069s, episode steps: 101, steps per second: 1474, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.697, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.068s, episode steps: 101, steps per second: 1490, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.602, 10.100], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.152s, episode steps: 101, steps per second: 665, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.502, 10.324], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.136s, episode steps: 101, steps per second: 742, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.684, 10.460], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 1.028s, episode steps: 101, steps per second: 98, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.390, 10.100], loss: 0.031693, mae: 0.079018, mean_q: -0.848599
  5151/100000: episode: 51, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.595, 10.100], loss: 0.028434, mae: 0.084016, mean_q: -0.845824
  5252/100000: episode: 52, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.823, 10.140], loss: 0.024286, mae: 0.069815, mean_q: -0.828614
  5353/100000: episode: 53, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.656, 10.100], loss: 0.014037, mae: 0.051018, mean_q: -0.828977
  5454/100000: episode: 54, duration: 0.676s, episode steps: 101, steps per second: 150, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.921, 10.114], loss: 0.015261, mae: 0.054256, mean_q: -0.813039
  5555/100000: episode: 55, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.357, 10.307], loss: 0.014534, mae: 0.057613, mean_q: -0.801671
  5656/100000: episode: 56, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.917, 10.305], loss: 0.012894, mae: 0.053876, mean_q: -0.776173
  5757/100000: episode: 57, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.509, 10.189], loss: 0.008652, mae: 0.046807, mean_q: -0.760414
  5858/100000: episode: 58, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.193], loss: 0.005280, mae: 0.035611, mean_q: -0.755039
  5959/100000: episode: 59, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.555, 10.100], loss: 0.006649, mae: 0.045702, mean_q: -0.729351
  6060/100000: episode: 60, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.070, 10.100], loss: 0.003248, mae: 0.025281, mean_q: -0.739372
  6161/100000: episode: 61, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.945, 10.100], loss: 0.002995, mae: 0.022944, mean_q: -0.727516
  6262/100000: episode: 62, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.852, 10.388], loss: 0.003108, mae: 0.021435, mean_q: -0.726876
  6363/100000: episode: 63, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.956, 10.209], loss: 0.002346, mae: 0.017839, mean_q: -0.713775
  6464/100000: episode: 64, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.195, 10.100], loss: 0.003208, mae: 0.020338, mean_q: -0.699218
  6565/100000: episode: 65, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.495, 10.100], loss: 0.002539, mae: 0.019370, mean_q: -0.692977
  6666/100000: episode: 66, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.514, 10.100], loss: 0.002224, mae: 0.018835, mean_q: -0.684630
  6767/100000: episode: 67, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.100], loss: 0.002052, mae: 0.018427, mean_q: -0.675220
  6868/100000: episode: 68, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.908, 10.100], loss: 0.001726, mae: 0.017895, mean_q: -0.671434
  6969/100000: episode: 69, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.539, 10.286], loss: 0.002111, mae: 0.019504, mean_q: -0.663544
  7070/100000: episode: 70, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.254, 10.176], loss: 0.001471, mae: 0.017407, mean_q: -0.661995
  7171/100000: episode: 71, duration: 0.649s, episode steps: 101, steps per second: 156, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.209, 10.129], loss: 0.002098, mae: 0.020395, mean_q: -0.645857
  7272/100000: episode: 72, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.082, 10.149], loss: 0.001380, mae: 0.017975, mean_q: -0.642693
  7373/100000: episode: 73, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.964, 10.135], loss: 0.001534, mae: 0.018354, mean_q: -0.628102
  7474/100000: episode: 74, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.867, mean reward: 0.009 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.259, 10.369], loss: 0.001477, mae: 0.018675, mean_q: -0.619243
  7575/100000: episode: 75, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.886, 10.188], loss: 0.001377, mae: 0.018076, mean_q: -0.610181
  7676/100000: episode: 76, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.102], loss: 0.001505, mae: 0.019519, mean_q: -0.594376
  7777/100000: episode: 77, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.100], loss: 0.001284, mae: 0.018476, mean_q: -0.585619
  7878/100000: episode: 78, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.129], loss: 0.001367, mae: 0.019239, mean_q: -0.584357
  7979/100000: episode: 79, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.749, 10.255], loss: 0.000942, mae: 0.017463, mean_q: -0.568199
  8080/100000: episode: 80, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.348, 10.452], loss: 0.001189, mae: 0.018400, mean_q: -0.573346
  8181/100000: episode: 81, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.764, 10.253], loss: 0.000956, mae: 0.018407, mean_q: -0.546761
  8282/100000: episode: 82, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.730, 10.299], loss: 0.000904, mae: 0.017972, mean_q: -0.528516
  8383/100000: episode: 83, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.359, 10.100], loss: 0.000843, mae: 0.017856, mean_q: -0.503187
  8484/100000: episode: 84, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.827, 10.100], loss: 0.000792, mae: 0.017525, mean_q: -0.502518
  8585/100000: episode: 85, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.684, 10.311], loss: 0.000809, mae: 0.016689, mean_q: -0.504372
  8686/100000: episode: 86, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.910, 10.302], loss: 0.000937, mae: 0.017436, mean_q: -0.472894
  8787/100000: episode: 87, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.310, 10.343], loss: 0.000778, mae: 0.017229, mean_q: -0.472791
  8888/100000: episode: 88, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.455, 10.193], loss: 0.000788, mae: 0.017418, mean_q: -0.452533
  8989/100000: episode: 89, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.838, 10.100], loss: 0.000598, mae: 0.015845, mean_q: -0.451620
  9090/100000: episode: 90, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.909, 10.100], loss: 0.000787, mae: 0.018185, mean_q: -0.424720
  9191/100000: episode: 91, duration: 0.724s, episode steps: 101, steps per second: 140, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.906, 10.330], loss: 0.000575, mae: 0.015804, mean_q: -0.408352
  9292/100000: episode: 92, duration: 0.719s, episode steps: 101, steps per second: 141, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.552, 10.108], loss: 0.000612, mae: 0.015880, mean_q: -0.378276
  9393/100000: episode: 93, duration: 0.625s, episode steps: 101, steps per second: 161, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.364, 10.100], loss: 0.000626, mae: 0.016661, mean_q: -0.378873
  9494/100000: episode: 94, duration: 0.740s, episode steps: 101, steps per second: 137, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.163, 10.245], loss: 0.000563, mae: 0.016019, mean_q: -0.352623
  9595/100000: episode: 95, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.610, 10.100], loss: 0.000476, mae: 0.016016, mean_q: -0.344609
  9696/100000: episode: 96, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.152], loss: 0.000399, mae: 0.015106, mean_q: -0.343026
  9797/100000: episode: 97, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.406, 10.359], loss: 0.000347, mae: 0.015233, mean_q: -0.322151
  9898/100000: episode: 98, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.235, 10.117], loss: 0.000285, mae: 0.013793, mean_q: -0.317082
  9999/100000: episode: 99, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.546, 10.320], loss: 0.000292, mae: 0.014359, mean_q: -0.288845
 10100/100000: episode: 100, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.816, mean reward: 0.008 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.100], loss: 0.000168, mae: 0.011975, mean_q: -0.279563
 10201/100000: episode: 101, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.443, 10.195], loss: 0.000208, mae: 0.013633, mean_q: -0.265606
 10302/100000: episode: 102, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.237, 10.100], loss: 0.000189, mae: 0.012388, mean_q: -0.220255
 10403/100000: episode: 103, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.661, 10.100], loss: 0.000152, mae: 0.011324, mean_q: -0.227003
 10504/100000: episode: 104, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.581, 10.109], loss: 0.000138, mae: 0.010612, mean_q: -0.187770
 10605/100000: episode: 105, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.100], loss: 0.000174, mae: 0.012261, mean_q: -0.194361
 10706/100000: episode: 106, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.472, 10.100], loss: 0.000154, mae: 0.011374, mean_q: -0.173296
 10807/100000: episode: 107, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.551, 10.100], loss: 0.000148, mae: 0.010942, mean_q: -0.145635
 10908/100000: episode: 108, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.933, 10.100], loss: 0.000128, mae: 0.010281, mean_q: -0.131258
 11009/100000: episode: 109, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.214], loss: 0.000179, mae: 0.012304, mean_q: -0.116427
 11110/100000: episode: 110, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.297, 10.100], loss: 0.000138, mae: 0.010490, mean_q: -0.116437
 11211/100000: episode: 111, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.100], loss: 0.000159, mae: 0.011393, mean_q: -0.081159
 11312/100000: episode: 112, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.825, 10.213], loss: 0.000147, mae: 0.010854, mean_q: -0.068916
 11413/100000: episode: 113, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.583, 10.100], loss: 0.000160, mae: 0.011272, mean_q: -0.049767
 11514/100000: episode: 114, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.491, 10.282], loss: 0.000142, mae: 0.010370, mean_q: -0.026271
 11615/100000: episode: 115, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.115, 10.100], loss: 0.000152, mae: 0.010812, mean_q: 0.003565
 11716/100000: episode: 116, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.866, mean reward: 0.009 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.412, 10.100], loss: 0.000128, mae: 0.009746, mean_q: 0.002549
 11817/100000: episode: 117, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.519, 10.119], loss: 0.000129, mae: 0.009834, mean_q: 0.021949
 11918/100000: episode: 118, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.638, 10.153], loss: 0.000099, mae: 0.008528, mean_q: 0.054732
 12019/100000: episode: 119, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.879, 10.228], loss: 0.000126, mae: 0.009472, mean_q: 0.038942
 12120/100000: episode: 120, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.100], loss: 0.000111, mae: 0.008754, mean_q: 0.077740
 12221/100000: episode: 121, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.849, 10.170], loss: 0.000107, mae: 0.008318, mean_q: 0.093634
 12322/100000: episode: 122, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.680, 10.108], loss: 0.000128, mae: 0.009408, mean_q: 0.115704
 12423/100000: episode: 123, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.152, 10.100], loss: 0.000089, mae: 0.007863, mean_q: 0.152865
 12524/100000: episode: 124, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.668, 10.121], loss: 0.000100, mae: 0.008170, mean_q: 0.155386
 12625/100000: episode: 125, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.700, 10.204], loss: 0.000078, mae: 0.007343, mean_q: 0.173956
 12726/100000: episode: 126, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.621, 10.228], loss: 0.000099, mae: 0.008164, mean_q: 0.186478
 12827/100000: episode: 127, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.672, 10.100], loss: 0.000076, mae: 0.006739, mean_q: 0.207363
 12928/100000: episode: 128, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.651, 10.245], loss: 0.000087, mae: 0.007258, mean_q: 0.234404
 13029/100000: episode: 129, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.201, 10.130], loss: 0.000089, mae: 0.007535, mean_q: 0.244827
 13130/100000: episode: 130, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.067, 10.100], loss: 0.000089, mae: 0.007282, mean_q: 0.270529
 13231/100000: episode: 131, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.935, mean reward: 0.009 [0.000, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.410, 10.100], loss: 0.000075, mae: 0.007114, mean_q: 0.277050
 13332/100000: episode: 132, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.316], loss: 0.000060, mae: 0.006195, mean_q: 0.302200
 13433/100000: episode: 133, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.053, 10.100], loss: 0.000074, mae: 0.007022, mean_q: 0.312273
 13534/100000: episode: 134, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.372, 10.100], loss: 0.000077, mae: 0.007412, mean_q: 0.336103
 13635/100000: episode: 135, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.559, 10.215], loss: 0.000064, mae: 0.006525, mean_q: 0.339359
 13736/100000: episode: 136, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.794, 10.100], loss: 0.000075, mae: 0.007314, mean_q: 0.354980
 13837/100000: episode: 137, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.793, 10.216], loss: 0.000061, mae: 0.006008, mean_q: 0.388488
 13938/100000: episode: 138, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.331, 10.102], loss: 0.000050, mae: 0.006044, mean_q: 0.385452
 14039/100000: episode: 139, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.638, 10.211], loss: 0.000051, mae: 0.005695, mean_q: 0.402236
 14140/100000: episode: 140, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.650, 10.233], loss: 0.000052, mae: 0.006238, mean_q: 0.420662
 14241/100000: episode: 141, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.587, 10.246], loss: 0.000056, mae: 0.005732, mean_q: 0.434097
 14342/100000: episode: 142, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.702, 10.258], loss: 0.000051, mae: 0.005716, mean_q: 0.461465
 14443/100000: episode: 143, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.277, 10.195], loss: 0.000047, mae: 0.005237, mean_q: 0.473147
 14544/100000: episode: 144, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.323, 10.124], loss: 0.000056, mae: 0.005599, mean_q: 0.472189
 14645/100000: episode: 145, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.838, 10.158], loss: 0.000048, mae: 0.005354, mean_q: 0.494375
 14746/100000: episode: 146, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.954, mean reward: 0.009 [0.000, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.100], loss: 0.000052, mae: 0.005572, mean_q: 0.509279
 14847/100000: episode: 147, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.443, 10.204], loss: 0.000050, mae: 0.005312, mean_q: 0.527641
 14948/100000: episode: 148, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.528, 10.145], loss: 0.000059, mae: 0.005385, mean_q: 0.532227
 15049/100000: episode: 149, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.101, 10.134], loss: 0.000042, mae: 0.004776, mean_q: 0.547724
 15150/100000: episode: 150, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.320, 10.209], loss: 0.000039, mae: 0.004822, mean_q: 0.561792
 15251/100000: episode: 151, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.326], loss: 0.000045, mae: 0.005339, mean_q: 0.563251
 15352/100000: episode: 152, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.822, 10.439], loss: 0.000042, mae: 0.005564, mean_q: 0.578951
 15453/100000: episode: 153, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.878, 10.159], loss: 0.000025, mae: 0.003920, mean_q: 0.589374
 15554/100000: episode: 154, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.616, 10.269], loss: 0.000030, mae: 0.004441, mean_q: 0.603353
 15655/100000: episode: 155, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.177, 10.100], loss: 0.000041, mae: 0.004450, mean_q: 0.613187
 15756/100000: episode: 156, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.836, 10.165], loss: 0.000028, mae: 0.003959, mean_q: 0.621342
 15857/100000: episode: 157, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.565, 10.100], loss: 0.000064, mae: 0.005897, mean_q: 0.630772
 15958/100000: episode: 158, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.925, mean reward: 0.009 [0.000, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.709, 10.100], loss: 0.000037, mae: 0.004383, mean_q: 0.641600
 16059/100000: episode: 159, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.607, 10.100], loss: 0.000034, mae: 0.004498, mean_q: 0.642831
 16160/100000: episode: 160, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.171, 10.184], loss: 0.000044, mae: 0.004503, mean_q: 0.649132
 16261/100000: episode: 161, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.762, 10.136], loss: 0.000036, mae: 0.003998, mean_q: 0.653432
 16362/100000: episode: 162, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.761, 10.100], loss: 0.000064, mae: 0.004580, mean_q: 0.663663
 16463/100000: episode: 163, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.801, 10.100], loss: 0.000039, mae: 0.003832, mean_q: 0.668387
 16564/100000: episode: 164, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.362, 10.100], loss: 0.000029, mae: 0.003033, mean_q: 0.672274
 16665/100000: episode: 165, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.241, 10.479], loss: 0.000028, mae: 0.002824, mean_q: 0.677264
 16766/100000: episode: 166, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.588, 10.100], loss: 0.000027, mae: 0.002712, mean_q: 0.678737
 16867/100000: episode: 167, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.787, 10.279], loss: 0.000025, mae: 0.003049, mean_q: 0.681441
 16968/100000: episode: 168, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.888, mean reward: 0.009 [0.000, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.961, 10.244], loss: 0.000022, mae: 0.002898, mean_q: 0.688255
 17069/100000: episode: 169, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.858, mean reward: 0.008 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.100], loss: 0.000027, mae: 0.003180, mean_q: 0.687382
 17170/100000: episode: 170, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.539, 10.100], loss: 0.000021, mae: 0.003039, mean_q: 0.691814
 17271/100000: episode: 171, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.903, 10.100], loss: 0.000059, mae: 0.005258, mean_q: 0.690970
 17372/100000: episode: 172, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.010, 10.182], loss: 0.000028, mae: 0.003439, mean_q: 0.697551
 17473/100000: episode: 173, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.876, 10.100], loss: 0.000020, mae: 0.002626, mean_q: 0.700458
 17574/100000: episode: 174, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.936, 10.100], loss: 0.000031, mae: 0.003346, mean_q: 0.705629
 17675/100000: episode: 175, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.943, 10.119], loss: 0.000033, mae: 0.003697, mean_q: 0.705698
 17776/100000: episode: 176, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.507, 10.199], loss: 0.000028, mae: 0.003570, mean_q: 0.706162
 17877/100000: episode: 177, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.392, 10.100], loss: 0.000018, mae: 0.002924, mean_q: 0.710140
 17978/100000: episode: 178, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.999, 10.438], loss: 0.000029, mae: 0.003083, mean_q: 0.713439
 18079/100000: episode: 179, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.676, 10.100], loss: 0.000024, mae: 0.003878, mean_q: 0.716444
 18180/100000: episode: 180, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.419, 10.100], loss: 0.000024, mae: 0.003378, mean_q: 0.719495
 18281/100000: episode: 181, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.100], loss: 0.000023, mae: 0.003007, mean_q: 0.722140
 18382/100000: episode: 182, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.592, 10.310], loss: 0.000017, mae: 0.003076, mean_q: 0.724922
 18483/100000: episode: 183, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.042, 10.162], loss: 0.000019, mae: 0.002897, mean_q: 0.725227
 18584/100000: episode: 184, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.420, 10.100], loss: 0.000020, mae: 0.003249, mean_q: 0.729146
 18685/100000: episode: 185, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.635, 10.274], loss: 0.000019, mae: 0.003504, mean_q: 0.730907
 18786/100000: episode: 186, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.462, 10.100], loss: 0.000033, mae: 0.003761, mean_q: 0.733298
 18887/100000: episode: 187, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.100], loss: 0.000021, mae: 0.003041, mean_q: 0.735534
 18988/100000: episode: 188, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.794, 10.142], loss: 0.000024, mae: 0.003863, mean_q: 0.738688
 19089/100000: episode: 189, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.678, 10.274], loss: 0.000017, mae: 0.002821, mean_q: 0.742308
 19190/100000: episode: 190, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.724, 10.128], loss: 0.000043, mae: 0.004907, mean_q: 0.742604
 19291/100000: episode: 191, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.047, 10.134], loss: 0.000026, mae: 0.002856, mean_q: 0.744156
 19392/100000: episode: 192, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.164], loss: 0.000033, mae: 0.004112, mean_q: 0.745464
 19493/100000: episode: 193, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.715, 10.315], loss: 0.000021, mae: 0.003294, mean_q: 0.748299
 19594/100000: episode: 194, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.365, 10.100], loss: 0.000015, mae: 0.002886, mean_q: 0.748604
 19695/100000: episode: 195, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.286, 10.372], loss: 0.000024, mae: 0.003139, mean_q: 0.750308
 19796/100000: episode: 196, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.773, 10.117], loss: 0.000015, mae: 0.003160, mean_q: 0.752074
 19897/100000: episode: 197, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.100], loss: 0.000020, mae: 0.003065, mean_q: 0.753414
 19998/100000: episode: 198, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.602, 10.204], loss: 0.000016, mae: 0.002852, mean_q: 0.754710
 20099/100000: episode: 199, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.388, 10.215], loss: 0.000010, mae: 0.002327, mean_q: 0.756152
[Info] 1-TH LEVEL FOUND: 0.7890351414680481, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.245s, episode steps: 101, steps per second: 19, episode reward: 0.883, mean reward: 0.009 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.612, 10.487], loss: 0.000014, mae: 0.002801, mean_q: 0.757270
 20201/100000: episode: 201, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.300, 10.100], loss: 0.000007, mae: 0.003582, mean_q: 0.761770
 20202/100000: episode: 202, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.309, 10.200], loss: 0.000272, mae: 0.006210, mean_q: 0.766587
 20207/100000: episode: 203, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.847, mean reward: 0.169 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.390, 10.100], loss: 0.000008, mae: 0.003217, mean_q: 0.754755
 20258/100000: episode: 204, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.758, mean reward: 0.015 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.830, 10.100], loss: 0.000022, mae: 0.003216, mean_q: 0.757711
 20261/100000: episode: 205, duration: 0.023s, episode steps: 3, steps per second: 128, episode reward: 0.769, mean reward: 0.256 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.409, 10.100], loss: 0.000098, mae: 0.003991, mean_q: 0.754229
 20312/100000: episode: 206, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 0.775, mean reward: 0.015 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.206, 10.100], loss: 0.000038, mae: 0.004947, mean_q: 0.758302
 20363/100000: episode: 207, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 0.812, mean reward: 0.016 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.197, 10.100], loss: 0.000012, mae: 0.002839, mean_q: 0.758856
 20414/100000: episode: 208, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.289, 10.100], loss: 0.000019, mae: 0.002967, mean_q: 0.759536
 20419/100000: episode: 209, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 0.767, mean reward: 0.153 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.316, 10.100], loss: 0.000005, mae: 0.001670, mean_q: 0.757925
 20474/100000: episode: 210, duration: 0.320s, episode steps: 55, steps per second: 172, episode reward: 0.736, mean reward: 0.013 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.081, 10.134], loss: 0.000028, mae: 0.003246, mean_q: 0.760339
 20475/100000: episode: 211, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.273, 10.200], loss: 0.000003, mae: 0.001894, mean_q: 0.756523
 20531/100000: episode: 212, duration: 0.332s, episode steps: 56, steps per second: 169, episode reward: 0.759, mean reward: 0.014 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.544, 10.142], loss: 0.000025, mae: 0.003637, mean_q: 0.760109
 20532/100000: episode: 213, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.690, mean reward: 0.690 [0.690, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.302, 10.200], loss: 0.000001, mae: 0.001039, mean_q: 0.763265
 20535/100000: episode: 214, duration: 0.028s, episode steps: 3, steps per second: 108, episode reward: 0.764, mean reward: 0.255 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.437, 10.100], loss: 0.000007, mae: 0.003140, mean_q: 0.758428
 20536/100000: episode: 215, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.314, 10.200], loss: 0.000011, mae: 0.004499, mean_q: 0.760736
 20537/100000: episode: 216, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.697, mean reward: 0.697 [0.697, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.302, 10.200], loss: 0.000002, mae: 0.001553, mean_q: 0.762735
 20588/100000: episode: 217, duration: 0.301s, episode steps: 51, steps per second: 169, episode reward: 0.775, mean reward: 0.015 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.421, 10.100], loss: 0.000024, mae: 0.003419, mean_q: 0.760114
 20593/100000: episode: 218, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 0.796, mean reward: 0.159 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.328, 10.100], loss: 0.000009, mae: 0.002356, mean_q: 0.760480
 20644/100000: episode: 219, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 0.741, mean reward: 0.015 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.403, 10.126], loss: 0.000028, mae: 0.003607, mean_q: 0.760784
 20645/100000: episode: 220, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.271, 10.100], loss: 0.000003, mae: 0.001834, mean_q: 0.760488
 20700/100000: episode: 221, duration: 0.334s, episode steps: 55, steps per second: 165, episode reward: 0.766, mean reward: 0.014 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.602, 10.147], loss: 0.000012, mae: 0.002274, mean_q: 0.761118
 20751/100000: episode: 222, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 0.760, mean reward: 0.015 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.451, 10.164], loss: 0.000039, mae: 0.004556, mean_q: 0.760760
 20807/100000: episode: 223, duration: 0.342s, episode steps: 56, steps per second: 164, episode reward: 0.849, mean reward: 0.015 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.034, 10.100], loss: 0.000029, mae: 0.004632, mean_q: 0.760922
 20811/100000: episode: 224, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 0.821, mean reward: 0.205 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.419, 10.100], loss: 0.000109, mae: 0.005324, mean_q: 0.762504
 20897/100000: episode: 225, duration: 0.492s, episode steps: 86, steps per second: 175, episode reward: 0.755, mean reward: 0.009 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.580 [-0.944, 10.361], loss: 0.000016, mae: 0.002711, mean_q: 0.761122
 20953/100000: episode: 226, duration: 0.320s, episode steps: 56, steps per second: 175, episode reward: 0.726, mean reward: 0.013 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.428, 10.100], loss: 0.000018, mae: 0.002500, mean_q: 0.761409
 21010/100000: episode: 227, duration: 0.325s, episode steps: 57, steps per second: 175, episode reward: 0.720, mean reward: 0.013 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.375, 10.100], loss: 0.000034, mae: 0.003964, mean_q: 0.761600
 21011/100000: episode: 228, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.743, mean reward: 0.743 [0.743, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.321, 10.100], loss: 0.000005, mae: 0.002727, mean_q: 0.759893
 21012/100000: episode: 229, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.732, mean reward: 0.732 [0.732, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.261, 10.100], loss: 0.000002, mae: 0.001521, mean_q: 0.757870
 21063/100000: episode: 230, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 0.701, mean reward: 0.014 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.437, 10.100], loss: 0.000014, mae: 0.002364, mean_q: 0.760921
 21066/100000: episode: 231, duration: 0.028s, episode steps: 3, steps per second: 107, episode reward: 0.768, mean reward: 0.256 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.400, 10.100], loss: 0.000015, mae: 0.004577, mean_q: 0.759379
 21123/100000: episode: 232, duration: 0.319s, episode steps: 57, steps per second: 179, episode reward: 0.911, mean reward: 0.016 [0.000, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.812 [-1.096, 10.100], loss: 0.000030, mae: 0.003476, mean_q: 0.760678
 21124/100000: episode: 233, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.795, mean reward: 0.795 [0.795, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.349, 10.200], loss: 0.000024, mae: 0.003158, mean_q: 0.763644
 21128/100000: episode: 234, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 0.782, mean reward: 0.195 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.296, 10.100], loss: 0.000028, mae: 0.004811, mean_q: 0.759211
 21129/100000: episode: 235, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.487, 10.100], loss: 0.000002, mae: 0.001626, mean_q: 0.759135
 21130/100000: episode: 236, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.699, mean reward: 0.699 [0.699, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.347, 10.200], loss: 0.000011, mae: 0.004275, mean_q: 0.755860
 21134/100000: episode: 237, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 0.956, mean reward: 0.239 [0.000, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.405, 10.100], loss: 0.000004, mae: 0.002511, mean_q: 0.762965
 21135/100000: episode: 238, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.722, mean reward: 0.722 [0.722, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.228, 10.100], loss: 0.000335, mae: 0.009219, mean_q: 0.754597
 21136/100000: episode: 239, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.715, mean reward: 0.715 [0.715, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.284, 10.200], loss: 0.000007, mae: 0.003624, mean_q: 0.763149
 21137/100000: episode: 240, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.702, mean reward: 0.702 [0.702, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.221, 10.200], loss: 0.000010, mae: 0.004195, mean_q: 0.765415
 21142/100000: episode: 241, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 0.818, mean reward: 0.164 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.382, 10.100], loss: 0.000012, mae: 0.003561, mean_q: 0.760163
 21146/100000: episode: 242, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 0.812, mean reward: 0.203 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.319, 10.100], loss: 0.000008, mae: 0.003464, mean_q: 0.758935
 21202/100000: episode: 243, duration: 0.317s, episode steps: 56, steps per second: 176, episode reward: 0.785, mean reward: 0.014 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.409, 10.100], loss: 0.000019, mae: 0.002791, mean_q: 0.761402
 21203/100000: episode: 244, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.737, mean reward: 0.737 [0.737, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.310, 10.200], loss: 0.000004, mae: 0.002595, mean_q: 0.762273
 21206/100000: episode: 245, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 0.800, mean reward: 0.267 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.432, 10.100], loss: 0.000004, mae: 0.002168, mean_q: 0.759988
 21262/100000: episode: 246, duration: 0.315s, episode steps: 56, steps per second: 178, episode reward: 0.728, mean reward: 0.013 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.202, 10.119], loss: 0.000020, mae: 0.002564, mean_q: 0.761440
 21318/100000: episode: 247, duration: 0.328s, episode steps: 56, steps per second: 171, episode reward: 0.777, mean reward: 0.014 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.415, 10.100], loss: 0.000038, mae: 0.003926, mean_q: 0.761162
 21323/100000: episode: 248, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 0.822, mean reward: 0.164 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.497, 10.100], loss: 0.000011, mae: 0.003261, mean_q: 0.759263
 21328/100000: episode: 249, duration: 0.040s, episode steps: 5, steps per second: 124, episode reward: 0.782, mean reward: 0.156 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.431, 10.100], loss: 0.000029, mae: 0.004644, mean_q: 0.761306
 21329/100000: episode: 250, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.725, mean reward: 0.725 [0.725, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.311, 10.200], loss: 0.000056, mae: 0.005020, mean_q: 0.767021
 21415/100000: episode: 251, duration: 0.501s, episode steps: 86, steps per second: 172, episode reward: 0.793, mean reward: 0.009 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.580 [-0.836, 10.100], loss: 0.000042, mae: 0.004627, mean_q: 0.761224
 21472/100000: episode: 252, duration: 0.338s, episode steps: 57, steps per second: 168, episode reward: 0.781, mean reward: 0.014 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.892, 10.210], loss: 0.000076, mae: 0.006379, mean_q: 0.761958
 21558/100000: episode: 253, duration: 0.492s, episode steps: 86, steps per second: 175, episode reward: 0.732, mean reward: 0.009 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.582 [-0.943, 10.100], loss: 0.000038, mae: 0.004683, mean_q: 0.761363
 21614/100000: episode: 254, duration: 0.330s, episode steps: 56, steps per second: 170, episode reward: 0.826, mean reward: 0.015 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.827 [-0.436, 10.100], loss: 0.000014, mae: 0.002302, mean_q: 0.761901
 21669/100000: episode: 255, duration: 0.319s, episode steps: 55, steps per second: 172, episode reward: 0.678, mean reward: 0.012 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.460, 10.100], loss: 0.000026, mae: 0.002881, mean_q: 0.760832
 21670/100000: episode: 256, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.681, mean reward: 0.681 [0.681, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.286, 10.100], loss: 0.000032, mae: 0.002558, mean_q: 0.761231
 21674/100000: episode: 257, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 0.825, mean reward: 0.206 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.371, 10.100], loss: 0.000001, mae: 0.001256, mean_q: 0.760534
 21729/100000: episode: 258, duration: 0.311s, episode steps: 55, steps per second: 177, episode reward: 0.706, mean reward: 0.013 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.852, 10.100], loss: 0.000032, mae: 0.003127, mean_q: 0.760772
 21733/100000: episode: 259, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.809, mean reward: 0.202 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.465, 10.100], loss: 0.000010, mae: 0.002642, mean_q: 0.760243
 21784/100000: episode: 260, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.731, mean reward: 0.014 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.411, 10.100], loss: 0.000033, mae: 0.003719, mean_q: 0.760584
 21870/100000: episode: 261, duration: 0.486s, episode steps: 86, steps per second: 177, episode reward: 0.706, mean reward: 0.008 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.581 [-1.411, 10.139], loss: 0.000039, mae: 0.003634, mean_q: 0.761399
 21925/100000: episode: 262, duration: 0.331s, episode steps: 55, steps per second: 166, episode reward: 0.764, mean reward: 0.014 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.729, 10.127], loss: 0.000039, mae: 0.004540, mean_q: 0.760317
 21981/100000: episode: 263, duration: 0.337s, episode steps: 56, steps per second: 166, episode reward: 0.771, mean reward: 0.014 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-1.796, 10.100], loss: 0.000038, mae: 0.002875, mean_q: 0.760721
 22067/100000: episode: 264, duration: 0.498s, episode steps: 86, steps per second: 173, episode reward: 0.727, mean reward: 0.008 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.585 [-0.606, 10.148], loss: 0.000016, mae: 0.002447, mean_q: 0.760202
 22123/100000: episode: 265, duration: 0.339s, episode steps: 56, steps per second: 165, episode reward: 0.692, mean reward: 0.012 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.591, 10.100], loss: 0.000038, mae: 0.003982, mean_q: 0.760482
 22128/100000: episode: 266, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.789, mean reward: 0.158 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.344, 10.100], loss: 0.000008, mae: 0.003021, mean_q: 0.760443
 22184/100000: episode: 267, duration: 0.319s, episode steps: 56, steps per second: 175, episode reward: 0.743, mean reward: 0.013 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.379, 10.100], loss: 0.000026, mae: 0.002987, mean_q: 0.760173
 22241/100000: episode: 268, duration: 0.322s, episode steps: 57, steps per second: 177, episode reward: 0.677, mean reward: 0.012 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.559, 10.117], loss: 0.000031, mae: 0.004127, mean_q: 0.759951
 22246/100000: episode: 269, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 0.825, mean reward: 0.165 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.469, 10.100], loss: 0.000049, mae: 0.003545, mean_q: 0.758775
 22247/100000: episode: 270, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.775, mean reward: 0.775 [0.775, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.306, 10.200], loss: 0.000002, mae: 0.001679, mean_q: 0.759293
 22252/100000: episode: 271, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 0.832, mean reward: 0.166 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.480, 10.100], loss: 0.000024, mae: 0.002647, mean_q: 0.759672
 22307/100000: episode: 272, duration: 0.302s, episode steps: 55, steps per second: 182, episode reward: 0.741, mean reward: 0.013 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.094, 10.277], loss: 0.000016, mae: 0.002481, mean_q: 0.759648
 22311/100000: episode: 273, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 0.815, mean reward: 0.204 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.345, 10.100], loss: 0.000005, mae: 0.001851, mean_q: 0.758941
 22368/100000: episode: 274, duration: 0.318s, episode steps: 57, steps per second: 179, episode reward: 0.742, mean reward: 0.013 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.311, 10.100], loss: 0.000030, mae: 0.003350, mean_q: 0.759653
 22369/100000: episode: 275, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.285, 10.100], loss: 0.000006, mae: 0.002779, mean_q: 0.757304
 22424/100000: episode: 276, duration: 0.325s, episode steps: 55, steps per second: 169, episode reward: 0.868, mean reward: 0.016 [0.000, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.609, 10.100], loss: 0.000031, mae: 0.003460, mean_q: 0.759618
 22425/100000: episode: 277, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.718, mean reward: 0.718 [0.718, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.253, 10.200], loss: 0.000062, mae: 0.003667, mean_q: 0.758885
 22429/100000: episode: 278, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 0.772, mean reward: 0.193 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.289, 10.100], loss: 0.000015, mae: 0.002901, mean_q: 0.759329
 22430/100000: episode: 279, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.325, 10.200], loss: 0.000131, mae: 0.007882, mean_q: 0.753330
 22485/100000: episode: 280, duration: 0.320s, episode steps: 55, steps per second: 172, episode reward: 0.750, mean reward: 0.014 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.198, 10.277], loss: 0.000060, mae: 0.005762, mean_q: 0.759021
 22542/100000: episode: 281, duration: 0.316s, episode steps: 57, steps per second: 180, episode reward: 0.723, mean reward: 0.013 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.724, 10.100], loss: 0.000057, mae: 0.004782, mean_q: 0.759385
 22597/100000: episode: 282, duration: 0.337s, episode steps: 55, steps per second: 163, episode reward: 0.825, mean reward: 0.015 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.666, 10.100], loss: 0.000027, mae: 0.003071, mean_q: 0.759345
 22653/100000: episode: 283, duration: 0.335s, episode steps: 56, steps per second: 167, episode reward: 0.769, mean reward: 0.014 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.493, 10.188], loss: 0.000033, mae: 0.003428, mean_q: 0.759674
 22708/100000: episode: 284, duration: 0.320s, episode steps: 55, steps per second: 172, episode reward: 0.722, mean reward: 0.013 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-1.258, 10.321], loss: 0.000055, mae: 0.003876, mean_q: 0.759719
 22709/100000: episode: 285, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.492, 10.100], loss: 0.000006, mae: 0.003082, mean_q: 0.757066
 22713/100000: episode: 286, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 0.855, mean reward: 0.214 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.413, 10.100], loss: 0.000032, mae: 0.004200, mean_q: 0.759125
 22718/100000: episode: 287, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 0.876, mean reward: 0.175 [0.000, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.334, 10.100], loss: 0.000016, mae: 0.003767, mean_q: 0.756420
 22804/100000: episode: 288, duration: 0.494s, episode steps: 86, steps per second: 174, episode reward: 0.701, mean reward: 0.008 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.590 [-0.649, 10.100], loss: 0.000051, mae: 0.004203, mean_q: 0.759085
 22855/100000: episode: 289, duration: 0.309s, episode steps: 51, steps per second: 165, episode reward: 0.752, mean reward: 0.015 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.307, 10.100], loss: 0.000036, mae: 0.002957, mean_q: 0.758936
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7890351414680481
1
 22858/100000: episode: 290, duration: 4.020s, episode steps: 3, steps per second: 1, episode reward: 0.754, mean reward: 0.251 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.118, 10.100], loss: 0.000030, mae: 0.002671, mean_q: 0.759104
 22959/100000: episode: 291, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.037, 10.147], loss: 0.000033, mae: 0.003624, mean_q: 0.758335
 23060/100000: episode: 292, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.659, 10.228], loss: 0.000059, mae: 0.004313, mean_q: 0.758482
 23161/100000: episode: 293, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.100], loss: 0.000044, mae: 0.003585, mean_q: 0.758530
 23262/100000: episode: 294, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.499, 10.287], loss: 0.000029, mae: 0.002828, mean_q: 0.758283
 23363/100000: episode: 295, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.391, 10.147], loss: 0.000045, mae: 0.003295, mean_q: 0.758392
 23464/100000: episode: 296, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.680, 10.169], loss: 0.000030, mae: 0.003388, mean_q: 0.757722
 23565/100000: episode: 297, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.779, 10.111], loss: 0.000037, mae: 0.003451, mean_q: 0.757608
 23666/100000: episode: 298, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.621, 10.247], loss: 0.000040, mae: 0.003482, mean_q: 0.757884
 23767/100000: episode: 299, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.784, 10.100], loss: 0.000044, mae: 0.004876, mean_q: 0.757123
 23868/100000: episode: 300, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.692, 10.158], loss: 0.000027, mae: 0.002807, mean_q: 0.757118
 23969/100000: episode: 301, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.911, 10.100], loss: 0.000050, mae: 0.004264, mean_q: 0.756862
 24070/100000: episode: 302, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.579, 10.100], loss: 0.000046, mae: 0.003388, mean_q: 0.756946
 24171/100000: episode: 303, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.688, 10.100], loss: 0.000054, mae: 0.003475, mean_q: 0.757196
 24272/100000: episode: 304, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.506, 10.100], loss: 0.000041, mae: 0.003358, mean_q: 0.756812
 24373/100000: episode: 305, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.402, 10.338], loss: 0.000035, mae: 0.003182, mean_q: 0.756644
 24474/100000: episode: 306, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.609, 10.100], loss: 0.000029, mae: 0.002690, mean_q: 0.756662
 24575/100000: episode: 307, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.792, 10.238], loss: 0.000048, mae: 0.003549, mean_q: 0.755903
 24676/100000: episode: 308, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.370], loss: 0.000022, mae: 0.002517, mean_q: 0.756257
 24777/100000: episode: 309, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.055, 10.100], loss: 0.000045, mae: 0.004029, mean_q: 0.756259
 24878/100000: episode: 310, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.489, 10.188], loss: 0.000052, mae: 0.003688, mean_q: 0.755926
 24979/100000: episode: 311, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.272, 10.100], loss: 0.000046, mae: 0.003629, mean_q: 0.756278
 25080/100000: episode: 312, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.920, 10.130], loss: 0.000041, mae: 0.003826, mean_q: 0.756403
 25181/100000: episode: 313, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.806, 10.118], loss: 0.000027, mae: 0.002659, mean_q: 0.756148
 25282/100000: episode: 314, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.801, 10.132], loss: 0.000025, mae: 0.002575, mean_q: 0.755654
 25383/100000: episode: 315, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.725, 10.324], loss: 0.000031, mae: 0.003282, mean_q: 0.755525
 25484/100000: episode: 316, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.981, 10.100], loss: 0.000043, mae: 0.003188, mean_q: 0.755254
 25585/100000: episode: 317, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.936, 10.261], loss: 0.000036, mae: 0.003269, mean_q: 0.755787
 25686/100000: episode: 318, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.705, 10.353], loss: 0.000047, mae: 0.003748, mean_q: 0.755518
 25787/100000: episode: 319, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.352, 10.111], loss: 0.000039, mae: 0.003337, mean_q: 0.755599
 25888/100000: episode: 320, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.590, 10.127], loss: 0.000042, mae: 0.003473, mean_q: 0.755690
 25989/100000: episode: 321, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.977, 10.137], loss: 0.000029, mae: 0.002600, mean_q: 0.755255
 26090/100000: episode: 322, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.660, mean reward: 0.007 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.654, 10.100], loss: 0.000031, mae: 0.002710, mean_q: 0.755290
 26191/100000: episode: 323, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.380, 10.100], loss: 0.000025, mae: 0.002810, mean_q: 0.754895
 26292/100000: episode: 324, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.252, 10.152], loss: 0.000020, mae: 0.002683, mean_q: 0.755180
 26393/100000: episode: 325, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.554, 10.360], loss: 0.000015, mae: 0.001847, mean_q: 0.755064
 26494/100000: episode: 326, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.883, mean reward: 0.009 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.072, 10.100], loss: 0.000025, mae: 0.002556, mean_q: 0.755150
 26595/100000: episode: 327, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.629, 10.486], loss: 0.000027, mae: 0.002324, mean_q: 0.754804
 26696/100000: episode: 328, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.794, 10.142], loss: 0.000032, mae: 0.002705, mean_q: 0.755080
 26797/100000: episode: 329, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.216, 10.290], loss: 0.000032, mae: 0.002928, mean_q: 0.754885
 26898/100000: episode: 330, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.687, 10.100], loss: 0.000028, mae: 0.002876, mean_q: 0.754539
 26999/100000: episode: 331, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.664, 10.125], loss: 0.000016, mae: 0.002077, mean_q: 0.754370
 27100/100000: episode: 332, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.100], loss: 0.000032, mae: 0.003178, mean_q: 0.754605
 27201/100000: episode: 333, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.343, 10.100], loss: 0.000020, mae: 0.002375, mean_q: 0.754469
 27302/100000: episode: 334, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.566, 10.100], loss: 0.000022, mae: 0.002330, mean_q: 0.754773
 27403/100000: episode: 335, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.565, 10.211], loss: 0.000030, mae: 0.002648, mean_q: 0.754348
 27504/100000: episode: 336, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.475, 10.167], loss: 0.000017, mae: 0.002034, mean_q: 0.754389
 27605/100000: episode: 337, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.426, 10.100], loss: 0.000022, mae: 0.002440, mean_q: 0.753900
 27706/100000: episode: 338, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.057, 10.203], loss: 0.000018, mae: 0.002034, mean_q: 0.754123
 27807/100000: episode: 339, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.015, 10.100], loss: 0.000011, mae: 0.001722, mean_q: 0.754049
 27908/100000: episode: 340, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.897, 10.359], loss: 0.000011, mae: 0.001705, mean_q: 0.754131
 28009/100000: episode: 341, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.911, 10.395], loss: 0.000008, mae: 0.001979, mean_q: 0.754177
 28110/100000: episode: 342, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.701, 10.100], loss: 0.000012, mae: 0.001698, mean_q: 0.754302
 28211/100000: episode: 343, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.628, 10.100], loss: 0.000023, mae: 0.002577, mean_q: 0.754512
 28312/100000: episode: 344, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.658, 10.387], loss: 0.000013, mae: 0.002240, mean_q: 0.754363
 28413/100000: episode: 345, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.697, 10.100], loss: 0.000013, mae: 0.001966, mean_q: 0.754699
 28514/100000: episode: 346, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.830, 10.100], loss: 0.000013, mae: 0.001855, mean_q: 0.755195
 28615/100000: episode: 347, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.941, 10.240], loss: 0.000017, mae: 0.001971, mean_q: 0.755410
 28716/100000: episode: 348, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.735, 10.191], loss: 0.000017, mae: 0.001989, mean_q: 0.755277
 28817/100000: episode: 349, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.000, 10.100], loss: 0.000013, mae: 0.001982, mean_q: 0.755617
 28918/100000: episode: 350, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.221, 10.189], loss: 0.000008, mae: 0.001602, mean_q: 0.755996
 29019/100000: episode: 351, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.751, 10.131], loss: 0.000016, mae: 0.002056, mean_q: 0.756155
 29120/100000: episode: 352, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.723, 10.165], loss: 0.000015, mae: 0.002114, mean_q: 0.755811
 29221/100000: episode: 353, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.581, 10.267], loss: 0.000015, mae: 0.002013, mean_q: 0.756191
 29322/100000: episode: 354, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.701, 10.103], loss: 0.000023, mae: 0.002874, mean_q: 0.756136
 29423/100000: episode: 355, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.265, 10.134], loss: 0.000010, mae: 0.002060, mean_q: 0.756345
 29524/100000: episode: 356, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.503, 10.100], loss: 0.000017, mae: 0.001891, mean_q: 0.756639
 29625/100000: episode: 357, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.950, 10.100], loss: 0.000014, mae: 0.002270, mean_q: 0.756790
 29726/100000: episode: 358, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.899, 10.100], loss: 0.000017, mae: 0.002128, mean_q: 0.756911
 29827/100000: episode: 359, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.634, 10.273], loss: 0.000006, mae: 0.001243, mean_q: 0.757486
 29928/100000: episode: 360, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.464, 10.100], loss: 0.000007, mae: 0.001077, mean_q: 0.757488
 30029/100000: episode: 361, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.171, 10.100], loss: 0.000020, mae: 0.001928, mean_q: 0.757748
 30130/100000: episode: 362, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.149, 10.183], loss: 0.000010, mae: 0.001681, mean_q: 0.757522
 30231/100000: episode: 363, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.687, 10.100], loss: 0.000009, mae: 0.001448, mean_q: 0.757778
 30332/100000: episode: 364, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.141, 10.100], loss: 0.000010, mae: 0.001624, mean_q: 0.757896
 30433/100000: episode: 365, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.473, 10.100], loss: 0.000005, mae: 0.001391, mean_q: 0.758208
 30534/100000: episode: 366, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.177, 10.119], loss: 0.000011, mae: 0.001554, mean_q: 0.758773
 30635/100000: episode: 367, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.477, 10.258], loss: 0.000014, mae: 0.001774, mean_q: 0.758989
 30736/100000: episode: 368, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.715, 10.409], loss: 0.000012, mae: 0.001875, mean_q: 0.759097
 30837/100000: episode: 369, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.931, 10.113], loss: 0.000017, mae: 0.001909, mean_q: 0.759583
 30938/100000: episode: 370, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.531, 10.180], loss: 0.000019, mae: 0.002198, mean_q: 0.759532
 31039/100000: episode: 371, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.215, 10.100], loss: 0.000015, mae: 0.002231, mean_q: 0.759550
 31140/100000: episode: 372, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.350, 10.197], loss: 0.000009, mae: 0.001507, mean_q: 0.759364
 31241/100000: episode: 373, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.100], loss: 0.000007, mae: 0.001647, mean_q: 0.759406
 31342/100000: episode: 374, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.168, 10.208], loss: 0.000011, mae: 0.002039, mean_q: 0.759721
 31443/100000: episode: 375, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.729, 10.407], loss: 0.000006, mae: 0.001289, mean_q: 0.759650
 31544/100000: episode: 376, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.002, 10.100], loss: 0.000008, mae: 0.001427, mean_q: 0.759779
 31645/100000: episode: 377, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.110, 10.305], loss: 0.000013, mae: 0.001989, mean_q: 0.759680
 31746/100000: episode: 378, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.149, 10.100], loss: 0.000010, mae: 0.001789, mean_q: 0.759357
 31847/100000: episode: 379, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.953, 10.101], loss: 0.000010, mae: 0.001426, mean_q: 0.759481
 31948/100000: episode: 380, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.944, 10.287], loss: 0.000009, mae: 0.001754, mean_q: 0.759709
 32049/100000: episode: 381, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.039, 10.100], loss: 0.000011, mae: 0.001811, mean_q: 0.759655
 32150/100000: episode: 382, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.078, 10.108], loss: 0.000009, mae: 0.001564, mean_q: 0.759758
 32251/100000: episode: 383, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.535, 10.539], loss: 0.000008, mae: 0.001361, mean_q: 0.759446
 32352/100000: episode: 384, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.100], loss: 0.000010, mae: 0.001644, mean_q: 0.759539
 32453/100000: episode: 385, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.631, 10.451], loss: 0.000008, mae: 0.001552, mean_q: 0.759485
 32554/100000: episode: 386, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.890, 10.114], loss: 0.000007, mae: 0.001290, mean_q: 0.759362
 32655/100000: episode: 387, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.013, 10.195], loss: 0.000008, mae: 0.001472, mean_q: 0.759544
 32756/100000: episode: 388, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.682, mean reward: 0.007 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.196, 10.133], loss: 0.000005, mae: 0.001000, mean_q: 0.759460
 32857/100000: episode: 389, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.589, 10.100], loss: 0.000009, mae: 0.001625, mean_q: 0.759578
[Info] 1-TH LEVEL FOUND: 0.767646849155426, Considering 10/100 traces
 32958/100000: episode: 390, duration: 4.789s, episode steps: 101, steps per second: 21, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.100], loss: 0.000014, mae: 0.002092, mean_q: 0.759479
 33009/100000: episode: 391, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.774, mean reward: 0.015 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.589, 10.100], loss: 0.000002, mae: 0.000804, mean_q: 0.759453
 33061/100000: episode: 392, duration: 0.301s, episode steps: 52, steps per second: 173, episode reward: 0.765, mean reward: 0.015 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.025, 10.100], loss: 0.000007, mae: 0.001393, mean_q: 0.759403
 33114/100000: episode: 393, duration: 0.296s, episode steps: 53, steps per second: 179, episode reward: 0.807, mean reward: 0.015 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.992, 10.100], loss: 0.000012, mae: 0.001590, mean_q: 0.759564
 33166/100000: episode: 394, duration: 0.296s, episode steps: 52, steps per second: 176, episode reward: 0.753, mean reward: 0.014 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.342, 10.272], loss: 0.000005, mae: 0.000941, mean_q: 0.759835
 33217/100000: episode: 395, duration: 0.296s, episode steps: 51, steps per second: 172, episode reward: 0.738, mean reward: 0.014 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.223, 10.112], loss: 0.000009, mae: 0.001728, mean_q: 0.759970
 33270/100000: episode: 396, duration: 0.281s, episode steps: 53, steps per second: 188, episode reward: 0.747, mean reward: 0.014 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.452, 10.133], loss: 0.000008, mae: 0.001453, mean_q: 0.759538
 33327/100000: episode: 397, duration: 0.334s, episode steps: 57, steps per second: 171, episode reward: 0.751, mean reward: 0.013 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-0.861, 10.100], loss: 0.000012, mae: 0.001899, mean_q: 0.759670
 33381/100000: episode: 398, duration: 0.305s, episode steps: 54, steps per second: 177, episode reward: 0.799, mean reward: 0.015 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.112, 10.100], loss: 0.000019, mae: 0.002499, mean_q: 0.760414
 33432/100000: episode: 399, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 0.780, mean reward: 0.015 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.215, 10.100], loss: 0.000008, mae: 0.001302, mean_q: 0.759854
 33486/100000: episode: 400, duration: 0.307s, episode steps: 54, steps per second: 176, episode reward: 0.755, mean reward: 0.014 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.294, 10.314], loss: 0.000010, mae: 0.001952, mean_q: 0.759877
 33543/100000: episode: 401, duration: 0.330s, episode steps: 57, steps per second: 173, episode reward: 0.738, mean reward: 0.013 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.601, 10.100], loss: 0.000005, mae: 0.001047, mean_q: 0.759970
 33595/100000: episode: 402, duration: 0.315s, episode steps: 52, steps per second: 165, episode reward: 0.717, mean reward: 0.014 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.742, 10.100], loss: 0.000004, mae: 0.001000, mean_q: 0.759948
 33646/100000: episode: 403, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 0.789, mean reward: 0.015 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.472, 10.189], loss: 0.000009, mae: 0.001667, mean_q: 0.759910
 33698/100000: episode: 404, duration: 0.316s, episode steps: 52, steps per second: 164, episode reward: 0.744, mean reward: 0.014 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.202, 10.181], loss: 0.000011, mae: 0.001393, mean_q: 0.759933
 33752/100000: episode: 405, duration: 0.333s, episode steps: 54, steps per second: 162, episode reward: 0.804, mean reward: 0.015 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.407, 10.116], loss: 0.000008, mae: 0.001382, mean_q: 0.759923
 33805/100000: episode: 406, duration: 0.305s, episode steps: 53, steps per second: 174, episode reward: 0.759, mean reward: 0.014 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.683, 10.207], loss: 0.000008, mae: 0.001395, mean_q: 0.760092
 33859/100000: episode: 407, duration: 0.321s, episode steps: 54, steps per second: 168, episode reward: 0.721, mean reward: 0.013 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.410, 10.100], loss: 0.000008, mae: 0.001539, mean_q: 0.759941
 33911/100000: episode: 408, duration: 0.304s, episode steps: 52, steps per second: 171, episode reward: 0.790, mean reward: 0.015 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.195, 10.100], loss: 0.000016, mae: 0.002194, mean_q: 0.759797
 33962/100000: episode: 409, duration: 0.306s, episode steps: 51, steps per second: 166, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.963, 10.100], loss: 0.000011, mae: 0.001568, mean_q: 0.759869
 34019/100000: episode: 410, duration: 0.333s, episode steps: 57, steps per second: 171, episode reward: 0.768, mean reward: 0.013 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.134, 10.100], loss: 0.000007, mae: 0.001675, mean_q: 0.759604
 34072/100000: episode: 411, duration: 0.317s, episode steps: 53, steps per second: 167, episode reward: 0.752, mean reward: 0.014 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.412, 10.100], loss: 0.000013, mae: 0.001643, mean_q: 0.759863
 34125/100000: episode: 412, duration: 0.292s, episode steps: 53, steps per second: 182, episode reward: 0.693, mean reward: 0.013 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.319, 10.100], loss: 0.000011, mae: 0.001590, mean_q: 0.759414
 34179/100000: episode: 413, duration: 0.312s, episode steps: 54, steps per second: 173, episode reward: 0.901, mean reward: 0.017 [0.000, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-2.735, 10.100], loss: 0.000016, mae: 0.002262, mean_q: 0.759669
 34231/100000: episode: 414, duration: 0.296s, episode steps: 52, steps per second: 176, episode reward: 0.829, mean reward: 0.016 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.655, 10.100], loss: 0.000019, mae: 0.001780, mean_q: 0.759646
 34283/100000: episode: 415, duration: 0.308s, episode steps: 52, steps per second: 169, episode reward: 0.703, mean reward: 0.014 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.282, 10.253], loss: 0.000015, mae: 0.001790, mean_q: 0.759160
 34336/100000: episode: 416, duration: 0.284s, episode steps: 53, steps per second: 187, episode reward: 0.771, mean reward: 0.015 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.746, 10.239], loss: 0.000016, mae: 0.002516, mean_q: 0.759013
 34387/100000: episode: 417, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 0.701, mean reward: 0.014 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.608, 10.181], loss: 0.000020, mae: 0.002500, mean_q: 0.759150
 34444/100000: episode: 418, duration: 0.332s, episode steps: 57, steps per second: 172, episode reward: 0.786, mean reward: 0.014 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.840 [-0.488, 10.100], loss: 0.000016, mae: 0.001596, mean_q: 0.759184
 34498/100000: episode: 419, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 0.720, mean reward: 0.013 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.472, 10.100], loss: 0.000017, mae: 0.001815, mean_q: 0.759187
 34551/100000: episode: 420, duration: 0.307s, episode steps: 53, steps per second: 173, episode reward: 0.797, mean reward: 0.015 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.387, 10.100], loss: 0.000027, mae: 0.002751, mean_q: 0.759249
 34605/100000: episode: 421, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 0.860, mean reward: 0.016 [0.000, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-1.080, 10.100], loss: 0.000006, mae: 0.001029, mean_q: 0.758944
 34656/100000: episode: 422, duration: 0.312s, episode steps: 51, steps per second: 164, episode reward: 0.726, mean reward: 0.014 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.160, 10.100], loss: 0.000016, mae: 0.001957, mean_q: 0.759003
 34708/100000: episode: 423, duration: 0.311s, episode steps: 52, steps per second: 167, episode reward: 0.771, mean reward: 0.015 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.616, 10.128], loss: 0.000007, mae: 0.001350, mean_q: 0.759087
 34762/100000: episode: 424, duration: 0.328s, episode steps: 54, steps per second: 165, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.358, 10.100], loss: 0.000015, mae: 0.001557, mean_q: 0.758869
 34814/100000: episode: 425, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 0.690, mean reward: 0.013 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.326, 10.206], loss: 0.000018, mae: 0.001993, mean_q: 0.759122
 34865/100000: episode: 426, duration: 0.303s, episode steps: 51, steps per second: 168, episode reward: 0.659, mean reward: 0.013 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.329, 10.100], loss: 0.000015, mae: 0.001808, mean_q: 0.758631
 34919/100000: episode: 427, duration: 0.322s, episode steps: 54, steps per second: 168, episode reward: 0.715, mean reward: 0.013 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.691, 10.121], loss: 0.000020, mae: 0.002395, mean_q: 0.758968
 34973/100000: episode: 428, duration: 0.322s, episode steps: 54, steps per second: 167, episode reward: 0.783, mean reward: 0.015 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.652, 10.105], loss: 0.000034, mae: 0.002696, mean_q: 0.759240
 35030/100000: episode: 429, duration: 0.344s, episode steps: 57, steps per second: 166, episode reward: 0.758, mean reward: 0.013 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.271, 10.100], loss: 0.000022, mae: 0.002659, mean_q: 0.758712
 35087/100000: episode: 430, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 0.702, mean reward: 0.012 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.131, 10.100], loss: 0.000018, mae: 0.002096, mean_q: 0.758774
 35140/100000: episode: 431, duration: 0.308s, episode steps: 53, steps per second: 172, episode reward: 0.712, mean reward: 0.013 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.070, 10.100], loss: 0.000015, mae: 0.001639, mean_q: 0.758628
 35194/100000: episode: 432, duration: 0.321s, episode steps: 54, steps per second: 168, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-1.674, 10.102], loss: 0.000017, mae: 0.002190, mean_q: 0.758457
 35248/100000: episode: 433, duration: 0.308s, episode steps: 54, steps per second: 175, episode reward: 0.759, mean reward: 0.014 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.741, 10.100], loss: 0.000026, mae: 0.002338, mean_q: 0.758274
 35301/100000: episode: 434, duration: 0.310s, episode steps: 53, steps per second: 171, episode reward: 0.722, mean reward: 0.014 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.304, 10.100], loss: 0.000019, mae: 0.001964, mean_q: 0.758415
 35354/100000: episode: 435, duration: 0.310s, episode steps: 53, steps per second: 171, episode reward: 0.864, mean reward: 0.016 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.373, 10.134], loss: 0.000020, mae: 0.002128, mean_q: 0.758230
 35408/100000: episode: 436, duration: 0.328s, episode steps: 54, steps per second: 165, episode reward: 0.740, mean reward: 0.014 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.035, 10.262], loss: 0.000024, mae: 0.002156, mean_q: 0.758156
 35460/100000: episode: 437, duration: 0.293s, episode steps: 52, steps per second: 178, episode reward: 0.695, mean reward: 0.013 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.916, 10.174], loss: 0.000014, mae: 0.001842, mean_q: 0.757966
 35513/100000: episode: 438, duration: 0.319s, episode steps: 53, steps per second: 166, episode reward: 0.791, mean reward: 0.015 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.246, 10.100], loss: 0.000023, mae: 0.001774, mean_q: 0.757986
 35570/100000: episode: 439, duration: 0.343s, episode steps: 57, steps per second: 166, episode reward: 0.809, mean reward: 0.014 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.707, 10.100], loss: 0.000012, mae: 0.001596, mean_q: 0.757834
 35622/100000: episode: 440, duration: 0.301s, episode steps: 52, steps per second: 173, episode reward: 0.734, mean reward: 0.014 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.575, 10.102], loss: 0.000025, mae: 0.002473, mean_q: 0.757835
 35676/100000: episode: 441, duration: 0.340s, episode steps: 54, steps per second: 159, episode reward: 0.727, mean reward: 0.013 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.527, 10.193], loss: 0.000035, mae: 0.002774, mean_q: 0.758147
 35730/100000: episode: 442, duration: 0.313s, episode steps: 54, steps per second: 172, episode reward: 0.770, mean reward: 0.014 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.582, 10.100], loss: 0.000008, mae: 0.001322, mean_q: 0.757983
 35783/100000: episode: 443, duration: 0.321s, episode steps: 53, steps per second: 165, episode reward: 0.787, mean reward: 0.015 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.372, 10.100], loss: 0.000011, mae: 0.001994, mean_q: 0.757676
 35840/100000: episode: 444, duration: 0.319s, episode steps: 57, steps per second: 179, episode reward: 0.729, mean reward: 0.013 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.035, 10.242], loss: 0.000024, mae: 0.002076, mean_q: 0.757417
 35893/100000: episode: 445, duration: 0.297s, episode steps: 53, steps per second: 179, episode reward: 0.760, mean reward: 0.014 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.318, 10.100], loss: 0.000011, mae: 0.001808, mean_q: 0.757523
 35947/100000: episode: 446, duration: 0.302s, episode steps: 54, steps per second: 179, episode reward: 0.799, mean reward: 0.015 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.591, 10.100], loss: 0.000014, mae: 0.002191, mean_q: 0.757343
 36001/100000: episode: 447, duration: 0.324s, episode steps: 54, steps per second: 167, episode reward: 0.804, mean reward: 0.015 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.634, 10.100], loss: 0.000011, mae: 0.001499, mean_q: 0.757132
 36052/100000: episode: 448, duration: 0.312s, episode steps: 51, steps per second: 163, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.960, 10.100], loss: 0.000021, mae: 0.001932, mean_q: 0.756748
 36106/100000: episode: 449, duration: 0.328s, episode steps: 54, steps per second: 165, episode reward: 0.807, mean reward: 0.015 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.601, 10.100], loss: 0.000024, mae: 0.002064, mean_q: 0.756721
 36160/100000: episode: 450, duration: 0.334s, episode steps: 54, steps per second: 162, episode reward: 0.773, mean reward: 0.014 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.814, 10.353], loss: 0.000023, mae: 0.002073, mean_q: 0.756809
 36213/100000: episode: 451, duration: 0.324s, episode steps: 53, steps per second: 164, episode reward: 0.889, mean reward: 0.017 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.439, 10.100], loss: 0.000020, mae: 0.002057, mean_q: 0.756799
 36270/100000: episode: 452, duration: 0.334s, episode steps: 57, steps per second: 171, episode reward: 0.858, mean reward: 0.015 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.865, 10.100], loss: 0.000030, mae: 0.002757, mean_q: 0.756488
 36321/100000: episode: 453, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.765, mean reward: 0.015 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.605, 10.100], loss: 0.000026, mae: 0.002727, mean_q: 0.756688
 36375/100000: episode: 454, duration: 0.333s, episode steps: 54, steps per second: 162, episode reward: 0.770, mean reward: 0.014 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.974, 10.100], loss: 0.000025, mae: 0.002364, mean_q: 0.756566
 36428/100000: episode: 455, duration: 0.298s, episode steps: 53, steps per second: 178, episode reward: 0.804, mean reward: 0.015 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.279, 10.100], loss: 0.000036, mae: 0.002505, mean_q: 0.756648
 36481/100000: episode: 456, duration: 0.324s, episode steps: 53, steps per second: 163, episode reward: 0.733, mean reward: 0.014 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.826, 10.100], loss: 0.000020, mae: 0.001897, mean_q: 0.756315
 36532/100000: episode: 457, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.653, mean reward: 0.013 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.038, 10.148], loss: 0.000011, mae: 0.001202, mean_q: 0.756318
 36589/100000: episode: 458, duration: 0.323s, episode steps: 57, steps per second: 176, episode reward: 0.801, mean reward: 0.014 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-1.033, 10.100], loss: 0.000026, mae: 0.002000, mean_q: 0.756708
 36643/100000: episode: 459, duration: 0.322s, episode steps: 54, steps per second: 168, episode reward: 0.917, mean reward: 0.017 [0.000, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.209, 10.191], loss: 0.000024, mae: 0.002493, mean_q: 0.756728
 36694/100000: episode: 460, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.674, mean reward: 0.013 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.363, 10.112], loss: 0.000027, mae: 0.002446, mean_q: 0.756968
 36747/100000: episode: 461, duration: 0.311s, episode steps: 53, steps per second: 171, episode reward: 0.755, mean reward: 0.014 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.800, 10.100], loss: 0.000022, mae: 0.002681, mean_q: 0.756796
 36801/100000: episode: 462, duration: 0.328s, episode steps: 54, steps per second: 165, episode reward: 0.787, mean reward: 0.015 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-1.080, 10.100], loss: 0.000034, mae: 0.002769, mean_q: 0.756672
 36854/100000: episode: 463, duration: 0.300s, episode steps: 53, steps per second: 177, episode reward: 0.707, mean reward: 0.013 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.089, 10.100], loss: 0.000016, mae: 0.002083, mean_q: 0.756106
 36907/100000: episode: 464, duration: 0.311s, episode steps: 53, steps per second: 171, episode reward: 0.835, mean reward: 0.016 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.670, 10.246], loss: 0.000029, mae: 0.002597, mean_q: 0.756299
 36960/100000: episode: 465, duration: 0.309s, episode steps: 53, steps per second: 171, episode reward: 0.741, mean reward: 0.014 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.781, 10.100], loss: 0.000020, mae: 0.002048, mean_q: 0.756480
 37014/100000: episode: 466, duration: 0.308s, episode steps: 54, steps per second: 176, episode reward: 0.824, mean reward: 0.015 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.660, 10.100], loss: 0.000030, mae: 0.002400, mean_q: 0.756317
 37065/100000: episode: 467, duration: 0.315s, episode steps: 51, steps per second: 162, episode reward: 0.653, mean reward: 0.013 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.476, 10.100], loss: 0.000037, mae: 0.002829, mean_q: 0.756886
 37117/100000: episode: 468, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 0.803, mean reward: 0.015 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.190, 10.100], loss: 0.000020, mae: 0.001752, mean_q: 0.756232
 37171/100000: episode: 469, duration: 0.297s, episode steps: 54, steps per second: 182, episode reward: 0.783, mean reward: 0.015 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.242, 10.100], loss: 0.000035, mae: 0.002391, mean_q: 0.756577
 37224/100000: episode: 470, duration: 0.313s, episode steps: 53, steps per second: 169, episode reward: 0.910, mean reward: 0.017 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.317, 10.100], loss: 0.000027, mae: 0.002315, mean_q: 0.756242
 37277/100000: episode: 471, duration: 0.329s, episode steps: 53, steps per second: 161, episode reward: 0.795, mean reward: 0.015 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.145, 10.100], loss: 0.000027, mae: 0.002026, mean_q: 0.756105
 37330/100000: episode: 472, duration: 0.343s, episode steps: 53, steps per second: 154, episode reward: 0.719, mean reward: 0.014 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.864, 10.116], loss: 0.000020, mae: 0.001710, mean_q: 0.756090
 37387/100000: episode: 473, duration: 0.357s, episode steps: 57, steps per second: 160, episode reward: 0.723, mean reward: 0.013 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.253, 10.103], loss: 0.000027, mae: 0.002042, mean_q: 0.756178
 37439/100000: episode: 474, duration: 0.303s, episode steps: 52, steps per second: 172, episode reward: 0.723, mean reward: 0.014 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.314, 10.100], loss: 0.000026, mae: 0.002220, mean_q: 0.756095
 37491/100000: episode: 475, duration: 0.302s, episode steps: 52, steps per second: 172, episode reward: 0.745, mean reward: 0.014 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.483, 10.100], loss: 0.000036, mae: 0.002645, mean_q: 0.756267
 37548/100000: episode: 476, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 0.725, mean reward: 0.013 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.771, 10.135], loss: 0.000025, mae: 0.002464, mean_q: 0.756498
 37601/100000: episode: 477, duration: 0.301s, episode steps: 53, steps per second: 176, episode reward: 0.729, mean reward: 0.014 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.147, 10.429], loss: 0.000022, mae: 0.001954, mean_q: 0.756178
 37655/100000: episode: 478, duration: 0.321s, episode steps: 54, steps per second: 168, episode reward: 0.756, mean reward: 0.014 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.162, 10.175], loss: 0.000034, mae: 0.002500, mean_q: 0.756216
 37707/100000: episode: 479, duration: 0.310s, episode steps: 52, steps per second: 168, episode reward: 0.833, mean reward: 0.016 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.074, 10.100], loss: 0.000023, mae: 0.001900, mean_q: 0.756218
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.767646849155426
1
 37761/100000: episode: 480, duration: 4.325s, episode steps: 54, steps per second: 12, episode reward: 0.774, mean reward: 0.014 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.954, 10.100], loss: 0.000021, mae: 0.001832, mean_q: 0.756189
 37862/100000: episode: 481, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.752, 10.122], loss: 0.000027, mae: 0.002165, mean_q: 0.756235
 37963/100000: episode: 482, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.529, 10.237], loss: 0.000020, mae: 0.001630, mean_q: 0.756459
 38064/100000: episode: 483, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.170, 10.100], loss: 0.000021, mae: 0.001810, mean_q: 0.756323
 38165/100000: episode: 484, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.644, 10.100], loss: 0.000033, mae: 0.002426, mean_q: 0.756531
 38266/100000: episode: 485, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.981, 10.100], loss: 0.000025, mae: 0.002211, mean_q: 0.756701
 38367/100000: episode: 486, duration: 0.635s, episode steps: 101, steps per second: 159, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.899, 10.100], loss: 0.000035, mae: 0.002596, mean_q: 0.756726
 38468/100000: episode: 487, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.926, 10.100], loss: 0.000040, mae: 0.003049, mean_q: 0.756972
 38569/100000: episode: 488, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.843, 10.100], loss: 0.000044, mae: 0.002631, mean_q: 0.757133
 38670/100000: episode: 489, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.709, 10.100], loss: 0.000029, mae: 0.001871, mean_q: 0.757308
 38771/100000: episode: 490, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.811, 10.242], loss: 0.000025, mae: 0.001859, mean_q: 0.757164
 38872/100000: episode: 491, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.490, 10.168], loss: 0.000043, mae: 0.002591, mean_q: 0.757468
 38973/100000: episode: 492, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.100], loss: 0.000024, mae: 0.002153, mean_q: 0.757071
 39074/100000: episode: 493, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.414, 10.149], loss: 0.000025, mae: 0.001862, mean_q: 0.757389
 39175/100000: episode: 494, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.100], loss: 0.000021, mae: 0.002054, mean_q: 0.757416
 39276/100000: episode: 495, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.836, 10.192], loss: 0.000017, mae: 0.001693, mean_q: 0.757271
 39377/100000: episode: 496, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.307, 10.201], loss: 0.000026, mae: 0.002023, mean_q: 0.757210
 39478/100000: episode: 497, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.992, 10.394], loss: 0.000024, mae: 0.001870, mean_q: 0.757171
 39579/100000: episode: 498, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.028, 10.165], loss: 0.000025, mae: 0.002289, mean_q: 0.756634
 39680/100000: episode: 499, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.453, 10.100], loss: 0.000024, mae: 0.001921, mean_q: 0.757009
 39781/100000: episode: 500, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.653, 10.246], loss: 0.000012, mae: 0.001509, mean_q: 0.756879
 39882/100000: episode: 501, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.500, 10.259], loss: 0.000026, mae: 0.002241, mean_q: 0.757297
 39983/100000: episode: 502, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.903, 10.302], loss: 0.000015, mae: 0.001668, mean_q: 0.756800
 40084/100000: episode: 503, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.431, 10.100], loss: 0.000012, mae: 0.001340, mean_q: 0.757095
 40185/100000: episode: 504, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.340, 10.393], loss: 0.000020, mae: 0.001911, mean_q: 0.757112
 40286/100000: episode: 505, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.602, 10.317], loss: 0.000026, mae: 0.002040, mean_q: 0.757096
 40387/100000: episode: 506, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.307, 10.100], loss: 0.000029, mae: 0.002107, mean_q: 0.757424
 40488/100000: episode: 507, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.533, 10.100], loss: 0.000010, mae: 0.001351, mean_q: 0.757043
 40589/100000: episode: 508, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.829, 10.100], loss: 0.000018, mae: 0.001687, mean_q: 0.757362
 40690/100000: episode: 509, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.461, 10.100], loss: 0.000018, mae: 0.001425, mean_q: 0.757447
 40791/100000: episode: 510, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.008, 10.100], loss: 0.000021, mae: 0.001975, mean_q: 0.757475
 40892/100000: episode: 511, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.482, 10.171], loss: 0.000012, mae: 0.001297, mean_q: 0.757336
 40993/100000: episode: 512, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.252, 10.190], loss: 0.000024, mae: 0.002696, mean_q: 0.757458
 41094/100000: episode: 513, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.157, 10.172], loss: 0.000012, mae: 0.001424, mean_q: 0.757423
 41195/100000: episode: 514, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.717, 10.100], loss: 0.000015, mae: 0.001276, mean_q: 0.757626
 41296/100000: episode: 515, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.393, 10.100], loss: 0.000019, mae: 0.001781, mean_q: 0.757554
 41397/100000: episode: 516, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.966, 10.116], loss: 0.000017, mae: 0.001559, mean_q: 0.757447
 41498/100000: episode: 517, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.145, 10.100], loss: 0.000023, mae: 0.001751, mean_q: 0.757300
 41599/100000: episode: 518, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.271, 10.215], loss: 0.000021, mae: 0.001924, mean_q: 0.756988
 41700/100000: episode: 519, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.311, 10.152], loss: 0.000017, mae: 0.001954, mean_q: 0.757029
 41801/100000: episode: 520, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.115, 10.124], loss: 0.000010, mae: 0.001259, mean_q: 0.756706
 41902/100000: episode: 521, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.642, 10.100], loss: 0.000008, mae: 0.001236, mean_q: 0.756547
 42003/100000: episode: 522, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.321, 10.306], loss: 0.000009, mae: 0.001181, mean_q: 0.756474
 42104/100000: episode: 523, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.540, 10.100], loss: 0.000013, mae: 0.001459, mean_q: 0.756313
 42205/100000: episode: 524, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.100], loss: 0.000001, mae: 0.000511, mean_q: 0.756420
 42306/100000: episode: 525, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.629, 10.100], loss: 0.000012, mae: 0.001395, mean_q: 0.756073
 42407/100000: episode: 526, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.300, 10.100], loss: 0.000006, mae: 0.001005, mean_q: 0.755843
 42508/100000: episode: 527, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.100], loss: 0.000010, mae: 0.001171, mean_q: 0.755809
 42609/100000: episode: 528, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.666, 10.170], loss: 0.000012, mae: 0.001610, mean_q: 0.755905
 42710/100000: episode: 529, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.488, 10.417], loss: 0.000008, mae: 0.001189, mean_q: 0.755789
 42811/100000: episode: 530, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.108, 10.100], loss: 0.000011, mae: 0.001415, mean_q: 0.755962
 42912/100000: episode: 531, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.158, 10.306], loss: 0.000011, mae: 0.001505, mean_q: 0.755866
 43013/100000: episode: 532, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.616, 10.256], loss: 0.000007, mae: 0.001023, mean_q: 0.756140
 43114/100000: episode: 533, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.674, 10.276], loss: 0.000010, mae: 0.001436, mean_q: 0.755973
 43215/100000: episode: 534, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.925, 10.261], loss: 0.000009, mae: 0.001359, mean_q: 0.756181
 43316/100000: episode: 535, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.305, 10.243], loss: 0.000005, mae: 0.000995, mean_q: 0.756014
 43417/100000: episode: 536, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.720, 10.155], loss: 0.000011, mae: 0.001332, mean_q: 0.756003
 43518/100000: episode: 537, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.639, 10.116], loss: 0.000005, mae: 0.000843, mean_q: 0.756318
 43619/100000: episode: 538, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.764, 10.287], loss: 0.000007, mae: 0.001203, mean_q: 0.755919
 43720/100000: episode: 539, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.708, 10.100], loss: 0.000013, mae: 0.001511, mean_q: 0.756100
 43821/100000: episode: 540, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.406, 10.348], loss: 0.000014, mae: 0.001688, mean_q: 0.756197
 43922/100000: episode: 541, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.421, 10.100], loss: 0.000006, mae: 0.000893, mean_q: 0.756170
 44023/100000: episode: 542, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.406, 10.158], loss: 0.000012, mae: 0.001293, mean_q: 0.756236
 44124/100000: episode: 543, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.312, 10.301], loss: 0.000006, mae: 0.001055, mean_q: 0.756238
 44225/100000: episode: 544, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.697, 10.261], loss: 0.000008, mae: 0.001119, mean_q: 0.756224
 44326/100000: episode: 545, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.567, 10.194], loss: 0.000008, mae: 0.001480, mean_q: 0.756333
 44427/100000: episode: 546, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.241, 10.242], loss: 0.000010, mae: 0.000977, mean_q: 0.756368
 44528/100000: episode: 547, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.506, 10.314], loss: 0.000013, mae: 0.001493, mean_q: 0.756505
 44629/100000: episode: 548, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.447, 10.100], loss: 0.000010, mae: 0.001425, mean_q: 0.756589
 44730/100000: episode: 549, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.070, 10.269], loss: 0.000009, mae: 0.001082, mean_q: 0.756446
 44831/100000: episode: 550, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.131, 10.100], loss: 0.000011, mae: 0.001521, mean_q: 0.756117
 44932/100000: episode: 551, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.759, 10.100], loss: 0.000013, mae: 0.001193, mean_q: 0.756388
 45033/100000: episode: 552, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.501, 10.100], loss: 0.000012, mae: 0.001370, mean_q: 0.756447
 45134/100000: episode: 553, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.778, 10.171], loss: 0.000012, mae: 0.001426, mean_q: 0.756208
 45235/100000: episode: 554, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.773, 10.431], loss: 0.000008, mae: 0.001226, mean_q: 0.756563
 45336/100000: episode: 555, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.559, 10.100], loss: 0.000017, mae: 0.001817, mean_q: 0.756687
 45437/100000: episode: 556, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.663, 10.100], loss: 0.000010, mae: 0.001604, mean_q: 0.756783
 45538/100000: episode: 557, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.925, 10.177], loss: 0.000013, mae: 0.001418, mean_q: 0.756893
 45639/100000: episode: 558, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.739, 10.105], loss: 0.000008, mae: 0.001070, mean_q: 0.756955
 45740/100000: episode: 559, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.190, 10.128], loss: 0.000007, mae: 0.001196, mean_q: 0.756832
 45841/100000: episode: 560, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.596, 10.240], loss: 0.000008, mae: 0.000970, mean_q: 0.756924
 45942/100000: episode: 561, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.110, 10.104], loss: 0.000011, mae: 0.001198, mean_q: 0.756948
 46043/100000: episode: 562, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.714, 10.100], loss: 0.000008, mae: 0.001184, mean_q: 0.757026
 46144/100000: episode: 563, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.618, 10.174], loss: 0.000006, mae: 0.001043, mean_q: 0.757057
 46245/100000: episode: 564, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.485, 10.100], loss: 0.000010, mae: 0.001277, mean_q: 0.757030
 46346/100000: episode: 565, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.829, 10.279], loss: 0.000012, mae: 0.001349, mean_q: 0.757283
 46447/100000: episode: 566, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.929, mean reward: 0.009 [0.000, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.785, 10.100], loss: 0.000016, mae: 0.001809, mean_q: 0.757189
 46548/100000: episode: 567, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.071, 10.140], loss: 0.000004, mae: 0.000922, mean_q: 0.757233
 46649/100000: episode: 568, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.245, 10.176], loss: 0.000021, mae: 0.001754, mean_q: 0.757125
 46750/100000: episode: 569, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.785, 10.144], loss: 0.000011, mae: 0.001275, mean_q: 0.757099
 46851/100000: episode: 570, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.569, 10.100], loss: 0.000010, mae: 0.001188, mean_q: 0.757291
 46952/100000: episode: 571, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.816, 10.316], loss: 0.000017, mae: 0.001335, mean_q: 0.757299
 47053/100000: episode: 572, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.100, 10.302], loss: 0.000010, mae: 0.001260, mean_q: 0.757443
 47154/100000: episode: 573, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.660, 10.100], loss: 0.000009, mae: 0.001674, mean_q: 0.757232
 47255/100000: episode: 574, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-2.396, 10.129], loss: 0.000008, mae: 0.000992, mean_q: 0.757192
 47356/100000: episode: 575, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.713, 10.154], loss: 0.000012, mae: 0.001134, mean_q: 0.757216
 47457/100000: episode: 576, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.068, 10.177], loss: 0.000020, mae: 0.001578, mean_q: 0.757423
 47558/100000: episode: 577, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.515, 10.100], loss: 0.000010, mae: 0.001270, mean_q: 0.757386
 47659/100000: episode: 578, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.863, 10.423], loss: 0.000009, mae: 0.001289, mean_q: 0.757261
 47760/100000: episode: 579, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.370, 10.163], loss: 0.000009, mae: 0.001035, mean_q: 0.757289
[Info] 1-TH LEVEL FOUND: 0.7603276968002319, Considering 10/100 traces
 47861/100000: episode: 580, duration: 4.777s, episode steps: 101, steps per second: 21, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.781, 10.325], loss: 0.000011, mae: 0.001251, mean_q: 0.757414
 47863/100000: episode: 581, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.716, mean reward: 0.358 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.258, 10.100], loss: 0.000001, mae: 0.001432, mean_q: 0.755968
 47864/100000: episode: 582, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.589, mean reward: 0.589 [0.589, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.327, 10.200], loss: 0.000000, mae: 0.000219, mean_q: 0.757725
 47868/100000: episode: 583, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.647, mean reward: 0.162 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.382, 10.100], loss: 0.000039, mae: 0.001924, mean_q: 0.758320
 47870/100000: episode: 584, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.660, mean reward: 0.330 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.284, 10.100], loss: 0.000000, mae: 0.000565, mean_q: 0.758034
 47871/100000: episode: 585, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.554, mean reward: 0.554 [0.554, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.338, 10.200], loss: 0.000022, mae: 0.001871, mean_q: 0.757070
 47872/100000: episode: 586, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.604, mean reward: 0.604 [0.604, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.313, 10.200], loss: 0.000001, mae: 0.000990, mean_q: 0.756057
 47876/100000: episode: 587, duration: 0.031s, episode steps: 4, steps per second: 131, episode reward: 0.717, mean reward: 0.179 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.353, 10.100], loss: 0.000001, mae: 0.000970, mean_q: 0.756410
 47878/100000: episode: 588, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.680, mean reward: 0.340 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.296, 10.100], loss: 0.000000, mae: 0.000779, mean_q: 0.758246
 47880/100000: episode: 589, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.604, mean reward: 0.302 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.253, 10.100], loss: 0.000023, mae: 0.001331, mean_q: 0.757349
 47881/100000: episode: 590, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.567, mean reward: 0.567 [0.567, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.176, 10.100], loss: 0.000001, mae: 0.001304, mean_q: 0.756340
 47882/100000: episode: 591, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.587, mean reward: 0.587 [0.587, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.341, 10.200], loss: 0.000001, mae: 0.000903, mean_q: 0.756499
 47884/100000: episode: 592, duration: 0.021s, episode steps: 2, steps per second: 95, episode reward: 0.728, mean reward: 0.364 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.306, 10.100], loss: 0.000022, mae: 0.001228, mean_q: 0.757675
 47888/100000: episode: 593, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 0.691, mean reward: 0.173 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.481, 10.100], loss: 0.000000, mae: 0.000510, mean_q: 0.757094
 47889/100000: episode: 594, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.627, mean reward: 0.627 [0.627, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.276, 10.100], loss: 0.000000, mae: 0.000480, mean_q: 0.757238
 47890/100000: episode: 595, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.265, 10.200], loss: 0.000002, mae: 0.001657, mean_q: 0.755757
 47894/100000: episode: 596, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 0.721, mean reward: 0.180 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.328, 10.100], loss: 0.000008, mae: 0.003041, mean_q: 0.754323
 47896/100000: episode: 597, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.714, mean reward: 0.357 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.295, 10.100], loss: 0.000002, mae: 0.001981, mean_q: 0.759218
 47897/100000: episode: 598, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.265, 10.100], loss: 0.000001, mae: 0.001342, mean_q: 0.758597
 47899/100000: episode: 599, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 0.604, mean reward: 0.302 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.213, 10.100], loss: 0.000008, mae: 0.003628, mean_q: 0.753690
 47901/100000: episode: 600, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.602, mean reward: 0.301 [0.000, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.254, 10.100], loss: 0.000008, mae: 0.003314, mean_q: 0.754277
 47902/100000: episode: 601, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.594, mean reward: 0.594 [0.594, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.318, 10.200], loss: 0.000002, mae: 0.001937, mean_q: 0.758586
 47903/100000: episode: 602, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.721, mean reward: 0.721 [0.721, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.248, 10.200], loss: 0.000005, mae: 0.002890, mean_q: 0.760286
 47904/100000: episode: 603, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.657, mean reward: 0.657 [0.657, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.262, 10.200], loss: 0.000003, mae: 0.002122, mean_q: 0.758994
 47905/100000: episode: 604, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.232, 10.200], loss: 0.000001, mae: 0.000920, mean_q: 0.756983
 47906/100000: episode: 605, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.598, mean reward: 0.598 [0.598, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.657, 10.200], loss: 0.000001, mae: 0.001164, mean_q: 0.755855
 47907/100000: episode: 606, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.569, mean reward: 0.569 [0.569, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.309, 10.200], loss: 0.000001, mae: 0.001362, mean_q: 0.756105
 47908/100000: episode: 607, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.626, mean reward: 0.626 [0.626, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.256, 10.200], loss: 0.000000, mae: 0.000692, mean_q: 0.756161
 47909/100000: episode: 608, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.232, 10.100], loss: 0.001104, mae: 0.012923, mean_q: 0.756271
 47910/100000: episode: 609, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.571, mean reward: 0.571 [0.571, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.221, 10.100], loss: 0.000080, mae: 0.003890, mean_q: 0.755603
 47911/100000: episode: 610, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.554, mean reward: 0.554 [0.554, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.386, 10.200], loss: 0.000000, mae: 0.000623, mean_q: 0.756806
 47915/100000: episode: 611, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 0.765, mean reward: 0.191 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.312, 10.100], loss: 0.000112, mae: 0.003656, mean_q: 0.755329
 47916/100000: episode: 612, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.207, 10.200], loss: 0.000354, mae: 0.008969, mean_q: 0.752142
 47917/100000: episode: 613, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.601, mean reward: 0.601 [0.601, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.203, 10.200], loss: 0.000012, mae: 0.004358, mean_q: 0.752761
 47918/100000: episode: 614, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.264, 10.200], loss: 0.000005, mae: 0.002311, mean_q: 0.753822
 47922/100000: episode: 615, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 0.806, mean reward: 0.202 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.265, 10.100], loss: 0.000135, mae: 0.004479, mean_q: 0.758913
 47924/100000: episode: 616, duration: 0.022s, episode steps: 2, steps per second: 91, episode reward: 0.676, mean reward: 0.338 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.258, 10.100], loss: 0.000011, mae: 0.003552, mean_q: 0.753705
 47925/100000: episode: 617, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.625, mean reward: 0.625 [0.625, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.288, 10.200], loss: 0.000016, mae: 0.004004, mean_q: 0.756108
 47926/100000: episode: 618, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.623, mean reward: 0.623 [0.623, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.302, 10.200], loss: 0.000005, mae: 0.002628, mean_q: 0.753677
 47927/100000: episode: 619, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.635, mean reward: 0.635 [0.635, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.266, 10.100], loss: 0.000124, mae: 0.005048, mean_q: 0.753638
 47928/100000: episode: 620, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.819, mean reward: 0.819 [0.819, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.258, 10.100], loss: 0.000002, mae: 0.001437, mean_q: 0.755746
 47929/100000: episode: 621, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.731, mean reward: 0.731 [0.731, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.220, 10.100], loss: 0.000035, mae: 0.003413, mean_q: 0.756495
 47930/100000: episode: 622, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.624, mean reward: 0.624 [0.624, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.520, 10.100], loss: 0.000154, mae: 0.005422, mean_q: 0.755183
 47931/100000: episode: 623, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.597, mean reward: 0.597 [0.597, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.276, 10.100], loss: 0.000003, mae: 0.001693, mean_q: 0.757233
 47932/100000: episode: 624, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.538, mean reward: 0.538 [0.538, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.265, 10.100], loss: 0.000002, mae: 0.001866, mean_q: 0.756838
 47933/100000: episode: 625, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.644, mean reward: 0.644 [0.644, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.301, 10.200], loss: 0.000005, mae: 0.002193, mean_q: 0.754541
 47935/100000: episode: 626, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.760, mean reward: 0.380 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.345, 10.100], loss: 0.000011, mae: 0.003346, mean_q: 0.752633
 47936/100000: episode: 627, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.578, mean reward: 0.578 [0.578, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.318, 10.200], loss: 0.000008, mae: 0.002115, mean_q: 0.757520
 47937/100000: episode: 628, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.641, mean reward: 0.641 [0.641, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.285, 10.200], loss: 0.000003, mae: 0.002149, mean_q: 0.757867
 47938/100000: episode: 629, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.595, mean reward: 0.595 [0.595, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.264, 10.200], loss: 0.000001, mae: 0.001278, mean_q: 0.757666
 47939/100000: episode: 630, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.668, mean reward: 0.668 [0.668, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.232, 10.200], loss: 0.000005, mae: 0.001462, mean_q: 0.755143
 47940/100000: episode: 631, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.215, 10.200], loss: 0.000002, mae: 0.001511, mean_q: 0.755141
 47941/100000: episode: 632, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.677, mean reward: 0.677 [0.677, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.605, 10.200], loss: 0.000374, mae: 0.005893, mean_q: 0.755360
 47942/100000: episode: 633, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.567, mean reward: 0.567 [0.567, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.334, 10.200], loss: 0.000005, mae: 0.002792, mean_q: 0.753798
 47943/100000: episode: 634, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.584, mean reward: 0.584 [0.584, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.362, 10.200], loss: 0.000005, mae: 0.002853, mean_q: 0.752710
 47944/100000: episode: 635, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.632, mean reward: 0.632 [0.632, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.250, 10.200], loss: 0.000001, mae: 0.001031, mean_q: 0.755319
 47948/100000: episode: 636, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 0.724, mean reward: 0.181 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.403, 10.100], loss: 0.000173, mae: 0.004599, mean_q: 0.756890
 47949/100000: episode: 637, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.616, mean reward: 0.616 [0.616, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.255, 10.200], loss: 0.000008, mae: 0.003682, mean_q: 0.752036
 47953/100000: episode: 638, duration: 0.032s, episode steps: 4, steps per second: 123, episode reward: 0.703, mean reward: 0.176 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.347, 10.100], loss: 0.000058, mae: 0.003942, mean_q: 0.753707
 47957/100000: episode: 639, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.648, mean reward: 0.162 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.054, 10.100], loss: 0.000104, mae: 0.004764, mean_q: 0.755570
 47961/100000: episode: 640, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 0.701, mean reward: 0.175 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.310, 10.100], loss: 0.000034, mae: 0.002758, mean_q: 0.756605
 47962/100000: episode: 641, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.264, 10.100], loss: 0.000012, mae: 0.002219, mean_q: 0.757467
 47963/100000: episode: 642, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.728, mean reward: 0.728 [0.728, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.373, 10.100], loss: 0.000411, mae: 0.006595, mean_q: 0.756551
 47964/100000: episode: 643, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.677, mean reward: 0.677 [0.677, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.261, 10.100], loss: 0.000006, mae: 0.003027, mean_q: 0.752674
 47965/100000: episode: 644, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.643, mean reward: 0.643 [0.643, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.302, 10.100], loss: 0.000008, mae: 0.003555, mean_q: 0.752320
 47966/100000: episode: 645, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.644, mean reward: 0.644 [0.644, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.245, 10.200], loss: 0.000010, mae: 0.002971, mean_q: 0.753250
 47967/100000: episode: 646, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.280, 10.100], loss: 0.000002, mae: 0.001984, mean_q: 0.754363
 47968/100000: episode: 647, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.249, 10.100], loss: 0.000000, mae: 0.000515, mean_q: 0.755616
 47969/100000: episode: 648, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.668, mean reward: 0.668 [0.668, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.268, 10.100], loss: 0.000006, mae: 0.002316, mean_q: 0.756899
 47970/100000: episode: 649, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.260, 10.200], loss: 0.000020, mae: 0.002977, mean_q: 0.756809
 47971/100000: episode: 650, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.575, mean reward: 0.575 [0.575, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.256, 10.100], loss: 0.000012, mae: 0.002225, mean_q: 0.754108
 47972/100000: episode: 651, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.321, 10.200], loss: 0.000019, mae: 0.003768, mean_q: 0.752778
 47973/100000: episode: 652, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.664, mean reward: 0.664 [0.664, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.287, 10.200], loss: 0.000360, mae: 0.006658, mean_q: 0.753326
 47974/100000: episode: 653, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.704, mean reward: 0.704 [0.704, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.286, 10.200], loss: 0.000581, mae: 0.009883, mean_q: 0.750679
 47975/100000: episode: 654, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.774, mean reward: 0.774 [0.774, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.198, 10.100], loss: 0.000012, mae: 0.004281, mean_q: 0.751402
 47977/100000: episode: 655, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.708, mean reward: 0.354 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.261, 10.100], loss: 0.000010, mae: 0.002877, mean_q: 0.753576
 47978/100000: episode: 656, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.628, mean reward: 0.628 [0.628, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.275, 10.200], loss: 0.000008, mae: 0.003285, mean_q: 0.754837
 47979/100000: episode: 657, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.658, mean reward: 0.658 [0.658, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.297, 10.200], loss: 0.000008, mae: 0.003697, mean_q: 0.758759
 47980/100000: episode: 658, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.698, mean reward: 0.698 [0.698, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.268, 10.100], loss: 0.000018, mae: 0.003169, mean_q: 0.757964
 47981/100000: episode: 659, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.572, mean reward: 0.572 [0.572, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.137, 10.100], loss: 0.000232, mae: 0.007386, mean_q: 0.751791
 47982/100000: episode: 660, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.751, mean reward: 0.751 [0.751, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.228, 10.100], loss: 0.000029, mae: 0.007287, mean_q: 0.747361
 47983/100000: episode: 661, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.702, mean reward: 0.702 [0.702, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.297, 10.100], loss: 0.000149, mae: 0.008881, mean_q: 0.748558
 47985/100000: episode: 662, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.616, mean reward: 0.308 [0.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.319, 10.100], loss: 0.000003, mae: 0.001621, mean_q: 0.755394
 47987/100000: episode: 663, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.797, mean reward: 0.399 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.261, 10.100], loss: 0.000263, mae: 0.007394, mean_q: 0.757498
 47988/100000: episode: 664, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.665, mean reward: 0.665 [0.665, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.239, 10.200], loss: 0.000008, mae: 0.002665, mean_q: 0.753150
 47992/100000: episode: 665, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 0.754, mean reward: 0.188 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.309, 10.100], loss: 0.000085, mae: 0.003734, mean_q: 0.752871
 47994/100000: episode: 666, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.610, mean reward: 0.305 [0.000, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.246, 10.100], loss: 0.000274, mae: 0.005818, mean_q: 0.754293
 47995/100000: episode: 667, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.559, mean reward: 0.559 [0.559, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.337, 10.200], loss: 0.000009, mae: 0.003581, mean_q: 0.752254
 47996/100000: episode: 668, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.720, mean reward: 0.720 [0.720, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.234, 10.100], loss: 0.000060, mae: 0.005598, mean_q: 0.750097
 48000/100000: episode: 669, duration: 0.034s, episode steps: 4, steps per second: 117, episode reward: 0.747, mean reward: 0.187 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.358, 10.100], loss: 0.000392, mae: 0.007904, mean_q: 0.754865
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7603276968002319
1
 48001/100000: episode: 670, duration: 4.033s, episode steps: 1, steps per second: 0, episode reward: 0.749, mean reward: 0.749 [0.749, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.241, 10.100], loss: 0.000065, mae: 0.008520, mean_q: 0.745983
 48102/100000: episode: 671, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.711, 10.156], loss: 0.000114, mae: 0.004853, mean_q: 0.753296
 48203/100000: episode: 672, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.294, 10.100], loss: 0.000100, mae: 0.004733, mean_q: 0.754918
 48304/100000: episode: 673, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.486, 10.100], loss: 0.000080, mae: 0.003841, mean_q: 0.754396
 48405/100000: episode: 674, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.556, 10.100], loss: 0.000101, mae: 0.004409, mean_q: 0.752462
 48506/100000: episode: 675, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.843, 10.100], loss: 0.000075, mae: 0.003383, mean_q: 0.751475
 48607/100000: episode: 676, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.609, 10.108], loss: 0.000069, mae: 0.003333, mean_q: 0.751595
 48708/100000: episode: 677, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.746, 10.100], loss: 0.000089, mae: 0.004115, mean_q: 0.751190
 48809/100000: episode: 678, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.773, 10.223], loss: 0.000097, mae: 0.004343, mean_q: 0.749551
 48910/100000: episode: 679, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.353, 10.308], loss: 0.000100, mae: 0.003908, mean_q: 0.749781
 49011/100000: episode: 680, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.578, 10.242], loss: 0.000082, mae: 0.003548, mean_q: 0.749563
 49112/100000: episode: 681, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.391, 10.238], loss: 0.000076, mae: 0.003637, mean_q: 0.747675
 49213/100000: episode: 682, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.880, 10.145], loss: 0.000079, mae: 0.003604, mean_q: 0.747402
 49314/100000: episode: 683, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.531, 10.291], loss: 0.000087, mae: 0.003692, mean_q: 0.748426
 49415/100000: episode: 684, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.770, 10.186], loss: 0.000092, mae: 0.003794, mean_q: 0.747462
 49516/100000: episode: 685, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.018, 10.378], loss: 0.000084, mae: 0.003734, mean_q: 0.747810
 49617/100000: episode: 686, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.190, 10.100], loss: 0.000068, mae: 0.003024, mean_q: 0.746947
 49718/100000: episode: 687, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.067, 10.100], loss: 0.000093, mae: 0.004027, mean_q: 0.746256
 49819/100000: episode: 688, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.005, 10.100], loss: 0.000113, mae: 0.003863, mean_q: 0.746647
 49920/100000: episode: 689, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.368, 10.378], loss: 0.000082, mae: 0.003531, mean_q: 0.745164
 50021/100000: episode: 690, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.100], loss: 0.000107, mae: 0.003781, mean_q: 0.745176
 50122/100000: episode: 691, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.566, 10.100], loss: 0.000105, mae: 0.004261, mean_q: 0.745266
 50223/100000: episode: 692, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.873, 10.148], loss: 0.000060, mae: 0.003162, mean_q: 0.744344
 50324/100000: episode: 693, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.240, 10.307], loss: 0.000073, mae: 0.003654, mean_q: 0.744302
 50425/100000: episode: 694, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.375, 10.155], loss: 0.000082, mae: 0.003399, mean_q: 0.742853
 50526/100000: episode: 695, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.650, 10.238], loss: 0.000075, mae: 0.003160, mean_q: 0.742946
 50627/100000: episode: 696, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.685, 10.100], loss: 0.000112, mae: 0.004013, mean_q: 0.742481
 50728/100000: episode: 697, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.075, 10.100], loss: 0.000096, mae: 0.003766, mean_q: 0.742534
 50829/100000: episode: 698, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.628, 10.343], loss: 0.000083, mae: 0.003591, mean_q: 0.740890
 50930/100000: episode: 699, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.913, 10.343], loss: 0.000084, mae: 0.003465, mean_q: 0.741283
 51031/100000: episode: 700, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.825, 10.100], loss: 0.000075, mae: 0.003560, mean_q: 0.740656
 51132/100000: episode: 701, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.159, 10.208], loss: 0.000055, mae: 0.003023, mean_q: 0.740147
 51233/100000: episode: 702, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.350, 10.100], loss: 0.000076, mae: 0.003765, mean_q: 0.739829
 51334/100000: episode: 703, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.100], loss: 0.000077, mae: 0.003431, mean_q: 0.739805
 51435/100000: episode: 704, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.212, 10.246], loss: 0.000097, mae: 0.004381, mean_q: 0.738663
 51536/100000: episode: 705, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.493, 10.164], loss: 0.000085, mae: 0.003406, mean_q: 0.739374
 51637/100000: episode: 706, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.463, 10.319], loss: 0.000059, mae: 0.002682, mean_q: 0.737665
 51738/100000: episode: 707, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.674, 10.255], loss: 0.000058, mae: 0.002902, mean_q: 0.737669
 51839/100000: episode: 708, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.762, 10.158], loss: 0.000063, mae: 0.003091, mean_q: 0.737632
 51940/100000: episode: 709, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.991, 10.258], loss: 0.000075, mae: 0.003392, mean_q: 0.736769
 52041/100000: episode: 710, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.875, mean reward: 0.009 [0.000, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.720, 10.217], loss: 0.000066, mae: 0.003183, mean_q: 0.735386
 52142/100000: episode: 711, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.449, 10.358], loss: 0.000091, mae: 0.004036, mean_q: 0.735105
 52243/100000: episode: 712, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.035, 10.108], loss: 0.000068, mae: 0.003498, mean_q: 0.735858
 52344/100000: episode: 713, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.100], loss: 0.000091, mae: 0.003778, mean_q: 0.734621
 52445/100000: episode: 714, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.698, 10.117], loss: 0.000099, mae: 0.004006, mean_q: 0.734562
 52546/100000: episode: 715, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.217, 10.100], loss: 0.000071, mae: 0.003545, mean_q: 0.733558
 52647/100000: episode: 716, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.657, 10.251], loss: 0.000083, mae: 0.003470, mean_q: 0.734127
 52748/100000: episode: 717, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.558, 10.325], loss: 0.000086, mae: 0.003778, mean_q: 0.732548
 52849/100000: episode: 718, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.587, 10.100], loss: 0.000052, mae: 0.002902, mean_q: 0.734068
 52950/100000: episode: 719, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.523, 10.100], loss: 0.000035, mae: 0.002651, mean_q: 0.734855
 53051/100000: episode: 720, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.822, 10.248], loss: 0.000022, mae: 0.001815, mean_q: 0.734600
 53152/100000: episode: 721, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.591, 10.335], loss: 0.000016, mae: 0.001522, mean_q: 0.734080
 53253/100000: episode: 722, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.120, 10.275], loss: 0.000018, mae: 0.001920, mean_q: 0.734720
 53354/100000: episode: 723, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.100], loss: 0.000022, mae: 0.002504, mean_q: 0.735126
 53455/100000: episode: 724, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.517, 10.100], loss: 0.000007, mae: 0.001371, mean_q: 0.733983
 53556/100000: episode: 725, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.438, 10.100], loss: 0.000011, mae: 0.001712, mean_q: 0.733938
 53657/100000: episode: 726, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.810, 10.177], loss: 0.000017, mae: 0.002249, mean_q: 0.734344
 53758/100000: episode: 727, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.798, 10.108], loss: 0.000012, mae: 0.001790, mean_q: 0.734279
 53859/100000: episode: 728, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.445, 10.174], loss: 0.000013, mae: 0.001858, mean_q: 0.734698
 53960/100000: episode: 729, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.255, 10.265], loss: 0.000007, mae: 0.001379, mean_q: 0.734765
 54061/100000: episode: 730, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.767, 10.100], loss: 0.000006, mae: 0.001211, mean_q: 0.734161
 54162/100000: episode: 731, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.799, 10.100], loss: 0.000008, mae: 0.001381, mean_q: 0.735270
 54263/100000: episode: 732, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.391, 10.246], loss: 0.000007, mae: 0.001390, mean_q: 0.734353
 54364/100000: episode: 733, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.011, 10.100], loss: 0.000005, mae: 0.001367, mean_q: 0.734902
 54465/100000: episode: 734, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.019, 10.178], loss: 0.000005, mae: 0.001046, mean_q: 0.734421
 54566/100000: episode: 735, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.486, 10.100], loss: 0.000007, mae: 0.001441, mean_q: 0.734880
 54667/100000: episode: 736, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.669, mean reward: 0.007 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.763, 10.334], loss: 0.000009, mae: 0.001427, mean_q: 0.735107
 54768/100000: episode: 737, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.100], loss: 0.000007, mae: 0.001230, mean_q: 0.734684
 54869/100000: episode: 738, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.068, 10.257], loss: 0.000013, mae: 0.001845, mean_q: 0.734585
 54970/100000: episode: 739, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.752, 10.100], loss: 0.000011, mae: 0.001608, mean_q: 0.734762
 55071/100000: episode: 740, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.544, 10.100], loss: 0.000011, mae: 0.002031, mean_q: 0.734972
 55172/100000: episode: 741, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.164, 10.380], loss: 0.000012, mae: 0.001582, mean_q: 0.734626
 55273/100000: episode: 742, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.907, 10.100], loss: 0.000014, mae: 0.001694, mean_q: 0.734528
 55374/100000: episode: 743, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.908, 10.100], loss: 0.000011, mae: 0.001746, mean_q: 0.734460
 55475/100000: episode: 744, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.471, 10.184], loss: 0.000010, mae: 0.001410, mean_q: 0.734243
 55576/100000: episode: 745, duration: 0.610s, episode steps: 101, steps per second: 165, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.565, 10.100], loss: 0.000013, mae: 0.001509, mean_q: 0.734435
 55677/100000: episode: 746, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.000, 10.100], loss: 0.000011, mae: 0.001611, mean_q: 0.734764
 55778/100000: episode: 747, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.587, 10.148], loss: 0.000014, mae: 0.002114, mean_q: 0.733972
 55879/100000: episode: 748, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.915, 10.310], loss: 0.000012, mae: 0.001618, mean_q: 0.734877
 55980/100000: episode: 749, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.913, 10.100], loss: 0.000014, mae: 0.001823, mean_q: 0.734843
 56081/100000: episode: 750, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.475, 10.190], loss: 0.000009, mae: 0.001270, mean_q: 0.734506
 56182/100000: episode: 751, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.736, 10.251], loss: 0.000010, mae: 0.001399, mean_q: 0.734557
 56283/100000: episode: 752, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.100], loss: 0.000016, mae: 0.001892, mean_q: 0.734202
 56384/100000: episode: 753, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.209, 10.100], loss: 0.000011, mae: 0.001347, mean_q: 0.734722
 56485/100000: episode: 754, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.023, 10.100], loss: 0.000006, mae: 0.001202, mean_q: 0.734504
 56586/100000: episode: 755, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.431], loss: 0.000005, mae: 0.000899, mean_q: 0.734584
 56687/100000: episode: 756, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.098, 10.100], loss: 0.000009, mae: 0.001485, mean_q: 0.734971
 56788/100000: episode: 757, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.979, 10.199], loss: 0.000005, mae: 0.001103, mean_q: 0.734826
 56889/100000: episode: 758, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.203, 10.347], loss: 0.000006, mae: 0.001116, mean_q: 0.735108
 56990/100000: episode: 759, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.100], loss: 0.000011, mae: 0.001206, mean_q: 0.735552
 57091/100000: episode: 760, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.100], loss: 0.000006, mae: 0.001123, mean_q: 0.735954
 57192/100000: episode: 761, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.115, 10.100], loss: 0.000010, mae: 0.001386, mean_q: 0.735498
 57293/100000: episode: 762, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.407, 10.100], loss: 0.000006, mae: 0.001197, mean_q: 0.735812
 57394/100000: episode: 763, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.407, 10.113], loss: 0.000011, mae: 0.001509, mean_q: 0.736130
 57495/100000: episode: 764, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.998, 10.127], loss: 0.000011, mae: 0.001298, mean_q: 0.736156
 57596/100000: episode: 765, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.965, 10.100], loss: 0.000006, mae: 0.000984, mean_q: 0.736184
 57697/100000: episode: 766, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.924, 10.101], loss: 0.000007, mae: 0.001078, mean_q: 0.736836
 57798/100000: episode: 767, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.645, 10.100], loss: 0.000007, mae: 0.001325, mean_q: 0.736701
 57899/100000: episode: 768, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.763, 10.440], loss: 0.000007, mae: 0.001183, mean_q: 0.736557
 58000/100000: episode: 769, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.319, 10.100], loss: 0.000007, mae: 0.001207, mean_q: 0.736554
[Info] 1-TH LEVEL FOUND: 0.7642684578895569, Considering 10/100 traces
 58101/100000: episode: 770, duration: 4.830s, episode steps: 101, steps per second: 21, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.674, 10.462], loss: 0.000004, mae: 0.000926, mean_q: 0.736629
 58106/100000: episode: 771, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 0.747, mean reward: 0.149 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.553, 10.303], loss: 0.000001, mae: 0.000574, mean_q: 0.737700
 58113/100000: episode: 772, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 0.636, mean reward: 0.091 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.427, 10.324], loss: 0.000003, mae: 0.000651, mean_q: 0.736353
 58118/100000: episode: 773, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 0.751, mean reward: 0.150 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.385], loss: 0.000023, mae: 0.001176, mean_q: 0.738254
 58123/100000: episode: 774, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 0.763, mean reward: 0.153 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-1.258, 10.251], loss: 0.000004, mae: 0.001459, mean_q: 0.735816
 58128/100000: episode: 775, duration: 0.038s, episode steps: 5, steps per second: 130, episode reward: 0.717, mean reward: 0.143 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.299, 10.440], loss: 0.000002, mae: 0.001602, mean_q: 0.736952
 58133/100000: episode: 776, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 0.695, mean reward: 0.139 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.284], loss: 0.000003, mae: 0.001506, mean_q: 0.736621
 58136/100000: episode: 777, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.621, mean reward: 0.207 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.234], loss: 0.000002, mae: 0.001030, mean_q: 0.738715
 58140/100000: episode: 778, duration: 0.034s, episode steps: 4, steps per second: 117, episode reward: 0.606, mean reward: 0.151 [0.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.243], loss: 0.000003, mae: 0.001966, mean_q: 0.736244
 58145/100000: episode: 779, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 0.680, mean reward: 0.136 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.404], loss: 0.000019, mae: 0.002137, mean_q: 0.738512
 58150/100000: episode: 780, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 0.714, mean reward: 0.143 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.377], loss: 0.000003, mae: 0.001914, mean_q: 0.736933
 58155/100000: episode: 781, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.676, mean reward: 0.135 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.380], loss: 0.000010, mae: 0.001970, mean_q: 0.739922
 58160/100000: episode: 782, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 0.687, mean reward: 0.137 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.334], loss: 0.000015, mae: 0.001444, mean_q: 0.738250
 58166/100000: episode: 783, duration: 0.046s, episode steps: 6, steps per second: 131, episode reward: 0.663, mean reward: 0.111 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.367], loss: 0.000030, mae: 0.002766, mean_q: 0.737590
 58173/100000: episode: 784, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 0.636, mean reward: 0.091 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.257], loss: 0.000005, mae: 0.002635, mean_q: 0.737244
 58176/100000: episode: 785, duration: 0.028s, episode steps: 3, steps per second: 106, episode reward: 0.568, mean reward: 0.189 [0.000, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.213], loss: 0.000011, mae: 0.001634, mean_q: 0.733911
 58183/100000: episode: 786, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.750, mean reward: 0.107 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.854, 10.402], loss: 0.000009, mae: 0.001424, mean_q: 0.739439
 58188/100000: episode: 787, duration: 0.045s, episode steps: 5, steps per second: 112, episode reward: 0.666, mean reward: 0.133 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.385], loss: 0.000041, mae: 0.003872, mean_q: 0.734193
 58193/100000: episode: 788, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 0.630, mean reward: 0.126 [0.000, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.329], loss: 0.000065, mae: 0.003391, mean_q: 0.737014
 58198/100000: episode: 789, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 0.818, mean reward: 0.164 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.565], loss: 0.000015, mae: 0.002817, mean_q: 0.736660
 58205/100000: episode: 790, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 0.672, mean reward: 0.096 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.278], loss: 0.000006, mae: 0.001619, mean_q: 0.736633
 58210/100000: episode: 791, duration: 0.043s, episode steps: 5, steps per second: 116, episode reward: 0.678, mean reward: 0.136 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.125, 10.334], loss: 0.000020, mae: 0.001409, mean_q: 0.737403
 58215/100000: episode: 792, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 0.660, mean reward: 0.132 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.757, 10.362], loss: 0.000010, mae: 0.002055, mean_q: 0.735238
 58220/100000: episode: 793, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.690, mean reward: 0.138 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.419], loss: 0.000016, mae: 0.002428, mean_q: 0.739189
 58227/100000: episode: 794, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 0.648, mean reward: 0.093 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.233, 10.227], loss: 0.000001, mae: 0.001447, mean_q: 0.735092
 58233/100000: episode: 795, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 0.694, mean reward: 0.116 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.061, 10.278], loss: 0.000010, mae: 0.001834, mean_q: 0.736102
 58238/100000: episode: 796, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 0.643, mean reward: 0.129 [0.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.304], loss: 0.000016, mae: 0.002221, mean_q: 0.735264
 58243/100000: episode: 797, duration: 0.038s, episode steps: 5, steps per second: 131, episode reward: 0.700, mean reward: 0.140 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.183, 10.343], loss: 0.000005, mae: 0.001383, mean_q: 0.740055
 58248/100000: episode: 798, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 0.689, mean reward: 0.138 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.391], loss: 0.000000, mae: 0.000729, mean_q: 0.737122
 58255/100000: episode: 799, duration: 0.055s, episode steps: 7, steps per second: 126, episode reward: 0.698, mean reward: 0.100 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.437], loss: 0.000013, mae: 0.001553, mean_q: 0.737830
 58260/100000: episode: 800, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: 0.656, mean reward: 0.131 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.371], loss: 0.000001, mae: 0.001170, mean_q: 0.738561
 58265/100000: episode: 801, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 0.624, mean reward: 0.125 [0.000, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.302], loss: 0.000070, mae: 0.003880, mean_q: 0.735833
 58271/100000: episode: 802, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 0.662, mean reward: 0.110 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.585, 10.289], loss: 0.000039, mae: 0.002088, mean_q: 0.740073
 58275/100000: episode: 803, duration: 0.037s, episode steps: 4, steps per second: 108, episode reward: 0.712, mean reward: 0.178 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.300], loss: 0.000041, mae: 0.003226, mean_q: 0.736029
 58280/100000: episode: 804, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 0.640, mean reward: 0.128 [0.000, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.237, 10.345], loss: 0.000049, mae: 0.002311, mean_q: 0.738476
 58285/100000: episode: 805, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.624, mean reward: 0.125 [0.000, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.275], loss: 0.000080, mae: 0.005180, mean_q: 0.733634
 58292/100000: episode: 806, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 0.777, mean reward: 0.111 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.396], loss: 0.000027, mae: 0.003236, mean_q: 0.740471
 58297/100000: episode: 807, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 0.673, mean reward: 0.135 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.325, 10.399], loss: 0.000106, mae: 0.004325, mean_q: 0.736436
 58301/100000: episode: 808, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 0.640, mean reward: 0.160 [0.000, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.469, 10.215], loss: 0.000119, mae: 0.003977, mean_q: 0.737095
 58306/100000: episode: 809, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 0.641, mean reward: 0.128 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.349], loss: 0.000015, mae: 0.002801, mean_q: 0.735744
 58311/100000: episode: 810, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 0.647, mean reward: 0.129 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.333], loss: 0.000044, mae: 0.002899, mean_q: 0.739589
 58316/100000: episode: 811, duration: 0.038s, episode steps: 5, steps per second: 132, episode reward: 0.685, mean reward: 0.137 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.396], loss: 0.000128, mae: 0.004702, mean_q: 0.736799
 58321/100000: episode: 812, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 0.656, mean reward: 0.131 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.375], loss: 0.000003, mae: 0.001540, mean_q: 0.740557
 58326/100000: episode: 813, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 0.688, mean reward: 0.138 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.353], loss: 0.000024, mae: 0.002241, mean_q: 0.737408
 58331/100000: episode: 814, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 0.694, mean reward: 0.139 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.385], loss: 0.000067, mae: 0.002345, mean_q: 0.737955
 58336/100000: episode: 815, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 0.692, mean reward: 0.138 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.324], loss: 0.000010, mae: 0.001469, mean_q: 0.738427
 58343/100000: episode: 816, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.737, mean reward: 0.105 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.455, 10.481], loss: 0.000069, mae: 0.002278, mean_q: 0.737610
 58350/100000: episode: 817, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 0.652, mean reward: 0.093 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.305], loss: 0.000036, mae: 0.003384, mean_q: 0.735840
 58353/100000: episode: 818, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 0.670, mean reward: 0.223 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.150, 10.192], loss: 0.000051, mae: 0.002957, mean_q: 0.737703
 58357/100000: episode: 819, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 0.627, mean reward: 0.157 [0.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.300], loss: 0.000003, mae: 0.001743, mean_q: 0.735093
 58362/100000: episode: 820, duration: 0.042s, episode steps: 5, steps per second: 118, episode reward: 0.635, mean reward: 0.127 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.251], loss: 0.000002, mae: 0.001629, mean_q: 0.740263
 58366/100000: episode: 821, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 0.712, mean reward: 0.178 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.204], loss: 0.000010, mae: 0.001647, mean_q: 0.735654
 58369/100000: episode: 822, duration: 0.027s, episode steps: 3, steps per second: 111, episode reward: 0.631, mean reward: 0.210 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.324], loss: 0.000098, mae: 0.002813, mean_q: 0.735831
 58374/100000: episode: 823, duration: 0.041s, episode steps: 5, steps per second: 122, episode reward: 0.663, mean reward: 0.133 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.345], loss: 0.000048, mae: 0.003419, mean_q: 0.735128
 58379/100000: episode: 824, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 0.680, mean reward: 0.136 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.697, 10.372], loss: 0.000005, mae: 0.002005, mean_q: 0.737742
 58384/100000: episode: 825, duration: 0.033s, episode steps: 5, steps per second: 149, episode reward: 0.739, mean reward: 0.148 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.321], loss: 0.000090, mae: 0.003794, mean_q: 0.736724
 58391/100000: episode: 826, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 0.667, mean reward: 0.095 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.181], loss: 0.000018, mae: 0.001808, mean_q: 0.737066
 58396/100000: episode: 827, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 0.614, mean reward: 0.123 [0.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.208], loss: 0.000059, mae: 0.002930, mean_q: 0.735874
 58401/100000: episode: 828, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 0.594, mean reward: 0.119 [0.000, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.252], loss: 0.000002, mae: 0.001496, mean_q: 0.736524
 58406/100000: episode: 829, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 0.729, mean reward: 0.146 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.085, 10.466], loss: 0.000060, mae: 0.002831, mean_q: 0.734715
 58409/100000: episode: 830, duration: 0.026s, episode steps: 3, steps per second: 115, episode reward: 0.642, mean reward: 0.214 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.235], loss: 0.000081, mae: 0.002544, mean_q: 0.735923
 58412/100000: episode: 831, duration: 0.030s, episode steps: 3, steps per second: 100, episode reward: 0.598, mean reward: 0.199 [0.000, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.277], loss: 0.000001, mae: 0.000889, mean_q: 0.736941
 58419/100000: episode: 832, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 0.717, mean reward: 0.102 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.386], loss: 0.000067, mae: 0.002990, mean_q: 0.735634
 58426/100000: episode: 833, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 0.706, mean reward: 0.101 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.844, 10.371], loss: 0.000062, mae: 0.003120, mean_q: 0.735628
 58431/100000: episode: 834, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 0.678, mean reward: 0.136 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.393, 10.270], loss: 0.000024, mae: 0.002648, mean_q: 0.737785
 58436/100000: episode: 835, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 0.738, mean reward: 0.148 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.465], loss: 0.000112, mae: 0.003921, mean_q: 0.735206
 58443/100000: episode: 836, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 0.744, mean reward: 0.106 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.379], loss: 0.000057, mae: 0.003778, mean_q: 0.735908
 58450/100000: episode: 837, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 0.645, mean reward: 0.092 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.354], loss: 0.000032, mae: 0.003305, mean_q: 0.734715
 58455/100000: episode: 838, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 0.692, mean reward: 0.138 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.408], loss: 0.000027, mae: 0.002810, mean_q: 0.739109
 58460/100000: episode: 839, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 0.678, mean reward: 0.136 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.367], loss: 0.000032, mae: 0.002715, mean_q: 0.736310
 58464/100000: episode: 840, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 0.676, mean reward: 0.169 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.328], loss: 0.000014, mae: 0.002827, mean_q: 0.734692
 58469/100000: episode: 841, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 0.653, mean reward: 0.131 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.372], loss: 0.000016, mae: 0.002124, mean_q: 0.739733
 58474/100000: episode: 842, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 0.634, mean reward: 0.127 [0.000, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.332], loss: 0.000064, mae: 0.004208, mean_q: 0.733613
 58481/100000: episode: 843, duration: 0.051s, episode steps: 7, steps per second: 136, episode reward: 0.650, mean reward: 0.093 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.352], loss: 0.000130, mae: 0.005078, mean_q: 0.736274
 58488/100000: episode: 844, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 0.629, mean reward: 0.090 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.318], loss: 0.000075, mae: 0.006430, mean_q: 0.732418
 58493/100000: episode: 845, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 0.737, mean reward: 0.147 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.464], loss: 0.000008, mae: 0.003709, mean_q: 0.737990
 58499/100000: episode: 846, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 0.734, mean reward: 0.122 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.413], loss: 0.000103, mae: 0.004991, mean_q: 0.734330
 58506/100000: episode: 847, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 0.708, mean reward: 0.101 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.359, 10.275], loss: 0.000057, mae: 0.002970, mean_q: 0.736714
 58511/100000: episode: 848, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 0.665, mean reward: 0.133 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.375], loss: 0.000041, mae: 0.003320, mean_q: 0.735246
 58516/100000: episode: 849, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 0.692, mean reward: 0.138 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.421], loss: 0.000085, mae: 0.003626, mean_q: 0.736364
 58521/100000: episode: 850, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 0.697, mean reward: 0.139 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.225], loss: 0.000027, mae: 0.002324, mean_q: 0.735496
 58524/100000: episode: 851, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 0.632, mean reward: 0.211 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.242], loss: 0.000012, mae: 0.001562, mean_q: 0.737441
 58529/100000: episode: 852, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 0.679, mean reward: 0.136 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.399], loss: 0.000008, mae: 0.001187, mean_q: 0.737129
 58536/100000: episode: 853, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 0.649, mean reward: 0.093 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.309], loss: 0.000052, mae: 0.002303, mean_q: 0.735608
 58541/100000: episode: 854, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 0.679, mean reward: 0.136 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.749, 10.368], loss: 0.000173, mae: 0.004723, mean_q: 0.734416
 58545/100000: episode: 855, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 0.621, mean reward: 0.155 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.232], loss: 0.000011, mae: 0.002521, mean_q: 0.733355
 58550/100000: episode: 856, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 0.635, mean reward: 0.127 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.343], loss: 0.000050, mae: 0.002967, mean_q: 0.735578
 58555/100000: episode: 857, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 0.630, mean reward: 0.126 [0.000, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.285], loss: 0.000111, mae: 0.006638, mean_q: 0.731655
 58560/100000: episode: 858, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 0.730, mean reward: 0.146 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.317], loss: 0.000010, mae: 0.004071, mean_q: 0.740041
 58565/100000: episode: 859, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 0.788, mean reward: 0.158 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.468], loss: 0.000043, mae: 0.004276, mean_q: 0.734736
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7642684578895569
1
 58570/100000: episode: 860, duration: 4.036s, episode steps: 5, steps per second: 1, episode reward: 0.626, mean reward: 0.125 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.297], loss: 0.000014, mae: 0.003095, mean_q: 0.734073
 58671/100000: episode: 861, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.100], loss: 0.000073, mae: 0.003981, mean_q: 0.733881
 58772/100000: episode: 862, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.449, 10.100], loss: 0.000063, mae: 0.003619, mean_q: 0.732587
 58873/100000: episode: 863, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.925, mean reward: 0.009 [0.000, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.182, 10.169], loss: 0.000046, mae: 0.002892, mean_q: 0.731704
 58974/100000: episode: 864, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.247, 10.257], loss: 0.000073, mae: 0.003501, mean_q: 0.731606
 59075/100000: episode: 865, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.662, 10.227], loss: 0.000064, mae: 0.003080, mean_q: 0.730280
 59176/100000: episode: 866, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.100], loss: 0.000076, mae: 0.003487, mean_q: 0.729423
 59277/100000: episode: 867, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.747, 10.435], loss: 0.000040, mae: 0.002463, mean_q: 0.729069
 59378/100000: episode: 868, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.711, 10.169], loss: 0.000055, mae: 0.002699, mean_q: 0.728210
 59479/100000: episode: 869, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.624, 10.227], loss: 0.000054, mae: 0.002865, mean_q: 0.727179
 59580/100000: episode: 870, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.663, 10.389], loss: 0.000045, mae: 0.002452, mean_q: 0.727341
 59681/100000: episode: 871, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.663, mean reward: 0.007 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.045, 10.183], loss: 0.000051, mae: 0.002650, mean_q: 0.726565
 59782/100000: episode: 872, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.740, 10.100], loss: 0.000045, mae: 0.002555, mean_q: 0.726676
 59883/100000: episode: 873, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.424, 10.100], loss: 0.000066, mae: 0.003288, mean_q: 0.726212
 59984/100000: episode: 874, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.969, 10.100], loss: 0.000046, mae: 0.002129, mean_q: 0.725795
 60085/100000: episode: 875, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.401, 10.210], loss: 0.000053, mae: 0.002702, mean_q: 0.725265
 60186/100000: episode: 876, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.801, 10.323], loss: 0.000063, mae: 0.002855, mean_q: 0.724756
 60287/100000: episode: 877, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.744, 10.384], loss: 0.000040, mae: 0.002039, mean_q: 0.725578
 60388/100000: episode: 878, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.559, 10.105], loss: 0.000064, mae: 0.002665, mean_q: 0.725231
 60489/100000: episode: 879, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.441, 10.100], loss: 0.000047, mae: 0.002369, mean_q: 0.725046
 60590/100000: episode: 880, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.846, 10.100], loss: 0.000045, mae: 0.002175, mean_q: 0.724904
 60691/100000: episode: 881, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.100], loss: 0.000046, mae: 0.002232, mean_q: 0.724993
 60792/100000: episode: 882, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.409, 10.234], loss: 0.000043, mae: 0.002478, mean_q: 0.724494
 60893/100000: episode: 883, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.987, 10.218], loss: 0.000046, mae: 0.002244, mean_q: 0.724778
 60994/100000: episode: 884, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.627, 10.271], loss: 0.000063, mae: 0.002983, mean_q: 0.723930
 61095/100000: episode: 885, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.117, 10.133], loss: 0.000039, mae: 0.002500, mean_q: 0.723719
 61196/100000: episode: 886, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.100], loss: 0.000049, mae: 0.002349, mean_q: 0.723607
 61297/100000: episode: 887, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.645, 10.304], loss: 0.000044, mae: 0.002288, mean_q: 0.723694
 61398/100000: episode: 888, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.825, 10.349], loss: 0.000066, mae: 0.002994, mean_q: 0.723455
 61499/100000: episode: 889, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.154, 10.222], loss: 0.000043, mae: 0.002459, mean_q: 0.722946
 61600/100000: episode: 890, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.770, 10.103], loss: 0.000041, mae: 0.002428, mean_q: 0.722737
 61701/100000: episode: 891, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.448, 10.187], loss: 0.000037, mae: 0.002033, mean_q: 0.723208
 61802/100000: episode: 892, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.910, 10.477], loss: 0.000048, mae: 0.002422, mean_q: 0.723238
 61903/100000: episode: 893, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.221, 10.190], loss: 0.000043, mae: 0.002386, mean_q: 0.722373
 62004/100000: episode: 894, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.422, 10.275], loss: 0.000044, mae: 0.002320, mean_q: 0.722001
 62105/100000: episode: 895, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.581, 10.104], loss: 0.000036, mae: 0.002416, mean_q: 0.721407
 62206/100000: episode: 896, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.169, 10.250], loss: 0.000034, mae: 0.002331, mean_q: 0.721306
 62307/100000: episode: 897, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.410, 10.319], loss: 0.000028, mae: 0.002068, mean_q: 0.721467
 62408/100000: episode: 898, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.832, 10.263], loss: 0.000050, mae: 0.002698, mean_q: 0.721239
 62509/100000: episode: 899, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.934, 10.262], loss: 0.000040, mae: 0.002635, mean_q: 0.721087
 62610/100000: episode: 900, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.541, 10.314], loss: 0.000042, mae: 0.002456, mean_q: 0.720498
 62711/100000: episode: 901, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.100], loss: 0.000056, mae: 0.002814, mean_q: 0.720731
 62812/100000: episode: 902, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.599, 10.130], loss: 0.000044, mae: 0.002521, mean_q: 0.720214
 62913/100000: episode: 903, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.037, 10.100], loss: 0.000041, mae: 0.002547, mean_q: 0.719861
 63014/100000: episode: 904, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.554, 10.102], loss: 0.000042, mae: 0.002417, mean_q: 0.719996
 63115/100000: episode: 905, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.956, 10.100], loss: 0.000038, mae: 0.002477, mean_q: 0.720309
 63216/100000: episode: 906, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.806, 10.120], loss: 0.000034, mae: 0.002325, mean_q: 0.719980
 63317/100000: episode: 907, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.463, 10.228], loss: 0.000019, mae: 0.001644, mean_q: 0.720691
 63418/100000: episode: 908, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.944, 10.143], loss: 0.000027, mae: 0.001854, mean_q: 0.721202
 63519/100000: episode: 909, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.625, 10.100], loss: 0.000021, mae: 0.001689, mean_q: 0.721680
 63620/100000: episode: 910, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.054, 10.100], loss: 0.000020, mae: 0.001749, mean_q: 0.722364
 63721/100000: episode: 911, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.100], loss: 0.000007, mae: 0.001163, mean_q: 0.722977
 63822/100000: episode: 912, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.846, mean reward: 0.008 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.512, 10.261], loss: 0.000012, mae: 0.001320, mean_q: 0.722976
 63923/100000: episode: 913, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.967, 10.100], loss: 0.000009, mae: 0.001233, mean_q: 0.722488
 64024/100000: episode: 914, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.591, 10.100], loss: 0.000012, mae: 0.001582, mean_q: 0.723560
 64125/100000: episode: 915, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.485, 10.211], loss: 0.000011, mae: 0.001572, mean_q: 0.723725
 64226/100000: episode: 916, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.876, 10.187], loss: 0.000012, mae: 0.001512, mean_q: 0.723395
 64327/100000: episode: 917, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.391, 10.161], loss: 0.000009, mae: 0.001348, mean_q: 0.724237
 64428/100000: episode: 918, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.431, 10.229], loss: 0.000011, mae: 0.001457, mean_q: 0.724098
 64529/100000: episode: 919, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.185, 10.100], loss: 0.000007, mae: 0.001272, mean_q: 0.724822
 64630/100000: episode: 920, duration: 0.623s, episode steps: 101, steps per second: 162, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.790, 10.166], loss: 0.000008, mae: 0.001343, mean_q: 0.724230
 64731/100000: episode: 921, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.748, 10.100], loss: 0.000012, mae: 0.001571, mean_q: 0.724589
 64832/100000: episode: 922, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.974, 10.364], loss: 0.000012, mae: 0.001497, mean_q: 0.724992
 64933/100000: episode: 923, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.385, 10.100], loss: 0.000010, mae: 0.001616, mean_q: 0.724456
 65034/100000: episode: 924, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.664, 10.295], loss: 0.000011, mae: 0.001504, mean_q: 0.724970
 65135/100000: episode: 925, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.343, 10.100], loss: 0.000009, mae: 0.001401, mean_q: 0.724990
 65236/100000: episode: 926, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.781, 10.100], loss: 0.000010, mae: 0.001306, mean_q: 0.725714
 65337/100000: episode: 927, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.556, 10.239], loss: 0.000016, mae: 0.001722, mean_q: 0.725451
 65438/100000: episode: 928, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.600, 10.100], loss: 0.000010, mae: 0.001481, mean_q: 0.725178
 65539/100000: episode: 929, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.544, 10.100], loss: 0.000009, mae: 0.001460, mean_q: 0.725999
 65640/100000: episode: 930, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.540, 10.202], loss: 0.000013, mae: 0.001440, mean_q: 0.725732
 65741/100000: episode: 931, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.487, 10.120], loss: 0.000012, mae: 0.001555, mean_q: 0.725825
 65842/100000: episode: 932, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.613, 10.158], loss: 0.000011, mae: 0.001567, mean_q: 0.726393
 65943/100000: episode: 933, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.785, 10.100], loss: 0.000011, mae: 0.001475, mean_q: 0.726462
 66044/100000: episode: 934, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.701, 10.196], loss: 0.000009, mae: 0.001343, mean_q: 0.726683
 66145/100000: episode: 935, duration: 0.617s, episode steps: 101, steps per second: 164, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.341, 10.255], loss: 0.000014, mae: 0.001834, mean_q: 0.726487
 66246/100000: episode: 936, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.132, 10.100], loss: 0.000011, mae: 0.001387, mean_q: 0.726298
 66347/100000: episode: 937, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.404, 10.100], loss: 0.000011, mae: 0.001574, mean_q: 0.726455
 66448/100000: episode: 938, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.871, mean reward: 0.009 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.949, 10.100], loss: 0.000014, mae: 0.001652, mean_q: 0.727450
 66549/100000: episode: 939, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.100], loss: 0.000012, mae: 0.001574, mean_q: 0.727577
 66650/100000: episode: 940, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.172, 10.150], loss: 0.000012, mae: 0.001551, mean_q: 0.727605
 66751/100000: episode: 941, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.904, 10.147], loss: 0.000010, mae: 0.001490, mean_q: 0.727058
 66852/100000: episode: 942, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.666, 10.128], loss: 0.000012, mae: 0.001325, mean_q: 0.727910
 66953/100000: episode: 943, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.561, 10.100], loss: 0.000011, mae: 0.001578, mean_q: 0.727757
 67054/100000: episode: 944, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.929, 10.100], loss: 0.000013, mae: 0.002011, mean_q: 0.727791
 67155/100000: episode: 945, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.646, 10.140], loss: 0.000012, mae: 0.001449, mean_q: 0.727304
 67256/100000: episode: 946, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.899, 10.162], loss: 0.000015, mae: 0.001544, mean_q: 0.728123
 67357/100000: episode: 947, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.955, 10.224], loss: 0.000011, mae: 0.001336, mean_q: 0.728072
 67458/100000: episode: 948, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.320, 10.280], loss: 0.000014, mae: 0.001728, mean_q: 0.728331
 67559/100000: episode: 949, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.100], loss: 0.000018, mae: 0.001648, mean_q: 0.729139
 67660/100000: episode: 950, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.798, 10.292], loss: 0.000013, mae: 0.001562, mean_q: 0.729002
 67761/100000: episode: 951, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.004, 10.173], loss: 0.000017, mae: 0.001762, mean_q: 0.729125
 67862/100000: episode: 952, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.626, 10.217], loss: 0.000015, mae: 0.002073, mean_q: 0.729222
 67963/100000: episode: 953, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.616, 10.100], loss: 0.000012, mae: 0.001277, mean_q: 0.729388
 68064/100000: episode: 954, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.305, 10.100], loss: 0.000018, mae: 0.001684, mean_q: 0.729201
 68165/100000: episode: 955, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.988, 10.117], loss: 0.000008, mae: 0.001268, mean_q: 0.729952
 68266/100000: episode: 956, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.406, 10.383], loss: 0.000012, mae: 0.001652, mean_q: 0.728974
 68367/100000: episode: 957, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.368, 10.100], loss: 0.000014, mae: 0.001660, mean_q: 0.729793
 68468/100000: episode: 958, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.094, 10.100], loss: 0.000012, mae: 0.001570, mean_q: 0.730353
 68569/100000: episode: 959, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.625, 10.265], loss: 0.000012, mae: 0.001427, mean_q: 0.730409
[Info] 1-TH LEVEL FOUND: 0.7616243362426758, Considering 10/100 traces
 68670/100000: episode: 960, duration: 4.865s, episode steps: 101, steps per second: 21, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.501, 10.137], loss: 0.000016, mae: 0.001574, mean_q: 0.731172
 68672/100000: episode: 961, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.719, mean reward: 0.360 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.187], loss: 0.000056, mae: 0.003833, mean_q: 0.731625
 68680/100000: episode: 962, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 0.720, mean reward: 0.090 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.425, 10.448], loss: 0.000029, mae: 0.001437, mean_q: 0.728934
 68687/100000: episode: 963, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 0.842, mean reward: 0.120 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.614], loss: 0.000012, mae: 0.002051, mean_q: 0.732977
 68689/100000: episode: 964, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.619, mean reward: 0.309 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.222], loss: 0.000011, mae: 0.002261, mean_q: 0.727870
 68694/100000: episode: 965, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 0.675, mean reward: 0.135 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.386], loss: 0.000002, mae: 0.001353, mean_q: 0.731114
 68696/100000: episode: 966, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.702, mean reward: 0.351 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.557, 10.165], loss: 0.000003, mae: 0.001718, mean_q: 0.731939
 68701/100000: episode: 967, duration: 0.041s, episode steps: 5, steps per second: 121, episode reward: 0.697, mean reward: 0.139 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.360], loss: 0.000001, mae: 0.001167, mean_q: 0.730097
 68704/100000: episode: 968, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 0.580, mean reward: 0.193 [0.000, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.194], loss: 0.000000, mae: 0.000621, mean_q: 0.732368
 68706/100000: episode: 969, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.708, mean reward: 0.354 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.529, 10.222], loss: 0.000005, mae: 0.001454, mean_q: 0.736184
 68711/100000: episode: 970, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 0.741, mean reward: 0.148 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.480], loss: 0.000002, mae: 0.000838, mean_q: 0.732720
 68713/100000: episode: 971, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.597, mean reward: 0.298 [0.000, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.282], loss: 0.000108, mae: 0.002775, mean_q: 0.730151
 68715/100000: episode: 972, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.632, mean reward: 0.316 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.308], loss: 0.000002, mae: 0.001808, mean_q: 0.734638
 68720/100000: episode: 973, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 0.734, mean reward: 0.147 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.070, 10.489], loss: 0.000009, mae: 0.001783, mean_q: 0.732010
 68723/100000: episode: 974, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 0.588, mean reward: 0.196 [0.000, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.261], loss: 0.000025, mae: 0.002053, mean_q: 0.728960
 68724/100000: episode: 975, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.652, mean reward: 0.652 [0.652, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.070, 10.265], loss: 0.000001, mae: 0.000838, mean_q: 0.725894
 68731/100000: episode: 976, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.810, mean reward: 0.116 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.587], loss: 0.000004, mae: 0.000838, mean_q: 0.730927
 68733/100000: episode: 977, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.615, mean reward: 0.307 [0.000, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.239], loss: 0.000001, mae: 0.000926, mean_q: 0.731269
 68738/100000: episode: 978, duration: 0.039s, episode steps: 5, steps per second: 128, episode reward: 0.747, mean reward: 0.149 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.659, 10.491], loss: 0.000022, mae: 0.001422, mean_q: 0.732609
 68740/100000: episode: 979, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.591, mean reward: 0.295 [0.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.237], loss: 0.000001, mae: 0.000843, mean_q: 0.733038
 68743/100000: episode: 980, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 0.653, mean reward: 0.218 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.254], loss: 0.000001, mae: 0.000766, mean_q: 0.732115
 68751/100000: episode: 981, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 0.698, mean reward: 0.087 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.766, 10.431], loss: 0.000009, mae: 0.001201, mean_q: 0.732728
 68756/100000: episode: 982, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 0.738, mean reward: 0.148 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.467], loss: 0.000083, mae: 0.002305, mean_q: 0.729824
 68763/100000: episode: 983, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 0.718, mean reward: 0.103 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.436], loss: 0.000021, mae: 0.002142, mean_q: 0.732032
 68765/100000: episode: 984, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.672, mean reward: 0.336 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.175], loss: 0.000001, mae: 0.000893, mean_q: 0.731580
 68768/100000: episode: 985, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 0.633, mean reward: 0.211 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.341], loss: 0.000014, mae: 0.001515, mean_q: 0.733363
 68775/100000: episode: 986, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 0.770, mean reward: 0.110 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.358, 10.521], loss: 0.000010, mae: 0.001516, mean_q: 0.736225
 68777/100000: episode: 987, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.606, mean reward: 0.303 [0.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.212], loss: 0.000156, mae: 0.003893, mean_q: 0.732293
 68779/100000: episode: 988, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.592, mean reward: 0.296 [0.000, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.263], loss: 0.000017, mae: 0.003632, mean_q: 0.728013
 68781/100000: episode: 989, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.617, mean reward: 0.309 [0.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.296], loss: 0.000030, mae: 0.004220, mean_q: 0.732502
 68783/100000: episode: 990, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.671, mean reward: 0.335 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.263], loss: 0.000001, mae: 0.001373, mean_q: 0.734747
 68785/100000: episode: 991, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.638, mean reward: 0.319 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.326], loss: 0.000002, mae: 0.001854, mean_q: 0.730782
 68787/100000: episode: 992, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.539, mean reward: 0.270 [0.000, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.166], loss: 0.000005, mae: 0.002506, mean_q: 0.732430
 68789/100000: episode: 993, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.573, mean reward: 0.287 [0.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.240], loss: 0.000020, mae: 0.002890, mean_q: 0.728699
 68790/100000: episode: 994, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.738, mean reward: 0.738 [0.738, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.070, 10.463], loss: 0.000001, mae: 0.000846, mean_q: 0.727319
 68795/100000: episode: 995, duration: 0.044s, episode steps: 5, steps per second: 114, episode reward: 0.716, mean reward: 0.143 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.389], loss: 0.000050, mae: 0.002153, mean_q: 0.733748
 68798/100000: episode: 996, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 0.673, mean reward: 0.224 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.392], loss: 0.000135, mae: 0.005590, mean_q: 0.730763
 68801/100000: episode: 997, duration: 0.029s, episode steps: 3, steps per second: 103, episode reward: 0.646, mean reward: 0.215 [0.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.339], loss: 0.000005, mae: 0.001921, mean_q: 0.726524
 68806/100000: episode: 998, duration: 0.039s, episode steps: 5, steps per second: 127, episode reward: 0.734, mean reward: 0.147 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.481], loss: 0.000003, mae: 0.002125, mean_q: 0.737786
 68808/100000: episode: 999, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.574, mean reward: 0.287 [0.000, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.235], loss: 0.000000, mae: 0.000749, mean_q: 0.735998
 68813/100000: episode: 1000, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 0.803, mean reward: 0.161 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.546], loss: 0.000082, mae: 0.004044, mean_q: 0.729285
 68821/100000: episode: 1001, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 0.682, mean reward: 0.085 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.299], loss: 0.000040, mae: 0.003127, mean_q: 0.734094
 68828/100000: episode: 1002, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.783, mean reward: 0.112 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.526], loss: 0.000007, mae: 0.001680, mean_q: 0.731556
 68829/100000: episode: 1003, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.706, 10.301], loss: 0.000044, mae: 0.002913, mean_q: 0.731952
 68831/100000: episode: 1004, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.609, mean reward: 0.305 [0.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.035, 10.302], loss: 0.000006, mae: 0.001896, mean_q: 0.736434
 68836/100000: episode: 1005, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 0.712, mean reward: 0.142 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.423], loss: 0.000090, mae: 0.004425, mean_q: 0.729661
 68838/100000: episode: 1006, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.576, mean reward: 0.288 [0.000, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.200], loss: 0.000051, mae: 0.004056, mean_q: 0.730874
 68843/100000: episode: 1007, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 0.688, mean reward: 0.138 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.414], loss: 0.000207, mae: 0.004431, mean_q: 0.732346
 68845/100000: episode: 1008, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.609, mean reward: 0.305 [0.000, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.263], loss: 0.000026, mae: 0.003120, mean_q: 0.726830
 68847/100000: episode: 1009, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.585, mean reward: 0.292 [0.000, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.262], loss: 0.000005, mae: 0.002228, mean_q: 0.728321
 68852/100000: episode: 1010, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 0.721, mean reward: 0.144 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.170, 10.437], loss: 0.000152, mae: 0.003222, mean_q: 0.731763
 68854/100000: episode: 1011, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.618, mean reward: 0.309 [0.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.692, 10.250], loss: 0.000010, mae: 0.001533, mean_q: 0.731255
 68862/100000: episode: 1012, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 0.733, mean reward: 0.092 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.456], loss: 0.000078, mae: 0.003122, mean_q: 0.730837
 68863/100000: episode: 1013, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.743, mean reward: 0.743 [0.743, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.070, 10.430], loss: 0.000021, mae: 0.002825, mean_q: 0.727362
 68870/100000: episode: 1014, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 0.737, mean reward: 0.105 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.493], loss: 0.000041, mae: 0.002565, mean_q: 0.732776
 68878/100000: episode: 1015, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 0.704, mean reward: 0.088 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.389], loss: 0.000030, mae: 0.001777, mean_q: 0.730805
 68880/100000: episode: 1016, duration: 0.021s, episode steps: 2, steps per second: 95, episode reward: 0.622, mean reward: 0.311 [0.000, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.284], loss: 0.000293, mae: 0.004802, mean_q: 0.730610
 68883/100000: episode: 1017, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 0.607, mean reward: 0.202 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.277], loss: 0.000014, mae: 0.002703, mean_q: 0.728754
 68885/100000: episode: 1018, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.583, mean reward: 0.291 [0.000, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.249], loss: 0.000002, mae: 0.001668, mean_q: 0.732642
 68887/100000: episode: 1019, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.604, mean reward: 0.302 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.197], loss: 0.000208, mae: 0.005518, mean_q: 0.734553
 68889/100000: episode: 1020, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.703, mean reward: 0.351 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.238], loss: 0.000052, mae: 0.004621, mean_q: 0.727075
 68894/100000: episode: 1021, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 0.729, mean reward: 0.146 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.341], loss: 0.000004, mae: 0.002019, mean_q: 0.728004
 68899/100000: episode: 1022, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 0.696, mean reward: 0.139 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.330], loss: 0.000008, mae: 0.001911, mean_q: 0.731860
 68901/100000: episode: 1023, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.654, mean reward: 0.327 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.240], loss: 0.000161, mae: 0.005077, mean_q: 0.731013
 68902/100000: episode: 1024, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.070, 10.310], loss: 0.000004, mae: 0.002092, mean_q: 0.724034
 68907/100000: episode: 1025, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 0.710, mean reward: 0.142 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.453], loss: 0.000111, mae: 0.004572, mean_q: 0.731241
 68909/100000: episode: 1026, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.608, mean reward: 0.304 [0.000, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.222], loss: 0.000149, mae: 0.004604, mean_q: 0.729440
 68910/100000: episode: 1027, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.070, 10.331], loss: 0.000033, mae: 0.003112, mean_q: 0.727263
 68911/100000: episode: 1028, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.070, 10.343], loss: 0.000005, mae: 0.002251, mean_q: 0.727760
 68912/100000: episode: 1029, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.660, mean reward: 0.660 [0.660, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.070, 10.376], loss: 0.000759, mae: 0.011237, mean_q: 0.730566
 68913/100000: episode: 1030, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.070, 10.337], loss: 0.000027, mae: 0.004262, mean_q: 0.732179
 68920/100000: episode: 1031, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 0.747, mean reward: 0.107 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.506], loss: 0.000040, mae: 0.003836, mean_q: 0.729478
 68927/100000: episode: 1032, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 0.759, mean reward: 0.108 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.464], loss: 0.000099, mae: 0.004166, mean_q: 0.733043
 68928/100000: episode: 1033, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.628, mean reward: 0.628 [0.628, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.322], loss: 0.000395, mae: 0.008038, mean_q: 0.732451
 68930/100000: episode: 1034, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.600, mean reward: 0.300 [0.000, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.237], loss: 0.000007, mae: 0.002567, mean_q: 0.728858
 68933/100000: episode: 1035, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 0.646, mean reward: 0.215 [0.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.902, 10.234], loss: 0.000119, mae: 0.003600, mean_q: 0.728787
 68935/100000: episode: 1036, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.617, mean reward: 0.309 [0.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.257], loss: 0.000007, mae: 0.002322, mean_q: 0.735161
 68937/100000: episode: 1037, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.654, mean reward: 0.327 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.331], loss: 0.000136, mae: 0.005244, mean_q: 0.731388
 68942/100000: episode: 1038, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 0.695, mean reward: 0.139 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.394], loss: 0.000033, mae: 0.002250, mean_q: 0.733526
 68947/100000: episode: 1039, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 0.758, mean reward: 0.152 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.430], loss: 0.000106, mae: 0.005321, mean_q: 0.729684
 68949/100000: episode: 1040, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.605, mean reward: 0.303 [0.000, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.283], loss: 0.000180, mae: 0.006353, mean_q: 0.730598
 68954/100000: episode: 1041, duration: 0.037s, episode steps: 5, steps per second: 134, episode reward: 0.686, mean reward: 0.137 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.397], loss: 0.000068, mae: 0.003253, mean_q: 0.732126
 68956/100000: episode: 1042, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.570, mean reward: 0.285 [0.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.365, 10.178], loss: 0.000090, mae: 0.004338, mean_q: 0.729471
 68959/100000: episode: 1043, duration: 0.022s, episode steps: 3, steps per second: 133, episode reward: 0.694, mean reward: 0.231 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.274], loss: 0.000034, mae: 0.002946, mean_q: 0.727922
 68960/100000: episode: 1044, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.070, 10.359], loss: 0.000392, mae: 0.006798, mean_q: 0.726409
 68962/100000: episode: 1045, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.636, mean reward: 0.318 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.035, 10.335], loss: 0.000004, mae: 0.002104, mean_q: 0.728754
 68964/100000: episode: 1046, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.575, mean reward: 0.288 [0.000, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.237], loss: 0.000195, mae: 0.005171, mean_q: 0.728872
 68969/100000: episode: 1047, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 0.680, mean reward: 0.136 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.311], loss: 0.000021, mae: 0.003664, mean_q: 0.728010
 68971/100000: episode: 1048, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.629, mean reward: 0.315 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.236], loss: 0.000066, mae: 0.003210, mean_q: 0.734887
 68978/100000: episode: 1049, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.776, mean reward: 0.111 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.540], loss: 0.000014, mae: 0.002274, mean_q: 0.729445
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7616243362426758
1
 68980/100000: episode: 1050, duration: 3.996s, episode steps: 2, steps per second: 1, episode reward: 0.556, mean reward: 0.278 [0.000, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.210], loss: 0.000169, mae: 0.004160, mean_q: 0.730601
 69081/100000: episode: 1051, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.283, 10.100], loss: 0.000090, mae: 0.004388, mean_q: 0.729450
 69182/100000: episode: 1052, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.635, 10.229], loss: 0.000116, mae: 0.004553, mean_q: 0.728123
 69283/100000: episode: 1053, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.574, 10.100], loss: 0.000103, mae: 0.004121, mean_q: 0.726776
 69384/100000: episode: 1054, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.597, 10.322], loss: 0.000093, mae: 0.003916, mean_q: 0.726014
 69485/100000: episode: 1055, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.209, 10.189], loss: 0.000076, mae: 0.003655, mean_q: 0.725569
 69586/100000: episode: 1056, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.837, 10.100], loss: 0.000093, mae: 0.004424, mean_q: 0.725782
 69687/100000: episode: 1057, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.375, 10.100], loss: 0.000091, mae: 0.003529, mean_q: 0.724787
 69788/100000: episode: 1058, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.436, 10.100], loss: 0.000095, mae: 0.003841, mean_q: 0.724944
 69889/100000: episode: 1059, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.302, 10.100], loss: 0.000058, mae: 0.002847, mean_q: 0.724654
 69990/100000: episode: 1060, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.028, 10.407], loss: 0.000094, mae: 0.003577, mean_q: 0.724031
 70091/100000: episode: 1061, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.426, 10.200], loss: 0.000072, mae: 0.003577, mean_q: 0.723301
 70192/100000: episode: 1062, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.621, 10.149], loss: 0.000083, mae: 0.003008, mean_q: 0.723072
 70293/100000: episode: 1063, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.028, 10.100], loss: 0.000076, mae: 0.003635, mean_q: 0.722780
 70394/100000: episode: 1064, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.921, 10.100], loss: 0.000085, mae: 0.003939, mean_q: 0.722224
 70495/100000: episode: 1065, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.278, 10.100], loss: 0.000109, mae: 0.004196, mean_q: 0.722573
 70596/100000: episode: 1066, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.505, 10.297], loss: 0.000077, mae: 0.003683, mean_q: 0.722211
 70697/100000: episode: 1067, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.573, 10.100], loss: 0.000071, mae: 0.003595, mean_q: 0.722139
 70798/100000: episode: 1068, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.806, 10.209], loss: 0.000047, mae: 0.002812, mean_q: 0.721837
 70899/100000: episode: 1069, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.766, 10.177], loss: 0.000083, mae: 0.003598, mean_q: 0.721144
 71000/100000: episode: 1070, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.534, 10.411], loss: 0.000062, mae: 0.003376, mean_q: 0.722027
 71101/100000: episode: 1071, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.112, 10.486], loss: 0.000054, mae: 0.002831, mean_q: 0.721491
 71202/100000: episode: 1072, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.098, 10.250], loss: 0.000063, mae: 0.003231, mean_q: 0.721490
 71303/100000: episode: 1073, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.009, 10.100], loss: 0.000071, mae: 0.003673, mean_q: 0.720604
 71404/100000: episode: 1074, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.225, 10.374], loss: 0.000038, mae: 0.002444, mean_q: 0.721542
 71505/100000: episode: 1075, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.130, 10.100], loss: 0.000071, mae: 0.003657, mean_q: 0.721662
 71606/100000: episode: 1076, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.753, 10.100], loss: 0.000074, mae: 0.003803, mean_q: 0.720972
 71707/100000: episode: 1077, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.212], loss: 0.000078, mae: 0.003853, mean_q: 0.720194
 71808/100000: episode: 1078, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.851, mean reward: 0.008 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.540, 10.218], loss: 0.000062, mae: 0.003051, mean_q: 0.720969
 71909/100000: episode: 1079, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.515, 10.146], loss: 0.000068, mae: 0.002945, mean_q: 0.720929
 72010/100000: episode: 1080, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.850, 10.230], loss: 0.000049, mae: 0.002894, mean_q: 0.720885
 72111/100000: episode: 1081, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.914, 10.232], loss: 0.000076, mae: 0.003925, mean_q: 0.720692
 72212/100000: episode: 1082, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.169], loss: 0.000074, mae: 0.004005, mean_q: 0.719967
 72313/100000: episode: 1083, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.100], loss: 0.000083, mae: 0.003726, mean_q: 0.720249
 72414/100000: episode: 1084, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.680, 10.276], loss: 0.000069, mae: 0.003400, mean_q: 0.720648
 72515/100000: episode: 1085, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.755, 10.170], loss: 0.000088, mae: 0.003648, mean_q: 0.719783
 72616/100000: episode: 1086, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.258, 10.167], loss: 0.000063, mae: 0.003070, mean_q: 0.719288
 72717/100000: episode: 1087, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.382, 10.100], loss: 0.000088, mae: 0.003582, mean_q: 0.719192
 72818/100000: episode: 1088, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.620, 10.108], loss: 0.000068, mae: 0.003539, mean_q: 0.719173
 72919/100000: episode: 1089, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.493, 10.198], loss: 0.000060, mae: 0.003534, mean_q: 0.718593
 73020/100000: episode: 1090, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.438, 10.198], loss: 0.000070, mae: 0.003906, mean_q: 0.718575
 73121/100000: episode: 1091, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.637, 10.202], loss: 0.000058, mae: 0.003306, mean_q: 0.717450
 73222/100000: episode: 1092, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.507, 10.100], loss: 0.000083, mae: 0.003581, mean_q: 0.717909
 73323/100000: episode: 1093, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.386, 10.205], loss: 0.000057, mae: 0.003348, mean_q: 0.717785
 73424/100000: episode: 1094, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.946, 10.176], loss: 0.000065, mae: 0.003681, mean_q: 0.717453
 73525/100000: episode: 1095, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.788, 10.511], loss: 0.000052, mae: 0.003303, mean_q: 0.718483
 73626/100000: episode: 1096, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.067, 10.165], loss: 0.000059, mae: 0.003754, mean_q: 0.718076
 73727/100000: episode: 1097, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.175, 10.105], loss: 0.000041, mae: 0.002501, mean_q: 0.718840
 73828/100000: episode: 1098, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.081, 10.154], loss: 0.000021, mae: 0.002033, mean_q: 0.718859
 73929/100000: episode: 1099, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.986, 10.394], loss: 0.000033, mae: 0.002150, mean_q: 0.719720
 74030/100000: episode: 1100, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.853, mean reward: 0.008 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.100], loss: 0.000026, mae: 0.001914, mean_q: 0.720113
 74131/100000: episode: 1101, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.701, 10.229], loss: 0.000022, mae: 0.001825, mean_q: 0.720659
 74232/100000: episode: 1102, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.109, 10.121], loss: 0.000020, mae: 0.001645, mean_q: 0.720762
 74333/100000: episode: 1103, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-2.063, 10.275], loss: 0.000010, mae: 0.001293, mean_q: 0.721108
 74434/100000: episode: 1104, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.657, 10.403], loss: 0.000014, mae: 0.001400, mean_q: 0.721719
 74535/100000: episode: 1105, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.318, 10.214], loss: 0.000004, mae: 0.001148, mean_q: 0.721592
 74636/100000: episode: 1106, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.872, 10.100], loss: 0.000003, mae: 0.000863, mean_q: 0.721529
 74737/100000: episode: 1107, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.023, 10.135], loss: 0.000016, mae: 0.001442, mean_q: 0.721875
 74838/100000: episode: 1108, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.863, 10.178], loss: 0.000004, mae: 0.000948, mean_q: 0.721810
 74939/100000: episode: 1109, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.328, 10.100], loss: 0.000018, mae: 0.001546, mean_q: 0.722590
 75040/100000: episode: 1110, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.230, 10.189], loss: 0.000017, mae: 0.001727, mean_q: 0.722515
 75141/100000: episode: 1111, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.809, 10.109], loss: 0.000004, mae: 0.001052, mean_q: 0.722597
 75242/100000: episode: 1112, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.480, 10.130], loss: 0.000008, mae: 0.001169, mean_q: 0.722751
 75343/100000: episode: 1113, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.706, 10.100], loss: 0.000020, mae: 0.001623, mean_q: 0.722784
 75444/100000: episode: 1114, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.038, 10.260], loss: 0.000006, mae: 0.001078, mean_q: 0.723129
 75545/100000: episode: 1115, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.888, 10.100], loss: 0.000005, mae: 0.000938, mean_q: 0.723237
 75646/100000: episode: 1116, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.292, 10.182], loss: 0.000017, mae: 0.001441, mean_q: 0.723156
 75747/100000: episode: 1117, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.702, 10.299], loss: 0.000011, mae: 0.001304, mean_q: 0.723650
 75848/100000: episode: 1118, duration: 0.491s, episode steps: 101, steps per second: 206, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.477, 10.261], loss: 0.000015, mae: 0.001407, mean_q: 0.723665
 75949/100000: episode: 1119, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.947, 10.100], loss: 0.000014, mae: 0.001470, mean_q: 0.724238
 76050/100000: episode: 1120, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.266, 10.157], loss: 0.000005, mae: 0.000962, mean_q: 0.724491
 76151/100000: episode: 1121, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.065, 10.100], loss: 0.000009, mae: 0.001133, mean_q: 0.724754
 76252/100000: episode: 1122, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.803, 10.147], loss: 0.000004, mae: 0.000885, mean_q: 0.724765
 76353/100000: episode: 1123, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.363, 10.100], loss: 0.000011, mae: 0.001240, mean_q: 0.725298
 76454/100000: episode: 1124, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.901, 10.352], loss: 0.000006, mae: 0.001004, mean_q: 0.724940
 76555/100000: episode: 1125, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.863, 10.116], loss: 0.000009, mae: 0.001219, mean_q: 0.724998
 76656/100000: episode: 1126, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.609, 10.212], loss: 0.000009, mae: 0.001084, mean_q: 0.724754
 76757/100000: episode: 1127, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.125], loss: 0.000011, mae: 0.001222, mean_q: 0.725849
 76858/100000: episode: 1128, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.874, 10.100], loss: 0.000009, mae: 0.001257, mean_q: 0.725556
 76959/100000: episode: 1129, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.274, 10.300], loss: 0.000008, mae: 0.001330, mean_q: 0.726034
 77060/100000: episode: 1130, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.806, 10.209], loss: 0.000010, mae: 0.001144, mean_q: 0.725980
 77161/100000: episode: 1131, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.955, 10.100], loss: 0.000005, mae: 0.000933, mean_q: 0.725956
 77262/100000: episode: 1132, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.469, 10.100], loss: 0.000006, mae: 0.001154, mean_q: 0.726351
 77363/100000: episode: 1133, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.365, 10.271], loss: 0.000009, mae: 0.001258, mean_q: 0.727127
 77464/100000: episode: 1134, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.538, 10.100], loss: 0.000008, mae: 0.001190, mean_q: 0.727332
 77565/100000: episode: 1135, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.520, 10.100], loss: 0.000008, mae: 0.001002, mean_q: 0.727778
 77666/100000: episode: 1136, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.765, 10.485], loss: 0.000006, mae: 0.000956, mean_q: 0.727364
 77767/100000: episode: 1137, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.998, mean reward: 0.010 [0.000, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.164, 10.100], loss: 0.000008, mae: 0.001133, mean_q: 0.727309
 77868/100000: episode: 1138, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.100], loss: 0.000010, mae: 0.001192, mean_q: 0.728439
 77969/100000: episode: 1139, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.077, 10.341], loss: 0.000037, mae: 0.002436, mean_q: 0.728410
 78070/100000: episode: 1140, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.095, 10.276], loss: 0.000017, mae: 0.001587, mean_q: 0.729269
 78171/100000: episode: 1141, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.784, 10.100], loss: 0.000006, mae: 0.001052, mean_q: 0.728989
 78272/100000: episode: 1142, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.028, 10.100], loss: 0.000008, mae: 0.001017, mean_q: 0.729551
 78373/100000: episode: 1143, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.345, 10.100], loss: 0.000008, mae: 0.001295, mean_q: 0.730031
 78474/100000: episode: 1144, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.596, 10.100], loss: 0.000009, mae: 0.001111, mean_q: 0.729982
 78575/100000: episode: 1145, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.976, 10.324], loss: 0.000010, mae: 0.001392, mean_q: 0.730289
 78676/100000: episode: 1146, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.247, 10.266], loss: 0.000036, mae: 0.002104, mean_q: 0.730955
 78777/100000: episode: 1147, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.189, 10.101], loss: 0.000026, mae: 0.001726, mean_q: 0.731521
 78878/100000: episode: 1148, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.236, 10.323], loss: 0.000005, mae: 0.000987, mean_q: 0.731690
 78979/100000: episode: 1149, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.840, 10.100], loss: 0.000010, mae: 0.001224, mean_q: 0.732424
[Info] 1-TH LEVEL FOUND: 0.7679640054702759, Considering 10/100 traces
 79080/100000: episode: 1150, duration: 4.828s, episode steps: 101, steps per second: 21, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.902, 10.100], loss: 0.000008, mae: 0.001115, mean_q: 0.732964
 79088/100000: episode: 1151, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 0.760, mean reward: 0.095 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.253, 10.100], loss: 0.000005, mae: 0.001010, mean_q: 0.730832
 79090/100000: episode: 1152, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.823, mean reward: 0.411 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.308, 10.100], loss: 0.000001, mae: 0.001111, mean_q: 0.733696
 79098/100000: episode: 1153, duration: 0.066s, episode steps: 8, steps per second: 122, episode reward: 0.764, mean reward: 0.095 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.385, 10.100], loss: 0.000014, mae: 0.001278, mean_q: 0.734120
 79099/100000: episode: 1154, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.711, mean reward: 0.711 [0.711, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.307, 10.200], loss: 0.000001, mae: 0.001150, mean_q: 0.730875
 79106/100000: episode: 1155, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 0.696, mean reward: 0.099 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.280, 10.100], loss: 0.000008, mae: 0.001175, mean_q: 0.732851
 79114/100000: episode: 1156, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 0.683, mean reward: 0.085 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.292, 10.100], loss: 0.000017, mae: 0.001844, mean_q: 0.734147
 79121/100000: episode: 1157, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 0.638, mean reward: 0.091 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.311, 10.100], loss: 0.000015, mae: 0.001499, mean_q: 0.733229
 79123/100000: episode: 1158, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.717, mean reward: 0.359 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.240, 10.100], loss: 0.000000, mae: 0.000738, mean_q: 0.734506
 79135/100000: episode: 1159, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 0.863, mean reward: 0.072 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.322, 10.100], loss: 0.000012, mae: 0.001254, mean_q: 0.732168
 79137/100000: episode: 1160, duration: 0.023s, episode steps: 2, steps per second: 88, episode reward: 0.665, mean reward: 0.333 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.279, 10.100], loss: 0.000004, mae: 0.001371, mean_q: 0.729668
 79145/100000: episode: 1161, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 0.719, mean reward: 0.090 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.219, 10.100], loss: 0.000007, mae: 0.001492, mean_q: 0.733672
 79147/100000: episode: 1162, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.755, mean reward: 0.377 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.202, 10.100], loss: 0.000035, mae: 0.002089, mean_q: 0.738890
 79149/100000: episode: 1163, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 0.671, mean reward: 0.336 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.300, 10.100], loss: 0.000011, mae: 0.001600, mean_q: 0.733500
 79151/100000: episode: 1164, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.691, mean reward: 0.345 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.176, 10.100], loss: 0.000015, mae: 0.001669, mean_q: 0.732464
 79160/100000: episode: 1165, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 0.932, mean reward: 0.104 [0.000, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.602, 10.100], loss: 0.000029, mae: 0.002334, mean_q: 0.734714
 79162/100000: episode: 1166, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.716, mean reward: 0.358 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.314, 10.100], loss: 0.000029, mae: 0.002870, mean_q: 0.734404
 79164/100000: episode: 1167, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.729, mean reward: 0.364 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.235, 10.100], loss: 0.000006, mae: 0.002547, mean_q: 0.732183
 79173/100000: episode: 1168, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 0.912, mean reward: 0.101 [0.000, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.552, 10.100], loss: 0.000020, mae: 0.001837, mean_q: 0.732811
 79181/100000: episode: 1169, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 0.719, mean reward: 0.090 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.295, 10.100], loss: 0.000005, mae: 0.001445, mean_q: 0.734230
 79192/100000: episode: 1170, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 0.789, mean reward: 0.072 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.467, 10.100], loss: 0.000004, mae: 0.001437, mean_q: 0.734924
 79194/100000: episode: 1171, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.674, mean reward: 0.337 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.235, 10.100], loss: 0.000001, mae: 0.001279, mean_q: 0.738674
 79196/100000: episode: 1172, duration: 0.023s, episode steps: 2, steps per second: 87, episode reward: 0.662, mean reward: 0.331 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.317, 10.100], loss: 0.000005, mae: 0.000885, mean_q: 0.732820
 79208/100000: episode: 1173, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 0.828, mean reward: 0.069 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.423, 10.100], loss: 0.000003, mae: 0.000801, mean_q: 0.732705
 79215/100000: episode: 1174, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 0.775, mean reward: 0.111 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.316, 10.100], loss: 0.000011, mae: 0.001594, mean_q: 0.734434
 79224/100000: episode: 1175, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 0.865, mean reward: 0.096 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.504, 10.100], loss: 0.000039, mae: 0.002352, mean_q: 0.734054
 79226/100000: episode: 1176, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.744, mean reward: 0.372 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.290, 10.100], loss: 0.000012, mae: 0.002253, mean_q: 0.733278
 79238/100000: episode: 1177, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 0.880, mean reward: 0.073 [0.000, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.554, 10.100], loss: 0.000018, mae: 0.002004, mean_q: 0.734137
 79246/100000: episode: 1178, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 0.681, mean reward: 0.085 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.136, 10.100], loss: 0.000009, mae: 0.001940, mean_q: 0.734256
 79247/100000: episode: 1179, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.718, mean reward: 0.718 [0.718, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.345, 10.200], loss: 0.000000, mae: 0.000824, mean_q: 0.726615
 79254/100000: episode: 1180, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.751, mean reward: 0.107 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.292, 10.100], loss: 0.000001, mae: 0.001239, mean_q: 0.735157
 79256/100000: episode: 1181, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.697, mean reward: 0.348 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.163, 10.100], loss: 0.000210, mae: 0.004330, mean_q: 0.736706
 79257/100000: episode: 1182, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.699, mean reward: 0.699 [0.699, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.297, 10.200], loss: 0.000001, mae: 0.001433, mean_q: 0.737146
 79259/100000: episode: 1183, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.719, mean reward: 0.360 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.304, 10.100], loss: 0.000067, mae: 0.002300, mean_q: 0.739346
 79268/100000: episode: 1184, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 1.070, mean reward: 0.119 [0.000, 1.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.672, 10.100], loss: 0.000027, mae: 0.001823, mean_q: 0.734863
 79270/100000: episode: 1185, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.717, mean reward: 0.359 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.261, 10.100], loss: 0.000022, mae: 0.002685, mean_q: 0.735491
 79278/100000: episode: 1186, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 0.780, mean reward: 0.097 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.272, 10.100], loss: 0.000015, mae: 0.002079, mean_q: 0.731836
 79287/100000: episode: 1187, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 0.953, mean reward: 0.106 [0.000, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.585, 10.100], loss: 0.000012, mae: 0.001495, mean_q: 0.735029
 79298/100000: episode: 1188, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 0.812, mean reward: 0.074 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.401, 10.100], loss: 0.000032, mae: 0.001908, mean_q: 0.734720
 79300/100000: episode: 1189, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.691, mean reward: 0.345 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.251, 10.100], loss: 0.000163, mae: 0.006047, mean_q: 0.734613
 79308/100000: episode: 1190, duration: 0.057s, episode steps: 8, steps per second: 139, episode reward: 0.765, mean reward: 0.096 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.305, 10.100], loss: 0.000048, mae: 0.003116, mean_q: 0.736883
 79315/100000: episode: 1191, duration: 0.053s, episode steps: 7, steps per second: 133, episode reward: 0.706, mean reward: 0.101 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.288, 10.100], loss: 0.000023, mae: 0.003537, mean_q: 0.735015
 79317/100000: episode: 1192, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.732, mean reward: 0.366 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.310, 10.100], loss: 0.000028, mae: 0.002572, mean_q: 0.735694
 79325/100000: episode: 1193, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 0.768, mean reward: 0.096 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.203, 10.100], loss: 0.000115, mae: 0.003653, mean_q: 0.737753
 79326/100000: episode: 1194, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.678, mean reward: 0.678 [0.678, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.284, 10.200], loss: 0.000214, mae: 0.007966, mean_q: 0.744278
 79328/100000: episode: 1195, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.719, mean reward: 0.359 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.282, 10.100], loss: 0.000045, mae: 0.003252, mean_q: 0.736560
 79339/100000: episode: 1196, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 0.747, mean reward: 0.068 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.294, 10.100], loss: 0.000024, mae: 0.002092, mean_q: 0.735340
 79347/100000: episode: 1197, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 0.701, mean reward: 0.088 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.374, 10.100], loss: 0.000022, mae: 0.003105, mean_q: 0.733958
 79358/100000: episode: 1198, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 0.818, mean reward: 0.074 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.380, 10.100], loss: 0.000177, mae: 0.004134, mean_q: 0.735380
 79370/100000: episode: 1199, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 0.810, mean reward: 0.068 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.338, 10.100], loss: 0.000043, mae: 0.004143, mean_q: 0.735384
 79377/100000: episode: 1200, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 0.729, mean reward: 0.104 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.179, 10.100], loss: 0.000278, mae: 0.007111, mean_q: 0.740257
 79386/100000: episode: 1201, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 0.950, mean reward: 0.106 [0.000, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.536, 10.100], loss: 0.000038, mae: 0.004632, mean_q: 0.735082
 79388/100000: episode: 1202, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.714, mean reward: 0.357 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.312, 10.100], loss: 0.000007, mae: 0.003653, mean_q: 0.736903
 79390/100000: episode: 1203, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.654, mean reward: 0.327 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.258, 10.100], loss: 0.000084, mae: 0.004356, mean_q: 0.739875
 79392/100000: episode: 1204, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.841, mean reward: 0.420 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.804, 10.100], loss: 0.000002, mae: 0.001593, mean_q: 0.733478
 79400/100000: episode: 1205, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 0.771, mean reward: 0.096 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.349, 10.100], loss: 0.000032, mae: 0.001890, mean_q: 0.736641
 79412/100000: episode: 1206, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 0.818, mean reward: 0.068 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.378, 10.100], loss: 0.000103, mae: 0.003273, mean_q: 0.736798
 79423/100000: episode: 1207, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 0.812, mean reward: 0.074 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.442, 10.100], loss: 0.000072, mae: 0.004393, mean_q: 0.735702
 79430/100000: episode: 1208, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 0.616, mean reward: 0.088 [0.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.461, 10.100], loss: 0.000053, mae: 0.004299, mean_q: 0.738007
 79442/100000: episode: 1209, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 0.897, mean reward: 0.075 [0.000, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.443, 10.100], loss: 0.000043, mae: 0.003339, mean_q: 0.737022
 79444/100000: episode: 1210, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.732, mean reward: 0.366 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.267, 10.100], loss: 0.000024, mae: 0.002296, mean_q: 0.736052
 79446/100000: episode: 1211, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.738, mean reward: 0.369 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.329, 10.100], loss: 0.000037, mae: 0.003238, mean_q: 0.736456
 79448/100000: episode: 1212, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.654, mean reward: 0.327 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.213, 10.100], loss: 0.000280, mae: 0.005305, mean_q: 0.735323
 79456/100000: episode: 1213, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 0.739, mean reward: 0.092 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.289, 10.100], loss: 0.000082, mae: 0.003972, mean_q: 0.739352
 79464/100000: episode: 1214, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 0.781, mean reward: 0.098 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.150, 10.100], loss: 0.000246, mae: 0.005926, mean_q: 0.734679
 79466/100000: episode: 1215, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 0.707, mean reward: 0.353 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.301, 10.100], loss: 0.000006, mae: 0.003291, mean_q: 0.743922
 79473/100000: episode: 1216, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 0.720, mean reward: 0.103 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.217, 10.100], loss: 0.000068, mae: 0.002790, mean_q: 0.736078
 79474/100000: episode: 1217, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.691, mean reward: 0.691 [0.691, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.317, 10.200], loss: 0.000001, mae: 0.001037, mean_q: 0.735520
 79481/100000: episode: 1218, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 0.713, mean reward: 0.102 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.325, 10.100], loss: 0.000118, mae: 0.003349, mean_q: 0.737917
 79488/100000: episode: 1219, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 0.760, mean reward: 0.109 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.338, 10.100], loss: 0.000010, mae: 0.001457, mean_q: 0.739756
 79495/100000: episode: 1220, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 0.792, mean reward: 0.113 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.286, 10.100], loss: 0.000035, mae: 0.001757, mean_q: 0.738202
 79497/100000: episode: 1221, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.780, mean reward: 0.390 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.291, 10.100], loss: 0.000058, mae: 0.003454, mean_q: 0.738191
 79499/100000: episode: 1222, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.672, mean reward: 0.336 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.191, 10.100], loss: 0.000096, mae: 0.003967, mean_q: 0.739233
 79506/100000: episode: 1223, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 0.727, mean reward: 0.104 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.710, 10.100], loss: 0.000017, mae: 0.001926, mean_q: 0.736382
 79514/100000: episode: 1224, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 0.738, mean reward: 0.092 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.359, 10.100], loss: 0.000009, mae: 0.001232, mean_q: 0.737942
 79516/100000: episode: 1225, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.697, mean reward: 0.348 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.315, 10.100], loss: 0.000001, mae: 0.001081, mean_q: 0.742730
 79518/100000: episode: 1226, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.684, mean reward: 0.342 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.296, 10.100], loss: 0.000539, mae: 0.007940, mean_q: 0.738432
 79526/100000: episode: 1227, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 0.704, mean reward: 0.088 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.273, 10.100], loss: 0.000083, mae: 0.004008, mean_q: 0.738520
 79534/100000: episode: 1228, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 0.677, mean reward: 0.085 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.328, 10.100], loss: 0.000033, mae: 0.003782, mean_q: 0.736081
 79536/100000: episode: 1229, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.773, mean reward: 0.386 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.313, 10.100], loss: 0.000237, mae: 0.006662, mean_q: 0.744267
 79537/100000: episode: 1230, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.684, mean reward: 0.684 [0.684, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.330, 10.200], loss: 0.000012, mae: 0.004410, mean_q: 0.740440
 79544/100000: episode: 1231, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 0.820, mean reward: 0.117 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.893, 10.100], loss: 0.000040, mae: 0.003209, mean_q: 0.735819
 79546/100000: episode: 1232, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.693, mean reward: 0.347 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.251, 10.100], loss: 0.000022, mae: 0.003756, mean_q: 0.733140
 79557/100000: episode: 1233, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 0.812, mean reward: 0.074 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.452, 10.100], loss: 0.000110, mae: 0.003642, mean_q: 0.740394
 79558/100000: episode: 1234, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.311, 10.200], loss: 0.000011, mae: 0.003760, mean_q: 0.737162
 79560/100000: episode: 1235, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.706, mean reward: 0.353 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.290, 10.100], loss: 0.000113, mae: 0.004621, mean_q: 0.734937
 79571/100000: episode: 1236, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 0.682, mean reward: 0.062 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.407, 10.100], loss: 0.000030, mae: 0.002729, mean_q: 0.739400
 79573/100000: episode: 1237, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.656, mean reward: 0.328 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.208, 10.100], loss: 0.000063, mae: 0.003789, mean_q: 0.732395
 79574/100000: episode: 1238, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.293, 10.200], loss: 0.000161, mae: 0.008022, mean_q: 0.721864
 79576/100000: episode: 1239, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.711, mean reward: 0.356 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.263, 10.100], loss: 0.000041, mae: 0.004946, mean_q: 0.730089
[Info] 2-TH LEVEL FOUND: 0.7723121643066406, Considering 10/100 traces
 79578/100000: episode: 1240, duration: 4.178s, episode steps: 2, steps per second: 0, episode reward: 0.695, mean reward: 0.347 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.251, 10.100], loss: 0.000160, mae: 0.005385, mean_q: 0.735551
 79585/100000: episode: 1241, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.840, mean reward: 0.120 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.580, 10.100], loss: 0.000045, mae: 0.004142, mean_q: 0.741954
 79597/100000: episode: 1242, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 1.025, mean reward: 0.085 [0.000, 1.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.462, 10.100], loss: 0.000040, mae: 0.002939, mean_q: 0.737065
 79609/100000: episode: 1243, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 0.992, mean reward: 0.083 [0.000, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.444, 10.100], loss: 0.000180, mae: 0.004638, mean_q: 0.740981
 79621/100000: episode: 1244, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 1.092, mean reward: 0.091 [0.000, 1.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.757, 10.100], loss: 0.000154, mae: 0.006608, mean_q: 0.738312
 79633/100000: episode: 1245, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 1.072, mean reward: 0.089 [0.000, 1.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.480, 10.100], loss: 0.000129, mae: 0.005568, mean_q: 0.739650
 79636/100000: episode: 1246, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.705, mean reward: 0.235 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.500, 10.100], loss: 0.000150, mae: 0.004019, mean_q: 0.739056
 79648/100000: episode: 1247, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 1.097, mean reward: 0.091 [0.000, 1.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.486, 10.100], loss: 0.000032, mae: 0.002000, mean_q: 0.737594
 79655/100000: episode: 1248, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.778, mean reward: 0.111 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.629, 10.100], loss: 0.000072, mae: 0.003766, mean_q: 0.738906
 79667/100000: episode: 1249, duration: 0.091s, episode steps: 12, steps per second: 133, episode reward: 0.914, mean reward: 0.076 [0.000, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.354, 10.100], loss: 0.000036, mae: 0.002484, mean_q: 0.737636
 79679/100000: episode: 1250, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 1.085, mean reward: 0.090 [0.000, 1.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.485, 10.100], loss: 0.000102, mae: 0.003709, mean_q: 0.737113
 79686/100000: episode: 1251, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 0.759, mean reward: 0.108 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.594, 10.100], loss: 0.000408, mae: 0.005505, mean_q: 0.739121
 79698/100000: episode: 1252, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 1.155, mean reward: 0.096 [0.000, 1.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.606, 10.100], loss: 0.000174, mae: 0.003931, mean_q: 0.739358
 79701/100000: episode: 1253, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 0.708, mean reward: 0.236 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.466, 10.100], loss: 0.000017, mae: 0.003347, mean_q: 0.743285
 79713/100000: episode: 1254, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 1.004, mean reward: 0.084 [0.000, 1.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.846, 10.100], loss: 0.000173, mae: 0.003788, mean_q: 0.741060
 79725/100000: episode: 1255, duration: 0.096s, episode steps: 12, steps per second: 126, episode reward: 1.050, mean reward: 0.087 [0.000, 1.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.458, 10.100], loss: 0.000029, mae: 0.002988, mean_q: 0.739654
 79737/100000: episode: 1256, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 0.983, mean reward: 0.082 [0.000, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.482, 10.100], loss: 0.000144, mae: 0.006211, mean_q: 0.737910
 79744/100000: episode: 1257, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.835, mean reward: 0.119 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.580, 10.100], loss: 0.000183, mae: 0.008786, mean_q: 0.745559
 79751/100000: episode: 1258, duration: 0.050s, episode steps: 7, steps per second: 140, episode reward: 0.796, mean reward: 0.114 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.539, 10.100], loss: 0.000175, mae: 0.005950, mean_q: 0.736912
 79763/100000: episode: 1259, duration: 0.090s, episode steps: 12, steps per second: 133, episode reward: 0.975, mean reward: 0.081 [0.000, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.544, 10.100], loss: 0.000263, mae: 0.005563, mean_q: 0.744043
 79770/100000: episode: 1260, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 0.743, mean reward: 0.106 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.457, 10.100], loss: 0.000169, mae: 0.006860, mean_q: 0.744372
 79773/100000: episode: 1261, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 0.700, mean reward: 0.233 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.432, 10.100], loss: 0.000067, mae: 0.004258, mean_q: 0.740058
[Info] FALSIFICATION!
 79785/100000: episode: 1262, duration: 0.559s, episode steps: 12, steps per second: 21, episode reward: 1.091, mean reward: 0.091 [0.000, 1.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.065, 10.100], loss: 0.000152, mae: 0.006115, mean_q: 0.740206
 79797/100000: episode: 1263, duration: 0.087s, episode steps: 12, steps per second: 138, episode reward: 1.081, mean reward: 0.090 [0.000, 1.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.576, 10.100], loss: 0.000174, mae: 0.003818, mean_q: 0.739866
 79809/100000: episode: 1264, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 1.106, mean reward: 0.092 [0.000, 1.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.598, 10.100], loss: 0.000122, mae: 0.004523, mean_q: 0.746042
 79821/100000: episode: 1265, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 1.003, mean reward: 0.084 [0.000, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.379, 10.100], loss: 0.000264, mae: 0.006445, mean_q: 0.743407
 79828/100000: episode: 1266, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 0.797, mean reward: 0.114 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.547, 10.100], loss: 0.000131, mae: 0.005147, mean_q: 0.737685
 79835/100000: episode: 1267, duration: 0.048s, episode steps: 7, steps per second: 144, episode reward: 0.813, mean reward: 0.116 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.456, 10.100], loss: 0.000064, mae: 0.004537, mean_q: 0.743596
 79838/100000: episode: 1268, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 0.681, mean reward: 0.227 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.421, 10.100], loss: 0.000462, mae: 0.005368, mean_q: 0.741692
 79850/100000: episode: 1269, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 1.021, mean reward: 0.085 [0.000, 1.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.468, 10.100], loss: 0.000018, mae: 0.002083, mean_q: 0.741412
 79862/100000: episode: 1270, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 0.980, mean reward: 0.082 [0.000, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.452, 10.100], loss: 0.000144, mae: 0.003502, mean_q: 0.741855
 79874/100000: episode: 1271, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 1.073, mean reward: 0.089 [0.000, 1.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.550, 10.100], loss: 0.000373, mae: 0.006854, mean_q: 0.744837
 79886/100000: episode: 1272, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 1.007, mean reward: 0.084 [0.000, 1.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.477, 10.100], loss: 0.000096, mae: 0.003830, mean_q: 0.740839
 79893/100000: episode: 1273, duration: 0.054s, episode steps: 7, steps per second: 129, episode reward: 0.782, mean reward: 0.112 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.767, 10.100], loss: 0.000459, mae: 0.006872, mean_q: 0.747215
 79905/100000: episode: 1274, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 1.000, mean reward: 0.083 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.042, 10.100], loss: 0.000173, mae: 0.006113, mean_q: 0.741329
 79917/100000: episode: 1275, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 1.029, mean reward: 0.086 [0.000, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.655, 10.100], loss: 0.000121, mae: 0.004571, mean_q: 0.745258
 79929/100000: episode: 1276, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 0.986, mean reward: 0.082 [0.000, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.407, 10.100], loss: 0.000191, mae: 0.005760, mean_q: 0.740402
 79941/100000: episode: 1277, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 1.005, mean reward: 0.084 [0.000, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.554, 10.100], loss: 0.000124, mae: 0.003437, mean_q: 0.743831
 79953/100000: episode: 1278, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 0.984, mean reward: 0.082 [0.000, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.448, 10.100], loss: 0.000303, mae: 0.005442, mean_q: 0.745200
 79965/100000: episode: 1279, duration: 0.086s, episode steps: 12, steps per second: 139, episode reward: 1.030, mean reward: 0.086 [0.000, 1.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.514, 10.100], loss: 0.000308, mae: 0.006892, mean_q: 0.747859
 79972/100000: episode: 1280, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 0.807, mean reward: 0.115 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.508, 10.100], loss: 0.000081, mae: 0.004190, mean_q: 0.744124
 79984/100000: episode: 1281, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 0.957, mean reward: 0.080 [0.000, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.367, 10.100], loss: 0.000114, mae: 0.005757, mean_q: 0.747166
 79996/100000: episode: 1282, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 0.975, mean reward: 0.081 [0.000, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.154, 10.100], loss: 0.000254, mae: 0.005814, mean_q: 0.743826
 80008/100000: episode: 1283, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 0.959, mean reward: 0.080 [0.000, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.599, 10.100], loss: 0.000263, mae: 0.007361, mean_q: 0.749298
 80020/100000: episode: 1284, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 1.081, mean reward: 0.090 [0.000, 1.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.534, 10.100], loss: 0.000176, mae: 0.005480, mean_q: 0.745125
 80032/100000: episode: 1285, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 1.093, mean reward: 0.091 [0.000, 1.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.539, 10.100], loss: 0.000025, mae: 0.002693, mean_q: 0.746084
 80044/100000: episode: 1286, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 0.948, mean reward: 0.079 [0.000, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.794, 10.100], loss: 0.000335, mae: 0.008949, mean_q: 0.751754
 80051/100000: episode: 1287, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.791, mean reward: 0.113 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.387, 10.100], loss: 0.000653, mae: 0.010672, mean_q: 0.749028
 80058/100000: episode: 1288, duration: 0.055s, episode steps: 7, steps per second: 127, episode reward: 0.782, mean reward: 0.112 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.448, 10.100], loss: 0.000318, mae: 0.007908, mean_q: 0.750791
 80070/100000: episode: 1289, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 0.988, mean reward: 0.082 [0.000, 0.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.637, 10.100], loss: 0.000425, mae: 0.010030, mean_q: 0.750386
[Info] FALSIFICATION!
 80082/100000: episode: 1290, duration: 0.357s, episode steps: 12, steps per second: 34, episode reward: 1.227, mean reward: 0.102 [0.000, 1.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.577, 10.100], loss: 0.000171, mae: 0.007889, mean_q: 0.747021
 80089/100000: episode: 1291, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 0.795, mean reward: 0.114 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.653, 10.100], loss: 0.000304, mae: 0.006754, mean_q: 0.749475
 80101/100000: episode: 1292, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 1.010, mean reward: 0.084 [0.000, 1.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.393, 10.100], loss: 0.000293, mae: 0.005488, mean_q: 0.749310
 80113/100000: episode: 1293, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 1.203, mean reward: 0.100 [0.000, 1.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.375, 10.100], loss: 0.000441, mae: 0.009178, mean_q: 0.755785
 80125/100000: episode: 1294, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 0.976, mean reward: 0.081 [0.000, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.425, 10.100], loss: 0.000243, mae: 0.008106, mean_q: 0.747090
 80137/100000: episode: 1295, duration: 0.087s, episode steps: 12, steps per second: 137, episode reward: 1.044, mean reward: 0.087 [0.000, 1.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.948, 10.100], loss: 0.000191, mae: 0.005978, mean_q: 0.754954
 80149/100000: episode: 1296, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 1.190, mean reward: 0.099 [0.000, 1.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.458, 10.100], loss: 0.000297, mae: 0.008759, mean_q: 0.752769
 80161/100000: episode: 1297, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 1.114, mean reward: 0.093 [0.000, 1.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.504, 10.100], loss: 0.000335, mae: 0.006503, mean_q: 0.748049
 80173/100000: episode: 1298, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 0.999, mean reward: 0.083 [0.000, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.394, 10.100], loss: 0.000305, mae: 0.006629, mean_q: 0.749916
[Info] FALSIFICATION!
 80185/100000: episode: 1299, duration: 0.246s, episode steps: 12, steps per second: 49, episode reward: 1.096, mean reward: 0.091 [0.000, 1.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.570, 10.100], loss: 0.000456, mae: 0.010632, mean_q: 0.755553
 80197/100000: episode: 1300, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 1.011, mean reward: 0.084 [0.000, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.532, 10.100], loss: 0.000274, mae: 0.007223, mean_q: 0.751706
 80209/100000: episode: 1301, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 1.044, mean reward: 0.087 [0.000, 1.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.545, 10.100], loss: 0.000323, mae: 0.005998, mean_q: 0.754080
 80221/100000: episode: 1302, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 1.003, mean reward: 0.084 [0.000, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.450, 10.100], loss: 0.000314, mae: 0.005511, mean_q: 0.753377
 80233/100000: episode: 1303, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 0.943, mean reward: 0.079 [0.000, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.472, 10.100], loss: 0.000808, mae: 0.010794, mean_q: 0.756244
 80245/100000: episode: 1304, duration: 0.098s, episode steps: 12, steps per second: 122, episode reward: 1.028, mean reward: 0.086 [0.000, 1.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.457, 10.100], loss: 0.000235, mae: 0.006151, mean_q: 0.757273
 80252/100000: episode: 1305, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 0.865, mean reward: 0.124 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.499, 10.100], loss: 0.000399, mae: 0.007274, mean_q: 0.755370
 80264/100000: episode: 1306, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 0.972, mean reward: 0.081 [0.000, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.448, 10.100], loss: 0.000176, mae: 0.004757, mean_q: 0.754781
 80271/100000: episode: 1307, duration: 0.058s, episode steps: 7, steps per second: 120, episode reward: 0.887, mean reward: 0.127 [0.000, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.534, 10.100], loss: 0.000665, mae: 0.008187, mean_q: 0.755714
 80283/100000: episode: 1308, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 0.928, mean reward: 0.077 [0.000, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.534, 10.100], loss: 0.000607, mae: 0.009985, mean_q: 0.759982
 80295/100000: episode: 1309, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 1.012, mean reward: 0.084 [0.000, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.490, 10.100], loss: 0.000593, mae: 0.008942, mean_q: 0.759966
 80307/100000: episode: 1310, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 1.096, mean reward: 0.091 [0.000, 1.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.622, 10.100], loss: 0.000359, mae: 0.007345, mean_q: 0.754832
[Info] FALSIFICATION!
 80319/100000: episode: 1311, duration: 0.331s, episode steps: 12, steps per second: 36, episode reward: 1.031, mean reward: 0.086 [0.000, 1.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.779, 10.100], loss: 0.000690, mae: 0.010217, mean_q: 0.759903
 80331/100000: episode: 1312, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 0.951, mean reward: 0.079 [0.000, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.423, 10.100], loss: 0.000388, mae: 0.008790, mean_q: 0.758480
 80334/100000: episode: 1313, duration: 0.022s, episode steps: 3, steps per second: 133, episode reward: 0.723, mean reward: 0.241 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.466, 10.100], loss: 0.001784, mae: 0.019466, mean_q: 0.767110
 80346/100000: episode: 1314, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 1.040, mean reward: 0.087 [0.000, 1.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.501, 10.100], loss: 0.000209, mae: 0.010282, mean_q: 0.762106
[Info] FALSIFICATION!
 80358/100000: episode: 1315, duration: 0.366s, episode steps: 12, steps per second: 33, episode reward: 1.084, mean reward: 0.090 [0.000, 1.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.515, 10.100], loss: 0.000179, mae: 0.005715, mean_q: 0.760487
 80370/100000: episode: 1316, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 0.950, mean reward: 0.079 [0.000, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.393, 10.100], loss: 0.000561, mae: 0.010830, mean_q: 0.767189
 80382/100000: episode: 1317, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 1.012, mean reward: 0.084 [0.000, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.509, 10.100], loss: 0.000622, mae: 0.011317, mean_q: 0.763280
 80394/100000: episode: 1318, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 0.987, mean reward: 0.082 [0.000, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.821, 10.100], loss: 0.000542, mae: 0.011665, mean_q: 0.756607
 80401/100000: episode: 1319, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 0.787, mean reward: 0.112 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.535, 10.100], loss: 0.000412, mae: 0.011031, mean_q: 0.767244
 80413/100000: episode: 1320, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 0.992, mean reward: 0.083 [0.000, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.508, 10.100], loss: 0.000479, mae: 0.009736, mean_q: 0.761330
 80416/100000: episode: 1321, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.779, mean reward: 0.260 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.400, 10.100], loss: 0.000101, mae: 0.005405, mean_q: 0.765191
 80428/100000: episode: 1322, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 1.001, mean reward: 0.083 [0.000, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.562, 10.100], loss: 0.000427, mae: 0.008997, mean_q: 0.763295
 80431/100000: episode: 1323, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.708, mean reward: 0.236 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.553, 10.100], loss: 0.000057, mae: 0.004479, mean_q: 0.768387
 80438/100000: episode: 1324, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 0.831, mean reward: 0.119 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.459, 10.100], loss: 0.000280, mae: 0.006924, mean_q: 0.756165
 80450/100000: episode: 1325, duration: 0.105s, episode steps: 12, steps per second: 114, episode reward: 1.011, mean reward: 0.084 [0.000, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.491, 10.100], loss: 0.000336, mae: 0.007227, mean_q: 0.764613
 80462/100000: episode: 1326, duration: 0.139s, episode steps: 12, steps per second: 86, episode reward: 0.947, mean reward: 0.079 [0.000, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.497, 10.100], loss: 0.000373, mae: 0.008148, mean_q: 0.766290
 80474/100000: episode: 1327, duration: 0.090s, episode steps: 12, steps per second: 134, episode reward: 1.026, mean reward: 0.085 [0.000, 1.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.590, 10.100], loss: 0.000366, mae: 0.007377, mean_q: 0.761130
 80486/100000: episode: 1328, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 0.986, mean reward: 0.082 [0.000, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.602, 10.100], loss: 0.000294, mae: 0.006868, mean_q: 0.767937
 80489/100000: episode: 1329, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 0.699, mean reward: 0.233 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.488, 10.100], loss: 0.000627, mae: 0.008859, mean_q: 0.757408
[Info] Complete ISplit Iteration
[Info] Levels: [0.767964, 0.77231216, 0.8332857]
[Info] Cond. Prob: [0.1, 0.1, 0.05]
[Info] Error Prob: 0.0005000000000000001

 80501/100000: episode: 1330, duration: 5.262s, episode steps: 12, steps per second: 2, episode reward: 0.984, mean reward: 0.082 [0.000, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.502, 10.100], loss: 0.000321, mae: 0.008867, mean_q: 0.769968
 80602/100000: episode: 1331, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.051, 10.100], loss: 0.000425, mae: 0.009937, mean_q: 0.769916
 80703/100000: episode: 1332, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.250, 10.100], loss: 0.000437, mae: 0.009417, mean_q: 0.771412
 80804/100000: episode: 1333, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.637, 10.100], loss: 0.000298, mae: 0.007788, mean_q: 0.770631
 80905/100000: episode: 1334, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.100], loss: 0.000372, mae: 0.008295, mean_q: 0.772355
 81006/100000: episode: 1335, duration: 0.622s, episode steps: 101, steps per second: 162, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.534, 10.100], loss: 0.000395, mae: 0.009580, mean_q: 0.773722
 81107/100000: episode: 1336, duration: 0.780s, episode steps: 101, steps per second: 129, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.678, 10.100], loss: 0.000363, mae: 0.009031, mean_q: 0.775002
 81208/100000: episode: 1337, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.500, 10.479], loss: 0.000343, mae: 0.008504, mean_q: 0.774984
 81309/100000: episode: 1338, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.162, 10.100], loss: 0.000337, mae: 0.009591, mean_q: 0.774182
 81410/100000: episode: 1339, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.850, mean reward: 0.008 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.390, 10.137], loss: 0.000302, mae: 0.008659, mean_q: 0.777376
 81511/100000: episode: 1340, duration: 0.694s, episode steps: 101, steps per second: 146, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.638, 10.174], loss: 0.000484, mae: 0.010854, mean_q: 0.779386
 81612/100000: episode: 1341, duration: 0.711s, episode steps: 101, steps per second: 142, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.425, 10.100], loss: 0.000272, mae: 0.008581, mean_q: 0.778650
 81713/100000: episode: 1342, duration: 0.667s, episode steps: 101, steps per second: 151, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.422, 10.100], loss: 0.000290, mae: 0.008051, mean_q: 0.778558
 81814/100000: episode: 1343, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.431, 10.100], loss: 0.000365, mae: 0.010160, mean_q: 0.778191
 81915/100000: episode: 1344, duration: 0.955s, episode steps: 101, steps per second: 106, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.138, 10.100], loss: 0.000348, mae: 0.010042, mean_q: 0.780249
 82016/100000: episode: 1345, duration: 1.102s, episode steps: 101, steps per second: 92, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.142, 10.251], loss: 0.000300, mae: 0.008645, mean_q: 0.779759
 82117/100000: episode: 1346, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.100], loss: 0.000316, mae: 0.009248, mean_q: 0.778135
 82218/100000: episode: 1347, duration: 0.726s, episode steps: 101, steps per second: 139, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.662, 10.100], loss: 0.000305, mae: 0.010074, mean_q: 0.779029
 82319/100000: episode: 1348, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.190, 10.101], loss: 0.000270, mae: 0.009022, mean_q: 0.778684
 82420/100000: episode: 1349, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.254, 10.237], loss: 0.000283, mae: 0.008444, mean_q: 0.779212
 82521/100000: episode: 1350, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.670, 10.100], loss: 0.000309, mae: 0.009224, mean_q: 0.782810
 82622/100000: episode: 1351, duration: 0.918s, episode steps: 101, steps per second: 110, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.495, 10.248], loss: 0.000399, mae: 0.011890, mean_q: 0.783802
 82723/100000: episode: 1352, duration: 1.184s, episode steps: 101, steps per second: 85, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.142, 10.100], loss: 0.000245, mae: 0.007874, mean_q: 0.780983
 82824/100000: episode: 1353, duration: 0.881s, episode steps: 101, steps per second: 115, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.249, 10.100], loss: 0.000357, mae: 0.010046, mean_q: 0.780218
 82925/100000: episode: 1354, duration: 1.143s, episode steps: 101, steps per second: 88, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.100], loss: 0.000304, mae: 0.008498, mean_q: 0.782607
 83026/100000: episode: 1355, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.583, 10.446], loss: 0.000351, mae: 0.010113, mean_q: 0.782306
 83127/100000: episode: 1356, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.456, 10.100], loss: 0.000290, mae: 0.009268, mean_q: 0.780253
 83228/100000: episode: 1357, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.624, 10.395], loss: 0.000233, mae: 0.008009, mean_q: 0.781145
 83329/100000: episode: 1358, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.089, 10.100], loss: 0.000249, mae: 0.008133, mean_q: 0.779641
 83430/100000: episode: 1359, duration: 0.525s, episode steps: 101, steps per second: 193, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.862, 10.183], loss: 0.000361, mae: 0.009354, mean_q: 0.781573
 83531/100000: episode: 1360, duration: 0.533s, episode steps: 101, steps per second: 189, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.766, 10.195], loss: 0.000280, mae: 0.009069, mean_q: 0.779323
 83632/100000: episode: 1361, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.900, 10.100], loss: 0.000218, mae: 0.008034, mean_q: 0.779861
 83733/100000: episode: 1362, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.496, 10.174], loss: 0.000295, mae: 0.008607, mean_q: 0.778453
 83834/100000: episode: 1363, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.679, 10.270], loss: 0.000379, mae: 0.010926, mean_q: 0.779471
 83935/100000: episode: 1364, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.620, 10.102], loss: 0.000280, mae: 0.009914, mean_q: 0.778749
 84036/100000: episode: 1365, duration: 0.529s, episode steps: 101, steps per second: 191, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.592, 10.258], loss: 0.000297, mae: 0.009072, mean_q: 0.777113
 84137/100000: episode: 1366, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.982, 10.100], loss: 0.000321, mae: 0.009240, mean_q: 0.776922
 84238/100000: episode: 1367, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.525, 10.145], loss: 0.000343, mae: 0.009397, mean_q: 0.773447
 84339/100000: episode: 1368, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.719, 10.138], loss: 0.000230, mae: 0.007435, mean_q: 0.775006
 84440/100000: episode: 1369, duration: 0.683s, episode steps: 101, steps per second: 148, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.430, 10.197], loss: 0.000216, mae: 0.006529, mean_q: 0.772271
 84541/100000: episode: 1370, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.982, 10.100], loss: 0.000198, mae: 0.007480, mean_q: 0.770847
 84642/100000: episode: 1371, duration: 0.708s, episode steps: 101, steps per second: 143, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.273, 10.136], loss: 0.000148, mae: 0.005791, mean_q: 0.767907
 84743/100000: episode: 1372, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.451, 10.122], loss: 0.000099, mae: 0.004505, mean_q: 0.761884
 84844/100000: episode: 1373, duration: 0.627s, episode steps: 101, steps per second: 161, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.888, 10.100], loss: 0.000108, mae: 0.005144, mean_q: 0.761293
 84945/100000: episode: 1374, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.545, 10.334], loss: 0.000059, mae: 0.003825, mean_q: 0.754977
 85046/100000: episode: 1375, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.936, 10.201], loss: 0.000090, mae: 0.004224, mean_q: 0.752639
 85147/100000: episode: 1376, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.543, 10.100], loss: 0.000047, mae: 0.003707, mean_q: 0.749643
 85248/100000: episode: 1377, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.611, 10.100], loss: 0.000074, mae: 0.004171, mean_q: 0.744154
 85349/100000: episode: 1378, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.035, 10.329], loss: 0.000081, mae: 0.004191, mean_q: 0.741122
 85450/100000: episode: 1379, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.821, 10.100], loss: 0.000029, mae: 0.003110, mean_q: 0.736973
 85551/100000: episode: 1380, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.633, 10.100], loss: 0.000019, mae: 0.002589, mean_q: 0.735371
 85652/100000: episode: 1381, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.542, 10.109], loss: 0.000018, mae: 0.002595, mean_q: 0.734910
 85753/100000: episode: 1382, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.149, 10.135], loss: 0.000018, mae: 0.002382, mean_q: 0.735038
 85854/100000: episode: 1383, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.603, 10.255], loss: 0.000026, mae: 0.002876, mean_q: 0.734870
 85955/100000: episode: 1384, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.100], loss: 0.000017, mae: 0.002469, mean_q: 0.735445
 86056/100000: episode: 1385, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.391, 10.100], loss: 0.000025, mae: 0.002773, mean_q: 0.734586
 86157/100000: episode: 1386, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.391, 10.142], loss: 0.000030, mae: 0.002994, mean_q: 0.735351
 86258/100000: episode: 1387, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.019, 10.115], loss: 0.000025, mae: 0.002779, mean_q: 0.735658
 86359/100000: episode: 1388, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.483, 10.378], loss: 0.000019, mae: 0.002793, mean_q: 0.735512
 86460/100000: episode: 1389, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.953, mean reward: 0.009 [0.000, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.907, 10.260], loss: 0.000017, mae: 0.002374, mean_q: 0.734942
 86561/100000: episode: 1390, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.275, 10.100], loss: 0.000036, mae: 0.003008, mean_q: 0.735529
 86662/100000: episode: 1391, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.518, 10.313], loss: 0.000020, mae: 0.002344, mean_q: 0.736206
 86763/100000: episode: 1392, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.757, 10.183], loss: 0.000028, mae: 0.002871, mean_q: 0.734833
 86864/100000: episode: 1393, duration: 0.637s, episode steps: 101, steps per second: 158, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.127], loss: 0.000031, mae: 0.002938, mean_q: 0.735301
 86965/100000: episode: 1394, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.931, 10.498], loss: 0.000036, mae: 0.002968, mean_q: 0.734938
 87066/100000: episode: 1395, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.846, 10.100], loss: 0.000043, mae: 0.003337, mean_q: 0.734680
 87167/100000: episode: 1396, duration: 0.661s, episode steps: 101, steps per second: 153, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.780, 10.299], loss: 0.000040, mae: 0.003109, mean_q: 0.733950
 87268/100000: episode: 1397, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.471, 10.139], loss: 0.000023, mae: 0.002464, mean_q: 0.733407
 87369/100000: episode: 1398, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.805, 10.100], loss: 0.000010, mae: 0.001937, mean_q: 0.733825
 87470/100000: episode: 1399, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.766, 10.100], loss: 0.000019, mae: 0.002385, mean_q: 0.733938
 87571/100000: episode: 1400, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.330, 10.237], loss: 0.000020, mae: 0.002299, mean_q: 0.733555
 87672/100000: episode: 1401, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.516, 10.100], loss: 0.000035, mae: 0.002536, mean_q: 0.733592
 87773/100000: episode: 1402, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.837, mean reward: 0.008 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.841, 10.100], loss: 0.000019, mae: 0.002290, mean_q: 0.734709
 87874/100000: episode: 1403, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.776, 10.100], loss: 0.000017, mae: 0.002215, mean_q: 0.735069
 87975/100000: episode: 1404, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.471, 10.100], loss: 0.000035, mae: 0.002955, mean_q: 0.734614
 88076/100000: episode: 1405, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.453, 10.100], loss: 0.000016, mae: 0.002130, mean_q: 0.734979
 88177/100000: episode: 1406, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.134, 10.135], loss: 0.000013, mae: 0.001905, mean_q: 0.735580
 88278/100000: episode: 1407, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.599, 10.195], loss: 0.000025, mae: 0.002680, mean_q: 0.735854
 88379/100000: episode: 1408, duration: 0.663s, episode steps: 101, steps per second: 152, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.227, 10.233], loss: 0.000026, mae: 0.002452, mean_q: 0.735571
 88480/100000: episode: 1409, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.100], loss: 0.000030, mae: 0.002277, mean_q: 0.736586
 88581/100000: episode: 1410, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.879, 10.100], loss: 0.000022, mae: 0.002244, mean_q: 0.736388
 88682/100000: episode: 1411, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.466, 10.119], loss: 0.000034, mae: 0.002948, mean_q: 0.736001
 88783/100000: episode: 1412, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.289, 10.415], loss: 0.000016, mae: 0.002010, mean_q: 0.736894
 88884/100000: episode: 1413, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.230, 10.100], loss: 0.000019, mae: 0.002381, mean_q: 0.736863
 88985/100000: episode: 1414, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.229, 10.141], loss: 0.000023, mae: 0.002620, mean_q: 0.737347
 89086/100000: episode: 1415, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.100], loss: 0.000024, mae: 0.002374, mean_q: 0.737460
 89187/100000: episode: 1416, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.374, 10.209], loss: 0.000024, mae: 0.002677, mean_q: 0.736918
 89288/100000: episode: 1417, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.319, 10.100], loss: 0.000022, mae: 0.002544, mean_q: 0.736899
 89389/100000: episode: 1418, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.818, 10.100], loss: 0.000019, mae: 0.002152, mean_q: 0.737531
 89490/100000: episode: 1419, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.886, 10.100], loss: 0.000019, mae: 0.002149, mean_q: 0.737822
 89591/100000: episode: 1420, duration: 0.493s, episode steps: 101, steps per second: 205, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.572, 10.262], loss: 0.000025, mae: 0.002437, mean_q: 0.738252
 89692/100000: episode: 1421, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.048, 10.358], loss: 0.000017, mae: 0.001996, mean_q: 0.737541
 89793/100000: episode: 1422, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.000, 10.267], loss: 0.000025, mae: 0.002265, mean_q: 0.737833
 89894/100000: episode: 1423, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.973, 10.100], loss: 0.000014, mae: 0.001777, mean_q: 0.738526
 89995/100000: episode: 1424, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.105], loss: 0.000019, mae: 0.002301, mean_q: 0.738522
 90096/100000: episode: 1425, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.821, 10.294], loss: 0.000020, mae: 0.002167, mean_q: 0.738572
 90197/100000: episode: 1426, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.007, 10.100], loss: 0.000017, mae: 0.001884, mean_q: 0.738843
 90298/100000: episode: 1427, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.445, 10.100], loss: 0.000016, mae: 0.002093, mean_q: 0.739178
 90399/100000: episode: 1428, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.029, 10.100], loss: 0.000014, mae: 0.002008, mean_q: 0.738655
 90500/100000: episode: 1429, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.349, 10.101], loss: 0.000012, mae: 0.001915, mean_q: 0.738413
[Info] 1-TH LEVEL FOUND: 0.7958552241325378, Considering 10/100 traces
 90601/100000: episode: 1430, duration: 4.851s, episode steps: 101, steps per second: 21, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.948, 10.100], loss: 0.000015, mae: 0.001790, mean_q: 0.739366
 90617/100000: episode: 1431, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 0.859, mean reward: 0.054 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.327, 10.100], loss: 0.000006, mae: 0.001440, mean_q: 0.738954
 90627/100000: episode: 1432, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 0.715, mean reward: 0.071 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.342, 10.100], loss: 0.000006, mae: 0.001404, mean_q: 0.740937
 90628/100000: episode: 1433, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.211, 10.100], loss: 0.000001, mae: 0.000811, mean_q: 0.736298
 90629/100000: episode: 1434, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.546, mean reward: 0.546 [0.546, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.354, 10.200], loss: 0.000003, mae: 0.001411, mean_q: 0.738565
 90630/100000: episode: 1435, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.537, mean reward: 0.537 [0.537, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.346, 10.200], loss: 0.000020, mae: 0.002693, mean_q: 0.741486
 90640/100000: episode: 1436, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 0.657, mean reward: 0.066 [0.000, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.347, 10.100], loss: 0.000011, mae: 0.001628, mean_q: 0.740097
 90641/100000: episode: 1437, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.273, 10.100], loss: 0.000004, mae: 0.001793, mean_q: 0.734748
 90642/100000: episode: 1438, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.604, mean reward: 0.604 [0.604, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.629, 10.200], loss: 0.000019, mae: 0.003320, mean_q: 0.735860
 90652/100000: episode: 1439, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.762, mean reward: 0.076 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.288, 10.100], loss: 0.000035, mae: 0.002725, mean_q: 0.739856
 90657/100000: episode: 1440, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 0.672, mean reward: 0.134 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.247, 10.100], loss: 0.000205, mae: 0.003335, mean_q: 0.736298
 90658/100000: episode: 1441, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.619, mean reward: 0.619 [0.619, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.305, 10.200], loss: 0.000020, mae: 0.004850, mean_q: 0.741892
 90662/100000: episode: 1442, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 0.655, mean reward: 0.164 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.305, 10.100], loss: 0.000004, mae: 0.002107, mean_q: 0.742440
 90672/100000: episode: 1443, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 0.702, mean reward: 0.070 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.116, 10.100], loss: 0.000106, mae: 0.003613, mean_q: 0.738521
 90673/100000: episode: 1444, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.780, mean reward: 0.780 [0.780, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.272, 10.100], loss: 0.000402, mae: 0.009797, mean_q: 0.746895
 90677/100000: episode: 1445, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.629, mean reward: 0.157 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.277, 10.100], loss: 0.000006, mae: 0.002563, mean_q: 0.738569
 90679/100000: episode: 1446, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.627, mean reward: 0.313 [0.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.339, 10.100], loss: 0.000005, mae: 0.002398, mean_q: 0.733326
 90680/100000: episode: 1447, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.225, 10.100], loss: 0.000004, mae: 0.001597, mean_q: 0.736088
 90682/100000: episode: 1448, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.603, mean reward: 0.301 [0.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.282, 10.100], loss: 0.000004, mae: 0.002594, mean_q: 0.741275
 90698/100000: episode: 1449, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 0.809, mean reward: 0.051 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.307, 10.100], loss: 0.000041, mae: 0.002461, mean_q: 0.741608
 90699/100000: episode: 1450, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.589, mean reward: 0.589 [0.589, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.380, 10.200], loss: 0.000001, mae: 0.000937, mean_q: 0.738024
 90715/100000: episode: 1451, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 0.759, mean reward: 0.047 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.208, 10.100], loss: 0.000141, mae: 0.005765, mean_q: 0.739390
 90717/100000: episode: 1452, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.603, mean reward: 0.302 [0.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.253, 10.100], loss: 0.000015, mae: 0.003469, mean_q: 0.736035
 90727/100000: episode: 1453, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.672, mean reward: 0.067 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.210, 10.100], loss: 0.000149, mae: 0.005357, mean_q: 0.739912
 90732/100000: episode: 1454, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 0.673, mean reward: 0.135 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.435, 10.100], loss: 0.000047, mae: 0.004741, mean_q: 0.741668
 90748/100000: episode: 1455, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 0.745, mean reward: 0.047 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.492, 10.100], loss: 0.000076, mae: 0.003744, mean_q: 0.740045
 90758/100000: episode: 1456, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 0.683, mean reward: 0.068 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.389, 10.100], loss: 0.000213, mae: 0.006441, mean_q: 0.739061
 90762/100000: episode: 1457, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 0.688, mean reward: 0.172 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.287, 10.100], loss: 0.000153, mae: 0.005776, mean_q: 0.741274
 90763/100000: episode: 1458, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.293, 10.200], loss: 0.000009, mae: 0.003213, mean_q: 0.745340
 90773/100000: episode: 1459, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 0.719, mean reward: 0.072 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.577, 10.100], loss: 0.000166, mae: 0.006663, mean_q: 0.736384
 90777/100000: episode: 1460, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 0.743, mean reward: 0.186 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.266, 10.100], loss: 0.000115, mae: 0.005323, mean_q: 0.744533
 90778/100000: episode: 1461, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.628, mean reward: 0.628 [0.628, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.286, 10.200], loss: 0.000012, mae: 0.003432, mean_q: 0.741029
 90782/100000: episode: 1462, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.786, mean reward: 0.196 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.201, 10.100], loss: 0.000067, mae: 0.003917, mean_q: 0.741700
 90793/100000: episode: 1463, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 0.708, mean reward: 0.064 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.309, 10.100], loss: 0.000009, mae: 0.002596, mean_q: 0.740749
 90795/100000: episode: 1464, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.696, mean reward: 0.348 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.290, 10.100], loss: 0.000003, mae: 0.001850, mean_q: 0.743712
 90805/100000: episode: 1465, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.881, mean reward: 0.088 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.337, 10.100], loss: 0.000144, mae: 0.006295, mean_q: 0.736554
 90821/100000: episode: 1466, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 0.739, mean reward: 0.046 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.261, 10.100], loss: 0.000102, mae: 0.006061, mean_q: 0.740187
 90822/100000: episode: 1467, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.249, 10.100], loss: 0.000015, mae: 0.004859, mean_q: 0.742036
 90824/100000: episode: 1468, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.591, mean reward: 0.296 [0.000, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.276, 10.100], loss: 0.000007, mae: 0.003160, mean_q: 0.744542
 90835/100000: episode: 1469, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 0.721, mean reward: 0.066 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.297, 10.100], loss: 0.000067, mae: 0.003566, mean_q: 0.738524
 90836/100000: episode: 1470, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.591, mean reward: 0.591 [0.591, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.389, 10.200], loss: 0.000450, mae: 0.006858, mean_q: 0.742839
 90837/100000: episode: 1471, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.274, 10.100], loss: 0.000010, mae: 0.003179, mean_q: 0.734802
 90841/100000: episode: 1472, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 0.634, mean reward: 0.159 [0.000, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.189, 10.100], loss: 0.000041, mae: 0.003598, mean_q: 0.738417
 90842/100000: episode: 1473, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.361, 10.200], loss: 0.000003, mae: 0.001935, mean_q: 0.737478
 90843/100000: episode: 1474, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.547, mean reward: 0.547 [0.547, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.351, 10.200], loss: 0.000528, mae: 0.008736, mean_q: 0.746809
 90845/100000: episode: 1475, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.566, mean reward: 0.283 [0.000, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.262, 10.100], loss: 0.000020, mae: 0.002561, mean_q: 0.739270
 90855/100000: episode: 1476, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 0.707, mean reward: 0.071 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.346, 10.100], loss: 0.000088, mae: 0.003638, mean_q: 0.738658
 90865/100000: episode: 1477, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 0.697, mean reward: 0.070 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.375, 10.100], loss: 0.000016, mae: 0.002399, mean_q: 0.741629
 90875/100000: episode: 1478, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 0.701, mean reward: 0.070 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.439, 10.100], loss: 0.000021, mae: 0.002210, mean_q: 0.739759
 90879/100000: episode: 1479, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 0.668, mean reward: 0.167 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.265, 10.100], loss: 0.000013, mae: 0.002026, mean_q: 0.741242
 90880/100000: episode: 1480, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.311, 10.200], loss: 0.000002, mae: 0.001314, mean_q: 0.741033
 90896/100000: episode: 1481, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 0.931, mean reward: 0.058 [0.000, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.356, 10.100], loss: 0.000090, mae: 0.004673, mean_q: 0.741367
 90898/100000: episode: 1482, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.717, mean reward: 0.358 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.320, 10.100], loss: 0.000010, mae: 0.001919, mean_q: 0.740554
 90899/100000: episode: 1483, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.561, mean reward: 0.561 [0.561, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.349, 10.200], loss: 0.000042, mae: 0.004012, mean_q: 0.736070
 90904/100000: episode: 1484, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 0.670, mean reward: 0.134 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.291, 10.100], loss: 0.000065, mae: 0.005006, mean_q: 0.737154
 90906/100000: episode: 1485, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.709, mean reward: 0.355 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.358, 10.100], loss: 0.000256, mae: 0.004799, mean_q: 0.742490
 90908/100000: episode: 1486, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.597, mean reward: 0.299 [0.000, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.324, 10.100], loss: 0.000262, mae: 0.005648, mean_q: 0.739014
 90909/100000: episode: 1487, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.701, mean reward: 0.701 [0.701, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.255, 10.100], loss: 0.000029, mae: 0.005502, mean_q: 0.736335
 90910/100000: episode: 1488, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.777, mean reward: 0.777 [0.777, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.994, 10.200], loss: 0.000007, mae: 0.002271, mean_q: 0.734415
 90914/100000: episode: 1489, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 0.635, mean reward: 0.159 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.278, 10.100], loss: 0.000042, mae: 0.003971, mean_q: 0.741258
 90915/100000: episode: 1490, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.670, mean reward: 0.670 [0.670, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.261, 10.100], loss: 0.000066, mae: 0.004774, mean_q: 0.745120
 90926/100000: episode: 1491, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 0.820, mean reward: 0.075 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.331, 10.100], loss: 0.000123, mae: 0.005858, mean_q: 0.737784
 90928/100000: episode: 1492, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.629, mean reward: 0.314 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.252, 10.100], loss: 0.000391, mae: 0.007987, mean_q: 0.740085
 90930/100000: episode: 1493, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.627, mean reward: 0.313 [0.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.295, 10.100], loss: 0.000052, mae: 0.003816, mean_q: 0.739886
 90934/100000: episode: 1494, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 0.704, mean reward: 0.176 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.266, 10.100], loss: 0.000054, mae: 0.003921, mean_q: 0.738385
 90936/100000: episode: 1495, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.617, mean reward: 0.309 [0.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.291, 10.100], loss: 0.000004, mae: 0.001822, mean_q: 0.741288
 90940/100000: episode: 1496, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 0.650, mean reward: 0.163 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.241, 10.100], loss: 0.000026, mae: 0.005620, mean_q: 0.746380
 90941/100000: episode: 1497, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.664, mean reward: 0.664 [0.664, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.255, 10.100], loss: 0.000142, mae: 0.004692, mean_q: 0.741984
 90943/100000: episode: 1498, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.584, mean reward: 0.292 [0.000, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.305, 10.100], loss: 0.000162, mae: 0.006962, mean_q: 0.736287
 90945/100000: episode: 1499, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.581, mean reward: 0.291 [0.000, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.316, 10.100], loss: 0.000325, mae: 0.006064, mean_q: 0.736482
 90955/100000: episode: 1500, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 0.756, mean reward: 0.076 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.356, 10.100], loss: 0.000012, mae: 0.001974, mean_q: 0.738246
 90960/100000: episode: 1501, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 0.776, mean reward: 0.155 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.283, 10.100], loss: 0.000284, mae: 0.007360, mean_q: 0.738635
 90964/100000: episode: 1502, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 0.720, mean reward: 0.180 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.285, 10.100], loss: 0.000044, mae: 0.005114, mean_q: 0.735214
 90966/100000: episode: 1503, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 0.603, mean reward: 0.302 [0.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.266, 10.100], loss: 0.000019, mae: 0.005843, mean_q: 0.742375
 90968/100000: episode: 1504, duration: 0.022s, episode steps: 2, steps per second: 90, episode reward: 0.570, mean reward: 0.285 [0.000, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.173, 10.100], loss: 0.000025, mae: 0.006418, mean_q: 0.747121
 90973/100000: episode: 1505, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 0.674, mean reward: 0.135 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.362, 10.100], loss: 0.000053, mae: 0.005549, mean_q: 0.737231
 90975/100000: episode: 1506, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.573, mean reward: 0.286 [0.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.249, 10.100], loss: 0.000087, mae: 0.004427, mean_q: 0.735569
 90976/100000: episode: 1507, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.633, mean reward: 0.633 [0.633, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.252, 10.200], loss: 0.000824, mae: 0.015223, mean_q: 0.739027
 90987/100000: episode: 1508, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 0.766, mean reward: 0.070 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.180, 10.100], loss: 0.000123, mae: 0.004409, mean_q: 0.739991
 90997/100000: episode: 1509, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 0.768, mean reward: 0.077 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.309, 10.100], loss: 0.000023, mae: 0.002764, mean_q: 0.739291
 91002/100000: episode: 1510, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 0.852, mean reward: 0.170 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.239, 10.100], loss: 0.000129, mae: 0.004311, mean_q: 0.742592
 91003/100000: episode: 1511, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.604, mean reward: 0.604 [0.604, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.315, 10.200], loss: 0.000681, mae: 0.010981, mean_q: 0.741081
 91007/100000: episode: 1512, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.671, mean reward: 0.168 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.215, 10.100], loss: 0.000170, mae: 0.006890, mean_q: 0.737133
 91023/100000: episode: 1513, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 0.669, mean reward: 0.042 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.336, 10.100], loss: 0.000113, mae: 0.005433, mean_q: 0.739414
 91024/100000: episode: 1514, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.285, 10.200], loss: 0.000350, mae: 0.010247, mean_q: 0.733739
 91029/100000: episode: 1515, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 0.733, mean reward: 0.147 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.290, 10.100], loss: 0.000055, mae: 0.005104, mean_q: 0.740964
 91031/100000: episode: 1516, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.638, mean reward: 0.319 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.323, 10.100], loss: 0.000237, mae: 0.005661, mean_q: 0.739242
 91035/100000: episode: 1517, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.659, mean reward: 0.165 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.330, 10.100], loss: 0.000011, mae: 0.002924, mean_q: 0.738726
 91037/100000: episode: 1518, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.597, mean reward: 0.299 [0.000, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.241, 10.100], loss: 0.000061, mae: 0.003579, mean_q: 0.737264
 91039/100000: episode: 1519, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.599, mean reward: 0.300 [0.000, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.323, 10.100], loss: 0.000102, mae: 0.004422, mean_q: 0.735582
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7958552241325378
1
 91044/100000: episode: 1520, duration: 4.141s, episode steps: 5, steps per second: 1, episode reward: 0.683, mean reward: 0.137 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.437, 10.100], loss: 0.000217, mae: 0.005297, mean_q: 0.737684
 91145/100000: episode: 1521, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.100], loss: 0.000105, mae: 0.004995, mean_q: 0.738599
 91246/100000: episode: 1522, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.976, 10.100], loss: 0.000124, mae: 0.005665, mean_q: 0.736830
 91347/100000: episode: 1523, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.066, 10.248], loss: 0.000104, mae: 0.004181, mean_q: 0.736994
 91448/100000: episode: 1524, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.416, 10.320], loss: 0.000091, mae: 0.004598, mean_q: 0.736201
 91549/100000: episode: 1525, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.184, 10.387], loss: 0.000116, mae: 0.004380, mean_q: 0.734647
 91650/100000: episode: 1526, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.853, mean reward: 0.008 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.595, 10.251], loss: 0.000067, mae: 0.003020, mean_q: 0.734678
 91751/100000: episode: 1527, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.310, 10.436], loss: 0.000083, mae: 0.003865, mean_q: 0.733497
 91852/100000: episode: 1528, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.864, 10.100], loss: 0.000109, mae: 0.004282, mean_q: 0.732367
 91953/100000: episode: 1529, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.580, 10.209], loss: 0.000106, mae: 0.004963, mean_q: 0.733727
 92054/100000: episode: 1530, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.209, 10.154], loss: 0.000092, mae: 0.003731, mean_q: 0.732534
 92155/100000: episode: 1531, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.055, 10.432], loss: 0.000101, mae: 0.003844, mean_q: 0.732160
 92256/100000: episode: 1532, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.272, 10.249], loss: 0.000106, mae: 0.004708, mean_q: 0.732244
 92357/100000: episode: 1533, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.564, 10.100], loss: 0.000101, mae: 0.004109, mean_q: 0.731638
 92458/100000: episode: 1534, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.539, 10.205], loss: 0.000090, mae: 0.004121, mean_q: 0.731241
 92559/100000: episode: 1535, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.619, 10.469], loss: 0.000090, mae: 0.003698, mean_q: 0.730776
 92660/100000: episode: 1536, duration: 0.529s, episode steps: 101, steps per second: 191, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.784, 10.193], loss: 0.000075, mae: 0.003638, mean_q: 0.730474
 92761/100000: episode: 1537, duration: 0.542s, episode steps: 101, steps per second: 187, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.685, 10.131], loss: 0.000092, mae: 0.004159, mean_q: 0.730811
 92862/100000: episode: 1538, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.721, 10.198], loss: 0.000112, mae: 0.004438, mean_q: 0.730089
 92963/100000: episode: 1539, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.857, 10.100], loss: 0.000080, mae: 0.003731, mean_q: 0.730517
 93064/100000: episode: 1540, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.438, 10.282], loss: 0.000080, mae: 0.003262, mean_q: 0.730759
 93165/100000: episode: 1541, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.341, 10.100], loss: 0.000114, mae: 0.004411, mean_q: 0.730205
 93266/100000: episode: 1542, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.101, 10.100], loss: 0.000108, mae: 0.004628, mean_q: 0.730026
 93367/100000: episode: 1543, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.564, 10.217], loss: 0.000111, mae: 0.004518, mean_q: 0.729716
 93468/100000: episode: 1544, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.840, 10.100], loss: 0.000112, mae: 0.004746, mean_q: 0.730212
 93569/100000: episode: 1545, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.920, 10.110], loss: 0.000063, mae: 0.003633, mean_q: 0.729674
 93670/100000: episode: 1546, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.000, 10.170], loss: 0.000099, mae: 0.003977, mean_q: 0.728977
 93771/100000: episode: 1547, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.478, 10.100], loss: 0.000085, mae: 0.004279, mean_q: 0.728599
 93872/100000: episode: 1548, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.214, 10.122], loss: 0.000089, mae: 0.003824, mean_q: 0.728304
 93973/100000: episode: 1549, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.730, 10.208], loss: 0.000091, mae: 0.004425, mean_q: 0.728374
 94074/100000: episode: 1550, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.250, 10.231], loss: 0.000125, mae: 0.004490, mean_q: 0.728737
 94175/100000: episode: 1551, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.567, 10.244], loss: 0.000100, mae: 0.003893, mean_q: 0.728251
 94276/100000: episode: 1552, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.760, 10.100], loss: 0.000065, mae: 0.003026, mean_q: 0.728239
 94377/100000: episode: 1553, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.605, 10.106], loss: 0.000119, mae: 0.004683, mean_q: 0.728133
 94478/100000: episode: 1554, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.815, 10.100], loss: 0.000068, mae: 0.003037, mean_q: 0.727984
 94579/100000: episode: 1555, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.880, 10.302], loss: 0.000085, mae: 0.003829, mean_q: 0.727926
 94680/100000: episode: 1556, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.583, 10.290], loss: 0.000080, mae: 0.003237, mean_q: 0.728140
 94781/100000: episode: 1557, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.200], loss: 0.000087, mae: 0.003478, mean_q: 0.726641
 94882/100000: episode: 1558, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.627, 10.146], loss: 0.000087, mae: 0.003913, mean_q: 0.726807
 94983/100000: episode: 1559, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.049, 10.308], loss: 0.000087, mae: 0.003496, mean_q: 0.727134
 95084/100000: episode: 1560, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.913, 10.284], loss: 0.000065, mae: 0.002646, mean_q: 0.726510
 95185/100000: episode: 1561, duration: 0.527s, episode steps: 101, steps per second: 191, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.093, 10.184], loss: 0.000097, mae: 0.004049, mean_q: 0.726320
 95286/100000: episode: 1562, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.504, 10.100], loss: 0.000080, mae: 0.003292, mean_q: 0.726456
 95387/100000: episode: 1563, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.100], loss: 0.000058, mae: 0.003458, mean_q: 0.725642
 95488/100000: episode: 1564, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.212], loss: 0.000106, mae: 0.003785, mean_q: 0.725661
 95589/100000: episode: 1565, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.499, 10.100], loss: 0.000098, mae: 0.003502, mean_q: 0.725931
 95690/100000: episode: 1566, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.056, 10.174], loss: 0.000057, mae: 0.002951, mean_q: 0.726088
 95791/100000: episode: 1567, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.737, 10.100], loss: 0.000060, mae: 0.002544, mean_q: 0.727187
 95892/100000: episode: 1568, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.402, 10.237], loss: 0.000035, mae: 0.002182, mean_q: 0.727747
 95993/100000: episode: 1569, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.032, 10.209], loss: 0.000045, mae: 0.002432, mean_q: 0.728368
 96094/100000: episode: 1570, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.600, 10.206], loss: 0.000012, mae: 0.001479, mean_q: 0.728797
 96195/100000: episode: 1571, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.745, 10.208], loss: 0.000027, mae: 0.001846, mean_q: 0.729001
 96296/100000: episode: 1572, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.599, 10.100], loss: 0.000036, mae: 0.002286, mean_q: 0.729711
 96397/100000: episode: 1573, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.226, 10.100], loss: 0.000019, mae: 0.001730, mean_q: 0.729832
 96498/100000: episode: 1574, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.435, 10.100], loss: 0.000006, mae: 0.000874, mean_q: 0.729375
 96599/100000: episode: 1575, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.429, 10.100], loss: 0.000013, mae: 0.001088, mean_q: 0.729767
 96700/100000: episode: 1576, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.678, mean reward: 0.007 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.603, 10.303], loss: 0.000014, mae: 0.001514, mean_q: 0.729942
 96801/100000: episode: 1577, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.627, 10.100], loss: 0.000012, mae: 0.001469, mean_q: 0.729903
 96902/100000: episode: 1578, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.391, 10.445], loss: 0.000009, mae: 0.001017, mean_q: 0.729872
 97003/100000: episode: 1579, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.833, 10.104], loss: 0.000009, mae: 0.001049, mean_q: 0.730084
 97104/100000: episode: 1580, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.461, 10.363], loss: 0.000009, mae: 0.001007, mean_q: 0.729998
 97205/100000: episode: 1581, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.292, 10.187], loss: 0.000011, mae: 0.001035, mean_q: 0.730337
 97306/100000: episode: 1582, duration: 0.536s, episode steps: 101, steps per second: 188, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.332, 10.123], loss: 0.000008, mae: 0.001187, mean_q: 0.730395
 97407/100000: episode: 1583, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.980, 10.100], loss: 0.000010, mae: 0.001382, mean_q: 0.730673
 97508/100000: episode: 1584, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.677, 10.100], loss: 0.000009, mae: 0.001064, mean_q: 0.730663
 97609/100000: episode: 1585, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.972, 10.100], loss: 0.000009, mae: 0.001409, mean_q: 0.730346
 97710/100000: episode: 1586, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.351, 10.118], loss: 0.000007, mae: 0.001026, mean_q: 0.730722
 97811/100000: episode: 1587, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.577, 10.110], loss: 0.000010, mae: 0.001521, mean_q: 0.730825
 97912/100000: episode: 1588, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.221, 10.135], loss: 0.000010, mae: 0.001324, mean_q: 0.730894
 98013/100000: episode: 1589, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.739, 10.465], loss: 0.000011, mae: 0.001220, mean_q: 0.731119
 98114/100000: episode: 1590, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.005, 10.100], loss: 0.000013, mae: 0.001255, mean_q: 0.731219
 98215/100000: episode: 1591, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.888, 10.100], loss: 0.000009, mae: 0.001132, mean_q: 0.731203
 98316/100000: episode: 1592, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.144, 10.172], loss: 0.000008, mae: 0.000786, mean_q: 0.731282
 98417/100000: episode: 1593, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.807, 10.100], loss: 0.000006, mae: 0.000920, mean_q: 0.731552
 98518/100000: episode: 1594, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.009, 10.125], loss: 0.000006, mae: 0.000767, mean_q: 0.731529
 98619/100000: episode: 1595, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.143, 10.205], loss: 0.000006, mae: 0.001094, mean_q: 0.731540
 98720/100000: episode: 1596, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.106, 10.253], loss: 0.000010, mae: 0.001159, mean_q: 0.731640
 98821/100000: episode: 1597, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.691, 10.100], loss: 0.000012, mae: 0.001570, mean_q: 0.732085
 98922/100000: episode: 1598, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.569, 10.100], loss: 0.000006, mae: 0.001087, mean_q: 0.731910
 99023/100000: episode: 1599, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.655, 10.100], loss: 0.000008, mae: 0.001125, mean_q: 0.731969
 99124/100000: episode: 1600, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.121, 10.151], loss: 0.000008, mae: 0.001054, mean_q: 0.732082
 99225/100000: episode: 1601, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.423, 10.100], loss: 0.000012, mae: 0.001310, mean_q: 0.732048
 99326/100000: episode: 1602, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.990, 10.108], loss: 0.000006, mae: 0.001028, mean_q: 0.732413
 99427/100000: episode: 1603, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.594, 10.226], loss: 0.000005, mae: 0.001034, mean_q: 0.732716
 99528/100000: episode: 1604, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.257, 10.217], loss: 0.000007, mae: 0.000802, mean_q: 0.732483
 99629/100000: episode: 1605, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.116, 10.100], loss: 0.000009, mae: 0.001345, mean_q: 0.732584
 99730/100000: episode: 1606, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.308, 10.100], loss: 0.000011, mae: 0.001606, mean_q: 0.732865
 99831/100000: episode: 1607, duration: 0.533s, episode steps: 101, steps per second: 189, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.446, 10.100], loss: 0.000007, mae: 0.001071, mean_q: 0.733130
 99932/100000: episode: 1608, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.639, 10.107], loss: 0.000004, mae: 0.000812, mean_q: 0.733129
done, took 622.378 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
