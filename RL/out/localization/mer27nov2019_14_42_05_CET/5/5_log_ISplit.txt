Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_14_42_05_CET/5, out/localization/mer27nov2019_14_42_05_CET/5/levels, out/localization/mer27nov2019_14_42_05_CET/5/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.199s, episode steps: 101, steps per second: 506, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.064, 10.310], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.067s, episode steps: 101, steps per second: 1506, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.470, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.067s, episode steps: 101, steps per second: 1509, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.100], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.091s, episode steps: 101, steps per second: 1111, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.136, 10.197], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.070s, episode steps: 101, steps per second: 1443, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.655, 10.142], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.067s, episode steps: 101, steps per second: 1512, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.421, 10.313], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.068s, episode steps: 101, steps per second: 1479, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.856, 10.100], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.067s, episode steps: 101, steps per second: 1509, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.496, 10.302], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.067s, episode steps: 101, steps per second: 1497, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.455, 10.176], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.067s, episode steps: 101, steps per second: 1512, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.100], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.073s, episode steps: 101, steps per second: 1380, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.788, 10.152], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.071s, episode steps: 101, steps per second: 1425, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.967, 10.261], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.067s, episode steps: 101, steps per second: 1512, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.203, 10.100], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.066s, episode steps: 101, steps per second: 1521, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.922, 10.175], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.069s, episode steps: 101, steps per second: 1466, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.780, 10.100], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.070s, episode steps: 101, steps per second: 1452, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.970, 10.143], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.071s, episode steps: 101, steps per second: 1414, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.847, 10.286], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.066s, episode steps: 101, steps per second: 1519, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.689, 10.100], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.066s, episode steps: 101, steps per second: 1523, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.898, 10.100], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.082s, episode steps: 101, steps per second: 1232, episode reward: 0.876, mean reward: 0.009 [0.000, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.061, 10.100], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.074s, episode steps: 101, steps per second: 1370, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.481, 10.100], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.066s, episode steps: 101, steps per second: 1525, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.763, 10.100], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.066s, episode steps: 101, steps per second: 1527, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.634, 10.100], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.089s, episode steps: 101, steps per second: 1138, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.271, 10.152], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.091s, episode steps: 101, steps per second: 1114, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.461, 10.121], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.075s, episode steps: 101, steps per second: 1349, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.159], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.074s, episode steps: 101, steps per second: 1369, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.330, 10.100], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.071s, episode steps: 101, steps per second: 1422, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.180, 10.193], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.085s, episode steps: 101, steps per second: 1186, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.115, 10.457], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.136s, episode steps: 101, steps per second: 742, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.709, 10.350], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.090s, episode steps: 101, steps per second: 1117, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.847, 10.100], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.066s, episode steps: 101, steps per second: 1528, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.628, 10.144], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.066s, episode steps: 101, steps per second: 1524, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.624, 10.243], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.072s, episode steps: 101, steps per second: 1400, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.888, 10.100], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.066s, episode steps: 101, steps per second: 1527, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.128, 10.153], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.072s, episode steps: 101, steps per second: 1410, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.243, 10.134], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.067s, episode steps: 101, steps per second: 1511, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.886, 10.231], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.066s, episode steps: 101, steps per second: 1525, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.979, 10.205], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.082s, episode steps: 101, steps per second: 1227, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.160, 10.102], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.067s, episode steps: 101, steps per second: 1499, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.731, 10.100], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.066s, episode steps: 101, steps per second: 1526, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.758, 10.131], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.066s, episode steps: 101, steps per second: 1530, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.140], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.066s, episode steps: 101, steps per second: 1520, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.635, 10.208], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.066s, episode steps: 101, steps per second: 1529, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.284, 10.100], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.066s, episode steps: 101, steps per second: 1523, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.759, 10.100], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.066s, episode steps: 101, steps per second: 1531, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.832, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.081s, episode steps: 101, steps per second: 1242, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.948, 10.292], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.085s, episode steps: 101, steps per second: 1183, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.180, 10.100], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.066s, episode steps: 101, steps per second: 1522, episode reward: 0.660, mean reward: 0.007 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.937, 10.105], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 0.991s, episode steps: 101, steps per second: 102, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.740, 10.100], loss: 0.023208, mae: 0.076352, mean_q: -0.858486
  5151/100000: episode: 51, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.378, 10.175], loss: 0.022753, mae: 0.060410, mean_q: -0.858034
  5252/100000: episode: 52, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.523, 10.100], loss: 0.021048, mae: 0.065388, mean_q: -0.846631
  5353/100000: episode: 53, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.226, 10.153], loss: 0.015125, mae: 0.054737, mean_q: -0.837431
  5454/100000: episode: 54, duration: 0.630s, episode steps: 101, steps per second: 160, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.875, 10.100], loss: 0.013312, mae: 0.048281, mean_q: -0.819238
  5555/100000: episode: 55, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.412, 10.100], loss: 0.009033, mae: 0.043658, mean_q: -0.809776
  5656/100000: episode: 56, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.860, 10.131], loss: 0.008265, mae: 0.046502, mean_q: -0.788994
  5757/100000: episode: 57, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.661, mean reward: 0.007 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.805, 10.104], loss: 0.003510, mae: 0.030099, mean_q: -0.778624
  5858/100000: episode: 58, duration: 0.641s, episode steps: 101, steps per second: 158, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-2.058, 10.100], loss: 0.002126, mae: 0.022800, mean_q: -0.780052
  5959/100000: episode: 59, duration: 0.669s, episode steps: 101, steps per second: 151, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.947, 10.332], loss: 0.002496, mae: 0.027560, mean_q: -0.761756
  6060/100000: episode: 60, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.145, 10.219], loss: 0.001006, mae: 0.018298, mean_q: -0.754515
  6161/100000: episode: 61, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.742, 10.100], loss: 0.000451, mae: 0.015611, mean_q: -0.749878
  6262/100000: episode: 62, duration: 0.707s, episode steps: 101, steps per second: 143, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.721, 10.224], loss: 0.000289, mae: 0.012380, mean_q: -0.727019
  6363/100000: episode: 63, duration: 0.614s, episode steps: 101, steps per second: 165, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.569, 10.178], loss: 0.000266, mae: 0.012241, mean_q: -0.723678
  6464/100000: episode: 64, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.915, 10.100], loss: 0.000210, mae: 0.012506, mean_q: -0.698094
  6565/100000: episode: 65, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.362, 10.100], loss: 0.000167, mae: 0.011429, mean_q: -0.682656
  6666/100000: episode: 66, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.391, 10.100], loss: 0.000178, mae: 0.012067, mean_q: -0.667527
  6767/100000: episode: 67, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.335, 10.186], loss: 0.000170, mae: 0.011956, mean_q: -0.667377
  6868/100000: episode: 68, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.824, 10.186], loss: 0.000185, mae: 0.012481, mean_q: -0.652841
  6969/100000: episode: 69, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.229], loss: 0.000157, mae: 0.011592, mean_q: -0.664224
  7070/100000: episode: 70, duration: 0.686s, episode steps: 101, steps per second: 147, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.456, 10.178], loss: 0.000158, mae: 0.011396, mean_q: -0.624944
  7171/100000: episode: 71, duration: 0.629s, episode steps: 101, steps per second: 161, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.575, 10.480], loss: 0.000136, mae: 0.010677, mean_q: -0.618301
  7272/100000: episode: 72, duration: 0.708s, episode steps: 101, steps per second: 143, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.100], loss: 0.000188, mae: 0.012577, mean_q: -0.609033
  7373/100000: episode: 73, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.367, 10.275], loss: 0.000164, mae: 0.012075, mean_q: -0.592457
  7474/100000: episode: 74, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.100], loss: 0.000176, mae: 0.012452, mean_q: -0.577160
  7575/100000: episode: 75, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.766, 10.143], loss: 0.000169, mae: 0.012135, mean_q: -0.568335
  7676/100000: episode: 76, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.225, 10.455], loss: 0.000183, mae: 0.012691, mean_q: -0.543692
  7777/100000: episode: 77, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.526, 10.346], loss: 0.000175, mae: 0.012129, mean_q: -0.543177
  7878/100000: episode: 78, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.096, 10.163], loss: 0.000201, mae: 0.013096, mean_q: -0.534490
  7979/100000: episode: 79, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.820, mean reward: 0.008 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.676, 10.259], loss: 0.000190, mae: 0.012886, mean_q: -0.495845
  8080/100000: episode: 80, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.261, 10.309], loss: 0.000198, mae: 0.013131, mean_q: -0.461636
  8181/100000: episode: 81, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.835, 10.366], loss: 0.000190, mae: 0.013344, mean_q: -0.475541
  8282/100000: episode: 82, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.633, 10.100], loss: 0.000173, mae: 0.012901, mean_q: -0.455122
  8383/100000: episode: 83, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.165, 10.179], loss: 0.000190, mae: 0.013608, mean_q: -0.463017
  8484/100000: episode: 84, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.369, 10.100], loss: 0.000215, mae: 0.013817, mean_q: -0.446367
  8585/100000: episode: 85, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.868, 10.163], loss: 0.000208, mae: 0.013695, mean_q: -0.437287
  8686/100000: episode: 86, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.142], loss: 0.000201, mae: 0.013708, mean_q: -0.409962
  8787/100000: episode: 87, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.100], loss: 0.000199, mae: 0.013559, mean_q: -0.401729
  8888/100000: episode: 88, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.408, 10.392], loss: 0.000277, mae: 0.016447, mean_q: -0.386269
  8989/100000: episode: 89, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.117, 10.100], loss: 0.000280, mae: 0.016751, mean_q: -0.372803
  9090/100000: episode: 90, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.786, 10.252], loss: 0.000198, mae: 0.013979, mean_q: -0.355662
  9191/100000: episode: 91, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.514, 10.201], loss: 0.000230, mae: 0.014994, mean_q: -0.346575
  9292/100000: episode: 92, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.645, 10.209], loss: 0.000233, mae: 0.015267, mean_q: -0.298859
  9393/100000: episode: 93, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.192, 10.100], loss: 0.000268, mae: 0.016278, mean_q: -0.315122
  9494/100000: episode: 94, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.991, 10.100], loss: 0.000213, mae: 0.014461, mean_q: -0.266877
  9595/100000: episode: 95, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.395, 10.111], loss: 0.000318, mae: 0.017892, mean_q: -0.258691
  9696/100000: episode: 96, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.878, 10.428], loss: 0.000217, mae: 0.014524, mean_q: -0.249860
  9797/100000: episode: 97, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.353, 10.100], loss: 0.000214, mae: 0.014230, mean_q: -0.248605
  9898/100000: episode: 98, duration: 0.615s, episode steps: 101, steps per second: 164, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.900, 10.408], loss: 0.000197, mae: 0.013478, mean_q: -0.229260
  9999/100000: episode: 99, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.942, mean reward: 0.009 [0.000, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.375, 10.339], loss: 0.000230, mae: 0.015076, mean_q: -0.203203
 10100/100000: episode: 100, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.749, 10.399], loss: 0.000218, mae: 0.014037, mean_q: -0.182527
 10201/100000: episode: 101, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.673, 10.176], loss: 0.000292, mae: 0.017009, mean_q: -0.195969
 10302/100000: episode: 102, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.167, 10.200], loss: 0.000227, mae: 0.015471, mean_q: -0.152753
 10403/100000: episode: 103, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.806, 10.128], loss: 0.000179, mae: 0.012978, mean_q: -0.151871
 10504/100000: episode: 104, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.007, 10.100], loss: 0.000172, mae: 0.012773, mean_q: -0.149592
 10605/100000: episode: 105, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.274, 10.100], loss: 0.000238, mae: 0.015471, mean_q: -0.131028
 10706/100000: episode: 106, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.946, 10.115], loss: 0.000427, mae: 0.020766, mean_q: -0.101737
 10807/100000: episode: 107, duration: 0.607s, episode steps: 101, steps per second: 167, episode reward: 0.886, mean reward: 0.009 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.121, 10.220], loss: 0.000171, mae: 0.012545, mean_q: -0.056992
 10908/100000: episode: 108, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.822, 10.292], loss: 0.000189, mae: 0.013824, mean_q: -0.039104
 11009/100000: episode: 109, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.003, 10.298], loss: 0.000208, mae: 0.013863, mean_q: -0.023040
 11110/100000: episode: 110, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.527, 10.100], loss: 0.000165, mae: 0.012532, mean_q: -0.046078
 11211/100000: episode: 111, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.300, 10.297], loss: 0.000208, mae: 0.013742, mean_q: -0.014068
 11312/100000: episode: 112, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.746, 10.196], loss: 0.000235, mae: 0.014954, mean_q: -0.000841
 11413/100000: episode: 113, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.626, 10.100], loss: 0.000164, mae: 0.012526, mean_q: 0.026083
 11514/100000: episode: 114, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.559, 10.157], loss: 0.000166, mae: 0.012562, mean_q: 0.049182
 11615/100000: episode: 115, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.150, 10.100], loss: 0.000172, mae: 0.013195, mean_q: 0.060871
 11716/100000: episode: 116, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.842, mean reward: 0.008 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.796, 10.100], loss: 0.000164, mae: 0.012559, mean_q: 0.080994
 11817/100000: episode: 117, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.261, 10.100], loss: 0.000168, mae: 0.013401, mean_q: 0.080581
 11918/100000: episode: 118, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.817, 10.100], loss: 0.000151, mae: 0.012286, mean_q: 0.133843
 12019/100000: episode: 119, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.161, 10.154], loss: 0.000144, mae: 0.012143, mean_q: 0.120910
 12120/100000: episode: 120, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.872, 10.413], loss: 0.000150, mae: 0.012719, mean_q: 0.166670
 12221/100000: episode: 121, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.589, 10.235], loss: 0.000136, mae: 0.011717, mean_q: 0.145701
 12322/100000: episode: 122, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.417, 10.304], loss: 0.000181, mae: 0.014586, mean_q: 0.192498
 12423/100000: episode: 123, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.195, 10.264], loss: 0.000144, mae: 0.012399, mean_q: 0.218108
 12524/100000: episode: 124, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.025, 10.100], loss: 0.000146, mae: 0.012638, mean_q: 0.216381
 12625/100000: episode: 125, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.211, 10.130], loss: 0.000121, mae: 0.011421, mean_q: 0.252914
 12726/100000: episode: 126, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.761, 10.100], loss: 0.000127, mae: 0.011273, mean_q: 0.312482
 12827/100000: episode: 127, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.198, 10.100], loss: 0.000154, mae: 0.013027, mean_q: 0.293383
 12928/100000: episode: 128, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.100], loss: 0.000192, mae: 0.015005, mean_q: 0.342097
 13029/100000: episode: 129, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.839, 10.134], loss: 0.000145, mae: 0.012772, mean_q: 0.335047
 13130/100000: episode: 130, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.451, 10.100], loss: 0.000156, mae: 0.012807, mean_q: 0.364711
 13231/100000: episode: 131, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.732, 10.100], loss: 0.000123, mae: 0.011617, mean_q: 0.383824
 13332/100000: episode: 132, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.807, 10.306], loss: 0.000101, mae: 0.010608, mean_q: 0.401720
 13433/100000: episode: 133, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.331, 10.226], loss: 0.000128, mae: 0.011544, mean_q: 0.400756
 13534/100000: episode: 134, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.745, 10.199], loss: 0.000153, mae: 0.012844, mean_q: 0.436948
 13635/100000: episode: 135, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.100], loss: 0.000108, mae: 0.010776, mean_q: 0.429191
 13736/100000: episode: 136, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.297, 10.126], loss: 0.000167, mae: 0.013817, mean_q: 0.472254
 13837/100000: episode: 137, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.100, 10.204], loss: 0.000141, mae: 0.012402, mean_q: 0.492524
 13938/100000: episode: 138, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.852, 10.100], loss: 0.000118, mae: 0.011290, mean_q: 0.519118
 14039/100000: episode: 139, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.565, 10.206], loss: 0.000112, mae: 0.010988, mean_q: 0.511868
 14140/100000: episode: 140, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.281, 10.100], loss: 0.000092, mae: 0.009629, mean_q: 0.523791
 14241/100000: episode: 141, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.429, 10.100], loss: 0.000085, mae: 0.009512, mean_q: 0.558921
 14342/100000: episode: 142, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.089, 10.302], loss: 0.000088, mae: 0.009555, mean_q: 0.584408
 14443/100000: episode: 143, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.476, 10.100], loss: 0.000091, mae: 0.009429, mean_q: 0.592474
 14544/100000: episode: 144, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.097, 10.202], loss: 0.000084, mae: 0.009363, mean_q: 0.601462
 14645/100000: episode: 145, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.367, 10.263], loss: 0.000121, mae: 0.011858, mean_q: 0.613020
 14746/100000: episode: 146, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.553, 10.302], loss: 0.000075, mae: 0.009069, mean_q: 0.630039
 14847/100000: episode: 147, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.100], loss: 0.000085, mae: 0.009486, mean_q: 0.648496
 14948/100000: episode: 148, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.944, 10.100], loss: 0.000074, mae: 0.008814, mean_q: 0.661767
 15049/100000: episode: 149, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.161, 10.226], loss: 0.000089, mae: 0.010019, mean_q: 0.668359
 15150/100000: episode: 150, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.945, 10.110], loss: 0.000117, mae: 0.011268, mean_q: 0.684016
 15251/100000: episode: 151, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.100], loss: 0.000067, mae: 0.008463, mean_q: 0.692582
 15352/100000: episode: 152, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.929, 10.100], loss: 0.000080, mae: 0.010037, mean_q: 0.700846
 15453/100000: episode: 153, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.780, 10.201], loss: 0.000084, mae: 0.009601, mean_q: 0.713266
 15554/100000: episode: 154, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.747, 10.263], loss: 0.000062, mae: 0.007852, mean_q: 0.723101
 15655/100000: episode: 155, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.945, 10.100], loss: 0.000058, mae: 0.007859, mean_q: 0.724544
 15756/100000: episode: 156, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.751, 10.100], loss: 0.000059, mae: 0.008032, mean_q: 0.732779
 15857/100000: episode: 157, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.044, 10.382], loss: 0.000050, mae: 0.007333, mean_q: 0.736495
 15958/100000: episode: 158, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.514, 10.100], loss: 0.000049, mae: 0.007265, mean_q: 0.744227
 16059/100000: episode: 159, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.461, 10.100], loss: 0.000043, mae: 0.006742, mean_q: 0.747578
 16160/100000: episode: 160, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.571, 10.140], loss: 0.000053, mae: 0.007573, mean_q: 0.750894
 16261/100000: episode: 161, duration: 0.619s, episode steps: 101, steps per second: 163, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.100], loss: 0.000055, mae: 0.007678, mean_q: 0.752259
 16362/100000: episode: 162, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.691, 10.178], loss: 0.000055, mae: 0.007739, mean_q: 0.756697
 16463/100000: episode: 163, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.356, 10.413], loss: 0.000071, mae: 0.008604, mean_q: 0.756946
 16564/100000: episode: 164, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.442, 10.100], loss: 0.000054, mae: 0.007672, mean_q: 0.760111
 16665/100000: episode: 165, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.572, 10.304], loss: 0.000050, mae: 0.007305, mean_q: 0.760764
 16766/100000: episode: 166, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.060, 10.100], loss: 0.000048, mae: 0.006820, mean_q: 0.761330
 16867/100000: episode: 167, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.148, 10.232], loss: 0.000055, mae: 0.007899, mean_q: 0.762435
 16968/100000: episode: 168, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.349, 10.379], loss: 0.000072, mae: 0.008982, mean_q: 0.762843
 17069/100000: episode: 169, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.368, 10.100], loss: 0.000082, mae: 0.009555, mean_q: 0.762508
 17170/100000: episode: 170, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.047, 10.100], loss: 0.000052, mae: 0.007662, mean_q: 0.763327
 17271/100000: episode: 171, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.127, 10.370], loss: 0.000052, mae: 0.007347, mean_q: 0.763533
 17372/100000: episode: 172, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.988, 10.182], loss: 0.000068, mae: 0.008584, mean_q: 0.763551
 17473/100000: episode: 173, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.491, 10.167], loss: 0.000055, mae: 0.007476, mean_q: 0.763656
 17574/100000: episode: 174, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.267, 10.100], loss: 0.000044, mae: 0.007100, mean_q: 0.763087
 17675/100000: episode: 175, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.100], loss: 0.000041, mae: 0.006634, mean_q: 0.763551
 17776/100000: episode: 176, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.982, 10.100], loss: 0.000034, mae: 0.006115, mean_q: 0.763362
 17877/100000: episode: 177, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.558, 10.100], loss: 0.000043, mae: 0.006650, mean_q: 0.762004
 17978/100000: episode: 178, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.658, mean reward: 0.007 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.393, 10.100], loss: 0.000032, mae: 0.005660, mean_q: 0.763309
 18079/100000: episode: 179, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.440, 10.257], loss: 0.000047, mae: 0.007394, mean_q: 0.762751
 18180/100000: episode: 180, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.659, 10.230], loss: 0.000036, mae: 0.006125, mean_q: 0.761569
 18281/100000: episode: 181, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.962, 10.366], loss: 0.000050, mae: 0.007526, mean_q: 0.761725
 18382/100000: episode: 182, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.902, 10.100], loss: 0.000036, mae: 0.006138, mean_q: 0.761198
 18483/100000: episode: 183, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.882, mean reward: 0.009 [0.000, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.585, 10.181], loss: 0.000062, mae: 0.008321, mean_q: 0.760386
 18584/100000: episode: 184, duration: 0.634s, episode steps: 101, steps per second: 159, episode reward: 0.863, mean reward: 0.009 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.592, 10.100], loss: 0.000060, mae: 0.007888, mean_q: 0.759541
 18685/100000: episode: 185, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.114, 10.100], loss: 0.000052, mae: 0.007034, mean_q: 0.759374
 18786/100000: episode: 186, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.969, 10.100], loss: 0.000046, mae: 0.006992, mean_q: 0.758772
 18887/100000: episode: 187, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.848, mean reward: 0.008 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.594, 10.225], loss: 0.000039, mae: 0.005790, mean_q: 0.757951
 18988/100000: episode: 188, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.874, 10.242], loss: 0.000052, mae: 0.007646, mean_q: 0.757372
 19089/100000: episode: 189, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.100], loss: 0.000046, mae: 0.006719, mean_q: 0.757118
 19190/100000: episode: 190, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.886, 10.501], loss: 0.000041, mae: 0.006791, mean_q: 0.757232
 19291/100000: episode: 191, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.906, 10.124], loss: 0.000035, mae: 0.005816, mean_q: 0.755104
 19392/100000: episode: 192, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.677, mean reward: 0.007 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.200], loss: 0.000104, mae: 0.009968, mean_q: 0.755048
 19493/100000: episode: 193, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.648, 10.183], loss: 0.000039, mae: 0.005972, mean_q: 0.754884
 19594/100000: episode: 194, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.281, 10.346], loss: 0.000045, mae: 0.006538, mean_q: 0.753996
 19695/100000: episode: 195, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.253], loss: 0.000045, mae: 0.006667, mean_q: 0.753283
 19796/100000: episode: 196, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.880, 10.100], loss: 0.000040, mae: 0.006583, mean_q: 0.752686
 19897/100000: episode: 197, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.100], loss: 0.000047, mae: 0.006957, mean_q: 0.752252
 19998/100000: episode: 198, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.466, 10.100], loss: 0.000038, mae: 0.005908, mean_q: 0.751490
 20099/100000: episode: 199, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.499, 10.100], loss: 0.000046, mae: 0.006346, mean_q: 0.751807
[Info] 1-TH LEVEL FOUND: 0.8224295377731323, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.610s, episode steps: 101, steps per second: 18, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.758, 10.100], loss: 0.000082, mae: 0.008762, mean_q: 0.750880
 20212/100000: episode: 201, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 0.678, mean reward: 0.057 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.327], loss: 0.000202, mae: 0.016235, mean_q: 0.753419
 20213/100000: episode: 202, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.574, mean reward: 0.574 [0.574, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.243], loss: 0.000124, mae: 0.014417, mean_q: 0.738055
 20214/100000: episode: 203, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.593, mean reward: 0.593 [0.593, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.267], loss: 0.000229, mae: 0.019146, mean_q: 0.763494
 20227/100000: episode: 204, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 0.757, mean reward: 0.058 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.233, 10.310], loss: 0.000081, mae: 0.009739, mean_q: 0.748896
 20284/100000: episode: 205, duration: 0.329s, episode steps: 57, steps per second: 173, episode reward: 0.710, mean reward: 0.012 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.826, 10.100], loss: 0.000053, mae: 0.007906, mean_q: 0.751091
 20287/100000: episode: 206, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 0.696, mean reward: 0.232 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.435], loss: 0.000041, mae: 0.005993, mean_q: 0.753884
 20300/100000: episode: 207, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 0.782, mean reward: 0.060 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.689, 10.463], loss: 0.000077, mae: 0.006895, mean_q: 0.752734
 20313/100000: episode: 208, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 0.809, mean reward: 0.062 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.465, 10.415], loss: 0.000028, mae: 0.005745, mean_q: 0.750399
 20322/100000: episode: 209, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 0.736, mean reward: 0.082 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.300], loss: 0.000062, mae: 0.007493, mean_q: 0.751904
 20415/100000: episode: 210, duration: 0.522s, episode steps: 93, steps per second: 178, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.567, 10.238], loss: 0.000056, mae: 0.007220, mean_q: 0.751732
 20473/100000: episode: 211, duration: 0.359s, episode steps: 58, steps per second: 162, episode reward: 0.774, mean reward: 0.013 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.669, 10.100], loss: 0.000055, mae: 0.007553, mean_q: 0.750883
 20486/100000: episode: 212, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 0.821, mean reward: 0.063 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.523], loss: 0.000189, mae: 0.011110, mean_q: 0.751841
 20579/100000: episode: 213, duration: 0.531s, episode steps: 93, steps per second: 175, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-0.507, 10.106], loss: 0.000122, mae: 0.010633, mean_q: 0.750903
 20591/100000: episode: 214, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 0.744, mean reward: 0.062 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.439], loss: 0.000192, mae: 0.016298, mean_q: 0.750315
 20594/100000: episode: 215, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.694, mean reward: 0.231 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.381], loss: 0.000225, mae: 0.015841, mean_q: 0.754404
 20597/100000: episode: 216, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 0.681, mean reward: 0.227 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.412], loss: 0.000165, mae: 0.014568, mean_q: 0.763984
 20598/100000: episode: 217, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.163, 10.190], loss: 0.000411, mae: 0.026026, mean_q: 0.726529
 20611/100000: episode: 218, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 0.736, mean reward: 0.057 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.094, 10.393], loss: 0.000187, mae: 0.015360, mean_q: 0.753472
 20619/100000: episode: 219, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 0.650, mean reward: 0.081 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.207], loss: 0.000082, mae: 0.010413, mean_q: 0.747814
 20620/100000: episode: 220, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.604, mean reward: 0.604 [0.604, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.273], loss: 0.000158, mae: 0.015833, mean_q: 0.763138
 20633/100000: episode: 221, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 0.783, mean reward: 0.060 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.353], loss: 0.000106, mae: 0.009563, mean_q: 0.748531
 20690/100000: episode: 222, duration: 0.325s, episode steps: 57, steps per second: 176, episode reward: 0.937, mean reward: 0.016 [0.000, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.614, 10.100], loss: 0.000069, mae: 0.007399, mean_q: 0.751353
 20691/100000: episode: 223, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.599, mean reward: 0.599 [0.599, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.168], loss: 0.000024, mae: 0.005457, mean_q: 0.754060
 20704/100000: episode: 224, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 0.683, mean reward: 0.053 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.374], loss: 0.000015, mae: 0.004182, mean_q: 0.749915
 20705/100000: episode: 225, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.189], loss: 0.000014, mae: 0.003907, mean_q: 0.754733
 20798/100000: episode: 226, duration: 0.537s, episode steps: 93, steps per second: 173, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.525 [-0.825, 10.127], loss: 0.000066, mae: 0.006659, mean_q: 0.750065
 20810/100000: episode: 227, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 0.781, mean reward: 0.065 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.563, 10.441], loss: 0.000089, mae: 0.005735, mean_q: 0.751456
 20813/100000: episode: 228, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 0.688, mean reward: 0.229 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.410], loss: 0.000034, mae: 0.006210, mean_q: 0.749321
 20814/100000: episode: 229, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.681, mean reward: 0.681 [0.681, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.249], loss: 0.000025, mae: 0.005789, mean_q: 0.752979
 20823/100000: episode: 230, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 0.744, mean reward: 0.083 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.397], loss: 0.000022, mae: 0.005363, mean_q: 0.750573
 20826/100000: episode: 231, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 0.696, mean reward: 0.232 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.390], loss: 0.000024, mae: 0.005651, mean_q: 0.751373
 20834/100000: episode: 232, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 0.696, mean reward: 0.087 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.365, 10.328], loss: 0.000017, mae: 0.004509, mean_q: 0.749589
 20847/100000: episode: 233, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 0.830, mean reward: 0.064 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.095, 10.374], loss: 0.000062, mae: 0.006752, mean_q: 0.750106
 20860/100000: episode: 234, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 0.787, mean reward: 0.061 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.064, 10.467], loss: 0.000035, mae: 0.006431, mean_q: 0.752628
 20861/100000: episode: 235, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.619, mean reward: 0.619 [0.619, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.216], loss: 0.000208, mae: 0.016552, mean_q: 0.736015
 20864/100000: episode: 236, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 0.705, mean reward: 0.235 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.445], loss: 0.000365, mae: 0.013255, mean_q: 0.755962
 20957/100000: episode: 237, duration: 0.543s, episode steps: 93, steps per second: 171, episode reward: 0.820, mean reward: 0.009 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.597, 10.279], loss: 0.000127, mae: 0.008628, mean_q: 0.749319
 20970/100000: episode: 238, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 0.771, mean reward: 0.059 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.432], loss: 0.000206, mae: 0.012483, mean_q: 0.749033
 21063/100000: episode: 239, duration: 0.552s, episode steps: 93, steps per second: 168, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.918, 10.100], loss: 0.000056, mae: 0.006755, mean_q: 0.749692
 21076/100000: episode: 240, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 0.725, mean reward: 0.056 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.235], loss: 0.000083, mae: 0.007595, mean_q: 0.748798
 21134/100000: episode: 241, duration: 0.337s, episode steps: 58, steps per second: 172, episode reward: 0.796, mean reward: 0.014 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.317, 10.105], loss: 0.000046, mae: 0.006336, mean_q: 0.749190
 21191/100000: episode: 242, duration: 0.329s, episode steps: 57, steps per second: 173, episode reward: 0.801, mean reward: 0.014 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.489, 10.217], loss: 0.000067, mae: 0.007683, mean_q: 0.749835
 21200/100000: episode: 243, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 0.693, mean reward: 0.077 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.316], loss: 0.000118, mae: 0.008528, mean_q: 0.749810
 21293/100000: episode: 244, duration: 0.518s, episode steps: 93, steps per second: 179, episode reward: 0.728, mean reward: 0.008 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.555, 10.448], loss: 0.000073, mae: 0.007171, mean_q: 0.749151
 21386/100000: episode: 245, duration: 0.528s, episode steps: 93, steps per second: 176, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.728, 10.100], loss: 0.000168, mae: 0.011733, mean_q: 0.749042
 21395/100000: episode: 246, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 0.842, mean reward: 0.094 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.779, 10.612], loss: 0.000083, mae: 0.009150, mean_q: 0.751098
 21396/100000: episode: 247, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.218], loss: 0.000123, mae: 0.014717, mean_q: 0.735155
 21404/100000: episode: 248, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 0.667, mean reward: 0.083 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.251], loss: 0.000166, mae: 0.014387, mean_q: 0.753762
 21417/100000: episode: 249, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 0.812, mean reward: 0.062 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.329, 10.444], loss: 0.000078, mae: 0.007942, mean_q: 0.748261
 21430/100000: episode: 250, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 0.824, mean reward: 0.063 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.288, 10.399], loss: 0.000085, mae: 0.006296, mean_q: 0.749059
 21442/100000: episode: 251, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 0.737, mean reward: 0.061 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.462], loss: 0.000142, mae: 0.011687, mean_q: 0.745216
 21535/100000: episode: 252, duration: 0.522s, episode steps: 93, steps per second: 178, episode reward: 0.713, mean reward: 0.008 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-0.414, 10.231], loss: 0.000113, mae: 0.009301, mean_q: 0.748462
 21538/100000: episode: 253, duration: 0.029s, episode steps: 3, steps per second: 104, episode reward: 0.656, mean reward: 0.219 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.377], loss: 0.000110, mae: 0.007566, mean_q: 0.747750
 21595/100000: episode: 254, duration: 0.315s, episode steps: 57, steps per second: 181, episode reward: 0.738, mean reward: 0.013 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.826 [-1.312, 10.100], loss: 0.000099, mae: 0.007069, mean_q: 0.748512
 21608/100000: episode: 255, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 0.768, mean reward: 0.059 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.349], loss: 0.000081, mae: 0.007038, mean_q: 0.748434
 21609/100000: episode: 256, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.589, mean reward: 0.589 [0.589, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.211], loss: 0.000247, mae: 0.008944, mean_q: 0.753337
 21617/100000: episode: 257, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 0.684, mean reward: 0.086 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.408], loss: 0.000044, mae: 0.006366, mean_q: 0.747426
 21620/100000: episode: 258, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.690, mean reward: 0.230 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.419], loss: 0.000018, mae: 0.004878, mean_q: 0.739502
 21628/100000: episode: 259, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 0.722, mean reward: 0.090 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.762, 10.467], loss: 0.000201, mae: 0.009440, mean_q: 0.748627
 21640/100000: episode: 260, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 0.749, mean reward: 0.062 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.509], loss: 0.000390, mae: 0.018700, mean_q: 0.747651
 21733/100000: episode: 261, duration: 0.522s, episode steps: 93, steps per second: 178, episode reward: 0.733, mean reward: 0.008 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.662, 10.398], loss: 0.000083, mae: 0.007529, mean_q: 0.747480
 21790/100000: episode: 262, duration: 0.315s, episode steps: 57, steps per second: 181, episode reward: 0.750, mean reward: 0.013 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.839 [-1.074, 10.100], loss: 0.000136, mae: 0.008583, mean_q: 0.746919
 21802/100000: episode: 263, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 0.844, mean reward: 0.070 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.357], loss: 0.000083, mae: 0.007427, mean_q: 0.745778
 21859/100000: episode: 264, duration: 0.325s, episode steps: 57, steps per second: 175, episode reward: 0.796, mean reward: 0.014 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-1.449, 10.100], loss: 0.000104, mae: 0.008164, mean_q: 0.747235
 21862/100000: episode: 265, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 0.616, mean reward: 0.205 [0.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.398, 10.307], loss: 0.000129, mae: 0.010695, mean_q: 0.741305
 21874/100000: episode: 266, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 0.664, mean reward: 0.055 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.052, 10.279], loss: 0.000207, mae: 0.013564, mean_q: 0.748489
 21967/100000: episode: 267, duration: 0.529s, episode steps: 93, steps per second: 176, episode reward: 0.723, mean reward: 0.008 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.127, 10.134], loss: 0.000105, mae: 0.009813, mean_q: 0.746618
 21979/100000: episode: 268, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 0.789, mean reward: 0.066 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.152, 10.437], loss: 0.000063, mae: 0.005654, mean_q: 0.747330
 22037/100000: episode: 269, duration: 0.333s, episode steps: 58, steps per second: 174, episode reward: 0.695, mean reward: 0.012 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.649, 10.100], loss: 0.000085, mae: 0.007810, mean_q: 0.745246
 22046/100000: episode: 270, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 0.721, mean reward: 0.080 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.353, 10.357], loss: 0.000132, mae: 0.010860, mean_q: 0.747278
 22054/100000: episode: 271, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 0.621, mean reward: 0.078 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.227], loss: 0.000062, mae: 0.008163, mean_q: 0.747597
 22057/100000: episode: 272, duration: 0.031s, episode steps: 3, steps per second: 96, episode reward: 0.696, mean reward: 0.232 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.432], loss: 0.000029, mae: 0.006028, mean_q: 0.742333
 22066/100000: episode: 273, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 0.753, mean reward: 0.084 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.467], loss: 0.000067, mae: 0.006977, mean_q: 0.749031
 22079/100000: episode: 274, duration: 0.092s, episode steps: 13, steps per second: 142, episode reward: 0.768, mean reward: 0.059 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.430], loss: 0.000089, mae: 0.007071, mean_q: 0.744391
 22088/100000: episode: 275, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 0.661, mean reward: 0.073 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.241], loss: 0.000111, mae: 0.007589, mean_q: 0.748725
 22101/100000: episode: 276, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 0.729, mean reward: 0.056 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.372], loss: 0.000088, mae: 0.005825, mean_q: 0.743914
 22104/100000: episode: 277, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.728, mean reward: 0.243 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.464], loss: 0.000014, mae: 0.004076, mean_q: 0.748398
 22116/100000: episode: 278, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 0.713, mean reward: 0.059 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.279], loss: 0.000125, mae: 0.007750, mean_q: 0.746301
 22124/100000: episode: 279, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 0.693, mean reward: 0.087 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.330], loss: 0.000191, mae: 0.014194, mean_q: 0.743063
 22181/100000: episode: 280, duration: 0.338s, episode steps: 57, steps per second: 169, episode reward: 0.787, mean reward: 0.014 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.035, 10.100], loss: 0.000089, mae: 0.007718, mean_q: 0.745058
 22238/100000: episode: 281, duration: 0.334s, episode steps: 57, steps per second: 171, episode reward: 0.791, mean reward: 0.014 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-0.553, 10.100], loss: 0.000051, mae: 0.006063, mean_q: 0.744378
 22241/100000: episode: 282, duration: 0.023s, episode steps: 3, steps per second: 131, episode reward: 0.726, mean reward: 0.242 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.435], loss: 0.000042, mae: 0.004620, mean_q: 0.748746
 22298/100000: episode: 283, duration: 0.333s, episode steps: 57, steps per second: 171, episode reward: 0.808, mean reward: 0.014 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.958, 10.100], loss: 0.000102, mae: 0.006336, mean_q: 0.744190
 22311/100000: episode: 284, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 0.787, mean reward: 0.061 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.250, 10.537], loss: 0.000199, mae: 0.009856, mean_q: 0.741764
 22404/100000: episode: 285, duration: 0.521s, episode steps: 93, steps per second: 178, episode reward: 0.749, mean reward: 0.008 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-0.290, 10.100], loss: 0.000118, mae: 0.009484, mean_q: 0.743790
 22462/100000: episode: 286, duration: 0.343s, episode steps: 58, steps per second: 169, episode reward: 0.762, mean reward: 0.013 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-0.795, 10.155], loss: 0.000077, mae: 0.006953, mean_q: 0.743909
 22519/100000: episode: 287, duration: 0.307s, episode steps: 57, steps per second: 185, episode reward: 0.913, mean reward: 0.016 [0.000, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-1.274, 10.326], loss: 0.000067, mae: 0.007271, mean_q: 0.743798
 22522/100000: episode: 288, duration: 0.030s, episode steps: 3, steps per second: 99, episode reward: 0.715, mean reward: 0.238 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.461], loss: 0.000188, mae: 0.007198, mean_q: 0.743803
 22535/100000: episode: 289, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 0.787, mean reward: 0.061 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.412], loss: 0.000104, mae: 0.008507, mean_q: 0.741586
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.8224295377731323
1
 22547/100000: episode: 290, duration: 4.416s, episode steps: 12, steps per second: 3, episode reward: 0.857, mean reward: 0.071 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.128, 10.341], loss: 0.000083, mae: 0.006206, mean_q: 0.743736
 22648/100000: episode: 291, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.914, 10.100], loss: 0.000067, mae: 0.006759, mean_q: 0.742944
 22749/100000: episode: 292, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.124], loss: 0.000085, mae: 0.007079, mean_q: 0.741952
 22850/100000: episode: 293, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.475, 10.160], loss: 0.000095, mae: 0.007167, mean_q: 0.742656
 22951/100000: episode: 294, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.918, 10.468], loss: 0.000100, mae: 0.007147, mean_q: 0.742315
 23052/100000: episode: 295, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.677, 10.100], loss: 0.000078, mae: 0.006297, mean_q: 0.741961
 23153/100000: episode: 296, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.100], loss: 0.000074, mae: 0.007169, mean_q: 0.741775
 23254/100000: episode: 297, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.795, 10.100], loss: 0.000094, mae: 0.007643, mean_q: 0.741760
 23355/100000: episode: 298, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.180, 10.100], loss: 0.000093, mae: 0.007493, mean_q: 0.740491
 23456/100000: episode: 299, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.713, 10.101], loss: 0.000100, mae: 0.009157, mean_q: 0.741515
 23557/100000: episode: 300, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.816, 10.133], loss: 0.000109, mae: 0.008457, mean_q: 0.741041
 23658/100000: episode: 301, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.102, 10.135], loss: 0.000120, mae: 0.009635, mean_q: 0.740048
 23759/100000: episode: 302, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.135, 10.102], loss: 0.000107, mae: 0.009605, mean_q: 0.739983
 23860/100000: episode: 303, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.490, 10.100], loss: 0.000106, mae: 0.008876, mean_q: 0.739870
 23961/100000: episode: 304, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.177, 10.100], loss: 0.000086, mae: 0.006648, mean_q: 0.739551
 24062/100000: episode: 305, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.970, 10.100], loss: 0.000080, mae: 0.006576, mean_q: 0.739251
 24163/100000: episode: 306, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.954, 10.100], loss: 0.000080, mae: 0.006527, mean_q: 0.739380
 24264/100000: episode: 307, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.161, 10.116], loss: 0.000067, mae: 0.006303, mean_q: 0.739214
 24365/100000: episode: 308, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.888, 10.100], loss: 0.000144, mae: 0.011532, mean_q: 0.739056
 24466/100000: episode: 309, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.704, 10.100], loss: 0.000065, mae: 0.006663, mean_q: 0.739150
 24567/100000: episode: 310, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.454, 10.100], loss: 0.000049, mae: 0.005745, mean_q: 0.739783
 24668/100000: episode: 311, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.869, 10.100], loss: 0.000070, mae: 0.006362, mean_q: 0.738791
 24769/100000: episode: 312, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.862, 10.100], loss: 0.000087, mae: 0.007522, mean_q: 0.739048
 24870/100000: episode: 313, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.908, 10.100], loss: 0.000117, mae: 0.008689, mean_q: 0.738265
 24971/100000: episode: 314, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.533, 10.370], loss: 0.000054, mae: 0.005533, mean_q: 0.737863
 25072/100000: episode: 315, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.082, 10.100], loss: 0.000071, mae: 0.007320, mean_q: 0.737528
 25173/100000: episode: 316, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.905, 10.106], loss: 0.000071, mae: 0.006847, mean_q: 0.737536
 25274/100000: episode: 317, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.845, mean reward: 0.008 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.552, 10.100], loss: 0.000049, mae: 0.005638, mean_q: 0.737531
 25375/100000: episode: 318, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.957, 10.250], loss: 0.000093, mae: 0.007627, mean_q: 0.737268
 25476/100000: episode: 319, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.581, 10.141], loss: 0.000144, mae: 0.010148, mean_q: 0.736125
 25577/100000: episode: 320, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.452, 10.220], loss: 0.000068, mae: 0.007168, mean_q: 0.736334
 25678/100000: episode: 321, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.268, 10.218], loss: 0.000056, mae: 0.006589, mean_q: 0.736363
 25779/100000: episode: 322, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.115], loss: 0.000064, mae: 0.006743, mean_q: 0.736295
 25880/100000: episode: 323, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.162, 10.100], loss: 0.000071, mae: 0.006032, mean_q: 0.736300
 25981/100000: episode: 324, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.718, 10.274], loss: 0.000050, mae: 0.005107, mean_q: 0.736249
 26082/100000: episode: 325, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.100], loss: 0.000039, mae: 0.005129, mean_q: 0.736090
 26183/100000: episode: 326, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.076, 10.146], loss: 0.000040, mae: 0.005151, mean_q: 0.736148
 26284/100000: episode: 327, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.900, 10.216], loss: 0.000050, mae: 0.005545, mean_q: 0.736325
 26385/100000: episode: 328, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.128, 10.100], loss: 0.000063, mae: 0.006890, mean_q: 0.736043
 26486/100000: episode: 329, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.779, 10.100], loss: 0.000048, mae: 0.005443, mean_q: 0.736440
 26587/100000: episode: 330, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.698, 10.100], loss: 0.000041, mae: 0.005653, mean_q: 0.735428
 26688/100000: episode: 331, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.709, 10.100], loss: 0.000030, mae: 0.004818, mean_q: 0.735660
 26789/100000: episode: 332, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.597, 10.100], loss: 0.000033, mae: 0.005258, mean_q: 0.735860
 26890/100000: episode: 333, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.073, 10.217], loss: 0.000047, mae: 0.005616, mean_q: 0.736277
 26991/100000: episode: 334, duration: 0.560s, episode steps: 101, steps per second: 181, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.419, 10.257], loss: 0.000033, mae: 0.005221, mean_q: 0.735599
 27092/100000: episode: 335, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.651, 10.302], loss: 0.000033, mae: 0.004774, mean_q: 0.735803
 27193/100000: episode: 336, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.100], loss: 0.000024, mae: 0.004291, mean_q: 0.735761
 27294/100000: episode: 337, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.302, 10.100], loss: 0.000062, mae: 0.007185, mean_q: 0.735797
 27395/100000: episode: 338, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.657, 10.100], loss: 0.000036, mae: 0.004782, mean_q: 0.735548
 27496/100000: episode: 339, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.797, 10.100], loss: 0.000031, mae: 0.004088, mean_q: 0.736568
 27597/100000: episode: 340, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.579, 10.100], loss: 0.000019, mae: 0.003794, mean_q: 0.735553
 27698/100000: episode: 341, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.158, 10.100], loss: 0.000021, mae: 0.004187, mean_q: 0.735735
 27799/100000: episode: 342, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.710, 10.100], loss: 0.000029, mae: 0.004578, mean_q: 0.735538
 27900/100000: episode: 343, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.147, 10.253], loss: 0.000035, mae: 0.004775, mean_q: 0.735439
 28001/100000: episode: 344, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.471, 10.322], loss: 0.000025, mae: 0.004818, mean_q: 0.734908
 28102/100000: episode: 345, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.360, 10.178], loss: 0.000033, mae: 0.004764, mean_q: 0.736188
 28203/100000: episode: 346, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.126, 10.300], loss: 0.000022, mae: 0.004162, mean_q: 0.734870
 28304/100000: episode: 347, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.172, 10.300], loss: 0.000022, mae: 0.004308, mean_q: 0.736163
 28405/100000: episode: 348, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.144, 10.100], loss: 0.000024, mae: 0.004112, mean_q: 0.735829
 28506/100000: episode: 349, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.062, 10.100], loss: 0.000025, mae: 0.004492, mean_q: 0.735478
 28607/100000: episode: 350, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.999, 10.232], loss: 0.000021, mae: 0.003994, mean_q: 0.735664
 28708/100000: episode: 351, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.844, 10.356], loss: 0.000029, mae: 0.004776, mean_q: 0.735635
 28809/100000: episode: 352, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.656, 10.100], loss: 0.000022, mae: 0.004228, mean_q: 0.735507
 28910/100000: episode: 353, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.072, 10.118], loss: 0.000021, mae: 0.004023, mean_q: 0.735315
 29011/100000: episode: 354, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.092, 10.202], loss: 0.000019, mae: 0.003485, mean_q: 0.735491
 29112/100000: episode: 355, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.092, 10.199], loss: 0.000019, mae: 0.003836, mean_q: 0.735673
 29213/100000: episode: 356, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.887, 10.100], loss: 0.000017, mae: 0.003606, mean_q: 0.736027
 29314/100000: episode: 357, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.199, 10.100], loss: 0.000036, mae: 0.005028, mean_q: 0.736135
 29415/100000: episode: 358, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.317, 10.100], loss: 0.000034, mae: 0.004867, mean_q: 0.735963
 29516/100000: episode: 359, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.359, 10.100], loss: 0.000026, mae: 0.004257, mean_q: 0.736459
 29617/100000: episode: 360, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.750, 10.100], loss: 0.000026, mae: 0.004264, mean_q: 0.736667
 29718/100000: episode: 361, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.793, 10.161], loss: 0.000019, mae: 0.003588, mean_q: 0.735883
 29819/100000: episode: 362, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.556, 10.160], loss: 0.000017, mae: 0.003701, mean_q: 0.737110
 29920/100000: episode: 363, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.337, 10.210], loss: 0.000046, mae: 0.006329, mean_q: 0.737419
 30021/100000: episode: 364, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.422, 10.238], loss: 0.000016, mae: 0.003713, mean_q: 0.737509
 30122/100000: episode: 365, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.706, 10.100], loss: 0.000017, mae: 0.003633, mean_q: 0.737535
 30223/100000: episode: 366, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.418, 10.408], loss: 0.000014, mae: 0.003399, mean_q: 0.737585
 30324/100000: episode: 367, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.623, 10.103], loss: 0.000020, mae: 0.004127, mean_q: 0.737325
 30425/100000: episode: 368, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.414, 10.175], loss: 0.000018, mae: 0.004002, mean_q: 0.737227
 30526/100000: episode: 369, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.144, 10.173], loss: 0.000028, mae: 0.004925, mean_q: 0.737623
 30627/100000: episode: 370, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.267, 10.328], loss: 0.000016, mae: 0.004036, mean_q: 0.738022
 30728/100000: episode: 371, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.793, 10.197], loss: 0.000021, mae: 0.004439, mean_q: 0.738256
 30829/100000: episode: 372, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.842, mean reward: 0.008 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.504, 10.399], loss: 0.000013, mae: 0.002942, mean_q: 0.738436
 30930/100000: episode: 373, duration: 0.608s, episode steps: 101, steps per second: 166, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.790, 10.390], loss: 0.000023, mae: 0.004244, mean_q: 0.739030
 31031/100000: episode: 374, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.814, 10.273], loss: 0.000028, mae: 0.004997, mean_q: 0.739550
 31132/100000: episode: 375, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.570, 10.183], loss: 0.000014, mae: 0.002950, mean_q: 0.740036
 31233/100000: episode: 376, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.656, 10.442], loss: 0.000014, mae: 0.002908, mean_q: 0.740288
 31334/100000: episode: 377, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.217, 10.100], loss: 0.000018, mae: 0.003908, mean_q: 0.740185
 31435/100000: episode: 378, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.919, 10.330], loss: 0.000018, mae: 0.003539, mean_q: 0.740357
 31536/100000: episode: 379, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.756, 10.139], loss: 0.000015, mae: 0.003007, mean_q: 0.741170
 31637/100000: episode: 380, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.604, 10.173], loss: 0.000017, mae: 0.003528, mean_q: 0.741933
 31738/100000: episode: 381, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.477, 10.260], loss: 0.000022, mae: 0.004087, mean_q: 0.742132
 31839/100000: episode: 382, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.717, 10.184], loss: 0.000016, mae: 0.003831, mean_q: 0.742072
 31940/100000: episode: 383, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.251], loss: 0.000028, mae: 0.004647, mean_q: 0.742402
 32041/100000: episode: 384, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.576, 10.100], loss: 0.000013, mae: 0.003173, mean_q: 0.742373
 32142/100000: episode: 385, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.742, 10.437], loss: 0.000024, mae: 0.004187, mean_q: 0.742976
 32243/100000: episode: 386, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.305, 10.101], loss: 0.000014, mae: 0.003371, mean_q: 0.742736
 32344/100000: episode: 387, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.779, 10.123], loss: 0.000013, mae: 0.003032, mean_q: 0.743016
 32445/100000: episode: 388, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.469, 10.425], loss: 0.000016, mae: 0.003406, mean_q: 0.743688
 32546/100000: episode: 389, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.508, 10.303], loss: 0.000015, mae: 0.003315, mean_q: 0.743476
[Info] 1-TH LEVEL FOUND: 0.7655444741249084, Considering 10/100 traces
 32647/100000: episode: 390, duration: 5.140s, episode steps: 101, steps per second: 20, episode reward: 0.841, mean reward: 0.008 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.877, 10.194], loss: 0.000016, mae: 0.003805, mean_q: 0.743856
 32697/100000: episode: 391, duration: 0.286s, episode steps: 50, steps per second: 175, episode reward: 0.812, mean reward: 0.016 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.542, 10.133], loss: 0.000011, mae: 0.002772, mean_q: 0.744811
 32750/100000: episode: 392, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 0.711, mean reward: 0.013 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.109, 10.100], loss: 0.000013, mae: 0.003224, mean_q: 0.744872
 32803/100000: episode: 393, duration: 0.327s, episode steps: 53, steps per second: 162, episode reward: 0.680, mean reward: 0.013 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.295, 10.101], loss: 0.000032, mae: 0.004814, mean_q: 0.744618
 32854/100000: episode: 394, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.721, 10.107], loss: 0.000025, mae: 0.004195, mean_q: 0.745362
 32907/100000: episode: 395, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 0.727, mean reward: 0.014 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.601, 10.100], loss: 0.000035, mae: 0.005535, mean_q: 0.746462
 32956/100000: episode: 396, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 0.677, mean reward: 0.014 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.367, 10.137], loss: 0.000038, mae: 0.005104, mean_q: 0.746722
 33006/100000: episode: 397, duration: 0.291s, episode steps: 50, steps per second: 172, episode reward: 0.662, mean reward: 0.013 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.035, 10.233], loss: 0.000026, mae: 0.004049, mean_q: 0.745750
 33059/100000: episode: 398, duration: 0.297s, episode steps: 53, steps per second: 178, episode reward: 0.743, mean reward: 0.014 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.361, 10.100], loss: 0.000023, mae: 0.003801, mean_q: 0.746580
 33109/100000: episode: 399, duration: 0.288s, episode steps: 50, steps per second: 174, episode reward: 0.817, mean reward: 0.016 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.000, 10.100], loss: 0.000037, mae: 0.005485, mean_q: 0.746854
 33165/100000: episode: 400, duration: 0.307s, episode steps: 56, steps per second: 183, episode reward: 0.743, mean reward: 0.013 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.763, 10.215], loss: 0.000027, mae: 0.004267, mean_q: 0.746164
 33215/100000: episode: 401, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 0.730, mean reward: 0.015 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.756, 10.100], loss: 0.000022, mae: 0.003291, mean_q: 0.746597
 33268/100000: episode: 402, duration: 0.319s, episode steps: 53, steps per second: 166, episode reward: 0.685, mean reward: 0.013 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.577, 10.393], loss: 0.000042, mae: 0.005706, mean_q: 0.746379
 33318/100000: episode: 403, duration: 0.287s, episode steps: 50, steps per second: 174, episode reward: 0.702, mean reward: 0.014 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.205, 10.100], loss: 0.000024, mae: 0.004345, mean_q: 0.746583
 33369/100000: episode: 404, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 0.794, mean reward: 0.016 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.470, 10.100], loss: 0.000011, mae: 0.003027, mean_q: 0.747878
 33419/100000: episode: 405, duration: 0.307s, episode steps: 50, steps per second: 163, episode reward: 0.757, mean reward: 0.015 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.414, 10.230], loss: 0.000011, mae: 0.002965, mean_q: 0.747279
 33476/100000: episode: 406, duration: 0.339s, episode steps: 57, steps per second: 168, episode reward: 0.710, mean reward: 0.012 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.664, 10.189], loss: 0.000020, mae: 0.003762, mean_q: 0.747575
 33529/100000: episode: 407, duration: 0.279s, episode steps: 53, steps per second: 190, episode reward: 0.744, mean reward: 0.014 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.384, 10.238], loss: 0.000015, mae: 0.003276, mean_q: 0.747629
 33579/100000: episode: 408, duration: 0.311s, episode steps: 50, steps per second: 161, episode reward: 0.781, mean reward: 0.016 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.616, 10.100], loss: 0.000033, mae: 0.005246, mean_q: 0.748369
 33628/100000: episode: 409, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 0.688, mean reward: 0.014 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.242, 10.100], loss: 0.000025, mae: 0.003819, mean_q: 0.748797
 33678/100000: episode: 410, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 0.735, mean reward: 0.015 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.417, 10.100], loss: 0.000025, mae: 0.004953, mean_q: 0.748360
 33734/100000: episode: 411, duration: 0.302s, episode steps: 56, steps per second: 185, episode reward: 0.719, mean reward: 0.013 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.850, 10.100], loss: 0.000015, mae: 0.003550, mean_q: 0.748070
 33784/100000: episode: 412, duration: 0.272s, episode steps: 50, steps per second: 184, episode reward: 0.702, mean reward: 0.014 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.504, 10.100], loss: 0.000010, mae: 0.003039, mean_q: 0.748290
 33834/100000: episode: 413, duration: 0.302s, episode steps: 50, steps per second: 165, episode reward: 0.719, mean reward: 0.014 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.009, 10.281], loss: 0.000014, mae: 0.002938, mean_q: 0.748383
 33884/100000: episode: 414, duration: 0.277s, episode steps: 50, steps per second: 180, episode reward: 0.760, mean reward: 0.015 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.596, 10.100], loss: 0.000033, mae: 0.005264, mean_q: 0.748080
 33933/100000: episode: 415, duration: 0.298s, episode steps: 49, steps per second: 164, episode reward: 0.685, mean reward: 0.014 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.417, 10.100], loss: 0.000010, mae: 0.003132, mean_q: 0.748048
 33990/100000: episode: 416, duration: 0.334s, episode steps: 57, steps per second: 170, episode reward: 0.720, mean reward: 0.013 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-1.226, 10.100], loss: 0.000016, mae: 0.003013, mean_q: 0.748149
 34041/100000: episode: 417, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 0.696, mean reward: 0.014 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.055, 10.239], loss: 0.000020, mae: 0.003105, mean_q: 0.748200
 34098/100000: episode: 418, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 0.784, mean reward: 0.014 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-1.238, 10.357], loss: 0.000041, mae: 0.005665, mean_q: 0.748272
 34148/100000: episode: 419, duration: 0.298s, episode steps: 50, steps per second: 168, episode reward: 0.689, mean reward: 0.014 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.564, 10.100], loss: 0.000015, mae: 0.002925, mean_q: 0.748637
 34204/100000: episode: 420, duration: 0.320s, episode steps: 56, steps per second: 175, episode reward: 0.684, mean reward: 0.012 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.704, 10.100], loss: 0.000029, mae: 0.004109, mean_q: 0.748244
 34255/100000: episode: 421, duration: 0.315s, episode steps: 51, steps per second: 162, episode reward: 0.767, mean reward: 0.015 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.006, 10.100], loss: 0.000022, mae: 0.003421, mean_q: 0.748558
 34306/100000: episode: 422, duration: 0.289s, episode steps: 51, steps per second: 176, episode reward: 0.847, mean reward: 0.017 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.801, 10.100], loss: 0.000022, mae: 0.003369, mean_q: 0.748804
 34356/100000: episode: 423, duration: 0.326s, episode steps: 50, steps per second: 153, episode reward: 0.722, mean reward: 0.014 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.944, 10.100], loss: 0.000025, mae: 0.004262, mean_q: 0.748336
 34412/100000: episode: 424, duration: 0.330s, episode steps: 56, steps per second: 170, episode reward: 0.782, mean reward: 0.014 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.886, 10.100], loss: 0.000024, mae: 0.003598, mean_q: 0.748296
 34469/100000: episode: 425, duration: 0.321s, episode steps: 57, steps per second: 177, episode reward: 0.770, mean reward: 0.014 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.914, 10.100], loss: 0.000027, mae: 0.004126, mean_q: 0.748878
 34520/100000: episode: 426, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.826, mean reward: 0.016 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.090, 10.191], loss: 0.000043, mae: 0.005910, mean_q: 0.748762
 34573/100000: episode: 427, duration: 0.323s, episode steps: 53, steps per second: 164, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.125, 10.100], loss: 0.000027, mae: 0.004118, mean_q: 0.749085
 34630/100000: episode: 428, duration: 0.333s, episode steps: 57, steps per second: 171, episode reward: 0.775, mean reward: 0.014 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.637, 10.100], loss: 0.000033, mae: 0.004816, mean_q: 0.748851
 34680/100000: episode: 429, duration: 0.305s, episode steps: 50, steps per second: 164, episode reward: 0.680, mean reward: 0.014 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.061, 10.100], loss: 0.000029, mae: 0.004370, mean_q: 0.748626
 34729/100000: episode: 430, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 0.920, mean reward: 0.019 [0.000, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.221, 10.162], loss: 0.000034, mae: 0.004757, mean_q: 0.748641
 34780/100000: episode: 431, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.790, mean reward: 0.015 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.188, 10.100], loss: 0.000028, mae: 0.004437, mean_q: 0.749095
 34833/100000: episode: 432, duration: 0.289s, episode steps: 53, steps per second: 184, episode reward: 0.729, mean reward: 0.014 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.520, 10.100], loss: 0.000025, mae: 0.003655, mean_q: 0.748272
 34882/100000: episode: 433, duration: 0.293s, episode steps: 49, steps per second: 168, episode reward: 0.648, mean reward: 0.013 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.514, 10.243], loss: 0.000062, mae: 0.006871, mean_q: 0.749305
 34932/100000: episode: 434, duration: 0.293s, episode steps: 50, steps per second: 171, episode reward: 0.742, mean reward: 0.015 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-2.255, 10.100], loss: 0.000038, mae: 0.005313, mean_q: 0.749061
 34983/100000: episode: 435, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 0.800, mean reward: 0.016 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.362, 10.100], loss: 0.000018, mae: 0.003635, mean_q: 0.748731
 35040/100000: episode: 436, duration: 0.311s, episode steps: 57, steps per second: 183, episode reward: 0.723, mean reward: 0.013 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-1.014, 10.226], loss: 0.000015, mae: 0.002777, mean_q: 0.748667
 35091/100000: episode: 437, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 0.682, mean reward: 0.013 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.215, 10.100], loss: 0.000022, mae: 0.003887, mean_q: 0.748286
 35141/100000: episode: 438, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 0.780, mean reward: 0.016 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.365, 10.155], loss: 0.000029, mae: 0.004736, mean_q: 0.748865
 35191/100000: episode: 439, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 0.773, mean reward: 0.015 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.423, 10.130], loss: 0.000032, mae: 0.004006, mean_q: 0.748714
 35247/100000: episode: 440, duration: 0.317s, episode steps: 56, steps per second: 177, episode reward: 0.802, mean reward: 0.014 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.497, 10.100], loss: 0.000023, mae: 0.003855, mean_q: 0.748432
 35298/100000: episode: 441, duration: 0.286s, episode steps: 51, steps per second: 179, episode reward: 0.800, mean reward: 0.016 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.361, 10.100], loss: 0.000024, mae: 0.004083, mean_q: 0.747942
 35349/100000: episode: 442, duration: 0.281s, episode steps: 51, steps per second: 181, episode reward: 0.769, mean reward: 0.015 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.586, 10.100], loss: 0.000037, mae: 0.004813, mean_q: 0.748099
 35399/100000: episode: 443, duration: 0.280s, episode steps: 50, steps per second: 178, episode reward: 0.667, mean reward: 0.013 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.101, 10.100], loss: 0.000042, mae: 0.004549, mean_q: 0.748343
 35449/100000: episode: 444, duration: 0.287s, episode steps: 50, steps per second: 174, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.782, 10.127], loss: 0.000053, mae: 0.006408, mean_q: 0.748067
 35499/100000: episode: 445, duration: 0.306s, episode steps: 50, steps per second: 163, episode reward: 0.674, mean reward: 0.013 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.848, 10.100], loss: 0.000024, mae: 0.003616, mean_q: 0.747266
 35548/100000: episode: 446, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 0.772, mean reward: 0.016 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.472, 10.122], loss: 0.000026, mae: 0.004334, mean_q: 0.747559
 35601/100000: episode: 447, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 0.673, mean reward: 0.013 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.859, 10.194], loss: 0.000026, mae: 0.004012, mean_q: 0.747923
 35651/100000: episode: 448, duration: 0.278s, episode steps: 50, steps per second: 180, episode reward: 0.799, mean reward: 0.016 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.383, 10.100], loss: 0.000031, mae: 0.004280, mean_q: 0.747139
 35704/100000: episode: 449, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 0.830, mean reward: 0.016 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.626, 10.162], loss: 0.000048, mae: 0.005503, mean_q: 0.747513
 35757/100000: episode: 450, duration: 0.305s, episode steps: 53, steps per second: 174, episode reward: 0.799, mean reward: 0.015 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.183, 10.425], loss: 0.000045, mae: 0.005098, mean_q: 0.747445
 35807/100000: episode: 451, duration: 0.316s, episode steps: 50, steps per second: 158, episode reward: 0.711, mean reward: 0.014 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.372, 10.100], loss: 0.000036, mae: 0.004825, mean_q: 0.747538
 35857/100000: episode: 452, duration: 0.287s, episode steps: 50, steps per second: 174, episode reward: 0.718, mean reward: 0.014 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.656, 10.100], loss: 0.000031, mae: 0.003320, mean_q: 0.747056
 35914/100000: episode: 453, duration: 0.363s, episode steps: 57, steps per second: 157, episode reward: 0.807, mean reward: 0.014 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.316, 10.100], loss: 0.000033, mae: 0.004991, mean_q: 0.747142
 35965/100000: episode: 454, duration: 0.302s, episode steps: 51, steps per second: 169, episode reward: 0.893, mean reward: 0.018 [0.000, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.122, 10.100], loss: 0.000026, mae: 0.004162, mean_q: 0.746378
 36018/100000: episode: 455, duration: 0.311s, episode steps: 53, steps per second: 171, episode reward: 0.729, mean reward: 0.014 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.515, 10.100], loss: 0.000030, mae: 0.003326, mean_q: 0.746891
 36071/100000: episode: 456, duration: 0.296s, episode steps: 53, steps per second: 179, episode reward: 0.848, mean reward: 0.016 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.543, 10.100], loss: 0.000026, mae: 0.004109, mean_q: 0.746210
 36124/100000: episode: 457, duration: 0.324s, episode steps: 53, steps per second: 164, episode reward: 0.739, mean reward: 0.014 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.218, 10.100], loss: 0.000052, mae: 0.005980, mean_q: 0.745911
 36174/100000: episode: 458, duration: 0.303s, episode steps: 50, steps per second: 165, episode reward: 0.744, mean reward: 0.015 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.528, 10.100], loss: 0.000046, mae: 0.004757, mean_q: 0.746385
 36225/100000: episode: 459, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.320, 10.197], loss: 0.000048, mae: 0.005438, mean_q: 0.745868
 36275/100000: episode: 460, duration: 0.288s, episode steps: 50, steps per second: 173, episode reward: 0.760, mean reward: 0.015 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.257, 10.202], loss: 0.000042, mae: 0.005041, mean_q: 0.745994
 36325/100000: episode: 461, duration: 0.292s, episode steps: 50, steps per second: 172, episode reward: 0.725, mean reward: 0.015 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.035, 10.100], loss: 0.000036, mae: 0.004620, mean_q: 0.745658
 36378/100000: episode: 462, duration: 0.330s, episode steps: 53, steps per second: 160, episode reward: 0.764, mean reward: 0.014 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.437, 10.100], loss: 0.000051, mae: 0.005520, mean_q: 0.746071
 36431/100000: episode: 463, duration: 0.286s, episode steps: 53, steps per second: 185, episode reward: 0.689, mean reward: 0.013 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.934, 10.256], loss: 0.000027, mae: 0.003964, mean_q: 0.745343
 36480/100000: episode: 464, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 0.788, mean reward: 0.016 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.497, 10.250], loss: 0.000042, mae: 0.004560, mean_q: 0.745345
 36530/100000: episode: 465, duration: 0.286s, episode steps: 50, steps per second: 175, episode reward: 0.767, mean reward: 0.015 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.893, 10.143], loss: 0.000039, mae: 0.004709, mean_q: 0.745461
 36583/100000: episode: 466, duration: 0.309s, episode steps: 53, steps per second: 171, episode reward: 0.686, mean reward: 0.013 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.858, 10.172], loss: 0.000050, mae: 0.005088, mean_q: 0.745388
 36633/100000: episode: 467, duration: 0.311s, episode steps: 50, steps per second: 161, episode reward: 0.741, mean reward: 0.015 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.536, 10.100], loss: 0.000034, mae: 0.004190, mean_q: 0.744736
 36683/100000: episode: 468, duration: 0.274s, episode steps: 50, steps per second: 182, episode reward: 0.759, mean reward: 0.015 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.718, 10.110], loss: 0.000038, mae: 0.005064, mean_q: 0.744949
 36739/100000: episode: 469, duration: 0.323s, episode steps: 56, steps per second: 173, episode reward: 0.740, mean reward: 0.013 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.641, 10.100], loss: 0.000040, mae: 0.005543, mean_q: 0.745271
 36788/100000: episode: 470, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 0.829, mean reward: 0.017 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.630, 10.100], loss: 0.000053, mae: 0.004328, mean_q: 0.744872
 36841/100000: episode: 471, duration: 0.291s, episode steps: 53, steps per second: 182, episode reward: 0.752, mean reward: 0.014 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.524, 10.100], loss: 0.000044, mae: 0.005131, mean_q: 0.745057
 36890/100000: episode: 472, duration: 0.291s, episode steps: 49, steps per second: 168, episode reward: 0.757, mean reward: 0.015 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.939, 10.100], loss: 0.000055, mae: 0.005107, mean_q: 0.744523
 36943/100000: episode: 473, duration: 0.310s, episode steps: 53, steps per second: 171, episode reward: 0.704, mean reward: 0.013 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.338, 10.100], loss: 0.000048, mae: 0.005962, mean_q: 0.744392
 36993/100000: episode: 474, duration: 0.302s, episode steps: 50, steps per second: 166, episode reward: 0.758, mean reward: 0.015 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.675, 10.100], loss: 0.000031, mae: 0.004038, mean_q: 0.744313
 37050/100000: episode: 475, duration: 0.340s, episode steps: 57, steps per second: 168, episode reward: 0.763, mean reward: 0.013 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.592, 10.100], loss: 0.000046, mae: 0.004481, mean_q: 0.743962
 37100/100000: episode: 476, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 0.795, mean reward: 0.016 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.625, 10.100], loss: 0.000022, mae: 0.003134, mean_q: 0.744133
 37151/100000: episode: 477, duration: 0.304s, episode steps: 51, steps per second: 168, episode reward: 0.794, mean reward: 0.016 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.527, 10.111], loss: 0.000028, mae: 0.004189, mean_q: 0.744441
 37202/100000: episode: 478, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.782, mean reward: 0.015 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.494, 10.247], loss: 0.000027, mae: 0.004812, mean_q: 0.744074
 37255/100000: episode: 479, duration: 0.309s, episode steps: 53, steps per second: 172, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.337, 10.100], loss: 0.000049, mae: 0.004239, mean_q: 0.744521
[Info] 2-TH LEVEL FOUND: 0.7705414295196533, Considering 10/100 traces
 37305/100000: episode: 480, duration: 4.802s, episode steps: 50, steps per second: 10, episode reward: 0.772, mean reward: 0.015 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.675, 10.100], loss: 0.000026, mae: 0.004014, mean_q: 0.744164
 37311/100000: episode: 481, duration: 0.048s, episode steps: 6, steps per second: 124, episode reward: 0.650, mean reward: 0.108 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.319], loss: 0.000048, mae: 0.006787, mean_q: 0.741062
 37312/100000: episode: 482, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.706, mean reward: 0.706 [0.706, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.356, 10.200], loss: 0.000004, mae: 0.002201, mean_q: 0.743394
 37313/100000: episode: 483, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.369, 10.200], loss: 0.000065, mae: 0.008711, mean_q: 0.734766
 37314/100000: episode: 484, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.656, mean reward: 0.656 [0.656, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.389, 10.100], loss: 0.000051, mae: 0.004087, mean_q: 0.746060
 37320/100000: episode: 485, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 0.641, mean reward: 0.107 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.167], loss: 0.000031, mae: 0.004210, mean_q: 0.746031
 37321/100000: episode: 486, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.688, mean reward: 0.688 [0.688, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.384, 10.100], loss: 0.000026, mae: 0.006668, mean_q: 0.738631
 37322/100000: episode: 487, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.379, 10.100], loss: 0.000111, mae: 0.005670, mean_q: 0.746608
 37324/100000: episode: 488, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.739, mean reward: 0.370 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.353, 10.100], loss: 0.000002, mae: 0.001821, mean_q: 0.743890
 37325/100000: episode: 489, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.364, 10.100], loss: 0.000002, mae: 0.001683, mean_q: 0.741546
 37326/100000: episode: 490, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.700, mean reward: 0.700 [0.700, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.331, 10.200], loss: 0.000085, mae: 0.005898, mean_q: 0.740385
 37327/100000: episode: 491, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.284, 10.200], loss: 0.000015, mae: 0.003756, mean_q: 0.741173
 37328/100000: episode: 492, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.665, mean reward: 0.665 [0.665, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.382, 10.100], loss: 0.000012, mae: 0.004342, mean_q: 0.747388
 37329/100000: episode: 493, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.655, mean reward: 0.655 [0.655, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.256, 10.200], loss: 0.000003, mae: 0.001889, mean_q: 0.742827
 37330/100000: episode: 494, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.712, mean reward: 0.712 [0.712, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.342, 10.100], loss: 0.000013, mae: 0.003548, mean_q: 0.739001
 37331/100000: episode: 495, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.719, mean reward: 0.719 [0.719, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.333, 10.100], loss: 0.000047, mae: 0.009232, mean_q: 0.752773
 37332/100000: episode: 496, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.715, mean reward: 0.715 [0.715, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.291, 10.200], loss: 0.000014, mae: 0.003829, mean_q: 0.739941
 37333/100000: episode: 497, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.701, mean reward: 0.701 [0.701, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.351, 10.100], loss: 0.000029, mae: 0.007353, mean_q: 0.737224
 37334/100000: episode: 498, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.777, mean reward: 0.777 [0.777, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.366, 10.100], loss: 0.000042, mae: 0.008235, mean_q: 0.751915
 37335/100000: episode: 499, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.343, 10.100], loss: 0.000054, mae: 0.005683, mean_q: 0.741192
 37337/100000: episode: 500, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.736, mean reward: 0.368 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.590, 10.100], loss: 0.000178, mae: 0.010507, mean_q: 0.740997
 37338/100000: episode: 501, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.661, mean reward: 0.661 [0.661, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.298, 10.100], loss: 0.000026, mae: 0.006294, mean_q: 0.752681
 37339/100000: episode: 502, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.715, mean reward: 0.715 [0.715, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.325, 10.100], loss: 0.000007, mae: 0.002913, mean_q: 0.740248
 37340/100000: episode: 503, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.342, 10.200], loss: 0.000004, mae: 0.002113, mean_q: 0.744768
 37341/100000: episode: 504, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.706, mean reward: 0.706 [0.706, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.253, 10.200], loss: 0.000008, mae: 0.003366, mean_q: 0.747530
 37343/100000: episode: 505, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.768, mean reward: 0.384 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.409, 10.100], loss: 0.000015, mae: 0.002833, mean_q: 0.740601
 37344/100000: episode: 506, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.705, mean reward: 0.705 [0.705, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.113, 10.100], loss: 0.000009, mae: 0.003211, mean_q: 0.747102
 37346/100000: episode: 507, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.779, mean reward: 0.389 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.412, 10.100], loss: 0.000004, mae: 0.002424, mean_q: 0.742185
 37359/100000: episode: 508, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 0.828, mean reward: 0.064 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.283, 10.391], loss: 0.000051, mae: 0.004670, mean_q: 0.743906
 37360/100000: episode: 509, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.627, mean reward: 0.627 [0.627, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.296, 10.200], loss: 0.000010, mae: 0.003019, mean_q: 0.742023
 37362/100000: episode: 510, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.785, mean reward: 0.392 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.502, 10.100], loss: 0.000010, mae: 0.003198, mean_q: 0.742261
 37363/100000: episode: 511, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.697, mean reward: 0.697 [0.697, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.258, 10.200], loss: 0.000004, mae: 0.002198, mean_q: 0.745228
 37364/100000: episode: 512, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.779, mean reward: 0.779 [0.779, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.237, 10.100], loss: 0.000007, mae: 0.003314, mean_q: 0.747280
 37365/100000: episode: 513, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.350, 10.200], loss: 0.000020, mae: 0.003968, mean_q: 0.745614
 37378/100000: episode: 514, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 0.742, mean reward: 0.057 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.429], loss: 0.000017, mae: 0.003341, mean_q: 0.743364
 37379/100000: episode: 515, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.665, mean reward: 0.665 [0.665, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.367, 10.200], loss: 0.000011, mae: 0.003986, mean_q: 0.740634
 37380/100000: episode: 516, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.257, 10.200], loss: 0.000009, mae: 0.003749, mean_q: 0.747156
 37381/100000: episode: 517, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.325, 10.200], loss: 0.000006, mae: 0.002011, mean_q: 0.742103
 37383/100000: episode: 518, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.843, mean reward: 0.422 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.440, 10.100], loss: 0.000058, mae: 0.004429, mean_q: 0.741139
 37389/100000: episode: 519, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 0.619, mean reward: 0.103 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.218], loss: 0.000041, mae: 0.005517, mean_q: 0.744177
 37391/100000: episode: 520, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.740, mean reward: 0.370 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.395, 10.100], loss: 0.000032, mae: 0.004706, mean_q: 0.741014
 37397/100000: episode: 521, duration: 0.045s, episode steps: 6, steps per second: 135, episode reward: 0.607, mean reward: 0.101 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.272], loss: 0.000020, mae: 0.004313, mean_q: 0.745580
 37398/100000: episode: 522, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.605, mean reward: 0.605 [0.605, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.304, 10.200], loss: 0.000144, mae: 0.006768, mean_q: 0.742306
 37399/100000: episode: 523, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.778, mean reward: 0.778 [0.778, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.319, 10.100], loss: 0.000021, mae: 0.006178, mean_q: 0.738256
 37400/100000: episode: 524, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.817, mean reward: 0.817 [0.817, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.324, 10.100], loss: 0.000005, mae: 0.002186, mean_q: 0.744123
 37401/100000: episode: 525, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.721, mean reward: 0.721 [0.721, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.329, 10.100], loss: 0.000047, mae: 0.006933, mean_q: 0.750425
 37402/100000: episode: 526, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.688, mean reward: 0.688 [0.688, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.283, 10.200], loss: 0.000013, mae: 0.004880, mean_q: 0.737650
 37415/100000: episode: 527, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 0.818, mean reward: 0.063 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.426], loss: 0.000097, mae: 0.006676, mean_q: 0.742615
 37416/100000: episode: 528, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.783, mean reward: 0.783 [0.783, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.287, 10.100], loss: 0.000012, mae: 0.003724, mean_q: 0.745834
 37422/100000: episode: 529, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 0.596, mean reward: 0.099 [0.000, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.153], loss: 0.000105, mae: 0.009130, mean_q: 0.743881
 37423/100000: episode: 530, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.730, mean reward: 0.730 [0.730, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.323, 10.100], loss: 0.000016, mae: 0.004999, mean_q: 0.740628
 37424/100000: episode: 531, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.356, 10.100], loss: 0.000193, mae: 0.014864, mean_q: 0.757048
 37437/100000: episode: 532, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 0.787, mean reward: 0.061 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.395, 10.332], loss: 0.000090, mae: 0.006582, mean_q: 0.742320
 37450/100000: episode: 533, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 0.699, mean reward: 0.054 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-1.000, 10.381], loss: 0.000085, mae: 0.007292, mean_q: 0.742976
 37451/100000: episode: 534, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.703, mean reward: 0.703 [0.703, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.294, 10.200], loss: 0.000007, mae: 0.003084, mean_q: 0.746886
 37453/100000: episode: 535, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.808, mean reward: 0.404 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.377, 10.100], loss: 0.000010, mae: 0.003812, mean_q: 0.744246
 37455/100000: episode: 536, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.718, mean reward: 0.359 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.312, 10.100], loss: 0.000117, mae: 0.007535, mean_q: 0.742023
 37456/100000: episode: 537, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.769, mean reward: 0.769 [0.769, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.303, 10.100], loss: 0.000028, mae: 0.006463, mean_q: 0.748946
 37457/100000: episode: 538, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.657, mean reward: 0.657 [0.657, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.310, 10.100], loss: 0.000067, mae: 0.011224, mean_q: 0.732592
 37470/100000: episode: 539, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 0.730, mean reward: 0.056 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.239], loss: 0.000042, mae: 0.006671, mean_q: 0.744246
 37472/100000: episode: 540, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.736, mean reward: 0.368 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.388, 10.100], loss: 0.000174, mae: 0.006325, mean_q: 0.745524
 37474/100000: episode: 541, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.700, mean reward: 0.350 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.350, 10.100], loss: 0.000358, mae: 0.010828, mean_q: 0.743180
 37475/100000: episode: 542, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.736, mean reward: 0.736 [0.736, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.307, 10.100], loss: 0.000003, mae: 0.002072, mean_q: 0.744708
 37476/100000: episode: 543, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.665, mean reward: 0.665 [0.665, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.250, 10.200], loss: 0.000039, mae: 0.008271, mean_q: 0.735746
 37477/100000: episode: 544, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.703, mean reward: 0.703 [0.703, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.329, 10.200], loss: 0.000028, mae: 0.004908, mean_q: 0.746703
 37490/100000: episode: 545, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 0.698, mean reward: 0.054 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.429], loss: 0.000084, mae: 0.007021, mean_q: 0.743908
 37491/100000: episode: 546, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 0.678, mean reward: 0.678 [0.678, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.265, 10.200], loss: 0.000103, mae: 0.013961, mean_q: 0.730285
 37492/100000: episode: 547, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.664, mean reward: 0.664 [0.664, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.323, 10.100], loss: 0.000003, mae: 0.001843, mean_q: 0.744350
 37493/100000: episode: 548, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.329, 10.200], loss: 0.000066, mae: 0.011180, mean_q: 0.755335
 37494/100000: episode: 549, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.687, mean reward: 0.687 [0.687, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.327, 10.200], loss: 0.000064, mae: 0.007229, mean_q: 0.740196
 37507/100000: episode: 550, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 0.808, mean reward: 0.062 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.736, 10.583], loss: 0.000098, mae: 0.008526, mean_q: 0.744448
 37509/100000: episode: 551, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.705, mean reward: 0.353 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.350, 10.100], loss: 0.000073, mae: 0.009813, mean_q: 0.734316
 37510/100000: episode: 552, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.788, mean reward: 0.788 [0.788, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.290, 10.200], loss: 0.000050, mae: 0.009553, mean_q: 0.753789
 37511/100000: episode: 553, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.616, mean reward: 0.616 [0.616, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.391, 10.100], loss: 0.000098, mae: 0.004636, mean_q: 0.740968
 37512/100000: episode: 554, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.333, 10.100], loss: 0.000165, mae: 0.017438, mean_q: 0.726296
 37525/100000: episode: 555, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 0.693, mean reward: 0.053 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.491, 10.364], loss: 0.000136, mae: 0.009892, mean_q: 0.744853
 37526/100000: episode: 556, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.634, mean reward: 0.634 [0.634, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.301, 10.100], loss: 0.000140, mae: 0.007802, mean_q: 0.745739
 37528/100000: episode: 557, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.708, mean reward: 0.354 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.402, 10.100], loss: 0.000205, mae: 0.010834, mean_q: 0.748890
 37534/100000: episode: 558, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 0.633, mean reward: 0.106 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.269], loss: 0.000203, mae: 0.012525, mean_q: 0.743212
 37535/100000: episode: 559, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.635, mean reward: 0.635 [0.635, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.353, 10.100], loss: 0.000091, mae: 0.009196, mean_q: 0.736674
 37537/100000: episode: 560, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.736, mean reward: 0.368 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.395, 10.100], loss: 0.000052, mae: 0.009193, mean_q: 0.750010
 37538/100000: episode: 561, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.709, mean reward: 0.709 [0.709, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.334, 10.100], loss: 0.000261, mae: 0.012419, mean_q: 0.736673
 37544/100000: episode: 562, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 0.719, mean reward: 0.120 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.548, 10.273], loss: 0.000106, mae: 0.010008, mean_q: 0.743360
 37545/100000: episode: 563, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.714, mean reward: 0.714 [0.714, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.230, 10.200], loss: 0.000004, mae: 0.002061, mean_q: 0.740133
 37546/100000: episode: 564, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.679, mean reward: 0.679 [0.679, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.359, 10.100], loss: 0.000136, mae: 0.016132, mean_q: 0.759061
 37547/100000: episode: 565, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.641, mean reward: 0.641 [0.641, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.402, 10.100], loss: 0.000077, mae: 0.008559, mean_q: 0.736047
 37553/100000: episode: 566, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 0.659, mean reward: 0.110 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.301], loss: 0.000050, mae: 0.007017, mean_q: 0.744864
 37554/100000: episode: 567, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.724, mean reward: 0.724 [0.724, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.353, 10.100], loss: 0.000006, mae: 0.002908, mean_q: 0.738946
 37555/100000: episode: 568, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.630, mean reward: 0.630 [0.630, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.351, 10.200], loss: 0.000022, mae: 0.005930, mean_q: 0.747895
 37556/100000: episode: 569, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.313, 10.200], loss: 0.000004, mae: 0.002228, mean_q: 0.742763
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7705414295196533
2
 37557/100000: episode: 570, duration: 4.354s, episode steps: 1, steps per second: 0, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.301, 10.200], loss: 0.000019, mae: 0.005683, mean_q: 0.736384
 37658/100000: episode: 571, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.850, 10.100], loss: 0.000068, mae: 0.005118, mean_q: 0.743022
 37759/100000: episode: 572, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.415, 10.139], loss: 0.000061, mae: 0.005294, mean_q: 0.742156
 37860/100000: episode: 573, duration: 0.604s, episode steps: 101, steps per second: 167, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.743, 10.280], loss: 0.000066, mae: 0.005604, mean_q: 0.742283
 37961/100000: episode: 574, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.341, 10.100], loss: 0.000076, mae: 0.006949, mean_q: 0.741580
 38062/100000: episode: 575, duration: 0.539s, episode steps: 101, steps per second: 188, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.066, 10.100], loss: 0.000065, mae: 0.005930, mean_q: 0.741235
 38163/100000: episode: 576, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.854, mean reward: 0.008 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.998, 10.100], loss: 0.000082, mae: 0.006244, mean_q: 0.740469
 38264/100000: episode: 577, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.815, 10.181], loss: 0.000066, mae: 0.004999, mean_q: 0.740050
 38365/100000: episode: 578, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.775, 10.181], loss: 0.000061, mae: 0.004755, mean_q: 0.740091
 38466/100000: episode: 579, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.370, 10.201], loss: 0.000076, mae: 0.004869, mean_q: 0.739654
 38567/100000: episode: 580, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.309, 10.133], loss: 0.000065, mae: 0.005298, mean_q: 0.739350
 38668/100000: episode: 581, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.445, 10.276], loss: 0.000065, mae: 0.005287, mean_q: 0.739885
 38769/100000: episode: 582, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.889, 10.326], loss: 0.000078, mae: 0.006310, mean_q: 0.739887
 38870/100000: episode: 583, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.141], loss: 0.000101, mae: 0.007695, mean_q: 0.740491
 38971/100000: episode: 584, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.907, 10.107], loss: 0.000082, mae: 0.006318, mean_q: 0.740015
 39072/100000: episode: 585, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.100], loss: 0.000075, mae: 0.006339, mean_q: 0.739618
 39173/100000: episode: 586, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.039, 10.100], loss: 0.000085, mae: 0.006643, mean_q: 0.739259
 39274/100000: episode: 587, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.347, 10.100], loss: 0.000078, mae: 0.005706, mean_q: 0.738830
 39375/100000: episode: 588, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.024, 10.155], loss: 0.000066, mae: 0.005368, mean_q: 0.738848
 39476/100000: episode: 589, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.172, 10.161], loss: 0.000081, mae: 0.005157, mean_q: 0.739612
 39577/100000: episode: 590, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.813, mean reward: 0.008 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.393, 10.165], loss: 0.000078, mae: 0.005575, mean_q: 0.739530
 39678/100000: episode: 591, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.594, 10.126], loss: 0.000075, mae: 0.005799, mean_q: 0.738160
 39779/100000: episode: 592, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.791, 10.181], loss: 0.000057, mae: 0.005604, mean_q: 0.738744
 39880/100000: episode: 593, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.301, 10.318], loss: 0.000057, mae: 0.004504, mean_q: 0.738577
 39981/100000: episode: 594, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.100], loss: 0.000079, mae: 0.005808, mean_q: 0.738682
 40082/100000: episode: 595, duration: 0.628s, episode steps: 101, steps per second: 161, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.670, 10.114], loss: 0.000061, mae: 0.005040, mean_q: 0.738428
 40183/100000: episode: 596, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.938, 10.391], loss: 0.000057, mae: 0.004715, mean_q: 0.738952
 40284/100000: episode: 597, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.821, 10.333], loss: 0.000052, mae: 0.005351, mean_q: 0.737859
 40385/100000: episode: 598, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.243, 10.100], loss: 0.000055, mae: 0.005063, mean_q: 0.737847
 40486/100000: episode: 599, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.504, 10.288], loss: 0.000058, mae: 0.005847, mean_q: 0.737839
 40587/100000: episode: 600, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.866, 10.202], loss: 0.000052, mae: 0.004746, mean_q: 0.737853
 40688/100000: episode: 601, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.629, 10.235], loss: 0.000072, mae: 0.006670, mean_q: 0.738102
 40789/100000: episode: 602, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.181, 10.100], loss: 0.000074, mae: 0.006153, mean_q: 0.737451
 40890/100000: episode: 603, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.949, 10.251], loss: 0.000055, mae: 0.004776, mean_q: 0.737818
 40991/100000: episode: 604, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.163, 10.100], loss: 0.000047, mae: 0.004188, mean_q: 0.737571
 41092/100000: episode: 605, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.567, 10.100], loss: 0.000064, mae: 0.005349, mean_q: 0.737265
 41193/100000: episode: 606, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.231, 10.100], loss: 0.000066, mae: 0.005872, mean_q: 0.737314
 41294/100000: episode: 607, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.552, 10.100], loss: 0.000070, mae: 0.006118, mean_q: 0.737353
 41395/100000: episode: 608, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.070, 10.100], loss: 0.000071, mae: 0.005265, mean_q: 0.736114
 41496/100000: episode: 609, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.574, 10.100], loss: 0.000065, mae: 0.004755, mean_q: 0.736888
 41597/100000: episode: 610, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.814, 10.420], loss: 0.000066, mae: 0.005249, mean_q: 0.736708
 41698/100000: episode: 611, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.643, 10.100], loss: 0.000045, mae: 0.003731, mean_q: 0.736918
 41799/100000: episode: 612, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.782, 10.404], loss: 0.000071, mae: 0.005503, mean_q: 0.736799
 41900/100000: episode: 613, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.402, 10.204], loss: 0.000057, mae: 0.005052, mean_q: 0.736655
 42001/100000: episode: 614, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.668, 10.206], loss: 0.000047, mae: 0.003995, mean_q: 0.736587
 42102/100000: episode: 615, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.663, mean reward: 0.007 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.100], loss: 0.000062, mae: 0.005325, mean_q: 0.736729
 42203/100000: episode: 616, duration: 0.667s, episode steps: 101, steps per second: 151, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.582, 10.100], loss: 0.000049, mae: 0.004245, mean_q: 0.736012
 42304/100000: episode: 617, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.947, 10.148], loss: 0.000052, mae: 0.004639, mean_q: 0.736283
 42405/100000: episode: 618, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.397, 10.100], loss: 0.000019, mae: 0.003083, mean_q: 0.736450
 42506/100000: episode: 619, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.093, 10.100], loss: 0.000030, mae: 0.003390, mean_q: 0.736135
 42607/100000: episode: 620, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.275, 10.362], loss: 0.000020, mae: 0.003150, mean_q: 0.736381
 42708/100000: episode: 621, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.216, 10.170], loss: 0.000014, mae: 0.002502, mean_q: 0.736683
 42809/100000: episode: 622, duration: 0.547s, episode steps: 101, steps per second: 184, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.689, 10.168], loss: 0.000013, mae: 0.002525, mean_q: 0.736336
 42910/100000: episode: 623, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.159, 10.220], loss: 0.000018, mae: 0.002710, mean_q: 0.736536
 43011/100000: episode: 624, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.260, 10.100], loss: 0.000015, mae: 0.002872, mean_q: 0.736565
 43112/100000: episode: 625, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.974, 10.205], loss: 0.000016, mae: 0.002504, mean_q: 0.736729
 43213/100000: episode: 626, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.143, 10.107], loss: 0.000013, mae: 0.002405, mean_q: 0.736707
 43314/100000: episode: 627, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.785, 10.100], loss: 0.000012, mae: 0.002567, mean_q: 0.736360
 43415/100000: episode: 628, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.032, 10.100], loss: 0.000016, mae: 0.002917, mean_q: 0.736600
 43516/100000: episode: 629, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.379, 10.346], loss: 0.000009, mae: 0.002075, mean_q: 0.736642
 43617/100000: episode: 630, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.225, 10.100], loss: 0.000012, mae: 0.002612, mean_q: 0.736584
 43718/100000: episode: 631, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.067, 10.316], loss: 0.000010, mae: 0.002322, mean_q: 0.736469
 43819/100000: episode: 632, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.324, 10.295], loss: 0.000013, mae: 0.002635, mean_q: 0.736353
 43920/100000: episode: 633, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.493, 10.225], loss: 0.000010, mae: 0.002443, mean_q: 0.736800
 44021/100000: episode: 634, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.734, 10.376], loss: 0.000015, mae: 0.003033, mean_q: 0.736633
 44122/100000: episode: 635, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: 0.850, mean reward: 0.008 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.845, 10.133], loss: 0.000010, mae: 0.002420, mean_q: 0.736389
 44223/100000: episode: 636, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.554, 10.100], loss: 0.000010, mae: 0.002530, mean_q: 0.736578
 44324/100000: episode: 637, duration: 0.603s, episode steps: 101, steps per second: 168, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.463, 10.100], loss: 0.000009, mae: 0.002690, mean_q: 0.736683
 44425/100000: episode: 638, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.712, 10.278], loss: 0.000011, mae: 0.002802, mean_q: 0.736877
 44526/100000: episode: 639, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.837, 10.281], loss: 0.000019, mae: 0.003072, mean_q: 0.736966
 44627/100000: episode: 640, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.671, 10.100], loss: 0.000018, mae: 0.003308, mean_q: 0.736848
 44728/100000: episode: 641, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.469, 10.100], loss: 0.000011, mae: 0.002745, mean_q: 0.737097
 44829/100000: episode: 642, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.623, 10.185], loss: 0.000016, mae: 0.003343, mean_q: 0.737308
 44930/100000: episode: 643, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.113, 10.356], loss: 0.000015, mae: 0.003104, mean_q: 0.737104
 45031/100000: episode: 644, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.835, mean reward: 0.008 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.194, 10.268], loss: 0.000016, mae: 0.002753, mean_q: 0.737417
 45132/100000: episode: 645, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.132, 10.264], loss: 0.000009, mae: 0.002225, mean_q: 0.737434
 45233/100000: episode: 646, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.494, 10.100], loss: 0.000011, mae: 0.002987, mean_q: 0.737337
 45334/100000: episode: 647, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.856, 10.207], loss: 0.000017, mae: 0.002981, mean_q: 0.737649
 45435/100000: episode: 648, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.799, 10.100], loss: 0.000015, mae: 0.003115, mean_q: 0.737877
 45536/100000: episode: 649, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.409, 10.100], loss: 0.000008, mae: 0.002460, mean_q: 0.737860
 45637/100000: episode: 650, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.847, 10.100], loss: 0.000014, mae: 0.002811, mean_q: 0.737617
 45738/100000: episode: 651, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.763, 10.105], loss: 0.000014, mae: 0.003056, mean_q: 0.737545
 45839/100000: episode: 652, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.859, 10.267], loss: 0.000011, mae: 0.002959, mean_q: 0.737976
 45940/100000: episode: 653, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.508, 10.333], loss: 0.000009, mae: 0.002629, mean_q: 0.738096
 46041/100000: episode: 654, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.067, 10.213], loss: 0.000014, mae: 0.003327, mean_q: 0.738384
 46142/100000: episode: 655, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.754, 10.177], loss: 0.000010, mae: 0.002619, mean_q: 0.738382
 46243/100000: episode: 656, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.694, 10.248], loss: 0.000010, mae: 0.002378, mean_q: 0.738456
 46344/100000: episode: 657, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.560, 10.142], loss: 0.000008, mae: 0.002379, mean_q: 0.738370
 46445/100000: episode: 658, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.548, 10.100], loss: 0.000007, mae: 0.002239, mean_q: 0.738316
 46546/100000: episode: 659, duration: 0.563s, episode steps: 101, steps per second: 180, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.850, 10.100], loss: 0.000012, mae: 0.002723, mean_q: 0.738690
 46647/100000: episode: 660, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.589, 10.117], loss: 0.000008, mae: 0.002424, mean_q: 0.738735
 46748/100000: episode: 661, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.103, 10.296], loss: 0.000010, mae: 0.002333, mean_q: 0.738981
 46849/100000: episode: 662, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.586, 10.307], loss: 0.000014, mae: 0.002868, mean_q: 0.739277
 46950/100000: episode: 663, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.853, mean reward: 0.008 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.221, 10.630], loss: 0.000014, mae: 0.003135, mean_q: 0.739244
 47051/100000: episode: 664, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.708, 10.100], loss: 0.000010, mae: 0.002759, mean_q: 0.739022
 47152/100000: episode: 665, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.549, 10.405], loss: 0.000010, mae: 0.002642, mean_q: 0.739580
 47253/100000: episode: 666, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.550, 10.116], loss: 0.000012, mae: 0.002607, mean_q: 0.739918
 47354/100000: episode: 667, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.834, 10.100], loss: 0.000017, mae: 0.003609, mean_q: 0.740165
 47455/100000: episode: 668, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.409, 10.341], loss: 0.000013, mae: 0.002696, mean_q: 0.740273
 47556/100000: episode: 669, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.313, 10.100], loss: 0.000011, mae: 0.002800, mean_q: 0.740497
[Info] 1-TH LEVEL FOUND: 0.7616394758224487, Considering 10/100 traces
 47657/100000: episode: 670, duration: 5.129s, episode steps: 101, steps per second: 20, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.100], loss: 0.000009, mae: 0.002418, mean_q: 0.740567
 47666/100000: episode: 671, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 0.738, mean reward: 0.082 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.256, 10.213], loss: 0.000003, mae: 0.001777, mean_q: 0.740703
 47674/100000: episode: 672, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 0.692, mean reward: 0.087 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.411], loss: 0.000006, mae: 0.002079, mean_q: 0.740064
 47682/100000: episode: 673, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 0.707, mean reward: 0.088 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.424], loss: 0.000023, mae: 0.002495, mean_q: 0.741434
 47691/100000: episode: 674, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 0.696, mean reward: 0.077 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.436], loss: 0.000006, mae: 0.002711, mean_q: 0.740550
 47696/100000: episode: 675, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 0.758, mean reward: 0.152 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.090, 10.282], loss: 0.000016, mae: 0.002499, mean_q: 0.740593
 47708/100000: episode: 676, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 0.751, mean reward: 0.063 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.037, 10.264], loss: 0.000015, mae: 0.002711, mean_q: 0.740273
 47720/100000: episode: 677, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 0.754, mean reward: 0.063 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.685, 10.452], loss: 0.000013, mae: 0.003427, mean_q: 0.741009
 47725/100000: episode: 678, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 0.689, mean reward: 0.138 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.427, 10.394], loss: 0.000018, mae: 0.003532, mean_q: 0.742985
 47727/100000: episode: 679, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.651, mean reward: 0.325 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.642, 10.352], loss: 0.000008, mae: 0.003284, mean_q: 0.739377
 47732/100000: episode: 680, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 0.699, mean reward: 0.140 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.425], loss: 0.000006, mae: 0.002951, mean_q: 0.741150
 47744/100000: episode: 681, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 0.835, mean reward: 0.070 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.906, 10.518], loss: 0.000004, mae: 0.002224, mean_q: 0.740897
 47752/100000: episode: 682, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 0.746, mean reward: 0.093 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.553, 10.473], loss: 0.000016, mae: 0.002325, mean_q: 0.741040
 47761/100000: episode: 683, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 0.772, mean reward: 0.086 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.440], loss: 0.000036, mae: 0.005007, mean_q: 0.741049
 47762/100000: episode: 684, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.070, 10.203], loss: 0.000013, mae: 0.004711, mean_q: 0.743767
 47770/100000: episode: 685, duration: 0.061s, episode steps: 8, steps per second: 130, episode reward: 0.774, mean reward: 0.097 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.526], loss: 0.000013, mae: 0.002776, mean_q: 0.739727
 47772/100000: episode: 686, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.865, mean reward: 0.433 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.183], loss: 0.000002, mae: 0.001679, mean_q: 0.742228
 47774/100000: episode: 687, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.705, mean reward: 0.353 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.249], loss: 0.000005, mae: 0.002719, mean_q: 0.744350
 47776/100000: episode: 688, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.656, mean reward: 0.328 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.035, 10.332], loss: 0.000007, mae: 0.003319, mean_q: 0.737071
 47788/100000: episode: 689, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 0.710, mean reward: 0.059 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.449], loss: 0.000022, mae: 0.003154, mean_q: 0.741108
 47797/100000: episode: 690, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 0.625, mean reward: 0.069 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.277], loss: 0.000011, mae: 0.003887, mean_q: 0.740829
 47802/100000: episode: 691, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 0.684, mean reward: 0.137 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.241], loss: 0.000004, mae: 0.002332, mean_q: 0.742877
 47807/100000: episode: 692, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 0.647, mean reward: 0.129 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.304], loss: 0.000080, mae: 0.003970, mean_q: 0.741792
 47809/100000: episode: 693, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.611, mean reward: 0.306 [0.000, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.263], loss: 0.000007, mae: 0.003129, mean_q: 0.744087
 47818/100000: episode: 694, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 0.659, mean reward: 0.073 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.234], loss: 0.000038, mae: 0.004706, mean_q: 0.742343
 47820/100000: episode: 695, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.653, mean reward: 0.327 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.363], loss: 0.000049, mae: 0.005209, mean_q: 0.737190
 47822/100000: episode: 696, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.642, mean reward: 0.321 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.317], loss: 0.000021, mae: 0.005522, mean_q: 0.737784
 47824/100000: episode: 697, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.755, mean reward: 0.377 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-1.402, 10.192], loss: 0.000019, mae: 0.004925, mean_q: 0.744527
 47826/100000: episode: 698, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.610, mean reward: 0.305 [0.000, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.281], loss: 0.000014, mae: 0.004072, mean_q: 0.739249
 47828/100000: episode: 699, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.661, mean reward: 0.330 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.247], loss: 0.000011, mae: 0.003677, mean_q: 0.745441
 47840/100000: episode: 700, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 0.794, mean reward: 0.066 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-1.358, 10.486], loss: 0.000087, mae: 0.005244, mean_q: 0.740204
 47841/100000: episode: 701, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.651, mean reward: 0.651 [0.651, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.070, 10.206], loss: 0.000046, mae: 0.009032, mean_q: 0.751170
 47846/100000: episode: 702, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 0.725, mean reward: 0.145 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.444], loss: 0.000022, mae: 0.005889, mean_q: 0.738969
 47854/100000: episode: 703, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 0.703, mean reward: 0.088 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.297], loss: 0.000067, mae: 0.007075, mean_q: 0.741288
 47859/100000: episode: 704, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 0.709, mean reward: 0.142 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.367], loss: 0.000021, mae: 0.005637, mean_q: 0.742692
 47867/100000: episode: 705, duration: 0.063s, episode steps: 8, steps per second: 128, episode reward: 0.751, mean reward: 0.094 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.424], loss: 0.000035, mae: 0.003310, mean_q: 0.739560
 47869/100000: episode: 706, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.718, mean reward: 0.359 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.249], loss: 0.000038, mae: 0.003092, mean_q: 0.742494
 47874/100000: episode: 707, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 0.711, mean reward: 0.142 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.249], loss: 0.000007, mae: 0.003068, mean_q: 0.741894
 47886/100000: episode: 708, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 0.780, mean reward: 0.065 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.509], loss: 0.000025, mae: 0.002875, mean_q: 0.740941
 47895/100000: episode: 709, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 0.716, mean reward: 0.080 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.445], loss: 0.000157, mae: 0.007585, mean_q: 0.739622
 47897/100000: episode: 710, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.668, mean reward: 0.334 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.324, 10.294], loss: 0.000010, mae: 0.003882, mean_q: 0.737092
 47905/100000: episode: 711, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 0.701, mean reward: 0.088 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.358], loss: 0.000015, mae: 0.003962, mean_q: 0.741558
 47917/100000: episode: 712, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 0.838, mean reward: 0.070 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.305, 10.406], loss: 0.000049, mae: 0.004696, mean_q: 0.741429
 47926/100000: episode: 713, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 0.696, mean reward: 0.077 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.414], loss: 0.000042, mae: 0.004318, mean_q: 0.740672
 47938/100000: episode: 714, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 0.695, mean reward: 0.058 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.241], loss: 0.000026, mae: 0.002853, mean_q: 0.741610
 47947/100000: episode: 715, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 0.696, mean reward: 0.077 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.350], loss: 0.000014, mae: 0.003019, mean_q: 0.740904
 47949/100000: episode: 716, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.604, mean reward: 0.302 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.290], loss: 0.000006, mae: 0.002816, mean_q: 0.746568
 47961/100000: episode: 717, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 0.806, mean reward: 0.067 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.419], loss: 0.000051, mae: 0.004731, mean_q: 0.740338
 47973/100000: episode: 718, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 0.811, mean reward: 0.068 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.455, 10.357], loss: 0.000052, mae: 0.005313, mean_q: 0.740423
 47985/100000: episode: 719, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 0.720, mean reward: 0.060 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.392], loss: 0.000054, mae: 0.004523, mean_q: 0.740745
 47997/100000: episode: 720, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 0.820, mean reward: 0.068 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.508], loss: 0.000031, mae: 0.004245, mean_q: 0.740922
 47999/100000: episode: 721, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.651, mean reward: 0.326 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.251], loss: 0.000024, mae: 0.006029, mean_q: 0.746543
 48011/100000: episode: 722, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 0.718, mean reward: 0.060 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.588, 10.361], loss: 0.000047, mae: 0.005469, mean_q: 0.740373
 48013/100000: episode: 723, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.690, mean reward: 0.345 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.244, 10.228], loss: 0.000016, mae: 0.004154, mean_q: 0.742014
 48015/100000: episode: 724, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 0.651, mean reward: 0.325 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.171], loss: 0.000066, mae: 0.004877, mean_q: 0.743927
 48027/100000: episode: 725, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 0.710, mean reward: 0.059 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.363], loss: 0.000064, mae: 0.005936, mean_q: 0.740185
 48039/100000: episode: 726, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 0.861, mean reward: 0.072 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.566], loss: 0.000094, mae: 0.006947, mean_q: 0.740661
 48048/100000: episode: 727, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 0.660, mean reward: 0.073 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.329], loss: 0.000067, mae: 0.006619, mean_q: 0.741694
 48057/100000: episode: 728, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 0.722, mean reward: 0.080 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.303], loss: 0.000057, mae: 0.004727, mean_q: 0.741618
 48059/100000: episode: 729, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.647, mean reward: 0.323 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.358], loss: 0.000006, mae: 0.002555, mean_q: 0.743035
 48071/100000: episode: 730, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 0.958, mean reward: 0.080 [0.000, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.732, 10.297], loss: 0.000042, mae: 0.003829, mean_q: 0.741195
 48079/100000: episode: 731, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 0.746, mean reward: 0.093 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-1.418, 10.412], loss: 0.000016, mae: 0.004827, mean_q: 0.741061
 48084/100000: episode: 732, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 0.640, mean reward: 0.128 [0.000, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.343], loss: 0.000007, mae: 0.003108, mean_q: 0.741012
 48089/100000: episode: 733, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 0.691, mean reward: 0.138 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.270], loss: 0.000051, mae: 0.004348, mean_q: 0.740303
 48091/100000: episode: 734, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.673, mean reward: 0.336 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.747, 10.317], loss: 0.000009, mae: 0.003159, mean_q: 0.739562
 48096/100000: episode: 735, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 0.734, mean reward: 0.147 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.325], loss: 0.000047, mae: 0.003819, mean_q: 0.741499
 48097/100000: episode: 736, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.070, 10.257], loss: 0.000008, mae: 0.003145, mean_q: 0.740235
 48106/100000: episode: 737, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 0.720, mean reward: 0.080 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.344], loss: 0.000091, mae: 0.005109, mean_q: 0.740643
 48111/100000: episode: 738, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 0.763, mean reward: 0.153 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.413], loss: 0.000098, mae: 0.005824, mean_q: 0.740469
 48123/100000: episode: 739, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 0.773, mean reward: 0.064 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.073, 10.319], loss: 0.000200, mae: 0.008107, mean_q: 0.740728
 48125/100000: episode: 740, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.641, mean reward: 0.321 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.285], loss: 0.000025, mae: 0.005896, mean_q: 0.743635
 48127/100000: episode: 741, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.603, mean reward: 0.301 [0.000, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.292], loss: 0.000016, mae: 0.004553, mean_q: 0.742958
 48132/100000: episode: 742, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 0.656, mean reward: 0.131 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.375], loss: 0.000043, mae: 0.007955, mean_q: 0.740613
 48134/100000: episode: 743, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.686, mean reward: 0.343 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.200], loss: 0.000030, mae: 0.006942, mean_q: 0.734772
 48146/100000: episode: 744, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 0.695, mean reward: 0.058 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.430], loss: 0.000058, mae: 0.005428, mean_q: 0.742714
 48151/100000: episode: 745, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 0.675, mean reward: 0.135 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.295], loss: 0.000020, mae: 0.004873, mean_q: 0.738273
 48160/100000: episode: 746, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 0.695, mean reward: 0.077 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.370], loss: 0.000029, mae: 0.003942, mean_q: 0.742686
 48162/100000: episode: 747, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.601, mean reward: 0.300 [0.000, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.252], loss: 0.000171, mae: 0.004595, mean_q: 0.739920
 48174/100000: episode: 748, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 0.801, mean reward: 0.067 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.556], loss: 0.000154, mae: 0.006885, mean_q: 0.743662
 48176/100000: episode: 749, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.641, mean reward: 0.320 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.240], loss: 0.000118, mae: 0.011097, mean_q: 0.731464
 48177/100000: episode: 750, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.761, mean reward: 0.761 [0.761, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.070, 10.207], loss: 0.000027, mae: 0.003313, mean_q: 0.740967
 48179/100000: episode: 751, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.618, mean reward: 0.309 [0.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.290], loss: 0.000017, mae: 0.004798, mean_q: 0.746556
 48184/100000: episode: 752, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 0.645, mean reward: 0.129 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.347], loss: 0.000044, mae: 0.005624, mean_q: 0.739380
 48189/100000: episode: 753, duration: 0.037s, episode steps: 5, steps per second: 134, episode reward: 0.662, mean reward: 0.132 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.370], loss: 0.000042, mae: 0.003880, mean_q: 0.740809
 48197/100000: episode: 754, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 0.713, mean reward: 0.089 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.407], loss: 0.000026, mae: 0.003511, mean_q: 0.741453
 48202/100000: episode: 755, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 0.648, mean reward: 0.130 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.350], loss: 0.000019, mae: 0.003623, mean_q: 0.739893
 48204/100000: episode: 756, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.733, mean reward: 0.366 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.035, 10.371], loss: 0.000032, mae: 0.003447, mean_q: 0.741532
 48212/100000: episode: 757, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 0.751, mean reward: 0.094 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.428], loss: 0.000075, mae: 0.005668, mean_q: 0.739393
 48224/100000: episode: 758, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 0.822, mean reward: 0.069 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-1.079, 10.469], loss: 0.000040, mae: 0.004698, mean_q: 0.741331
 48233/100000: episode: 759, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 0.665, mean reward: 0.074 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.802, 10.355], loss: 0.000087, mae: 0.004883, mean_q: 0.741801
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7616394758224487
1
 48242/100000: episode: 760, duration: 4.345s, episode steps: 9, steps per second: 2, episode reward: 0.719, mean reward: 0.080 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.202, 10.388], loss: 0.000116, mae: 0.006205, mean_q: 0.737535
 48343/100000: episode: 761, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.207, 10.159], loss: 0.000053, mae: 0.004897, mean_q: 0.740185
 48444/100000: episode: 762, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.821, 10.385], loss: 0.000052, mae: 0.004621, mean_q: 0.739387
 48545/100000: episode: 763, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.855, 10.227], loss: 0.000039, mae: 0.003749, mean_q: 0.738492
 48646/100000: episode: 764, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.713, 10.247], loss: 0.000073, mae: 0.005684, mean_q: 0.737590
 48747/100000: episode: 765, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.236, 10.100], loss: 0.000045, mae: 0.004261, mean_q: 0.736961
 48848/100000: episode: 766, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.242, 10.100], loss: 0.000070, mae: 0.005527, mean_q: 0.736889
 48949/100000: episode: 767, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.248, 10.100], loss: 0.000077, mae: 0.005057, mean_q: 0.736209
 49050/100000: episode: 768, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.866, 10.100], loss: 0.000064, mae: 0.004661, mean_q: 0.736156
 49151/100000: episode: 769, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.885, 10.221], loss: 0.000059, mae: 0.005941, mean_q: 0.735945
 49252/100000: episode: 770, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.880, 10.224], loss: 0.000089, mae: 0.005785, mean_q: 0.736671
 49353/100000: episode: 771, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.045, 10.100], loss: 0.000071, mae: 0.004654, mean_q: 0.735783
 49454/100000: episode: 772, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.309, 10.100], loss: 0.000056, mae: 0.004545, mean_q: 0.736174
 49555/100000: episode: 773, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.668, mean reward: 0.007 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.681, 10.272], loss: 0.000054, mae: 0.004602, mean_q: 0.735242
 49656/100000: episode: 774, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.127, 10.241], loss: 0.000055, mae: 0.004684, mean_q: 0.735516
 49757/100000: episode: 775, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.671, 10.100], loss: 0.000059, mae: 0.004370, mean_q: 0.734693
 49858/100000: episode: 776, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.496, 10.108], loss: 0.000073, mae: 0.004906, mean_q: 0.735424
 49959/100000: episode: 777, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.371, 10.100], loss: 0.000073, mae: 0.004793, mean_q: 0.735005
 50060/100000: episode: 778, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.449, 10.196], loss: 0.000062, mae: 0.005001, mean_q: 0.734981
 50161/100000: episode: 779, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.591, 10.100], loss: 0.000064, mae: 0.004089, mean_q: 0.734520
 50262/100000: episode: 780, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.498, 10.107], loss: 0.000043, mae: 0.003527, mean_q: 0.734484
 50363/100000: episode: 781, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.747, 10.100], loss: 0.000078, mae: 0.005663, mean_q: 0.734504
 50464/100000: episode: 782, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.100], loss: 0.000066, mae: 0.004840, mean_q: 0.733864
 50565/100000: episode: 783, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.337, 10.131], loss: 0.000046, mae: 0.003683, mean_q: 0.734019
 50666/100000: episode: 784, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.265, 10.100], loss: 0.000082, mae: 0.005593, mean_q: 0.734150
 50767/100000: episode: 785, duration: 0.582s, episode steps: 101, steps per second: 173, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.703, 10.100], loss: 0.000063, mae: 0.004307, mean_q: 0.733912
 50868/100000: episode: 786, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.903, 10.100], loss: 0.000058, mae: 0.004862, mean_q: 0.733695
 50969/100000: episode: 787, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.211, 10.141], loss: 0.000065, mae: 0.003989, mean_q: 0.734239
 51070/100000: episode: 788, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.100], loss: 0.000071, mae: 0.005161, mean_q: 0.733760
 51171/100000: episode: 789, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.675, mean reward: 0.007 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.100], loss: 0.000084, mae: 0.006261, mean_q: 0.733408
 51272/100000: episode: 790, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.824, 10.100], loss: 0.000065, mae: 0.004033, mean_q: 0.733693
 51373/100000: episode: 791, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.641, 10.100], loss: 0.000050, mae: 0.003734, mean_q: 0.733346
 51474/100000: episode: 792, duration: 0.592s, episode steps: 101, steps per second: 170, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.181, 10.145], loss: 0.000073, mae: 0.004963, mean_q: 0.732552
 51575/100000: episode: 793, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.250, 10.219], loss: 0.000068, mae: 0.004116, mean_q: 0.732581
 51676/100000: episode: 794, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.459, 10.225], loss: 0.000048, mae: 0.003949, mean_q: 0.732327
 51777/100000: episode: 795, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.740, 10.131], loss: 0.000069, mae: 0.004160, mean_q: 0.733007
 51878/100000: episode: 796, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.910, 10.100], loss: 0.000040, mae: 0.003363, mean_q: 0.732737
 51979/100000: episode: 797, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.555, 10.163], loss: 0.000083, mae: 0.005917, mean_q: 0.732777
 52080/100000: episode: 798, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.094, 10.100], loss: 0.000045, mae: 0.004164, mean_q: 0.732030
 52181/100000: episode: 799, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.195, 10.100], loss: 0.000059, mae: 0.003692, mean_q: 0.731943
 52282/100000: episode: 800, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.610, 10.337], loss: 0.000065, mae: 0.004681, mean_q: 0.731505
 52383/100000: episode: 801, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.706, 10.301], loss: 0.000054, mae: 0.004272, mean_q: 0.731204
 52484/100000: episode: 802, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.987, 10.109], loss: 0.000068, mae: 0.004761, mean_q: 0.731376
 52585/100000: episode: 803, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.077, 10.158], loss: 0.000058, mae: 0.004227, mean_q: 0.731291
 52686/100000: episode: 804, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.811, 10.129], loss: 0.000039, mae: 0.003125, mean_q: 0.731000
 52787/100000: episode: 805, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.236], loss: 0.000049, mae: 0.003805, mean_q: 0.731623
 52888/100000: episode: 806, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.046, 10.188], loss: 0.000052, mae: 0.004364, mean_q: 0.730660
 52989/100000: episode: 807, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.663, 10.150], loss: 0.000019, mae: 0.002282, mean_q: 0.730977
 53090/100000: episode: 808, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.366, 10.100], loss: 0.000023, mae: 0.003065, mean_q: 0.731908
 53191/100000: episode: 809, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.207], loss: 0.000012, mae: 0.001866, mean_q: 0.731786
 53292/100000: episode: 810, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.261, 10.100], loss: 0.000013, mae: 0.002402, mean_q: 0.731721
 53393/100000: episode: 811, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.770, 10.187], loss: 0.000010, mae: 0.001897, mean_q: 0.732160
 53494/100000: episode: 812, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.854, 10.100], loss: 0.000010, mae: 0.002077, mean_q: 0.732060
 53595/100000: episode: 813, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.536, 10.300], loss: 0.000014, mae: 0.002270, mean_q: 0.732648
 53696/100000: episode: 814, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.418, 10.137], loss: 0.000012, mae: 0.002070, mean_q: 0.732289
 53797/100000: episode: 815, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.576, 10.286], loss: 0.000012, mae: 0.002051, mean_q: 0.732405
 53898/100000: episode: 816, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.482, 10.158], loss: 0.000007, mae: 0.001530, mean_q: 0.732070
 53999/100000: episode: 817, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.257, 10.100], loss: 0.000014, mae: 0.002414, mean_q: 0.732530
 54100/100000: episode: 818, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.390, 10.100], loss: 0.000011, mae: 0.002233, mean_q: 0.732857
 54201/100000: episode: 819, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.862, 10.116], loss: 0.000017, mae: 0.002675, mean_q: 0.732510
 54302/100000: episode: 820, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.191, 10.100], loss: 0.000014, mae: 0.002584, mean_q: 0.732387
 54403/100000: episode: 821, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.017, 10.144], loss: 0.000010, mae: 0.001901, mean_q: 0.732586
 54504/100000: episode: 822, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.414, 10.131], loss: 0.000012, mae: 0.002165, mean_q: 0.732525
 54605/100000: episode: 823, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.527, 10.100], loss: 0.000015, mae: 0.002245, mean_q: 0.732800
 54706/100000: episode: 824, duration: 0.538s, episode steps: 101, steps per second: 188, episode reward: 0.892, mean reward: 0.009 [0.000, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.518, 10.306], loss: 0.000014, mae: 0.002341, mean_q: 0.732910
 54807/100000: episode: 825, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.447, 10.100], loss: 0.000011, mae: 0.002096, mean_q: 0.732741
 54908/100000: episode: 826, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.323, 10.140], loss: 0.000015, mae: 0.002679, mean_q: 0.732932
 55009/100000: episode: 827, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.969, 10.100], loss: 0.000018, mae: 0.002510, mean_q: 0.732826
 55110/100000: episode: 828, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.698, 10.100], loss: 0.000021, mae: 0.003414, mean_q: 0.732809
 55211/100000: episode: 829, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.100], loss: 0.000016, mae: 0.002523, mean_q: 0.733018
 55312/100000: episode: 830, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.358, 10.100], loss: 0.000012, mae: 0.002151, mean_q: 0.733462
 55413/100000: episode: 831, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.252, 10.280], loss: 0.000017, mae: 0.002468, mean_q: 0.733432
 55514/100000: episode: 832, duration: 0.614s, episode steps: 101, steps per second: 164, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.641, 10.194], loss: 0.000013, mae: 0.002523, mean_q: 0.733091
 55615/100000: episode: 833, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.739, 10.127], loss: 0.000018, mae: 0.002867, mean_q: 0.733939
 55716/100000: episode: 834, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.941, 10.161], loss: 0.000016, mae: 0.002377, mean_q: 0.733594
 55817/100000: episode: 835, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.100], loss: 0.000015, mae: 0.002313, mean_q: 0.733356
 55918/100000: episode: 836, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.806, 10.343], loss: 0.000013, mae: 0.002083, mean_q: 0.733387
 56019/100000: episode: 837, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.974, 10.272], loss: 0.000007, mae: 0.001738, mean_q: 0.733603
 56120/100000: episode: 838, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.245, 10.100], loss: 0.000018, mae: 0.003150, mean_q: 0.733337
 56221/100000: episode: 839, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.196, 10.186], loss: 0.000019, mae: 0.002749, mean_q: 0.733542
 56322/100000: episode: 840, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.875, 10.120], loss: 0.000019, mae: 0.002737, mean_q: 0.733662
 56423/100000: episode: 841, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.661, mean reward: 0.007 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.635, 10.100], loss: 0.000018, mae: 0.002590, mean_q: 0.733316
 56524/100000: episode: 842, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.475, 10.100], loss: 0.000022, mae: 0.003265, mean_q: 0.733823
 56625/100000: episode: 843, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.930, 10.125], loss: 0.000020, mae: 0.002621, mean_q: 0.733524
 56726/100000: episode: 844, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.649, 10.182], loss: 0.000014, mae: 0.002138, mean_q: 0.733479
 56827/100000: episode: 845, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.135, 10.130], loss: 0.000023, mae: 0.003606, mean_q: 0.733803
 56928/100000: episode: 846, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.319, 10.100], loss: 0.000012, mae: 0.002533, mean_q: 0.733391
 57029/100000: episode: 847, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.205, 10.299], loss: 0.000019, mae: 0.002448, mean_q: 0.733817
 57130/100000: episode: 848, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.651, 10.101], loss: 0.000026, mae: 0.002938, mean_q: 0.733841
 57231/100000: episode: 849, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.755, 10.215], loss: 0.000017, mae: 0.002559, mean_q: 0.733799
 57332/100000: episode: 850, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.186, 10.242], loss: 0.000014, mae: 0.002108, mean_q: 0.733954
 57433/100000: episode: 851, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.742, 10.112], loss: 0.000007, mae: 0.001619, mean_q: 0.733933
 57534/100000: episode: 852, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.901, 10.100], loss: 0.000019, mae: 0.002301, mean_q: 0.734030
 57635/100000: episode: 853, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.803, 10.211], loss: 0.000007, mae: 0.001497, mean_q: 0.733996
 57736/100000: episode: 854, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.864, 10.275], loss: 0.000008, mae: 0.001580, mean_q: 0.734162
 57837/100000: episode: 855, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.629, 10.194], loss: 0.000013, mae: 0.002298, mean_q: 0.733999
 57938/100000: episode: 856, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.647, mean reward: 0.006 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.261, 10.188], loss: 0.000014, mae: 0.001991, mean_q: 0.734287
 58039/100000: episode: 857, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.326, 10.167], loss: 0.000018, mae: 0.002706, mean_q: 0.734427
 58140/100000: episode: 858, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.512, 10.100], loss: 0.000031, mae: 0.003499, mean_q: 0.734729
 58241/100000: episode: 859, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.268, 10.350], loss: 0.000026, mae: 0.003407, mean_q: 0.734207
[Info] 1-TH LEVEL FOUND: 0.7507318258285522, Considering 10/100 traces
 58342/100000: episode: 860, duration: 5.106s, episode steps: 101, steps per second: 20, episode reward: 0.954, mean reward: 0.009 [0.000, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.859, 10.353], loss: 0.000006, mae: 0.001475, mean_q: 0.734253
 58358/100000: episode: 861, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 0.648, mean reward: 0.040 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.140], loss: 0.000006, mae: 0.001335, mean_q: 0.734196
 58368/100000: episode: 862, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.746, mean reward: 0.075 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.198, 10.100], loss: 0.000053, mae: 0.002533, mean_q: 0.735036
 58375/100000: episode: 863, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 0.730, mean reward: 0.104 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.089, 10.167], loss: 0.000023, mae: 0.003036, mean_q: 0.734242
 58397/100000: episode: 864, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 0.676, mean reward: 0.031 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.138, 10.300], loss: 0.000018, mae: 0.002859, mean_q: 0.734059
 58416/100000: episode: 865, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 0.723, mean reward: 0.038 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.627, 10.153], loss: 0.000024, mae: 0.002790, mean_q: 0.734627
 58423/100000: episode: 866, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.696, mean reward: 0.099 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.064, 10.194], loss: 0.000040, mae: 0.004179, mean_q: 0.735227
 58435/100000: episode: 867, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 0.811, mean reward: 0.068 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.100], loss: 0.000018, mae: 0.003558, mean_q: 0.733902
 58451/100000: episode: 868, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 0.645, mean reward: 0.040 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.078, 10.100], loss: 0.000014, mae: 0.003220, mean_q: 0.734382
 58463/100000: episode: 869, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 0.710, mean reward: 0.059 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.299, 10.119], loss: 0.000045, mae: 0.005197, mean_q: 0.733956
 58470/100000: episode: 870, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 0.696, mean reward: 0.099 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.086, 10.100], loss: 0.000003, mae: 0.002079, mean_q: 0.734741
 58473/100000: episode: 871, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.774, mean reward: 0.258 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.075, 10.154], loss: 0.000003, mae: 0.002023, mean_q: 0.732443
 58484/100000: episode: 872, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 0.662, mean reward: 0.060 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.184, 10.100], loss: 0.000021, mae: 0.003745, mean_q: 0.735464
 58503/100000: episode: 873, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 0.680, mean reward: 0.036 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.095, 10.100], loss: 0.000017, mae: 0.002044, mean_q: 0.733925
 58525/100000: episode: 874, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 0.809, mean reward: 0.037 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.156], loss: 0.000012, mae: 0.002169, mean_q: 0.735054
 58541/100000: episode: 875, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 0.680, mean reward: 0.043 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.395, 10.100], loss: 0.000025, mae: 0.004252, mean_q: 0.734072
 58551/100000: episode: 876, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 0.762, mean reward: 0.076 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.239, 10.100], loss: 0.000007, mae: 0.003203, mean_q: 0.736452
 58570/100000: episode: 877, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 0.820, mean reward: 0.043 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.320, 10.100], loss: 0.000009, mae: 0.002463, mean_q: 0.734686
 58589/100000: episode: 878, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 0.691, mean reward: 0.036 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.047, 10.100], loss: 0.000011, mae: 0.002139, mean_q: 0.734580
 58608/100000: episode: 879, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 0.641, mean reward: 0.034 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.634, 10.100], loss: 0.000024, mae: 0.003771, mean_q: 0.734944
 58630/100000: episode: 880, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 0.843, mean reward: 0.038 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.136], loss: 0.000015, mae: 0.002501, mean_q: 0.735292
 58652/100000: episode: 881, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 0.743, mean reward: 0.034 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.313], loss: 0.000049, mae: 0.004108, mean_q: 0.734729
 58668/100000: episode: 882, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 0.806, mean reward: 0.050 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.201, 10.100], loss: 0.000085, mae: 0.007780, mean_q: 0.735492
 58684/100000: episode: 883, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 0.792, mean reward: 0.049 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.380], loss: 0.000051, mae: 0.006435, mean_q: 0.734539
 58700/100000: episode: 884, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 0.691, mean reward: 0.043 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.469, 10.100], loss: 0.000018, mae: 0.004500, mean_q: 0.735363
 58716/100000: episode: 885, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 0.662, mean reward: 0.041 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.167, 10.151], loss: 0.000015, mae: 0.002433, mean_q: 0.734954
 58735/100000: episode: 886, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 0.774, mean reward: 0.041 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.327, 10.100], loss: 0.000019, mae: 0.002444, mean_q: 0.735114
 58742/100000: episode: 887, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 0.631, mean reward: 0.090 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.467, 10.171], loss: 0.000026, mae: 0.003544, mean_q: 0.735560
 58764/100000: episode: 888, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.800, mean reward: 0.036 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.144, 10.205], loss: 0.000008, mae: 0.002265, mean_q: 0.734814
 58776/100000: episode: 889, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 0.681, mean reward: 0.057 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.062, 10.100], loss: 0.000015, mae: 0.002022, mean_q: 0.735095
 58795/100000: episode: 890, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 0.750, mean reward: 0.039 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.232, 10.100], loss: 0.000052, mae: 0.004623, mean_q: 0.735611
 58814/100000: episode: 891, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 0.629, mean reward: 0.033 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.173], loss: 0.000007, mae: 0.002192, mean_q: 0.735655
 58817/100000: episode: 892, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 0.716, mean reward: 0.239 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.213], loss: 0.000001, mae: 0.001315, mean_q: 0.732070
 58820/100000: episode: 893, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 0.908, mean reward: 0.303 [0.000, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.116], loss: 0.000033, mae: 0.002638, mean_q: 0.735324
 58827/100000: episode: 894, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 0.736, mean reward: 0.105 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.083, 10.100], loss: 0.000030, mae: 0.003218, mean_q: 0.735041
 58839/100000: episode: 895, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 0.674, mean reward: 0.056 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.172, 10.100], loss: 0.000016, mae: 0.002049, mean_q: 0.735080
 58850/100000: episode: 896, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 0.623, mean reward: 0.057 [0.000, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.150], loss: 0.000049, mae: 0.004638, mean_q: 0.736255
 58866/100000: episode: 897, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 0.619, mean reward: 0.039 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.131], loss: 0.000029, mae: 0.003078, mean_q: 0.735464
 58885/100000: episode: 898, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 0.744, mean reward: 0.039 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.543, 10.100], loss: 0.000020, mae: 0.003042, mean_q: 0.735312
 58901/100000: episode: 899, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 0.712, mean reward: 0.044 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.291], loss: 0.000021, mae: 0.003022, mean_q: 0.735370
 58920/100000: episode: 900, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 0.823, mean reward: 0.043 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.739, 10.100], loss: 0.000057, mae: 0.003477, mean_q: 0.734832
 58923/100000: episode: 901, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 0.616, mean reward: 0.205 [0.000, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.269], loss: 0.000075, mae: 0.003689, mean_q: 0.735099
 58934/100000: episode: 902, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 0.618, mean reward: 0.056 [0.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.053, 10.146], loss: 0.000038, mae: 0.003471, mean_q: 0.735665
 58941/100000: episode: 903, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 0.595, mean reward: 0.085 [0.000, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.223], loss: 0.000030, mae: 0.003808, mean_q: 0.734492
 58953/100000: episode: 904, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 0.833, mean reward: 0.069 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.680, 10.100], loss: 0.000103, mae: 0.003841, mean_q: 0.735334
 58972/100000: episode: 905, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.858, mean reward: 0.045 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.744, 10.100], loss: 0.000041, mae: 0.002540, mean_q: 0.735802
 58991/100000: episode: 906, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.690, mean reward: 0.036 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.118, 10.100], loss: 0.000016, mae: 0.002308, mean_q: 0.735063
 59013/100000: episode: 907, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 0.687, mean reward: 0.031 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.199], loss: 0.000008, mae: 0.002265, mean_q: 0.735428
 59029/100000: episode: 908, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 0.710, mean reward: 0.044 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.131, 10.100], loss: 0.000048, mae: 0.003478, mean_q: 0.735599
 59045/100000: episode: 909, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 0.621, mean reward: 0.039 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.123, 10.100], loss: 0.000037, mae: 0.002619, mean_q: 0.735608
 59067/100000: episode: 910, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.766, mean reward: 0.035 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.644, 10.351], loss: 0.000057, mae: 0.003645, mean_q: 0.735774
 59077/100000: episode: 911, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 0.722, mean reward: 0.072 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.288, 10.100], loss: 0.000064, mae: 0.004396, mean_q: 0.735755
 59093/100000: episode: 912, duration: 0.102s, episode steps: 16, steps per second: 156, episode reward: 0.732, mean reward: 0.046 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.377], loss: 0.000106, mae: 0.005693, mean_q: 0.735343
 59103/100000: episode: 913, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 0.771, mean reward: 0.077 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.255, 10.100], loss: 0.000134, mae: 0.008949, mean_q: 0.734672
 59114/100000: episode: 914, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 0.682, mean reward: 0.062 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.672, 10.193], loss: 0.000056, mae: 0.005042, mean_q: 0.733905
 59130/100000: episode: 915, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 0.683, mean reward: 0.043 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.118, 10.100], loss: 0.000041, mae: 0.004425, mean_q: 0.735835
 59137/100000: episode: 916, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 0.661, mean reward: 0.094 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.100], loss: 0.000006, mae: 0.001568, mean_q: 0.735658
 59144/100000: episode: 917, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 0.729, mean reward: 0.104 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.168], loss: 0.000122, mae: 0.004624, mean_q: 0.735516
 59160/100000: episode: 918, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 0.723, mean reward: 0.045 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.057, 10.100], loss: 0.000059, mae: 0.005292, mean_q: 0.734549
 59179/100000: episode: 919, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 0.828, mean reward: 0.044 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.185, 10.100], loss: 0.000043, mae: 0.003904, mean_q: 0.733887
 59191/100000: episode: 920, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 0.614, mean reward: 0.051 [0.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.260, 10.100], loss: 0.000018, mae: 0.002728, mean_q: 0.736159
 59210/100000: episode: 921, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 0.748, mean reward: 0.039 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.328, 10.100], loss: 0.000027, mae: 0.002625, mean_q: 0.734437
 59220/100000: episode: 922, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 0.783, mean reward: 0.078 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.095, 10.100], loss: 0.000034, mae: 0.004059, mean_q: 0.734872
 59242/100000: episode: 923, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 0.809, mean reward: 0.037 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.000, 10.249], loss: 0.000142, mae: 0.008826, mean_q: 0.735381
 59245/100000: episode: 924, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 0.754, mean reward: 0.251 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.256, 10.160], loss: 0.000068, mae: 0.006886, mean_q: 0.732432
 59257/100000: episode: 925, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 0.656, mean reward: 0.055 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.055, 10.144], loss: 0.000055, mae: 0.005198, mean_q: 0.735717
 59273/100000: episode: 926, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 0.651, mean reward: 0.041 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.289, 10.100], loss: 0.000089, mae: 0.005816, mean_q: 0.736207
 59285/100000: episode: 927, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 0.683, mean reward: 0.057 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.334, 10.100], loss: 0.000042, mae: 0.003769, mean_q: 0.733301
 59301/100000: episode: 928, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 0.709, mean reward: 0.044 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.176], loss: 0.000038, mae: 0.004408, mean_q: 0.735755
 59311/100000: episode: 929, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 0.807, mean reward: 0.081 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.721, 10.100], loss: 0.000040, mae: 0.003337, mean_q: 0.733260
 59323/100000: episode: 930, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 0.667, mean reward: 0.056 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.726, 10.124], loss: 0.000046, mae: 0.002750, mean_q: 0.734444
 59334/100000: episode: 931, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 0.666, mean reward: 0.061 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.405, 10.100], loss: 0.000050, mae: 0.002591, mean_q: 0.734786
 59344/100000: episode: 932, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 0.781, mean reward: 0.078 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.171, 10.100], loss: 0.000038, mae: 0.003133, mean_q: 0.734493
 59360/100000: episode: 933, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 0.691, mean reward: 0.043 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.060, 10.171], loss: 0.000042, mae: 0.003025, mean_q: 0.734622
 59371/100000: episode: 934, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 0.726, mean reward: 0.066 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.209, 10.138], loss: 0.000021, mae: 0.002311, mean_q: 0.734280
 59387/100000: episode: 935, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 0.604, mean reward: 0.038 [0.000, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.197], loss: 0.000090, mae: 0.005392, mean_q: 0.735267
 59406/100000: episode: 936, duration: 0.104s, episode steps: 19, steps per second: 184, episode reward: 0.725, mean reward: 0.038 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.288, 10.100], loss: 0.000082, mae: 0.005648, mean_q: 0.734072
 59409/100000: episode: 937, duration: 0.026s, episode steps: 3, steps per second: 113, episode reward: 0.709, mean reward: 0.236 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.153], loss: 0.000048, mae: 0.003027, mean_q: 0.732896
 59412/100000: episode: 938, duration: 0.027s, episode steps: 3, steps per second: 111, episode reward: 0.702, mean reward: 0.234 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.100], loss: 0.000050, mae: 0.004742, mean_q: 0.738901
 59428/100000: episode: 939, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 0.619, mean reward: 0.039 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.116], loss: 0.000047, mae: 0.005257, mean_q: 0.733555
 59435/100000: episode: 940, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 0.625, mean reward: 0.089 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.546, 10.165], loss: 0.000067, mae: 0.005798, mean_q: 0.733054
 59454/100000: episode: 941, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 0.674, mean reward: 0.035 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.509, 10.100], loss: 0.000062, mae: 0.004177, mean_q: 0.734693
 59461/100000: episode: 942, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 0.656, mean reward: 0.094 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.037, 10.161], loss: 0.000043, mae: 0.004814, mean_q: 0.734476
 59480/100000: episode: 943, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 0.746, mean reward: 0.039 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.331, 10.100], loss: 0.000099, mae: 0.004467, mean_q: 0.732298
 59496/100000: episode: 944, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 0.631, mean reward: 0.039 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.932, 10.143], loss: 0.000076, mae: 0.004795, mean_q: 0.734314
 59512/100000: episode: 945, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 0.646, mean reward: 0.040 [0.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.192], loss: 0.000068, mae: 0.004433, mean_q: 0.733532
 59528/100000: episode: 946, duration: 0.112s, episode steps: 16, steps per second: 143, episode reward: 0.717, mean reward: 0.045 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.145, 10.100], loss: 0.000048, mae: 0.003851, mean_q: 0.734426
 59531/100000: episode: 947, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 0.719, mean reward: 0.240 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.076, 10.129], loss: 0.000011, mae: 0.002595, mean_q: 0.732789
 59550/100000: episode: 948, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 0.669, mean reward: 0.035 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.363, 10.100], loss: 0.000019, mae: 0.002983, mean_q: 0.733951
 59566/100000: episode: 949, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 0.746, mean reward: 0.047 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.409, 10.100], loss: 0.000048, mae: 0.003158, mean_q: 0.733649
[Info] 2-TH LEVEL FOUND: 0.7532005310058594, Considering 21/100 traces
 59573/100000: episode: 950, duration: 4.532s, episode steps: 7, steps per second: 2, episode reward: 0.651, mean reward: 0.093 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.045, 10.161], loss: 0.000050, mae: 0.004187, mean_q: 0.731512
 59605/100000: episode: 951, duration: 0.190s, episode steps: 32, steps per second: 169, episode reward: 0.670, mean reward: 0.021 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.776, 10.296], loss: 0.000047, mae: 0.003222, mean_q: 0.733659
 59627/100000: episode: 952, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 0.730, mean reward: 0.033 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.103, 10.100], loss: 0.000043, mae: 0.003592, mean_q: 0.733323
 59659/100000: episode: 953, duration: 0.180s, episode steps: 32, steps per second: 177, episode reward: 0.701, mean reward: 0.022 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.379, 10.100], loss: 0.000045, mae: 0.003768, mean_q: 0.732937
 59681/100000: episode: 954, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 0.741, mean reward: 0.034 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.873, 10.100], loss: 0.000081, mae: 0.005232, mean_q: 0.732743
 59703/100000: episode: 955, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 0.833, mean reward: 0.038 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.045, 10.212], loss: 0.000068, mae: 0.005455, mean_q: 0.732086
 59725/100000: episode: 956, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 0.670, mean reward: 0.030 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.042, 10.166], loss: 0.000031, mae: 0.004344, mean_q: 0.732579
 59757/100000: episode: 957, duration: 0.196s, episode steps: 32, steps per second: 164, episode reward: 0.705, mean reward: 0.022 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.445, 10.100], loss: 0.000025, mae: 0.003182, mean_q: 0.732593
 59789/100000: episode: 958, duration: 0.192s, episode steps: 32, steps per second: 167, episode reward: 0.677, mean reward: 0.021 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.619, 10.100], loss: 0.000068, mae: 0.003794, mean_q: 0.731376
 59821/100000: episode: 959, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 0.716, mean reward: 0.022 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.133], loss: 0.000069, mae: 0.005213, mean_q: 0.733570
 59853/100000: episode: 960, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 0.695, mean reward: 0.022 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.171, 10.148], loss: 0.000053, mae: 0.003732, mean_q: 0.731849
 59885/100000: episode: 961, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 0.738, mean reward: 0.023 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.359, 10.100], loss: 0.000032, mae: 0.003462, mean_q: 0.731973
 59917/100000: episode: 962, duration: 0.182s, episode steps: 32, steps per second: 175, episode reward: 0.681, mean reward: 0.021 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.445, 10.100], loss: 0.000041, mae: 0.003919, mean_q: 0.731951
 59949/100000: episode: 963, duration: 0.198s, episode steps: 32, steps per second: 161, episode reward: 0.755, mean reward: 0.024 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.558, 10.314], loss: 0.000058, mae: 0.003418, mean_q: 0.731912
 59981/100000: episode: 964, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 0.752, mean reward: 0.024 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.120, 10.232], loss: 0.000075, mae: 0.003323, mean_q: 0.731763
 60003/100000: episode: 965, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 0.682, mean reward: 0.031 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.183], loss: 0.000055, mae: 0.004297, mean_q: 0.730909
 60035/100000: episode: 966, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 0.760, mean reward: 0.024 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.138, 10.100], loss: 0.000045, mae: 0.004063, mean_q: 0.730589
 60067/100000: episode: 967, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 0.629, mean reward: 0.020 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.231, 10.194], loss: 0.000077, mae: 0.005535, mean_q: 0.731483
 60099/100000: episode: 968, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 0.680, mean reward: 0.021 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.303, 10.100], loss: 0.000070, mae: 0.005244, mean_q: 0.731552
 60121/100000: episode: 969, duration: 0.135s, episode steps: 22, steps per second: 164, episode reward: 0.905, mean reward: 0.041 [0.000, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.087, 10.131], loss: 0.000066, mae: 0.006462, mean_q: 0.729564
 60153/100000: episode: 970, duration: 0.189s, episode steps: 32, steps per second: 170, episode reward: 0.612, mean reward: 0.019 [0.000, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.094, 10.100], loss: 0.000040, mae: 0.003094, mean_q: 0.730773
 60175/100000: episode: 971, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 0.753, mean reward: 0.034 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.071, 10.320], loss: 0.000059, mae: 0.004251, mean_q: 0.730381
 60207/100000: episode: 972, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 0.767, mean reward: 0.024 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.117], loss: 0.000066, mae: 0.004737, mean_q: 0.729373
 60239/100000: episode: 973, duration: 0.206s, episode steps: 32, steps per second: 155, episode reward: 0.783, mean reward: 0.024 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.088, 10.110], loss: 0.000071, mae: 0.005895, mean_q: 0.730377
 60261/100000: episode: 974, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 0.787, mean reward: 0.036 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.156], loss: 0.000087, mae: 0.005442, mean_q: 0.728090
 60293/100000: episode: 975, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 0.676, mean reward: 0.021 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.626, 10.136], loss: 0.000043, mae: 0.004043, mean_q: 0.730220
 60325/100000: episode: 976, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 0.631, mean reward: 0.020 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.336, 10.100], loss: 0.000042, mae: 0.003105, mean_q: 0.729252
 60347/100000: episode: 977, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 0.675, mean reward: 0.031 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.110], loss: 0.000074, mae: 0.004753, mean_q: 0.729226
 60379/100000: episode: 978, duration: 0.188s, episode steps: 32, steps per second: 171, episode reward: 0.681, mean reward: 0.021 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.975, 10.120], loss: 0.000093, mae: 0.006661, mean_q: 0.728425
 60411/100000: episode: 979, duration: 0.193s, episode steps: 32, steps per second: 165, episode reward: 0.754, mean reward: 0.024 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.315, 10.216], loss: 0.000040, mae: 0.004478, mean_q: 0.729759
 60443/100000: episode: 980, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 0.770, mean reward: 0.024 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.343, 10.108], loss: 0.000029, mae: 0.002448, mean_q: 0.728518
 60475/100000: episode: 981, duration: 0.191s, episode steps: 32, steps per second: 167, episode reward: 0.702, mean reward: 0.022 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.248, 10.100], loss: 0.000121, mae: 0.006369, mean_q: 0.729213
 60497/100000: episode: 982, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 0.751, mean reward: 0.034 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.145], loss: 0.000058, mae: 0.003633, mean_q: 0.727347
 60519/100000: episode: 983, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 0.665, mean reward: 0.030 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.476, 10.170], loss: 0.000069, mae: 0.003952, mean_q: 0.727431
 60541/100000: episode: 984, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 0.746, mean reward: 0.034 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.047, 10.157], loss: 0.000061, mae: 0.004584, mean_q: 0.728184
 60573/100000: episode: 985, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 0.669, mean reward: 0.021 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.426, 10.100], loss: 0.000065, mae: 0.004768, mean_q: 0.726497
 60595/100000: episode: 986, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 0.779, mean reward: 0.035 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.332, 10.100], loss: 0.000060, mae: 0.003787, mean_q: 0.727739
 60617/100000: episode: 987, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 0.631, mean reward: 0.029 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.493, 10.100], loss: 0.000087, mae: 0.007533, mean_q: 0.727419
 60649/100000: episode: 988, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 0.701, mean reward: 0.022 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.575, 10.246], loss: 0.000068, mae: 0.005135, mean_q: 0.727757
 60681/100000: episode: 989, duration: 0.196s, episode steps: 32, steps per second: 163, episode reward: 0.704, mean reward: 0.022 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.469, 10.154], loss: 0.000073, mae: 0.005953, mean_q: 0.727524
 60703/100000: episode: 990, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 0.724, mean reward: 0.033 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.660, 10.100], loss: 0.000042, mae: 0.003156, mean_q: 0.726861
 60725/100000: episode: 991, duration: 0.154s, episode steps: 22, steps per second: 142, episode reward: 0.827, mean reward: 0.038 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.181], loss: 0.000052, mae: 0.003873, mean_q: 0.726193
 60747/100000: episode: 992, duration: 0.136s, episode steps: 22, steps per second: 161, episode reward: 0.722, mean reward: 0.033 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.654, 10.171], loss: 0.000076, mae: 0.004339, mean_q: 0.726446
 60779/100000: episode: 993, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.695, mean reward: 0.022 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.613, 10.100], loss: 0.000076, mae: 0.004300, mean_q: 0.726624
 60811/100000: episode: 994, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 0.778, mean reward: 0.024 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.578, 10.116], loss: 0.000077, mae: 0.004777, mean_q: 0.724929
 60833/100000: episode: 995, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.682, mean reward: 0.031 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.441, 10.100], loss: 0.000045, mae: 0.002801, mean_q: 0.725380
 60855/100000: episode: 996, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 0.832, mean reward: 0.038 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.631, 10.325], loss: 0.000075, mae: 0.005407, mean_q: 0.724999
 60887/100000: episode: 997, duration: 0.208s, episode steps: 32, steps per second: 154, episode reward: 0.671, mean reward: 0.021 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.821, 10.100], loss: 0.000059, mae: 0.005726, mean_q: 0.725714
 60919/100000: episode: 998, duration: 0.193s, episode steps: 32, steps per second: 165, episode reward: 0.772, mean reward: 0.024 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.028, 10.247], loss: 0.000081, mae: 0.004535, mean_q: 0.724657
 60951/100000: episode: 999, duration: 0.179s, episode steps: 32, steps per second: 178, episode reward: 0.728, mean reward: 0.023 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.042, 10.250], loss: 0.000033, mae: 0.002837, mean_q: 0.724799
 60983/100000: episode: 1000, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 0.673, mean reward: 0.021 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.161, 10.127], loss: 0.000049, mae: 0.004377, mean_q: 0.723894
 61015/100000: episode: 1001, duration: 0.180s, episode steps: 32, steps per second: 177, episode reward: 0.748, mean reward: 0.023 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.509, 10.198], loss: 0.000103, mae: 0.006119, mean_q: 0.724639
 61047/100000: episode: 1002, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 0.726, mean reward: 0.023 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-2.158, 10.136], loss: 0.000071, mae: 0.004968, mean_q: 0.723293
 61069/100000: episode: 1003, duration: 0.138s, episode steps: 22, steps per second: 160, episode reward: 0.747, mean reward: 0.034 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.112, 10.100], loss: 0.000039, mae: 0.002754, mean_q: 0.724139
 61101/100000: episode: 1004, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 0.733, mean reward: 0.023 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.256, 10.225], loss: 0.000108, mae: 0.005565, mean_q: 0.724993
 61133/100000: episode: 1005, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 0.766, mean reward: 0.024 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.608, 10.273], loss: 0.000102, mae: 0.005910, mean_q: 0.724453
 61155/100000: episode: 1006, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 1.000, mean reward: 0.045 [0.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.291], loss: 0.000028, mae: 0.002810, mean_q: 0.722431
 61187/100000: episode: 1007, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 0.711, mean reward: 0.022 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.157, 10.285], loss: 0.000123, mae: 0.004057, mean_q: 0.723959
 61209/100000: episode: 1008, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 0.822, mean reward: 0.037 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.113, 10.109], loss: 0.000080, mae: 0.004582, mean_q: 0.723608
 61231/100000: episode: 1009, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 0.678, mean reward: 0.031 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.510, 10.321], loss: 0.000056, mae: 0.003076, mean_q: 0.722439
 61253/100000: episode: 1010, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 0.813, mean reward: 0.037 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.094, 10.268], loss: 0.000044, mae: 0.002779, mean_q: 0.722708
 61285/100000: episode: 1011, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 0.729, mean reward: 0.023 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.095, 10.100], loss: 0.000030, mae: 0.002728, mean_q: 0.722259
 61317/100000: episode: 1012, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 0.668, mean reward: 0.021 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.292, 10.100], loss: 0.000085, mae: 0.003949, mean_q: 0.721950
 61349/100000: episode: 1013, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 0.719, mean reward: 0.022 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.492, 10.291], loss: 0.000126, mae: 0.005739, mean_q: 0.722414
 61381/100000: episode: 1014, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 0.791, mean reward: 0.025 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.159], loss: 0.000042, mae: 0.004006, mean_q: 0.722021
 61403/100000: episode: 1015, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 0.711, mean reward: 0.032 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.192, 10.279], loss: 0.000085, mae: 0.004054, mean_q: 0.722268
 61435/100000: episode: 1016, duration: 0.189s, episode steps: 32, steps per second: 169, episode reward: 0.739, mean reward: 0.023 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.910, 10.249], loss: 0.000081, mae: 0.003678, mean_q: 0.722264
 61467/100000: episode: 1017, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.788, mean reward: 0.025 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.582, 10.137], loss: 0.000058, mae: 0.004168, mean_q: 0.721881
 61489/100000: episode: 1018, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 0.699, mean reward: 0.032 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.259], loss: 0.000063, mae: 0.005476, mean_q: 0.722061
 61511/100000: episode: 1019, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 0.734, mean reward: 0.033 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.042, 10.254], loss: 0.000090, mae: 0.004224, mean_q: 0.721576
 61543/100000: episode: 1020, duration: 0.190s, episode steps: 32, steps per second: 169, episode reward: 0.658, mean reward: 0.021 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.230, 10.207], loss: 0.000062, mae: 0.004870, mean_q: 0.722120
 61565/100000: episode: 1021, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 0.751, mean reward: 0.034 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.037, 10.186], loss: 0.000088, mae: 0.006724, mean_q: 0.720266
 61587/100000: episode: 1022, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.877, mean reward: 0.040 [0.000, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.373, 10.158], loss: 0.000085, mae: 0.006878, mean_q: 0.721243
 61609/100000: episode: 1023, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 0.892, mean reward: 0.041 [0.000, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.396, 10.255], loss: 0.000143, mae: 0.009296, mean_q: 0.721045
 61631/100000: episode: 1024, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 0.844, mean reward: 0.038 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.278, 10.231], loss: 0.000065, mae: 0.005645, mean_q: 0.719353
 61653/100000: episode: 1025, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 0.724, mean reward: 0.033 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.414, 10.160], loss: 0.000121, mae: 0.003705, mean_q: 0.721740
 61675/100000: episode: 1026, duration: 0.145s, episode steps: 22, steps per second: 152, episode reward: 0.701, mean reward: 0.032 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.609, 10.220], loss: 0.000145, mae: 0.007698, mean_q: 0.720785
 61707/100000: episode: 1027, duration: 0.192s, episode steps: 32, steps per second: 167, episode reward: 0.662, mean reward: 0.021 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.950, 10.117], loss: 0.000055, mae: 0.003843, mean_q: 0.720622
 61739/100000: episode: 1028, duration: 0.197s, episode steps: 32, steps per second: 162, episode reward: 0.734, mean reward: 0.023 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.273, 10.259], loss: 0.000124, mae: 0.007735, mean_q: 0.720876
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7532005310058594
2
 61771/100000: episode: 1029, duration: 4.532s, episode steps: 32, steps per second: 7, episode reward: 0.729, mean reward: 0.023 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.275, 10.100], loss: 0.000129, mae: 0.005220, mean_q: 0.721052
 61872/100000: episode: 1030, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.292, 10.393], loss: 0.000102, mae: 0.004749, mean_q: 0.720721
 61973/100000: episode: 1031, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.447, 10.100], loss: 0.000098, mae: 0.004542, mean_q: 0.721386
 62074/100000: episode: 1032, duration: 0.600s, episode steps: 101, steps per second: 168, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.520, 10.100], loss: 0.000074, mae: 0.004221, mean_q: 0.721133
 62175/100000: episode: 1033, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.159, 10.140], loss: 0.000090, mae: 0.004966, mean_q: 0.720931
 62276/100000: episode: 1034, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.100], loss: 0.000091, mae: 0.004734, mean_q: 0.721233
 62377/100000: episode: 1035, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.100], loss: 0.000076, mae: 0.004620, mean_q: 0.721076
 62478/100000: episode: 1036, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.815, 10.100], loss: 0.000093, mae: 0.004615, mean_q: 0.721269
 62579/100000: episode: 1037, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.064, 10.181], loss: 0.000090, mae: 0.004585, mean_q: 0.721140
 62680/100000: episode: 1038, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.750, 10.146], loss: 0.000094, mae: 0.005522, mean_q: 0.720533
 62781/100000: episode: 1039, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.100], loss: 0.000074, mae: 0.003966, mean_q: 0.721262
 62882/100000: episode: 1040, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.258, 10.100], loss: 0.000108, mae: 0.004617, mean_q: 0.721598
 62983/100000: episode: 1041, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.171, 10.418], loss: 0.000108, mae: 0.004929, mean_q: 0.722147
 63084/100000: episode: 1042, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.452, 10.311], loss: 0.000100, mae: 0.005158, mean_q: 0.723073
 63185/100000: episode: 1043, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.779, 10.300], loss: 0.000083, mae: 0.004351, mean_q: 0.722883
 63286/100000: episode: 1044, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.073, 10.483], loss: 0.000104, mae: 0.005134, mean_q: 0.723230
 63387/100000: episode: 1045, duration: 0.605s, episode steps: 101, steps per second: 167, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.476, 10.346], loss: 0.000064, mae: 0.004144, mean_q: 0.723611
 63488/100000: episode: 1046, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.137, 10.100], loss: 0.000095, mae: 0.004383, mean_q: 0.724035
 63589/100000: episode: 1047, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.405, 10.170], loss: 0.000066, mae: 0.004063, mean_q: 0.724339
 63690/100000: episode: 1048, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.841, mean reward: 0.008 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.369, 10.100], loss: 0.000067, mae: 0.004112, mean_q: 0.724124
 63791/100000: episode: 1049, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.146, 10.205], loss: 0.000067, mae: 0.003558, mean_q: 0.724158
 63892/100000: episode: 1050, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.598, 10.149], loss: 0.000073, mae: 0.003984, mean_q: 0.724647
 63993/100000: episode: 1051, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.899, mean reward: 0.009 [0.000, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.022, 10.100], loss: 0.000095, mae: 0.004733, mean_q: 0.724735
 64094/100000: episode: 1052, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.684, 10.352], loss: 0.000048, mae: 0.003480, mean_q: 0.724767
 64195/100000: episode: 1053, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.046, 10.100], loss: 0.000047, mae: 0.003188, mean_q: 0.724553
 64296/100000: episode: 1054, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.705, 10.167], loss: 0.000067, mae: 0.003697, mean_q: 0.725115
 64397/100000: episode: 1055, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.964, 10.153], loss: 0.000062, mae: 0.004202, mean_q: 0.725519
 64498/100000: episode: 1056, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.896, 10.100], loss: 0.000045, mae: 0.003181, mean_q: 0.725947
 64599/100000: episode: 1057, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.780, 10.290], loss: 0.000057, mae: 0.003222, mean_q: 0.726687
 64700/100000: episode: 1058, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.652, 10.100], loss: 0.000048, mae: 0.003323, mean_q: 0.727276
 64801/100000: episode: 1059, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.569, 10.100], loss: 0.000047, mae: 0.002832, mean_q: 0.727497
 64902/100000: episode: 1060, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.816, 10.206], loss: 0.000045, mae: 0.002940, mean_q: 0.728169
 65003/100000: episode: 1061, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.634, 10.100], loss: 0.000034, mae: 0.002888, mean_q: 0.728255
 65104/100000: episode: 1062, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.890, 10.100], loss: 0.000037, mae: 0.002887, mean_q: 0.728425
 65205/100000: episode: 1063, duration: 0.612s, episode steps: 101, steps per second: 165, episode reward: 0.674, mean reward: 0.007 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.741, 10.100], loss: 0.000027, mae: 0.002408, mean_q: 0.729009
 65306/100000: episode: 1064, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.858, mean reward: 0.008 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.715, 10.375], loss: 0.000048, mae: 0.003193, mean_q: 0.728567
 65407/100000: episode: 1065, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.257, 10.239], loss: 0.000039, mae: 0.003120, mean_q: 0.729196
 65508/100000: episode: 1066, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.171, 10.363], loss: 0.000040, mae: 0.002764, mean_q: 0.729160
 65609/100000: episode: 1067, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.742, 10.385], loss: 0.000037, mae: 0.002862, mean_q: 0.729404
 65710/100000: episode: 1068, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.462, 10.197], loss: 0.000031, mae: 0.002056, mean_q: 0.729420
 65811/100000: episode: 1069, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.073, 10.424], loss: 0.000035, mae: 0.002808, mean_q: 0.730107
 65912/100000: episode: 1070, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.158, 10.100], loss: 0.000055, mae: 0.003205, mean_q: 0.730134
 66013/100000: episode: 1071, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.301, 10.358], loss: 0.000023, mae: 0.002048, mean_q: 0.730340
 66114/100000: episode: 1072, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.725, 10.186], loss: 0.000031, mae: 0.002653, mean_q: 0.730749
 66215/100000: episode: 1073, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.743, 10.204], loss: 0.000021, mae: 0.002268, mean_q: 0.730559
 66316/100000: episode: 1074, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.335, 10.100], loss: 0.000025, mae: 0.002683, mean_q: 0.730622
 66417/100000: episode: 1075, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.423, 10.100], loss: 0.000007, mae: 0.001251, mean_q: 0.730598
 66518/100000: episode: 1076, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.329, 10.100], loss: 0.000019, mae: 0.002355, mean_q: 0.730932
 66619/100000: episode: 1077, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.811, 10.100], loss: 0.000011, mae: 0.001326, mean_q: 0.730540
 66720/100000: episode: 1078, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.840, 10.100], loss: 0.000011, mae: 0.001687, mean_q: 0.730622
 66821/100000: episode: 1079, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.001, 10.100], loss: 0.000011, mae: 0.001689, mean_q: 0.730993
 66922/100000: episode: 1080, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.468, 10.133], loss: 0.000017, mae: 0.001910, mean_q: 0.730956
 67023/100000: episode: 1081, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.126], loss: 0.000013, mae: 0.001723, mean_q: 0.731472
 67124/100000: episode: 1082, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.282, 10.120], loss: 0.000021, mae: 0.002042, mean_q: 0.731769
 67225/100000: episode: 1083, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.757, 10.100], loss: 0.000018, mae: 0.002178, mean_q: 0.731695
 67326/100000: episode: 1084, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.100], loss: 0.000016, mae: 0.002198, mean_q: 0.732011
 67427/100000: episode: 1085, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.186, 10.100], loss: 0.000013, mae: 0.001806, mean_q: 0.732819
 67528/100000: episode: 1086, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.032, 10.100], loss: 0.000020, mae: 0.002054, mean_q: 0.733634
 67629/100000: episode: 1087, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.255, 10.100], loss: 0.000020, mae: 0.002898, mean_q: 0.732919
 67730/100000: episode: 1088, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.100], loss: 0.000011, mae: 0.001470, mean_q: 0.732930
 67831/100000: episode: 1089, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.157, 10.289], loss: 0.000016, mae: 0.002146, mean_q: 0.733740
 67932/100000: episode: 1090, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.729, 10.335], loss: 0.000014, mae: 0.001925, mean_q: 0.733729
 68033/100000: episode: 1091, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.207, 10.241], loss: 0.000012, mae: 0.001757, mean_q: 0.734353
 68134/100000: episode: 1092, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.121, 10.100], loss: 0.000015, mae: 0.002332, mean_q: 0.734330
 68235/100000: episode: 1093, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.100], loss: 0.000012, mae: 0.001954, mean_q: 0.734500
 68336/100000: episode: 1094, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.662, mean reward: 0.007 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.100], loss: 0.000018, mae: 0.002173, mean_q: 0.735328
 68437/100000: episode: 1095, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.990, 10.100], loss: 0.000015, mae: 0.001981, mean_q: 0.735298
 68538/100000: episode: 1096, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.594, 10.100], loss: 0.000017, mae: 0.001988, mean_q: 0.736223
 68639/100000: episode: 1097, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.846, mean reward: 0.008 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.566, 10.104], loss: 0.000015, mae: 0.002445, mean_q: 0.735412
 68740/100000: episode: 1098, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.245, 10.207], loss: 0.000015, mae: 0.001776, mean_q: 0.736143
 68841/100000: episode: 1099, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.100], loss: 0.000011, mae: 0.002162, mean_q: 0.736513
 68942/100000: episode: 1100, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.612, 10.100], loss: 0.000009, mae: 0.002265, mean_q: 0.736443
 69043/100000: episode: 1101, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.336, 10.107], loss: 0.000008, mae: 0.001468, mean_q: 0.737006
 69144/100000: episode: 1102, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.718, 10.203], loss: 0.000011, mae: 0.001650, mean_q: 0.737507
 69245/100000: episode: 1103, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.295, 10.197], loss: 0.000017, mae: 0.002350, mean_q: 0.737846
 69346/100000: episode: 1104, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.529, 10.100], loss: 0.000011, mae: 0.001424, mean_q: 0.737906
 69447/100000: episode: 1105, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.078, 10.161], loss: 0.000014, mae: 0.002435, mean_q: 0.738281
 69548/100000: episode: 1106, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.685, 10.100], loss: 0.000007, mae: 0.001384, mean_q: 0.738223
 69649/100000: episode: 1107, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.015, 10.360], loss: 0.000006, mae: 0.001193, mean_q: 0.739095
 69750/100000: episode: 1108, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.556, 10.129], loss: 0.000013, mae: 0.001847, mean_q: 0.739138
 69851/100000: episode: 1109, duration: 0.546s, episode steps: 101, steps per second: 185, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.625, 10.195], loss: 0.000012, mae: 0.001719, mean_q: 0.738927
 69952/100000: episode: 1110, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.259], loss: 0.000012, mae: 0.002209, mean_q: 0.739254
 70053/100000: episode: 1111, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.104], loss: 0.000011, mae: 0.001866, mean_q: 0.740062
 70154/100000: episode: 1112, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.100], loss: 0.000015, mae: 0.002254, mean_q: 0.740071
 70255/100000: episode: 1113, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.906, 10.352], loss: 0.000011, mae: 0.001835, mean_q: 0.740589
 70356/100000: episode: 1114, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.148, 10.116], loss: 0.000008, mae: 0.001482, mean_q: 0.741023
 70457/100000: episode: 1115, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.322, 10.358], loss: 0.000010, mae: 0.001756, mean_q: 0.741175
 70558/100000: episode: 1116, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.418, 10.176], loss: 0.000015, mae: 0.001978, mean_q: 0.741334
 70659/100000: episode: 1117, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.100], loss: 0.000016, mae: 0.002315, mean_q: 0.741835
 70760/100000: episode: 1118, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.906, mean reward: 0.009 [0.000, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.962, 10.441], loss: 0.000015, mae: 0.001915, mean_q: 0.741909
 70861/100000: episode: 1119, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.611, 10.100], loss: 0.000019, mae: 0.002029, mean_q: 0.742541
 70962/100000: episode: 1120, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.874, 10.100], loss: 0.000010, mae: 0.001664, mean_q: 0.743226
 71063/100000: episode: 1121, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.100], loss: 0.000014, mae: 0.002184, mean_q: 0.743056
 71164/100000: episode: 1122, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.950, 10.100], loss: 0.000012, mae: 0.001634, mean_q: 0.743063
 71265/100000: episode: 1123, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.140, 10.100], loss: 0.000009, mae: 0.001705, mean_q: 0.743670
 71366/100000: episode: 1124, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.098, 10.100], loss: 0.000006, mae: 0.001299, mean_q: 0.744330
 71467/100000: episode: 1125, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.621, 10.175], loss: 0.000007, mae: 0.001370, mean_q: 0.744670
 71568/100000: episode: 1126, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.383, 10.313], loss: 0.000017, mae: 0.002280, mean_q: 0.745012
 71669/100000: episode: 1127, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.100], loss: 0.000017, mae: 0.001854, mean_q: 0.745064
 71770/100000: episode: 1128, duration: 0.613s, episode steps: 101, steps per second: 165, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.702, 10.127], loss: 0.000008, mae: 0.001616, mean_q: 0.745497
[Info] 1-TH LEVEL FOUND: 0.7608287334442139, Considering 10/100 traces
 71871/100000: episode: 1129, duration: 5.108s, episode steps: 101, steps per second: 20, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.175, 10.100], loss: 0.000012, mae: 0.001845, mean_q: 0.745858
 71878/100000: episode: 1130, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 0.691, mean reward: 0.099 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.320], loss: 0.000016, mae: 0.001602, mean_q: 0.746256
 71893/100000: episode: 1131, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 0.785, mean reward: 0.052 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.448], loss: 0.000013, mae: 0.001603, mean_q: 0.745504
 71901/100000: episode: 1132, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 0.724, mean reward: 0.091 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-1.500, 10.447], loss: 0.000010, mae: 0.001262, mean_q: 0.745732
 71905/100000: episode: 1133, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.666, mean reward: 0.167 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.367], loss: 0.000002, mae: 0.001147, mean_q: 0.747698
 71928/100000: episode: 1134, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 0.856, mean reward: 0.037 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.341], loss: 0.000009, mae: 0.001424, mean_q: 0.745618
 71930/100000: episode: 1135, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.678, mean reward: 0.339 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.397], loss: 0.000001, mae: 0.000870, mean_q: 0.743593
 71948/100000: episode: 1136, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 0.692, mean reward: 0.038 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.566, 10.392], loss: 0.000022, mae: 0.002586, mean_q: 0.746541
 71963/100000: episode: 1137, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 0.736, mean reward: 0.049 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.398], loss: 0.000021, mae: 0.003483, mean_q: 0.746029
 71965/100000: episode: 1138, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.721, mean reward: 0.361 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.345], loss: 0.000004, mae: 0.002799, mean_q: 0.747295
 71987/100000: episode: 1139, duration: 0.150s, episode steps: 22, steps per second: 146, episode reward: 0.839, mean reward: 0.038 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.115, 10.615], loss: 0.000013, mae: 0.001982, mean_q: 0.747078
 71989/100000: episode: 1140, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.700, mean reward: 0.350 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.381], loss: 0.000001, mae: 0.001294, mean_q: 0.747290
 72004/100000: episode: 1141, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 0.774, mean reward: 0.052 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.941, 10.379], loss: 0.000007, mae: 0.001385, mean_q: 0.747205
 72019/100000: episode: 1142, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 0.727, mean reward: 0.048 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.416], loss: 0.000015, mae: 0.002091, mean_q: 0.745775
 72044/100000: episode: 1143, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 0.781, mean reward: 0.031 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.529], loss: 0.000012, mae: 0.001445, mean_q: 0.747266
 72059/100000: episode: 1144, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 0.650, mean reward: 0.043 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.318, 10.249], loss: 0.000027, mae: 0.002758, mean_q: 0.748123
 72078/100000: episode: 1145, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 0.782, mean reward: 0.041 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.534], loss: 0.000017, mae: 0.001573, mean_q: 0.746844
 72086/100000: episode: 1146, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 0.696, mean reward: 0.087 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.843, 10.376], loss: 0.000004, mae: 0.001649, mean_q: 0.746195
 72111/100000: episode: 1147, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 0.839, mean reward: 0.034 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.092, 10.403], loss: 0.000013, mae: 0.001450, mean_q: 0.747128
 72136/100000: episode: 1148, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 0.818, mean reward: 0.033 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.581, 10.373], loss: 0.000034, mae: 0.003755, mean_q: 0.747715
 72158/100000: episode: 1149, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 0.793, mean reward: 0.036 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.199, 10.491], loss: 0.000018, mae: 0.002680, mean_q: 0.747750
 72173/100000: episode: 1150, duration: 0.106s, episode steps: 15, steps per second: 141, episode reward: 0.630, mean reward: 0.042 [0.000, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.577, 10.276], loss: 0.000017, mae: 0.001855, mean_q: 0.748686
 72175/100000: episode: 1151, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.795, mean reward: 0.397 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.917, 10.341], loss: 0.000010, mae: 0.002958, mean_q: 0.748979
 72197/100000: episode: 1152, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 0.889, mean reward: 0.040 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.945, 10.488], loss: 0.000041, mae: 0.004311, mean_q: 0.748047
 72216/100000: episode: 1153, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 0.827, mean reward: 0.044 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.566, 10.435], loss: 0.000016, mae: 0.002598, mean_q: 0.747819
 72238/100000: episode: 1154, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 0.736, mean reward: 0.033 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.312, 10.308], loss: 0.000024, mae: 0.001909, mean_q: 0.748032
 72245/100000: episode: 1155, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 0.683, mean reward: 0.098 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.387], loss: 0.000006, mae: 0.001852, mean_q: 0.748865
 72264/100000: episode: 1156, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 0.824, mean reward: 0.043 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.310, 10.465], loss: 0.000017, mae: 0.002123, mean_q: 0.748113
 72266/100000: episode: 1157, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.754, mean reward: 0.377 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.762, 10.421], loss: 0.000029, mae: 0.003635, mean_q: 0.747581
 72291/100000: episode: 1158, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 0.806, mean reward: 0.032 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.754, 10.406], loss: 0.000039, mae: 0.003968, mean_q: 0.747948
 72295/100000: episode: 1159, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.646, mean reward: 0.161 [0.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.586, 10.288], loss: 0.000045, mae: 0.004044, mean_q: 0.751835
 72320/100000: episode: 1160, duration: 0.162s, episode steps: 25, steps per second: 155, episode reward: 0.817, mean reward: 0.033 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.226, 10.575], loss: 0.000061, mae: 0.004334, mean_q: 0.748735
 72338/100000: episode: 1161, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 0.731, mean reward: 0.041 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.593, 10.275], loss: 0.000047, mae: 0.003795, mean_q: 0.750183
 72357/100000: episode: 1162, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 0.814, mean reward: 0.043 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.425, 10.358], loss: 0.000021, mae: 0.001828, mean_q: 0.748825
 72372/100000: episode: 1163, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 0.700, mean reward: 0.047 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.389], loss: 0.000011, mae: 0.001846, mean_q: 0.750090
 72374/100000: episode: 1164, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.698, mean reward: 0.349 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.389], loss: 0.000002, mae: 0.001861, mean_q: 0.747086
 72399/100000: episode: 1165, duration: 0.148s, episode steps: 25, steps per second: 168, episode reward: 0.828, mean reward: 0.033 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.442], loss: 0.000041, mae: 0.002442, mean_q: 0.748825
 72403/100000: episode: 1166, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 0.652, mean reward: 0.163 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.600, 10.277], loss: 0.000092, mae: 0.003244, mean_q: 0.749385
 72410/100000: episode: 1167, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 0.715, mean reward: 0.102 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.766, 10.366], loss: 0.000075, mae: 0.006886, mean_q: 0.749900
 72418/100000: episode: 1168, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 0.647, mean reward: 0.081 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.354], loss: 0.000050, mae: 0.006859, mean_q: 0.749015
 72437/100000: episode: 1169, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 0.761, mean reward: 0.040 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.378], loss: 0.000037, mae: 0.004642, mean_q: 0.748395
 72462/100000: episode: 1170, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 0.831, mean reward: 0.033 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.522, 10.259], loss: 0.000041, mae: 0.002638, mean_q: 0.748596
 72481/100000: episode: 1171, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 0.788, mean reward: 0.041 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.710, 10.468], loss: 0.000062, mae: 0.004433, mean_q: 0.749351
 72488/100000: episode: 1172, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 0.713, mean reward: 0.102 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.424], loss: 0.000006, mae: 0.002277, mean_q: 0.750192
 72513/100000: episode: 1173, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 0.806, mean reward: 0.032 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.722, 10.354], loss: 0.000043, mae: 0.003031, mean_q: 0.748857
 72532/100000: episode: 1174, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 0.709, mean reward: 0.037 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.395], loss: 0.000031, mae: 0.002038, mean_q: 0.749506
 72555/100000: episode: 1175, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 0.878, mean reward: 0.038 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.365], loss: 0.000054, mae: 0.004577, mean_q: 0.748041
 72570/100000: episode: 1176, duration: 0.107s, episode steps: 15, steps per second: 140, episode reward: 0.719, mean reward: 0.048 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.666, 10.432], loss: 0.000037, mae: 0.004051, mean_q: 0.750128
 72577/100000: episode: 1177, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 0.708, mean reward: 0.101 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.925, 10.348], loss: 0.000055, mae: 0.003472, mean_q: 0.750765
 72585/100000: episode: 1178, duration: 0.050s, episode steps: 8, steps per second: 162, episode reward: 0.692, mean reward: 0.086 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.421], loss: 0.000100, mae: 0.004840, mean_q: 0.747449
 72600/100000: episode: 1179, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 0.649, mean reward: 0.043 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.091, 10.208], loss: 0.000012, mae: 0.003402, mean_q: 0.749277
 72619/100000: episode: 1180, duration: 0.123s, episode steps: 19, steps per second: 155, episode reward: 0.842, mean reward: 0.044 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.403, 10.446], loss: 0.000030, mae: 0.003541, mean_q: 0.748238
 72621/100000: episode: 1181, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.701, mean reward: 0.351 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.371], loss: 0.000159, mae: 0.004440, mean_q: 0.749158
 72625/100000: episode: 1182, duration: 0.034s, episode steps: 4, steps per second: 119, episode reward: 0.669, mean reward: 0.167 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.376], loss: 0.000036, mae: 0.002462, mean_q: 0.749636
 72629/100000: episode: 1183, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 0.629, mean reward: 0.157 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.301], loss: 0.000050, mae: 0.004329, mean_q: 0.749026
 72633/100000: episode: 1184, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 0.654, mean reward: 0.163 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.369], loss: 0.000018, mae: 0.003202, mean_q: 0.751421
 72656/100000: episode: 1185, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 0.783, mean reward: 0.034 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.553, 10.334], loss: 0.000042, mae: 0.003726, mean_q: 0.747981
 72663/100000: episode: 1186, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 0.715, mean reward: 0.102 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-1.367, 10.419], loss: 0.000035, mae: 0.004011, mean_q: 0.748111
 72685/100000: episode: 1187, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 0.836, mean reward: 0.038 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.506, 10.407], loss: 0.000018, mae: 0.002754, mean_q: 0.750226
 72704/100000: episode: 1188, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 0.925, mean reward: 0.049 [0.000, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.518, 10.546], loss: 0.000034, mae: 0.001510, mean_q: 0.749599
 72726/100000: episode: 1189, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 0.731, mean reward: 0.033 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.848, 10.310], loss: 0.000035, mae: 0.002002, mean_q: 0.749392
 72745/100000: episode: 1190, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 0.767, mean reward: 0.040 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.473], loss: 0.000034, mae: 0.003771, mean_q: 0.748740
 72770/100000: episode: 1191, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 0.808, mean reward: 0.032 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.451], loss: 0.000047, mae: 0.003659, mean_q: 0.749938
 72789/100000: episode: 1192, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 0.837, mean reward: 0.044 [0.000, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.782, 10.382], loss: 0.000031, mae: 0.002575, mean_q: 0.749808
 72811/100000: episode: 1193, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 0.676, mean reward: 0.031 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.333], loss: 0.000073, mae: 0.004840, mean_q: 0.749840
 72830/100000: episode: 1194, duration: 0.119s, episode steps: 19, steps per second: 159, episode reward: 0.740, mean reward: 0.039 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.433], loss: 0.000057, mae: 0.003610, mean_q: 0.750403
 72849/100000: episode: 1195, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 0.670, mean reward: 0.035 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.247], loss: 0.000039, mae: 0.003359, mean_q: 0.749922
 72874/100000: episode: 1196, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 0.744, mean reward: 0.030 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.275], loss: 0.000045, mae: 0.002781, mean_q: 0.749290
 72882/100000: episode: 1197, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 0.775, mean reward: 0.097 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.529], loss: 0.000100, mae: 0.004068, mean_q: 0.751534
 72905/100000: episode: 1198, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 0.811, mean reward: 0.035 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.328], loss: 0.000056, mae: 0.003578, mean_q: 0.749539
 72909/100000: episode: 1199, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 0.689, mean reward: 0.172 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.362], loss: 0.000014, mae: 0.003098, mean_q: 0.747108
 72927/100000: episode: 1200, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 0.815, mean reward: 0.045 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.582], loss: 0.000051, mae: 0.003199, mean_q: 0.750001
 72950/100000: episode: 1201, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 0.831, mean reward: 0.036 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.536, 10.242], loss: 0.000057, mae: 0.003647, mean_q: 0.749440
 72975/100000: episode: 1202, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 0.830, mean reward: 0.033 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.289, 10.324], loss: 0.000050, mae: 0.002651, mean_q: 0.750250
 72979/100000: episode: 1203, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.631, mean reward: 0.158 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.321], loss: 0.000032, mae: 0.003267, mean_q: 0.749006
 72987/100000: episode: 1204, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 0.720, mean reward: 0.090 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.157, 10.354], loss: 0.000027, mae: 0.002497, mean_q: 0.749935
 73005/100000: episode: 1205, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 0.737, mean reward: 0.041 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.013, 10.332], loss: 0.000085, mae: 0.003751, mean_q: 0.750644
 73009/100000: episode: 1206, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 0.710, mean reward: 0.178 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.484, 10.359], loss: 0.000017, mae: 0.002269, mean_q: 0.748848
 73032/100000: episode: 1207, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 0.986, mean reward: 0.043 [0.000, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.343, 10.626], loss: 0.000050, mae: 0.002991, mean_q: 0.750221
 73034/100000: episode: 1208, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.702, mean reward: 0.351 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.444], loss: 0.000166, mae: 0.007996, mean_q: 0.745186
 73059/100000: episode: 1209, duration: 0.158s, episode steps: 25, steps per second: 158, episode reward: 0.817, mean reward: 0.033 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.376, 10.334], loss: 0.000051, mae: 0.004304, mean_q: 0.750538
 73074/100000: episode: 1210, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 0.682, mean reward: 0.045 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.537, 10.340], loss: 0.000091, mae: 0.005299, mean_q: 0.749851
 73082/100000: episode: 1211, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 0.713, mean reward: 0.089 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.367], loss: 0.000043, mae: 0.004440, mean_q: 0.748878
 73090/100000: episode: 1212, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 0.705, mean reward: 0.088 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.398], loss: 0.000104, mae: 0.004635, mean_q: 0.750609
 73112/100000: episode: 1213, duration: 0.121s, episode steps: 22, steps per second: 183, episode reward: 0.726, mean reward: 0.033 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.516, 10.363], loss: 0.000068, mae: 0.003423, mean_q: 0.750381
 73116/100000: episode: 1214, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 0.661, mean reward: 0.165 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.261], loss: 0.000051, mae: 0.004820, mean_q: 0.752877
 73141/100000: episode: 1215, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 0.730, mean reward: 0.029 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.094, 10.315], loss: 0.000129, mae: 0.005449, mean_q: 0.749710
 73145/100000: episode: 1216, duration: 0.031s, episode steps: 4, steps per second: 130, episode reward: 0.690, mean reward: 0.173 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.417], loss: 0.000043, mae: 0.007829, mean_q: 0.757852
 73170/100000: episode: 1217, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 0.765, mean reward: 0.031 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.389], loss: 0.000095, mae: 0.006660, mean_q: 0.749807
 73177/100000: episode: 1218, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 0.690, mean reward: 0.099 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.387], loss: 0.000077, mae: 0.005324, mean_q: 0.751417
[Info] 2-TH LEVEL FOUND: 0.7635816335678101, Considering 22/100 traces
 73199/100000: episode: 1219, duration: 4.637s, episode steps: 22, steps per second: 5, episode reward: 0.745, mean reward: 0.034 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.504], loss: 0.000077, mae: 0.003915, mean_q: 0.750010
 73250/100000: episode: 1220, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.727, mean reward: 0.014 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.438, 10.147], loss: 0.000069, mae: 0.004162, mean_q: 0.751289
 73301/100000: episode: 1221, duration: 0.313s, episode steps: 51, steps per second: 163, episode reward: 0.828, mean reward: 0.016 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.687, 10.463], loss: 0.000058, mae: 0.003709, mean_q: 0.751171
 73352/100000: episode: 1222, duration: 0.305s, episode steps: 51, steps per second: 167, episode reward: 0.768, mean reward: 0.015 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.535, 10.343], loss: 0.000047, mae: 0.002795, mean_q: 0.751260
 73403/100000: episode: 1223, duration: 0.317s, episode steps: 51, steps per second: 161, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.498, 10.100], loss: 0.000064, mae: 0.004173, mean_q: 0.751245
 73454/100000: episode: 1224, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 0.674, mean reward: 0.013 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.642, 10.100], loss: 0.000052, mae: 0.003603, mean_q: 0.750782
 73505/100000: episode: 1225, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 0.765, mean reward: 0.015 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.576, 10.100], loss: 0.000044, mae: 0.003254, mean_q: 0.751189
 73556/100000: episode: 1226, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 0.703, mean reward: 0.014 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.507, 10.228], loss: 0.000056, mae: 0.002823, mean_q: 0.751578
 73607/100000: episode: 1227, duration: 0.294s, episode steps: 51, steps per second: 174, episode reward: 0.823, mean reward: 0.016 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.429, 10.100], loss: 0.000038, mae: 0.002456, mean_q: 0.751639
 73658/100000: episode: 1228, duration: 0.345s, episode steps: 51, steps per second: 148, episode reward: 0.766, mean reward: 0.015 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.579, 10.100], loss: 0.000045, mae: 0.002068, mean_q: 0.751681
 73709/100000: episode: 1229, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 0.786, mean reward: 0.015 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.934, 10.173], loss: 0.000042, mae: 0.003238, mean_q: 0.752113
 73760/100000: episode: 1230, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.722, mean reward: 0.014 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.602, 10.247], loss: 0.000045, mae: 0.002131, mean_q: 0.752085
 73811/100000: episode: 1231, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 0.663, mean reward: 0.013 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.371, 10.100], loss: 0.000067, mae: 0.004105, mean_q: 0.752580
 73862/100000: episode: 1232, duration: 0.301s, episode steps: 51, steps per second: 169, episode reward: 0.681, mean reward: 0.013 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.313, 10.100], loss: 0.000067, mae: 0.004152, mean_q: 0.751704
 73913/100000: episode: 1233, duration: 0.294s, episode steps: 51, steps per second: 174, episode reward: 0.692, mean reward: 0.014 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.202, 10.138], loss: 0.000039, mae: 0.002963, mean_q: 0.751753
 73964/100000: episode: 1234, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 0.737, mean reward: 0.014 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.636, 10.100], loss: 0.000048, mae: 0.004170, mean_q: 0.751563
 74015/100000: episode: 1235, duration: 0.312s, episode steps: 51, steps per second: 164, episode reward: 0.740, mean reward: 0.015 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.886, 10.179], loss: 0.000049, mae: 0.003150, mean_q: 0.751953
 74066/100000: episode: 1236, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 0.780, mean reward: 0.015 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.119, 10.351], loss: 0.000048, mae: 0.003091, mean_q: 0.752197
 74117/100000: episode: 1237, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.686, mean reward: 0.013 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.226, 10.100], loss: 0.000045, mae: 0.003127, mean_q: 0.751796
 74168/100000: episode: 1238, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.686, mean reward: 0.013 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.297, 10.100], loss: 0.000063, mae: 0.003419, mean_q: 0.752409
 74219/100000: episode: 1239, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 0.781, mean reward: 0.015 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.733, 10.104], loss: 0.000043, mae: 0.003733, mean_q: 0.751535
 74270/100000: episode: 1240, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 0.704, mean reward: 0.014 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.140, 10.170], loss: 0.000069, mae: 0.004298, mean_q: 0.752222
 74321/100000: episode: 1241, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.560, 10.100], loss: 0.000068, mae: 0.004295, mean_q: 0.751768
 74372/100000: episode: 1242, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.640, mean reward: 0.013 [0.000, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.727, 10.222], loss: 0.000059, mae: 0.005088, mean_q: 0.751474
 74423/100000: episode: 1243, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.765, mean reward: 0.015 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.783, 10.100], loss: 0.000071, mae: 0.003430, mean_q: 0.752086
 74474/100000: episode: 1244, duration: 0.309s, episode steps: 51, steps per second: 165, episode reward: 0.753, mean reward: 0.015 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.579, 10.263], loss: 0.000063, mae: 0.003412, mean_q: 0.752239
 74525/100000: episode: 1245, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 0.687, mean reward: 0.013 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.309, 10.152], loss: 0.000070, mae: 0.003439, mean_q: 0.751582
 74576/100000: episode: 1246, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 0.749, mean reward: 0.015 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.935, 10.100], loss: 0.000117, mae: 0.004849, mean_q: 0.752289
 74627/100000: episode: 1247, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 0.718, mean reward: 0.014 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.584, 10.100], loss: 0.000079, mae: 0.003605, mean_q: 0.752281
 74678/100000: episode: 1248, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.803, mean reward: 0.016 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.676, 10.534], loss: 0.000093, mae: 0.003853, mean_q: 0.752206
 74729/100000: episode: 1249, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 0.735, mean reward: 0.014 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.046, 10.100], loss: 0.000059, mae: 0.002921, mean_q: 0.751622
 74780/100000: episode: 1250, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.858, mean reward: 0.017 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.116, 10.380], loss: 0.000034, mae: 0.002681, mean_q: 0.751616
 74831/100000: episode: 1251, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.854, 10.144], loss: 0.000073, mae: 0.004067, mean_q: 0.752036
 74882/100000: episode: 1252, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.756, mean reward: 0.015 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.239, 10.100], loss: 0.000084, mae: 0.004277, mean_q: 0.752023
 74933/100000: episode: 1253, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 0.719, mean reward: 0.014 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.616, 10.100], loss: 0.000063, mae: 0.003632, mean_q: 0.751793
 74984/100000: episode: 1254, duration: 0.312s, episode steps: 51, steps per second: 163, episode reward: 0.697, mean reward: 0.014 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.801, 10.244], loss: 0.000048, mae: 0.002741, mean_q: 0.751088
 75035/100000: episode: 1255, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 0.748, mean reward: 0.015 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.284, 10.188], loss: 0.000050, mae: 0.002685, mean_q: 0.750809
 75086/100000: episode: 1256, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 0.736, mean reward: 0.014 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.429, 10.254], loss: 0.000070, mae: 0.003304, mean_q: 0.751060
 75137/100000: episode: 1257, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.423, 10.470], loss: 0.000064, mae: 0.003821, mean_q: 0.750920
 75188/100000: episode: 1258, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 0.728, mean reward: 0.014 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.813, 10.302], loss: 0.000051, mae: 0.003073, mean_q: 0.750719
 75239/100000: episode: 1259, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 0.713, mean reward: 0.014 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.370, 10.100], loss: 0.000031, mae: 0.001723, mean_q: 0.750407
 75290/100000: episode: 1260, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 0.795, mean reward: 0.016 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.681, 10.100], loss: 0.000064, mae: 0.003296, mean_q: 0.750928
 75341/100000: episode: 1261, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.425, 10.318], loss: 0.000075, mae: 0.003520, mean_q: 0.751105
 75392/100000: episode: 1262, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 0.706, mean reward: 0.014 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.284, 10.100], loss: 0.000114, mae: 0.004866, mean_q: 0.750545
 75443/100000: episode: 1263, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.722, mean reward: 0.014 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.457, 10.360], loss: 0.000075, mae: 0.004352, mean_q: 0.749745
 75494/100000: episode: 1264, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 0.795, mean reward: 0.016 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.554, 10.100], loss: 0.000056, mae: 0.002924, mean_q: 0.750472
 75545/100000: episode: 1265, duration: 0.314s, episode steps: 51, steps per second: 162, episode reward: 0.723, mean reward: 0.014 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.707, 10.100], loss: 0.000051, mae: 0.003093, mean_q: 0.750206
 75596/100000: episode: 1266, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.668, mean reward: 0.013 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-1.012, 10.162], loss: 0.000066, mae: 0.003901, mean_q: 0.750458
 75647/100000: episode: 1267, duration: 0.304s, episode steps: 51, steps per second: 168, episode reward: 0.793, mean reward: 0.016 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.205, 10.247], loss: 0.000054, mae: 0.002613, mean_q: 0.749821
 75698/100000: episode: 1268, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 0.694, mean reward: 0.014 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.252, 10.148], loss: 0.000067, mae: 0.002940, mean_q: 0.749714
 75749/100000: episode: 1269, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.725, 10.100], loss: 0.000091, mae: 0.004580, mean_q: 0.749155
 75800/100000: episode: 1270, duration: 0.299s, episode steps: 51, steps per second: 170, episode reward: 0.758, mean reward: 0.015 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.975, 10.356], loss: 0.000081, mae: 0.004431, mean_q: 0.749885
 75851/100000: episode: 1271, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 0.744, mean reward: 0.015 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.084, 10.100], loss: 0.000078, mae: 0.004269, mean_q: 0.748798
 75902/100000: episode: 1272, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 0.715, mean reward: 0.014 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-1.204, 10.153], loss: 0.000074, mae: 0.003607, mean_q: 0.748732
 75953/100000: episode: 1273, duration: 0.301s, episode steps: 51, steps per second: 170, episode reward: 0.817, mean reward: 0.016 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.437, 10.100], loss: 0.000057, mae: 0.002840, mean_q: 0.748947
 76004/100000: episode: 1274, duration: 0.321s, episode steps: 51, steps per second: 159, episode reward: 0.712, mean reward: 0.014 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.144, 10.100], loss: 0.000076, mae: 0.003525, mean_q: 0.748917
 76055/100000: episode: 1275, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 0.822, mean reward: 0.016 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.225, 10.455], loss: 0.000053, mae: 0.002601, mean_q: 0.748996
 76106/100000: episode: 1276, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.824, mean reward: 0.016 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.480, 10.100], loss: 0.000067, mae: 0.003467, mean_q: 0.748569
 76157/100000: episode: 1277, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 0.714, mean reward: 0.014 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.225, 10.201], loss: 0.000044, mae: 0.003816, mean_q: 0.747895
 76208/100000: episode: 1278, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.615, 10.100], loss: 0.000094, mae: 0.004440, mean_q: 0.748413
 76259/100000: episode: 1279, duration: 0.305s, episode steps: 51, steps per second: 167, episode reward: 0.678, mean reward: 0.013 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.625, 10.100], loss: 0.000062, mae: 0.004126, mean_q: 0.748104
 76310/100000: episode: 1280, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.643, mean reward: 0.013 [0.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.422, 10.162], loss: 0.000082, mae: 0.003521, mean_q: 0.747916
 76361/100000: episode: 1281, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.726, mean reward: 0.014 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.332, 10.100], loss: 0.000077, mae: 0.004037, mean_q: 0.747450
 76412/100000: episode: 1282, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 0.780, mean reward: 0.015 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.185, 10.100], loss: 0.000065, mae: 0.003631, mean_q: 0.746779
 76463/100000: episode: 1283, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 0.829, mean reward: 0.016 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.430, 10.135], loss: 0.000062, mae: 0.003153, mean_q: 0.747497
 76514/100000: episode: 1284, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 0.665, mean reward: 0.013 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.396, 10.164], loss: 0.000055, mae: 0.003561, mean_q: 0.746514
 76565/100000: episode: 1285, duration: 0.298s, episode steps: 51, steps per second: 171, episode reward: 0.640, mean reward: 0.013 [0.000, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.276, 10.100], loss: 0.000080, mae: 0.003788, mean_q: 0.747165
 76616/100000: episode: 1286, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.702, mean reward: 0.014 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.121, 10.127], loss: 0.000056, mae: 0.003096, mean_q: 0.746875
 76667/100000: episode: 1287, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.710, mean reward: 0.014 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.391, 10.100], loss: 0.000076, mae: 0.003214, mean_q: 0.746344
 76718/100000: episode: 1288, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 0.672, mean reward: 0.013 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.632, 10.310], loss: 0.000032, mae: 0.002119, mean_q: 0.746029
 76769/100000: episode: 1289, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.714, 10.247], loss: 0.000105, mae: 0.004396, mean_q: 0.746293
 76820/100000: episode: 1290, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 0.745, mean reward: 0.015 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.719, 10.400], loss: 0.000081, mae: 0.003728, mean_q: 0.745601
 76871/100000: episode: 1291, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 0.656, mean reward: 0.013 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.118, 10.175], loss: 0.000075, mae: 0.003777, mean_q: 0.745812
 76922/100000: episode: 1292, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 0.765, mean reward: 0.015 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.650, 10.387], loss: 0.000103, mae: 0.003806, mean_q: 0.746279
 76973/100000: episode: 1293, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.684, mean reward: 0.013 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.494, 10.100], loss: 0.000053, mae: 0.003266, mean_q: 0.746035
 77024/100000: episode: 1294, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 0.740, mean reward: 0.015 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.742, 10.105], loss: 0.000072, mae: 0.004032, mean_q: 0.745578
 77075/100000: episode: 1295, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 0.835, mean reward: 0.016 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.004, 10.139], loss: 0.000093, mae: 0.003286, mean_q: 0.745827
 77126/100000: episode: 1296, duration: 0.308s, episode steps: 51, steps per second: 166, episode reward: 0.708, mean reward: 0.014 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.405, 10.100], loss: 0.000038, mae: 0.003626, mean_q: 0.744583
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7635816335678101
2
 77177/100000: episode: 1297, duration: 4.590s, episode steps: 51, steps per second: 11, episode reward: 0.708, mean reward: 0.014 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.112, 10.100], loss: 0.000062, mae: 0.003456, mean_q: 0.744539
 77278/100000: episode: 1298, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.337, 10.100], loss: 0.000054, mae: 0.003263, mean_q: 0.744726
 77379/100000: episode: 1299, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.707, 10.100], loss: 0.000059, mae: 0.003577, mean_q: 0.743885
 77480/100000: episode: 1300, duration: 0.618s, episode steps: 101, steps per second: 163, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.255, 10.222], loss: 0.000061, mae: 0.002828, mean_q: 0.743951
 77581/100000: episode: 1301, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.374, 10.100], loss: 0.000049, mae: 0.002756, mean_q: 0.743939
 77682/100000: episode: 1302, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.565, 10.100], loss: 0.000037, mae: 0.003155, mean_q: 0.743252
 77783/100000: episode: 1303, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.724, 10.100], loss: 0.000032, mae: 0.002847, mean_q: 0.743116
 77884/100000: episode: 1304, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.501, 10.123], loss: 0.000038, mae: 0.002790, mean_q: 0.742764
 77985/100000: episode: 1305, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.017, 10.103], loss: 0.000023, mae: 0.001882, mean_q: 0.742777
 78086/100000: episode: 1306, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.685, mean reward: 0.007 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.520, 10.100], loss: 0.000029, mae: 0.002314, mean_q: 0.742467
 78187/100000: episode: 1307, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.756, 10.210], loss: 0.000025, mae: 0.001827, mean_q: 0.741935
 78288/100000: episode: 1308, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.100], loss: 0.000025, mae: 0.002127, mean_q: 0.741927
 78389/100000: episode: 1309, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.208, 10.112], loss: 0.000033, mae: 0.002339, mean_q: 0.742148
 78490/100000: episode: 1310, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.067, 10.100], loss: 0.000017, mae: 0.001361, mean_q: 0.741606
 78591/100000: episode: 1311, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.537, 10.112], loss: 0.000023, mae: 0.002090, mean_q: 0.741620
 78692/100000: episode: 1312, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.842, 10.139], loss: 0.000028, mae: 0.002427, mean_q: 0.741291
 78793/100000: episode: 1313, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.408, 10.100], loss: 0.000014, mae: 0.001395, mean_q: 0.741386
 78894/100000: episode: 1314, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.893, 10.141], loss: 0.000021, mae: 0.001787, mean_q: 0.740631
 78995/100000: episode: 1315, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.868, mean reward: 0.009 [0.000, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.100], loss: 0.000025, mae: 0.001913, mean_q: 0.740699
 79096/100000: episode: 1316, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.732, 10.212], loss: 0.000021, mae: 0.001659, mean_q: 0.740256
 79197/100000: episode: 1317, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.042, 10.100], loss: 0.000015, mae: 0.001686, mean_q: 0.739879
 79298/100000: episode: 1318, duration: 0.556s, episode steps: 101, steps per second: 182, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.020, 10.194], loss: 0.000017, mae: 0.001724, mean_q: 0.739677
 79399/100000: episode: 1319, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.960, 10.100], loss: 0.000018, mae: 0.001671, mean_q: 0.739810
 79500/100000: episode: 1320, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.075, 10.306], loss: 0.000024, mae: 0.002026, mean_q: 0.739807
 79601/100000: episode: 1321, duration: 0.577s, episode steps: 101, steps per second: 175, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.227, 10.100], loss: 0.000019, mae: 0.001844, mean_q: 0.739739
 79702/100000: episode: 1322, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.093, 10.158], loss: 0.000015, mae: 0.001714, mean_q: 0.739635
 79803/100000: episode: 1323, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.185, 10.100], loss: 0.000018, mae: 0.001997, mean_q: 0.739625
 79904/100000: episode: 1324, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.925, 10.364], loss: 0.000017, mae: 0.001724, mean_q: 0.739106
 80005/100000: episode: 1325, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.507, 10.104], loss: 0.000017, mae: 0.001555, mean_q: 0.739313
 80106/100000: episode: 1326, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.106, 10.283], loss: 0.000012, mae: 0.001364, mean_q: 0.738851
 80207/100000: episode: 1327, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.701, 10.100], loss: 0.000025, mae: 0.002063, mean_q: 0.739073
 80308/100000: episode: 1328, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.531, 10.280], loss: 0.000016, mae: 0.001754, mean_q: 0.738463
 80409/100000: episode: 1329, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.138, 10.358], loss: 0.000013, mae: 0.001487, mean_q: 0.738496
 80510/100000: episode: 1330, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.100], loss: 0.000023, mae: 0.001873, mean_q: 0.738394
 80611/100000: episode: 1331, duration: 0.566s, episode steps: 101, steps per second: 179, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.725, 10.404], loss: 0.000023, mae: 0.001908, mean_q: 0.738833
 80712/100000: episode: 1332, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.596, 10.352], loss: 0.000018, mae: 0.001584, mean_q: 0.738768
 80813/100000: episode: 1333, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.636, 10.100], loss: 0.000016, mae: 0.001661, mean_q: 0.738784
 80914/100000: episode: 1334, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.572, 10.278], loss: 0.000018, mae: 0.001747, mean_q: 0.739037
 81015/100000: episode: 1335, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.030, 10.100], loss: 0.000014, mae: 0.001873, mean_q: 0.739126
 81116/100000: episode: 1336, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.897, 10.100], loss: 0.000022, mae: 0.001841, mean_q: 0.738850
 81217/100000: episode: 1337, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.902, 10.165], loss: 0.000021, mae: 0.002453, mean_q: 0.738844
 81318/100000: episode: 1338, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.846, mean reward: 0.008 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.047, 10.100], loss: 0.000020, mae: 0.001942, mean_q: 0.738469
 81419/100000: episode: 1339, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.816, 10.100], loss: 0.000019, mae: 0.001821, mean_q: 0.738764
 81520/100000: episode: 1340, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.788, 10.100], loss: 0.000024, mae: 0.002097, mean_q: 0.738809
 81621/100000: episode: 1341, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.410, 10.223], loss: 0.000012, mae: 0.001487, mean_q: 0.738730
 81722/100000: episode: 1342, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.894, 10.157], loss: 0.000016, mae: 0.001297, mean_q: 0.739009
 81823/100000: episode: 1343, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.635, 10.100], loss: 0.000020, mae: 0.002215, mean_q: 0.738718
 81924/100000: episode: 1344, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.555, 10.100], loss: 0.000010, mae: 0.001099, mean_q: 0.739075
 82025/100000: episode: 1345, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.821, 10.374], loss: 0.000007, mae: 0.001190, mean_q: 0.738932
 82126/100000: episode: 1346, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.275, 10.100], loss: 0.000020, mae: 0.001506, mean_q: 0.738706
 82227/100000: episode: 1347, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.726, 10.100], loss: 0.000010, mae: 0.001293, mean_q: 0.738997
 82328/100000: episode: 1348, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.463, 10.118], loss: 0.000012, mae: 0.001385, mean_q: 0.738878
 82429/100000: episode: 1349, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.294, 10.287], loss: 0.000013, mae: 0.001509, mean_q: 0.738914
 82530/100000: episode: 1350, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.579, 10.201], loss: 0.000012, mae: 0.001498, mean_q: 0.738786
 82631/100000: episode: 1351, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.829, 10.100], loss: 0.000012, mae: 0.001487, mean_q: 0.739147
 82732/100000: episode: 1352, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.259, 10.100], loss: 0.000011, mae: 0.001243, mean_q: 0.739280
 82833/100000: episode: 1353, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.646, 10.241], loss: 0.000007, mae: 0.000951, mean_q: 0.739168
 82934/100000: episode: 1354, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.768, 10.100], loss: 0.000008, mae: 0.001178, mean_q: 0.739252
 83035/100000: episode: 1355, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.895, 10.185], loss: 0.000011, mae: 0.001533, mean_q: 0.739206
 83136/100000: episode: 1356, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.196, 10.134], loss: 0.000004, mae: 0.001030, mean_q: 0.739048
 83237/100000: episode: 1357, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.683, 10.100], loss: 0.000010, mae: 0.001515, mean_q: 0.739278
 83338/100000: episode: 1358, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.306, 10.200], loss: 0.000016, mae: 0.001626, mean_q: 0.739220
 83439/100000: episode: 1359, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.583, 10.172], loss: 0.000008, mae: 0.001333, mean_q: 0.739057
 83540/100000: episode: 1360, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.100], loss: 0.000007, mae: 0.001012, mean_q: 0.739566
 83641/100000: episode: 1361, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.800, 10.100], loss: 0.000010, mae: 0.001974, mean_q: 0.739558
 83742/100000: episode: 1362, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.444, 10.100], loss: 0.000007, mae: 0.001724, mean_q: 0.739547
 83843/100000: episode: 1363, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.236, 10.205], loss: 0.000011, mae: 0.001624, mean_q: 0.739590
 83944/100000: episode: 1364, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.752, 10.258], loss: 0.000008, mae: 0.001185, mean_q: 0.739358
 84045/100000: episode: 1365, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.290, 10.100], loss: 0.000005, mae: 0.000957, mean_q: 0.739433
 84146/100000: episode: 1366, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.316, 10.128], loss: 0.000009, mae: 0.001466, mean_q: 0.739764
 84247/100000: episode: 1367, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.961, 10.239], loss: 0.000010, mae: 0.001553, mean_q: 0.739877
 84348/100000: episode: 1368, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.345, 10.179], loss: 0.000004, mae: 0.000896, mean_q: 0.740076
 84449/100000: episode: 1369, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.106, 10.109], loss: 0.000006, mae: 0.000942, mean_q: 0.740067
 84550/100000: episode: 1370, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.646, 10.203], loss: 0.000009, mae: 0.001467, mean_q: 0.740356
 84651/100000: episode: 1371, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.445, 10.255], loss: 0.000005, mae: 0.000974, mean_q: 0.740421
 84752/100000: episode: 1372, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.065, 10.359], loss: 0.000008, mae: 0.001255, mean_q: 0.740339
 84853/100000: episode: 1373, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.100], loss: 0.000010, mae: 0.001518, mean_q: 0.740529
 84954/100000: episode: 1374, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.018, 10.100], loss: 0.000007, mae: 0.001326, mean_q: 0.740887
 85055/100000: episode: 1375, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.896, 10.124], loss: 0.000006, mae: 0.001242, mean_q: 0.740648
 85156/100000: episode: 1376, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.351, 10.100], loss: 0.000009, mae: 0.001294, mean_q: 0.741161
 85257/100000: episode: 1377, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.164, 10.180], loss: 0.000004, mae: 0.000827, mean_q: 0.741291
 85358/100000: episode: 1378, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.211, 10.211], loss: 0.000007, mae: 0.001150, mean_q: 0.741365
 85459/100000: episode: 1379, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.183, 10.450], loss: 0.000006, mae: 0.001249, mean_q: 0.741419
 85560/100000: episode: 1380, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.984, 10.100], loss: 0.000010, mae: 0.001479, mean_q: 0.741886
 85661/100000: episode: 1381, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.136, 10.280], loss: 0.000008, mae: 0.001177, mean_q: 0.741675
 85762/100000: episode: 1382, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.648, 10.189], loss: 0.000007, mae: 0.001134, mean_q: 0.741922
 85863/100000: episode: 1383, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.923, 10.157], loss: 0.000008, mae: 0.001663, mean_q: 0.741876
 85964/100000: episode: 1384, duration: 0.567s, episode steps: 101, steps per second: 178, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.215, 10.179], loss: 0.000007, mae: 0.001492, mean_q: 0.742079
 86065/100000: episode: 1385, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.640, 10.124], loss: 0.000007, mae: 0.001060, mean_q: 0.742350
 86166/100000: episode: 1386, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.593, 10.100], loss: 0.000009, mae: 0.001526, mean_q: 0.742501
 86267/100000: episode: 1387, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.837, 10.100], loss: 0.000007, mae: 0.001274, mean_q: 0.742619
 86368/100000: episode: 1388, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.993, 10.189], loss: 0.000005, mae: 0.001127, mean_q: 0.742652
 86469/100000: episode: 1389, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.865, 10.171], loss: 0.000008, mae: 0.001352, mean_q: 0.742989
 86570/100000: episode: 1390, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.423, 10.353], loss: 0.000010, mae: 0.001538, mean_q: 0.743073
 86671/100000: episode: 1391, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.630, 10.482], loss: 0.000004, mae: 0.001133, mean_q: 0.743347
 86772/100000: episode: 1392, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.524, 10.100], loss: 0.000005, mae: 0.001250, mean_q: 0.743403
 86873/100000: episode: 1393, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.530, 10.100], loss: 0.000008, mae: 0.001177, mean_q: 0.743206
 86974/100000: episode: 1394, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.097, 10.100], loss: 0.000012, mae: 0.001895, mean_q: 0.743499
 87075/100000: episode: 1395, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.609, 10.253], loss: 0.000009, mae: 0.001474, mean_q: 0.743805
 87176/100000: episode: 1396, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.856, mean reward: 0.008 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.351, 10.100], loss: 0.000010, mae: 0.001648, mean_q: 0.744045
[Info] 1-TH LEVEL FOUND: 0.7551651000976562, Considering 10/100 traces
 87277/100000: episode: 1397, duration: 5.153s, episode steps: 101, steps per second: 20, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.420, 10.100], loss: 0.000005, mae: 0.001073, mean_q: 0.743825
 87294/100000: episode: 1398, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 0.701, mean reward: 0.041 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.490, 10.392], loss: 0.000005, mae: 0.000747, mean_q: 0.743633
 87311/100000: episode: 1399, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 0.682, mean reward: 0.040 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.122, 10.100], loss: 0.000017, mae: 0.001151, mean_q: 0.744138
 87329/100000: episode: 1400, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 0.656, mean reward: 0.036 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.662, 10.313], loss: 0.000015, mae: 0.001300, mean_q: 0.744104
 87345/100000: episode: 1401, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 0.800, mean reward: 0.050 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.058, 10.100], loss: 0.000032, mae: 0.002650, mean_q: 0.743815
 87362/100000: episode: 1402, duration: 0.112s, episode steps: 17, steps per second: 151, episode reward: 0.573, mean reward: 0.034 [0.000, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.110, 10.152], loss: 0.000014, mae: 0.002634, mean_q: 0.745317
 87378/100000: episode: 1403, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 0.672, mean reward: 0.042 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.063, 10.100], loss: 0.000015, mae: 0.001802, mean_q: 0.744581
 87394/100000: episode: 1404, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 0.683, mean reward: 0.043 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.309, 10.100], loss: 0.000010, mae: 0.001810, mean_q: 0.744853
 87409/100000: episode: 1405, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 0.624, mean reward: 0.042 [0.000, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.126, 10.100], loss: 0.000005, mae: 0.000729, mean_q: 0.744063
 87427/100000: episode: 1406, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 0.729, mean reward: 0.041 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.079, 10.287], loss: 0.000023, mae: 0.001449, mean_q: 0.744256
 87444/100000: episode: 1407, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 0.642, mean reward: 0.038 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.407, 10.100], loss: 0.000013, mae: 0.002098, mean_q: 0.744700
 87461/100000: episode: 1408, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 0.696, mean reward: 0.041 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.400], loss: 0.000008, mae: 0.001403, mean_q: 0.744777
 87477/100000: episode: 1409, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 0.630, mean reward: 0.039 [0.000, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.100], loss: 0.000107, mae: 0.005378, mean_q: 0.744484
 87493/100000: episode: 1410, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 0.821, mean reward: 0.051 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.938, 10.100], loss: 0.000024, mae: 0.003167, mean_q: 0.743398
 87508/100000: episode: 1411, duration: 0.102s, episode steps: 15, steps per second: 147, episode reward: 0.668, mean reward: 0.045 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.232], loss: 0.000031, mae: 0.001851, mean_q: 0.744209
 87525/100000: episode: 1412, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 0.680, mean reward: 0.040 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.047, 10.218], loss: 0.000010, mae: 0.001774, mean_q: 0.744337
 87543/100000: episode: 1413, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 0.687, mean reward: 0.038 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.081, 10.100], loss: 0.000025, mae: 0.002845, mean_q: 0.744800
 87559/100000: episode: 1414, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 0.779, mean reward: 0.049 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.685, 10.100], loss: 0.000031, mae: 0.002554, mean_q: 0.743536
 87575/100000: episode: 1415, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 0.824, mean reward: 0.051 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.371, 10.100], loss: 0.000023, mae: 0.002696, mean_q: 0.743857
 87592/100000: episode: 1416, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 0.643, mean reward: 0.038 [0.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.560, 10.120], loss: 0.000013, mae: 0.002114, mean_q: 0.743908
 87603/100000: episode: 1417, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 0.653, mean reward: 0.059 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-1.147, 10.274], loss: 0.000024, mae: 0.002071, mean_q: 0.743670
 87621/100000: episode: 1418, duration: 0.119s, episode steps: 18, steps per second: 151, episode reward: 0.751, mean reward: 0.042 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.859, 10.285], loss: 0.000016, mae: 0.002074, mean_q: 0.745045
 87638/100000: episode: 1419, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 0.644, mean reward: 0.038 [0.000, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.321], loss: 0.000031, mae: 0.002282, mean_q: 0.743928
 87655/100000: episode: 1420, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 0.645, mean reward: 0.038 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.164, 10.100], loss: 0.000054, mae: 0.003104, mean_q: 0.743713
 87666/100000: episode: 1421, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 0.672, mean reward: 0.061 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.305], loss: 0.000020, mae: 0.002413, mean_q: 0.744032
 87683/100000: episode: 1422, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 0.741, mean reward: 0.044 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.185], loss: 0.000058, mae: 0.003441, mean_q: 0.743984
 87700/100000: episode: 1423, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 0.650, mean reward: 0.038 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.359, 10.134], loss: 0.000013, mae: 0.002516, mean_q: 0.744501
 87717/100000: episode: 1424, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 0.664, mean reward: 0.039 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.256, 10.192], loss: 0.000026, mae: 0.002071, mean_q: 0.744999
 87733/100000: episode: 1425, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 0.679, mean reward: 0.042 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.387, 10.100], loss: 0.000083, mae: 0.004868, mean_q: 0.742499
 87748/100000: episode: 1426, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 0.621, mean reward: 0.041 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.521, 10.269], loss: 0.000027, mae: 0.003483, mean_q: 0.744277
 87764/100000: episode: 1427, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 0.759, mean reward: 0.047 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.434, 10.100], loss: 0.000056, mae: 0.002684, mean_q: 0.743659
 87781/100000: episode: 1428, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 0.671, mean reward: 0.039 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.145, 10.112], loss: 0.000042, mae: 0.002326, mean_q: 0.743416
 87798/100000: episode: 1429, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 0.635, mean reward: 0.037 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.564, 10.207], loss: 0.000062, mae: 0.003148, mean_q: 0.743675
 87815/100000: episode: 1430, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 0.726, mean reward: 0.043 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.593, 10.161], loss: 0.000008, mae: 0.002148, mean_q: 0.744146
 87832/100000: episode: 1431, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 0.706, mean reward: 0.042 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.303], loss: 0.000035, mae: 0.002752, mean_q: 0.742800
 87843/100000: episode: 1432, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 0.745, mean reward: 0.068 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.100, 10.485], loss: 0.000002, mae: 0.001573, mean_q: 0.743243
 87860/100000: episode: 1433, duration: 0.124s, episode steps: 17, steps per second: 138, episode reward: 0.632, mean reward: 0.037 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.257], loss: 0.000059, mae: 0.003875, mean_q: 0.742855
 87871/100000: episode: 1434, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 0.701, mean reward: 0.064 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.044, 10.174], loss: 0.000022, mae: 0.004259, mean_q: 0.743337
 87882/100000: episode: 1435, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 0.766, mean reward: 0.070 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.511], loss: 0.000005, mae: 0.002222, mean_q: 0.743818
 87899/100000: episode: 1436, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 0.686, mean reward: 0.040 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.470, 10.117], loss: 0.000013, mae: 0.002594, mean_q: 0.742877
 87914/100000: episode: 1437, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 0.625, mean reward: 0.042 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.166, 10.279], loss: 0.000053, mae: 0.003882, mean_q: 0.742915
 87932/100000: episode: 1438, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 0.694, mean reward: 0.039 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.420], loss: 0.000052, mae: 0.003659, mean_q: 0.743224
 87949/100000: episode: 1439, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 0.661, mean reward: 0.039 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.244], loss: 0.000022, mae: 0.002059, mean_q: 0.742584
 87965/100000: episode: 1440, duration: 0.104s, episode steps: 16, steps per second: 153, episode reward: 0.611, mean reward: 0.038 [0.000, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.207, 10.100], loss: 0.000018, mae: 0.001842, mean_q: 0.742340
 87982/100000: episode: 1441, duration: 0.109s, episode steps: 17, steps per second: 157, episode reward: 0.721, mean reward: 0.042 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.167, 10.128], loss: 0.000033, mae: 0.002556, mean_q: 0.742125
 87997/100000: episode: 1442, duration: 0.093s, episode steps: 15, steps per second: 160, episode reward: 0.639, mean reward: 0.043 [0.000, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.263], loss: 0.000050, mae: 0.003229, mean_q: 0.743520
 88014/100000: episode: 1443, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 0.681, mean reward: 0.040 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.503, 10.339], loss: 0.000017, mae: 0.002722, mean_q: 0.742302
 88030/100000: episode: 1444, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 0.772, mean reward: 0.048 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.391, 10.100], loss: 0.000038, mae: 0.004108, mean_q: 0.741300
 88048/100000: episode: 1445, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 0.722, mean reward: 0.040 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.318], loss: 0.000029, mae: 0.001942, mean_q: 0.742349
 88063/100000: episode: 1446, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 0.633, mean reward: 0.042 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.069, 10.189], loss: 0.000025, mae: 0.002259, mean_q: 0.742687
 88080/100000: episode: 1447, duration: 0.117s, episode steps: 17, steps per second: 145, episode reward: 0.681, mean reward: 0.040 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.228], loss: 0.000047, mae: 0.002021, mean_q: 0.741586
 88097/100000: episode: 1448, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 0.638, mean reward: 0.038 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.246, 10.230], loss: 0.000045, mae: 0.005296, mean_q: 0.741653
 88108/100000: episode: 1449, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 0.663, mean reward: 0.060 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.154], loss: 0.000020, mae: 0.002387, mean_q: 0.743206
 88119/100000: episode: 1450, duration: 0.077s, episode steps: 11, steps per second: 143, episode reward: 0.668, mean reward: 0.061 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.163, 10.278], loss: 0.000014, mae: 0.001611, mean_q: 0.741839
 88136/100000: episode: 1451, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 0.612, mean reward: 0.036 [0.000, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.900, 10.100], loss: 0.000080, mae: 0.004332, mean_q: 0.740156
 88154/100000: episode: 1452, duration: 0.123s, episode steps: 18, steps per second: 147, episode reward: 0.696, mean reward: 0.039 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.344], loss: 0.000022, mae: 0.002870, mean_q: 0.741673
 88171/100000: episode: 1453, duration: 0.113s, episode steps: 17, steps per second: 151, episode reward: 0.757, mean reward: 0.045 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.801, 10.171], loss: 0.000012, mae: 0.001894, mean_q: 0.741558
 88189/100000: episode: 1454, duration: 0.111s, episode steps: 18, steps per second: 163, episode reward: 0.664, mean reward: 0.037 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.618, 10.261], loss: 0.000052, mae: 0.003632, mean_q: 0.741772
 88206/100000: episode: 1455, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 0.695, mean reward: 0.041 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.564, 10.100], loss: 0.000039, mae: 0.002960, mean_q: 0.740827
 88217/100000: episode: 1456, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 0.808, mean reward: 0.073 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.488], loss: 0.000035, mae: 0.002969, mean_q: 0.741065
 88228/100000: episode: 1457, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 0.769, mean reward: 0.070 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.389], loss: 0.000042, mae: 0.003124, mean_q: 0.742460
 88244/100000: episode: 1458, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 0.832, mean reward: 0.052 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.101, 10.100], loss: 0.000017, mae: 0.003424, mean_q: 0.740290
 88255/100000: episode: 1459, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 0.681, mean reward: 0.062 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.061, 10.369], loss: 0.000054, mae: 0.003148, mean_q: 0.741848
 88266/100000: episode: 1460, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 0.632, mean reward: 0.057 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.334], loss: 0.000044, mae: 0.002427, mean_q: 0.739433
 88281/100000: episode: 1461, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 0.694, mean reward: 0.046 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.794, 10.171], loss: 0.000026, mae: 0.002367, mean_q: 0.742299
 88297/100000: episode: 1462, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 0.792, mean reward: 0.050 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.773, 10.125], loss: 0.000022, mae: 0.002477, mean_q: 0.740761
 88314/100000: episode: 1463, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 0.628, mean reward: 0.037 [0.000, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.235, 10.238], loss: 0.000041, mae: 0.003333, mean_q: 0.740619
 88331/100000: episode: 1464, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 0.685, mean reward: 0.040 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.210, 10.250], loss: 0.000079, mae: 0.003994, mean_q: 0.738963
 88347/100000: episode: 1465, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 0.754, mean reward: 0.047 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.100], loss: 0.000062, mae: 0.003297, mean_q: 0.740846
 88358/100000: episode: 1466, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 0.746, mean reward: 0.068 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.310], loss: 0.000037, mae: 0.002555, mean_q: 0.740841
 88375/100000: episode: 1467, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 0.638, mean reward: 0.038 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.340, 10.190], loss: 0.000031, mae: 0.002434, mean_q: 0.740087
 88386/100000: episode: 1468, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 0.623, mean reward: 0.057 [0.000, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.170], loss: 0.000054, mae: 0.002991, mean_q: 0.740457
 88403/100000: episode: 1469, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 0.703, mean reward: 0.041 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.187], loss: 0.000029, mae: 0.001981, mean_q: 0.739567
 88420/100000: episode: 1470, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 0.758, mean reward: 0.045 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.117, 10.100], loss: 0.000068, mae: 0.003398, mean_q: 0.740502
 88436/100000: episode: 1471, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 0.686, mean reward: 0.043 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.523, 10.190], loss: 0.000025, mae: 0.002310, mean_q: 0.740616
 88447/100000: episode: 1472, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 0.751, mean reward: 0.068 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.437], loss: 0.000022, mae: 0.002102, mean_q: 0.738551
 88463/100000: episode: 1473, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 0.705, mean reward: 0.044 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.096, 10.100], loss: 0.000028, mae: 0.002312, mean_q: 0.739330
 88479/100000: episode: 1474, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 0.647, mean reward: 0.040 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.689, 10.100], loss: 0.000049, mae: 0.002255, mean_q: 0.739061
 88496/100000: episode: 1475, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 0.653, mean reward: 0.038 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.262], loss: 0.000013, mae: 0.001688, mean_q: 0.739116
 88507/100000: episode: 1476, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 0.786, mean reward: 0.071 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.351], loss: 0.000034, mae: 0.002561, mean_q: 0.739202
 88524/100000: episode: 1477, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 0.628, mean reward: 0.037 [0.000, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.151], loss: 0.000079, mae: 0.003836, mean_q: 0.738424
 88541/100000: episode: 1478, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 0.647, mean reward: 0.038 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.258, 10.237], loss: 0.000083, mae: 0.003390, mean_q: 0.739280
 88552/100000: episode: 1479, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 0.738, mean reward: 0.067 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.220, 10.441], loss: 0.000026, mae: 0.002416, mean_q: 0.737801
 88569/100000: episode: 1480, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 0.652, mean reward: 0.038 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.194, 10.151], loss: 0.000070, mae: 0.003867, mean_q: 0.737853
 88580/100000: episode: 1481, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 0.766, mean reward: 0.070 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.488], loss: 0.000035, mae: 0.003247, mean_q: 0.737944
 88595/100000: episode: 1482, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 0.629, mean reward: 0.042 [0.000, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-1.171, 10.117], loss: 0.000076, mae: 0.003827, mean_q: 0.736709
 88612/100000: episode: 1483, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 0.665, mean reward: 0.039 [0.000, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.401, 10.100], loss: 0.000060, mae: 0.003071, mean_q: 0.738110
 88623/100000: episode: 1484, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 0.659, mean reward: 0.060 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.284], loss: 0.000033, mae: 0.002968, mean_q: 0.737530
 88640/100000: episode: 1485, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 0.735, mean reward: 0.043 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.078, 10.100], loss: 0.000049, mae: 0.002510, mean_q: 0.737937
 88656/100000: episode: 1486, duration: 0.100s, episode steps: 16, steps per second: 161, episode reward: 0.717, mean reward: 0.045 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.882, 10.100], loss: 0.000043, mae: 0.002562, mean_q: 0.738236
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7551651000976562
1
 88673/100000: episode: 1487, duration: 4.487s, episode steps: 17, steps per second: 4, episode reward: 0.702, mean reward: 0.041 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.206], loss: 0.000053, mae: 0.002991, mean_q: 0.737852
 88774/100000: episode: 1488, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.644, 10.100], loss: 0.000047, mae: 0.003557, mean_q: 0.736867
 88875/100000: episode: 1489, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.371, 10.275], loss: 0.000042, mae: 0.003348, mean_q: 0.735656
 88976/100000: episode: 1490, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.437, 10.376], loss: 0.000045, mae: 0.002520, mean_q: 0.734650
 89077/100000: episode: 1491, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.892, 10.304], loss: 0.000033, mae: 0.002393, mean_q: 0.734590
 89178/100000: episode: 1492, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.448, 10.100], loss: 0.000047, mae: 0.002907, mean_q: 0.733405
 89279/100000: episode: 1493, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.232, 10.135], loss: 0.000045, mae: 0.003030, mean_q: 0.731331
 89380/100000: episode: 1494, duration: 0.571s, episode steps: 101, steps per second: 177, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.652, 10.234], loss: 0.000044, mae: 0.002876, mean_q: 0.731325
 89481/100000: episode: 1495, duration: 0.569s, episode steps: 101, steps per second: 177, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.420, 10.174], loss: 0.000032, mae: 0.002392, mean_q: 0.729765
 89582/100000: episode: 1496, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.019, 10.100], loss: 0.000035, mae: 0.002662, mean_q: 0.730105
 89683/100000: episode: 1497, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.365, 10.193], loss: 0.000040, mae: 0.002992, mean_q: 0.728714
 89784/100000: episode: 1498, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.305, 10.100], loss: 0.000048, mae: 0.003030, mean_q: 0.728200
 89885/100000: episode: 1499, duration: 0.599s, episode steps: 101, steps per second: 169, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.675, 10.100], loss: 0.000032, mae: 0.002769, mean_q: 0.727480
 89986/100000: episode: 1500, duration: 0.581s, episode steps: 101, steps per second: 174, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.296, 10.245], loss: 0.000054, mae: 0.003233, mean_q: 0.726703
 90087/100000: episode: 1501, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.145, 10.184], loss: 0.000037, mae: 0.002922, mean_q: 0.726866
 90188/100000: episode: 1502, duration: 0.551s, episode steps: 101, steps per second: 183, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.711, 10.268], loss: 0.000043, mae: 0.002774, mean_q: 0.725678
 90289/100000: episode: 1503, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.100], loss: 0.000040, mae: 0.002935, mean_q: 0.725786
 90390/100000: episode: 1504, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.536, 10.100], loss: 0.000044, mae: 0.002693, mean_q: 0.724532
 90491/100000: episode: 1505, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.652, 10.100], loss: 0.000043, mae: 0.002617, mean_q: 0.724631
 90592/100000: episode: 1506, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.047, 10.168], loss: 0.000048, mae: 0.003565, mean_q: 0.724452
 90693/100000: episode: 1507, duration: 0.579s, episode steps: 101, steps per second: 174, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.169], loss: 0.000055, mae: 0.003314, mean_q: 0.723925
 90794/100000: episode: 1508, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.465, 10.140], loss: 0.000043, mae: 0.002467, mean_q: 0.723648
 90895/100000: episode: 1509, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.710, 10.214], loss: 0.000041, mae: 0.002551, mean_q: 0.724276
 90996/100000: episode: 1510, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.354, 10.100], loss: 0.000044, mae: 0.002932, mean_q: 0.723734
 91097/100000: episode: 1511, duration: 0.573s, episode steps: 101, steps per second: 176, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.023, 10.134], loss: 0.000040, mae: 0.002428, mean_q: 0.723369
 91198/100000: episode: 1512, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.100], loss: 0.000060, mae: 0.003516, mean_q: 0.724132
 91299/100000: episode: 1513, duration: 0.553s, episode steps: 101, steps per second: 183, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.944, 10.100], loss: 0.000037, mae: 0.002547, mean_q: 0.723765
 91400/100000: episode: 1514, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.059, 10.124], loss: 0.000034, mae: 0.002354, mean_q: 0.723486
 91501/100000: episode: 1515, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.346, 10.100], loss: 0.000033, mae: 0.002701, mean_q: 0.722944
 91602/100000: episode: 1516, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.759, 10.107], loss: 0.000053, mae: 0.003278, mean_q: 0.722364
 91703/100000: episode: 1517, duration: 0.557s, episode steps: 101, steps per second: 181, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.512, 10.100], loss: 0.000042, mae: 0.003000, mean_q: 0.722687
 91804/100000: episode: 1518, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.881, 10.174], loss: 0.000047, mae: 0.002924, mean_q: 0.722206
 91905/100000: episode: 1519, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.133, 10.122], loss: 0.000037, mae: 0.002668, mean_q: 0.722398
 92006/100000: episode: 1520, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.028, 10.100], loss: 0.000056, mae: 0.002896, mean_q: 0.722185
 92107/100000: episode: 1521, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.701, 10.239], loss: 0.000044, mae: 0.003592, mean_q: 0.721971
 92208/100000: episode: 1522, duration: 0.550s, episode steps: 101, steps per second: 184, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.266, 10.100], loss: 0.000039, mae: 0.002645, mean_q: 0.721386
 92309/100000: episode: 1523, duration: 0.548s, episode steps: 101, steps per second: 184, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.445, 10.100], loss: 0.000046, mae: 0.002939, mean_q: 0.722390
 92410/100000: episode: 1524, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.852, mean reward: 0.008 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.612, 10.100], loss: 0.000032, mae: 0.002331, mean_q: 0.722634
 92511/100000: episode: 1525, duration: 0.579s, episode steps: 101, steps per second: 175, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.744, 10.100], loss: 0.000043, mae: 0.002925, mean_q: 0.723037
 92612/100000: episode: 1526, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.267, 10.175], loss: 0.000035, mae: 0.002407, mean_q: 0.722756
 92713/100000: episode: 1527, duration: 0.602s, episode steps: 101, steps per second: 168, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.259, 10.256], loss: 0.000028, mae: 0.002386, mean_q: 0.722254
 92814/100000: episode: 1528, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.748, 10.193], loss: 0.000048, mae: 0.003415, mean_q: 0.722831
 92915/100000: episode: 1529, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.742, 10.195], loss: 0.000035, mae: 0.002754, mean_q: 0.723225
 93016/100000: episode: 1530, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.473, 10.100], loss: 0.000022, mae: 0.001905, mean_q: 0.723782
 93117/100000: episode: 1531, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.068, 10.189], loss: 0.000036, mae: 0.002601, mean_q: 0.723863
 93218/100000: episode: 1532, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.100], loss: 0.000026, mae: 0.002325, mean_q: 0.725017
 93319/100000: episode: 1533, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.418, 10.244], loss: 0.000026, mae: 0.002189, mean_q: 0.724970
 93420/100000: episode: 1534, duration: 0.609s, episode steps: 101, steps per second: 166, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.100], loss: 0.000013, mae: 0.001721, mean_q: 0.724532
 93521/100000: episode: 1535, duration: 0.563s, episode steps: 101, steps per second: 179, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.468, 10.358], loss: 0.000020, mae: 0.002203, mean_q: 0.725561
 93622/100000: episode: 1536, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.475, 10.292], loss: 0.000012, mae: 0.001829, mean_q: 0.725517
 93723/100000: episode: 1537, duration: 0.544s, episode steps: 101, steps per second: 186, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.377, 10.100], loss: 0.000007, mae: 0.001517, mean_q: 0.725239
 93824/100000: episode: 1538, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.649, 10.100], loss: 0.000005, mae: 0.001528, mean_q: 0.725396
 93925/100000: episode: 1539, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.481, 10.289], loss: 0.000007, mae: 0.001474, mean_q: 0.725794
 94026/100000: episode: 1540, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.893, 10.100], loss: 0.000008, mae: 0.001557, mean_q: 0.725698
 94127/100000: episode: 1541, duration: 0.553s, episode steps: 101, steps per second: 182, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.719, 10.265], loss: 0.000009, mae: 0.001572, mean_q: 0.725605
 94228/100000: episode: 1542, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.259, 10.120], loss: 0.000010, mae: 0.001720, mean_q: 0.725354
 94329/100000: episode: 1543, duration: 0.589s, episode steps: 101, steps per second: 171, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.297, 10.100], loss: 0.000014, mae: 0.002064, mean_q: 0.725512
 94430/100000: episode: 1544, duration: 0.576s, episode steps: 101, steps per second: 175, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.237, 10.103], loss: 0.000013, mae: 0.001715, mean_q: 0.725556
 94531/100000: episode: 1545, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.816, 10.162], loss: 0.000008, mae: 0.001549, mean_q: 0.725455
 94632/100000: episode: 1546, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.693, 10.423], loss: 0.000011, mae: 0.001844, mean_q: 0.725654
 94733/100000: episode: 1547, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.045, 10.100], loss: 0.000012, mae: 0.001963, mean_q: 0.725744
 94834/100000: episode: 1548, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.163, 10.249], loss: 0.000011, mae: 0.001737, mean_q: 0.726072
 94935/100000: episode: 1549, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.380, 10.119], loss: 0.000006, mae: 0.001300, mean_q: 0.725967
 95036/100000: episode: 1550, duration: 0.561s, episode steps: 101, steps per second: 180, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.677, 10.304], loss: 0.000011, mae: 0.001860, mean_q: 0.725635
 95137/100000: episode: 1551, duration: 0.555s, episode steps: 101, steps per second: 182, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.512, 10.175], loss: 0.000010, mae: 0.001778, mean_q: 0.725791
 95238/100000: episode: 1552, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.203, 10.270], loss: 0.000011, mae: 0.001700, mean_q: 0.726130
 95339/100000: episode: 1553, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.442, 10.100], loss: 0.000011, mae: 0.001773, mean_q: 0.726114
 95440/100000: episode: 1554, duration: 0.574s, episode steps: 101, steps per second: 176, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.429, 10.100], loss: 0.000013, mae: 0.002069, mean_q: 0.725712
 95541/100000: episode: 1555, duration: 0.560s, episode steps: 101, steps per second: 180, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.100], loss: 0.000007, mae: 0.001309, mean_q: 0.726214
 95642/100000: episode: 1556, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.086, 10.100], loss: 0.000008, mae: 0.001363, mean_q: 0.726621
 95743/100000: episode: 1557, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.137, 10.100], loss: 0.000016, mae: 0.002060, mean_q: 0.725739
 95844/100000: episode: 1558, duration: 0.552s, episode steps: 101, steps per second: 183, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.080, 10.100], loss: 0.000006, mae: 0.001211, mean_q: 0.725465
 95945/100000: episode: 1559, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.480, 10.348], loss: 0.000013, mae: 0.002085, mean_q: 0.726036
 96046/100000: episode: 1560, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.862, 10.128], loss: 0.000011, mae: 0.001733, mean_q: 0.726239
 96147/100000: episode: 1561, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.338, 10.196], loss: 0.000009, mae: 0.001405, mean_q: 0.725838
 96248/100000: episode: 1562, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.354, 10.361], loss: 0.000010, mae: 0.001716, mean_q: 0.726221
 96349/100000: episode: 1563, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.282, 10.100], loss: 0.000006, mae: 0.001163, mean_q: 0.726584
 96450/100000: episode: 1564, duration: 0.564s, episode steps: 101, steps per second: 179, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.977, 10.152], loss: 0.000009, mae: 0.001350, mean_q: 0.726299
 96551/100000: episode: 1565, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.588, 10.100], loss: 0.000008, mae: 0.001256, mean_q: 0.726783
 96652/100000: episode: 1566, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.608, 10.100], loss: 0.000009, mae: 0.001610, mean_q: 0.726698
 96753/100000: episode: 1567, duration: 0.589s, episode steps: 101, steps per second: 172, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.697, 10.100], loss: 0.000009, mae: 0.001617, mean_q: 0.726491
 96854/100000: episode: 1568, duration: 0.584s, episode steps: 101, steps per second: 173, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.266, 10.417], loss: 0.000006, mae: 0.001263, mean_q: 0.726830
 96955/100000: episode: 1569, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.129, 10.151], loss: 0.000009, mae: 0.001407, mean_q: 0.726733
 97056/100000: episode: 1570, duration: 0.607s, episode steps: 101, steps per second: 166, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.663, 10.100], loss: 0.000013, mae: 0.001609, mean_q: 0.726826
 97157/100000: episode: 1571, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.895, 10.315], loss: 0.000011, mae: 0.001753, mean_q: 0.727444
 97258/100000: episode: 1572, duration: 0.568s, episode steps: 101, steps per second: 178, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.484, 10.206], loss: 0.000009, mae: 0.001450, mean_q: 0.728058
 97359/100000: episode: 1573, duration: 0.559s, episode steps: 101, steps per second: 181, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.921, 10.418], loss: 0.000007, mae: 0.001260, mean_q: 0.727846
 97460/100000: episode: 1574, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.147, 10.100], loss: 0.000011, mae: 0.001549, mean_q: 0.727564
 97561/100000: episode: 1575, duration: 0.554s, episode steps: 101, steps per second: 182, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.100], loss: 0.000010, mae: 0.001394, mean_q: 0.727645
 97662/100000: episode: 1576, duration: 0.545s, episode steps: 101, steps per second: 185, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.778, 10.217], loss: 0.000010, mae: 0.001493, mean_q: 0.728662
 97763/100000: episode: 1577, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.323, 10.100], loss: 0.000010, mae: 0.001531, mean_q: 0.728800
 97864/100000: episode: 1578, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.294, 10.159], loss: 0.000008, mae: 0.001490, mean_q: 0.728502
 97965/100000: episode: 1579, duration: 0.586s, episode steps: 101, steps per second: 172, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.329, 10.100], loss: 0.000007, mae: 0.001393, mean_q: 0.728849
 98066/100000: episode: 1580, duration: 0.566s, episode steps: 101, steps per second: 178, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.529, 10.100], loss: 0.000005, mae: 0.001086, mean_q: 0.729423
 98167/100000: episode: 1581, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.243, 10.369], loss: 0.000009, mae: 0.001432, mean_q: 0.729043
 98268/100000: episode: 1582, duration: 0.603s, episode steps: 101, steps per second: 167, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.640, 10.196], loss: 0.000005, mae: 0.001226, mean_q: 0.729505
 98369/100000: episode: 1583, duration: 0.587s, episode steps: 101, steps per second: 172, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.105, 10.100], loss: 0.000009, mae: 0.001546, mean_q: 0.729473
 98470/100000: episode: 1584, duration: 0.572s, episode steps: 101, steps per second: 176, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.035, 10.100], loss: 0.000007, mae: 0.001354, mean_q: 0.730084
 98571/100000: episode: 1585, duration: 0.597s, episode steps: 101, steps per second: 169, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.486, 10.100], loss: 0.000015, mae: 0.001939, mean_q: 0.729835
 98672/100000: episode: 1586, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.924, 10.100], loss: 0.000007, mae: 0.001217, mean_q: 0.730752
[Info] 1-TH LEVEL FOUND: 0.7545100450515747, Considering 10/100 traces
 98773/100000: episode: 1587, duration: 5.218s, episode steps: 101, steps per second: 19, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.572, 10.305], loss: 0.000010, mae: 0.001552, mean_q: 0.730264
 98795/100000: episode: 1588, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 0.662, mean reward: 0.030 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.974, 10.139], loss: 0.000015, mae: 0.002593, mean_q: 0.730906
 98815/100000: episode: 1589, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 0.728, mean reward: 0.036 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.737, 10.340], loss: 0.000012, mae: 0.001998, mean_q: 0.730522
 98836/100000: episode: 1590, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 0.720, mean reward: 0.034 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.305, 10.221], loss: 0.000009, mae: 0.001352, mean_q: 0.732448
 98858/100000: episode: 1591, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 0.782, mean reward: 0.036 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.744, 10.303], loss: 0.000012, mae: 0.001540, mean_q: 0.732091
 98882/100000: episode: 1592, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 0.677, mean reward: 0.028 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.222, 10.100], loss: 0.000005, mae: 0.001206, mean_q: 0.731321
 98906/100000: episode: 1593, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 0.745, mean reward: 0.031 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.372, 10.186], loss: 0.000007, mae: 0.001268, mean_q: 0.732682
 98926/100000: episode: 1594, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 0.706, mean reward: 0.035 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.205, 10.167], loss: 0.000017, mae: 0.001758, mean_q: 0.733144
 98947/100000: episode: 1595, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 0.700, mean reward: 0.033 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.359, 10.345], loss: 0.000006, mae: 0.001703, mean_q: 0.732042
 98968/100000: episode: 1596, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 0.641, mean reward: 0.031 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.184], loss: 0.000010, mae: 0.001274, mean_q: 0.732693
 98989/100000: episode: 1597, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 0.706, mean reward: 0.034 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.644, 10.233], loss: 0.000015, mae: 0.001875, mean_q: 0.733043
 99013/100000: episode: 1598, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 0.770, mean reward: 0.032 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.244, 10.100], loss: 0.000005, mae: 0.001754, mean_q: 0.732297
 99035/100000: episode: 1599, duration: 0.144s, episode steps: 22, steps per second: 153, episode reward: 0.679, mean reward: 0.031 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.831, 10.227], loss: 0.000037, mae: 0.002290, mean_q: 0.733812
 99056/100000: episode: 1600, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 0.768, mean reward: 0.037 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.445], loss: 0.000018, mae: 0.002811, mean_q: 0.735345
 99075/100000: episode: 1601, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 0.728, mean reward: 0.038 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.547, 10.274], loss: 0.000009, mae: 0.001704, mean_q: 0.733189
 99096/100000: episode: 1602, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 0.637, mean reward: 0.030 [0.000, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.365, 10.100], loss: 0.000014, mae: 0.001616, mean_q: 0.732753
 99120/100000: episode: 1603, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 0.704, mean reward: 0.029 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.407, 10.100], loss: 0.000010, mae: 0.001534, mean_q: 0.733208
 99141/100000: episode: 1604, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 0.737, mean reward: 0.035 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.425], loss: 0.000010, mae: 0.001548, mean_q: 0.733718
 99163/100000: episode: 1605, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.672, mean reward: 0.031 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.504, 10.318], loss: 0.000010, mae: 0.001640, mean_q: 0.734662
 99185/100000: episode: 1606, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 0.675, mean reward: 0.031 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.335, 10.100], loss: 0.000022, mae: 0.002065, mean_q: 0.736288
 99207/100000: episode: 1607, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 0.645, mean reward: 0.029 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.499, 10.100], loss: 0.000023, mae: 0.002301, mean_q: 0.734324
 99228/100000: episode: 1608, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 0.648, mean reward: 0.031 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.129, 10.131], loss: 0.000024, mae: 0.002946, mean_q: 0.734116
 99249/100000: episode: 1609, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 0.682, mean reward: 0.032 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.755, 10.199], loss: 0.000009, mae: 0.001330, mean_q: 0.735998
 99271/100000: episode: 1610, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 0.714, mean reward: 0.032 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.061, 10.262], loss: 0.000009, mae: 0.001444, mean_q: 0.735351
 99292/100000: episode: 1611, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 0.758, mean reward: 0.036 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.399], loss: 0.000020, mae: 0.002150, mean_q: 0.733899
 99312/100000: episode: 1612, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 0.698, mean reward: 0.035 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.253], loss: 0.000003, mae: 0.001070, mean_q: 0.734463
 99333/100000: episode: 1613, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 0.643, mean reward: 0.031 [0.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.514, 10.130], loss: 0.000022, mae: 0.001437, mean_q: 0.735133
 99355/100000: episode: 1614, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 0.694, mean reward: 0.032 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.732, 10.241], loss: 0.000005, mae: 0.001153, mean_q: 0.734455
 99379/100000: episode: 1615, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 0.762, mean reward: 0.032 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.411, 10.100], loss: 0.000024, mae: 0.002006, mean_q: 0.735331
 99401/100000: episode: 1616, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 0.689, mean reward: 0.031 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.166, 10.333], loss: 0.000022, mae: 0.002467, mean_q: 0.736074
 99422/100000: episode: 1617, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 0.607, mean reward: 0.029 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.100], loss: 0.000022, mae: 0.002295, mean_q: 0.734907
 99444/100000: episode: 1618, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 0.761, mean reward: 0.035 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.370], loss: 0.000009, mae: 0.001546, mean_q: 0.735392
 99466/100000: episode: 1619, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 0.633, mean reward: 0.029 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.100], loss: 0.000007, mae: 0.001484, mean_q: 0.735753
 99488/100000: episode: 1620, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 0.673, mean reward: 0.031 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.432, 10.186], loss: 0.000044, mae: 0.002272, mean_q: 0.734921
 99509/100000: episode: 1621, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 0.687, mean reward: 0.033 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.675, 10.100], loss: 0.000010, mae: 0.001821, mean_q: 0.735306
 99533/100000: episode: 1622, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 0.754, mean reward: 0.031 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.121, 10.100], loss: 0.000029, mae: 0.002756, mean_q: 0.734616
 99554/100000: episode: 1623, duration: 0.139s, episode steps: 21, steps per second: 151, episode reward: 0.707, mean reward: 0.034 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.020, 10.224], loss: 0.000030, mae: 0.002795, mean_q: 0.735733
 99575/100000: episode: 1624, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 0.708, mean reward: 0.034 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.765, 10.194], loss: 0.000034, mae: 0.002354, mean_q: 0.735659
 99595/100000: episode: 1625, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 0.654, mean reward: 0.033 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.071, 10.245], loss: 0.000024, mae: 0.002900, mean_q: 0.735296
 99616/100000: episode: 1626, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 0.673, mean reward: 0.032 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.229], loss: 0.000017, mae: 0.002039, mean_q: 0.734564
 99636/100000: episode: 1627, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 0.719, mean reward: 0.036 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.164, 10.352], loss: 0.000015, mae: 0.001547, mean_q: 0.736269
 99657/100000: episode: 1628, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 0.675, mean reward: 0.032 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.267], loss: 0.000024, mae: 0.002545, mean_q: 0.735912
 99679/100000: episode: 1629, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 0.647, mean reward: 0.029 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.757, 10.226], loss: 0.000010, mae: 0.001804, mean_q: 0.735396
 99700/100000: episode: 1630, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 0.636, mean reward: 0.030 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.269], loss: 0.000034, mae: 0.002094, mean_q: 0.734530
 99721/100000: episode: 1631, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 0.621, mean reward: 0.030 [0.000, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.252, 10.237], loss: 0.000021, mae: 0.002602, mean_q: 0.735733
 99742/100000: episode: 1632, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 0.759, mean reward: 0.036 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.869, 10.250], loss: 0.000047, mae: 0.004084, mean_q: 0.735646
 99766/100000: episode: 1633, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 0.720, mean reward: 0.030 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.681, 10.100], loss: 0.000018, mae: 0.002064, mean_q: 0.735852
 99785/100000: episode: 1634, duration: 0.122s, episode steps: 19, steps per second: 156, episode reward: 0.692, mean reward: 0.036 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.347, 10.203], loss: 0.000012, mae: 0.001409, mean_q: 0.735612
 99807/100000: episode: 1635, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 0.619, mean reward: 0.028 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.280, 10.210], loss: 0.000024, mae: 0.002088, mean_q: 0.734923
 99829/100000: episode: 1636, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 0.656, mean reward: 0.030 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.346, 10.234], loss: 0.000039, mae: 0.002710, mean_q: 0.734957
 99848/100000: episode: 1637, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 0.741, mean reward: 0.039 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.066, 10.346], loss: 0.000016, mae: 0.001779, mean_q: 0.735705
 99869/100000: episode: 1638, duration: 0.123s, episode steps: 21, steps per second: 170, episode reward: 0.663, mean reward: 0.032 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.214], loss: 0.000052, mae: 0.003633, mean_q: 0.735858
 99890/100000: episode: 1639, duration: 0.143s, episode steps: 21, steps per second: 147, episode reward: 0.801, mean reward: 0.038 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.179, 10.428], loss: 0.000010, mae: 0.001785, mean_q: 0.736574
 99911/100000: episode: 1640, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 0.651, mean reward: 0.031 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.247], loss: 0.000023, mae: 0.002493, mean_q: 0.735712
 99932/100000: episode: 1641, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 0.704, mean reward: 0.034 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.379, 10.329], loss: 0.000044, mae: 0.003047, mean_q: 0.736034
 99953/100000: episode: 1642, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 0.607, mean reward: 0.029 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.770, 10.233], loss: 0.000054, mae: 0.005102, mean_q: 0.735923
 99974/100000: episode: 1643, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 0.667, mean reward: 0.032 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.227], loss: 0.000028, mae: 0.002332, mean_q: 0.735737
 99994/100000: episode: 1644, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 0.670, mean reward: 0.033 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.453, 10.104], loss: 0.000010, mae: 0.002151, mean_q: 0.735496
done, took 621.200 seconds
[Info] End Importance Splitting. Falsification occurred 0 times.
