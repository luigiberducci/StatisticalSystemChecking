Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Configuration ISplitting
[Info] Use ISplitting: True
[Info] Output Dirs: out/localization/mer27nov2019_14_42_05_CET/3, out/localization/mer27nov2019_14_42_05_CET/3/levels, out/localization/mer27nov2019_14_42_05_CET/3/traces
[Info] Num particles: 100
[Info] Adaptive Multilevel Splitting: True, delta=0, k=10
[Info] Fixed Level Splitting: []
[Info] Start Importance Splitting.
Training for 100000 steps ...
   101/100000: episode: 1, duration: 0.174s, episode steps: 101, steps per second: 581, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.078, 10.100], loss: --, mae: --, mean_q: --
   202/100000: episode: 2, duration: 0.086s, episode steps: 101, steps per second: 1169, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.075, 10.100], loss: --, mae: --, mean_q: --
   303/100000: episode: 3, duration: 0.065s, episode steps: 101, steps per second: 1561, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.267, 10.259], loss: --, mae: --, mean_q: --
   404/100000: episode: 4, duration: 0.069s, episode steps: 101, steps per second: 1466, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.567, 10.272], loss: --, mae: --, mean_q: --
   505/100000: episode: 5, duration: 0.117s, episode steps: 101, steps per second: 866, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.736, 10.222], loss: --, mae: --, mean_q: --
   606/100000: episode: 6, duration: 0.077s, episode steps: 101, steps per second: 1314, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.874, 10.100], loss: --, mae: --, mean_q: --
   707/100000: episode: 7, duration: 0.075s, episode steps: 101, steps per second: 1345, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.915, 10.100], loss: --, mae: --, mean_q: --
   808/100000: episode: 8, duration: 0.073s, episode steps: 101, steps per second: 1379, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.932, 10.154], loss: --, mae: --, mean_q: --
   909/100000: episode: 9, duration: 0.083s, episode steps: 101, steps per second: 1220, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.568, 10.102], loss: --, mae: --, mean_q: --
  1010/100000: episode: 10, duration: 0.104s, episode steps: 101, steps per second: 970, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.293, 10.100], loss: --, mae: --, mean_q: --
  1111/100000: episode: 11, duration: 0.071s, episode steps: 101, steps per second: 1426, episode reward: 0.854, mean reward: 0.008 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.231, 10.214], loss: --, mae: --, mean_q: --
  1212/100000: episode: 12, duration: 0.067s, episode steps: 101, steps per second: 1498, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.113, 10.532], loss: --, mae: --, mean_q: --
  1313/100000: episode: 13, duration: 0.068s, episode steps: 101, steps per second: 1496, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.089, 10.168], loss: --, mae: --, mean_q: --
  1414/100000: episode: 14, duration: 0.067s, episode steps: 101, steps per second: 1501, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.548, 10.100], loss: --, mae: --, mean_q: --
  1515/100000: episode: 15, duration: 0.067s, episode steps: 101, steps per second: 1509, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.303], loss: --, mae: --, mean_q: --
  1616/100000: episode: 16, duration: 0.067s, episode steps: 101, steps per second: 1500, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.456, 10.234], loss: --, mae: --, mean_q: --
  1717/100000: episode: 17, duration: 0.067s, episode steps: 101, steps per second: 1503, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.100], loss: --, mae: --, mean_q: --
  1818/100000: episode: 18, duration: 0.067s, episode steps: 101, steps per second: 1507, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.563, 10.100], loss: --, mae: --, mean_q: --
  1919/100000: episode: 19, duration: 0.067s, episode steps: 101, steps per second: 1505, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.532, 10.155], loss: --, mae: --, mean_q: --
  2020/100000: episode: 20, duration: 0.069s, episode steps: 101, steps per second: 1464, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.746, 10.293], loss: --, mae: --, mean_q: --
  2121/100000: episode: 21, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.538, 10.132], loss: --, mae: --, mean_q: --
  2222/100000: episode: 22, duration: 0.073s, episode steps: 101, steps per second: 1378, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.000, 10.186], loss: --, mae: --, mean_q: --
  2323/100000: episode: 23, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.384, 10.108], loss: --, mae: --, mean_q: --
  2424/100000: episode: 24, duration: 0.068s, episode steps: 101, steps per second: 1493, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.761, 10.100], loss: --, mae: --, mean_q: --
  2525/100000: episode: 25, duration: 0.067s, episode steps: 101, steps per second: 1498, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.304, 10.100], loss: --, mae: --, mean_q: --
  2626/100000: episode: 26, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.687, 10.173], loss: --, mae: --, mean_q: --
  2727/100000: episode: 27, duration: 0.067s, episode steps: 101, steps per second: 1500, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.106, 10.100], loss: --, mae: --, mean_q: --
  2828/100000: episode: 28, duration: 0.068s, episode steps: 101, steps per second: 1489, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.816, 10.194], loss: --, mae: --, mean_q: --
  2929/100000: episode: 29, duration: 0.070s, episode steps: 101, steps per second: 1452, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.541, 10.230], loss: --, mae: --, mean_q: --
  3030/100000: episode: 30, duration: 0.070s, episode steps: 101, steps per second: 1435, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.577, 10.118], loss: --, mae: --, mean_q: --
  3131/100000: episode: 31, duration: 0.069s, episode steps: 101, steps per second: 1468, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.100], loss: --, mae: --, mean_q: --
  3232/100000: episode: 32, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.835, 10.244], loss: --, mae: --, mean_q: --
  3333/100000: episode: 33, duration: 0.067s, episode steps: 101, steps per second: 1500, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.600, 10.100], loss: --, mae: --, mean_q: --
  3434/100000: episode: 34, duration: 0.067s, episode steps: 101, steps per second: 1504, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.222, 10.275], loss: --, mae: --, mean_q: --
  3535/100000: episode: 35, duration: 0.067s, episode steps: 101, steps per second: 1501, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.023, 10.236], loss: --, mae: --, mean_q: --
  3636/100000: episode: 36, duration: 0.067s, episode steps: 101, steps per second: 1502, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.672, 10.100], loss: --, mae: --, mean_q: --
  3737/100000: episode: 37, duration: 0.068s, episode steps: 101, steps per second: 1490, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.489, 10.420], loss: --, mae: --, mean_q: --
  3838/100000: episode: 38, duration: 0.069s, episode steps: 101, steps per second: 1465, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.294, 10.100], loss: --, mae: --, mean_q: --
  3939/100000: episode: 39, duration: 0.068s, episode steps: 101, steps per second: 1479, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.644, 10.340], loss: --, mae: --, mean_q: --
  4040/100000: episode: 40, duration: 0.067s, episode steps: 101, steps per second: 1507, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.613, 10.323], loss: --, mae: --, mean_q: --
  4141/100000: episode: 41, duration: 0.067s, episode steps: 101, steps per second: 1508, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.559, 10.382], loss: --, mae: --, mean_q: --
  4242/100000: episode: 42, duration: 0.068s, episode steps: 101, steps per second: 1477, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.287, 10.230], loss: --, mae: --, mean_q: --
  4343/100000: episode: 43, duration: 0.069s, episode steps: 101, steps per second: 1472, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.000, 10.212], loss: --, mae: --, mean_q: --
  4444/100000: episode: 44, duration: 0.068s, episode steps: 101, steps per second: 1480, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.518, 10.170], loss: --, mae: --, mean_q: --
  4545/100000: episode: 45, duration: 0.067s, episode steps: 101, steps per second: 1498, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.512, 10.100], loss: --, mae: --, mean_q: --
  4646/100000: episode: 46, duration: 0.070s, episode steps: 101, steps per second: 1453, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.198, 10.100], loss: --, mae: --, mean_q: --
  4747/100000: episode: 47, duration: 0.070s, episode steps: 101, steps per second: 1444, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.775, 10.100], loss: --, mae: --, mean_q: --
  4848/100000: episode: 48, duration: 0.068s, episode steps: 101, steps per second: 1492, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.405, 10.100], loss: --, mae: --, mean_q: --
  4949/100000: episode: 49, duration: 0.068s, episode steps: 101, steps per second: 1482, episode reward: 0.670, mean reward: 0.007 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.981, 10.239], loss: --, mae: --, mean_q: --
  5050/100000: episode: 50, duration: 0.970s, episode steps: 101, steps per second: 104, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.400, 10.100], loss: 0.023397, mae: 0.079419, mean_q: -0.811792
  5151/100000: episode: 51, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.100], loss: 0.020998, mae: 0.067656, mean_q: -0.796909
  5252/100000: episode: 52, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.581, 10.250], loss: 0.015462, mae: 0.055891, mean_q: -0.775851
  5353/100000: episode: 53, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.411, 10.443], loss: 0.008587, mae: 0.041355, mean_q: -0.763716
  5454/100000: episode: 54, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.168, 10.170], loss: 0.008327, mae: 0.037885, mean_q: -0.769310
  5555/100000: episode: 55, duration: 0.519s, episode steps: 101, steps per second: 194, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.784, 10.150], loss: 0.004856, mae: 0.030493, mean_q: -0.749720
  5656/100000: episode: 56, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.384, 10.235], loss: 0.006190, mae: 0.030903, mean_q: -0.735789
  5757/100000: episode: 57, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.794, 10.264], loss: 0.003264, mae: 0.027720, mean_q: -0.727143
  5858/100000: episode: 58, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.091, 10.109], loss: 0.003031, mae: 0.024676, mean_q: -0.713729
  5959/100000: episode: 59, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.332, 10.100], loss: 0.001752, mae: 0.024384, mean_q: -0.704689
  6060/100000: episode: 60, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.803, 10.100], loss: 0.000841, mae: 0.018204, mean_q: -0.698057
  6161/100000: episode: 61, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.653, 10.262], loss: 0.000668, mae: 0.018791, mean_q: -0.683158
  6262/100000: episode: 62, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.309], loss: 0.000283, mae: 0.014329, mean_q: -0.673615
  6363/100000: episode: 63, duration: 0.493s, episode steps: 101, steps per second: 205, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.925, 10.100], loss: 0.000119, mae: 0.010924, mean_q: -0.687424
  6464/100000: episode: 64, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.477, 10.253], loss: 0.000138, mae: 0.011736, mean_q: -0.663955
  6565/100000: episode: 65, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.807, 10.100], loss: 0.000121, mae: 0.011049, mean_q: -0.640847
  6666/100000: episode: 66, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.817, 10.100], loss: 0.000131, mae: 0.011978, mean_q: -0.643969
  6767/100000: episode: 67, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.997, 10.173], loss: 0.000116, mae: 0.011138, mean_q: -0.614617
  6868/100000: episode: 68, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.005, 10.111], loss: 0.000125, mae: 0.011572, mean_q: -0.614481
  6969/100000: episode: 69, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.248, 10.156], loss: 0.000137, mae: 0.012376, mean_q: -0.628228
  7070/100000: episode: 70, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.066, 10.288], loss: 0.000112, mae: 0.011065, mean_q: -0.607077
  7171/100000: episode: 71, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.769, 10.146], loss: 0.000124, mae: 0.011795, mean_q: -0.586462
  7272/100000: episode: 72, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.377, 10.100], loss: 0.000121, mae: 0.011641, mean_q: -0.566721
  7373/100000: episode: 73, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.988, 10.352], loss: 0.000108, mae: 0.011110, mean_q: -0.562582
  7474/100000: episode: 74, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.986, 10.127], loss: 0.000117, mae: 0.011668, mean_q: -0.538010
  7575/100000: episode: 75, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.920, 10.254], loss: 0.000145, mae: 0.012315, mean_q: -0.553917
  7676/100000: episode: 76, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.847, mean reward: 0.008 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.100], loss: 0.000126, mae: 0.012141, mean_q: -0.512124
  7777/100000: episode: 77, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.138, 10.100], loss: 0.000154, mae: 0.012722, mean_q: -0.496592
  7878/100000: episode: 78, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.504, 10.255], loss: 0.000167, mae: 0.013319, mean_q: -0.493224
  7979/100000: episode: 79, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.465, 10.100], loss: 0.000165, mae: 0.013421, mean_q: -0.472517
  8080/100000: episode: 80, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.834, mean reward: 0.008 [0.000, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.820, 10.100], loss: 0.000189, mae: 0.014378, mean_q: -0.479932
  8181/100000: episode: 81, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.659, mean reward: 0.007 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.063, 10.182], loss: 0.000151, mae: 0.012687, mean_q: -0.464893
  8282/100000: episode: 82, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.638, 10.100], loss: 0.000156, mae: 0.012679, mean_q: -0.432113
  8383/100000: episode: 83, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.680, mean reward: 0.007 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.018, 10.199], loss: 0.000152, mae: 0.012918, mean_q: -0.421055
  8484/100000: episode: 84, duration: 0.549s, episode steps: 101, steps per second: 184, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.119, 10.237], loss: 0.000166, mae: 0.013549, mean_q: -0.413421
  8585/100000: episode: 85, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.754, 10.201], loss: 0.000134, mae: 0.012291, mean_q: -0.386577
  8686/100000: episode: 86, duration: 0.532s, episode steps: 101, steps per second: 190, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.782, 10.100], loss: 0.000244, mae: 0.015805, mean_q: -0.361539
  8787/100000: episode: 87, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.387, 10.114], loss: 0.000134, mae: 0.011941, mean_q: -0.364877
  8888/100000: episode: 88, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.873, 10.196], loss: 0.000171, mae: 0.013386, mean_q: -0.357386
  8989/100000: episode: 89, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.001, 10.259], loss: 0.000141, mae: 0.011989, mean_q: -0.336266
  9090/100000: episode: 90, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.010, 10.303], loss: 0.000148, mae: 0.013310, mean_q: -0.311541
  9191/100000: episode: 91, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.663, mean reward: 0.007 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.714, 10.100], loss: 0.000169, mae: 0.012975, mean_q: -0.297260
  9292/100000: episode: 92, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.157, 10.317], loss: 0.000117, mae: 0.011079, mean_q: -0.288075
  9393/100000: episode: 93, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.180, 10.257], loss: 0.000147, mae: 0.012275, mean_q: -0.258811
  9494/100000: episode: 94, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.028, 10.100], loss: 0.000165, mae: 0.012931, mean_q: -0.229776
  9595/100000: episode: 95, duration: 0.504s, episode steps: 101, steps per second: 201, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.270, 10.158], loss: 0.000155, mae: 0.012456, mean_q: -0.225480
  9696/100000: episode: 96, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.438, 10.100], loss: 0.000134, mae: 0.011776, mean_q: -0.241032
  9797/100000: episode: 97, duration: 0.511s, episode steps: 101, steps per second: 197, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.496, 10.100], loss: 0.000208, mae: 0.014247, mean_q: -0.222222
  9898/100000: episode: 98, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.855, 10.311], loss: 0.000207, mae: 0.014472, mean_q: -0.198177
  9999/100000: episode: 99, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.672, 10.293], loss: 0.000182, mae: 0.013364, mean_q: -0.173096
 10100/100000: episode: 100, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.721, 10.303], loss: 0.000209, mae: 0.014282, mean_q: -0.159584
 10201/100000: episode: 101, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.896, 10.183], loss: 0.000159, mae: 0.012586, mean_q: -0.151199
 10302/100000: episode: 102, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.100], loss: 0.000171, mae: 0.012931, mean_q: -0.142197
 10403/100000: episode: 103, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.780, 10.347], loss: 0.000171, mae: 0.013306, mean_q: -0.131055
 10504/100000: episode: 104, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.683, 10.100], loss: 0.000231, mae: 0.013967, mean_q: -0.103201
 10605/100000: episode: 105, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.882, 10.116], loss: 0.000177, mae: 0.012817, mean_q: -0.062354
 10706/100000: episode: 106, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.792, 10.113], loss: 0.000152, mae: 0.012195, mean_q: -0.061993
 10807/100000: episode: 107, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.636, 10.100], loss: 0.000119, mae: 0.010985, mean_q: -0.046364
 10908/100000: episode: 108, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.823, 10.182], loss: 0.000118, mae: 0.010739, mean_q: -0.032669
 11009/100000: episode: 109, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.733, 10.100], loss: 0.000127, mae: 0.011091, mean_q: 0.001376
 11110/100000: episode: 110, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.517, 10.100], loss: 0.000142, mae: 0.012231, mean_q: 0.032883
 11211/100000: episode: 111, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.672, mean reward: 0.007 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.001, 10.195], loss: 0.000093, mae: 0.009600, mean_q: 0.020842
 11312/100000: episode: 112, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.163, 10.133], loss: 0.000095, mae: 0.010251, mean_q: 0.037600
 11413/100000: episode: 113, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.871, 10.137], loss: 0.000089, mae: 0.009773, mean_q: 0.069475
 11514/100000: episode: 114, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.740, 10.100], loss: 0.000075, mae: 0.008801, mean_q: 0.108993
 11615/100000: episode: 115, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.462, 10.113], loss: 0.000102, mae: 0.010026, mean_q: 0.101695
 11716/100000: episode: 116, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.165, 10.100], loss: 0.000075, mae: 0.008916, mean_q: 0.135223
 11817/100000: episode: 117, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.184, 10.107], loss: 0.000075, mae: 0.008819, mean_q: 0.150717
 11918/100000: episode: 118, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.481, 10.100], loss: 0.000070, mae: 0.008345, mean_q: 0.158661
 12019/100000: episode: 119, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.651, mean reward: 0.006 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.050, 10.157], loss: 0.000075, mae: 0.009028, mean_q: 0.180271
 12120/100000: episode: 120, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.190, 10.235], loss: 0.000089, mae: 0.008955, mean_q: 0.182390
 12221/100000: episode: 121, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.256, 10.199], loss: 0.000066, mae: 0.007873, mean_q: 0.200332
 12322/100000: episode: 122, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.897, mean reward: 0.009 [0.000, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.518, 10.328], loss: 0.000055, mae: 0.007576, mean_q: 0.226992
 12423/100000: episode: 123, duration: 0.529s, episode steps: 101, steps per second: 191, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.860, 10.248], loss: 0.000071, mae: 0.008650, mean_q: 0.227546
 12524/100000: episode: 124, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.065, 10.319], loss: 0.000069, mae: 0.008156, mean_q: 0.263815
 12625/100000: episode: 125, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.872, 10.195], loss: 0.000063, mae: 0.007791, mean_q: 0.274864
 12726/100000: episode: 126, duration: 0.517s, episode steps: 101, steps per second: 196, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.581, 10.109], loss: 0.000058, mae: 0.007368, mean_q: 0.299927
 12827/100000: episode: 127, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.156, 10.230], loss: 0.000071, mae: 0.008157, mean_q: 0.308743
 12928/100000: episode: 128, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.511, 10.228], loss: 0.000061, mae: 0.007204, mean_q: 0.338960
 13029/100000: episode: 129, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.397, 10.201], loss: 0.000053, mae: 0.006784, mean_q: 0.346657
 13130/100000: episode: 130, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.495, 10.129], loss: 0.000093, mae: 0.009913, mean_q: 0.355481
 13231/100000: episode: 131, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.903, 10.307], loss: 0.000067, mae: 0.008096, mean_q: 0.379600
 13332/100000: episode: 132, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.384, 10.102], loss: 0.000055, mae: 0.007161, mean_q: 0.406873
 13433/100000: episode: 133, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.071, 10.382], loss: 0.000063, mae: 0.007755, mean_q: 0.421440
 13534/100000: episode: 134, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.666, mean reward: 0.007 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.100], loss: 0.000066, mae: 0.007860, mean_q: 0.423880
 13635/100000: episode: 135, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.364, 10.100], loss: 0.000055, mae: 0.007324, mean_q: 0.444939
 13736/100000: episode: 136, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.366, 10.100], loss: 0.000061, mae: 0.007430, mean_q: 0.483181
 13837/100000: episode: 137, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.147, 10.100], loss: 0.000090, mae: 0.009557, mean_q: 0.484975
 13938/100000: episode: 138, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.838, 10.100], loss: 0.000059, mae: 0.007533, mean_q: 0.512981
 14039/100000: episode: 139, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.765, mean reward: 0.008 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.147], loss: 0.000054, mae: 0.006475, mean_q: 0.523658
 14140/100000: episode: 140, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.855, mean reward: 0.008 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.463, 10.449], loss: 0.000067, mae: 0.007751, mean_q: 0.535249
 14241/100000: episode: 141, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.650, mean reward: 0.006 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.447, 10.276], loss: 0.000061, mae: 0.007116, mean_q: 0.569072
 14342/100000: episode: 142, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.100], loss: 0.000059, mae: 0.007525, mean_q: 0.592317
 14443/100000: episode: 143, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.204, 10.284], loss: 0.000063, mae: 0.007346, mean_q: 0.598298
 14544/100000: episode: 144, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.701, 10.345], loss: 0.000062, mae: 0.007336, mean_q: 0.601860
 14645/100000: episode: 145, duration: 0.532s, episode steps: 101, steps per second: 190, episode reward: 0.924, mean reward: 0.009 [0.000, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.098, 10.100], loss: 0.000083, mae: 0.008998, mean_q: 0.622811
 14746/100000: episode: 146, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.905, 10.194], loss: 0.000051, mae: 0.006793, mean_q: 0.641192
 14847/100000: episode: 147, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.629, 10.255], loss: 0.000056, mae: 0.006534, mean_q: 0.653563
 14948/100000: episode: 148, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.130, 10.100], loss: 0.000044, mae: 0.005749, mean_q: 0.667620
 15049/100000: episode: 149, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.178, 10.101], loss: 0.000055, mae: 0.006917, mean_q: 0.670583
 15150/100000: episode: 150, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.435, 10.383], loss: 0.000051, mae: 0.006280, mean_q: 0.681409
 15251/100000: episode: 151, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.231, 10.387], loss: 0.000071, mae: 0.007328, mean_q: 0.695086
 15352/100000: episode: 152, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.264], loss: 0.000047, mae: 0.006027, mean_q: 0.702355
 15453/100000: episode: 153, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.726, 10.100], loss: 0.000040, mae: 0.005200, mean_q: 0.713346
 15554/100000: episode: 154, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.502, 10.511], loss: 0.000059, mae: 0.006195, mean_q: 0.719629
 15655/100000: episode: 155, duration: 0.496s, episode steps: 101, steps per second: 204, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.234, 10.107], loss: 0.000088, mae: 0.007845, mean_q: 0.722828
 15756/100000: episode: 156, duration: 0.530s, episode steps: 101, steps per second: 190, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.820, 10.163], loss: 0.000040, mae: 0.005377, mean_q: 0.730537
 15857/100000: episode: 157, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.530, 10.303], loss: 0.000048, mae: 0.006100, mean_q: 0.736355
 15958/100000: episode: 158, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.322, 10.140], loss: 0.000030, mae: 0.004803, mean_q: 0.740447
 16059/100000: episode: 159, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.260, 10.208], loss: 0.000029, mae: 0.004638, mean_q: 0.741440
 16160/100000: episode: 160, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.541, 10.100], loss: 0.000037, mae: 0.005309, mean_q: 0.744902
 16261/100000: episode: 161, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.941, 10.216], loss: 0.000025, mae: 0.004541, mean_q: 0.747023
 16362/100000: episode: 162, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.590, 10.189], loss: 0.000048, mae: 0.005768, mean_q: 0.748150
 16463/100000: episode: 163, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.667, 10.100], loss: 0.000037, mae: 0.005417, mean_q: 0.749126
 16564/100000: episode: 164, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.515, 10.100], loss: 0.000039, mae: 0.004900, mean_q: 0.751479
 16665/100000: episode: 165, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.967, 10.100], loss: 0.000022, mae: 0.004371, mean_q: 0.751409
 16766/100000: episode: 166, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.322, 10.100], loss: 0.000020, mae: 0.004050, mean_q: 0.751682
 16867/100000: episode: 167, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.449, 10.100], loss: 0.000027, mae: 0.004179, mean_q: 0.753331
 16968/100000: episode: 168, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.736, 10.136], loss: 0.000032, mae: 0.004152, mean_q: 0.753704
 17069/100000: episode: 169, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.557, 10.100], loss: 0.000040, mae: 0.005346, mean_q: 0.754637
 17170/100000: episode: 170, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.933, 10.100], loss: 0.000037, mae: 0.005693, mean_q: 0.755254
 17271/100000: episode: 171, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.167, 10.121], loss: 0.000041, mae: 0.005594, mean_q: 0.755750
 17372/100000: episode: 172, duration: 0.495s, episode steps: 101, steps per second: 204, episode reward: 0.804, mean reward: 0.008 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.811, 10.392], loss: 0.000025, mae: 0.004049, mean_q: 0.756087
 17473/100000: episode: 173, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.894, 10.100], loss: 0.000047, mae: 0.004874, mean_q: 0.756881
 17574/100000: episode: 174, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.326, 10.446], loss: 0.000026, mae: 0.005317, mean_q: 0.756412
 17675/100000: episode: 175, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.180], loss: 0.000031, mae: 0.004630, mean_q: 0.757875
 17776/100000: episode: 176, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.182, 10.245], loss: 0.000029, mae: 0.004501, mean_q: 0.757399
 17877/100000: episode: 177, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.214], loss: 0.000045, mae: 0.005230, mean_q: 0.757754
 17978/100000: episode: 178, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.692, 10.267], loss: 0.000037, mae: 0.005601, mean_q: 0.757479
 18079/100000: episode: 179, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.165, 10.100], loss: 0.000032, mae: 0.004950, mean_q: 0.757599
 18180/100000: episode: 180, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.273, 10.100], loss: 0.000028, mae: 0.004670, mean_q: 0.756794
 18281/100000: episode: 181, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.776, mean reward: 0.008 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.202, 10.283], loss: 0.000035, mae: 0.005520, mean_q: 0.756038
 18382/100000: episode: 182, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.322, 10.151], loss: 0.000033, mae: 0.004525, mean_q: 0.755804
 18483/100000: episode: 183, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.839, mean reward: 0.008 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.279, 10.367], loss: 0.000030, mae: 0.004726, mean_q: 0.755703
 18584/100000: episode: 184, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.453, 10.148], loss: 0.000016, mae: 0.003344, mean_q: 0.754824
 18685/100000: episode: 185, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.672, 10.296], loss: 0.000060, mae: 0.006631, mean_q: 0.754650
 18786/100000: episode: 186, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.790, 10.121], loss: 0.000027, mae: 0.004077, mean_q: 0.754539
 18887/100000: episode: 187, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.825, mean reward: 0.008 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.165, 10.235], loss: 0.000022, mae: 0.004266, mean_q: 0.754237
 18988/100000: episode: 188, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.991, 10.100], loss: 0.000023, mae: 0.003927, mean_q: 0.754367
 19089/100000: episode: 189, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.244, 10.100], loss: 0.000020, mae: 0.003595, mean_q: 0.753852
 19190/100000: episode: 190, duration: 0.496s, episode steps: 101, steps per second: 203, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.684, 10.100], loss: 0.000018, mae: 0.003777, mean_q: 0.753072
 19291/100000: episode: 191, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.096, 10.176], loss: 0.000032, mae: 0.005248, mean_q: 0.753074
 19392/100000: episode: 192, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.798, 10.223], loss: 0.000031, mae: 0.004964, mean_q: 0.752395
 19493/100000: episode: 193, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.953, 10.120], loss: 0.000023, mae: 0.004142, mean_q: 0.752361
 19594/100000: episode: 194, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.476, 10.325], loss: 0.000033, mae: 0.005704, mean_q: 0.752103
 19695/100000: episode: 195, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.965, 10.100], loss: 0.000022, mae: 0.004469, mean_q: 0.752527
 19796/100000: episode: 196, duration: 0.511s, episode steps: 101, steps per second: 197, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.808, 10.287], loss: 0.000018, mae: 0.003774, mean_q: 0.751700
 19897/100000: episode: 197, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.100], loss: 0.000016, mae: 0.003201, mean_q: 0.750783
 19998/100000: episode: 198, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.665, 10.100], loss: 0.000025, mae: 0.004174, mean_q: 0.750903
 20099/100000: episode: 199, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.980, 10.100], loss: 0.000029, mae: 0.004714, mean_q: 0.750503
[Info] 1-TH LEVEL FOUND: 0.7936851978302002, Considering 10/100 traces
 20200/100000: episode: 200, duration: 5.388s, episode steps: 101, steps per second: 19, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.427, 10.213], loss: 0.000016, mae: 0.003316, mean_q: 0.750207
 20251/100000: episode: 201, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 0.789, mean reward: 0.015 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.905, 10.100], loss: 0.000013, mae: 0.003059, mean_q: 0.751744
 20302/100000: episode: 202, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.826, mean reward: 0.016 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.426, 10.100], loss: 0.000020, mae: 0.003616, mean_q: 0.750337
 20353/100000: episode: 203, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 0.715, mean reward: 0.014 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.686, 10.156], loss: 0.000014, mae: 0.002816, mean_q: 0.750729
 20404/100000: episode: 204, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 0.768, mean reward: 0.015 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.299, 10.100], loss: 0.000021, mae: 0.004300, mean_q: 0.750884
 20455/100000: episode: 205, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 0.696, mean reward: 0.014 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.286, 10.197], loss: 0.000017, mae: 0.003229, mean_q: 0.750845
 20506/100000: episode: 206, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.727, mean reward: 0.014 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.876, 10.230], loss: 0.000014, mae: 0.003133, mean_q: 0.749709
 20557/100000: episode: 207, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 0.689, mean reward: 0.014 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.189, 10.100], loss: 0.000013, mae: 0.003073, mean_q: 0.750263
 20608/100000: episode: 208, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 0.735, mean reward: 0.014 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.422, 10.224], loss: 0.000026, mae: 0.004574, mean_q: 0.749927
 20660/100000: episode: 209, duration: 0.265s, episode steps: 52, steps per second: 197, episode reward: 0.721, mean reward: 0.014 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.491, 10.328], loss: 0.000024, mae: 0.003834, mean_q: 0.749723
 20711/100000: episode: 210, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 0.797, mean reward: 0.016 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.577, 10.283], loss: 0.000023, mae: 0.003771, mean_q: 0.750434
 20762/100000: episode: 211, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.715, mean reward: 0.014 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.392, 10.282], loss: 0.000015, mae: 0.002924, mean_q: 0.749857
 20813/100000: episode: 212, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 0.725, mean reward: 0.014 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.288, 10.100], loss: 0.000013, mae: 0.002749, mean_q: 0.750047
 20864/100000: episode: 213, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.677, mean reward: 0.013 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.471, 10.275], loss: 0.000011, mae: 0.003085, mean_q: 0.749767
 20915/100000: episode: 214, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.703, mean reward: 0.014 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.416, 10.233], loss: 0.000020, mae: 0.003625, mean_q: 0.749385
 20966/100000: episode: 215, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.658, mean reward: 0.013 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.926, 10.100], loss: 0.000020, mae: 0.003463, mean_q: 0.749881
 21017/100000: episode: 216, duration: 0.262s, episode steps: 51, steps per second: 194, episode reward: 0.819, mean reward: 0.016 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.226, 10.150], loss: 0.000032, mae: 0.004692, mean_q: 0.749564
 21069/100000: episode: 217, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 0.713, mean reward: 0.014 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.965, 10.127], loss: 0.000025, mae: 0.004340, mean_q: 0.749070
 21120/100000: episode: 218, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.948, 10.179], loss: 0.000020, mae: 0.003644, mean_q: 0.749736
 21171/100000: episode: 219, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 0.685, mean reward: 0.013 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.215, 10.100], loss: 0.000021, mae: 0.003483, mean_q: 0.749105
 21222/100000: episode: 220, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 0.691, mean reward: 0.014 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.535, 10.100], loss: 0.000036, mae: 0.005271, mean_q: 0.749660
 21273/100000: episode: 221, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 0.784, mean reward: 0.015 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.139, 10.100], loss: 0.000031, mae: 0.004565, mean_q: 0.749986
 21325/100000: episode: 222, duration: 0.279s, episode steps: 52, steps per second: 187, episode reward: 0.719, mean reward: 0.014 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.981, 10.398], loss: 0.000045, mae: 0.006120, mean_q: 0.748795
 21377/100000: episode: 223, duration: 0.266s, episode steps: 52, steps per second: 196, episode reward: 0.723, mean reward: 0.014 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.172, 10.216], loss: 0.000020, mae: 0.003738, mean_q: 0.749652
 21428/100000: episode: 224, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 0.772, mean reward: 0.015 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.216, 10.100], loss: 0.000032, mae: 0.004802, mean_q: 0.749195
 21479/100000: episode: 225, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.779, mean reward: 0.015 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.752, 10.204], loss: 0.000020, mae: 0.003804, mean_q: 0.749241
 21530/100000: episode: 226, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 0.814, mean reward: 0.016 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.846, 10.235], loss: 0.000025, mae: 0.003878, mean_q: 0.749337
 21581/100000: episode: 227, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 0.772, mean reward: 0.015 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.957, 10.271], loss: 0.000028, mae: 0.004813, mean_q: 0.750167
 21632/100000: episode: 228, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.974, 10.165], loss: 0.000024, mae: 0.004169, mean_q: 0.750379
 21684/100000: episode: 229, duration: 0.268s, episode steps: 52, steps per second: 194, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.342, 10.181], loss: 0.000024, mae: 0.004261, mean_q: 0.750093
 21735/100000: episode: 230, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 0.730, mean reward: 0.014 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.665, 10.248], loss: 0.000034, mae: 0.004913, mean_q: 0.749790
 21786/100000: episode: 231, duration: 0.258s, episode steps: 51, steps per second: 197, episode reward: 0.690, mean reward: 0.014 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.343, 10.227], loss: 0.000027, mae: 0.004746, mean_q: 0.750601
 21837/100000: episode: 232, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 0.854, mean reward: 0.017 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.384, 10.403], loss: 0.000025, mae: 0.004083, mean_q: 0.750067
 21889/100000: episode: 233, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 0.718, mean reward: 0.014 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.166, 10.100], loss: 0.000038, mae: 0.004738, mean_q: 0.750405
 21940/100000: episode: 234, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.664, mean reward: 0.013 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.686, 10.162], loss: 0.000028, mae: 0.004261, mean_q: 0.750477
 21991/100000: episode: 235, duration: 0.248s, episode steps: 51, steps per second: 205, episode reward: 0.711, mean reward: 0.014 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.398, 10.100], loss: 0.000027, mae: 0.004511, mean_q: 0.750048
 22042/100000: episode: 236, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 0.747, mean reward: 0.015 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.397, 10.100], loss: 0.000040, mae: 0.005012, mean_q: 0.749975
 22093/100000: episode: 237, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.786, mean reward: 0.015 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.662, 10.418], loss: 0.000027, mae: 0.005405, mean_q: 0.750350
 22144/100000: episode: 238, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.687, mean reward: 0.013 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.228, 10.100], loss: 0.000027, mae: 0.004279, mean_q: 0.750462
 22195/100000: episode: 239, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 0.739, mean reward: 0.014 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.461, 10.296], loss: 0.000057, mae: 0.007354, mean_q: 0.749814
 22246/100000: episode: 240, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 0.696, mean reward: 0.014 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.359, 10.100], loss: 0.000038, mae: 0.006023, mean_q: 0.750402
 22297/100000: episode: 241, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 0.795, mean reward: 0.016 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.458, 10.138], loss: 0.000027, mae: 0.004058, mean_q: 0.750701
 22348/100000: episode: 242, duration: 0.260s, episode steps: 51, steps per second: 197, episode reward: 0.749, mean reward: 0.015 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.185, 10.241], loss: 0.000030, mae: 0.003988, mean_q: 0.750169
 22399/100000: episode: 243, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 0.717, mean reward: 0.014 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.450, 10.194], loss: 0.000035, mae: 0.004831, mean_q: 0.749536
 22450/100000: episode: 244, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.700, mean reward: 0.014 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.715, 10.159], loss: 0.000030, mae: 0.003526, mean_q: 0.750219
 22501/100000: episode: 245, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.626, mean reward: 0.012 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.212, 10.100], loss: 0.000030, mae: 0.004574, mean_q: 0.750347
 22552/100000: episode: 246, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.702, mean reward: 0.014 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.540, 10.186], loss: 0.000041, mae: 0.004636, mean_q: 0.750193
 22603/100000: episode: 247, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.839, mean reward: 0.016 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.782, 10.100], loss: 0.000023, mae: 0.003712, mean_q: 0.750351
 22654/100000: episode: 248, duration: 0.257s, episode steps: 51, steps per second: 198, episode reward: 0.691, mean reward: 0.014 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.491, 10.104], loss: 0.000025, mae: 0.003941, mean_q: 0.750188
 22705/100000: episode: 249, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.776, mean reward: 0.015 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.596, 10.100], loss: 0.000037, mae: 0.004792, mean_q: 0.750081
 22756/100000: episode: 250, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 0.666, mean reward: 0.013 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.463, 10.209], loss: 0.000032, mae: 0.004936, mean_q: 0.749130
 22808/100000: episode: 251, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 0.725, mean reward: 0.014 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.734, 10.418], loss: 0.000018, mae: 0.002978, mean_q: 0.749871
 22859/100000: episode: 252, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 0.726, mean reward: 0.014 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.493, 10.216], loss: 0.000034, mae: 0.004145, mean_q: 0.749417
 22910/100000: episode: 253, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 0.743, mean reward: 0.015 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.354, 10.100], loss: 0.000038, mae: 0.005188, mean_q: 0.749775
 22961/100000: episode: 254, duration: 0.256s, episode steps: 51, steps per second: 200, episode reward: 0.717, mean reward: 0.014 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.580, 10.230], loss: 0.000036, mae: 0.004508, mean_q: 0.750115
 23012/100000: episode: 255, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.398, 10.100], loss: 0.000036, mae: 0.005781, mean_q: 0.749618
 23064/100000: episode: 256, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 0.767, mean reward: 0.015 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.636, 10.100], loss: 0.000026, mae: 0.003742, mean_q: 0.749495
 23115/100000: episode: 257, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.644, mean reward: 0.013 [0.000, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.453, 10.255], loss: 0.000032, mae: 0.004798, mean_q: 0.749170
 23166/100000: episode: 258, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.138, 10.322], loss: 0.000049, mae: 0.005950, mean_q: 0.749345
 23218/100000: episode: 259, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 0.713, mean reward: 0.014 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.385, 10.253], loss: 0.000030, mae: 0.004197, mean_q: 0.748790
 23269/100000: episode: 260, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 0.703, mean reward: 0.014 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.658, 10.100], loss: 0.000025, mae: 0.004463, mean_q: 0.748691
 23320/100000: episode: 261, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.742, mean reward: 0.015 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.875, 10.266], loss: 0.000017, mae: 0.002752, mean_q: 0.748571
 23371/100000: episode: 262, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 0.691, mean reward: 0.014 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.626, 10.100], loss: 0.000030, mae: 0.004544, mean_q: 0.748734
 23422/100000: episode: 263, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 0.732, mean reward: 0.014 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.919, 10.197], loss: 0.000034, mae: 0.004758, mean_q: 0.748885
 23473/100000: episode: 264, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 0.706, mean reward: 0.014 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.537, 10.100], loss: 0.000027, mae: 0.004671, mean_q: 0.748133
 23525/100000: episode: 265, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 0.793, mean reward: 0.015 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.738, 10.514], loss: 0.000052, mae: 0.006375, mean_q: 0.748402
 23576/100000: episode: 266, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 0.836, mean reward: 0.016 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.346, 10.283], loss: 0.000033, mae: 0.004330, mean_q: 0.748578
 23627/100000: episode: 267, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 0.704, mean reward: 0.014 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.718, 10.264], loss: 0.000034, mae: 0.005344, mean_q: 0.748566
 23678/100000: episode: 268, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 0.743, mean reward: 0.015 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.245, 10.108], loss: 0.000022, mae: 0.003737, mean_q: 0.747736
 23729/100000: episode: 269, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.768, mean reward: 0.015 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.278, 10.162], loss: 0.000024, mae: 0.003866, mean_q: 0.747871
 23781/100000: episode: 270, duration: 0.271s, episode steps: 52, steps per second: 192, episode reward: 0.811, mean reward: 0.016 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.757, 10.415], loss: 0.000032, mae: 0.004383, mean_q: 0.747280
 23832/100000: episode: 271, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.643, mean reward: 0.013 [0.000, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.301, 10.101], loss: 0.000022, mae: 0.003662, mean_q: 0.747317
 23883/100000: episode: 272, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 0.750, mean reward: 0.015 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.308, 10.100], loss: 0.000023, mae: 0.003206, mean_q: 0.747179
 23935/100000: episode: 273, duration: 0.267s, episode steps: 52, steps per second: 195, episode reward: 0.695, mean reward: 0.013 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.473, 10.182], loss: 0.000043, mae: 0.005236, mean_q: 0.746528
 23986/100000: episode: 274, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 0.675, mean reward: 0.013 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.217, 10.280], loss: 0.000038, mae: 0.004608, mean_q: 0.747055
 24038/100000: episode: 275, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 0.672, mean reward: 0.013 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.112, 10.335], loss: 0.000025, mae: 0.003992, mean_q: 0.746351
 24089/100000: episode: 276, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.125, 10.100], loss: 0.000027, mae: 0.002807, mean_q: 0.745773
 24140/100000: episode: 277, duration: 0.281s, episode steps: 51, steps per second: 181, episode reward: 0.707, mean reward: 0.014 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.420, 10.100], loss: 0.000024, mae: 0.003272, mean_q: 0.745365
 24191/100000: episode: 278, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 0.688, mean reward: 0.013 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.072, 10.247], loss: 0.000032, mae: 0.003697, mean_q: 0.745747
 24242/100000: episode: 279, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 0.689, mean reward: 0.014 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.138, 10.100], loss: 0.000030, mae: 0.004894, mean_q: 0.745181
 24293/100000: episode: 280, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.817, mean reward: 0.016 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.695, 10.302], loss: 0.000041, mae: 0.004796, mean_q: 0.745198
 24345/100000: episode: 281, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 0.795, mean reward: 0.015 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.405, 10.169], loss: 0.000032, mae: 0.004376, mean_q: 0.744612
 24397/100000: episode: 282, duration: 0.264s, episode steps: 52, steps per second: 197, episode reward: 0.722, mean reward: 0.014 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.550, 10.369], loss: 0.000026, mae: 0.003689, mean_q: 0.744854
 24448/100000: episode: 283, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 0.681, mean reward: 0.013 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.776, 10.100], loss: 0.000024, mae: 0.003510, mean_q: 0.743977
 24499/100000: episode: 284, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 0.696, mean reward: 0.014 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.492, 10.257], loss: 0.000044, mae: 0.004437, mean_q: 0.744108
 24550/100000: episode: 285, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 0.787, mean reward: 0.015 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.006, 10.326], loss: 0.000034, mae: 0.004289, mean_q: 0.743933
 24601/100000: episode: 286, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.741, mean reward: 0.015 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.096, 10.320], loss: 0.000037, mae: 0.005006, mean_q: 0.743409
 24652/100000: episode: 287, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 0.675, mean reward: 0.013 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.605, 10.174], loss: 0.000029, mae: 0.003560, mean_q: 0.743789
 24703/100000: episode: 288, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 0.672, mean reward: 0.013 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.795, 10.192], loss: 0.000032, mae: 0.003471, mean_q: 0.743271
 24754/100000: episode: 289, duration: 0.261s, episode steps: 51, steps per second: 196, episode reward: 0.741, mean reward: 0.015 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.740, 10.100], loss: 0.000032, mae: 0.004325, mean_q: 0.743308
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7936851978302002
1
 24805/100000: episode: 290, duration: 4.479s, episode steps: 51, steps per second: 11, episode reward: 0.709, mean reward: 0.014 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.038, 10.383], loss: 0.000048, mae: 0.005698, mean_q: 0.742751
 24906/100000: episode: 291, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.262, 10.219], loss: 0.000044, mae: 0.005629, mean_q: 0.743384
 25007/100000: episode: 292, duration: 0.499s, episode steps: 101, steps per second: 203, episode reward: 0.826, mean reward: 0.008 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.694, 10.115], loss: 0.000030, mae: 0.003791, mean_q: 0.742680
 25108/100000: episode: 293, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.923, mean reward: 0.009 [0.000, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.482, 10.255], loss: 0.000041, mae: 0.004618, mean_q: 0.742517
 25209/100000: episode: 294, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.014, 10.118], loss: 0.000041, mae: 0.003597, mean_q: 0.742235
 25310/100000: episode: 295, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.342, 10.107], loss: 0.000049, mae: 0.004334, mean_q: 0.742139
 25411/100000: episode: 296, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.818, 10.425], loss: 0.000040, mae: 0.003888, mean_q: 0.741508
 25512/100000: episode: 297, duration: 0.499s, episode steps: 101, steps per second: 203, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.701, 10.160], loss: 0.000039, mae: 0.003936, mean_q: 0.740995
 25613/100000: episode: 298, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.920, 10.128], loss: 0.000044, mae: 0.004587, mean_q: 0.741055
 25714/100000: episode: 299, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.449, 10.100], loss: 0.000056, mae: 0.004911, mean_q: 0.740953
 25815/100000: episode: 300, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.874, 10.120], loss: 0.000035, mae: 0.004641, mean_q: 0.740552
 25916/100000: episode: 301, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.367, 10.100], loss: 0.000028, mae: 0.003438, mean_q: 0.740430
 26017/100000: episode: 302, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.526, 10.152], loss: 0.000039, mae: 0.004178, mean_q: 0.740073
 26118/100000: episode: 303, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.574, 10.191], loss: 0.000024, mae: 0.003491, mean_q: 0.739902
 26219/100000: episode: 304, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.417, 10.191], loss: 0.000046, mae: 0.004844, mean_q: 0.739443
 26320/100000: episode: 305, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.210, 10.100], loss: 0.000025, mae: 0.002859, mean_q: 0.739086
 26421/100000: episode: 306, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.991, 10.100], loss: 0.000014, mae: 0.002539, mean_q: 0.738614
 26522/100000: episode: 307, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.694, mean reward: 0.007 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.591, 10.168], loss: 0.000024, mae: 0.003499, mean_q: 0.738126
 26623/100000: episode: 308, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.569, 10.390], loss: 0.000040, mae: 0.003784, mean_q: 0.738014
 26724/100000: episode: 309, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.527, 10.100], loss: 0.000034, mae: 0.003791, mean_q: 0.737954
 26825/100000: episode: 310, duration: 0.499s, episode steps: 101, steps per second: 202, episode reward: 0.741, mean reward: 0.007 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.864, 10.278], loss: 0.000030, mae: 0.004424, mean_q: 0.736903
 26926/100000: episode: 311, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.843, mean reward: 0.008 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.112, 10.241], loss: 0.000025, mae: 0.003386, mean_q: 0.736715
 27027/100000: episode: 312, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.196, 10.153], loss: 0.000029, mae: 0.004120, mean_q: 0.736247
 27128/100000: episode: 313, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.024, 10.100], loss: 0.000039, mae: 0.004601, mean_q: 0.735889
 27229/100000: episode: 314, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.433, 10.262], loss: 0.000027, mae: 0.003809, mean_q: 0.735380
 27330/100000: episode: 315, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.146, 10.140], loss: 0.000021, mae: 0.002929, mean_q: 0.734920
 27431/100000: episode: 316, duration: 0.499s, episode steps: 101, steps per second: 203, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.550, 10.408], loss: 0.000030, mae: 0.003520, mean_q: 0.734414
 27532/100000: episode: 317, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.021, 10.100], loss: 0.000033, mae: 0.003769, mean_q: 0.734638
 27633/100000: episode: 318, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.211, 10.348], loss: 0.000035, mae: 0.004265, mean_q: 0.734370
 27734/100000: episode: 319, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.828, 10.253], loss: 0.000023, mae: 0.003274, mean_q: 0.734145
 27835/100000: episode: 320, duration: 0.511s, episode steps: 101, steps per second: 197, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.112, 10.356], loss: 0.000019, mae: 0.002911, mean_q: 0.733945
 27936/100000: episode: 321, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.100], loss: 0.000030, mae: 0.003283, mean_q: 0.734050
 28037/100000: episode: 322, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.806, 10.249], loss: 0.000031, mae: 0.004299, mean_q: 0.733824
 28138/100000: episode: 323, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.590, 10.100], loss: 0.000026, mae: 0.003486, mean_q: 0.733446
 28239/100000: episode: 324, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.858, 10.100], loss: 0.000021, mae: 0.003481, mean_q: 0.733339
 28340/100000: episode: 325, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.682, 10.100], loss: 0.000029, mae: 0.003666, mean_q: 0.733733
 28441/100000: episode: 326, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.858, 10.189], loss: 0.000032, mae: 0.004692, mean_q: 0.733543
 28542/100000: episode: 327, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.906, mean reward: 0.009 [0.000, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.709, 10.217], loss: 0.000021, mae: 0.002943, mean_q: 0.734037
 28643/100000: episode: 328, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.900, 10.227], loss: 0.000036, mae: 0.003865, mean_q: 0.733652
 28744/100000: episode: 329, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.976, 10.212], loss: 0.000028, mae: 0.003500, mean_q: 0.733854
 28845/100000: episode: 330, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.069, 10.360], loss: 0.000023, mae: 0.003674, mean_q: 0.734119
 28946/100000: episode: 331, duration: 0.543s, episode steps: 101, steps per second: 186, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.120, 10.240], loss: 0.000035, mae: 0.003709, mean_q: 0.734208
 29047/100000: episode: 332, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.185, 10.385], loss: 0.000040, mae: 0.004140, mean_q: 0.734788
 29148/100000: episode: 333, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.735, 10.427], loss: 0.000025, mae: 0.003229, mean_q: 0.735188
 29249/100000: episode: 334, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.100], loss: 0.000016, mae: 0.002909, mean_q: 0.735557
 29350/100000: episode: 335, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.817, mean reward: 0.008 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.984, 10.373], loss: 0.000018, mae: 0.002465, mean_q: 0.735674
 29451/100000: episode: 336, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.244, 10.284], loss: 0.000029, mae: 0.004112, mean_q: 0.736220
 29552/100000: episode: 337, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.821, mean reward: 0.008 [0.000, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.759, 10.282], loss: 0.000018, mae: 0.002942, mean_q: 0.736669
 29653/100000: episode: 338, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.797, 10.100], loss: 0.000016, mae: 0.003014, mean_q: 0.737066
 29754/100000: episode: 339, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.128, 10.242], loss: 0.000044, mae: 0.004732, mean_q: 0.738579
 29855/100000: episode: 340, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.071, 10.227], loss: 0.000021, mae: 0.003357, mean_q: 0.738356
 29956/100000: episode: 341, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.024, 10.100], loss: 0.000018, mae: 0.003184, mean_q: 0.738801
 30057/100000: episode: 342, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.900, 10.376], loss: 0.000024, mae: 0.003716, mean_q: 0.739067
 30158/100000: episode: 343, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.161], loss: 0.000028, mae: 0.003862, mean_q: 0.739523
 30259/100000: episode: 344, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.871, mean reward: 0.009 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.804, 10.100], loss: 0.000016, mae: 0.002539, mean_q: 0.740457
 30360/100000: episode: 345, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.048, 10.114], loss: 0.000026, mae: 0.004128, mean_q: 0.740751
 30461/100000: episode: 346, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 1.008, mean reward: 0.010 [0.000, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.614, 10.249], loss: 0.000036, mae: 0.004698, mean_q: 0.741618
 30562/100000: episode: 347, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.920, 10.255], loss: 0.000020, mae: 0.003159, mean_q: 0.741754
 30663/100000: episode: 348, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.272], loss: 0.000033, mae: 0.003282, mean_q: 0.742540
 30764/100000: episode: 349, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.041, 10.282], loss: 0.000038, mae: 0.004894, mean_q: 0.742768
 30865/100000: episode: 350, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.316, 10.116], loss: 0.000042, mae: 0.004158, mean_q: 0.743409
 30966/100000: episode: 351, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.784, 10.120], loss: 0.000026, mae: 0.003041, mean_q: 0.742831
 31067/100000: episode: 352, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.100], loss: 0.000030, mae: 0.003544, mean_q: 0.743677
 31168/100000: episode: 353, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.527, 10.100], loss: 0.000035, mae: 0.003656, mean_q: 0.744351
 31269/100000: episode: 354, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.820, 10.175], loss: 0.000029, mae: 0.003144, mean_q: 0.745250
 31370/100000: episode: 355, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.431, 10.100], loss: 0.000014, mae: 0.002539, mean_q: 0.745172
 31471/100000: episode: 356, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.100], loss: 0.000040, mae: 0.004394, mean_q: 0.745140
 31572/100000: episode: 357, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.892, 10.219], loss: 0.000019, mae: 0.003045, mean_q: 0.745940
 31673/100000: episode: 358, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.238, 10.293], loss: 0.000018, mae: 0.003306, mean_q: 0.746212
 31774/100000: episode: 359, duration: 0.504s, episode steps: 101, steps per second: 201, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.599, 10.218], loss: 0.000014, mae: 0.002244, mean_q: 0.746028
 31875/100000: episode: 360, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.501, 10.331], loss: 0.000052, mae: 0.005028, mean_q: 0.746644
 31976/100000: episode: 361, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.214, 10.100], loss: 0.000030, mae: 0.003836, mean_q: 0.746695
 32077/100000: episode: 362, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.817, 10.128], loss: 0.000041, mae: 0.005103, mean_q: 0.747435
 32178/100000: episode: 363, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.685, 10.100], loss: 0.000030, mae: 0.003204, mean_q: 0.747946
 32279/100000: episode: 364, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.248, 10.100], loss: 0.000030, mae: 0.003966, mean_q: 0.747363
 32380/100000: episode: 365, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.393, 10.202], loss: 0.000019, mae: 0.002817, mean_q: 0.747216
 32481/100000: episode: 366, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.079, 10.100], loss: 0.000025, mae: 0.003358, mean_q: 0.748009
 32582/100000: episode: 367, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.476, 10.197], loss: 0.000014, mae: 0.002295, mean_q: 0.748043
 32683/100000: episode: 368, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.885, 10.100], loss: 0.000039, mae: 0.004330, mean_q: 0.748765
 32784/100000: episode: 369, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.259, 10.230], loss: 0.000026, mae: 0.003892, mean_q: 0.749217
 32885/100000: episode: 370, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.221, 10.180], loss: 0.000019, mae: 0.002611, mean_q: 0.749222
 32986/100000: episode: 371, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.599, 10.246], loss: 0.000023, mae: 0.002866, mean_q: 0.748973
 33087/100000: episode: 372, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.691, 10.100], loss: 0.000017, mae: 0.003190, mean_q: 0.749685
 33188/100000: episode: 373, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.547, 10.100], loss: 0.000016, mae: 0.002484, mean_q: 0.749426
 33289/100000: episode: 374, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.109], loss: 0.000017, mae: 0.002968, mean_q: 0.750023
 33390/100000: episode: 375, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.868, 10.100], loss: 0.000048, mae: 0.003806, mean_q: 0.749944
 33491/100000: episode: 376, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.729, 10.105], loss: 0.000023, mae: 0.003193, mean_q: 0.750630
 33592/100000: episode: 377, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.642, 10.155], loss: 0.000011, mae: 0.002237, mean_q: 0.751286
 33693/100000: episode: 378, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.836, 10.244], loss: 0.000013, mae: 0.002812, mean_q: 0.750204
 33794/100000: episode: 379, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.673, 10.100], loss: 0.000025, mae: 0.003701, mean_q: 0.750921
 33895/100000: episode: 380, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.774, mean reward: 0.008 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.906, 10.162], loss: 0.000014, mae: 0.002641, mean_q: 0.750999
 33996/100000: episode: 381, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.869, 10.100], loss: 0.000032, mae: 0.004154, mean_q: 0.751749
 34097/100000: episode: 382, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.164, 10.100], loss: 0.000017, mae: 0.002856, mean_q: 0.751483
 34198/100000: episode: 383, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.121, 10.234], loss: 0.000032, mae: 0.003826, mean_q: 0.752032
 34299/100000: episode: 384, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.106], loss: 0.000012, mae: 0.002341, mean_q: 0.752138
 34400/100000: episode: 385, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.101, 10.118], loss: 0.000014, mae: 0.002328, mean_q: 0.752399
 34501/100000: episode: 386, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.046, 10.460], loss: 0.000013, mae: 0.002626, mean_q: 0.752297
 34602/100000: episode: 387, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.414, 10.100], loss: 0.000046, mae: 0.004456, mean_q: 0.752777
 34703/100000: episode: 388, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.491, 10.495], loss: 0.000015, mae: 0.002630, mean_q: 0.753196
 34804/100000: episode: 389, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.746, mean reward: 0.007 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.887, 10.104], loss: 0.000024, mae: 0.002948, mean_q: 0.753166
[Info] 1-TH LEVEL FOUND: 0.7746196985244751, Considering 10/100 traces
 34905/100000: episode: 390, duration: 4.901s, episode steps: 101, steps per second: 21, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.100], loss: 0.000015, mae: 0.002715, mean_q: 0.753404
 34943/100000: episode: 391, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 0.817, mean reward: 0.021 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.285, 10.238], loss: 0.000010, mae: 0.002494, mean_q: 0.754147
 34981/100000: episode: 392, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 0.635, mean reward: 0.017 [0.000, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.667, 10.185], loss: 0.000018, mae: 0.002546, mean_q: 0.753316
 35015/100000: episode: 393, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 0.672, mean reward: 0.020 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.485, 10.100], loss: 0.000024, mae: 0.002562, mean_q: 0.754454
 35050/100000: episode: 394, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 0.658, mean reward: 0.019 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.311, 10.113], loss: 0.000022, mae: 0.002434, mean_q: 0.753411
 35081/100000: episode: 395, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 0.712, mean reward: 0.023 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.942, 10.100], loss: 0.000016, mae: 0.003072, mean_q: 0.753965
 35114/100000: episode: 396, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 0.811, mean reward: 0.025 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.487, 10.137], loss: 0.000035, mae: 0.004951, mean_q: 0.753866
 35147/100000: episode: 397, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 0.645, mean reward: 0.020 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.611, 10.100], loss: 0.000032, mae: 0.004422, mean_q: 0.754038
 35183/100000: episode: 398, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 0.674, mean reward: 0.019 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.483, 10.100], loss: 0.000017, mae: 0.002400, mean_q: 0.754833
 35216/100000: episode: 399, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 0.727, mean reward: 0.022 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.284, 10.100], loss: 0.000054, mae: 0.003574, mean_q: 0.755127
 35251/100000: episode: 400, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 0.667, mean reward: 0.019 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.363, 10.100], loss: 0.000025, mae: 0.003071, mean_q: 0.754920
 35283/100000: episode: 401, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 0.795, mean reward: 0.025 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.609, 10.450], loss: 0.000032, mae: 0.004818, mean_q: 0.753958
 35321/100000: episode: 402, duration: 0.212s, episode steps: 38, steps per second: 180, episode reward: 0.713, mean reward: 0.019 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.286, 10.100], loss: 0.000032, mae: 0.002975, mean_q: 0.754816
 35356/100000: episode: 403, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 0.850, mean reward: 0.024 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.224, 10.100], loss: 0.000015, mae: 0.002353, mean_q: 0.755620
 35388/100000: episode: 404, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 0.682, mean reward: 0.021 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.183, 10.195], loss: 0.000026, mae: 0.003030, mean_q: 0.754598
 35420/100000: episode: 405, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.824, mean reward: 0.026 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.551, 10.100], loss: 0.000039, mae: 0.004293, mean_q: 0.754421
 35455/100000: episode: 406, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 0.789, mean reward: 0.023 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.245, 10.100], loss: 0.000027, mae: 0.003203, mean_q: 0.754029
 35487/100000: episode: 407, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.666, mean reward: 0.021 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.898, 10.109], loss: 0.000029, mae: 0.003343, mean_q: 0.754837
 35518/100000: episode: 408, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 0.683, mean reward: 0.022 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.070, 10.100], loss: 0.000023, mae: 0.002501, mean_q: 0.755114
 35551/100000: episode: 409, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 0.746, mean reward: 0.023 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.035, 10.100], loss: 0.000021, mae: 0.002855, mean_q: 0.755000
 35585/100000: episode: 410, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 0.674, mean reward: 0.020 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.188, 10.100], loss: 0.000023, mae: 0.002837, mean_q: 0.755238
 35619/100000: episode: 411, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 0.658, mean reward: 0.019 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.587, 10.128], loss: 0.000021, mae: 0.003161, mean_q: 0.754820
 35657/100000: episode: 412, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 0.697, mean reward: 0.018 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.473, 10.167], loss: 0.000028, mae: 0.003360, mean_q: 0.752963
 35689/100000: episode: 413, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 0.731, mean reward: 0.023 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.191, 10.349], loss: 0.000024, mae: 0.003720, mean_q: 0.754404
 35721/100000: episode: 414, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 0.737, mean reward: 0.023 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.961, 10.100], loss: 0.000037, mae: 0.004177, mean_q: 0.754462
 35752/100000: episode: 415, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 0.718, mean reward: 0.023 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.234, 10.102], loss: 0.000024, mae: 0.003577, mean_q: 0.754569
 35787/100000: episode: 416, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 0.779, mean reward: 0.022 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.002, 10.100], loss: 0.000036, mae: 0.004864, mean_q: 0.753560
 35821/100000: episode: 417, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 0.620, mean reward: 0.018 [0.000, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.465, 10.137], loss: 0.000011, mae: 0.003023, mean_q: 0.754040
 35856/100000: episode: 418, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 0.723, mean reward: 0.021 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.286, 10.100], loss: 0.000030, mae: 0.002943, mean_q: 0.754222
 35892/100000: episode: 419, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 0.832, mean reward: 0.023 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.413, 10.100], loss: 0.000032, mae: 0.004122, mean_q: 0.754487
 35928/100000: episode: 420, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 0.648, mean reward: 0.018 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.132, 10.114], loss: 0.000022, mae: 0.003213, mean_q: 0.754125
 35959/100000: episode: 421, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 0.651, mean reward: 0.021 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.179, 10.100], loss: 0.000040, mae: 0.004485, mean_q: 0.753889
 35993/100000: episode: 422, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 0.726, mean reward: 0.021 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-2.530, 10.100], loss: 0.000056, mae: 0.004725, mean_q: 0.753947
 36026/100000: episode: 423, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 0.745, mean reward: 0.023 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.262, 10.100], loss: 0.000050, mae: 0.005527, mean_q: 0.754172
 36061/100000: episode: 424, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 0.731, mean reward: 0.021 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.416, 10.140], loss: 0.000053, mae: 0.005476, mean_q: 0.754892
 36096/100000: episode: 425, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 0.727, mean reward: 0.021 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.704, 10.100], loss: 0.000024, mae: 0.003266, mean_q: 0.754076
 36134/100000: episode: 426, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 0.631, mean reward: 0.017 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.401, 10.221], loss: 0.000026, mae: 0.002948, mean_q: 0.753597
 36166/100000: episode: 427, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 0.802, mean reward: 0.025 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.635, 10.100], loss: 0.000016, mae: 0.002067, mean_q: 0.754114
 36201/100000: episode: 428, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 0.679, mean reward: 0.019 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.825, 10.100], loss: 0.000021, mae: 0.002666, mean_q: 0.753329
 36239/100000: episode: 429, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 0.760, mean reward: 0.020 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.140, 10.100], loss: 0.000017, mae: 0.001885, mean_q: 0.754030
 36270/100000: episode: 430, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 0.613, mean reward: 0.020 [0.000, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.076, 10.100], loss: 0.000035, mae: 0.003482, mean_q: 0.754777
 36303/100000: episode: 431, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 0.656, mean reward: 0.020 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.785, 10.100], loss: 0.000030, mae: 0.003322, mean_q: 0.753762
 36334/100000: episode: 432, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 0.755, mean reward: 0.024 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.357, 10.100], loss: 0.000042, mae: 0.004253, mean_q: 0.752764
 36366/100000: episode: 433, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 0.756, mean reward: 0.024 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.079, 10.100], loss: 0.000034, mae: 0.004101, mean_q: 0.752663
 36400/100000: episode: 434, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 0.761, mean reward: 0.022 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.315, 10.100], loss: 0.000029, mae: 0.002710, mean_q: 0.752259
 36432/100000: episode: 435, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 0.676, mean reward: 0.021 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.372, 10.133], loss: 0.000036, mae: 0.003850, mean_q: 0.753071
 36466/100000: episode: 436, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 0.714, mean reward: 0.021 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.489, 10.100], loss: 0.000029, mae: 0.004109, mean_q: 0.752833
 36500/100000: episode: 437, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 0.652, mean reward: 0.019 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.856, 10.121], loss: 0.000052, mae: 0.005062, mean_q: 0.752184
 36533/100000: episode: 438, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 0.653, mean reward: 0.020 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.667, 10.343], loss: 0.000054, mae: 0.004992, mean_q: 0.752456
 36566/100000: episode: 439, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 0.710, mean reward: 0.022 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.763, 10.207], loss: 0.000040, mae: 0.002950, mean_q: 0.751799
 36598/100000: episode: 440, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.650, mean reward: 0.020 [0.000, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.078, 10.120], loss: 0.000030, mae: 0.004276, mean_q: 0.752256
 36632/100000: episode: 441, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 0.751, mean reward: 0.022 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.429, 10.100], loss: 0.000039, mae: 0.002840, mean_q: 0.751888
 36663/100000: episode: 442, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 0.798, mean reward: 0.026 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.279, 10.100], loss: 0.000040, mae: 0.003243, mean_q: 0.751912
 36698/100000: episode: 443, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 0.708, mean reward: 0.020 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.636, 10.108], loss: 0.000053, mae: 0.005571, mean_q: 0.751393
 36729/100000: episode: 444, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 0.732, mean reward: 0.024 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.157, 10.183], loss: 0.000047, mae: 0.004087, mean_q: 0.751589
 36767/100000: episode: 445, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 0.664, mean reward: 0.017 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.448, 10.100], loss: 0.000032, mae: 0.002720, mean_q: 0.751310
 36800/100000: episode: 446, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 0.723, mean reward: 0.022 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.696, 10.350], loss: 0.000039, mae: 0.003929, mean_q: 0.751997
 36832/100000: episode: 447, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.686, mean reward: 0.021 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.442, 10.100], loss: 0.000018, mae: 0.002396, mean_q: 0.750606
 36870/100000: episode: 448, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 0.677, mean reward: 0.018 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.050, 10.184], loss: 0.000049, mae: 0.004434, mean_q: 0.750500
 36905/100000: episode: 449, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 0.666, mean reward: 0.019 [0.000, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.853, 10.147], loss: 0.000067, mae: 0.006336, mean_q: 0.750469
 36943/100000: episode: 450, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 0.677, mean reward: 0.018 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.470, 10.192], loss: 0.000029, mae: 0.003171, mean_q: 0.750577
 36975/100000: episode: 451, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.703, mean reward: 0.022 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.683, 10.100], loss: 0.000046, mae: 0.004408, mean_q: 0.750066
 37010/100000: episode: 452, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 0.728, mean reward: 0.021 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.704, 10.100], loss: 0.000031, mae: 0.004158, mean_q: 0.749269
 37045/100000: episode: 453, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 0.700, mean reward: 0.020 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.241, 10.279], loss: 0.000028, mae: 0.002826, mean_q: 0.750086
 37077/100000: episode: 454, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.648, mean reward: 0.020 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.825, 10.100], loss: 0.000028, mae: 0.003318, mean_q: 0.748288
 37108/100000: episode: 455, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 0.654, mean reward: 0.021 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.273, 10.100], loss: 0.000023, mae: 0.002233, mean_q: 0.749774
 37139/100000: episode: 456, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 0.691, mean reward: 0.022 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.161, 10.161], loss: 0.000032, mae: 0.004420, mean_q: 0.749414
 37175/100000: episode: 457, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 0.759, mean reward: 0.021 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.234, 10.212], loss: 0.000033, mae: 0.003530, mean_q: 0.748115
 37211/100000: episode: 458, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 0.664, mean reward: 0.018 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.369, 10.145], loss: 0.000024, mae: 0.003459, mean_q: 0.748416
 37246/100000: episode: 459, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 0.663, mean reward: 0.019 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.730, 10.100], loss: 0.000027, mae: 0.003373, mean_q: 0.747616
 37282/100000: episode: 460, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 0.675, mean reward: 0.019 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.210, 10.100], loss: 0.000023, mae: 0.003119, mean_q: 0.747661
 37317/100000: episode: 461, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 0.721, mean reward: 0.021 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.200, 10.100], loss: 0.000035, mae: 0.004042, mean_q: 0.746840
 37352/100000: episode: 462, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 0.725, mean reward: 0.021 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.342, 10.100], loss: 0.000045, mae: 0.003832, mean_q: 0.746964
 37383/100000: episode: 463, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 0.693, mean reward: 0.022 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.141, 10.360], loss: 0.000041, mae: 0.004196, mean_q: 0.746291
 37419/100000: episode: 464, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 0.779, mean reward: 0.022 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.134, 10.100], loss: 0.000036, mae: 0.003686, mean_q: 0.745876
 37450/100000: episode: 465, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 0.697, mean reward: 0.022 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.233, 10.100], loss: 0.000021, mae: 0.002864, mean_q: 0.745248
 37482/100000: episode: 466, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.698, mean reward: 0.022 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.706, 10.100], loss: 0.000085, mae: 0.006508, mean_q: 0.745124
 37520/100000: episode: 467, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 0.902, mean reward: 0.024 [0.000, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.420, 10.100], loss: 0.000043, mae: 0.004358, mean_q: 0.744314
 37551/100000: episode: 468, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 0.632, mean reward: 0.020 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.245, 10.100], loss: 0.000046, mae: 0.003349, mean_q: 0.744276
 37586/100000: episode: 469, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 0.704, mean reward: 0.020 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.003, 10.100], loss: 0.000031, mae: 0.002986, mean_q: 0.744086
 37621/100000: episode: 470, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 0.709, mean reward: 0.020 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.347, 10.100], loss: 0.000011, mae: 0.001572, mean_q: 0.744324
 37653/100000: episode: 471, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 0.684, mean reward: 0.021 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.732, 10.113], loss: 0.000018, mae: 0.002042, mean_q: 0.743716
 37687/100000: episode: 472, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 0.656, mean reward: 0.019 [0.000, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.108, 10.100], loss: 0.000024, mae: 0.002822, mean_q: 0.743958
 37719/100000: episode: 473, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 0.738, mean reward: 0.023 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.655, 10.109], loss: 0.000044, mae: 0.003501, mean_q: 0.741882
 37757/100000: episode: 474, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 0.700, mean reward: 0.018 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.344, 10.100], loss: 0.000039, mae: 0.003745, mean_q: 0.742427
 37789/100000: episode: 475, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 0.720, mean reward: 0.022 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.492, 10.257], loss: 0.000028, mae: 0.002881, mean_q: 0.741346
 37822/100000: episode: 476, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 0.673, mean reward: 0.020 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.799, 10.100], loss: 0.000069, mae: 0.004629, mean_q: 0.742037
 37857/100000: episode: 477, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 0.732, mean reward: 0.021 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.514, 10.100], loss: 0.000036, mae: 0.003157, mean_q: 0.741541
 37891/100000: episode: 478, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 0.725, mean reward: 0.021 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.173], loss: 0.000026, mae: 0.002694, mean_q: 0.740535
 37925/100000: episode: 479, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 0.694, mean reward: 0.020 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.223], loss: 0.000030, mae: 0.003064, mean_q: 0.739578
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7746196985244751
1
 37960/100000: episode: 480, duration: 4.394s, episode steps: 35, steps per second: 8, episode reward: 0.729, mean reward: 0.021 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.874, 10.100], loss: 0.000055, mae: 0.003919, mean_q: 0.740327
 38061/100000: episode: 481, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.621, 10.494], loss: 0.000052, mae: 0.004657, mean_q: 0.739971
 38162/100000: episode: 482, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.587, 10.346], loss: 0.000043, mae: 0.004259, mean_q: 0.738976
 38263/100000: episode: 483, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.020, 10.100], loss: 0.000040, mae: 0.003678, mean_q: 0.739025
 38364/100000: episode: 484, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.596, 10.100], loss: 0.000045, mae: 0.004261, mean_q: 0.737896
 38465/100000: episode: 485, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.630, mean reward: 0.006 [0.000, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.145], loss: 0.000059, mae: 0.004085, mean_q: 0.736911
 38566/100000: episode: 486, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.501, 10.100], loss: 0.000067, mae: 0.004313, mean_q: 0.737119
 38667/100000: episode: 487, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.689, 10.100], loss: 0.000054, mae: 0.004693, mean_q: 0.736560
 38768/100000: episode: 488, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.322, 10.133], loss: 0.000043, mae: 0.003313, mean_q: 0.736107
 38869/100000: episode: 489, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.174, 10.100], loss: 0.000059, mae: 0.004816, mean_q: 0.735084
 38970/100000: episode: 490, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.124, 10.100], loss: 0.000044, mae: 0.003574, mean_q: 0.734869
 39071/100000: episode: 491, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.616, 10.100], loss: 0.000044, mae: 0.003621, mean_q: 0.733790
 39172/100000: episode: 492, duration: 0.534s, episode steps: 101, steps per second: 189, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.541, 10.100], loss: 0.000048, mae: 0.004338, mean_q: 0.733610
 39273/100000: episode: 493, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.609, 10.100], loss: 0.000038, mae: 0.003845, mean_q: 0.733060
 39374/100000: episode: 494, duration: 0.537s, episode steps: 101, steps per second: 188, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.039, 10.100], loss: 0.000040, mae: 0.003267, mean_q: 0.732271
 39475/100000: episode: 495, duration: 0.496s, episode steps: 101, steps per second: 204, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.421, 10.166], loss: 0.000043, mae: 0.003245, mean_q: 0.732524
 39576/100000: episode: 496, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.812, mean reward: 0.008 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.476, 10.437], loss: 0.000044, mae: 0.004049, mean_q: 0.731992
 39677/100000: episode: 497, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.670, 10.228], loss: 0.000035, mae: 0.003250, mean_q: 0.731612
 39778/100000: episode: 498, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.875, 10.257], loss: 0.000036, mae: 0.003603, mean_q: 0.731546
 39879/100000: episode: 499, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.055, 10.296], loss: 0.000034, mae: 0.003432, mean_q: 0.731048
 39980/100000: episode: 500, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.886, mean reward: 0.009 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.448, 10.267], loss: 0.000027, mae: 0.003103, mean_q: 0.731172
 40081/100000: episode: 501, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.691, 10.119], loss: 0.000056, mae: 0.003813, mean_q: 0.731521
 40182/100000: episode: 502, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.282, 10.100], loss: 0.000031, mae: 0.003320, mean_q: 0.731610
 40283/100000: episode: 503, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.268, 10.222], loss: 0.000038, mae: 0.003393, mean_q: 0.732283
 40384/100000: episode: 504, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.705, 10.100], loss: 0.000030, mae: 0.002717, mean_q: 0.732074
 40485/100000: episode: 505, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.334, 10.100], loss: 0.000019, mae: 0.002113, mean_q: 0.732066
 40586/100000: episode: 506, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-2.002, 10.254], loss: 0.000021, mae: 0.002761, mean_q: 0.732172
 40687/100000: episode: 507, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.799, 10.258], loss: 0.000034, mae: 0.002938, mean_q: 0.731937
 40788/100000: episode: 508, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.973, 10.100], loss: 0.000041, mae: 0.003356, mean_q: 0.732862
 40889/100000: episode: 509, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.514, 10.195], loss: 0.000031, mae: 0.003447, mean_q: 0.732466
 40990/100000: episode: 510, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.585, 10.175], loss: 0.000035, mae: 0.003331, mean_q: 0.732692
 41091/100000: episode: 511, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.524, 10.106], loss: 0.000022, mae: 0.002661, mean_q: 0.732771
 41192/100000: episode: 512, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.166, 10.173], loss: 0.000032, mae: 0.003299, mean_q: 0.732645
 41293/100000: episode: 513, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.212, 10.100], loss: 0.000029, mae: 0.003052, mean_q: 0.732530
 41394/100000: episode: 514, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.086, 10.177], loss: 0.000030, mae: 0.002710, mean_q: 0.732766
 41495/100000: episode: 515, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.531, 10.164], loss: 0.000019, mae: 0.002672, mean_q: 0.733127
 41596/100000: episode: 516, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.361], loss: 0.000020, mae: 0.003199, mean_q: 0.733051
 41697/100000: episode: 517, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.461, 10.100], loss: 0.000028, mae: 0.002484, mean_q: 0.732687
 41798/100000: episode: 518, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.172], loss: 0.000028, mae: 0.002871, mean_q: 0.733742
 41899/100000: episode: 519, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.544, 10.173], loss: 0.000036, mae: 0.003135, mean_q: 0.734192
 42000/100000: episode: 520, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.600, 10.100], loss: 0.000020, mae: 0.002434, mean_q: 0.733842
 42101/100000: episode: 521, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.810, 10.100], loss: 0.000015, mae: 0.002254, mean_q: 0.733966
 42202/100000: episode: 522, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.956, 10.100], loss: 0.000018, mae: 0.002522, mean_q: 0.733845
 42303/100000: episode: 523, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.888, 10.308], loss: 0.000024, mae: 0.003239, mean_q: 0.734135
 42404/100000: episode: 524, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.522, 10.304], loss: 0.000014, mae: 0.001959, mean_q: 0.734377
 42505/100000: episode: 525, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.109, 10.260], loss: 0.000017, mae: 0.002937, mean_q: 0.734621
 42606/100000: episode: 526, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.757, 10.122], loss: 0.000015, mae: 0.002299, mean_q: 0.734768
 42707/100000: episode: 527, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.836, mean reward: 0.008 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.539, 10.162], loss: 0.000021, mae: 0.002705, mean_q: 0.735468
 42808/100000: episode: 528, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.351, 10.100], loss: 0.000011, mae: 0.002054, mean_q: 0.735376
 42909/100000: episode: 529, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.719, 10.266], loss: 0.000012, mae: 0.002105, mean_q: 0.734854
 43010/100000: episode: 530, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.352, 10.220], loss: 0.000020, mae: 0.003082, mean_q: 0.734842
 43111/100000: episode: 531, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.575, 10.100], loss: 0.000013, mae: 0.002248, mean_q: 0.734921
 43212/100000: episode: 532, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.481, 10.100], loss: 0.000017, mae: 0.002342, mean_q: 0.735694
 43313/100000: episode: 533, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.322, 10.134], loss: 0.000014, mae: 0.002177, mean_q: 0.735396
 43414/100000: episode: 534, duration: 0.532s, episode steps: 101, steps per second: 190, episode reward: 0.740, mean reward: 0.007 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.657, 10.377], loss: 0.000004, mae: 0.001529, mean_q: 0.735482
 43515/100000: episode: 535, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.806, mean reward: 0.008 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.596, 10.504], loss: 0.000009, mae: 0.002127, mean_q: 0.735699
 43616/100000: episode: 536, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.309, 10.219], loss: 0.000008, mae: 0.001713, mean_q: 0.735080
 43717/100000: episode: 537, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.668, mean reward: 0.007 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.152, 10.130], loss: 0.000018, mae: 0.002567, mean_q: 0.735307
 43818/100000: episode: 538, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.338, 10.100], loss: 0.000008, mae: 0.001535, mean_q: 0.734771
 43919/100000: episode: 539, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.814, 10.100], loss: 0.000013, mae: 0.002080, mean_q: 0.735809
 44020/100000: episode: 540, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.631, 10.100], loss: 0.000010, mae: 0.002020, mean_q: 0.735726
 44121/100000: episode: 541, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.219, 10.100], loss: 0.000012, mae: 0.002406, mean_q: 0.735633
 44222/100000: episode: 542, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.844, mean reward: 0.008 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.685, 10.120], loss: 0.000010, mae: 0.001868, mean_q: 0.735268
 44323/100000: episode: 543, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.997, 10.186], loss: 0.000014, mae: 0.002037, mean_q: 0.736074
 44424/100000: episode: 544, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.866, mean reward: 0.009 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.198, 10.100], loss: 0.000012, mae: 0.002584, mean_q: 0.735981
 44525/100000: episode: 545, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.092, 10.275], loss: 0.000013, mae: 0.002508, mean_q: 0.735938
 44626/100000: episode: 546, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.921, 10.285], loss: 0.000016, mae: 0.002663, mean_q: 0.736067
 44727/100000: episode: 547, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.311, 10.129], loss: 0.000014, mae: 0.002053, mean_q: 0.736418
 44828/100000: episode: 548, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.701, mean reward: 0.007 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.277, 10.209], loss: 0.000014, mae: 0.002287, mean_q: 0.737071
 44929/100000: episode: 549, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.941, mean reward: 0.009 [0.000, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.760, 10.100], loss: 0.000017, mae: 0.002982, mean_q: 0.737465
 45030/100000: episode: 550, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.962, 10.319], loss: 0.000019, mae: 0.002565, mean_q: 0.737229
 45131/100000: episode: 551, duration: 0.494s, episode steps: 101, steps per second: 204, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.362, 10.254], loss: 0.000013, mae: 0.002110, mean_q: 0.737498
 45232/100000: episode: 552, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.789, mean reward: 0.008 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.302, 10.100], loss: 0.000028, mae: 0.002692, mean_q: 0.738161
 45333/100000: episode: 553, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-0.422, 10.337], loss: 0.000007, mae: 0.001625, mean_q: 0.737461
 45434/100000: episode: 554, duration: 0.517s, episode steps: 101, steps per second: 196, episode reward: 0.696, mean reward: 0.007 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.430, 10.100], loss: 0.000011, mae: 0.002155, mean_q: 0.738264
 45535/100000: episode: 555, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.122, 10.424], loss: 0.000021, mae: 0.002807, mean_q: 0.738561
 45636/100000: episode: 556, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.100], loss: 0.000020, mae: 0.002847, mean_q: 0.739026
 45737/100000: episode: 557, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.633, 10.275], loss: 0.000020, mae: 0.002904, mean_q: 0.739300
 45838/100000: episode: 558, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.766, 10.397], loss: 0.000016, mae: 0.002199, mean_q: 0.739338
 45939/100000: episode: 559, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.140, 10.100], loss: 0.000012, mae: 0.002062, mean_q: 0.738889
 46040/100000: episode: 560, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.564, 10.100], loss: 0.000016, mae: 0.002744, mean_q: 0.739746
 46141/100000: episode: 561, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.169, 10.159], loss: 0.000012, mae: 0.002219, mean_q: 0.739735
 46242/100000: episode: 562, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.658, 10.100], loss: 0.000022, mae: 0.002582, mean_q: 0.740122
 46343/100000: episode: 563, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.742, 10.190], loss: 0.000015, mae: 0.003260, mean_q: 0.739998
 46444/100000: episode: 564, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.824, 10.116], loss: 0.000022, mae: 0.002420, mean_q: 0.740429
 46545/100000: episode: 565, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.549, 10.100], loss: 0.000015, mae: 0.002285, mean_q: 0.740058
 46646/100000: episode: 566, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.100], loss: 0.000015, mae: 0.002780, mean_q: 0.740888
 46747/100000: episode: 567, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.807, mean reward: 0.008 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.987, 10.127], loss: 0.000015, mae: 0.002432, mean_q: 0.741081
 46848/100000: episode: 568, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.278, 10.192], loss: 0.000015, mae: 0.002127, mean_q: 0.741726
 46949/100000: episode: 569, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.254, 10.100], loss: 0.000016, mae: 0.002347, mean_q: 0.741859
 47050/100000: episode: 570, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.461, 10.100], loss: 0.000009, mae: 0.001624, mean_q: 0.742108
 47151/100000: episode: 571, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.597, 10.225], loss: 0.000017, mae: 0.002377, mean_q: 0.742214
 47252/100000: episode: 572, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.400, 10.207], loss: 0.000022, mae: 0.002884, mean_q: 0.742588
 47353/100000: episode: 573, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.324, 10.354], loss: 0.000017, mae: 0.002536, mean_q: 0.742346
 47454/100000: episode: 574, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.706, 10.100], loss: 0.000021, mae: 0.002887, mean_q: 0.742835
 47555/100000: episode: 575, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.750, mean reward: 0.007 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.320, 10.136], loss: 0.000025, mae: 0.002718, mean_q: 0.742986
 47656/100000: episode: 576, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.388, 10.193], loss: 0.000023, mae: 0.002955, mean_q: 0.742785
 47757/100000: episode: 577, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.465, 10.432], loss: 0.000014, mae: 0.002219, mean_q: 0.743260
 47858/100000: episode: 578, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.512, 10.166], loss: 0.000009, mae: 0.001643, mean_q: 0.743549
 47959/100000: episode: 579, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.712, 10.100], loss: 0.000020, mae: 0.002863, mean_q: 0.744264
[Info] 1-TH LEVEL FOUND: 0.7711884379386902, Considering 10/100 traces
 48060/100000: episode: 580, duration: 4.910s, episode steps: 101, steps per second: 21, episode reward: 0.859, mean reward: 0.009 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.759, 10.100], loss: 0.000017, mae: 0.002301, mean_q: 0.743308
 48082/100000: episode: 581, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 0.674, mean reward: 0.031 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.573, 10.128], loss: 0.000020, mae: 0.002501, mean_q: 0.744033
 48117/100000: episode: 582, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 0.662, mean reward: 0.019 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.812, 10.100], loss: 0.000026, mae: 0.004015, mean_q: 0.743770
 48145/100000: episode: 583, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 0.659, mean reward: 0.024 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.695, 10.232], loss: 0.000010, mae: 0.002240, mean_q: 0.744810
 48176/100000: episode: 584, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 0.651, mean reward: 0.021 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.234, 10.212], loss: 0.000032, mae: 0.003128, mean_q: 0.744959
 48211/100000: episode: 585, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 0.690, mean reward: 0.020 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.041, 10.100], loss: 0.000014, mae: 0.002431, mean_q: 0.745639
 48242/100000: episode: 586, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 0.878, mean reward: 0.028 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.433, 10.100], loss: 0.000007, mae: 0.001957, mean_q: 0.745005
 48270/100000: episode: 587, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 0.664, mean reward: 0.024 [0.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.419, 10.182], loss: 0.000014, mae: 0.002040, mean_q: 0.745810
 48301/100000: episode: 588, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 0.800, mean reward: 0.026 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.409, 10.100], loss: 0.000007, mae: 0.001399, mean_q: 0.745075
 48336/100000: episode: 589, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 0.719, mean reward: 0.021 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.206, 10.100], loss: 0.000027, mae: 0.002603, mean_q: 0.745547
 48367/100000: episode: 590, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 0.648, mean reward: 0.021 [0.000, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.318, 10.100], loss: 0.000025, mae: 0.002768, mean_q: 0.743925
 48395/100000: episode: 591, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 0.762, mean reward: 0.027 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.127, 10.100], loss: 0.000029, mae: 0.003408, mean_q: 0.745604
 48426/100000: episode: 592, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 0.684, mean reward: 0.022 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.735, 10.334], loss: 0.000053, mae: 0.004204, mean_q: 0.746618
 48454/100000: episode: 593, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 0.632, mean reward: 0.023 [0.000, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.925, 10.100], loss: 0.000038, mae: 0.004942, mean_q: 0.746090
 48482/100000: episode: 594, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 0.702, mean reward: 0.025 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.426, 10.100], loss: 0.000050, mae: 0.005174, mean_q: 0.746987
 48510/100000: episode: 595, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 0.773, mean reward: 0.028 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.292], loss: 0.000051, mae: 0.003048, mean_q: 0.746905
 48541/100000: episode: 596, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 0.726, mean reward: 0.023 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.043, 10.100], loss: 0.000049, mae: 0.003719, mean_q: 0.746245
 48571/100000: episode: 597, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 0.814, mean reward: 0.027 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.396, 10.100], loss: 0.000029, mae: 0.003469, mean_q: 0.746545
 48599/100000: episode: 598, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 0.686, mean reward: 0.024 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.439, 10.100], loss: 0.000018, mae: 0.002548, mean_q: 0.747021
 48629/100000: episode: 599, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 0.721, mean reward: 0.024 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.956, 10.235], loss: 0.000023, mae: 0.002824, mean_q: 0.746663
 48659/100000: episode: 600, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 0.776, mean reward: 0.026 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.259, 10.269], loss: 0.000043, mae: 0.003145, mean_q: 0.746758
 48689/100000: episode: 601, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 0.722, mean reward: 0.024 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.991, 10.105], loss: 0.000016, mae: 0.001989, mean_q: 0.746117
 48711/100000: episode: 602, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 0.618, mean reward: 0.028 [0.000, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.350, 10.100], loss: 0.000015, mae: 0.001956, mean_q: 0.746283
 48746/100000: episode: 603, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 0.699, mean reward: 0.020 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.450, 10.100], loss: 0.000030, mae: 0.002938, mean_q: 0.746427
 48776/100000: episode: 604, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 0.790, mean reward: 0.026 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.778, 10.518], loss: 0.000022, mae: 0.003178, mean_q: 0.746235
 48804/100000: episode: 605, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 0.693, mean reward: 0.025 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.100], loss: 0.000049, mae: 0.004342, mean_q: 0.746954
 48826/100000: episode: 606, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 0.800, mean reward: 0.036 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.426, 10.111], loss: 0.000057, mae: 0.003389, mean_q: 0.747391
 48856/100000: episode: 607, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 0.727, mean reward: 0.024 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.215, 10.100], loss: 0.000024, mae: 0.003197, mean_q: 0.747407
 48891/100000: episode: 608, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 0.705, mean reward: 0.020 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.369, 10.100], loss: 0.000025, mae: 0.003080, mean_q: 0.746925
 48913/100000: episode: 609, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 0.807, mean reward: 0.037 [0.000, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.859, 10.100], loss: 0.000020, mae: 0.003594, mean_q: 0.747717
 48943/100000: episode: 610, duration: 0.147s, episode steps: 30, steps per second: 205, episode reward: 0.754, mean reward: 0.025 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.998, 10.100], loss: 0.000022, mae: 0.002831, mean_q: 0.747248
 48974/100000: episode: 611, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 0.826, mean reward: 0.027 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.542, 10.251], loss: 0.000032, mae: 0.002938, mean_q: 0.747636
 49002/100000: episode: 612, duration: 0.140s, episode steps: 28, steps per second: 199, episode reward: 0.706, mean reward: 0.025 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.535, 10.100], loss: 0.000030, mae: 0.002971, mean_q: 0.747824
 49030/100000: episode: 613, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 0.726, mean reward: 0.026 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.295, 10.100], loss: 0.000079, mae: 0.005671, mean_q: 0.746802
 49058/100000: episode: 614, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 0.708, mean reward: 0.025 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.849, 10.100], loss: 0.000054, mae: 0.004236, mean_q: 0.747558
 49080/100000: episode: 615, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 0.674, mean reward: 0.031 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.106, 10.112], loss: 0.000017, mae: 0.002592, mean_q: 0.748420
 49111/100000: episode: 616, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 0.764, mean reward: 0.025 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.176, 10.100], loss: 0.000027, mae: 0.002581, mean_q: 0.748504
 49133/100000: episode: 617, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 0.835, mean reward: 0.038 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.215, 10.100], loss: 0.000037, mae: 0.003823, mean_q: 0.747879
 49164/100000: episode: 618, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 0.695, mean reward: 0.022 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.112, 10.100], loss: 0.000034, mae: 0.003755, mean_q: 0.747277
 49194/100000: episode: 619, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 0.732, mean reward: 0.024 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.808, 10.128], loss: 0.000032, mae: 0.003220, mean_q: 0.748327
 49224/100000: episode: 620, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 0.749, mean reward: 0.025 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.753, 10.169], loss: 0.000065, mae: 0.004205, mean_q: 0.747783
 49254/100000: episode: 621, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 0.660, mean reward: 0.022 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.183, 10.100], loss: 0.000033, mae: 0.003626, mean_q: 0.746685
 49282/100000: episode: 622, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 0.678, mean reward: 0.024 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.100, 10.100], loss: 0.000062, mae: 0.003415, mean_q: 0.747754
 49313/100000: episode: 623, duration: 0.167s, episode steps: 31, steps per second: 185, episode reward: 0.706, mean reward: 0.023 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.864, 10.100], loss: 0.000029, mae: 0.003080, mean_q: 0.747674
 49341/100000: episode: 624, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 0.668, mean reward: 0.024 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.149, 10.100], loss: 0.000044, mae: 0.003104, mean_q: 0.748479
 49372/100000: episode: 625, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 0.738, mean reward: 0.024 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.387, 10.100], loss: 0.000021, mae: 0.002812, mean_q: 0.748085
 49402/100000: episode: 626, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 0.800, mean reward: 0.027 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.330, 10.208], loss: 0.000048, mae: 0.002614, mean_q: 0.750092
 49432/100000: episode: 627, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 0.713, mean reward: 0.024 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.325, 10.100], loss: 0.000033, mae: 0.002829, mean_q: 0.747954
 49463/100000: episode: 628, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 0.647, mean reward: 0.021 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.528, 10.100], loss: 0.000029, mae: 0.003100, mean_q: 0.747764
 49485/100000: episode: 629, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 0.731, mean reward: 0.033 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.396], loss: 0.000028, mae: 0.003370, mean_q: 0.748105
 49516/100000: episode: 630, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 0.658, mean reward: 0.021 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.655, 10.184], loss: 0.000031, mae: 0.002706, mean_q: 0.747459
 49546/100000: episode: 631, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 0.675, mean reward: 0.023 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.218, 10.100], loss: 0.000039, mae: 0.003136, mean_q: 0.748495
 49568/100000: episode: 632, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.684, mean reward: 0.031 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.173, 10.100], loss: 0.000038, mae: 0.002775, mean_q: 0.749828
 49603/100000: episode: 633, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 0.778, mean reward: 0.022 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.205, 10.267], loss: 0.000028, mae: 0.003288, mean_q: 0.747355
 49625/100000: episode: 634, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 0.623, mean reward: 0.028 [0.000, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.063, 10.173], loss: 0.000027, mae: 0.002115, mean_q: 0.747562
 49656/100000: episode: 635, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 0.658, mean reward: 0.021 [0.000, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.490, 10.180], loss: 0.000059, mae: 0.004111, mean_q: 0.748533
 49686/100000: episode: 636, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 0.715, mean reward: 0.024 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.474, 10.226], loss: 0.000037, mae: 0.005172, mean_q: 0.747500
 49708/100000: episode: 637, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 0.756, mean reward: 0.034 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.150, 10.100], loss: 0.000024, mae: 0.002983, mean_q: 0.748088
 49730/100000: episode: 638, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.771, mean reward: 0.035 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.774, 10.195], loss: 0.000021, mae: 0.002690, mean_q: 0.747822
 49752/100000: episode: 639, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 0.702, mean reward: 0.032 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.402, 10.100], loss: 0.000022, mae: 0.002936, mean_q: 0.747987
 49774/100000: episode: 640, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.672, mean reward: 0.031 [0.000, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.186, 10.100], loss: 0.000056, mae: 0.003820, mean_q: 0.747865
 49802/100000: episode: 641, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 0.652, mean reward: 0.023 [0.000, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.007, 10.100], loss: 0.000044, mae: 0.003296, mean_q: 0.748197
 49832/100000: episode: 642, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 0.731, mean reward: 0.024 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.642, 10.305], loss: 0.000037, mae: 0.003728, mean_q: 0.746891
 49862/100000: episode: 643, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 0.724, mean reward: 0.024 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.452, 10.100], loss: 0.000037, mae: 0.004362, mean_q: 0.747813
 49890/100000: episode: 644, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 0.669, mean reward: 0.024 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.627, 10.100], loss: 0.000038, mae: 0.004620, mean_q: 0.747572
 49918/100000: episode: 645, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 0.685, mean reward: 0.024 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.198, 10.100], loss: 0.000022, mae: 0.002989, mean_q: 0.747864
 49946/100000: episode: 646, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 0.718, mean reward: 0.026 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.214, 10.100], loss: 0.000023, mae: 0.002673, mean_q: 0.747080
 49977/100000: episode: 647, duration: 0.164s, episode steps: 31, steps per second: 188, episode reward: 0.751, mean reward: 0.024 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.884, 10.100], loss: 0.000060, mae: 0.004503, mean_q: 0.747320
 50005/100000: episode: 648, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 0.677, mean reward: 0.024 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.256, 10.100], loss: 0.000023, mae: 0.004131, mean_q: 0.746697
 50033/100000: episode: 649, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 0.681, mean reward: 0.024 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.417, 10.100], loss: 0.000028, mae: 0.002834, mean_q: 0.746827
 50063/100000: episode: 650, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 0.739, mean reward: 0.025 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.474, 10.100], loss: 0.000044, mae: 0.004072, mean_q: 0.746071
 50094/100000: episode: 651, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 0.739, mean reward: 0.024 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.489], loss: 0.000029, mae: 0.003600, mean_q: 0.747256
 50122/100000: episode: 652, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 0.814, mean reward: 0.029 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.985, 10.131], loss: 0.000026, mae: 0.002648, mean_q: 0.746315
 50152/100000: episode: 653, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 0.710, mean reward: 0.024 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.691, 10.346], loss: 0.000062, mae: 0.003907, mean_q: 0.745654
 50183/100000: episode: 654, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 0.800, mean reward: 0.026 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.698, 10.100], loss: 0.000046, mae: 0.004987, mean_q: 0.745691
 50214/100000: episode: 655, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 0.686, mean reward: 0.022 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.180, 10.100], loss: 0.000037, mae: 0.003284, mean_q: 0.746633
 50244/100000: episode: 656, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 0.679, mean reward: 0.023 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.130, 10.100], loss: 0.000044, mae: 0.004246, mean_q: 0.745465
 50275/100000: episode: 657, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 0.804, mean reward: 0.026 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.176, 10.100], loss: 0.000054, mae: 0.005150, mean_q: 0.746980
 50303/100000: episode: 658, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 0.780, mean reward: 0.028 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.416, 10.100], loss: 0.000018, mae: 0.003374, mean_q: 0.744735
 50331/100000: episode: 659, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 0.699, mean reward: 0.025 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.093, 10.100], loss: 0.000030, mae: 0.002798, mean_q: 0.745211
 50362/100000: episode: 660, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 0.641, mean reward: 0.021 [0.000, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.189, 10.241], loss: 0.000026, mae: 0.003097, mean_q: 0.745311
 50393/100000: episode: 661, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 0.773, mean reward: 0.025 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.772, 10.100], loss: 0.000035, mae: 0.003427, mean_q: 0.745577
 50421/100000: episode: 662, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 0.670, mean reward: 0.024 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.579, 10.100], loss: 0.000028, mae: 0.003004, mean_q: 0.745410
 50443/100000: episode: 663, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 0.645, mean reward: 0.029 [0.000, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.115, 10.146], loss: 0.000040, mae: 0.004123, mean_q: 0.744104
 50471/100000: episode: 664, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 0.627, mean reward: 0.022 [0.000, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.489, 10.131], loss: 0.000047, mae: 0.003365, mean_q: 0.743343
 50493/100000: episode: 665, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 0.769, mean reward: 0.035 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.175, 10.126], loss: 0.000044, mae: 0.004305, mean_q: 0.744198
 50523/100000: episode: 666, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 0.770, mean reward: 0.026 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.333, 10.192], loss: 0.000050, mae: 0.003842, mean_q: 0.744701
 50558/100000: episode: 667, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 0.725, mean reward: 0.021 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.449, 10.100], loss: 0.000041, mae: 0.003760, mean_q: 0.742971
 50580/100000: episode: 668, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 0.698, mean reward: 0.032 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.203, 10.100], loss: 0.000043, mae: 0.003168, mean_q: 0.743775
 50602/100000: episode: 669, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.731, mean reward: 0.033 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.782, 10.100], loss: 0.000047, mae: 0.004649, mean_q: 0.743225
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7711884379386902
1
 50624/100000: episode: 670, duration: 4.320s, episode steps: 22, steps per second: 5, episode reward: 0.631, mean reward: 0.029 [0.000, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.605, 10.212], loss: 0.000018, mae: 0.003738, mean_q: 0.742246
 50725/100000: episode: 671, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.530, 10.124], loss: 0.000033, mae: 0.002772, mean_q: 0.742521
 50826/100000: episode: 672, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.714, mean reward: 0.007 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.105, 10.138], loss: 0.000049, mae: 0.003238, mean_q: 0.741597
 50927/100000: episode: 673, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.586, 10.184], loss: 0.000037, mae: 0.003063, mean_q: 0.741563
 51028/100000: episode: 674, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.776, 10.100], loss: 0.000045, mae: 0.003885, mean_q: 0.740683
 51129/100000: episode: 675, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.882, mean reward: 0.009 [0.000, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.306, 10.114], loss: 0.000034, mae: 0.003054, mean_q: 0.739946
 51230/100000: episode: 676, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.183, 10.100], loss: 0.000033, mae: 0.003030, mean_q: 0.739932
 51331/100000: episode: 677, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.565, 10.100], loss: 0.000042, mae: 0.003046, mean_q: 0.739262
 51432/100000: episode: 678, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.499, 10.213], loss: 0.000029, mae: 0.002693, mean_q: 0.738682
 51533/100000: episode: 679, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.155], loss: 0.000040, mae: 0.003774, mean_q: 0.737742
 51634/100000: episode: 680, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.888, 10.146], loss: 0.000039, mae: 0.003680, mean_q: 0.737789
 51735/100000: episode: 681, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.922, 10.100], loss: 0.000031, mae: 0.002700, mean_q: 0.737094
 51836/100000: episode: 682, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.259], loss: 0.000041, mae: 0.004008, mean_q: 0.737269
 51937/100000: episode: 683, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.338, 10.273], loss: 0.000057, mae: 0.003938, mean_q: 0.736623
 52038/100000: episode: 684, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.350, 10.269], loss: 0.000044, mae: 0.003566, mean_q: 0.736360
 52139/100000: episode: 685, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.163, 10.100], loss: 0.000035, mae: 0.002740, mean_q: 0.736899
 52240/100000: episode: 686, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.829, mean reward: 0.008 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.334, 10.100], loss: 0.000031, mae: 0.003262, mean_q: 0.735854
 52341/100000: episode: 687, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.505, 10.100], loss: 0.000053, mae: 0.004084, mean_q: 0.735688
 52442/100000: episode: 688, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.668, 10.100], loss: 0.000051, mae: 0.004601, mean_q: 0.735670
 52543/100000: episode: 689, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.687, mean reward: 0.007 [0.000, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.879, 10.131], loss: 0.000034, mae: 0.002556, mean_q: 0.735344
 52644/100000: episode: 690, duration: 0.542s, episode steps: 101, steps per second: 186, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.059, 10.100], loss: 0.000045, mae: 0.003131, mean_q: 0.735206
 52745/100000: episode: 691, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.386, 10.142], loss: 0.000035, mae: 0.002646, mean_q: 0.734425
 52846/100000: episode: 692, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.986, 10.277], loss: 0.000045, mae: 0.002677, mean_q: 0.734232
 52947/100000: episode: 693, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.092, 10.334], loss: 0.000041, mae: 0.003830, mean_q: 0.733716
 53048/100000: episode: 694, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.643, 10.260], loss: 0.000048, mae: 0.003690, mean_q: 0.734235
 53149/100000: episode: 695, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.766, 10.100], loss: 0.000042, mae: 0.003346, mean_q: 0.734787
 53250/100000: episode: 696, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.667, mean reward: 0.007 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.139, 10.182], loss: 0.000034, mae: 0.003125, mean_q: 0.734278
 53351/100000: episode: 697, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.221], loss: 0.000040, mae: 0.003169, mean_q: 0.734214
 53452/100000: episode: 698, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.798, mean reward: 0.008 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.521, 10.100], loss: 0.000026, mae: 0.002570, mean_q: 0.734877
 53553/100000: episode: 699, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.038, 10.173], loss: 0.000023, mae: 0.002737, mean_q: 0.735532
 53654/100000: episode: 700, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.814, mean reward: 0.008 [0.000, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.942, 10.119], loss: 0.000036, mae: 0.003736, mean_q: 0.735072
 53755/100000: episode: 701, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.013, 10.100], loss: 0.000029, mae: 0.002601, mean_q: 0.735700
 53856/100000: episode: 702, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.779, mean reward: 0.008 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.281, 10.222], loss: 0.000037, mae: 0.003430, mean_q: 0.735813
 53957/100000: episode: 703, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.857, 10.197], loss: 0.000032, mae: 0.002686, mean_q: 0.736379
 54058/100000: episode: 704, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.585, 10.100], loss: 0.000030, mae: 0.003086, mean_q: 0.736101
 54159/100000: episode: 705, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.998, 10.100], loss: 0.000023, mae: 0.002579, mean_q: 0.736278
 54260/100000: episode: 706, duration: 0.511s, episode steps: 101, steps per second: 197, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.807, 10.100], loss: 0.000023, mae: 0.002408, mean_q: 0.736270
 54361/100000: episode: 707, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.609, 10.100], loss: 0.000037, mae: 0.002921, mean_q: 0.737403
 54462/100000: episode: 708, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.662, 10.300], loss: 0.000028, mae: 0.002490, mean_q: 0.737397
 54563/100000: episode: 709, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.579, 10.344], loss: 0.000028, mae: 0.002912, mean_q: 0.738127
 54664/100000: episode: 710, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.231, 10.141], loss: 0.000032, mae: 0.002834, mean_q: 0.737910
 54765/100000: episode: 711, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.868, mean reward: 0.009 [0.000, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.103, 10.440], loss: 0.000027, mae: 0.003143, mean_q: 0.738049
 54866/100000: episode: 712, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.692, 10.240], loss: 0.000029, mae: 0.002466, mean_q: 0.738747
 54967/100000: episode: 713, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.886, 10.232], loss: 0.000027, mae: 0.002740, mean_q: 0.738848
 55068/100000: episode: 714, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.976, 10.131], loss: 0.000032, mae: 0.003167, mean_q: 0.738901
 55169/100000: episode: 715, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.267, 10.233], loss: 0.000034, mae: 0.003161, mean_q: 0.739469
 55270/100000: episode: 716, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.842, mean reward: 0.008 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.100], loss: 0.000014, mae: 0.001814, mean_q: 0.738571
 55371/100000: episode: 717, duration: 0.499s, episode steps: 101, steps per second: 202, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.095, 10.214], loss: 0.000021, mae: 0.002311, mean_q: 0.738917
 55472/100000: episode: 718, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.743, mean reward: 0.007 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.189, 10.115], loss: 0.000017, mae: 0.002285, mean_q: 0.739583
 55573/100000: episode: 719, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.757, mean reward: 0.007 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.045, 10.389], loss: 0.000013, mae: 0.001702, mean_q: 0.739589
 55674/100000: episode: 720, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.746, 10.100], loss: 0.000011, mae: 0.001583, mean_q: 0.739647
 55775/100000: episode: 721, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.605, 10.246], loss: 0.000012, mae: 0.001825, mean_q: 0.739504
 55876/100000: episode: 722, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.856, 10.100], loss: 0.000012, mae: 0.001971, mean_q: 0.739820
 55977/100000: episode: 723, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.898, 10.162], loss: 0.000018, mae: 0.002185, mean_q: 0.739665
 56078/100000: episode: 724, duration: 0.533s, episode steps: 101, steps per second: 189, episode reward: 0.831, mean reward: 0.008 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.636, 10.100], loss: 0.000014, mae: 0.001931, mean_q: 0.738981
 56179/100000: episode: 725, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.757, 10.100], loss: 0.000017, mae: 0.002391, mean_q: 0.739306
 56280/100000: episode: 726, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.051, 10.165], loss: 0.000012, mae: 0.002110, mean_q: 0.739768
 56381/100000: episode: 727, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.948, 10.268], loss: 0.000015, mae: 0.001735, mean_q: 0.739764
 56482/100000: episode: 728, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.100], loss: 0.000013, mae: 0.002119, mean_q: 0.738792
 56583/100000: episode: 729, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.936, mean reward: 0.009 [0.000, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.100], loss: 0.000007, mae: 0.001561, mean_q: 0.739342
 56684/100000: episode: 730, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.788, 10.248], loss: 0.000028, mae: 0.002942, mean_q: 0.739463
 56785/100000: episode: 731, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.086, 10.363], loss: 0.000018, mae: 0.002331, mean_q: 0.739109
 56886/100000: episode: 732, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.816, 10.156], loss: 0.000028, mae: 0.003391, mean_q: 0.738531
 56987/100000: episode: 733, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.559, 10.100], loss: 0.000016, mae: 0.002194, mean_q: 0.739226
 57088/100000: episode: 734, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.067, 10.100], loss: 0.000014, mae: 0.001872, mean_q: 0.738892
 57189/100000: episode: 735, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.398, 10.100], loss: 0.000023, mae: 0.002766, mean_q: 0.738849
 57290/100000: episode: 736, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.405, 10.250], loss: 0.000019, mae: 0.002193, mean_q: 0.738755
 57391/100000: episode: 737, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.954, 10.100], loss: 0.000014, mae: 0.002029, mean_q: 0.738781
 57492/100000: episode: 738, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.805, mean reward: 0.008 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.774, 10.100], loss: 0.000013, mae: 0.001943, mean_q: 0.738654
 57593/100000: episode: 739, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.590, 10.100], loss: 0.000019, mae: 0.002171, mean_q: 0.738581
 57694/100000: episode: 740, duration: 0.496s, episode steps: 101, steps per second: 204, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.495, 10.105], loss: 0.000016, mae: 0.002559, mean_q: 0.738712
 57795/100000: episode: 741, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.152, 10.100], loss: 0.000031, mae: 0.002643, mean_q: 0.738746
 57896/100000: episode: 742, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.783, mean reward: 0.008 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.116, 10.225], loss: 0.000010, mae: 0.001592, mean_q: 0.739381
 57997/100000: episode: 743, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.468, 10.100], loss: 0.000020, mae: 0.002448, mean_q: 0.739519
 58098/100000: episode: 744, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.764, mean reward: 0.008 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.806, 10.264], loss: 0.000008, mae: 0.001683, mean_q: 0.739361
 58199/100000: episode: 745, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.092, 10.470], loss: 0.000012, mae: 0.002195, mean_q: 0.738781
 58300/100000: episode: 746, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.754, 10.279], loss: 0.000015, mae: 0.002483, mean_q: 0.739476
 58401/100000: episode: 747, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.716, mean reward: 0.007 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.144, 10.384], loss: 0.000027, mae: 0.002938, mean_q: 0.739541
 58502/100000: episode: 748, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.808, mean reward: 0.008 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.497, 10.100], loss: 0.000021, mae: 0.002328, mean_q: 0.739478
 58603/100000: episode: 749, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.745, 10.100], loss: 0.000020, mae: 0.002865, mean_q: 0.739775
 58704/100000: episode: 750, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.902, mean reward: 0.009 [0.000, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.497, 10.100], loss: 0.000017, mae: 0.002292, mean_q: 0.739588
 58805/100000: episode: 751, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.499, 10.100], loss: 0.000029, mae: 0.002727, mean_q: 0.739847
 58906/100000: episode: 752, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.846, 10.244], loss: 0.000012, mae: 0.002123, mean_q: 0.740241
 59007/100000: episode: 753, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.725, mean reward: 0.007 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.138, 10.100], loss: 0.000018, mae: 0.002522, mean_q: 0.740093
 59108/100000: episode: 754, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.531, 10.294], loss: 0.000030, mae: 0.003293, mean_q: 0.741183
 59209/100000: episode: 755, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.859, 10.300], loss: 0.000009, mae: 0.001593, mean_q: 0.740946
 59310/100000: episode: 756, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.370, 10.288], loss: 0.000015, mae: 0.002103, mean_q: 0.741145
 59411/100000: episode: 757, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.916, 10.100], loss: 0.000015, mae: 0.001992, mean_q: 0.741919
 59512/100000: episode: 758, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.427, 10.100], loss: 0.000010, mae: 0.001659, mean_q: 0.742098
 59613/100000: episode: 759, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.762, mean reward: 0.008 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.477, 10.100], loss: 0.000021, mae: 0.002510, mean_q: 0.741863
 59714/100000: episode: 760, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.841, 10.332], loss: 0.000009, mae: 0.001709, mean_q: 0.742467
 59815/100000: episode: 761, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.971, 10.321], loss: 0.000027, mae: 0.003127, mean_q: 0.742733
 59916/100000: episode: 762, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.507, 10.100], loss: 0.000015, mae: 0.002172, mean_q: 0.743140
 60017/100000: episode: 763, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.690, mean reward: 0.007 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.642, 10.211], loss: 0.000012, mae: 0.002108, mean_q: 0.743373
 60118/100000: episode: 764, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.625, 10.216], loss: 0.000012, mae: 0.002386, mean_q: 0.743230
 60219/100000: episode: 765, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.275, 10.247], loss: 0.000015, mae: 0.002266, mean_q: 0.743909
 60320/100000: episode: 766, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.520, 10.106], loss: 0.000016, mae: 0.002680, mean_q: 0.744341
 60421/100000: episode: 767, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.449, 10.100], loss: 0.000009, mae: 0.001947, mean_q: 0.744723
 60522/100000: episode: 768, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.742, 10.100], loss: 0.000009, mae: 0.001605, mean_q: 0.744602
 60623/100000: episode: 769, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.744, mean reward: 0.007 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.045, 10.129], loss: 0.000010, mae: 0.001854, mean_q: 0.745289
[Info] 1-TH LEVEL FOUND: 0.7651928663253784, Considering 10/100 traces
 60724/100000: episode: 770, duration: 4.901s, episode steps: 101, steps per second: 21, episode reward: 0.778, mean reward: 0.008 [0.000, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.645, 10.105], loss: 0.000007, mae: 0.001815, mean_q: 0.745255
 60746/100000: episode: 771, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.681, mean reward: 0.031 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.549, 10.100], loss: 0.000025, mae: 0.002153, mean_q: 0.745753
 60772/100000: episode: 772, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 0.651, mean reward: 0.025 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.116, 10.100], loss: 0.000015, mae: 0.002222, mean_q: 0.745556
 60797/100000: episode: 773, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 0.762, mean reward: 0.030 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.797, 10.100], loss: 0.000024, mae: 0.001604, mean_q: 0.746360
 60822/100000: episode: 774, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 0.790, mean reward: 0.032 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.566, 10.100], loss: 0.000010, mae: 0.002117, mean_q: 0.744830
 60847/100000: episode: 775, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 0.780, mean reward: 0.031 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.908, 10.100], loss: 0.000015, mae: 0.001749, mean_q: 0.747345
 60874/100000: episode: 776, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 0.663, mean reward: 0.025 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.179, 10.100], loss: 0.000008, mae: 0.001454, mean_q: 0.745000
 60899/100000: episode: 777, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 0.683, mean reward: 0.027 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.138, 10.128], loss: 0.000034, mae: 0.003477, mean_q: 0.747491
 60922/100000: episode: 778, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 0.738, mean reward: 0.032 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.302, 10.100], loss: 0.000031, mae: 0.002173, mean_q: 0.746511
 60945/100000: episode: 779, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 0.739, mean reward: 0.032 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.173, 10.100], loss: 0.000009, mae: 0.001694, mean_q: 0.746464
 60970/100000: episode: 780, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 0.728, mean reward: 0.029 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.341, 10.100], loss: 0.000042, mae: 0.003503, mean_q: 0.747094
 60996/100000: episode: 781, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 0.712, mean reward: 0.027 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.554, 10.100], loss: 0.000060, mae: 0.004154, mean_q: 0.748831
 61018/100000: episode: 782, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 0.688, mean reward: 0.031 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.064, 10.100], loss: 0.000014, mae: 0.002383, mean_q: 0.746867
 61041/100000: episode: 783, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 0.686, mean reward: 0.030 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.373, 10.100], loss: 0.000034, mae: 0.002603, mean_q: 0.746402
 61068/100000: episode: 784, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 0.720, mean reward: 0.027 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.225, 10.100], loss: 0.000031, mae: 0.002861, mean_q: 0.747744
 61093/100000: episode: 785, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 0.812, mean reward: 0.032 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.337, 10.100], loss: 0.000017, mae: 0.002079, mean_q: 0.747069
 61118/100000: episode: 786, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 0.667, mean reward: 0.027 [0.000, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.578, 10.100], loss: 0.000014, mae: 0.001764, mean_q: 0.747138
 61141/100000: episode: 787, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 0.646, mean reward: 0.028 [0.000, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.283, 10.100], loss: 0.000006, mae: 0.001092, mean_q: 0.746978
 61164/100000: episode: 788, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 0.705, mean reward: 0.031 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.925, 10.100], loss: 0.000011, mae: 0.001845, mean_q: 0.748323
 61190/100000: episode: 789, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 0.727, mean reward: 0.028 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.783, 10.100], loss: 0.000031, mae: 0.002447, mean_q: 0.748026
 61213/100000: episode: 790, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 0.712, mean reward: 0.031 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.795, 10.100], loss: 0.000019, mae: 0.002198, mean_q: 0.747387
 61238/100000: episode: 791, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 0.715, mean reward: 0.029 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.248, 10.100], loss: 0.000012, mae: 0.001348, mean_q: 0.747958
 61260/100000: episode: 792, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 0.736, mean reward: 0.033 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.309, 10.100], loss: 0.000019, mae: 0.002303, mean_q: 0.747833
 61286/100000: episode: 793, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 0.649, mean reward: 0.025 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.148, 10.100], loss: 0.000023, mae: 0.001913, mean_q: 0.747482
 61311/100000: episode: 794, duration: 0.131s, episode steps: 25, steps per second: 192, episode reward: 0.803, mean reward: 0.032 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.180, 10.100], loss: 0.000027, mae: 0.003358, mean_q: 0.747599
 61334/100000: episode: 795, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 0.721, mean reward: 0.031 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.457, 10.100], loss: 0.000010, mae: 0.001716, mean_q: 0.748713
 61361/100000: episode: 796, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 0.775, mean reward: 0.029 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.279, 10.100], loss: 0.000033, mae: 0.003975, mean_q: 0.747720
 61387/100000: episode: 797, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 0.771, mean reward: 0.030 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.153, 10.100], loss: 0.000032, mae: 0.004067, mean_q: 0.749137
 61412/100000: episode: 798, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 0.610, mean reward: 0.024 [0.000, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.193, 10.100], loss: 0.000008, mae: 0.001433, mean_q: 0.748017
 61435/100000: episode: 799, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 0.725, mean reward: 0.032 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.236, 10.100], loss: 0.000021, mae: 0.003216, mean_q: 0.749605
 61460/100000: episode: 800, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 0.686, mean reward: 0.027 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.595, 10.100], loss: 0.000019, mae: 0.002984, mean_q: 0.748366
 61486/100000: episode: 801, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 0.748, mean reward: 0.029 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.773, 10.100], loss: 0.000031, mae: 0.003410, mean_q: 0.748308
 61509/100000: episode: 802, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 0.696, mean reward: 0.030 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.845, 10.100], loss: 0.000008, mae: 0.001613, mean_q: 0.748137
 61535/100000: episode: 803, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 0.697, mean reward: 0.027 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.561, 10.100], loss: 0.000029, mae: 0.002884, mean_q: 0.749157
 61562/100000: episode: 804, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 0.779, mean reward: 0.029 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.231, 10.100], loss: 0.000020, mae: 0.002089, mean_q: 0.748686
 61588/100000: episode: 805, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 0.710, mean reward: 0.027 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.161, 10.100], loss: 0.000012, mae: 0.001774, mean_q: 0.749030
 61614/100000: episode: 806, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 0.867, mean reward: 0.033 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.377, 10.100], loss: 0.000030, mae: 0.003870, mean_q: 0.749123
 61639/100000: episode: 807, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 0.811, mean reward: 0.032 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.216, 10.109], loss: 0.000039, mae: 0.003893, mean_q: 0.748453
 61665/100000: episode: 808, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 0.671, mean reward: 0.026 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.098, 10.154], loss: 0.000038, mae: 0.003744, mean_q: 0.749754
 61688/100000: episode: 809, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 0.688, mean reward: 0.030 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.169, 10.100], loss: 0.000026, mae: 0.003473, mean_q: 0.748086
 61715/100000: episode: 810, duration: 0.150s, episode steps: 27, steps per second: 179, episode reward: 0.812, mean reward: 0.030 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.992, 10.100], loss: 0.000012, mae: 0.002606, mean_q: 0.748505
 61737/100000: episode: 811, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 0.705, mean reward: 0.032 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.257, 10.100], loss: 0.000036, mae: 0.003750, mean_q: 0.747884
 61763/100000: episode: 812, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 0.736, mean reward: 0.028 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.265, 10.100], loss: 0.000009, mae: 0.001309, mean_q: 0.749498
 61788/100000: episode: 813, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 0.606, mean reward: 0.024 [0.000, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.050, 10.100], loss: 0.000039, mae: 0.002640, mean_q: 0.749027
 61815/100000: episode: 814, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 0.910, mean reward: 0.034 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.568, 10.125], loss: 0.000019, mae: 0.001970, mean_q: 0.749202
 61838/100000: episode: 815, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 0.705, mean reward: 0.031 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.182, 10.100], loss: 0.000023, mae: 0.002259, mean_q: 0.748946
 61863/100000: episode: 816, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 0.657, mean reward: 0.026 [0.000, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.113, 10.100], loss: 0.000046, mae: 0.002739, mean_q: 0.747691
 61888/100000: episode: 817, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 0.686, mean reward: 0.027 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.100, 10.100], loss: 0.000012, mae: 0.001661, mean_q: 0.749548
 61913/100000: episode: 818, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 0.796, mean reward: 0.032 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.204, 10.100], loss: 0.000017, mae: 0.002027, mean_q: 0.749001
 61935/100000: episode: 819, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 0.706, mean reward: 0.032 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.548, 10.100], loss: 0.000050, mae: 0.003003, mean_q: 0.749261
 61961/100000: episode: 820, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 0.670, mean reward: 0.026 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.649, 10.100], loss: 0.000057, mae: 0.003654, mean_q: 0.749117
 61986/100000: episode: 821, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 0.867, mean reward: 0.035 [0.000, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.349, 10.100], loss: 0.000025, mae: 0.003130, mean_q: 0.748728
 62009/100000: episode: 822, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 0.709, mean reward: 0.031 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.311, 10.100], loss: 0.000049, mae: 0.003920, mean_q: 0.748429
 62032/100000: episode: 823, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 0.725, mean reward: 0.032 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.161, 10.100], loss: 0.000027, mae: 0.002770, mean_q: 0.748626
 62057/100000: episode: 824, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 0.783, mean reward: 0.031 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.667, 10.100], loss: 0.000069, mae: 0.003578, mean_q: 0.749982
 62080/100000: episode: 825, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 0.710, mean reward: 0.031 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.374, 10.100], loss: 0.000043, mae: 0.003678, mean_q: 0.747614
 62106/100000: episode: 826, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 0.734, mean reward: 0.028 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.147, 10.100], loss: 0.000021, mae: 0.002649, mean_q: 0.749015
 62131/100000: episode: 827, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 0.699, mean reward: 0.028 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.468, 10.100], loss: 0.000054, mae: 0.003524, mean_q: 0.748481
 62154/100000: episode: 828, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 0.625, mean reward: 0.027 [0.000, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.212, 10.100], loss: 0.000047, mae: 0.003480, mean_q: 0.748122
 62176/100000: episode: 829, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.654, mean reward: 0.030 [0.000, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.361, 10.100], loss: 0.000044, mae: 0.003147, mean_q: 0.749925
 62203/100000: episode: 830, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 0.745, mean reward: 0.028 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.759, 10.100], loss: 0.000046, mae: 0.003801, mean_q: 0.748423
 62226/100000: episode: 831, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 0.704, mean reward: 0.031 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.007, 10.100], loss: 0.000048, mae: 0.003935, mean_q: 0.749100
 62252/100000: episode: 832, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 0.745, mean reward: 0.029 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.317, 10.100], loss: 0.000034, mae: 0.002854, mean_q: 0.748759
 62279/100000: episode: 833, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 0.673, mean reward: 0.025 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.353, 10.100], loss: 0.000030, mae: 0.002564, mean_q: 0.748165
 62304/100000: episode: 834, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 0.673, mean reward: 0.027 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.407, 10.100], loss: 0.000055, mae: 0.003227, mean_q: 0.747894
 62330/100000: episode: 835, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 0.702, mean reward: 0.027 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.414, 10.100], loss: 0.000031, mae: 0.003571, mean_q: 0.747668
 62353/100000: episode: 836, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 0.655, mean reward: 0.028 [0.000, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.257, 10.100], loss: 0.000044, mae: 0.004774, mean_q: 0.748343
 62376/100000: episode: 837, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 0.708, mean reward: 0.031 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.026, 10.100], loss: 0.000082, mae: 0.006114, mean_q: 0.748622
 62403/100000: episode: 838, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 0.815, mean reward: 0.030 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.422, 10.100], loss: 0.000026, mae: 0.002566, mean_q: 0.747747
 62429/100000: episode: 839, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 0.806, mean reward: 0.031 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.400, 10.100], loss: 0.000039, mae: 0.003552, mean_q: 0.748149
 62452/100000: episode: 840, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 0.716, mean reward: 0.031 [0.000, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.825, 10.100], loss: 0.000058, mae: 0.005538, mean_q: 0.747755
 62477/100000: episode: 841, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 0.809, mean reward: 0.032 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.698, 10.100], loss: 0.000025, mae: 0.003001, mean_q: 0.747101
 62500/100000: episode: 842, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 0.659, mean reward: 0.029 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.530, 10.100], loss: 0.000057, mae: 0.003966, mean_q: 0.746973
 62522/100000: episode: 843, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 0.682, mean reward: 0.031 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.523, 10.219], loss: 0.000015, mae: 0.002183, mean_q: 0.747129
 62547/100000: episode: 844, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 0.642, mean reward: 0.026 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.721, 10.100], loss: 0.000069, mae: 0.004347, mean_q: 0.746341
 62569/100000: episode: 845, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 0.721, mean reward: 0.033 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.232, 10.100], loss: 0.000061, mae: 0.005152, mean_q: 0.747158
 62594/100000: episode: 846, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 0.757, mean reward: 0.030 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.521, 10.100], loss: 0.000099, mae: 0.004271, mean_q: 0.748080
 62616/100000: episode: 847, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 0.735, mean reward: 0.033 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.094, 10.100], loss: 0.000040, mae: 0.002638, mean_q: 0.747117
 62639/100000: episode: 848, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 0.619, mean reward: 0.027 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.100], loss: 0.000032, mae: 0.003383, mean_q: 0.745925
 62662/100000: episode: 849, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 0.711, mean reward: 0.031 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.324, 10.100], loss: 0.000058, mae: 0.003665, mean_q: 0.747407
 62689/100000: episode: 850, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 0.718, mean reward: 0.027 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.887, 10.100], loss: 0.000041, mae: 0.002939, mean_q: 0.746345
 62714/100000: episode: 851, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 0.884, mean reward: 0.035 [0.000, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.406, 10.100], loss: 0.000046, mae: 0.003865, mean_q: 0.745517
 62737/100000: episode: 852, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 0.734, mean reward: 0.032 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.167, 10.100], loss: 0.000044, mae: 0.004499, mean_q: 0.746497
 62764/100000: episode: 853, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 0.704, mean reward: 0.026 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.117, 10.100], loss: 0.000029, mae: 0.002771, mean_q: 0.746115
 62789/100000: episode: 854, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 0.702, mean reward: 0.028 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.183, 10.144], loss: 0.000018, mae: 0.002810, mean_q: 0.745182
 62814/100000: episode: 855, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 0.649, mean reward: 0.026 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.890, 10.100], loss: 0.000035, mae: 0.002408, mean_q: 0.745981
 62837/100000: episode: 856, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 0.675, mean reward: 0.029 [0.000, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.073, 10.100], loss: 0.000023, mae: 0.002134, mean_q: 0.744912
 62860/100000: episode: 857, duration: 0.123s, episode steps: 23, steps per second: 186, episode reward: 0.718, mean reward: 0.031 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.954, 10.100], loss: 0.000073, mae: 0.004591, mean_q: 0.745556
 62886/100000: episode: 858, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 0.789, mean reward: 0.030 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.714, 10.100], loss: 0.000023, mae: 0.001948, mean_q: 0.745033
 62911/100000: episode: 859, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 0.717, mean reward: 0.029 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.371, 10.139], loss: 0.000024, mae: 0.001999, mean_q: 0.744785
[Info] 2-TH LEVEL FOUND: 0.7671933174133301, Considering 11/100 traces
 62937/100000: episode: 860, duration: 4.480s, episode steps: 26, steps per second: 6, episode reward: 0.617, mean reward: 0.024 [0.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.214, 10.100], loss: 0.000026, mae: 0.002371, mean_q: 0.745008
 62982/100000: episode: 861, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 0.721, mean reward: 0.016 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.982, 10.100], loss: 0.000072, mae: 0.005128, mean_q: 0.744696
 63027/100000: episode: 862, duration: 0.230s, episode steps: 45, steps per second: 195, episode reward: 0.714, mean reward: 0.016 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.239, 10.100], loss: 0.000030, mae: 0.003081, mean_q: 0.744349
 63072/100000: episode: 863, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 0.732, mean reward: 0.016 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.117, 10.100], loss: 0.000037, mae: 0.002718, mean_q: 0.743866
 63117/100000: episode: 864, duration: 0.233s, episode steps: 45, steps per second: 194, episode reward: 0.682, mean reward: 0.015 [0.000, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.393, 10.244], loss: 0.000072, mae: 0.004396, mean_q: 0.744242
 63162/100000: episode: 865, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.888, mean reward: 0.020 [0.000, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.201, 10.100], loss: 0.000040, mae: 0.003744, mean_q: 0.744426
 63207/100000: episode: 866, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 0.856, mean reward: 0.019 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.236, 10.100], loss: 0.000043, mae: 0.003342, mean_q: 0.742927
 63252/100000: episode: 867, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.663, mean reward: 0.015 [0.000, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.660, 10.123], loss: 0.000098, mae: 0.005137, mean_q: 0.743155
 63297/100000: episode: 868, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 0.753, mean reward: 0.017 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.546, 10.100], loss: 0.000044, mae: 0.004013, mean_q: 0.742706
 63342/100000: episode: 869, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.711, mean reward: 0.016 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.089, 10.100], loss: 0.000070, mae: 0.004743, mean_q: 0.742877
 63387/100000: episode: 870, duration: 0.222s, episode steps: 45, steps per second: 202, episode reward: 0.844, mean reward: 0.019 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.285, 10.100], loss: 0.000035, mae: 0.003603, mean_q: 0.741333
 63432/100000: episode: 871, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 0.761, mean reward: 0.017 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.556, 10.100], loss: 0.000020, mae: 0.001877, mean_q: 0.741629
 63477/100000: episode: 872, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 0.689, mean reward: 0.015 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.179, 10.329], loss: 0.000044, mae: 0.003022, mean_q: 0.741961
 63522/100000: episode: 873, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 0.740, mean reward: 0.016 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.139, 10.100], loss: 0.000080, mae: 0.004332, mean_q: 0.741438
 63567/100000: episode: 874, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 0.726, mean reward: 0.016 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-1.111, 10.100], loss: 0.000085, mae: 0.004740, mean_q: 0.741813
 63612/100000: episode: 875, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 0.737, mean reward: 0.016 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.512, 10.186], loss: 0.000055, mae: 0.003866, mean_q: 0.741038
 63657/100000: episode: 876, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.741, mean reward: 0.016 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.820, 10.100], loss: 0.000056, mae: 0.003856, mean_q: 0.741493
 63702/100000: episode: 877, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 0.787, mean reward: 0.017 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.465, 10.385], loss: 0.000061, mae: 0.003237, mean_q: 0.741096
 63747/100000: episode: 878, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 0.766, mean reward: 0.017 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.655, 10.146], loss: 0.000025, mae: 0.002812, mean_q: 0.740226
 63792/100000: episode: 879, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.763, mean reward: 0.017 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.436, 10.100], loss: 0.000042, mae: 0.002514, mean_q: 0.740134
 63837/100000: episode: 880, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 0.732, mean reward: 0.016 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.931, 10.281], loss: 0.000054, mae: 0.002821, mean_q: 0.739296
 63882/100000: episode: 881, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.770, mean reward: 0.017 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.286, 10.100], loss: 0.000031, mae: 0.002599, mean_q: 0.739338
 63927/100000: episode: 882, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.738, mean reward: 0.016 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.228, 10.100], loss: 0.000038, mae: 0.003213, mean_q: 0.739726
 63972/100000: episode: 883, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.711, mean reward: 0.016 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.721, 10.100], loss: 0.000047, mae: 0.002853, mean_q: 0.738975
 64017/100000: episode: 884, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 0.709, mean reward: 0.016 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.528, 10.100], loss: 0.000042, mae: 0.002354, mean_q: 0.738665
 64062/100000: episode: 885, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.731, mean reward: 0.016 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.767, 10.100], loss: 0.000042, mae: 0.002937, mean_q: 0.738522
 64107/100000: episode: 886, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 0.798, mean reward: 0.018 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.227, 10.100], loss: 0.000041, mae: 0.003364, mean_q: 0.737657
 64152/100000: episode: 887, duration: 0.229s, episode steps: 45, steps per second: 197, episode reward: 0.721, mean reward: 0.016 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.239, 10.100], loss: 0.000039, mae: 0.002597, mean_q: 0.737611
 64197/100000: episode: 888, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.772, mean reward: 0.017 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.200, 10.100], loss: 0.000044, mae: 0.004056, mean_q: 0.737383
 64242/100000: episode: 889, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 0.729, mean reward: 0.016 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.597, 10.442], loss: 0.000091, mae: 0.004850, mean_q: 0.738219
 64287/100000: episode: 890, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.678, mean reward: 0.015 [0.000, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.322, 10.142], loss: 0.000031, mae: 0.002465, mean_q: 0.737518
 64332/100000: episode: 891, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 0.795, mean reward: 0.018 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.497, 10.100], loss: 0.000047, mae: 0.002771, mean_q: 0.737182
 64377/100000: episode: 892, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 0.694, mean reward: 0.015 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.577, 10.214], loss: 0.000067, mae: 0.005223, mean_q: 0.736940
 64422/100000: episode: 893, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.662, mean reward: 0.015 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.287, 10.100], loss: 0.000066, mae: 0.004696, mean_q: 0.737102
 64467/100000: episode: 894, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.787, mean reward: 0.017 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.145, 10.100], loss: 0.000032, mae: 0.002244, mean_q: 0.736287
 64512/100000: episode: 895, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.785, mean reward: 0.017 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.709, 10.100], loss: 0.000062, mae: 0.003648, mean_q: 0.736099
 64557/100000: episode: 896, duration: 0.240s, episode steps: 45, steps per second: 188, episode reward: 0.673, mean reward: 0.015 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.110, 10.136], loss: 0.000031, mae: 0.002964, mean_q: 0.735653
 64602/100000: episode: 897, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 0.727, mean reward: 0.016 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.480, 10.100], loss: 0.000084, mae: 0.004862, mean_q: 0.735702
 64647/100000: episode: 898, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 0.703, mean reward: 0.016 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.866, 10.100], loss: 0.000058, mae: 0.003244, mean_q: 0.735542
 64692/100000: episode: 899, duration: 0.255s, episode steps: 45, steps per second: 176, episode reward: 0.685, mean reward: 0.015 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.076, 10.119], loss: 0.000057, mae: 0.003643, mean_q: 0.734797
 64737/100000: episode: 900, duration: 0.229s, episode steps: 45, steps per second: 197, episode reward: 0.754, mean reward: 0.017 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.944, 10.100], loss: 0.000048, mae: 0.003578, mean_q: 0.734703
 64782/100000: episode: 901, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.690, mean reward: 0.015 [0.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.358, 10.100], loss: 0.000023, mae: 0.002414, mean_q: 0.733537
 64827/100000: episode: 902, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.770, mean reward: 0.017 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.394, 10.100], loss: 0.000049, mae: 0.003134, mean_q: 0.734602
 64872/100000: episode: 903, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 0.695, mean reward: 0.015 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.166, 10.100], loss: 0.000053, mae: 0.004817, mean_q: 0.733528
 64917/100000: episode: 904, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.701, mean reward: 0.016 [0.000, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.896, 10.100], loss: 0.000036, mae: 0.002478, mean_q: 0.733256
 64962/100000: episode: 905, duration: 0.228s, episode steps: 45, steps per second: 197, episode reward: 0.721, mean reward: 0.016 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.096, 10.209], loss: 0.000039, mae: 0.003545, mean_q: 0.733389
 65007/100000: episode: 906, duration: 0.241s, episode steps: 45, steps per second: 186, episode reward: 0.715, mean reward: 0.016 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.103, 10.312], loss: 0.000034, mae: 0.002814, mean_q: 0.733043
 65052/100000: episode: 907, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 0.784, mean reward: 0.017 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.384, 10.100], loss: 0.000072, mae: 0.003673, mean_q: 0.733060
 65097/100000: episode: 908, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 0.691, mean reward: 0.015 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.095, 10.100], loss: 0.000063, mae: 0.005191, mean_q: 0.733053
 65142/100000: episode: 909, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.729, mean reward: 0.016 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.896, 10.100], loss: 0.000041, mae: 0.002749, mean_q: 0.732291
 65187/100000: episode: 910, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 0.946, mean reward: 0.021 [0.000, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.692, 10.225], loss: 0.000050, mae: 0.003076, mean_q: 0.731718
 65232/100000: episode: 911, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.736, mean reward: 0.016 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.715, 10.179], loss: 0.000061, mae: 0.003871, mean_q: 0.732129
 65277/100000: episode: 912, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.647, mean reward: 0.014 [0.000, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.272, 10.100], loss: 0.000032, mae: 0.002702, mean_q: 0.730650
 65322/100000: episode: 913, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.740, mean reward: 0.016 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.577, 10.100], loss: 0.000056, mae: 0.004444, mean_q: 0.731079
 65367/100000: episode: 914, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 0.707, mean reward: 0.016 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.716, 10.319], loss: 0.000042, mae: 0.002792, mean_q: 0.730656
 65412/100000: episode: 915, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 0.733, mean reward: 0.016 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.183, 10.100], loss: 0.000054, mae: 0.003308, mean_q: 0.730912
 65457/100000: episode: 916, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 0.762, mean reward: 0.017 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.717, 10.338], loss: 0.000060, mae: 0.003761, mean_q: 0.730046
 65502/100000: episode: 917, duration: 0.231s, episode steps: 45, steps per second: 194, episode reward: 0.699, mean reward: 0.016 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.685, 10.246], loss: 0.000034, mae: 0.002486, mean_q: 0.730604
 65547/100000: episode: 918, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 0.769, mean reward: 0.017 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.602, 10.302], loss: 0.000046, mae: 0.002944, mean_q: 0.729834
 65592/100000: episode: 919, duration: 0.236s, episode steps: 45, steps per second: 191, episode reward: 0.674, mean reward: 0.015 [0.000, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.614, 10.261], loss: 0.000077, mae: 0.005526, mean_q: 0.728391
 65637/100000: episode: 920, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.677, mean reward: 0.015 [0.000, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.062, 10.100], loss: 0.000026, mae: 0.002601, mean_q: 0.729068
 65682/100000: episode: 921, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: 0.738, mean reward: 0.016 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.540, 10.100], loss: 0.000080, mae: 0.004758, mean_q: 0.729833
 65727/100000: episode: 922, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.702, mean reward: 0.016 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.532, 10.100], loss: 0.000090, mae: 0.004911, mean_q: 0.729255
 65772/100000: episode: 923, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 0.703, mean reward: 0.016 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.318, 10.100], loss: 0.000063, mae: 0.003869, mean_q: 0.728397
 65817/100000: episode: 924, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 0.697, mean reward: 0.015 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.950, 10.100], loss: 0.000067, mae: 0.004467, mean_q: 0.728538
 65862/100000: episode: 925, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 0.726, mean reward: 0.016 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.022, 10.100], loss: 0.000038, mae: 0.003010, mean_q: 0.728334
 65907/100000: episode: 926, duration: 0.239s, episode steps: 45, steps per second: 189, episode reward: 0.739, mean reward: 0.016 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.382, 10.130], loss: 0.000067, mae: 0.003497, mean_q: 0.728911
 65952/100000: episode: 927, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.638, mean reward: 0.014 [0.000, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.917, 10.119], loss: 0.000044, mae: 0.003130, mean_q: 0.728601
 65997/100000: episode: 928, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 0.740, mean reward: 0.016 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.715, 10.100], loss: 0.000042, mae: 0.002497, mean_q: 0.728512
 66042/100000: episode: 929, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 0.731, mean reward: 0.016 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.348, 10.171], loss: 0.000018, mae: 0.002646, mean_q: 0.728078
 66087/100000: episode: 930, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 0.670, mean reward: 0.015 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.556, 10.230], loss: 0.000058, mae: 0.003248, mean_q: 0.728849
 66132/100000: episode: 931, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 0.812, mean reward: 0.018 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.886, 10.200], loss: 0.000050, mae: 0.002858, mean_q: 0.727791
 66177/100000: episode: 932, duration: 0.222s, episode steps: 45, steps per second: 203, episode reward: 0.735, mean reward: 0.016 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.895, 10.100], loss: 0.000064, mae: 0.003704, mean_q: 0.727955
 66222/100000: episode: 933, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 0.836, mean reward: 0.019 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.377, 10.250], loss: 0.000057, mae: 0.002809, mean_q: 0.727907
 66267/100000: episode: 934, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 0.715, mean reward: 0.016 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.382, 10.239], loss: 0.000023, mae: 0.002824, mean_q: 0.727623
 66312/100000: episode: 935, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 0.685, mean reward: 0.015 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.735, 10.100], loss: 0.000040, mae: 0.002087, mean_q: 0.727776
 66357/100000: episode: 936, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 0.681, mean reward: 0.015 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.329, 10.100], loss: 0.000065, mae: 0.003326, mean_q: 0.728021
 66402/100000: episode: 937, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 0.679, mean reward: 0.015 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.849, 10.100], loss: 0.000082, mae: 0.003823, mean_q: 0.727556
 66447/100000: episode: 938, duration: 0.230s, episode steps: 45, steps per second: 196, episode reward: 0.730, mean reward: 0.016 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.575, 10.100], loss: 0.000045, mae: 0.003460, mean_q: 0.727101
 66492/100000: episode: 939, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 0.751, mean reward: 0.017 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.887, 10.100], loss: 0.000053, mae: 0.003838, mean_q: 0.727845
 66537/100000: episode: 940, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 0.817, mean reward: 0.018 [0.000, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.522, 10.542], loss: 0.000051, mae: 0.002610, mean_q: 0.727864
 66582/100000: episode: 941, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 0.745, mean reward: 0.017 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.396, 10.132], loss: 0.000041, mae: 0.003406, mean_q: 0.726987
 66627/100000: episode: 942, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 0.894, mean reward: 0.020 [0.000, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.419, 10.100], loss: 0.000038, mae: 0.003251, mean_q: 0.727217
 66672/100000: episode: 943, duration: 0.239s, episode steps: 45, steps per second: 189, episode reward: 0.661, mean reward: 0.015 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.652, 10.100], loss: 0.000030, mae: 0.002085, mean_q: 0.727123
 66717/100000: episode: 944, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.776, mean reward: 0.017 [0.000, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.257, 10.100], loss: 0.000042, mae: 0.002605, mean_q: 0.727029
 66762/100000: episode: 945, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 0.803, mean reward: 0.018 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.398, 10.100], loss: 0.000041, mae: 0.002277, mean_q: 0.726875
 66807/100000: episode: 946, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 0.744, mean reward: 0.017 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.644, 10.100], loss: 0.000033, mae: 0.002322, mean_q: 0.726739
 66852/100000: episode: 947, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 0.734, mean reward: 0.016 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.345, 10.101], loss: 0.000022, mae: 0.002124, mean_q: 0.725714
 66897/100000: episode: 948, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 0.707, mean reward: 0.016 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.754, 10.100], loss: 0.000025, mae: 0.002581, mean_q: 0.725772
[Info] NOT FOUND NEW LEVEL, Current Best Level is 0.7671933174133301
2
 66942/100000: episode: 949, duration: 4.452s, episode steps: 45, steps per second: 10, episode reward: 0.696, mean reward: 0.015 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.500, 10.100], loss: 0.000038, mae: 0.003367, mean_q: 0.726864
 67043/100000: episode: 950, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.493, 10.100], loss: 0.000033, mae: 0.002667, mean_q: 0.725871
 67144/100000: episode: 951, duration: 0.541s, episode steps: 101, steps per second: 187, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.318, 10.245], loss: 0.000033, mae: 0.002783, mean_q: 0.726028
 67245/100000: episode: 952, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.703, mean reward: 0.007 [0.000, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.265, 10.105], loss: 0.000038, mae: 0.003189, mean_q: 0.725950
 67346/100000: episode: 953, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.753, mean reward: 0.007 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.965, 10.100], loss: 0.000058, mae: 0.003849, mean_q: 0.726173
 67447/100000: episode: 954, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.256, 10.312], loss: 0.000036, mae: 0.002741, mean_q: 0.725846
 67548/100000: episode: 955, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.132, 10.100], loss: 0.000043, mae: 0.003043, mean_q: 0.726128
 67649/100000: episode: 956, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.720, mean reward: 0.007 [0.000, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.289, 10.225], loss: 0.000038, mae: 0.002796, mean_q: 0.725820
 67750/100000: episode: 957, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.801, 10.304], loss: 0.000041, mae: 0.002907, mean_q: 0.726178
 67851/100000: episode: 958, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.724, mean reward: 0.007 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.838, 10.100], loss: 0.000022, mae: 0.002024, mean_q: 0.725914
 67952/100000: episode: 959, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.020, 10.286], loss: 0.000024, mae: 0.002222, mean_q: 0.725901
 68053/100000: episode: 960, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.218, 10.100], loss: 0.000039, mae: 0.003374, mean_q: 0.726403
 68154/100000: episode: 961, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.301, 10.100], loss: 0.000039, mae: 0.003284, mean_q: 0.726571
 68255/100000: episode: 962, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.143], loss: 0.000027, mae: 0.002315, mean_q: 0.726237
 68356/100000: episode: 963, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.822, mean reward: 0.008 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.223, 10.125], loss: 0.000036, mae: 0.002963, mean_q: 0.726724
 68457/100000: episode: 964, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.799, mean reward: 0.008 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.295, 10.100], loss: 0.000036, mae: 0.002793, mean_q: 0.726871
 68558/100000: episode: 965, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.165, 10.105], loss: 0.000042, mae: 0.003544, mean_q: 0.727135
 68659/100000: episode: 966, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.100], loss: 0.000024, mae: 0.002445, mean_q: 0.727096
 68760/100000: episode: 967, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.631, 10.100], loss: 0.000017, mae: 0.002402, mean_q: 0.727294
 68861/100000: episode: 968, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.444, 10.100], loss: 0.000027, mae: 0.002292, mean_q: 0.727360
 68962/100000: episode: 969, duration: 0.547s, episode steps: 101, steps per second: 185, episode reward: 0.718, mean reward: 0.007 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.226], loss: 0.000021, mae: 0.001780, mean_q: 0.727172
 69063/100000: episode: 970, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.840, mean reward: 0.008 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.150, 10.124], loss: 0.000033, mae: 0.002748, mean_q: 0.727548
 69164/100000: episode: 971, duration: 0.495s, episode steps: 101, steps per second: 204, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.467, 10.190], loss: 0.000030, mae: 0.002254, mean_q: 0.727573
 69265/100000: episode: 972, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.100], loss: 0.000024, mae: 0.002146, mean_q: 0.727926
 69366/100000: episode: 973, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.723, mean reward: 0.007 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.853, 10.100], loss: 0.000018, mae: 0.001790, mean_q: 0.727777
 69467/100000: episode: 974, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.123, 10.113], loss: 0.000026, mae: 0.002277, mean_q: 0.727923
 69568/100000: episode: 975, duration: 0.492s, episode steps: 101, steps per second: 205, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.811, 10.100], loss: 0.000018, mae: 0.001768, mean_q: 0.727758
 69669/100000: episode: 976, duration: 0.498s, episode steps: 101, steps per second: 203, episode reward: 0.886, mean reward: 0.009 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.749, 10.406], loss: 0.000031, mae: 0.002468, mean_q: 0.728118
 69770/100000: episode: 977, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.755, mean reward: 0.007 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.047, 10.100], loss: 0.000038, mae: 0.002931, mean_q: 0.728403
 69871/100000: episode: 978, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.728, mean reward: 0.007 [0.000, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.855, 10.100], loss: 0.000026, mae: 0.002672, mean_q: 0.728673
 69972/100000: episode: 979, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.745, mean reward: 0.007 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.460, 10.262], loss: 0.000017, mae: 0.002024, mean_q: 0.728482
 70073/100000: episode: 980, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.051, 10.182], loss: 0.000023, mae: 0.002500, mean_q: 0.728762
 70174/100000: episode: 981, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.686, mean reward: 0.007 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.105, 10.155], loss: 0.000036, mae: 0.003640, mean_q: 0.729182
 70275/100000: episode: 982, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.823, 10.177], loss: 0.000017, mae: 0.001873, mean_q: 0.729213
 70376/100000: episode: 983, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.754, mean reward: 0.007 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.882, 10.221], loss: 0.000021, mae: 0.002361, mean_q: 0.729046
 70477/100000: episode: 984, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.838, mean reward: 0.008 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.251, 10.100], loss: 0.000024, mae: 0.002193, mean_q: 0.729514
 70578/100000: episode: 985, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.295, 10.100], loss: 0.000015, mae: 0.001801, mean_q: 0.729512
 70679/100000: episode: 986, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.663, 10.201], loss: 0.000025, mae: 0.002606, mean_q: 0.729788
 70780/100000: episode: 987, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.380, 10.396], loss: 0.000012, mae: 0.001623, mean_q: 0.730058
 70881/100000: episode: 988, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.956, 10.284], loss: 0.000015, mae: 0.001625, mean_q: 0.729986
 70982/100000: episode: 989, duration: 0.511s, episode steps: 101, steps per second: 197, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.769, 10.100], loss: 0.000019, mae: 0.002294, mean_q: 0.729975
 71083/100000: episode: 990, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.267, 10.429], loss: 0.000023, mae: 0.002124, mean_q: 0.730369
 71184/100000: episode: 991, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.717, 10.100], loss: 0.000025, mae: 0.002258, mean_q: 0.730462
 71285/100000: episode: 992, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.998, 10.100], loss: 0.000018, mae: 0.002099, mean_q: 0.730818
 71386/100000: episode: 993, duration: 0.501s, episode steps: 101, steps per second: 201, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.751, 10.100], loss: 0.000012, mae: 0.001498, mean_q: 0.730813
 71487/100000: episode: 994, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.802, mean reward: 0.008 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.401, 10.465], loss: 0.000021, mae: 0.002185, mean_q: 0.730561
 71588/100000: episode: 995, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.100], loss: 0.000019, mae: 0.002200, mean_q: 0.730963
 71689/100000: episode: 996, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.735, 10.100], loss: 0.000019, mae: 0.002030, mean_q: 0.731374
 71790/100000: episode: 997, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.780, mean reward: 0.008 [0.000, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.601, 10.294], loss: 0.000012, mae: 0.001647, mean_q: 0.731166
 71891/100000: episode: 998, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.100], loss: 0.000010, mae: 0.001944, mean_q: 0.731426
 71992/100000: episode: 999, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.673, mean reward: 0.007 [0.000, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.962, 10.100], loss: 0.000010, mae: 0.001440, mean_q: 0.731777
 72093/100000: episode: 1000, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.937, 10.100], loss: 0.000013, mae: 0.001845, mean_q: 0.732048
 72194/100000: episode: 1001, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.731, mean reward: 0.007 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.081, 10.179], loss: 0.000015, mae: 0.002088, mean_q: 0.732541
 72295/100000: episode: 1002, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.761, 10.100], loss: 0.000013, mae: 0.001926, mean_q: 0.732571
 72396/100000: episode: 1003, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.752, mean reward: 0.007 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.630, 10.100], loss: 0.000009, mae: 0.001531, mean_q: 0.732714
 72497/100000: episode: 1004, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.695, mean reward: 0.007 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.097, 10.312], loss: 0.000009, mae: 0.001405, mean_q: 0.733029
 72598/100000: episode: 1005, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.785, mean reward: 0.008 [0.000, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.845, 10.100], loss: 0.000018, mae: 0.002390, mean_q: 0.733446
 72699/100000: episode: 1006, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.100], loss: 0.000014, mae: 0.001813, mean_q: 0.733375
 72800/100000: episode: 1007, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.403, 10.289], loss: 0.000015, mae: 0.002047, mean_q: 0.733986
 72901/100000: episode: 1008, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.656, 10.109], loss: 0.000013, mae: 0.001778, mean_q: 0.734205
 73002/100000: episode: 1009, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.198], loss: 0.000011, mae: 0.001613, mean_q: 0.734574
 73103/100000: episode: 1010, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.790, mean reward: 0.008 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.075, 10.140], loss: 0.000021, mae: 0.002903, mean_q: 0.734722
 73204/100000: episode: 1011, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.357, 10.254], loss: 0.000016, mae: 0.002266, mean_q: 0.735089
 73305/100000: episode: 1012, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.833, mean reward: 0.008 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.607, 10.100], loss: 0.000018, mae: 0.002006, mean_q: 0.735736
 73406/100000: episode: 1013, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.898, mean reward: 0.009 [0.000, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.851, 10.100], loss: 0.000013, mae: 0.001664, mean_q: 0.736016
 73507/100000: episode: 1014, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.842, 10.100], loss: 0.000017, mae: 0.002243, mean_q: 0.736467
 73608/100000: episode: 1015, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.613, 10.100], loss: 0.000020, mae: 0.002596, mean_q: 0.736768
 73709/100000: episode: 1016, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.856, mean reward: 0.008 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.095, 10.277], loss: 0.000009, mae: 0.001487, mean_q: 0.736758
 73810/100000: episode: 1017, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.681, mean reward: 0.007 [0.000, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.351, 10.185], loss: 0.000012, mae: 0.001965, mean_q: 0.737162
 73911/100000: episode: 1018, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.773, mean reward: 0.008 [0.000, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.039, 10.242], loss: 0.000025, mae: 0.002555, mean_q: 0.738012
 74012/100000: episode: 1019, duration: 0.528s, episode steps: 101, steps per second: 191, episode reward: 0.796, mean reward: 0.008 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.100], loss: 0.000020, mae: 0.002242, mean_q: 0.738137
 74113/100000: episode: 1020, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.982, 10.100], loss: 0.000016, mae: 0.002263, mean_q: 0.738627
 74214/100000: episode: 1021, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.936, 10.166], loss: 0.000014, mae: 0.001878, mean_q: 0.738287
 74315/100000: episode: 1022, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.465, 10.127], loss: 0.000015, mae: 0.001890, mean_q: 0.738779
 74416/100000: episode: 1023, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.786, 10.100], loss: 0.000025, mae: 0.002199, mean_q: 0.739478
 74517/100000: episode: 1024, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.689, mean reward: 0.007 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.484, 10.290], loss: 0.000010, mae: 0.001697, mean_q: 0.739805
 74618/100000: episode: 1025, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.396, 10.136], loss: 0.000018, mae: 0.002227, mean_q: 0.739988
 74719/100000: episode: 1026, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.059, 10.100], loss: 0.000017, mae: 0.002235, mean_q: 0.740275
 74820/100000: episode: 1027, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.683, mean reward: 0.007 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.895, 10.104], loss: 0.000008, mae: 0.001361, mean_q: 0.740179
 74921/100000: episode: 1028, duration: 0.496s, episode steps: 101, steps per second: 204, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.277, 10.100], loss: 0.000018, mae: 0.001946, mean_q: 0.740754
 75022/100000: episode: 1029, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.791, mean reward: 0.008 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.464, 10.163], loss: 0.000017, mae: 0.002160, mean_q: 0.741074
 75123/100000: episode: 1030, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.939, mean reward: 0.009 [0.000, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.348, 10.100], loss: 0.000017, mae: 0.002162, mean_q: 0.740739
 75224/100000: episode: 1031, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.810, mean reward: 0.008 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.483, 10.215], loss: 0.000019, mae: 0.001973, mean_q: 0.741265
 75325/100000: episode: 1032, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.734, mean reward: 0.007 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.385, 10.188], loss: 0.000019, mae: 0.002265, mean_q: 0.741489
 75426/100000: episode: 1033, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.743, 10.125], loss: 0.000013, mae: 0.001655, mean_q: 0.741953
 75527/100000: episode: 1034, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.794, mean reward: 0.008 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.832, 10.231], loss: 0.000019, mae: 0.001937, mean_q: 0.741842
 75628/100000: episode: 1035, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.809, mean reward: 0.008 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.805, 10.503], loss: 0.000015, mae: 0.002115, mean_q: 0.742279
 75729/100000: episode: 1036, duration: 0.521s, episode steps: 101, steps per second: 194, episode reward: 0.842, mean reward: 0.008 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.240, 10.154], loss: 0.000015, mae: 0.001779, mean_q: 0.742332
 75830/100000: episode: 1037, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.904, 10.100], loss: 0.000023, mae: 0.002512, mean_q: 0.743035
 75931/100000: episode: 1038, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.100], loss: 0.000024, mae: 0.002173, mean_q: 0.743776
 76032/100000: episode: 1039, duration: 0.520s, episode steps: 101, steps per second: 194, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.331], loss: 0.000013, mae: 0.002297, mean_q: 0.743576
 76133/100000: episode: 1040, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.756, mean reward: 0.007 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.601, 10.100], loss: 0.000020, mae: 0.002282, mean_q: 0.743813
 76234/100000: episode: 1041, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.385, 10.334], loss: 0.000015, mae: 0.001892, mean_q: 0.744134
 76335/100000: episode: 1042, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.818, mean reward: 0.008 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.494, 10.100], loss: 0.000009, mae: 0.001528, mean_q: 0.744191
 76436/100000: episode: 1043, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: 0.853, mean reward: 0.008 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.991, 10.228], loss: 0.000009, mae: 0.001255, mean_q: 0.745118
 76537/100000: episode: 1044, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.792, mean reward: 0.008 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.519, 10.314], loss: 0.000022, mae: 0.002114, mean_q: 0.745531
 76638/100000: episode: 1045, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.782, mean reward: 0.008 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.167, 10.189], loss: 0.000018, mae: 0.001904, mean_q: 0.745762
 76739/100000: episode: 1046, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.676, mean reward: 0.007 [0.000, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.100], loss: 0.000013, mae: 0.002212, mean_q: 0.745644
 76840/100000: episode: 1047, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.769, mean reward: 0.008 [0.000, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.601, 10.159], loss: 0.000014, mae: 0.001767, mean_q: 0.745992
 76941/100000: episode: 1048, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.721, 10.100], loss: 0.000017, mae: 0.002069, mean_q: 0.746648
[Info] 1-TH LEVEL FOUND: 0.7583467960357666, Considering 10/100 traces
 77042/100000: episode: 1049, duration: 4.958s, episode steps: 101, steps per second: 20, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.388, 10.218], loss: 0.000023, mae: 0.002391, mean_q: 0.746918
 77074/100000: episode: 1050, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.765, mean reward: 0.024 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.168, 10.100], loss: 0.000044, mae: 0.002835, mean_q: 0.747743
 77108/100000: episode: 1051, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 0.827, mean reward: 0.024 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.084, 10.100], loss: 0.000019, mae: 0.002067, mean_q: 0.747447
 77139/100000: episode: 1052, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 0.715, mean reward: 0.023 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.219, 10.100], loss: 0.000028, mae: 0.002628, mean_q: 0.747651
 77171/100000: episode: 1053, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 0.712, mean reward: 0.022 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.198, 10.100], loss: 0.000028, mae: 0.004038, mean_q: 0.747537
 77193/100000: episode: 1054, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.668, mean reward: 0.030 [0.000, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.058, 10.311], loss: 0.000011, mae: 0.002350, mean_q: 0.747466
 77225/100000: episode: 1055, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 0.644, mean reward: 0.020 [0.000, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.229, 10.109], loss: 0.000018, mae: 0.001597, mean_q: 0.747359
 77259/100000: episode: 1056, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 0.842, mean reward: 0.025 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.719, 10.100], loss: 0.000041, mae: 0.002714, mean_q: 0.747790
 77293/100000: episode: 1057, duration: 0.173s, episode steps: 34, steps per second: 196, episode reward: 0.804, mean reward: 0.024 [0.000, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.787, 10.100], loss: 0.000032, mae: 0.003024, mean_q: 0.748051
 77315/100000: episode: 1058, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 0.774, mean reward: 0.035 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.549, 10.100], loss: 0.000026, mae: 0.002474, mean_q: 0.747239
 77343/100000: episode: 1059, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 0.767, mean reward: 0.027 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.645, 10.100], loss: 0.000038, mae: 0.002303, mean_q: 0.748481
 77365/100000: episode: 1060, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 0.688, mean reward: 0.031 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.935, 10.100], loss: 0.000040, mae: 0.002772, mean_q: 0.748361
 77393/100000: episode: 1061, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 0.727, mean reward: 0.026 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.838, 10.100], loss: 0.000026, mae: 0.003392, mean_q: 0.747910
 77415/100000: episode: 1062, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 0.702, mean reward: 0.032 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.864, 10.199], loss: 0.000015, mae: 0.001534, mean_q: 0.748519
 77447/100000: episode: 1063, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 0.805, mean reward: 0.025 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.707, 10.100], loss: 0.000012, mae: 0.002122, mean_q: 0.748078
 77478/100000: episode: 1064, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 0.727, mean reward: 0.023 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.491, 10.100], loss: 0.000016, mae: 0.001988, mean_q: 0.747837
 77512/100000: episode: 1065, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 0.857, mean reward: 0.025 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.201, 10.100], loss: 0.000029, mae: 0.002489, mean_q: 0.748131
 77546/100000: episode: 1066, duration: 0.172s, episode steps: 34, steps per second: 197, episode reward: 0.881, mean reward: 0.026 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.532, 10.100], loss: 0.000030, mae: 0.002158, mean_q: 0.748357
 77568/100000: episode: 1067, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 0.782, mean reward: 0.036 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.387, 10.209], loss: 0.000055, mae: 0.003260, mean_q: 0.749927
 77600/100000: episode: 1068, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.748, mean reward: 0.023 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.217, 10.101], loss: 0.000016, mae: 0.002727, mean_q: 0.748875
 77622/100000: episode: 1069, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 0.803, mean reward: 0.037 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.101, 10.362], loss: 0.000011, mae: 0.002013, mean_q: 0.749172
 77643/100000: episode: 1070, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 0.743, mean reward: 0.035 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.290, 10.104], loss: 0.000013, mae: 0.001875, mean_q: 0.748661
 77665/100000: episode: 1071, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 0.725, mean reward: 0.033 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.269], loss: 0.000030, mae: 0.002079, mean_q: 0.749996
 77696/100000: episode: 1072, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 0.737, mean reward: 0.024 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.212, 10.100], loss: 0.000014, mae: 0.001926, mean_q: 0.749181
 77718/100000: episode: 1073, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 0.729, mean reward: 0.033 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.513, 10.356], loss: 0.000021, mae: 0.002148, mean_q: 0.749958
 77752/100000: episode: 1074, duration: 0.180s, episode steps: 34, steps per second: 188, episode reward: 0.792, mean reward: 0.023 [0.000, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.762, 10.100], loss: 0.000020, mae: 0.002343, mean_q: 0.749379
 77782/100000: episode: 1075, duration: 0.157s, episode steps: 30, steps per second: 192, episode reward: 0.636, mean reward: 0.021 [0.000, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.312, 10.100], loss: 0.000036, mae: 0.002539, mean_q: 0.750452
 77804/100000: episode: 1076, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 0.841, mean reward: 0.038 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.829, 10.450], loss: 0.000011, mae: 0.002710, mean_q: 0.749713
 77826/100000: episode: 1077, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.639, mean reward: 0.029 [0.000, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.261, 10.100], loss: 0.000031, mae: 0.002963, mean_q: 0.749339
 77854/100000: episode: 1078, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 0.755, mean reward: 0.027 [0.000, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.916, 10.100], loss: 0.000042, mae: 0.003709, mean_q: 0.750623
 77882/100000: episode: 1079, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 0.705, mean reward: 0.025 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.776, 10.100], loss: 0.000022, mae: 0.002818, mean_q: 0.750736
 77913/100000: episode: 1080, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 0.764, mean reward: 0.025 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.698, 10.100], loss: 0.000025, mae: 0.002402, mean_q: 0.750480
 77944/100000: episode: 1081, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 0.725, mean reward: 0.023 [0.000, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.317, 10.100], loss: 0.000021, mae: 0.002119, mean_q: 0.750594
 77965/100000: episode: 1082, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 0.789, mean reward: 0.038 [0.000, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.571, 10.368], loss: 0.000010, mae: 0.001479, mean_q: 0.750819
 77987/100000: episode: 1083, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.697, mean reward: 0.032 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.083, 10.204], loss: 0.000061, mae: 0.002805, mean_q: 0.752477
 78021/100000: episode: 1084, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 0.950, mean reward: 0.028 [0.000, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.659, 10.100], loss: 0.000031, mae: 0.003750, mean_q: 0.751122
 78052/100000: episode: 1085, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 0.740, mean reward: 0.024 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.330, 10.100], loss: 0.000037, mae: 0.002570, mean_q: 0.751249
 78074/100000: episode: 1086, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 0.685, mean reward: 0.031 [0.000, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.426, 10.204], loss: 0.000027, mae: 0.002820, mean_q: 0.751631
 78104/100000: episode: 1087, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 0.726, mean reward: 0.024 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.458, 10.100], loss: 0.000032, mae: 0.002536, mean_q: 0.751685
 78138/100000: episode: 1088, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 0.839, mean reward: 0.025 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.329, 10.100], loss: 0.000012, mae: 0.002097, mean_q: 0.751743
 78160/100000: episode: 1089, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 0.766, mean reward: 0.035 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.187, 10.100], loss: 0.000038, mae: 0.001811, mean_q: 0.751555
 78194/100000: episode: 1090, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 0.865, mean reward: 0.025 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.887, 10.100], loss: 0.000051, mae: 0.003256, mean_q: 0.751853
 78228/100000: episode: 1091, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 0.864, mean reward: 0.025 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.298, 10.100], loss: 0.000044, mae: 0.002992, mean_q: 0.752126
 78260/100000: episode: 1092, duration: 0.170s, episode steps: 32, steps per second: 189, episode reward: 0.734, mean reward: 0.023 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.082, 10.100], loss: 0.000042, mae: 0.002538, mean_q: 0.751704
 78290/100000: episode: 1093, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 0.686, mean reward: 0.023 [0.000, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.265, 10.100], loss: 0.000042, mae: 0.003607, mean_q: 0.753010
 78311/100000: episode: 1094, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 0.683, mean reward: 0.033 [0.000, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.361, 10.156], loss: 0.000004, mae: 0.001255, mean_q: 0.752090
 78343/100000: episode: 1095, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 0.743, mean reward: 0.023 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.717, 10.100], loss: 0.000046, mae: 0.003005, mean_q: 0.752165
 78375/100000: episode: 1096, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 0.662, mean reward: 0.021 [0.000, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.163, 10.100], loss: 0.000020, mae: 0.002789, mean_q: 0.752669
 78405/100000: episode: 1097, duration: 0.150s, episode steps: 30, steps per second: 201, episode reward: 0.808, mean reward: 0.027 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.020, 10.112], loss: 0.000050, mae: 0.003257, mean_q: 0.752636
 78427/100000: episode: 1098, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 0.700, mean reward: 0.032 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.310, 10.221], loss: 0.000027, mae: 0.002712, mean_q: 0.752471
 78458/100000: episode: 1099, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 0.722, mean reward: 0.023 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.797, 10.100], loss: 0.000064, mae: 0.004867, mean_q: 0.752652
 78480/100000: episode: 1100, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 0.745, mean reward: 0.034 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.849, 10.100], loss: 0.000029, mae: 0.003660, mean_q: 0.752430
 78512/100000: episode: 1101, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 0.823, mean reward: 0.026 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.904, 10.100], loss: 0.000054, mae: 0.002440, mean_q: 0.752228
 78534/100000: episode: 1102, duration: 0.111s, episode steps: 22, steps per second: 197, episode reward: 0.670, mean reward: 0.030 [0.000, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.238, 10.100], loss: 0.000020, mae: 0.002441, mean_q: 0.752401
 78562/100000: episode: 1103, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 0.709, mean reward: 0.025 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.571, 10.100], loss: 0.000030, mae: 0.002040, mean_q: 0.752840
 78590/100000: episode: 1104, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 0.765, mean reward: 0.027 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.281, 10.100], loss: 0.000009, mae: 0.001482, mean_q: 0.753553
 78611/100000: episode: 1105, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 0.777, mean reward: 0.037 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.062, 10.100], loss: 0.000033, mae: 0.002157, mean_q: 0.753215
 78639/100000: episode: 1106, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 0.795, mean reward: 0.028 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.333, 10.100], loss: 0.000042, mae: 0.003192, mean_q: 0.752878
 78671/100000: episode: 1107, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 0.760, mean reward: 0.024 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.123, 10.100], loss: 0.000022, mae: 0.001683, mean_q: 0.753425
 78693/100000: episode: 1108, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 0.757, mean reward: 0.034 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.503, 10.100], loss: 0.000051, mae: 0.002831, mean_q: 0.753193
 78715/100000: episode: 1109, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.714, mean reward: 0.032 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.785, 10.129], loss: 0.000023, mae: 0.002086, mean_q: 0.753201
 78743/100000: episode: 1110, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 0.770, mean reward: 0.028 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.147, 10.100], loss: 0.000026, mae: 0.002688, mean_q: 0.753142
 78775/100000: episode: 1111, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 0.718, mean reward: 0.022 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.225, 10.100], loss: 0.000035, mae: 0.003389, mean_q: 0.753044
 78797/100000: episode: 1112, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 0.734, mean reward: 0.033 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.263, 10.216], loss: 0.000050, mae: 0.003504, mean_q: 0.754173
 78819/100000: episode: 1113, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 0.767, mean reward: 0.035 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.037, 10.355], loss: 0.000016, mae: 0.002038, mean_q: 0.752952
 78850/100000: episode: 1114, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 0.642, mean reward: 0.021 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.393, 10.100], loss: 0.000025, mae: 0.002224, mean_q: 0.753651
 78872/100000: episode: 1115, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 0.688, mean reward: 0.031 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.755, 10.126], loss: 0.000037, mae: 0.002324, mean_q: 0.754110
 78904/100000: episode: 1116, duration: 0.170s, episode steps: 32, steps per second: 189, episode reward: 0.771, mean reward: 0.024 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.306, 10.100], loss: 0.000028, mae: 0.002418, mean_q: 0.753823
 78935/100000: episode: 1117, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 0.710, mean reward: 0.023 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.890, 10.100], loss: 0.000053, mae: 0.003380, mean_q: 0.754088
 78966/100000: episode: 1118, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 0.717, mean reward: 0.023 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.209, 10.100], loss: 0.000042, mae: 0.002561, mean_q: 0.753436
 79000/100000: episode: 1119, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 0.734, mean reward: 0.022 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.055, 10.100], loss: 0.000037, mae: 0.002368, mean_q: 0.753533
 79030/100000: episode: 1120, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 0.608, mean reward: 0.020 [0.000, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.457, 10.100], loss: 0.000039, mae: 0.003319, mean_q: 0.754283
 79052/100000: episode: 1121, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 0.712, mean reward: 0.032 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.395, 10.155], loss: 0.000078, mae: 0.004176, mean_q: 0.753234
 79074/100000: episode: 1122, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.623, mean reward: 0.028 [0.000, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.655, 10.172], loss: 0.000028, mae: 0.002923, mean_q: 0.753310
 79096/100000: episode: 1123, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 0.738, mean reward: 0.034 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.150, 10.139], loss: 0.000059, mae: 0.002293, mean_q: 0.754398
 79118/100000: episode: 1124, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 0.651, mean reward: 0.030 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.324, 10.100], loss: 0.000030, mae: 0.002895, mean_q: 0.753549
 79146/100000: episode: 1125, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 0.721, mean reward: 0.026 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.083, 10.100], loss: 0.000070, mae: 0.003274, mean_q: 0.753973
 79167/100000: episode: 1126, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 0.722, mean reward: 0.034 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.638, 10.107], loss: 0.000057, mae: 0.002934, mean_q: 0.753747
 79201/100000: episode: 1127, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 0.833, mean reward: 0.024 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.571, 10.100], loss: 0.000059, mae: 0.003878, mean_q: 0.753888
 79229/100000: episode: 1128, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 0.835, mean reward: 0.030 [0.000, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.047, 10.100], loss: 0.000053, mae: 0.004504, mean_q: 0.753958
 79257/100000: episode: 1129, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 0.733, mean reward: 0.026 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.100], loss: 0.000049, mae: 0.002751, mean_q: 0.753848
 79289/100000: episode: 1130, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 0.742, mean reward: 0.023 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.340, 10.100], loss: 0.000103, mae: 0.004526, mean_q: 0.755128
 79317/100000: episode: 1131, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 0.797, mean reward: 0.028 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.722, 10.100], loss: 0.000021, mae: 0.001969, mean_q: 0.753457
 79338/100000: episode: 1132, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 0.715, mean reward: 0.034 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.345], loss: 0.000060, mae: 0.004509, mean_q: 0.754342
 79369/100000: episode: 1133, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 0.794, mean reward: 0.026 [0.000, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.686, 10.150], loss: 0.000021, mae: 0.002511, mean_q: 0.754109
 79391/100000: episode: 1134, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 0.696, mean reward: 0.032 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.576, 10.100], loss: 0.000035, mae: 0.002191, mean_q: 0.754074
 79412/100000: episode: 1135, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 0.764, mean reward: 0.036 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.528, 10.158], loss: 0.000054, mae: 0.003299, mean_q: 0.753808
 79440/100000: episode: 1136, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 0.722, mean reward: 0.026 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.961, 10.215], loss: 0.000038, mae: 0.003122, mean_q: 0.754133
 79462/100000: episode: 1137, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 0.705, mean reward: 0.032 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.474, 10.156], loss: 0.000066, mae: 0.003778, mean_q: 0.753818
 79484/100000: episode: 1138, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 0.633, mean reward: 0.029 [0.000, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.176, 10.100], loss: 0.000061, mae: 0.003490, mean_q: 0.753864
[Info] 2-TH LEVEL FOUND: 0.7612953186035156, Considering 13/100 traces
 79506/100000: episode: 1139, duration: 4.512s, episode steps: 22, steps per second: 5, episode reward: 0.642, mean reward: 0.029 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.162, 10.229], loss: 0.000052, mae: 0.003104, mean_q: 0.753662
 79554/100000: episode: 1140, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.903, mean reward: 0.019 [0.000, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.962, 10.100], loss: 0.000086, mae: 0.004528, mean_q: 0.753059
 79602/100000: episode: 1141, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.823, mean reward: 0.017 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.091, 10.100], loss: 0.000061, mae: 0.003542, mean_q: 0.753401
 79650/100000: episode: 1142, duration: 0.234s, episode steps: 48, steps per second: 205, episode reward: 0.799, mean reward: 0.017 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.928, 10.100], loss: 0.000035, mae: 0.002490, mean_q: 0.753820
 79698/100000: episode: 1143, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 0.838, mean reward: 0.017 [0.000, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.278, 10.100], loss: 0.000017, mae: 0.001571, mean_q: 0.753467
 79746/100000: episode: 1144, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.721, mean reward: 0.015 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.951, 10.100], loss: 0.000066, mae: 0.004019, mean_q: 0.753364
 79794/100000: episode: 1145, duration: 0.292s, episode steps: 48, steps per second: 165, episode reward: 0.782, mean reward: 0.016 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.023, 10.335], loss: 0.000043, mae: 0.003077, mean_q: 0.752920
 79842/100000: episode: 1146, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.848, mean reward: 0.018 [0.000, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.439, 10.100], loss: 0.000033, mae: 0.002902, mean_q: 0.752663
 79890/100000: episode: 1147, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 0.782, mean reward: 0.016 [0.000, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.754, 10.100], loss: 0.000042, mae: 0.002892, mean_q: 0.753174
 79938/100000: episode: 1148, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.772, mean reward: 0.016 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.301, 10.100], loss: 0.000058, mae: 0.003962, mean_q: 0.753451
 79986/100000: episode: 1149, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 0.850, mean reward: 0.018 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.524, 10.246], loss: 0.000046, mae: 0.002446, mean_q: 0.753023
 80034/100000: episode: 1150, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 0.845, mean reward: 0.018 [0.000, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.348, 10.100], loss: 0.000070, mae: 0.004754, mean_q: 0.752711
 80082/100000: episode: 1151, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.805, mean reward: 0.017 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-1.192, 10.100], loss: 0.000060, mae: 0.003030, mean_q: 0.753587
 80130/100000: episode: 1152, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 0.761, mean reward: 0.016 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.481, 10.100], loss: 0.000054, mae: 0.002853, mean_q: 0.753578
 80178/100000: episode: 1153, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 0.818, mean reward: 0.017 [0.000, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.115, 10.100], loss: 0.000036, mae: 0.002048, mean_q: 0.753323
 80226/100000: episode: 1154, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.764, mean reward: 0.016 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.574, 10.100], loss: 0.000044, mae: 0.002264, mean_q: 0.753684
 80274/100000: episode: 1155, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 0.921, mean reward: 0.019 [0.000, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.715, 10.100], loss: 0.000080, mae: 0.003299, mean_q: 0.753394
 80322/100000: episode: 1156, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.740, mean reward: 0.015 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.035, 10.236], loss: 0.000070, mae: 0.004709, mean_q: 0.753694
 80370/100000: episode: 1157, duration: 0.244s, episode steps: 48, steps per second: 196, episode reward: 0.850, mean reward: 0.018 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.093, 10.100], loss: 0.000047, mae: 0.002643, mean_q: 0.753097
 80418/100000: episode: 1158, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 0.768, mean reward: 0.016 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.174, 10.100], loss: 0.000045, mae: 0.003117, mean_q: 0.753855
 80466/100000: episode: 1159, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.813, mean reward: 0.017 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.218, 10.142], loss: 0.000049, mae: 0.002937, mean_q: 0.753799
 80514/100000: episode: 1160, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 0.864, mean reward: 0.018 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.212, 10.100], loss: 0.000072, mae: 0.004505, mean_q: 0.753692
 80562/100000: episode: 1161, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.740, mean reward: 0.015 [0.000, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.416, 10.100], loss: 0.000061, mae: 0.004788, mean_q: 0.753177
 80610/100000: episode: 1162, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 0.858, mean reward: 0.018 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.571, 10.100], loss: 0.000057, mae: 0.004172, mean_q: 0.754070
 80658/100000: episode: 1163, duration: 0.238s, episode steps: 48, steps per second: 201, episode reward: 0.810, mean reward: 0.017 [0.000, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.281, 10.100], loss: 0.000049, mae: 0.003526, mean_q: 0.753396
 80706/100000: episode: 1164, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 0.765, mean reward: 0.016 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.326, 10.100], loss: 0.000047, mae: 0.003469, mean_q: 0.753692
 80754/100000: episode: 1165, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 0.846, mean reward: 0.018 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.026, 10.100], loss: 0.000063, mae: 0.002961, mean_q: 0.754129
 80802/100000: episode: 1166, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 0.689, mean reward: 0.014 [0.000, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.173, 10.100], loss: 0.000073, mae: 0.003147, mean_q: 0.753908
 80850/100000: episode: 1167, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.815, mean reward: 0.017 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.818, 10.100], loss: 0.000060, mae: 0.003504, mean_q: 0.754193
 80898/100000: episode: 1168, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 0.771, mean reward: 0.016 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.297, 10.100], loss: 0.000050, mae: 0.003235, mean_q: 0.753550
 80946/100000: episode: 1169, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 0.830, mean reward: 0.017 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.368, 10.100], loss: 0.000039, mae: 0.002570, mean_q: 0.753540
 80994/100000: episode: 1170, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.770, mean reward: 0.016 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.147, 10.100], loss: 0.000074, mae: 0.003838, mean_q: 0.753822
 81042/100000: episode: 1171, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 0.896, mean reward: 0.019 [0.000, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.250, 10.100], loss: 0.000067, mae: 0.003975, mean_q: 0.753441
 81090/100000: episode: 1172, duration: 0.236s, episode steps: 48, steps per second: 203, episode reward: 0.860, mean reward: 0.018 [0.000, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.489, 10.100], loss: 0.000049, mae: 0.003477, mean_q: 0.754380
 81138/100000: episode: 1173, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.747, mean reward: 0.016 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.716, 10.100], loss: 0.000063, mae: 0.003641, mean_q: 0.754741
 81186/100000: episode: 1174, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 0.826, mean reward: 0.017 [0.000, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.364, 10.100], loss: 0.000065, mae: 0.003638, mean_q: 0.754284
 81234/100000: episode: 1175, duration: 0.238s, episode steps: 48, steps per second: 201, episode reward: 0.784, mean reward: 0.016 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.816, 10.100], loss: 0.000067, mae: 0.003521, mean_q: 0.754639
 81282/100000: episode: 1176, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 0.813, mean reward: 0.017 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.214, 10.261], loss: 0.000073, mae: 0.003760, mean_q: 0.754505
 81330/100000: episode: 1177, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 1.015, mean reward: 0.021 [0.000, 1.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.391, 10.100], loss: 0.000087, mae: 0.004349, mean_q: 0.754626
 81378/100000: episode: 1178, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 0.753, mean reward: 0.016 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.035, 10.100], loss: 0.000112, mae: 0.003834, mean_q: 0.754801
 81426/100000: episode: 1179, duration: 0.253s, episode steps: 48, steps per second: 190, episode reward: 0.888, mean reward: 0.018 [0.000, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.298, 10.100], loss: 0.000066, mae: 0.002994, mean_q: 0.755161
 81474/100000: episode: 1180, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.763, mean reward: 0.016 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.356, 10.115], loss: 0.000041, mae: 0.002898, mean_q: 0.755067
 81522/100000: episode: 1181, duration: 0.260s, episode steps: 48, steps per second: 184, episode reward: 0.859, mean reward: 0.018 [0.000, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.251, 10.100], loss: 0.000075, mae: 0.003886, mean_q: 0.755526
 81570/100000: episode: 1182, duration: 0.260s, episode steps: 48, steps per second: 185, episode reward: 0.813, mean reward: 0.017 [0.000, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.035, 10.124], loss: 0.000083, mae: 0.004095, mean_q: 0.755536
 81618/100000: episode: 1183, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.801, mean reward: 0.017 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.265, 10.100], loss: 0.000074, mae: 0.004192, mean_q: 0.755473
 81666/100000: episode: 1184, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.762, mean reward: 0.016 [0.000, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.993, 10.100], loss: 0.000065, mae: 0.003141, mean_q: 0.755991
 81714/100000: episode: 1185, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.833, mean reward: 0.017 [0.000, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.185, 10.100], loss: 0.000085, mae: 0.004488, mean_q: 0.755720
 81762/100000: episode: 1186, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 0.849, mean reward: 0.018 [0.000, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.888, 10.100], loss: 0.000069, mae: 0.004201, mean_q: 0.755543
 81810/100000: episode: 1187, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 0.795, mean reward: 0.017 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.480, 10.100], loss: 0.000062, mae: 0.003629, mean_q: 0.755806
 81858/100000: episode: 1188, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.752, mean reward: 0.016 [0.000, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.388, 10.100], loss: 0.000061, mae: 0.004610, mean_q: 0.755720
 81906/100000: episode: 1189, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 0.764, mean reward: 0.016 [0.000, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.174, 10.256], loss: 0.000060, mae: 0.003161, mean_q: 0.756180
 81954/100000: episode: 1190, duration: 0.238s, episode steps: 48, steps per second: 202, episode reward: 0.847, mean reward: 0.018 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.807, 10.100], loss: 0.000050, mae: 0.004401, mean_q: 0.756185
 82002/100000: episode: 1191, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 0.976, mean reward: 0.020 [0.000, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.557, 10.100], loss: 0.000068, mae: 0.003515, mean_q: 0.756789
 82050/100000: episode: 1192, duration: 0.252s, episode steps: 48, steps per second: 191, episode reward: 0.864, mean reward: 0.018 [0.000, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.072, 10.100], loss: 0.000149, mae: 0.005845, mean_q: 0.757383
 82098/100000: episode: 1193, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.900, mean reward: 0.019 [0.000, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.804, 10.100], loss: 0.000085, mae: 0.004107, mean_q: 0.757936
 82146/100000: episode: 1194, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 0.756, mean reward: 0.016 [0.000, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.425, 10.100], loss: 0.000104, mae: 0.005144, mean_q: 0.758982
 82194/100000: episode: 1195, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.827, mean reward: 0.017 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.858, 10.100], loss: 0.000107, mae: 0.004679, mean_q: 0.759199
 82242/100000: episode: 1196, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 0.981, mean reward: 0.020 [0.000, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.511, 10.100], loss: 0.000084, mae: 0.003518, mean_q: 0.758418
 82290/100000: episode: 1197, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.852, mean reward: 0.018 [0.000, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.541, 10.100], loss: 0.000066, mae: 0.003741, mean_q: 0.758724
 82338/100000: episode: 1198, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.779, mean reward: 0.016 [0.000, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.884, 10.100], loss: 0.000064, mae: 0.004094, mean_q: 0.759706
 82386/100000: episode: 1199, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 0.738, mean reward: 0.015 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.863, 10.100], loss: 0.000053, mae: 0.003117, mean_q: 0.758907
 82434/100000: episode: 1200, duration: 0.244s, episode steps: 48, steps per second: 196, episode reward: 0.856, mean reward: 0.018 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.199, 10.100], loss: 0.000081, mae: 0.004847, mean_q: 0.759949
 82482/100000: episode: 1201, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.936, mean reward: 0.019 [0.000, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.716, 10.100], loss: 0.000090, mae: 0.003317, mean_q: 0.760555
 82530/100000: episode: 1202, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 0.926, mean reward: 0.019 [0.000, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.957, 10.100], loss: 0.000062, mae: 0.004298, mean_q: 0.760227
 82578/100000: episode: 1203, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.919, mean reward: 0.019 [0.000, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.370, 10.100], loss: 0.000117, mae: 0.007531, mean_q: 0.761916
 82626/100000: episode: 1204, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 0.790, mean reward: 0.016 [0.000, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.644, 10.100], loss: 0.000087, mae: 0.004250, mean_q: 0.761724
 82674/100000: episode: 1205, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 0.730, mean reward: 0.015 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.344, 10.100], loss: 0.000068, mae: 0.005687, mean_q: 0.761811
 82722/100000: episode: 1206, duration: 0.259s, episode steps: 48, steps per second: 185, episode reward: 0.732, mean reward: 0.015 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.922, 10.100], loss: 0.000060, mae: 0.003142, mean_q: 0.762775
 82770/100000: episode: 1207, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.719, mean reward: 0.015 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.016, 10.100], loss: 0.000067, mae: 0.004133, mean_q: 0.762106
 82818/100000: episode: 1208, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 0.766, mean reward: 0.016 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.965, 10.100], loss: 0.000091, mae: 0.003826, mean_q: 0.763137
 82866/100000: episode: 1209, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.842, mean reward: 0.018 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.746, 10.100], loss: 0.000057, mae: 0.003597, mean_q: 0.762874
 82914/100000: episode: 1210, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.825, mean reward: 0.017 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.254, 10.100], loss: 0.000096, mae: 0.004085, mean_q: 0.764268
 82962/100000: episode: 1211, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.797, mean reward: 0.017 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.978, 10.100], loss: 0.000102, mae: 0.005813, mean_q: 0.765661
 83010/100000: episode: 1212, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.767, mean reward: 0.016 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.235, 10.100], loss: 0.000095, mae: 0.006147, mean_q: 0.764450
 83058/100000: episode: 1213, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.786, mean reward: 0.016 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.620, 10.100], loss: 0.000051, mae: 0.003218, mean_q: 0.764406
 83106/100000: episode: 1214, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.798, mean reward: 0.017 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.405, 10.100], loss: 0.000091, mae: 0.004884, mean_q: 0.764977
 83154/100000: episode: 1215, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.680, mean reward: 0.014 [0.000, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.750, 10.100], loss: 0.000077, mae: 0.003716, mean_q: 0.765178
 83202/100000: episode: 1216, duration: 0.243s, episode steps: 48, steps per second: 198, episode reward: 0.729, mean reward: 0.015 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.792, 10.119], loss: 0.000070, mae: 0.003443, mean_q: 0.765612
 83250/100000: episode: 1217, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 0.802, mean reward: 0.017 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.499, 10.152], loss: 0.000121, mae: 0.006442, mean_q: 0.765668
 83298/100000: episode: 1218, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.847, mean reward: 0.018 [0.000, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.818, 10.100], loss: 0.000038, mae: 0.002571, mean_q: 0.766713
 83346/100000: episode: 1219, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 0.761, mean reward: 0.016 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.943, 10.100], loss: 0.000051, mae: 0.003964, mean_q: 0.766855
 83394/100000: episode: 1220, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.787, mean reward: 0.016 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.265, 10.100], loss: 0.000081, mae: 0.004019, mean_q: 0.767017
 83442/100000: episode: 1221, duration: 0.236s, episode steps: 48, steps per second: 203, episode reward: 0.774, mean reward: 0.016 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.685, 10.100], loss: 0.000058, mae: 0.003339, mean_q: 0.766681
 83490/100000: episode: 1222, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 0.734, mean reward: 0.015 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.756, 10.100], loss: 0.000036, mae: 0.002776, mean_q: 0.766807
 83538/100000: episode: 1223, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 0.714, mean reward: 0.015 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.179, 10.100], loss: 0.000061, mae: 0.003038, mean_q: 0.767486
 83586/100000: episode: 1224, duration: 0.243s, episode steps: 48, steps per second: 198, episode reward: 0.894, mean reward: 0.019 [0.000, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.621, 10.100], loss: 0.000051, mae: 0.003223, mean_q: 0.767606
 83634/100000: episode: 1225, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 0.802, mean reward: 0.017 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.277, 10.100], loss: 0.000049, mae: 0.003246, mean_q: 0.768232
[Info] 3-TH LEVEL FOUND: 0.8175555467605591, Considering 10/100 traces
 83682/100000: episode: 1226, duration: 4.634s, episode steps: 48, steps per second: 10, episode reward: 0.744, mean reward: 0.015 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.259, 10.130], loss: 0.000074, mae: 0.003455, mean_q: 0.767868
 83686/100000: episode: 1227, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.811, mean reward: 0.203 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.402, 10.100], loss: 0.000022, mae: 0.003343, mean_q: 0.766767
 83687/100000: episode: 1228, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.719, mean reward: 0.719 [0.719, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.455, 10.100], loss: 0.000004, mae: 0.002405, mean_q: 0.762153
 83691/100000: episode: 1229, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 0.842, mean reward: 0.210 [0.000, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.397, 10.100], loss: 0.000062, mae: 0.004998, mean_q: 0.770196
 83692/100000: episode: 1230, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.815, mean reward: 0.815 [0.815, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.422, 10.200], loss: 0.000198, mae: 0.004882, mean_q: 0.769977
 83695/100000: episode: 1231, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 0.714, mean reward: 0.238 [0.000, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.352, 10.100], loss: 0.000045, mae: 0.004915, mean_q: 0.765217
 83698/100000: episode: 1232, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 0.694, mean reward: 0.231 [0.000, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.324, 10.100], loss: 0.000180, mae: 0.003947, mean_q: 0.766462
 83701/100000: episode: 1233, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 0.770, mean reward: 0.257 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.366, 10.100], loss: 0.000030, mae: 0.002846, mean_q: 0.764832
 83703/100000: episode: 1234, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.719, mean reward: 0.359 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.404, 10.100], loss: 0.000369, mae: 0.007920, mean_q: 0.771454
 83704/100000: episode: 1235, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.727, mean reward: 0.727 [0.727, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.454, 10.100], loss: 0.000016, mae: 0.002539, mean_q: 0.767089
 83709/100000: episode: 1236, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 0.796, mean reward: 0.159 [0.000, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.341, 10.100], loss: 0.000028, mae: 0.002763, mean_q: 0.766729
 83713/100000: episode: 1237, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 0.801, mean reward: 0.200 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.432, 10.100], loss: 0.000006, mae: 0.001961, mean_q: 0.772754
 83714/100000: episode: 1238, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.774, mean reward: 0.774 [0.774, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.385, 10.200], loss: 0.000001, mae: 0.001215, mean_q: 0.765501
 83715/100000: episode: 1239, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.703, mean reward: 0.703 [0.703, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.429, 10.100], loss: 0.000301, mae: 0.007904, mean_q: 0.766748
 83719/100000: episode: 1240, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 0.843, mean reward: 0.211 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.351, 10.100], loss: 0.000068, mae: 0.002535, mean_q: 0.766774
 83720/100000: episode: 1241, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.794, mean reward: 0.794 [0.794, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.382, 10.200], loss: 0.000002, mae: 0.001758, mean_q: 0.765774
 83722/100000: episode: 1242, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.880, mean reward: 0.440 [0.000, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.418, 10.100], loss: 0.000029, mae: 0.002645, mean_q: 0.768457
 83723/100000: episode: 1243, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.795, mean reward: 0.795 [0.795, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.419, 10.200], loss: 0.000002, mae: 0.001687, mean_q: 0.767509
 83726/100000: episode: 1244, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.746, mean reward: 0.249 [0.000, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.340, 10.100], loss: 0.000069, mae: 0.003753, mean_q: 0.769085
 83731/100000: episode: 1245, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 0.770, mean reward: 0.154 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.357, 10.100], loss: 0.000058, mae: 0.003851, mean_q: 0.770772
 83736/100000: episode: 1246, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 0.873, mean reward: 0.175 [0.000, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.381, 10.100], loss: 0.000040, mae: 0.003646, mean_q: 0.770967
 83741/100000: episode: 1247, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 0.836, mean reward: 0.167 [0.000, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.457, 10.100], loss: 0.000148, mae: 0.006063, mean_q: 0.769858
 83744/100000: episode: 1248, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 0.695, mean reward: 0.232 [0.000, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.354, 10.100], loss: 0.000241, mae: 0.005907, mean_q: 0.767290
 83746/100000: episode: 1249, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.822, mean reward: 0.411 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.381, 10.100], loss: 0.000144, mae: 0.003885, mean_q: 0.770310
 83749/100000: episode: 1250, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 0.730, mean reward: 0.243 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.358, 10.100], loss: 0.000159, mae: 0.004865, mean_q: 0.771395
 83753/100000: episode: 1251, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 0.844, mean reward: 0.211 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.464, 10.100], loss: 0.000007, mae: 0.001697, mean_q: 0.767371
 83755/100000: episode: 1252, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.763, mean reward: 0.382 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.408, 10.100], loss: 0.000015, mae: 0.002656, mean_q: 0.769012
 83756/100000: episode: 1253, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.829, mean reward: 0.829 [0.829, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.365, 10.200], loss: 0.000002, mae: 0.001485, mean_q: 0.770539
 83758/100000: episode: 1254, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.825, mean reward: 0.412 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.391, 10.100], loss: 0.000002, mae: 0.001458, mean_q: 0.774168
 83760/100000: episode: 1255, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.736, mean reward: 0.368 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.370, 10.100], loss: 0.000001, mae: 0.001082, mean_q: 0.764295
 83762/100000: episode: 1256, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.799, mean reward: 0.399 [0.000, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.364, 10.100], loss: 0.000073, mae: 0.004066, mean_q: 0.769580
 83765/100000: episode: 1257, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.715, mean reward: 0.238 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.289, 10.100], loss: 0.000029, mae: 0.004368, mean_q: 0.767689
 83766/100000: episode: 1258, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.755, mean reward: 0.755 [0.755, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.443, 10.200], loss: 0.000137, mae: 0.005874, mean_q: 0.770330
 83768/100000: episode: 1259, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.911, mean reward: 0.455 [0.000, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.415, 10.100], loss: 0.000002, mae: 0.001729, mean_q: 0.772012
 83774/100000: episode: 1260, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 0.871, mean reward: 0.145 [0.000, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.431, 10.100], loss: 0.000059, mae: 0.002882, mean_q: 0.769349
 83776/100000: episode: 1261, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.750, mean reward: 0.375 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.037, 10.100], loss: 0.000015, mae: 0.003072, mean_q: 0.768188
 83777/100000: episode: 1262, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.805, mean reward: 0.805 [0.805, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.393, 10.200], loss: 0.000007, mae: 0.001903, mean_q: 0.770973
 83783/100000: episode: 1263, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 0.878, mean reward: 0.146 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.579, 10.100], loss: 0.000039, mae: 0.002927, mean_q: 0.770203
 83787/100000: episode: 1264, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 0.816, mean reward: 0.204 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.406, 10.100], loss: 0.000073, mae: 0.005646, mean_q: 0.775509
 83788/100000: episode: 1265, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.772, mean reward: 0.772 [0.772, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.372, 10.200], loss: 0.000049, mae: 0.002975, mean_q: 0.773134
 83789/100000: episode: 1266, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.724, mean reward: 0.724 [0.724, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.453, 10.100], loss: 0.000019, mae: 0.005947, mean_q: 0.763241
 83790/100000: episode: 1267, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.761, mean reward: 0.761 [0.761, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.348, 10.200], loss: 0.000058, mae: 0.005387, mean_q: 0.770232
 83796/100000: episode: 1268, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 0.888, mean reward: 0.148 [0.000, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.443, 10.100], loss: 0.000116, mae: 0.004959, mean_q: 0.773452
 83797/100000: episode: 1269, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.774, mean reward: 0.774 [0.774, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.379, 10.200], loss: 0.000003, mae: 0.002085, mean_q: 0.756965
 83802/100000: episode: 1270, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 0.808, mean reward: 0.162 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.424, 10.100], loss: 0.000027, mae: 0.002551, mean_q: 0.769403
 83808/100000: episode: 1271, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 0.851, mean reward: 0.142 [0.000, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.522, 10.100], loss: 0.000074, mae: 0.004136, mean_q: 0.770401
 83810/100000: episode: 1272, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.806, mean reward: 0.403 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.368, 10.100], loss: 0.000054, mae: 0.003817, mean_q: 0.773855
 83815/100000: episode: 1273, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 0.806, mean reward: 0.161 [0.000, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.466, 10.100], loss: 0.000047, mae: 0.003630, mean_q: 0.771270
 83817/100000: episode: 1274, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.884, mean reward: 0.442 [0.000, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.406, 10.100], loss: 0.000099, mae: 0.002922, mean_q: 0.770559
 83819/100000: episode: 1275, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.841, mean reward: 0.420 [0.000, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.405, 10.100], loss: 0.000124, mae: 0.006269, mean_q: 0.767734
 83822/100000: episode: 1276, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 0.660, mean reward: 0.220 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.428, 10.100], loss: 0.000006, mae: 0.002491, mean_q: 0.774229
 83823/100000: episode: 1277, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.787, mean reward: 0.787 [0.787, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.399, 10.200], loss: 0.000088, mae: 0.004902, mean_q: 0.783966
 83826/100000: episode: 1278, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.699, mean reward: 0.233 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.543, 10.100], loss: 0.000135, mae: 0.005580, mean_q: 0.771806
 83827/100000: episode: 1279, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.818, mean reward: 0.818 [0.818, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.442, 10.200], loss: 0.000045, mae: 0.003768, mean_q: 0.767070
 83828/100000: episode: 1280, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.772, mean reward: 0.772 [0.772, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.358, 10.200], loss: 0.000006, mae: 0.001710, mean_q: 0.769205
 83829/100000: episode: 1281, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.766, mean reward: 0.766 [0.766, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.442, 10.200], loss: 0.000001, mae: 0.001111, mean_q: 0.767625
 83831/100000: episode: 1282, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.820, mean reward: 0.410 [0.000, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.399, 10.100], loss: 0.000004, mae: 0.001660, mean_q: 0.770579
 83833/100000: episode: 1283, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.767, mean reward: 0.384 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.406, 10.100], loss: 0.000073, mae: 0.003842, mean_q: 0.770349
 83834/100000: episode: 1284, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.805, mean reward: 0.805 [0.805, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.436, 10.200], loss: 0.000000, mae: 0.000828, mean_q: 0.768424
 83835/100000: episode: 1285, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.790, mean reward: 0.790 [0.790, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.402, 10.200], loss: 0.000005, mae: 0.001723, mean_q: 0.771348
 83841/100000: episode: 1286, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 0.886, mean reward: 0.148 [0.000, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.469, 10.100], loss: 0.000075, mae: 0.003480, mean_q: 0.773469
 83845/100000: episode: 1287, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 0.809, mean reward: 0.202 [0.000, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.540, 10.100], loss: 0.000015, mae: 0.002179, mean_q: 0.769343
 83850/100000: episode: 1288, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 0.786, mean reward: 0.157 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.395, 10.100], loss: 0.000045, mae: 0.003835, mean_q: 0.769375
 83855/100000: episode: 1289, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 0.793, mean reward: 0.159 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.365, 10.100], loss: 0.000114, mae: 0.005894, mean_q: 0.777375
 83856/100000: episode: 1290, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.757, mean reward: 0.757 [0.757, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.456, 10.100], loss: 0.000099, mae: 0.007055, mean_q: 0.769825
 83857/100000: episode: 1291, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.762, mean reward: 0.762 [0.762, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.409, 10.200], loss: 0.000025, mae: 0.005891, mean_q: 0.770278
 83858/100000: episode: 1292, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.835, mean reward: 0.835 [0.835, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.442, 10.200], loss: 0.000002, mae: 0.001646, mean_q: 0.765457
 83860/100000: episode: 1293, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.758, mean reward: 0.379 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.426, 10.100], loss: 0.000011, mae: 0.004412, mean_q: 0.773832
 83862/100000: episode: 1294, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.798, mean reward: 0.399 [0.000, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.383, 10.100], loss: 0.000057, mae: 0.003344, mean_q: 0.771401
 83863/100000: episode: 1295, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.719, mean reward: 0.719 [0.719, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.445, 10.100], loss: 0.000274, mae: 0.011626, mean_q: 0.763794
 83866/100000: episode: 1296, duration: 0.023s, episode steps: 3, steps per second: 131, episode reward: 0.745, mean reward: 0.248 [0.000, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.477, 10.100], loss: 0.000070, mae: 0.005367, mean_q: 0.774987
 83869/100000: episode: 1297, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 0.637, mean reward: 0.212 [0.000, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.408, 10.100], loss: 0.000065, mae: 0.005594, mean_q: 0.772465
 83871/100000: episode: 1298, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.783, mean reward: 0.392 [0.000, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.384, 10.100], loss: 0.000246, mae: 0.010916, mean_q: 0.763364
 83873/100000: episode: 1299, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.731, mean reward: 0.365 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.375, 10.100], loss: 0.000099, mae: 0.006835, mean_q: 0.775942
 83876/100000: episode: 1300, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.707, mean reward: 0.236 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.421, 10.100], loss: 0.000046, mae: 0.005448, mean_q: 0.774411
 83879/100000: episode: 1301, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.723, mean reward: 0.241 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.456, 10.100], loss: 0.000120, mae: 0.006382, mean_q: 0.768087
 83880/100000: episode: 1302, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.785, mean reward: 0.785 [0.785, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.386, 10.200], loss: 0.000014, mae: 0.003771, mean_q: 0.776553
 83881/100000: episode: 1303, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.833, mean reward: 0.833 [0.833, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.361, 10.200], loss: 0.000048, mae: 0.005017, mean_q: 0.776014
 83882/100000: episode: 1304, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.786, mean reward: 0.786 [0.786, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.323, 10.200], loss: 0.000002, mae: 0.001688, mean_q: 0.766678
 83883/100000: episode: 1305, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.724, mean reward: 0.724 [0.724, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.438, 10.100], loss: 0.000001, mae: 0.001263, mean_q: 0.774187
 83884/100000: episode: 1306, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.807, mean reward: 0.807 [0.807, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.410, 10.200], loss: 0.000002, mae: 0.001381, mean_q: 0.767309
 83889/100000: episode: 1307, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 0.906, mean reward: 0.181 [0.000, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.489, 10.100], loss: 0.000128, mae: 0.005075, mean_q: 0.770457
 83894/100000: episode: 1308, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 0.788, mean reward: 0.158 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.365, 10.100], loss: 0.000033, mae: 0.002495, mean_q: 0.770194
 83897/100000: episode: 1309, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 0.734, mean reward: 0.245 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.364, 10.100], loss: 0.000006, mae: 0.001896, mean_q: 0.772465
 83902/100000: episode: 1310, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 0.800, mean reward: 0.160 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.409, 10.100], loss: 0.000074, mae: 0.004516, mean_q: 0.770832
 83903/100000: episode: 1311, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.805, mean reward: 0.805 [0.805, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.472, 10.200], loss: 0.000092, mae: 0.004963, mean_q: 0.770655
 83909/100000: episode: 1312, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 1.029, mean reward: 0.171 [0.000, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.412, 10.100], loss: 0.000068, mae: 0.005014, mean_q: 0.774436
 83910/100000: episode: 1313, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.777, mean reward: 0.777 [0.777, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.394, 10.100], loss: 0.000024, mae: 0.006498, mean_q: 0.767183
 83914/100000: episode: 1314, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 0.805, mean reward: 0.201 [0.000, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.417, 10.100], loss: 0.000206, mae: 0.007545, mean_q: 0.781257
 83919/100000: episode: 1315, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 0.873, mean reward: 0.175 [0.000, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.299, 10.100], loss: 0.000101, mae: 0.005754, mean_q: 0.770950
[Info] 4-TH LEVEL FOUND: 0.8193186521530151, Considering 10/100 traces
 83924/100000: episode: 1316, duration: 4.472s, episode steps: 5, steps per second: 1, episode reward: 0.824, mean reward: 0.165 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.455, 10.100], loss: 0.000061, mae: 0.004521, mean_q: 0.773772
 83925/100000: episode: 1317, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.710, mean reward: 0.710 [0.710, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.486, 10.200], loss: 0.000023, mae: 0.003632, mean_q: 0.773144
 83928/100000: episode: 1318, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 0.883, mean reward: 0.294 [0.000, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.408, 10.100], loss: 0.000035, mae: 0.003707, mean_q: 0.775218
 83932/100000: episode: 1319, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 0.875, mean reward: 0.219 [0.000, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.503, 10.100], loss: 0.000122, mae: 0.004954, mean_q: 0.769467
 83934/100000: episode: 1320, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.843, mean reward: 0.422 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.236, 10.100], loss: 0.000023, mae: 0.004788, mean_q: 0.770219
 83937/100000: episode: 1321, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.934, mean reward: 0.311 [0.000, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.545, 10.100], loss: 0.000124, mae: 0.006864, mean_q: 0.775979
 83941/100000: episode: 1322, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 0.916, mean reward: 0.229 [0.000, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.589, 10.100], loss: 0.000188, mae: 0.009974, mean_q: 0.765048
 83943/100000: episode: 1323, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.879, mean reward: 0.440 [0.000, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.416, 10.100], loss: 0.000355, mae: 0.006584, mean_q: 0.771448
 83946/100000: episode: 1324, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 0.846, mean reward: 0.282 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.488, 10.100], loss: 0.000219, mae: 0.011244, mean_q: 0.783409
 83947/100000: episode: 1325, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.757, mean reward: 0.757 [0.757, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.521, 10.100], loss: 0.000018, mae: 0.005427, mean_q: 0.766411
 83950/100000: episode: 1326, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 0.882, mean reward: 0.294 [0.000, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.489, 10.100], loss: 0.000053, mae: 0.006937, mean_q: 0.766650
 83953/100000: episode: 1327, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 0.910, mean reward: 0.303 [0.000, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.580, 10.100], loss: 0.000071, mae: 0.006741, mean_q: 0.778403
 83956/100000: episode: 1328, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 0.922, mean reward: 0.307 [0.000, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.568, 10.100], loss: 0.000041, mae: 0.004134, mean_q: 0.773696
 83958/100000: episode: 1329, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.816, mean reward: 0.408 [0.000, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.512, 10.100], loss: 0.000118, mae: 0.006363, mean_q: 0.767146
 83959/100000: episode: 1330, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.726, mean reward: 0.726 [0.726, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.648, 10.200], loss: 0.000006, mae: 0.003199, mean_q: 0.776250
 83962/100000: episode: 1331, duration: 0.025s, episode steps: 3, steps per second: 118, episode reward: 0.830, mean reward: 0.277 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.609, 10.100], loss: 0.000062, mae: 0.004263, mean_q: 0.775487
 83963/100000: episode: 1332, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.765, mean reward: 0.765 [0.765, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.523, 10.100], loss: 0.000010, mae: 0.004213, mean_q: 0.769554
 83964/100000: episode: 1333, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.661, mean reward: 0.661 [0.661, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.503, 10.200], loss: 0.000003, mae: 0.002067, mean_q: 0.770316
 83966/100000: episode: 1334, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.872, mean reward: 0.436 [0.000, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.500, 10.100], loss: 0.000130, mae: 0.006800, mean_q: 0.780989
 83969/100000: episode: 1335, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 0.824, mean reward: 0.275 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.491, 10.100], loss: 0.000463, mae: 0.009951, mean_q: 0.776194
 83971/100000: episode: 1336, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.853, mean reward: 0.427 [0.000, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.559, 10.100], loss: 0.000078, mae: 0.010825, mean_q: 0.760142
 83975/100000: episode: 1337, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 0.863, mean reward: 0.216 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.479, 10.100], loss: 0.000177, mae: 0.005853, mean_q: 0.772364
 83976/100000: episode: 1338, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.732, mean reward: 0.732 [0.732, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.523, 10.200], loss: 0.000165, mae: 0.007104, mean_q: 0.770735
 83980/100000: episode: 1339, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 0.915, mean reward: 0.229 [0.000, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.608, 10.100], loss: 0.000034, mae: 0.004183, mean_q: 0.771256
 83981/100000: episode: 1340, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.774, mean reward: 0.774 [0.774, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.511, 10.100], loss: 0.000009, mae: 0.003306, mean_q: 0.774244
 83983/100000: episode: 1341, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.765, mean reward: 0.383 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.489, 10.100], loss: 0.000207, mae: 0.007584, mean_q: 0.774060
 83987/100000: episode: 1342, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 0.854, mean reward: 0.214 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.473, 10.100], loss: 0.000033, mae: 0.003858, mean_q: 0.774152
 83988/100000: episode: 1343, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.767, mean reward: 0.767 [0.767, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.519, 10.100], loss: 0.000003, mae: 0.002229, mean_q: 0.771255
 83990/100000: episode: 1344, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.889, mean reward: 0.445 [0.000, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.487, 10.100], loss: 0.000107, mae: 0.005292, mean_q: 0.768114
 83992/100000: episode: 1345, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.844, mean reward: 0.422 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.475, 10.100], loss: 0.000060, mae: 0.005042, mean_q: 0.769076
 83996/100000: episode: 1346, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.869, mean reward: 0.217 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.478, 10.100], loss: 0.000185, mae: 0.007741, mean_q: 0.779028
 84000/100000: episode: 1347, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 0.863, mean reward: 0.216 [0.000, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.446, 10.100], loss: 0.000135, mae: 0.007556, mean_q: 0.771570
 84004/100000: episode: 1348, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 0.974, mean reward: 0.243 [0.000, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.689, 10.100], loss: 0.000093, mae: 0.004839, mean_q: 0.775891
 84005/100000: episode: 1349, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.782, mean reward: 0.782 [0.782, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.499, 10.200], loss: 0.000001, mae: 0.001037, mean_q: 0.772519
 84007/100000: episode: 1350, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.895, mean reward: 0.447 [0.000, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.490, 10.100], loss: 0.000215, mae: 0.007286, mean_q: 0.774475
 84009/100000: episode: 1351, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.765, mean reward: 0.382 [0.000, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.543, 10.100], loss: 0.000117, mae: 0.007909, mean_q: 0.767941
 84013/100000: episode: 1352, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 0.891, mean reward: 0.223 [0.000, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.474, 10.100], loss: 0.000204, mae: 0.007240, mean_q: 0.776752
 84016/100000: episode: 1353, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 0.954, mean reward: 0.318 [0.000, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.582, 10.100], loss: 0.000074, mae: 0.005114, mean_q: 0.776836
 84019/100000: episode: 1354, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 0.939, mean reward: 0.313 [0.000, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.475, 10.100], loss: 0.000138, mae: 0.007662, mean_q: 0.770052
 84020/100000: episode: 1355, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.708, mean reward: 0.708 [0.708, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.603, 10.200], loss: 0.000289, mae: 0.009527, mean_q: 0.777679
 84021/100000: episode: 1356, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.758, mean reward: 0.758 [0.758, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.522, 10.100], loss: 0.000398, mae: 0.013970, mean_q: 0.780672
 84024/100000: episode: 1357, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.808, mean reward: 0.269 [0.000, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.523, 10.100], loss: 0.000175, mae: 0.006769, mean_q: 0.779082
 84027/100000: episode: 1358, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.924, mean reward: 0.308 [0.000, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.527, 10.100], loss: 0.000326, mae: 0.015612, mean_q: 0.763558
 84029/100000: episode: 1359, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.868, mean reward: 0.434 [0.000, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.576, 10.100], loss: 0.000118, mae: 0.006386, mean_q: 0.773926
 84030/100000: episode: 1360, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.768, mean reward: 0.768 [0.768, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.544, 10.200], loss: 0.000035, mae: 0.008059, mean_q: 0.784491
 84034/100000: episode: 1361, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 0.872, mean reward: 0.218 [0.000, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.445, 10.100], loss: 0.000089, mae: 0.007158, mean_q: 0.770661
 84037/100000: episode: 1362, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 0.971, mean reward: 0.324 [0.000, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.546, 10.100], loss: 0.000134, mae: 0.006471, mean_q: 0.775424
 84038/100000: episode: 1363, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.711, mean reward: 0.711 [0.711, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.582, 10.200], loss: 0.000081, mae: 0.007224, mean_q: 0.779867
 84040/100000: episode: 1364, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.879, mean reward: 0.440 [0.000, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.456, 10.100], loss: 0.000057, mae: 0.003544, mean_q: 0.773477
 84043/100000: episode: 1365, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.914, mean reward: 0.305 [0.000, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.503, 10.100], loss: 0.000380, mae: 0.008696, mean_q: 0.771365
 84045/100000: episode: 1366, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.850, mean reward: 0.425 [0.000, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.427, 10.100], loss: 0.000216, mae: 0.007329, mean_q: 0.777940
 84046/100000: episode: 1367, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.740, mean reward: 0.740 [0.740, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.616, 10.200], loss: 0.000069, mae: 0.003129, mean_q: 0.776629
 84048/100000: episode: 1368, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.909, mean reward: 0.455 [0.000, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.481, 10.100], loss: 0.000026, mae: 0.006870, mean_q: 0.766222
 84051/100000: episode: 1369, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 0.952, mean reward: 0.317 [0.000, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.833, 10.100], loss: 0.000108, mae: 0.005254, mean_q: 0.773831
 84052/100000: episode: 1370, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.720, mean reward: 0.720 [0.720, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.563, 10.200], loss: 0.000024, mae: 0.004533, mean_q: 0.776139
 84054/100000: episode: 1371, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.822, mean reward: 0.411 [0.000, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.597, 10.100], loss: 0.000207, mae: 0.004672, mean_q: 0.773617
 84056/100000: episode: 1372, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.872, mean reward: 0.436 [0.000, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.468, 10.100], loss: 0.000338, mae: 0.012081, mean_q: 0.770285
 84059/100000: episode: 1373, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 0.894, mean reward: 0.298 [0.000, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.542, 10.100], loss: 0.000147, mae: 0.004152, mean_q: 0.774696
 84061/100000: episode: 1374, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.857, mean reward: 0.429 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.529, 10.100], loss: 0.000070, mae: 0.004470, mean_q: 0.776703
 84064/100000: episode: 1375, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 0.929, mean reward: 0.310 [0.000, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.475, 10.100], loss: 0.000160, mae: 0.006191, mean_q: 0.776478
 84068/100000: episode: 1376, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.920, mean reward: 0.230 [0.000, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.542, 10.100], loss: 0.000167, mae: 0.004618, mean_q: 0.776568
 84070/100000: episode: 1377, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.891, mean reward: 0.446 [0.000, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.452, 10.100], loss: 0.000008, mae: 0.003619, mean_q: 0.772100
 84072/100000: episode: 1378, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.857, mean reward: 0.429 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.468, 10.100], loss: 0.000120, mae: 0.003846, mean_q: 0.773384
 84076/100000: episode: 1379, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 0.878, mean reward: 0.219 [0.000, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.603, 10.100], loss: 0.000194, mae: 0.006055, mean_q: 0.775635
 84080/100000: episode: 1380, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 0.858, mean reward: 0.214 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.516, 10.100], loss: 0.000081, mae: 0.003604, mean_q: 0.773009
 84084/100000: episode: 1381, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 0.802, mean reward: 0.201 [0.000, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.464, 10.100], loss: 0.000027, mae: 0.004812, mean_q: 0.767967
 84087/100000: episode: 1382, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 0.901, mean reward: 0.300 [0.000, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.527, 10.100], loss: 0.000044, mae: 0.003730, mean_q: 0.771823
 84088/100000: episode: 1383, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.748, mean reward: 0.748 [0.748, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.482, 10.200], loss: 0.000003, mae: 0.002066, mean_q: 0.771409
 84091/100000: episode: 1384, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.840, mean reward: 0.280 [0.000, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.579, 10.100], loss: 0.000175, mae: 0.005786, mean_q: 0.771682
 84093/100000: episode: 1385, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.862, mean reward: 0.431 [0.000, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.522, 10.100], loss: 0.000070, mae: 0.004727, mean_q: 0.776281
 84096/100000: episode: 1386, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 0.927, mean reward: 0.309 [0.000, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.547, 10.100], loss: 0.000093, mae: 0.008012, mean_q: 0.781458
 84100/100000: episode: 1387, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 0.839, mean reward: 0.210 [0.000, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.486, 10.100], loss: 0.000266, mae: 0.007854, mean_q: 0.782523
 84103/100000: episode: 1388, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 0.939, mean reward: 0.313 [0.000, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.543, 10.100], loss: 0.000117, mae: 0.004424, mean_q: 0.776750
 84105/100000: episode: 1389, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.831, mean reward: 0.415 [0.000, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.513, 10.100], loss: 0.000033, mae: 0.003290, mean_q: 0.775273
 84108/100000: episode: 1390, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.895, mean reward: 0.298 [0.000, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.530, 10.100], loss: 0.000068, mae: 0.004670, mean_q: 0.771717
 84109/100000: episode: 1391, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.710, mean reward: 0.710 [0.710, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.590, 10.200], loss: 0.000003, mae: 0.001950, mean_q: 0.780025
 84110/100000: episode: 1392, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.532, 10.200], loss: 0.000005, mae: 0.002641, mean_q: 0.776793
 84112/100000: episode: 1393, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.856, mean reward: 0.428 [0.000, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.543, 10.100], loss: 0.000218, mae: 0.005215, mean_q: 0.770351
 84116/100000: episode: 1394, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 0.885, mean reward: 0.221 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.579, 10.100], loss: 0.000098, mae: 0.007517, mean_q: 0.770526
 84118/100000: episode: 1395, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.795, mean reward: 0.397 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.544, 10.100], loss: 0.000041, mae: 0.003906, mean_q: 0.775374
 84120/100000: episode: 1396, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.763, mean reward: 0.381 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.594, 10.100], loss: 0.000111, mae: 0.005130, mean_q: 0.776816
 84123/100000: episode: 1397, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.881, mean reward: 0.294 [0.000, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.243, 10.100], loss: 0.000167, mae: 0.004604, mean_q: 0.775534
 84125/100000: episode: 1398, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.854, mean reward: 0.427 [0.000, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.502, 10.100], loss: 0.000100, mae: 0.006168, mean_q: 0.774229
 84127/100000: episode: 1399, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.829, mean reward: 0.414 [0.000, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.575, 10.100], loss: 0.000025, mae: 0.002890, mean_q: 0.774723
 84129/100000: episode: 1400, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.819, mean reward: 0.409 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.547, 10.100], loss: 0.000040, mae: 0.003919, mean_q: 0.771609
 84133/100000: episode: 1401, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 0.885, mean reward: 0.221 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.546, 10.100], loss: 0.000352, mae: 0.008964, mean_q: 0.776869
 84137/100000: episode: 1402, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 0.872, mean reward: 0.218 [0.000, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.560, 10.100], loss: 0.000060, mae: 0.004457, mean_q: 0.777754
 84140/100000: episode: 1403, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 0.825, mean reward: 0.275 [0.000, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.518, 10.100], loss: 0.000045, mae: 0.006926, mean_q: 0.769053
 84142/100000: episode: 1404, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.869, mean reward: 0.434 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.576, 10.100], loss: 0.000010, mae: 0.003419, mean_q: 0.778135
 84143/100000: episode: 1405, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.747, mean reward: 0.747 [0.747, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.507, 10.100], loss: 0.000378, mae: 0.013681, mean_q: 0.789557
[Info] 5-TH LEVEL FOUND: 0.8288923501968384, Considering 10/100 traces
 84145/100000: episode: 1406, duration: 4.544s, episode steps: 2, steps per second: 0, episode reward: 0.787, mean reward: 0.394 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.552, 10.100], loss: 0.000404, mae: 0.011956, mean_q: 0.784912
[Info] FALSIFICATION!
 84146/100000: episode: 1407, duration: 0.378s, episode steps: 1, steps per second: 3, episode reward: 1.025, mean reward: 1.025 [1.025, 1.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.607, 10.100], loss: 0.000003, mae: 0.002300, mean_q: 0.771808
 84147/100000: episode: 1408, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.950, mean reward: 0.950 [0.950, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.578, 10.100], loss: 0.000020, mae: 0.006006, mean_q: 0.772829
 84148/100000: episode: 1409, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.917, mean reward: 0.917 [0.917, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.632, 10.100], loss: 0.000053, mae: 0.006808, mean_q: 0.771237
 84149/100000: episode: 1410, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.866, mean reward: 0.866 [0.866, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.596, 10.100], loss: 0.000008, mae: 0.003076, mean_q: 0.778127
 84150/100000: episode: 1411, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: 0.879, mean reward: 0.879 [0.879, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.613, 10.200], loss: 0.000030, mae: 0.007208, mean_q: 0.780840
 84151/100000: episode: 1412, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.895, mean reward: 0.895 [0.895, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.627, 10.100], loss: 0.000066, mae: 0.005762, mean_q: 0.783207
 84152/100000: episode: 1413, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.930, mean reward: 0.930 [0.930, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.642, 10.100], loss: 0.000094, mae: 0.004828, mean_q: 0.773297
 84153/100000: episode: 1414, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.950, mean reward: 0.950 [0.950, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.585, 10.100], loss: 0.000063, mae: 0.005806, mean_q: 0.776815
 84154/100000: episode: 1415, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 0.878, mean reward: 0.878 [0.878, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.617, 10.100], loss: 0.000013, mae: 0.004860, mean_q: 0.771905
 84155/100000: episode: 1416, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.850, mean reward: 0.850 [0.850, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.601, 10.100], loss: 0.000257, mae: 0.008091, mean_q: 0.783444
 84156/100000: episode: 1417, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.902, mean reward: 0.902 [0.902, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.581, 10.100], loss: 0.000047, mae: 0.006665, mean_q: 0.783534
 84157/100000: episode: 1418, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.891, mean reward: 0.891 [0.891, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.534, 10.100], loss: 0.000014, mae: 0.004676, mean_q: 0.777104
 84158/100000: episode: 1419, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.880, mean reward: 0.880 [0.880, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.594, 10.100], loss: 0.000049, mae: 0.004411, mean_q: 0.779661
 84159/100000: episode: 1420, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.835, mean reward: 0.835 [0.835, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.538, 10.100], loss: 0.000024, mae: 0.004755, mean_q: 0.767635
 84160/100000: episode: 1421, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.823, mean reward: 0.823 [0.823, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.603, 10.200], loss: 0.000059, mae: 0.006633, mean_q: 0.778831
 84161/100000: episode: 1422, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.898, mean reward: 0.898 [0.898, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.607, 10.100], loss: 0.000029, mae: 0.003555, mean_q: 0.774191
 84162/100000: episode: 1423, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.864, mean reward: 0.864 [0.864, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.600, 10.100], loss: 0.000515, mae: 0.012926, mean_q: 0.788669
 84163/100000: episode: 1424, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.845, mean reward: 0.845 [0.845, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.631, 10.200], loss: 0.000104, mae: 0.005161, mean_q: 0.774443
 84164/100000: episode: 1425, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.897, mean reward: 0.897 [0.897, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.605, 10.100], loss: 0.000186, mae: 0.006379, mean_q: 0.775642
 84165/100000: episode: 1426, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.868, mean reward: 0.868 [0.868, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.638, 10.200], loss: 0.000010, mae: 0.003665, mean_q: 0.777895
 84166/100000: episode: 1427, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.889, mean reward: 0.889 [0.889, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.608, 10.200], loss: 0.000024, mae: 0.005063, mean_q: 0.771616
 84167/100000: episode: 1428, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.849, mean reward: 0.849 [0.849, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.602, 10.200], loss: 0.000292, mae: 0.006834, mean_q: 0.772187
 84168/100000: episode: 1429, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.935, mean reward: 0.935 [0.935, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.658, 10.100], loss: 0.000004, mae: 0.002300, mean_q: 0.784059
 84169/100000: episode: 1430, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.842, mean reward: 0.842 [0.842, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.584, 10.200], loss: 0.000066, mae: 0.005786, mean_q: 0.779163
 84170/100000: episode: 1431, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.888, mean reward: 0.888 [0.888, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.568, 10.100], loss: 0.000088, mae: 0.006566, mean_q: 0.783133
 84171/100000: episode: 1432, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.865, mean reward: 0.865 [0.865, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.594, 10.200], loss: 0.000071, mae: 0.005666, mean_q: 0.784651
 84172/100000: episode: 1433, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.860, mean reward: 0.860 [0.860, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.560, 10.100], loss: 0.000132, mae: 0.006921, mean_q: 0.776457
 84173/100000: episode: 1434, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.861, mean reward: 0.861 [0.861, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.677, 10.200], loss: 0.000267, mae: 0.007083, mean_q: 0.778088
 84174/100000: episode: 1435, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.885, mean reward: 0.885 [0.885, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.558, 10.100], loss: 0.000013, mae: 0.004740, mean_q: 0.781693
 84175/100000: episode: 1436, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.863, mean reward: 0.863 [0.863, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.598, 10.100], loss: 0.000047, mae: 0.006995, mean_q: 0.784034
 84176/100000: episode: 1437, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.859, mean reward: 0.859 [0.859, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.602, 10.100], loss: 0.000004, mae: 0.002228, mean_q: 0.780828
 84177/100000: episode: 1438, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.910, mean reward: 0.910 [0.910, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.607, 10.100], loss: 0.000446, mae: 0.013138, mean_q: 0.775762
 84178/100000: episode: 1439, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.871, mean reward: 0.871 [0.871, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.621, 10.200], loss: 0.000096, mae: 0.007775, mean_q: 0.775295
 84179/100000: episode: 1440, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.845, mean reward: 0.845 [0.845, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.568, 10.200], loss: 0.000429, mae: 0.010633, mean_q: 0.778155
 84180/100000: episode: 1441, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.876, mean reward: 0.876 [0.876, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.581, 10.100], loss: 0.000013, mae: 0.004581, mean_q: 0.775269
 84181/100000: episode: 1442, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.843, mean reward: 0.843 [0.843, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.590, 10.100], loss: 0.000155, mae: 0.008895, mean_q: 0.787031
 84182/100000: episode: 1443, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.777, mean reward: 0.777 [0.777, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.540, 10.200], loss: 0.000015, mae: 0.002430, mean_q: 0.773615
 84183/100000: episode: 1444, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.949, mean reward: 0.949 [0.949, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.581, 10.100], loss: 0.000500, mae: 0.015135, mean_q: 0.780268
 84184/100000: episode: 1445, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.833, mean reward: 0.833 [0.833, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.597, 10.200], loss: 0.000195, mae: 0.008267, mean_q: 0.773564
 84185/100000: episode: 1446, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.937, mean reward: 0.937 [0.937, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.618, 10.200], loss: 0.000879, mae: 0.016509, mean_q: 0.792185
 84186/100000: episode: 1447, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.909, mean reward: 0.909 [0.909, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.658, 10.100], loss: 0.000034, mae: 0.003350, mean_q: 0.771668
 84187/100000: episode: 1448, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.841, mean reward: 0.841 [0.841, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.714, 10.200], loss: 0.000003, mae: 0.001980, mean_q: 0.774907
 84188/100000: episode: 1449, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.932, mean reward: 0.932 [0.932, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.661, 10.100], loss: 0.000003, mae: 0.001584, mean_q: 0.770952
 84189/100000: episode: 1450, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.927, mean reward: 0.927 [0.927, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.631, 10.100], loss: 0.000069, mae: 0.006692, mean_q: 0.782669
 84190/100000: episode: 1451, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.956, mean reward: 0.956 [0.956, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.620, 10.100], loss: 0.000028, mae: 0.006464, mean_q: 0.786937
 84191/100000: episode: 1452, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.846, mean reward: 0.846 [0.846, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.576, 10.200], loss: 0.000595, mae: 0.011024, mean_q: 0.786309
 84192/100000: episode: 1453, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.959, mean reward: 0.959 [0.959, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.651, 10.100], loss: 0.000482, mae: 0.012370, mean_q: 0.775730
 84193/100000: episode: 1454, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.896, mean reward: 0.896 [0.896, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.611, 10.100], loss: 0.000128, mae: 0.004809, mean_q: 0.774060
 84194/100000: episode: 1455, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.829, mean reward: 0.829 [0.829, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.638, 10.200], loss: 0.000087, mae: 0.007273, mean_q: 0.791080
 84195/100000: episode: 1456, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.833, mean reward: 0.833 [0.833, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.627, 10.200], loss: 0.000068, mae: 0.007156, mean_q: 0.790151
 84196/100000: episode: 1457, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.857, mean reward: 0.857 [0.857, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.595, 10.200], loss: 0.000109, mae: 0.006124, mean_q: 0.777801
 84197/100000: episode: 1458, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.879, mean reward: 0.879 [0.879, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.673, 10.100], loss: 0.000669, mae: 0.015562, mean_q: 0.770392
 84198/100000: episode: 1459, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.956, mean reward: 0.956 [0.956, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.591, 10.100], loss: 0.000057, mae: 0.003617, mean_q: 0.768189
 84199/100000: episode: 1460, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.830, mean reward: 0.830 [0.830, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.592, 10.100], loss: 0.000132, mae: 0.007896, mean_q: 0.786056
 84200/100000: episode: 1461, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.866, mean reward: 0.866 [0.866, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.605, 10.200], loss: 0.000021, mae: 0.005998, mean_q: 0.783565
 84201/100000: episode: 1462, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.902, mean reward: 0.902 [0.902, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.612, 10.200], loss: 0.000153, mae: 0.005099, mean_q: 0.783561
 84202/100000: episode: 1463, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.864, mean reward: 0.864 [0.864, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.580, 10.200], loss: 0.000051, mae: 0.004733, mean_q: 0.782335
 84203/100000: episode: 1464, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.903, mean reward: 0.903 [0.903, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.623, 10.200], loss: 0.000019, mae: 0.005681, mean_q: 0.770892
 84204/100000: episode: 1465, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.861, mean reward: 0.861 [0.861, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.644, 10.100], loss: 0.000037, mae: 0.003977, mean_q: 0.772576
 84205/100000: episode: 1466, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.815, mean reward: 0.815 [0.815, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.572, 10.200], loss: 0.000232, mae: 0.008131, mean_q: 0.784603
 84206/100000: episode: 1467, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.841, mean reward: 0.841 [0.841, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.550, 10.100], loss: 0.000010, mae: 0.004285, mean_q: 0.780940
 84207/100000: episode: 1468, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.947, mean reward: 0.947 [0.947, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.605, 10.100], loss: 0.000150, mae: 0.006418, mean_q: 0.783382
 84208/100000: episode: 1469, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.875, mean reward: 0.875 [0.875, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.609, 10.200], loss: 0.000253, mae: 0.006776, mean_q: 0.783678
 84209/100000: episode: 1470, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.839, mean reward: 0.839 [0.839, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.611, 10.200], loss: 0.000100, mae: 0.005728, mean_q: 0.776018
 84210/100000: episode: 1471, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.832, mean reward: 0.832 [0.832, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.628, 10.200], loss: 0.000291, mae: 0.009898, mean_q: 0.781046
 84211/100000: episode: 1472, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.943, mean reward: 0.943 [0.943, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.675, 10.100], loss: 0.000109, mae: 0.005223, mean_q: 0.780784
 84212/100000: episode: 1473, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.824, mean reward: 0.824 [0.824, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.583, 10.100], loss: 0.000784, mae: 0.016390, mean_q: 0.794502
 84213/100000: episode: 1474, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.951, mean reward: 0.951 [0.951, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.639, 10.100], loss: 0.000266, mae: 0.009127, mean_q: 0.789352
 84214/100000: episode: 1475, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.852, mean reward: 0.852 [0.852, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.547, 10.200], loss: 0.000129, mae: 0.008562, mean_q: 0.767041
 84215/100000: episode: 1476, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.820, mean reward: 0.820 [0.820, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.667, 10.200], loss: 0.000110, mae: 0.010735, mean_q: 0.774225
 84216/100000: episode: 1477, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.841, mean reward: 0.841 [0.841, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.507, 10.100], loss: 0.000137, mae: 0.009595, mean_q: 0.773740
 84217/100000: episode: 1478, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.947, mean reward: 0.947 [0.947, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.639, 10.100], loss: 0.000193, mae: 0.005490, mean_q: 0.787502
 84218/100000: episode: 1479, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.942, mean reward: 0.942 [0.942, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.586, 10.100], loss: 0.000051, mae: 0.008170, mean_q: 0.789624
 84219/100000: episode: 1480, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.843, mean reward: 0.843 [0.843, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.591, 10.200], loss: 0.000151, mae: 0.008437, mean_q: 0.783415
 84220/100000: episode: 1481, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.825, mean reward: 0.825 [0.825, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.562, 10.200], loss: 0.000356, mae: 0.011557, mean_q: 0.786563
 84221/100000: episode: 1482, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.871, mean reward: 0.871 [0.871, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.568, 10.100], loss: 0.000063, mae: 0.007214, mean_q: 0.768192
 84222/100000: episode: 1483, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.944, mean reward: 0.944 [0.944, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.574, 10.100], loss: 0.000114, mae: 0.008906, mean_q: 0.769608
 84223/100000: episode: 1484, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.890, mean reward: 0.890 [0.890, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.598, 10.100], loss: 0.000138, mae: 0.004890, mean_q: 0.777770
 84224/100000: episode: 1485, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.856, mean reward: 0.856 [0.856, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.618, 10.100], loss: 0.000090, mae: 0.004830, mean_q: 0.776019
 84225/100000: episode: 1486, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.939, mean reward: 0.939 [0.939, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.593, 10.100], loss: 0.000003, mae: 0.002364, mean_q: 0.768328
 84226/100000: episode: 1487, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.877, mean reward: 0.877 [0.877, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.615, 10.200], loss: 0.000340, mae: 0.006885, mean_q: 0.771708
 84227/100000: episode: 1488, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.805, mean reward: 0.805 [0.805, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.584, 10.200], loss: 0.000337, mae: 0.010832, mean_q: 0.781375
 84228/100000: episode: 1489, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.891, mean reward: 0.891 [0.891, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.567, 10.100], loss: 0.000059, mae: 0.004733, mean_q: 0.777411
 84229/100000: episode: 1490, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.853, mean reward: 0.853 [0.853, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.607, 10.200], loss: 0.000003, mae: 0.002006, mean_q: 0.772193
 84230/100000: episode: 1491, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.898, mean reward: 0.898 [0.898, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.573, 10.100], loss: 0.000108, mae: 0.006705, mean_q: 0.780452
 84231/100000: episode: 1492, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.827, mean reward: 0.827 [0.827, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.621, 10.200], loss: 0.000728, mae: 0.015225, mean_q: 0.781009
 84232/100000: episode: 1493, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.866, mean reward: 0.866 [0.866, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.582, 10.100], loss: 0.000023, mae: 0.003027, mean_q: 0.782063
 84233/100000: episode: 1494, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.881, mean reward: 0.881 [0.881, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.642, 10.100], loss: 0.000134, mae: 0.007504, mean_q: 0.783942
 84234/100000: episode: 1495, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.875, mean reward: 0.875 [0.875, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.615, 10.200], loss: 0.000166, mae: 0.005334, mean_q: 0.777494
[Info] Complete ISplit Iteration
[Info] Levels: [0.7583468, 0.7612953, 0.81755555, 0.81931865, 0.82889235, 0.83276314]
[Info] Cond. Prob: [0.1, 0.13, 0.1, 0.1, 0.1, 0.01]
[Info] Error Prob: 1.3000000000000003e-07

 84235/100000: episode: 1496, duration: 4.764s, episode steps: 1, steps per second: 0, episode reward: 0.849, mean reward: 0.849 [0.849, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.579, 10.200], loss: 0.000424, mae: 0.011889, mean_q: 0.778874
 84336/100000: episode: 1497, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.712, mean reward: 0.007 [0.000, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.173, 10.100], loss: 0.000184, mae: 0.007006, mean_q: 0.780934
 84437/100000: episode: 1498, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.758, mean reward: 0.008 [0.000, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.290, 10.100], loss: 0.000163, mae: 0.006767, mean_q: 0.781980
 84538/100000: episode: 1499, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.246, 10.100], loss: 0.000180, mae: 0.007673, mean_q: 0.784975
 84639/100000: episode: 1500, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.865, mean reward: 0.009 [0.000, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.843, 10.100], loss: 0.000153, mae: 0.006220, mean_q: 0.784211
 84740/100000: episode: 1501, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.748, mean reward: 0.007 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.100], loss: 0.000200, mae: 0.009415, mean_q: 0.784268
 84841/100000: episode: 1502, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.507, 10.100], loss: 0.000162, mae: 0.007016, mean_q: 0.784095
 84942/100000: episode: 1503, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.091, 10.268], loss: 0.000190, mae: 0.007258, mean_q: 0.783553
 85043/100000: episode: 1504, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.706, mean reward: 0.007 [0.000, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.858, 10.100], loss: 0.000127, mae: 0.005387, mean_q: 0.782681
 85144/100000: episode: 1505, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.384, 10.100], loss: 0.000181, mae: 0.007219, mean_q: 0.783554
 85245/100000: episode: 1506, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.656, 10.195], loss: 0.000175, mae: 0.007563, mean_q: 0.785159
 85346/100000: episode: 1507, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.638, 10.100], loss: 0.000181, mae: 0.007359, mean_q: 0.785537
 85447/100000: episode: 1508, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.739, mean reward: 0.007 [0.000, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.547, 10.107], loss: 0.000164, mae: 0.007468, mean_q: 0.783118
 85548/100000: episode: 1509, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.858, mean reward: 0.008 [0.000, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.067, 10.375], loss: 0.000174, mae: 0.007558, mean_q: 0.783745
 85649/100000: episode: 1510, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.811, mean reward: 0.008 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.729, 10.169], loss: 0.000168, mae: 0.007085, mean_q: 0.784307
 85750/100000: episode: 1511, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.544, 10.100], loss: 0.000149, mae: 0.006600, mean_q: 0.783993
 85851/100000: episode: 1512, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.866, mean reward: 0.009 [0.000, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.826, 10.520], loss: 0.000132, mae: 0.006463, mean_q: 0.783747
 85952/100000: episode: 1513, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.715, mean reward: 0.007 [0.000, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.124, 10.407], loss: 0.000164, mae: 0.007987, mean_q: 0.784536
 86053/100000: episode: 1514, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.397, 10.100], loss: 0.000152, mae: 0.007019, mean_q: 0.783066
 86154/100000: episode: 1515, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.911, 10.100], loss: 0.000139, mae: 0.006007, mean_q: 0.783172
 86255/100000: episode: 1516, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.707, mean reward: 0.007 [0.000, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.275, 10.124], loss: 0.000195, mae: 0.008166, mean_q: 0.784354
 86356/100000: episode: 1517, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.708, mean reward: 0.007 [0.000, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.387, 10.299], loss: 0.000163, mae: 0.007559, mean_q: 0.783364
 86457/100000: episode: 1518, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.885, mean reward: 0.009 [0.000, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.354, 10.433], loss: 0.000167, mae: 0.007330, mean_q: 0.783487
 86558/100000: episode: 1519, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.846, mean reward: 0.008 [0.000, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.100], loss: 0.000129, mae: 0.005853, mean_q: 0.781833
 86659/100000: episode: 1520, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.699, mean reward: 0.007 [0.000, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.528, 10.150], loss: 0.000155, mae: 0.006770, mean_q: 0.782833
 86760/100000: episode: 1521, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.239], loss: 0.000140, mae: 0.006869, mean_q: 0.781781
 86861/100000: episode: 1522, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.671, mean reward: 0.007 [0.000, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.718, 10.100], loss: 0.000155, mae: 0.006899, mean_q: 0.782999
 86962/100000: episode: 1523, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.684, mean reward: 0.007 [0.000, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.005, 10.154], loss: 0.000161, mae: 0.006981, mean_q: 0.781804
 87063/100000: episode: 1524, duration: 0.540s, episode steps: 101, steps per second: 187, episode reward: 0.719, mean reward: 0.007 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.223, 10.172], loss: 0.000168, mae: 0.006424, mean_q: 0.781513
 87164/100000: episode: 1525, duration: 0.529s, episode steps: 101, steps per second: 191, episode reward: 0.869, mean reward: 0.009 [0.000, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.673, 10.247], loss: 0.000133, mae: 0.005782, mean_q: 0.779815
 87265/100000: episode: 1526, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.830, mean reward: 0.008 [0.000, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.129, 10.125], loss: 0.000151, mae: 0.006677, mean_q: 0.778959
 87366/100000: episode: 1527, duration: 0.508s, episode steps: 101, steps per second: 199, episode reward: 0.793, mean reward: 0.008 [0.000, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.875, 10.195], loss: 0.000144, mae: 0.006932, mean_q: 0.777962
 87467/100000: episode: 1528, duration: 0.506s, episode steps: 101, steps per second: 199, episode reward: 0.732, mean reward: 0.007 [0.000, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.580, 10.100], loss: 0.000156, mae: 0.006851, mean_q: 0.778601
 87568/100000: episode: 1529, duration: 0.522s, episode steps: 101, steps per second: 193, episode reward: 0.815, mean reward: 0.008 [0.000, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.316, 10.100], loss: 0.000181, mae: 0.008308, mean_q: 0.777397
 87669/100000: episode: 1530, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.828, mean reward: 0.008 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.236, 10.100], loss: 0.000177, mae: 0.006720, mean_q: 0.777966
 87770/100000: episode: 1531, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.751, mean reward: 0.007 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.450, 10.100], loss: 0.000103, mae: 0.005661, mean_q: 0.777905
 87871/100000: episode: 1532, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.700, mean reward: 0.007 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.197, 10.212], loss: 0.000132, mae: 0.006587, mean_q: 0.775978
 87972/100000: episode: 1533, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.908, mean reward: 0.009 [0.000, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.249, 10.100], loss: 0.000178, mae: 0.006923, mean_q: 0.777034
 88073/100000: episode: 1534, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.824, mean reward: 0.008 [0.000, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.885, 10.100], loss: 0.000152, mae: 0.006259, mean_q: 0.777666
 88174/100000: episode: 1535, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.767, mean reward: 0.008 [0.000, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.965, 10.100], loss: 0.000150, mae: 0.006408, mean_q: 0.777024
 88275/100000: episode: 1536, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.729, mean reward: 0.007 [0.000, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.049, 10.158], loss: 0.000132, mae: 0.006652, mean_q: 0.776319
 88376/100000: episode: 1537, duration: 0.502s, episode steps: 101, steps per second: 201, episode reward: 0.711, mean reward: 0.007 [0.000, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.205, 10.100], loss: 0.000127, mae: 0.005489, mean_q: 0.775963
 88477/100000: episode: 1538, duration: 0.525s, episode steps: 101, steps per second: 193, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.406, 10.142], loss: 0.000147, mae: 0.005747, mean_q: 0.775963
 88578/100000: episode: 1539, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: 0.823, mean reward: 0.008 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.179, 10.100], loss: 0.000126, mae: 0.006139, mean_q: 0.775194
 88679/100000: episode: 1540, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.969, 10.106], loss: 0.000110, mae: 0.005336, mean_q: 0.774593
 88780/100000: episode: 1541, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.704, mean reward: 0.007 [0.000, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.804, 10.215], loss: 0.000109, mae: 0.005040, mean_q: 0.774393
 88881/100000: episode: 1542, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.874, mean reward: 0.009 [0.000, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.595, 10.100], loss: 0.000074, mae: 0.004594, mean_q: 0.772940
 88982/100000: episode: 1543, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.679, 10.316], loss: 0.000041, mae: 0.003389, mean_q: 0.771653
 89083/100000: episode: 1544, duration: 0.497s, episode steps: 101, steps per second: 203, episode reward: 0.735, mean reward: 0.007 [0.000, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.826, 10.313], loss: 0.000032, mae: 0.002880, mean_q: 0.770599
 89184/100000: episode: 1545, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.780, 10.239], loss: 0.000028, mae: 0.002761, mean_q: 0.769807
 89285/100000: episode: 1546, duration: 0.499s, episode steps: 101, steps per second: 202, episode reward: 0.832, mean reward: 0.008 [0.000, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.631, 10.100], loss: 0.000012, mae: 0.002033, mean_q: 0.769427
 89386/100000: episode: 1547, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.766, mean reward: 0.008 [0.000, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.643, 10.100], loss: 0.000025, mae: 0.002502, mean_q: 0.769224
 89487/100000: episode: 1548, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.932, 10.100], loss: 0.000012, mae: 0.002039, mean_q: 0.770066
 89588/100000: episode: 1549, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.923, 10.100], loss: 0.000023, mae: 0.002407, mean_q: 0.769499
 89689/100000: episode: 1550, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.056, 10.196], loss: 0.000012, mae: 0.002114, mean_q: 0.769013
 89790/100000: episode: 1551, duration: 0.522s, episode steps: 101, steps per second: 194, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.289, 10.100], loss: 0.000031, mae: 0.002737, mean_q: 0.769642
 89891/100000: episode: 1552, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.713, mean reward: 0.007 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.468, 10.171], loss: 0.000028, mae: 0.002281, mean_q: 0.769851
 89992/100000: episode: 1553, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.550, 10.100], loss: 0.000018, mae: 0.002618, mean_q: 0.770149
 90093/100000: episode: 1554, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.819, mean reward: 0.008 [0.000, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.717, 10.308], loss: 0.000016, mae: 0.002441, mean_q: 0.769740
 90194/100000: episode: 1555, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.512, 10.198], loss: 0.000023, mae: 0.002404, mean_q: 0.769691
 90295/100000: episode: 1556, duration: 0.514s, episode steps: 101, steps per second: 197, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.722, 10.174], loss: 0.000033, mae: 0.003059, mean_q: 0.770370
 90396/100000: episode: 1557, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.210, 10.100], loss: 0.000013, mae: 0.002159, mean_q: 0.769376
 90497/100000: episode: 1558, duration: 0.509s, episode steps: 101, steps per second: 199, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.754, 10.100], loss: 0.000021, mae: 0.002443, mean_q: 0.770380
 90598/100000: episode: 1559, duration: 0.516s, episode steps: 101, steps per second: 196, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.855, 10.100], loss: 0.000029, mae: 0.002828, mean_q: 0.770557
 90699/100000: episode: 1560, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.803, mean reward: 0.008 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.508, 10.100], loss: 0.000024, mae: 0.002672, mean_q: 0.770855
 90800/100000: episode: 1561, duration: 0.512s, episode steps: 101, steps per second: 197, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.516, 10.289], loss: 0.000019, mae: 0.002306, mean_q: 0.770523
 90901/100000: episode: 1562, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.786, mean reward: 0.008 [0.000, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.333, 10.100], loss: 0.000030, mae: 0.002614, mean_q: 0.770736
 91002/100000: episode: 1563, duration: 0.504s, episode steps: 101, steps per second: 201, episode reward: 0.721, mean reward: 0.007 [0.000, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.602, 10.184], loss: 0.000015, mae: 0.002648, mean_q: 0.771038
 91103/100000: episode: 1564, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.049, 10.117], loss: 0.000020, mae: 0.002397, mean_q: 0.770890
 91204/100000: episode: 1565, duration: 0.504s, episode steps: 101, steps per second: 200, episode reward: 0.742, mean reward: 0.007 [0.000, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.394, 10.222], loss: 0.000018, mae: 0.002624, mean_q: 0.771251
 91305/100000: episode: 1566, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.760, mean reward: 0.008 [0.000, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.100], loss: 0.000020, mae: 0.002720, mean_q: 0.771294
 91406/100000: episode: 1567, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.786, 10.508], loss: 0.000011, mae: 0.001967, mean_q: 0.771041
 91507/100000: episode: 1568, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.697, mean reward: 0.007 [0.000, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.826, 10.377], loss: 0.000010, mae: 0.002035, mean_q: 0.771379
 91608/100000: episode: 1569, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: 0.738, mean reward: 0.007 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.743, 10.215], loss: 0.000011, mae: 0.001831, mean_q: 0.770892
 91709/100000: episode: 1570, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.827, mean reward: 0.008 [0.000, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.100], loss: 0.000009, mae: 0.001860, mean_q: 0.771125
 91810/100000: episode: 1571, duration: 0.523s, episode steps: 101, steps per second: 193, episode reward: 0.801, mean reward: 0.008 [0.000, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.418, 10.367], loss: 0.000012, mae: 0.002227, mean_q: 0.771125
 91911/100000: episode: 1572, duration: 0.535s, episode steps: 101, steps per second: 189, episode reward: 0.787, mean reward: 0.008 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.116], loss: 0.000014, mae: 0.002160, mean_q: 0.771180
 92012/100000: episode: 1573, duration: 0.509s, episode steps: 101, steps per second: 198, episode reward: 0.679, mean reward: 0.007 [0.000, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.528, 10.185], loss: 0.000010, mae: 0.001931, mean_q: 0.771072
 92113/100000: episode: 1574, duration: 0.500s, episode steps: 101, steps per second: 202, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.707, 10.297], loss: 0.000016, mae: 0.002120, mean_q: 0.771264
 92214/100000: episode: 1575, duration: 0.511s, episode steps: 101, steps per second: 198, episode reward: 0.698, mean reward: 0.007 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.931, 10.100], loss: 0.000014, mae: 0.002452, mean_q: 0.771089
 92315/100000: episode: 1576, duration: 0.524s, episode steps: 101, steps per second: 193, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.444, 10.100], loss: 0.000013, mae: 0.002134, mean_q: 0.771542
 92416/100000: episode: 1577, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.717, mean reward: 0.007 [0.000, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.045, 10.221], loss: 0.000015, mae: 0.002296, mean_q: 0.771221
 92517/100000: episode: 1578, duration: 0.519s, episode steps: 101, steps per second: 194, episode reward: 0.857, mean reward: 0.008 [0.000, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.384, 10.455], loss: 0.000012, mae: 0.002276, mean_q: 0.771084
 92618/100000: episode: 1579, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.771, mean reward: 0.008 [0.000, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.991, 10.100], loss: 0.000011, mae: 0.001863, mean_q: 0.770978
 92719/100000: episode: 1580, duration: 0.517s, episode steps: 101, steps per second: 195, episode reward: 0.727, mean reward: 0.007 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.575, 10.190], loss: 0.000014, mae: 0.002138, mean_q: 0.770925
 92820/100000: episode: 1581, duration: 0.525s, episode steps: 101, steps per second: 192, episode reward: 0.737, mean reward: 0.007 [0.000, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.456, 10.100], loss: 0.000018, mae: 0.002501, mean_q: 0.770453
 92921/100000: episode: 1582, duration: 0.526s, episode steps: 101, steps per second: 192, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.361, 10.142], loss: 0.000016, mae: 0.002211, mean_q: 0.770534
 93022/100000: episode: 1583, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.688, mean reward: 0.007 [0.000, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.100, 10.168], loss: 0.000010, mae: 0.002091, mean_q: 0.770505
 93123/100000: episode: 1584, duration: 0.518s, episode steps: 101, steps per second: 195, episode reward: 0.795, mean reward: 0.008 [0.000, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.100], loss: 0.000012, mae: 0.001936, mean_q: 0.770259
 93224/100000: episode: 1585, duration: 0.507s, episode steps: 101, steps per second: 199, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.100], loss: 0.000012, mae: 0.001891, mean_q: 0.770413
 93325/100000: episode: 1586, duration: 0.519s, episode steps: 101, steps per second: 195, episode reward: 0.722, mean reward: 0.007 [0.000, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.187, 10.256], loss: 0.000012, mae: 0.002166, mean_q: 0.770186
 93426/100000: episode: 1587, duration: 0.514s, episode steps: 101, steps per second: 196, episode reward: 0.726, mean reward: 0.007 [0.000, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.053, 10.100], loss: 0.000010, mae: 0.001785, mean_q: 0.770042
 93527/100000: episode: 1588, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.761, mean reward: 0.008 [0.000, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.036, 10.252], loss: 0.000014, mae: 0.002240, mean_q: 0.769210
 93628/100000: episode: 1589, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.797, mean reward: 0.008 [0.000, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.460, 10.100], loss: 0.000009, mae: 0.001830, mean_q: 0.769581
 93729/100000: episode: 1590, duration: 0.531s, episode steps: 101, steps per second: 190, episode reward: 0.733, mean reward: 0.007 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.346, 10.198], loss: 0.000015, mae: 0.002269, mean_q: 0.769391
 93830/100000: episode: 1591, duration: 0.513s, episode steps: 101, steps per second: 197, episode reward: 0.749, mean reward: 0.007 [0.000, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.267, 10.164], loss: 0.000008, mae: 0.001731, mean_q: 0.768696
 93931/100000: episode: 1592, duration: 0.503s, episode steps: 101, steps per second: 201, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.444, 10.363], loss: 0.000009, mae: 0.002006, mean_q: 0.768441
 94032/100000: episode: 1593, duration: 0.530s, episode steps: 101, steps per second: 191, episode reward: 0.898, mean reward: 0.009 [0.000, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.885, 10.294], loss: 0.000012, mae: 0.001980, mean_q: 0.768587
 94133/100000: episode: 1594, duration: 0.505s, episode steps: 101, steps per second: 200, episode reward: 0.747, mean reward: 0.007 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.117, 10.240], loss: 0.000011, mae: 0.001768, mean_q: 0.768169
 94234/100000: episode: 1595, duration: 0.510s, episode steps: 101, steps per second: 198, episode reward: 0.692, mean reward: 0.007 [0.000, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.019, 10.268], loss: 0.000007, mae: 0.001632, mean_q: 0.768115
[Info] 1-TH LEVEL FOUND: 0.7892822623252869, Considering 10/100 traces
 94335/100000: episode: 1596, duration: 4.950s, episode steps: 101, steps per second: 20, episode reward: 0.770, mean reward: 0.008 [0.000, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.603, 10.309], loss: 0.000010, mae: 0.001905, mean_q: 0.767659
 94430/100000: episode: 1597, duration: 0.493s, episode steps: 95, steps per second: 193, episode reward: 0.713, mean reward: 0.008 [0.000, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.524, 10.100], loss: 0.000019, mae: 0.002449, mean_q: 0.767555
 94432/100000: episode: 1598, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.774, mean reward: 0.387 [0.000, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.280, 10.100], loss: 0.000004, mae: 0.001386, mean_q: 0.769196
 94527/100000: episode: 1599, duration: 0.490s, episode steps: 95, steps per second: 194, episode reward: 0.747, mean reward: 0.008 [0.000, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.423, 10.179], loss: 0.000020, mae: 0.002597, mean_q: 0.767262
 94626/100000: episode: 1600, duration: 0.502s, episode steps: 99, steps per second: 197, episode reward: 0.730, mean reward: 0.007 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.260, 10.235], loss: 0.000010, mae: 0.001758, mean_q: 0.766638
 94725/100000: episode: 1601, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 0.710, mean reward: 0.007 [0.000, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.973, 10.100], loss: 0.000022, mae: 0.001967, mean_q: 0.766173
 94820/100000: episode: 1602, duration: 0.486s, episode steps: 95, steps per second: 195, episode reward: 0.754, mean reward: 0.008 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-1.429, 10.251], loss: 0.000005, mae: 0.001348, mean_q: 0.766209
 94915/100000: episode: 1603, duration: 0.477s, episode steps: 95, steps per second: 199, episode reward: 0.731, mean reward: 0.008 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.484, 10.100], loss: 0.000013, mae: 0.001971, mean_q: 0.765553
 94917/100000: episode: 1604, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.614, mean reward: 0.307 [0.000, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.381, 10.100], loss: 0.000009, mae: 0.002399, mean_q: 0.766459
 95012/100000: episode: 1605, duration: 0.474s, episode steps: 95, steps per second: 200, episode reward: 0.741, mean reward: 0.008 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-1.013, 10.194], loss: 0.000017, mae: 0.001963, mean_q: 0.765028
 95107/100000: episode: 1606, duration: 0.476s, episode steps: 95, steps per second: 199, episode reward: 0.741, mean reward: 0.008 [0.000, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.972, 10.100], loss: 0.000014, mae: 0.001770, mean_q: 0.764737
 95109/100000: episode: 1607, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.791, mean reward: 0.395 [0.000, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.328, 10.100], loss: 0.000001, mae: 0.001354, mean_q: 0.764801
 95203/100000: episode: 1608, duration: 0.484s, episode steps: 94, steps per second: 194, episode reward: 0.788, mean reward: 0.008 [0.000, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.460, 10.284], loss: 0.000010, mae: 0.001710, mean_q: 0.764513
 95298/100000: episode: 1609, duration: 0.465s, episode steps: 95, steps per second: 204, episode reward: 0.748, mean reward: 0.008 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.729, 10.155], loss: 0.000016, mae: 0.001840, mean_q: 0.764112
 95393/100000: episode: 1610, duration: 0.482s, episode steps: 95, steps per second: 197, episode reward: 0.743, mean reward: 0.008 [0.000, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.481, 10.145], loss: 0.000022, mae: 0.001991, mean_q: 0.763873
 95487/100000: episode: 1611, duration: 0.503s, episode steps: 94, steps per second: 187, episode reward: 0.803, mean reward: 0.009 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-1.027, 10.197], loss: 0.000011, mae: 0.001712, mean_q: 0.763340
 95582/100000: episode: 1612, duration: 0.490s, episode steps: 95, steps per second: 194, episode reward: 0.777, mean reward: 0.008 [0.000, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-2.166, 10.151], loss: 0.000006, mae: 0.001392, mean_q: 0.763058
 95677/100000: episode: 1613, duration: 0.491s, episode steps: 95, steps per second: 194, episode reward: 0.724, mean reward: 0.008 [0.000, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.907, 10.114], loss: 0.000021, mae: 0.002006, mean_q: 0.762517
 95772/100000: episode: 1614, duration: 0.483s, episode steps: 95, steps per second: 197, episode reward: 0.659, mean reward: 0.007 [0.000, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.390, 10.145], loss: 0.000005, mae: 0.001257, mean_q: 0.762116
 95774/100000: episode: 1615, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.613, mean reward: 0.307 [0.000, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.393, 10.100], loss: 0.000001, mae: 0.001145, mean_q: 0.760468
 95869/100000: episode: 1616, duration: 0.486s, episode steps: 95, steps per second: 195, episode reward: 0.709, mean reward: 0.007 [0.000, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.652, 10.354], loss: 0.000005, mae: 0.001167, mean_q: 0.761791
 95964/100000: episode: 1617, duration: 0.482s, episode steps: 95, steps per second: 197, episode reward: 0.730, mean reward: 0.008 [0.000, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.995, 10.244], loss: 0.000021, mae: 0.001716, mean_q: 0.761528
 96063/100000: episode: 1618, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.882, 10.231], loss: 0.000016, mae: 0.002140, mean_q: 0.760720
 96158/100000: episode: 1619, duration: 0.474s, episode steps: 95, steps per second: 200, episode reward: 0.823, mean reward: 0.009 [0.000, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.822, 10.270], loss: 0.000014, mae: 0.001413, mean_q: 0.760362
 96160/100000: episode: 1620, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.649, mean reward: 0.325 [0.000, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.405, 10.100], loss: 0.000002, mae: 0.001822, mean_q: 0.762011
 96255/100000: episode: 1621, duration: 0.474s, episode steps: 95, steps per second: 200, episode reward: 0.719, mean reward: 0.008 [0.000, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.749, 10.100], loss: 0.000023, mae: 0.002081, mean_q: 0.759824
 96350/100000: episode: 1622, duration: 0.480s, episode steps: 95, steps per second: 198, episode reward: 0.772, mean reward: 0.008 [0.000, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.144, 10.253], loss: 0.000028, mae: 0.002605, mean_q: 0.759685
 96445/100000: episode: 1623, duration: 0.496s, episode steps: 95, steps per second: 192, episode reward: 0.781, mean reward: 0.008 [0.000, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.104, 10.110], loss: 0.000021, mae: 0.002125, mean_q: 0.759614
 96540/100000: episode: 1624, duration: 0.491s, episode steps: 95, steps per second: 193, episode reward: 0.775, mean reward: 0.008 [0.000, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.896, 10.206], loss: 0.000015, mae: 0.001537, mean_q: 0.759388
 96635/100000: episode: 1625, duration: 0.484s, episode steps: 95, steps per second: 196, episode reward: 0.828, mean reward: 0.009 [0.000, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.419, 10.282], loss: 0.000026, mae: 0.001962, mean_q: 0.759053
 96637/100000: episode: 1626, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.723, mean reward: 0.361 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.275, 10.100], loss: 0.000001, mae: 0.001050, mean_q: 0.760376
 96732/100000: episode: 1627, duration: 0.498s, episode steps: 95, steps per second: 191, episode reward: 0.812, mean reward: 0.009 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.291, 10.313], loss: 0.000016, mae: 0.001602, mean_q: 0.758690
 96826/100000: episode: 1628, duration: 0.484s, episode steps: 94, steps per second: 194, episode reward: 0.803, mean reward: 0.009 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.550, 10.278], loss: 0.000014, mae: 0.001543, mean_q: 0.758364
 96828/100000: episode: 1629, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.661, mean reward: 0.331 [0.000, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.322, 10.100], loss: 0.000027, mae: 0.005547, mean_q: 0.764726
 96830/100000: episode: 1630, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.619, mean reward: 0.310 [0.000, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.359, 10.100], loss: 0.000002, mae: 0.001409, mean_q: 0.759835
 96925/100000: episode: 1631, duration: 0.487s, episode steps: 95, steps per second: 195, episode reward: 0.731, mean reward: 0.008 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.502, 10.100], loss: 0.000013, mae: 0.001558, mean_q: 0.758139
 97020/100000: episode: 1632, duration: 0.494s, episode steps: 95, steps per second: 192, episode reward: 0.727, mean reward: 0.008 [0.000, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.926, 10.366], loss: 0.000012, mae: 0.001557, mean_q: 0.758349
 97115/100000: episode: 1633, duration: 0.473s, episode steps: 95, steps per second: 201, episode reward: 0.702, mean reward: 0.007 [0.000, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.405, 10.100], loss: 0.000023, mae: 0.001877, mean_q: 0.757825
 97210/100000: episode: 1634, duration: 0.484s, episode steps: 95, steps per second: 196, episode reward: 0.843, mean reward: 0.009 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.448, 10.255], loss: 0.000018, mae: 0.001864, mean_q: 0.757561
 97304/100000: episode: 1635, duration: 0.466s, episode steps: 94, steps per second: 202, episode reward: 0.757, mean reward: 0.008 [0.000, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.591, 10.100], loss: 0.000029, mae: 0.003091, mean_q: 0.757618
 97403/100000: episode: 1636, duration: 0.504s, episode steps: 99, steps per second: 197, episode reward: 0.750, mean reward: 0.008 [0.000, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.443, 10.224], loss: 0.000024, mae: 0.002003, mean_q: 0.757526
 97497/100000: episode: 1637, duration: 0.485s, episode steps: 94, steps per second: 194, episode reward: 0.844, mean reward: 0.009 [0.000, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.550, 10.350], loss: 0.000029, mae: 0.002713, mean_q: 0.757303
 97499/100000: episode: 1638, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 0.723, mean reward: 0.361 [0.000, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.294, 10.100], loss: 0.000002, mae: 0.001808, mean_q: 0.756524
 97501/100000: episode: 1639, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.607, mean reward: 0.304 [0.000, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.401, 10.100], loss: 0.000025, mae: 0.002479, mean_q: 0.755736
 97596/100000: episode: 1640, duration: 0.477s, episode steps: 95, steps per second: 199, episode reward: 0.811, mean reward: 0.009 [0.000, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-1.032, 10.421], loss: 0.000010, mae: 0.001393, mean_q: 0.757137
 97691/100000: episode: 1641, duration: 0.485s, episode steps: 95, steps per second: 196, episode reward: 0.731, mean reward: 0.008 [0.000, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.857, 10.434], loss: 0.000023, mae: 0.002142, mean_q: 0.756836
 97693/100000: episode: 1642, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.698, mean reward: 0.349 [0.000, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.336, 10.100], loss: 0.000001, mae: 0.000960, mean_q: 0.757756
 97695/100000: episode: 1643, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.718, mean reward: 0.359 [0.000, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.306, 10.100], loss: 0.000102, mae: 0.003138, mean_q: 0.755603
 97697/100000: episode: 1644, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.669, mean reward: 0.334 [0.000, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.299, 10.100], loss: 0.000004, mae: 0.002450, mean_q: 0.754555
 97792/100000: episode: 1645, duration: 0.476s, episode steps: 95, steps per second: 200, episode reward: 0.691, mean reward: 0.007 [0.000, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.422, 10.221], loss: 0.000026, mae: 0.002393, mean_q: 0.756361
 97794/100000: episode: 1646, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.787, mean reward: 0.393 [0.000, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.484, 10.100], loss: 0.000002, mae: 0.001389, mean_q: 0.757242
 97796/100000: episode: 1647, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.642, mean reward: 0.321 [0.000, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.340, 10.100], loss: 0.000001, mae: 0.001067, mean_q: 0.755704
 97891/100000: episode: 1648, duration: 0.487s, episode steps: 95, steps per second: 195, episode reward: 0.733, mean reward: 0.008 [0.000, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-1.104, 10.104], loss: 0.000019, mae: 0.001692, mean_q: 0.756744
 97986/100000: episode: 1649, duration: 0.486s, episode steps: 95, steps per second: 196, episode reward: 0.872, mean reward: 0.009 [0.000, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.586, 10.100], loss: 0.000020, mae: 0.001999, mean_q: 0.756595
 98080/100000: episode: 1650, duration: 0.476s, episode steps: 94, steps per second: 197, episode reward: 0.803, mean reward: 0.009 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.127, 10.145], loss: 0.000024, mae: 0.002086, mean_q: 0.756661
 98082/100000: episode: 1651, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.754, mean reward: 0.377 [0.000, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.295, 10.100], loss: 0.000011, mae: 0.001887, mean_q: 0.756390
 98177/100000: episode: 1652, duration: 0.473s, episode steps: 95, steps per second: 201, episode reward: 0.784, mean reward: 0.008 [0.000, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.771, 10.352], loss: 0.000015, mae: 0.002082, mean_q: 0.756659
 98179/100000: episode: 1653, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.753, mean reward: 0.377 [0.000, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.327, 10.100], loss: 0.000000, mae: 0.000598, mean_q: 0.757695
 98273/100000: episode: 1654, duration: 0.476s, episode steps: 94, steps per second: 198, episode reward: 0.861, mean reward: 0.009 [0.000, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-1.022, 10.309], loss: 0.000018, mae: 0.001497, mean_q: 0.756431
 98368/100000: episode: 1655, duration: 0.495s, episode steps: 95, steps per second: 192, episode reward: 0.738, mean reward: 0.008 [0.000, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.513, 10.173], loss: 0.000024, mae: 0.002012, mean_q: 0.756391
 98463/100000: episode: 1656, duration: 0.481s, episode steps: 95, steps per second: 198, episode reward: 0.744, mean reward: 0.008 [0.000, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-1.300, 10.100], loss: 0.000019, mae: 0.002252, mean_q: 0.756407
 98562/100000: episode: 1657, duration: 0.513s, episode steps: 99, steps per second: 193, episode reward: 0.736, mean reward: 0.007 [0.000, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.207, 10.193], loss: 0.000015, mae: 0.001942, mean_q: 0.756225
 98564/100000: episode: 1658, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.653, mean reward: 0.326 [0.000, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.308, 10.100], loss: 0.000004, mae: 0.002257, mean_q: 0.754670
 98566/100000: episode: 1659, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.617, mean reward: 0.309 [0.000, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.386, 10.100], loss: 0.000074, mae: 0.003387, mean_q: 0.754976
 98568/100000: episode: 1660, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.696, mean reward: 0.348 [0.000, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.370, 10.100], loss: 0.000029, mae: 0.002017, mean_q: 0.755295
 98663/100000: episode: 1661, duration: 0.497s, episode steps: 95, steps per second: 191, episode reward: 0.705, mean reward: 0.007 [0.000, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-0.863, 10.100], loss: 0.000018, mae: 0.001828, mean_q: 0.756586
 98758/100000: episode: 1662, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 0.751, mean reward: 0.008 [0.000, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.646, 10.239], loss: 0.000024, mae: 0.001834, mean_q: 0.756176
 98857/100000: episode: 1663, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 0.651, mean reward: 0.007 [0.000, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.146, 10.205], loss: 0.000024, mae: 0.002302, mean_q: 0.755941
 98951/100000: episode: 1664, duration: 0.471s, episode steps: 94, steps per second: 200, episode reward: 0.803, mean reward: 0.009 [0.000, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.450, 10.100], loss: 0.000021, mae: 0.001860, mean_q: 0.755591
 99046/100000: episode: 1665, duration: 0.486s, episode steps: 95, steps per second: 195, episode reward: 0.763, mean reward: 0.008 [0.000, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.814, 10.335], loss: 0.000033, mae: 0.002783, mean_q: 0.755645
 99048/100000: episode: 1666, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.657, mean reward: 0.329 [0.000, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.370, 10.100], loss: 0.000010, mae: 0.001575, mean_q: 0.755464
 99143/100000: episode: 1667, duration: 0.485s, episode steps: 95, steps per second: 196, episode reward: 0.855, mean reward: 0.009 [0.000, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.802, 10.627], loss: 0.000027, mae: 0.002452, mean_q: 0.755539
 99145/100000: episode: 1668, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.700, mean reward: 0.350 [0.000, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.415, 10.100], loss: 0.000004, mae: 0.002644, mean_q: 0.759047
 99240/100000: episode: 1669, duration: 0.480s, episode steps: 95, steps per second: 198, episode reward: 0.843, mean reward: 0.009 [0.000, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.360, 10.277], loss: 0.000026, mae: 0.002669, mean_q: 0.755094
 99335/100000: episode: 1670, duration: 0.482s, episode steps: 95, steps per second: 197, episode reward: 0.800, mean reward: 0.008 [0.000, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.935, 10.241], loss: 0.000025, mae: 0.002486, mean_q: 0.755318
 99337/100000: episode: 1671, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.626, mean reward: 0.313 [0.000, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.371, 10.100], loss: 0.000040, mae: 0.003082, mean_q: 0.757228
 99339/100000: episode: 1672, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.734, mean reward: 0.367 [0.000, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.349, 10.100], loss: 0.000058, mae: 0.003982, mean_q: 0.757517
 99341/100000: episode: 1673, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.660, mean reward: 0.330 [0.000, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.268, 10.100], loss: 0.000033, mae: 0.003711, mean_q: 0.753383
 99436/100000: episode: 1674, duration: 0.485s, episode steps: 95, steps per second: 196, episode reward: 0.936, mean reward: 0.010 [0.000, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.783, 10.100], loss: 0.000031, mae: 0.002608, mean_q: 0.755723
 99531/100000: episode: 1675, duration: 0.462s, episode steps: 95, steps per second: 206, episode reward: 0.768, mean reward: 0.008 [0.000, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.543, 10.100], loss: 0.000019, mae: 0.001601, mean_q: 0.755466
 99626/100000: episode: 1676, duration: 0.488s, episode steps: 95, steps per second: 195, episode reward: 0.748, mean reward: 0.008 [0.000, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.374, 10.100], loss: 0.000029, mae: 0.002566, mean_q: 0.755026
 99721/100000: episode: 1677, duration: 0.475s, episode steps: 95, steps per second: 200, episode reward: 0.812, mean reward: 0.009 [0.000, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.283, 10.100], loss: 0.000031, mae: 0.002332, mean_q: 0.755089
 99816/100000: episode: 1678, duration: 0.471s, episode steps: 95, steps per second: 202, episode reward: 0.759, mean reward: 0.008 [0.000, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.698, 10.214], loss: 0.000031, mae: 0.002586, mean_q: 0.755357
 99911/100000: episode: 1679, duration: 0.483s, episode steps: 95, steps per second: 197, episode reward: 0.693, mean reward: 0.007 [0.000, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.346, 10.116], loss: 0.000028, mae: 0.002563, mean_q: 0.755036
done, took 563.409 seconds
[Info] End Importance Splitting. Falsification occurred 1 times.
