Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.172s, episode steps: 100, steps per second: 581, episode reward: 15.463, mean reward: 0.155 [0.017, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.365, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.070s, episode steps: 100, steps per second: 1424, episode reward: 20.177, mean reward: 0.202 [0.009, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.141, 10.155], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 12.359, mean reward: 0.124 [0.011, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.963, 10.185], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 18.121, mean reward: 0.181 [0.023, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.768, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 16.753, mean reward: 0.168 [0.008, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.884, 10.254], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.278s, episode steps: 100, steps per second: 78, episode reward: 17.248, mean reward: 0.172 [0.005, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.098], loss: 0.014365, mae: 0.121170, mean_q: 0.028567
   700/100000: episode: 7, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 15.643, mean reward: 0.156 [0.008, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.331, 10.098], loss: 0.004278, mae: 0.073885, mean_q: 0.148259
   800/100000: episode: 8, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 13.307, mean reward: 0.133 [0.018, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.098], loss: 0.003718, mae: 0.068397, mean_q: 0.219914
   900/100000: episode: 9, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 18.678, mean reward: 0.187 [0.030, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.143], loss: 0.003604, mae: 0.067800, mean_q: 0.256843
  1000/100000: episode: 10, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 16.913, mean reward: 0.169 [0.021, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.007, 10.098], loss: 0.003662, mae: 0.068647, mean_q: 0.288493
  1100/100000: episode: 11, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 16.160, mean reward: 0.162 [0.005, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.107], loss: 0.003612, mae: 0.068383, mean_q: 0.301366
  1200/100000: episode: 12, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.996, mean reward: 0.170 [0.022, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.919, 10.408], loss: 0.003526, mae: 0.067469, mean_q: 0.308111
  1300/100000: episode: 13, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 12.860, mean reward: 0.129 [0.009, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.809, 10.098], loss: 0.003654, mae: 0.068453, mean_q: 0.312906
  1400/100000: episode: 14, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 17.332, mean reward: 0.173 [0.019, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.646, 10.098], loss: 0.003884, mae: 0.070579, mean_q: 0.318762
  1500/100000: episode: 15, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 20.033, mean reward: 0.200 [0.026, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.263, 10.350], loss: 0.004102, mae: 0.071660, mean_q: 0.319989
  1600/100000: episode: 16, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 13.784, mean reward: 0.138 [0.009, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.761, 10.098], loss: 0.003953, mae: 0.070604, mean_q: 0.322575
  1700/100000: episode: 17, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 16.588, mean reward: 0.166 [0.044, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.790, 10.114], loss: 0.003915, mae: 0.070576, mean_q: 0.321483
  1800/100000: episode: 18, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 18.366, mean reward: 0.184 [0.007, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.641, 10.234], loss: 0.003972, mae: 0.070875, mean_q: 0.323972
  1900/100000: episode: 19, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 14.434, mean reward: 0.144 [0.011, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.725, 10.218], loss: 0.004180, mae: 0.072326, mean_q: 0.321619
  2000/100000: episode: 20, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 15.104, mean reward: 0.151 [0.006, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.130, 10.168], loss: 0.003999, mae: 0.070470, mean_q: 0.325998
  2100/100000: episode: 21, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 13.943, mean reward: 0.139 [0.005, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.016, 10.098], loss: 0.003838, mae: 0.069861, mean_q: 0.320729
  2200/100000: episode: 22, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 15.171, mean reward: 0.152 [0.025, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.174, 10.246], loss: 0.003808, mae: 0.069344, mean_q: 0.317287
  2300/100000: episode: 23, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 15.364, mean reward: 0.154 [0.021, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.159], loss: 0.003860, mae: 0.069826, mean_q: 0.318307
  2400/100000: episode: 24, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 16.963, mean reward: 0.170 [0.015, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.986, 10.322], loss: 0.003892, mae: 0.070072, mean_q: 0.320040
  2500/100000: episode: 25, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 16.265, mean reward: 0.163 [0.017, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.356, 10.150], loss: 0.003959, mae: 0.070230, mean_q: 0.316939
  2600/100000: episode: 26, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 21.837, mean reward: 0.218 [0.012, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.969, 10.347], loss: 0.004029, mae: 0.070855, mean_q: 0.321675
  2700/100000: episode: 27, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 17.366, mean reward: 0.174 [0.011, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.678, 10.098], loss: 0.003983, mae: 0.071128, mean_q: 0.322156
  2800/100000: episode: 28, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 16.889, mean reward: 0.169 [0.015, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.355, 10.234], loss: 0.004022, mae: 0.070975, mean_q: 0.324542
  2900/100000: episode: 29, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 18.933, mean reward: 0.189 [0.026, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.699, 10.201], loss: 0.004076, mae: 0.072204, mean_q: 0.325100
  3000/100000: episode: 30, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 19.213, mean reward: 0.192 [0.043, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.951, 10.218], loss: 0.004211, mae: 0.073005, mean_q: 0.321372
  3100/100000: episode: 31, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 14.405, mean reward: 0.144 [0.009, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.929, 10.142], loss: 0.004002, mae: 0.071422, mean_q: 0.325479
  3200/100000: episode: 32, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.415, mean reward: 0.144 [0.025, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.459, 10.308], loss: 0.003961, mae: 0.071749, mean_q: 0.324985
  3300/100000: episode: 33, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 15.950, mean reward: 0.159 [0.009, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.117], loss: 0.004021, mae: 0.071463, mean_q: 0.325390
  3400/100000: episode: 34, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 16.776, mean reward: 0.168 [0.015, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.920, 10.098], loss: 0.004036, mae: 0.071455, mean_q: 0.323737
  3500/100000: episode: 35, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 20.836, mean reward: 0.208 [0.035, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.663, 10.098], loss: 0.003976, mae: 0.071238, mean_q: 0.329387
  3600/100000: episode: 36, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 14.543, mean reward: 0.145 [0.014, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.253, 10.115], loss: 0.004082, mae: 0.072313, mean_q: 0.326853
  3700/100000: episode: 37, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 16.998, mean reward: 0.170 [0.022, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.149, 10.098], loss: 0.004363, mae: 0.073851, mean_q: 0.326886
  3800/100000: episode: 38, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 14.918, mean reward: 0.149 [0.013, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.012, 10.099], loss: 0.003951, mae: 0.070446, mean_q: 0.326016
  3900/100000: episode: 39, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 15.197, mean reward: 0.152 [0.014, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.932, 10.326], loss: 0.004151, mae: 0.073068, mean_q: 0.324613
  4000/100000: episode: 40, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 18.485, mean reward: 0.185 [0.022, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.394, 10.098], loss: 0.004012, mae: 0.071129, mean_q: 0.325410
  4100/100000: episode: 41, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 16.505, mean reward: 0.165 [0.030, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.011, 10.137], loss: 0.003933, mae: 0.071450, mean_q: 0.324945
  4200/100000: episode: 42, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 19.496, mean reward: 0.195 [0.017, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.127], loss: 0.004206, mae: 0.072957, mean_q: 0.326944
  4300/100000: episode: 43, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 15.140, mean reward: 0.151 [0.009, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.679, 10.098], loss: 0.004368, mae: 0.075136, mean_q: 0.331121
  4400/100000: episode: 44, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 16.177, mean reward: 0.162 [0.022, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.958, 10.098], loss: 0.003961, mae: 0.071074, mean_q: 0.330301
  4500/100000: episode: 45, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 21.934, mean reward: 0.219 [0.015, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.359], loss: 0.003906, mae: 0.070695, mean_q: 0.326829
  4600/100000: episode: 46, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 15.420, mean reward: 0.154 [0.007, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.182, 10.134], loss: 0.004012, mae: 0.071465, mean_q: 0.327917
  4700/100000: episode: 47, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 16.381, mean reward: 0.164 [0.017, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.516, 10.107], loss: 0.004124, mae: 0.072264, mean_q: 0.332861
  4800/100000: episode: 48, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 15.162, mean reward: 0.152 [0.019, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.100], loss: 0.004248, mae: 0.073734, mean_q: 0.328511
  4900/100000: episode: 49, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 17.251, mean reward: 0.173 [0.012, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.257, 10.098], loss: 0.003910, mae: 0.070405, mean_q: 0.331130
  5000/100000: episode: 50, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 13.445, mean reward: 0.134 [0.014, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.403, 10.135], loss: 0.004005, mae: 0.071587, mean_q: 0.331689
  5100/100000: episode: 51, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 25.182, mean reward: 0.252 [0.028, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.662, 10.383], loss: 0.004278, mae: 0.074385, mean_q: 0.331294
  5200/100000: episode: 52, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 13.226, mean reward: 0.132 [0.011, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.619, 10.111], loss: 0.003943, mae: 0.070643, mean_q: 0.327955
  5300/100000: episode: 53, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 21.065, mean reward: 0.211 [0.025, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.081, 10.437], loss: 0.004137, mae: 0.073117, mean_q: 0.332750
  5400/100000: episode: 54, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 16.691, mean reward: 0.167 [0.025, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.583, 10.152], loss: 0.004048, mae: 0.072809, mean_q: 0.330480
  5500/100000: episode: 55, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 14.508, mean reward: 0.145 [0.006, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.884, 10.123], loss: 0.004080, mae: 0.073200, mean_q: 0.332383
  5600/100000: episode: 56, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 17.236, mean reward: 0.172 [0.016, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.413, 10.111], loss: 0.004217, mae: 0.074080, mean_q: 0.336462
  5700/100000: episode: 57, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 18.088, mean reward: 0.181 [0.019, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.371, 10.306], loss: 0.004126, mae: 0.073208, mean_q: 0.330749
  5800/100000: episode: 58, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 13.808, mean reward: 0.138 [0.008, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.153, 10.227], loss: 0.003939, mae: 0.071044, mean_q: 0.331120
  5900/100000: episode: 59, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 13.234, mean reward: 0.132 [0.016, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.842, 10.143], loss: 0.004204, mae: 0.073073, mean_q: 0.329540
  6000/100000: episode: 60, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 16.017, mean reward: 0.160 [0.005, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.879, 10.098], loss: 0.004081, mae: 0.072129, mean_q: 0.328675
  6100/100000: episode: 61, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 17.571, mean reward: 0.176 [0.023, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.269], loss: 0.004289, mae: 0.074067, mean_q: 0.329848
  6200/100000: episode: 62, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 17.332, mean reward: 0.173 [0.024, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.200, 10.098], loss: 0.003994, mae: 0.071674, mean_q: 0.329497
  6300/100000: episode: 63, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 17.085, mean reward: 0.171 [0.021, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.238, 10.266], loss: 0.003995, mae: 0.071773, mean_q: 0.328848
  6400/100000: episode: 64, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.034, mean reward: 0.150 [0.007, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.342, 10.296], loss: 0.003982, mae: 0.071631, mean_q: 0.330762
  6500/100000: episode: 65, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 16.785, mean reward: 0.168 [0.010, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.290, 10.098], loss: 0.004204, mae: 0.073237, mean_q: 0.331812
  6600/100000: episode: 66, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 16.351, mean reward: 0.164 [0.015, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.502, 10.350], loss: 0.004452, mae: 0.075496, mean_q: 0.329529
  6700/100000: episode: 67, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.837, mean reward: 0.168 [0.012, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.605, 10.098], loss: 0.003991, mae: 0.071226, mean_q: 0.329529
  6800/100000: episode: 68, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 19.724, mean reward: 0.197 [0.013, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.511, 10.098], loss: 0.004063, mae: 0.071777, mean_q: 0.329117
  6900/100000: episode: 69, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 19.413, mean reward: 0.194 [0.012, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.844, 10.227], loss: 0.004101, mae: 0.072529, mean_q: 0.332565
  7000/100000: episode: 70, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 17.330, mean reward: 0.173 [0.033, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.394], loss: 0.004132, mae: 0.072388, mean_q: 0.332811
  7100/100000: episode: 71, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 17.893, mean reward: 0.179 [0.026, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.801, 10.260], loss: 0.004176, mae: 0.072890, mean_q: 0.329574
  7200/100000: episode: 72, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 20.106, mean reward: 0.201 [0.015, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.113, 10.098], loss: 0.004233, mae: 0.073820, mean_q: 0.337198
  7300/100000: episode: 73, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 17.249, mean reward: 0.172 [0.009, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.414, 10.098], loss: 0.004022, mae: 0.071729, mean_q: 0.334186
  7400/100000: episode: 74, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 17.674, mean reward: 0.177 [0.023, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.515, 10.393], loss: 0.004197, mae: 0.073164, mean_q: 0.337951
  7500/100000: episode: 75, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 16.476, mean reward: 0.165 [0.030, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.003, 10.098], loss: 0.004301, mae: 0.073730, mean_q: 0.337841
  7600/100000: episode: 76, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 19.210, mean reward: 0.192 [0.021, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.194, 10.098], loss: 0.003906, mae: 0.069933, mean_q: 0.334207
  7700/100000: episode: 77, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 17.741, mean reward: 0.177 [0.036, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.273, 10.265], loss: 0.004007, mae: 0.071321, mean_q: 0.333637
  7800/100000: episode: 78, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 16.804, mean reward: 0.168 [0.026, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.836, 10.155], loss: 0.004122, mae: 0.072385, mean_q: 0.331126
  7900/100000: episode: 79, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 12.490, mean reward: 0.125 [0.016, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.707, 10.206], loss: 0.004145, mae: 0.072557, mean_q: 0.335115
  8000/100000: episode: 80, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 17.606, mean reward: 0.176 [0.036, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.238], loss: 0.004117, mae: 0.072175, mean_q: 0.334884
  8100/100000: episode: 81, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 16.505, mean reward: 0.165 [0.024, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.343, 10.166], loss: 0.003987, mae: 0.070914, mean_q: 0.331048
  8200/100000: episode: 82, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 17.556, mean reward: 0.176 [0.012, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.184], loss: 0.004116, mae: 0.072091, mean_q: 0.335096
  8300/100000: episode: 83, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 18.356, mean reward: 0.184 [0.024, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.958, 10.198], loss: 0.004023, mae: 0.071685, mean_q: 0.337825
  8400/100000: episode: 84, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 18.082, mean reward: 0.181 [0.012, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.233], loss: 0.004220, mae: 0.072571, mean_q: 0.337303
  8500/100000: episode: 85, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 16.438, mean reward: 0.164 [0.002, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.098], loss: 0.004284, mae: 0.073212, mean_q: 0.338666
  8600/100000: episode: 86, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 22.610, mean reward: 0.226 [0.038, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.098], loss: 0.004248, mae: 0.073523, mean_q: 0.335252
  8700/100000: episode: 87, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 15.645, mean reward: 0.156 [0.025, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.221, 10.191], loss: 0.004421, mae: 0.075114, mean_q: 0.336303
  8800/100000: episode: 88, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 17.213, mean reward: 0.172 [0.020, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.237, 10.171], loss: 0.004532, mae: 0.076291, mean_q: 0.340020
  8900/100000: episode: 89, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 13.469, mean reward: 0.135 [0.008, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.640, 10.098], loss: 0.004324, mae: 0.074235, mean_q: 0.339281
  9000/100000: episode: 90, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 15.112, mean reward: 0.151 [0.009, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.030, 10.098], loss: 0.004048, mae: 0.071322, mean_q: 0.339356
  9100/100000: episode: 91, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 15.925, mean reward: 0.159 [0.011, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.051, 10.191], loss: 0.004203, mae: 0.072841, mean_q: 0.338232
  9200/100000: episode: 92, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 19.467, mean reward: 0.195 [0.013, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.638, 10.318], loss: 0.004065, mae: 0.072253, mean_q: 0.337657
  9300/100000: episode: 93, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 13.388, mean reward: 0.134 [0.009, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.173, 10.237], loss: 0.004222, mae: 0.073750, mean_q: 0.336080
  9400/100000: episode: 94, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 14.958, mean reward: 0.150 [0.017, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.759, 10.098], loss: 0.004013, mae: 0.071590, mean_q: 0.335639
  9500/100000: episode: 95, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 16.996, mean reward: 0.170 [0.019, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.036, 10.164], loss: 0.004211, mae: 0.073528, mean_q: 0.335863
  9600/100000: episode: 96, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 20.769, mean reward: 0.208 [0.032, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.243, 10.098], loss: 0.004201, mae: 0.072688, mean_q: 0.334357
  9700/100000: episode: 97, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 13.115, mean reward: 0.131 [0.006, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.599, 10.098], loss: 0.004488, mae: 0.075214, mean_q: 0.336570
  9800/100000: episode: 98, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 13.851, mean reward: 0.139 [0.011, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.206, 10.098], loss: 0.004149, mae: 0.072895, mean_q: 0.338784
  9900/100000: episode: 99, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 20.063, mean reward: 0.201 [0.041, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.098], loss: 0.004185, mae: 0.072865, mean_q: 0.334368
 10000/100000: episode: 100, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 24.517, mean reward: 0.245 [0.047, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.296, 10.588], loss: 0.004546, mae: 0.076373, mean_q: 0.339597
 10100/100000: episode: 101, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 15.506, mean reward: 0.155 [0.014, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.871, 10.105], loss: 0.004159, mae: 0.071961, mean_q: 0.336511
 10200/100000: episode: 102, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 22.415, mean reward: 0.224 [0.022, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.407, 10.098], loss: 0.004374, mae: 0.074619, mean_q: 0.337500
 10300/100000: episode: 103, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 18.925, mean reward: 0.189 [0.006, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.539, 10.313], loss: 0.004379, mae: 0.073762, mean_q: 0.334430
 10400/100000: episode: 104, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 21.539, mean reward: 0.215 [0.035, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.739, 10.163], loss: 0.004437, mae: 0.074673, mean_q: 0.338294
 10500/100000: episode: 105, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 15.835, mean reward: 0.158 [0.004, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.042, 10.225], loss: 0.004086, mae: 0.071270, mean_q: 0.336800
 10600/100000: episode: 106, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 14.637, mean reward: 0.146 [0.023, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.702, 10.180], loss: 0.004197, mae: 0.072224, mean_q: 0.332295
 10700/100000: episode: 107, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 19.373, mean reward: 0.194 [0.045, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.573, 10.098], loss: 0.004337, mae: 0.073736, mean_q: 0.337644
 10800/100000: episode: 108, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 15.776, mean reward: 0.158 [0.014, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.832, 10.098], loss: 0.004396, mae: 0.074176, mean_q: 0.337919
 10900/100000: episode: 109, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 16.909, mean reward: 0.169 [0.022, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.975, 10.098], loss: 0.004587, mae: 0.075312, mean_q: 0.338593
 11000/100000: episode: 110, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 17.724, mean reward: 0.177 [0.024, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.289], loss: 0.004404, mae: 0.073511, mean_q: 0.340034
 11100/100000: episode: 111, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 18.565, mean reward: 0.186 [0.015, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.531, 10.379], loss: 0.004474, mae: 0.075046, mean_q: 0.339432
 11200/100000: episode: 112, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 16.782, mean reward: 0.168 [0.010, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.984, 10.127], loss: 0.004347, mae: 0.073858, mean_q: 0.340650
 11300/100000: episode: 113, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 17.372, mean reward: 0.174 [0.017, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.430, 10.176], loss: 0.004518, mae: 0.075077, mean_q: 0.336970
 11400/100000: episode: 114, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 15.877, mean reward: 0.159 [0.031, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.513, 10.268], loss: 0.004340, mae: 0.073869, mean_q: 0.338153
 11500/100000: episode: 115, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 18.477, mean reward: 0.185 [0.013, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.347], loss: 0.004418, mae: 0.073721, mean_q: 0.343736
 11600/100000: episode: 116, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 14.371, mean reward: 0.144 [0.008, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.368, 10.098], loss: 0.004324, mae: 0.072896, mean_q: 0.341067
 11700/100000: episode: 117, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 16.499, mean reward: 0.165 [0.011, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.191, 10.098], loss: 0.004026, mae: 0.070975, mean_q: 0.337789
 11800/100000: episode: 118, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 14.393, mean reward: 0.144 [0.014, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.098], loss: 0.004344, mae: 0.072561, mean_q: 0.334329
 11900/100000: episode: 119, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 18.487, mean reward: 0.185 [0.024, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.847, 10.533], loss: 0.004391, mae: 0.073185, mean_q: 0.337277
 12000/100000: episode: 120, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 23.976, mean reward: 0.240 [0.016, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.452, 10.377], loss: 0.004273, mae: 0.072411, mean_q: 0.339131
 12100/100000: episode: 121, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 17.151, mean reward: 0.172 [0.026, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.787, 10.243], loss: 0.004517, mae: 0.075225, mean_q: 0.338784
 12200/100000: episode: 122, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 16.214, mean reward: 0.162 [0.007, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.515, 10.098], loss: 0.004296, mae: 0.072732, mean_q: 0.335161
 12300/100000: episode: 123, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 16.805, mean reward: 0.168 [0.011, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.098], loss: 0.004321, mae: 0.072303, mean_q: 0.337386
 12400/100000: episode: 124, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 23.196, mean reward: 0.232 [0.023, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.009, 10.098], loss: 0.004233, mae: 0.072649, mean_q: 0.337078
 12500/100000: episode: 125, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 16.110, mean reward: 0.161 [0.020, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.629, 10.098], loss: 0.003974, mae: 0.070635, mean_q: 0.338205
 12600/100000: episode: 126, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 16.892, mean reward: 0.169 [0.018, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.692, 10.328], loss: 0.004364, mae: 0.073269, mean_q: 0.339721
 12700/100000: episode: 127, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 14.844, mean reward: 0.148 [0.022, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.320], loss: 0.004086, mae: 0.071321, mean_q: 0.337908
 12800/100000: episode: 128, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 16.218, mean reward: 0.162 [0.037, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.449, 10.150], loss: 0.004349, mae: 0.073576, mean_q: 0.338528
 12900/100000: episode: 129, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 19.029, mean reward: 0.190 [0.014, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.330, 10.119], loss: 0.003891, mae: 0.069149, mean_q: 0.340892
 13000/100000: episode: 130, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 17.361, mean reward: 0.174 [0.007, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.827, 10.098], loss: 0.004038, mae: 0.071199, mean_q: 0.342245
 13100/100000: episode: 131, duration: 0.593s, episode steps: 100, steps per second: 168, episode reward: 13.879, mean reward: 0.139 [0.021, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.493, 10.179], loss: 0.003998, mae: 0.070644, mean_q: 0.342172
 13200/100000: episode: 132, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 22.044, mean reward: 0.220 [0.012, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.071, 10.098], loss: 0.003946, mae: 0.069779, mean_q: 0.340909
 13300/100000: episode: 133, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 18.062, mean reward: 0.181 [0.011, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.227], loss: 0.003977, mae: 0.070422, mean_q: 0.340632
 13400/100000: episode: 134, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 15.947, mean reward: 0.159 [0.015, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.219, 10.207], loss: 0.003739, mae: 0.067769, mean_q: 0.341467
 13500/100000: episode: 135, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 15.555, mean reward: 0.156 [0.020, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.168], loss: 0.003902, mae: 0.068939, mean_q: 0.337656
 13600/100000: episode: 136, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 17.468, mean reward: 0.175 [0.038, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.347, 10.098], loss: 0.004111, mae: 0.071433, mean_q: 0.341216
 13700/100000: episode: 137, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 20.716, mean reward: 0.207 [0.012, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.438, 10.432], loss: 0.003784, mae: 0.068078, mean_q: 0.336264
 13800/100000: episode: 138, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 14.720, mean reward: 0.147 [0.010, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.133], loss: 0.003681, mae: 0.067661, mean_q: 0.342618
 13900/100000: episode: 139, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 15.220, mean reward: 0.152 [0.023, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.140, 10.212], loss: 0.003818, mae: 0.069303, mean_q: 0.344739
 14000/100000: episode: 140, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 17.600, mean reward: 0.176 [0.012, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.347], loss: 0.003847, mae: 0.068850, mean_q: 0.341569
 14100/100000: episode: 141, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 19.863, mean reward: 0.199 [0.051, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.747, 10.261], loss: 0.004001, mae: 0.070095, mean_q: 0.344349
 14200/100000: episode: 142, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 15.208, mean reward: 0.152 [0.021, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.006, 10.098], loss: 0.003988, mae: 0.069714, mean_q: 0.346495
 14300/100000: episode: 143, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 14.796, mean reward: 0.148 [0.004, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.485, 10.120], loss: 0.004005, mae: 0.070782, mean_q: 0.341801
 14400/100000: episode: 144, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 19.667, mean reward: 0.197 [0.016, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.918, 10.179], loss: 0.003775, mae: 0.068773, mean_q: 0.340589
 14500/100000: episode: 145, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 16.069, mean reward: 0.161 [0.014, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.287], loss: 0.003932, mae: 0.070018, mean_q: 0.348461
 14600/100000: episode: 146, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 14.659, mean reward: 0.147 [0.019, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.286, 10.135], loss: 0.003744, mae: 0.067929, mean_q: 0.338207
 14700/100000: episode: 147, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 14.302, mean reward: 0.143 [0.019, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.353, 10.098], loss: 0.003669, mae: 0.067239, mean_q: 0.338267
 14800/100000: episode: 148, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 18.343, mean reward: 0.183 [0.029, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.972, 10.098], loss: 0.003972, mae: 0.070585, mean_q: 0.348163
 14900/100000: episode: 149, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 17.272, mean reward: 0.173 [0.013, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.076, 10.098], loss: 0.004074, mae: 0.070904, mean_q: 0.347424
[Info] New level: 0.7172239422798157 | Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.529s, episode steps: 100, steps per second: 22, episode reward: 19.933, mean reward: 0.199 [0.013, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.871, 10.488], loss: 0.003668, mae: 0.067440, mean_q: 0.340726
 15027/100000: episode: 151, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 10.121, mean reward: 0.375 [0.234, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.634, 10.407], loss: 0.004453, mae: 0.073070, mean_q: 0.342133
 15054/100000: episode: 152, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 10.102, mean reward: 0.374 [0.243, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.436], loss: 0.003622, mae: 0.067134, mean_q: 0.343323
 15077/100000: episode: 153, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 5.678, mean reward: 0.247 [0.063, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.270], loss: 0.003897, mae: 0.068997, mean_q: 0.354706
 15078/100000: episode: 154, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.480, mean reward: 0.480 [0.480, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.488], loss: 0.003171, mae: 0.063837, mean_q: 0.352441
 15103/100000: episode: 155, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 9.381, mean reward: 0.375 [0.306, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.518], loss: 0.003900, mae: 0.069043, mean_q: 0.349871
 15150/100000: episode: 156, duration: 0.269s, episode steps: 47, steps per second: 174, episode reward: 10.532, mean reward: 0.224 [0.014, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.276, 10.100], loss: 0.004050, mae: 0.071517, mean_q: 0.346097
 15165/100000: episode: 157, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.473, mean reward: 0.432 [0.345, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.364, 10.100], loss: 0.003820, mae: 0.069140, mean_q: 0.344746
 15190/100000: episode: 158, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 10.911, mean reward: 0.436 [0.314, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.398, 10.496], loss: 0.003693, mae: 0.068385, mean_q: 0.349269
 15211/100000: episode: 159, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 8.681, mean reward: 0.413 [0.300, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.547, 10.100], loss: 0.003547, mae: 0.064964, mean_q: 0.339298
 15236/100000: episode: 160, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 10.417, mean reward: 0.417 [0.314, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.803, 10.524], loss: 0.004358, mae: 0.073300, mean_q: 0.352635
 15251/100000: episode: 161, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 6.933, mean reward: 0.462 [0.346, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.342, 10.100], loss: 0.004101, mae: 0.073421, mean_q: 0.378698
 15298/100000: episode: 162, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 17.754, mean reward: 0.378 [0.193, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.273, 10.317], loss: 0.003612, mae: 0.066756, mean_q: 0.354506
 15314/100000: episode: 163, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 4.370, mean reward: 0.273 [0.180, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.269, 10.100], loss: 0.003597, mae: 0.066999, mean_q: 0.357280
 15361/100000: episode: 164, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 10.295, mean reward: 0.219 [0.035, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.356, 10.374], loss: 0.003756, mae: 0.069491, mean_q: 0.363767
 15377/100000: episode: 165, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 5.468, mean reward: 0.342 [0.260, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.342, 10.100], loss: 0.004463, mae: 0.072941, mean_q: 0.348396
 15378/100000: episode: 166, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.035, 10.477], loss: 0.003091, mae: 0.065601, mean_q: 0.357686
 15390/100000: episode: 167, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 4.480, mean reward: 0.373 [0.268, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.333], loss: 0.004334, mae: 0.073128, mean_q: 0.352692
 15391/100000: episode: 168, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.546, mean reward: 0.546 [0.546, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.498], loss: 0.003184, mae: 0.067699, mean_q: 0.377823
 15407/100000: episode: 169, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 3.472, mean reward: 0.217 [0.060, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.287, 10.100], loss: 0.004023, mae: 0.070518, mean_q: 0.356072
 15432/100000: episode: 170, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 7.571, mean reward: 0.303 [0.075, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.090, 10.307], loss: 0.004066, mae: 0.069388, mean_q: 0.360649
 15455/100000: episode: 171, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 7.058, mean reward: 0.307 [0.172, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.299, 10.331], loss: 0.003685, mae: 0.069104, mean_q: 0.361767
 15476/100000: episode: 172, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 6.757, mean reward: 0.322 [0.214, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.480, 10.100], loss: 0.004674, mae: 0.072724, mean_q: 0.366356
 15523/100000: episode: 173, duration: 0.278s, episode steps: 47, steps per second: 169, episode reward: 10.522, mean reward: 0.224 [0.016, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.948, 10.100], loss: 0.004050, mae: 0.070483, mean_q: 0.362181
 15556/100000: episode: 174, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 14.525, mean reward: 0.440 [0.321, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.724, 10.507], loss: 0.004122, mae: 0.071265, mean_q: 0.363754
 15589/100000: episode: 175, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 10.278, mean reward: 0.311 [0.220, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.296, 10.385], loss: 0.003698, mae: 0.067205, mean_q: 0.375063
 15614/100000: episode: 176, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 8.783, mean reward: 0.351 [0.238, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.203, 10.365], loss: 0.003955, mae: 0.069523, mean_q: 0.368421
 15637/100000: episode: 177, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 5.778, mean reward: 0.251 [0.132, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.131, 10.228], loss: 0.003662, mae: 0.068244, mean_q: 0.365565
 15658/100000: episode: 178, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 7.763, mean reward: 0.370 [0.243, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.484, 10.100], loss: 0.003664, mae: 0.068568, mean_q: 0.374634
 15679/100000: episode: 179, duration: 0.131s, episode steps: 21, steps per second: 161, episode reward: 5.938, mean reward: 0.283 [0.125, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.292, 10.100], loss: 0.003432, mae: 0.066239, mean_q: 0.378644
 15712/100000: episode: 180, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 10.013, mean reward: 0.303 [0.223, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.501], loss: 0.003428, mae: 0.065305, mean_q: 0.375591
 15745/100000: episode: 181, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 8.322, mean reward: 0.252 [0.103, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.582, 10.367], loss: 0.003790, mae: 0.068816, mean_q: 0.380056
 15757/100000: episode: 182, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 4.554, mean reward: 0.380 [0.313, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.496, 10.534], loss: 0.003567, mae: 0.067604, mean_q: 0.373558
 15778/100000: episode: 183, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 7.052, mean reward: 0.336 [0.275, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.700, 10.100], loss: 0.004081, mae: 0.071206, mean_q: 0.375822
 15803/100000: episode: 184, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 8.554, mean reward: 0.342 [0.248, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.479], loss: 0.003667, mae: 0.067926, mean_q: 0.388718
 15819/100000: episode: 185, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 5.940, mean reward: 0.371 [0.193, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.300, 10.100], loss: 0.004078, mae: 0.072118, mean_q: 0.385518
 15842/100000: episode: 186, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 6.440, mean reward: 0.280 [0.163, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.344, 10.368], loss: 0.003824, mae: 0.070433, mean_q: 0.370635
 15857/100000: episode: 187, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 5.644, mean reward: 0.376 [0.316, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.399, 10.100], loss: 0.003452, mae: 0.066393, mean_q: 0.378537
 15904/100000: episode: 188, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 15.248, mean reward: 0.324 [0.183, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.196, 10.368], loss: 0.004007, mae: 0.070543, mean_q: 0.388354
 15951/100000: episode: 189, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 12.255, mean reward: 0.261 [0.067, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.331, 10.100], loss: 0.003807, mae: 0.068982, mean_q: 0.385407
 15978/100000: episode: 190, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 11.109, mean reward: 0.411 [0.324, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.436, 10.432], loss: 0.004290, mae: 0.074439, mean_q: 0.405169
 15990/100000: episode: 191, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 4.730, mean reward: 0.394 [0.296, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.562], loss: 0.003640, mae: 0.067217, mean_q: 0.382368
 16017/100000: episode: 192, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 11.204, mean reward: 0.415 [0.278, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.093, 10.524], loss: 0.003270, mae: 0.065034, mean_q: 0.379991
 16038/100000: episode: 193, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 6.933, mean reward: 0.330 [0.218, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.213, 10.100], loss: 0.003310, mae: 0.063987, mean_q: 0.401959
 16053/100000: episode: 194, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 5.615, mean reward: 0.374 [0.228, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.100, 10.100], loss: 0.003665, mae: 0.068950, mean_q: 0.399137
 16080/100000: episode: 195, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 8.441, mean reward: 0.313 [0.207, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.664, 10.398], loss: 0.003776, mae: 0.069297, mean_q: 0.399892
 16103/100000: episode: 196, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 5.450, mean reward: 0.237 [0.033, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.059, 10.150], loss: 0.004225, mae: 0.071588, mean_q: 0.403735
 16136/100000: episode: 197, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 9.399, mean reward: 0.285 [0.164, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.398, 10.449], loss: 0.003651, mae: 0.067945, mean_q: 0.397466
 16169/100000: episode: 198, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 10.319, mean reward: 0.313 [0.210, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.088, 10.338], loss: 0.004185, mae: 0.072307, mean_q: 0.405206
 16194/100000: episode: 199, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 6.414, mean reward: 0.257 [0.108, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.115, 10.262], loss: 0.004130, mae: 0.070740, mean_q: 0.405664
 16217/100000: episode: 200, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 9.313, mean reward: 0.405 [0.311, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.315, 10.485], loss: 0.004150, mae: 0.070306, mean_q: 0.402912
 16264/100000: episode: 201, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 9.601, mean reward: 0.204 [0.028, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.524, 10.100], loss: 0.003687, mae: 0.067481, mean_q: 0.406553
 16287/100000: episode: 202, duration: 0.151s, episode steps: 23, steps per second: 152, episode reward: 7.461, mean reward: 0.324 [0.231, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.450], loss: 0.003711, mae: 0.068328, mean_q: 0.409900
 16312/100000: episode: 203, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 7.559, mean reward: 0.302 [0.213, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.564, 10.362], loss: 0.004257, mae: 0.071793, mean_q: 0.404512
 16327/100000: episode: 204, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 6.963, mean reward: 0.464 [0.376, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.286, 10.100], loss: 0.003908, mae: 0.068891, mean_q: 0.412836
 16374/100000: episode: 205, duration: 0.274s, episode steps: 47, steps per second: 172, episode reward: 11.063, mean reward: 0.235 [0.065, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.773, 10.100], loss: 0.003824, mae: 0.069525, mean_q: 0.416172
 16421/100000: episode: 206, duration: 0.284s, episode steps: 47, steps per second: 166, episode reward: 19.065, mean reward: 0.406 [0.139, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.284, 10.222], loss: 0.003946, mae: 0.069364, mean_q: 0.425569
 16448/100000: episode: 207, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 7.832, mean reward: 0.290 [0.221, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.210, 10.395], loss: 0.004271, mae: 0.071404, mean_q: 0.413065
 16475/100000: episode: 208, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 13.232, mean reward: 0.490 [0.300, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.218, 10.559], loss: 0.004962, mae: 0.078557, mean_q: 0.422674
 16500/100000: episode: 209, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 12.167, mean reward: 0.487 [0.319, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.428, 10.520], loss: 0.004639, mae: 0.075967, mean_q: 0.425737
 16527/100000: episode: 210, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 7.582, mean reward: 0.281 [0.186, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.187, 10.361], loss: 0.004821, mae: 0.076850, mean_q: 0.428275
 16574/100000: episode: 211, duration: 0.247s, episode steps: 47, steps per second: 191, episode reward: 12.795, mean reward: 0.272 [0.093, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.721, 10.325], loss: 0.003844, mae: 0.067270, mean_q: 0.430593
 16586/100000: episode: 212, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 4.655, mean reward: 0.388 [0.275, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.285, 10.483], loss: 0.004515, mae: 0.074609, mean_q: 0.419598
 16607/100000: episode: 213, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 6.408, mean reward: 0.305 [0.188, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.428, 10.100], loss: 0.003947, mae: 0.069786, mean_q: 0.449317
 16622/100000: episode: 214, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 5.864, mean reward: 0.391 [0.337, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.521, 10.100], loss: 0.003520, mae: 0.066775, mean_q: 0.430760
 16623/100000: episode: 215, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.462], loss: 0.003790, mae: 0.062483, mean_q: 0.408476
 16648/100000: episode: 216, duration: 0.159s, episode steps: 25, steps per second: 158, episode reward: 10.406, mean reward: 0.416 [0.331, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.699], loss: 0.003997, mae: 0.070229, mean_q: 0.420405
 16660/100000: episode: 217, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.400, mean reward: 0.283 [0.230, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.334], loss: 0.003270, mae: 0.065227, mean_q: 0.435788
 16683/100000: episode: 218, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 6.671, mean reward: 0.290 [0.145, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.235, 10.360], loss: 0.003744, mae: 0.065729, mean_q: 0.435013
 16730/100000: episode: 219, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 16.410, mean reward: 0.349 [0.176, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.379, 10.340], loss: 0.004024, mae: 0.070160, mean_q: 0.431319
 16745/100000: episode: 220, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 5.921, mean reward: 0.395 [0.315, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.365, 10.100], loss: 0.004177, mae: 0.071722, mean_q: 0.437828
 16792/100000: episode: 221, duration: 0.259s, episode steps: 47, steps per second: 182, episode reward: 11.400, mean reward: 0.243 [0.050, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.336, 10.100], loss: 0.003948, mae: 0.070166, mean_q: 0.436480
 16817/100000: episode: 222, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 10.835, mean reward: 0.433 [0.342, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.087, 10.530], loss: 0.004711, mae: 0.075940, mean_q: 0.442249
 16840/100000: episode: 223, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 7.034, mean reward: 0.306 [0.189, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.230, 10.461], loss: 0.003979, mae: 0.070216, mean_q: 0.445841
 16841/100000: episode: 224, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.440, mean reward: 0.440 [0.440, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.035, 10.521], loss: 0.004809, mae: 0.080863, mean_q: 0.446674
 16888/100000: episode: 225, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 14.684, mean reward: 0.312 [0.073, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.917, 10.202], loss: 0.004110, mae: 0.071266, mean_q: 0.445810
 16911/100000: episode: 226, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 7.356, mean reward: 0.320 [0.224, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.263, 10.498], loss: 0.003782, mae: 0.067315, mean_q: 0.451988
 16958/100000: episode: 227, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 8.810, mean reward: 0.187 [0.056, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.592, 10.100], loss: 0.003781, mae: 0.067859, mean_q: 0.451449
 16959/100000: episode: 228, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.410, mean reward: 0.410 [0.410, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.035, 10.492], loss: 0.003251, mae: 0.065183, mean_q: 0.472619
 16974/100000: episode: 229, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 5.683, mean reward: 0.379 [0.210, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.472, 10.100], loss: 0.003790, mae: 0.068637, mean_q: 0.446579
 16995/100000: episode: 230, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 4.852, mean reward: 0.231 [0.085, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.133, 10.100], loss: 0.003882, mae: 0.071190, mean_q: 0.439544
 17042/100000: episode: 231, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 13.150, mean reward: 0.280 [0.175, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.267, 10.343], loss: 0.004478, mae: 0.073971, mean_q: 0.448177
 17067/100000: episode: 232, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: 7.602, mean reward: 0.304 [0.169, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.407], loss: 0.003648, mae: 0.068493, mean_q: 0.465719
 17100/100000: episode: 233, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 15.213, mean reward: 0.461 [0.374, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.639, 10.559], loss: 0.003745, mae: 0.067603, mean_q: 0.458143
 17147/100000: episode: 234, duration: 0.282s, episode steps: 47, steps per second: 167, episode reward: 10.475, mean reward: 0.223 [0.081, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.263, 10.202], loss: 0.003694, mae: 0.067312, mean_q: 0.459796
 17180/100000: episode: 235, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 12.260, mean reward: 0.372 [0.287, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.199, 10.342], loss: 0.004016, mae: 0.069582, mean_q: 0.465578
 17227/100000: episode: 236, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 9.273, mean reward: 0.197 [0.045, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.736, 10.100], loss: 0.003801, mae: 0.068151, mean_q: 0.465863
 17248/100000: episode: 237, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 6.914, mean reward: 0.329 [0.216, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.579, 10.100], loss: 0.003761, mae: 0.067276, mean_q: 0.462671
 17263/100000: episode: 238, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 6.342, mean reward: 0.423 [0.362, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.243, 10.100], loss: 0.003818, mae: 0.069062, mean_q: 0.443867
 17290/100000: episode: 239, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 9.794, mean reward: 0.363 [0.286, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.357, 10.484], loss: 0.003879, mae: 0.068354, mean_q: 0.453587
[Info] New level: 0.9015417695045471 | Considering 10/90 traces
 17291/100000: episode: 240, duration: 3.974s, episode steps: 1, steps per second: 0, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.410], loss: 0.006513, mae: 0.095416, mean_q: 0.454420
 17309/100000: episode: 241, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 9.154, mean reward: 0.509 [0.415, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.818, 10.511], loss: 0.003738, mae: 0.067374, mean_q: 0.463623
 17322/100000: episode: 242, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 4.025, mean reward: 0.310 [0.244, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.209, 10.100], loss: 0.003245, mae: 0.062927, mean_q: 0.453778
 17344/100000: episode: 243, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 9.230, mean reward: 0.420 [0.286, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.579, 10.393], loss: 0.003784, mae: 0.066647, mean_q: 0.453268
 17361/100000: episode: 244, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 7.811, mean reward: 0.459 [0.346, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.548], loss: 0.005827, mae: 0.084647, mean_q: 0.476508
 17379/100000: episode: 245, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 6.656, mean reward: 0.370 [0.270, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.382], loss: 0.003905, mae: 0.068681, mean_q: 0.479108
 17400/100000: episode: 246, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 10.018, mean reward: 0.477 [0.339, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.497, 10.492], loss: 0.003825, mae: 0.068572, mean_q: 0.458227
 17413/100000: episode: 247, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 5.865, mean reward: 0.451 [0.387, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.580, 10.100], loss: 0.003652, mae: 0.067287, mean_q: 0.481884
 17426/100000: episode: 248, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 6.196, mean reward: 0.477 [0.356, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.648], loss: 0.003584, mae: 0.067293, mean_q: 0.479028
[RESULT] FALSIFICATION!
 17430/100000: episode: 249, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 11.597, mean reward: 2.899 [0.521, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.016, 10.414], loss: 0.003935, mae: 0.071711, mean_q: 0.442221
 17451/100000: episode: 250, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 9.845, mean reward: 0.469 [0.398, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.377, 10.474], loss: 0.003613, mae: 0.067472, mean_q: 0.471361
 17464/100000: episode: 251, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 4.754, mean reward: 0.366 [0.304, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.380], loss: 0.003826, mae: 0.068809, mean_q: 0.485437
 17486/100000: episode: 252, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 9.460, mean reward: 0.430 [0.341, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.429, 10.538], loss: 0.003100, mae: 0.062776, mean_q: 0.489810
 17503/100000: episode: 253, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 8.689, mean reward: 0.511 [0.413, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.341, 10.580], loss: 0.003951, mae: 0.072108, mean_q: 0.477258
 17516/100000: episode: 254, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 5.756, mean reward: 0.443 [0.316, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.284, 10.100], loss: 0.004605, mae: 0.076636, mean_q: 0.484806
 17534/100000: episode: 255, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 6.945, mean reward: 0.386 [0.300, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.411], loss: 0.004317, mae: 0.073707, mean_q: 0.483973
 17547/100000: episode: 256, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 6.907, mean reward: 0.531 [0.427, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.550], loss: 0.003780, mae: 0.067631, mean_q: 0.492701
 17566/100000: episode: 257, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 7.483, mean reward: 0.394 [0.292, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.398], loss: 0.003836, mae: 0.069295, mean_q: 0.507509
 17588/100000: episode: 258, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 8.123, mean reward: 0.369 [0.304, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.451], loss: 0.003926, mae: 0.071031, mean_q: 0.502769
 17601/100000: episode: 259, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 5.348, mean reward: 0.411 [0.317, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.307, 10.100], loss: 0.003766, mae: 0.068106, mean_q: 0.487053
 17619/100000: episode: 260, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 6.717, mean reward: 0.373 [0.254, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.467], loss: 0.003767, mae: 0.066213, mean_q: 0.514117
 17641/100000: episode: 261, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 8.721, mean reward: 0.396 [0.192, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.085, 10.312], loss: 0.003574, mae: 0.066948, mean_q: 0.510577
 17658/100000: episode: 262, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 8.880, mean reward: 0.522 [0.385, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.553], loss: 0.003108, mae: 0.061728, mean_q: 0.501068
 17680/100000: episode: 263, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 11.363, mean reward: 0.517 [0.425, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.764, 10.706], loss: 0.003483, mae: 0.065123, mean_q: 0.510689
 17697/100000: episode: 264, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 8.269, mean reward: 0.486 [0.366, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.577], loss: 0.003679, mae: 0.063380, mean_q: 0.520947
 17722/100000: episode: 265, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 9.964, mean reward: 0.399 [0.273, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.419], loss: 0.058436, mae: 0.107218, mean_q: 0.534283
 17744/100000: episode: 266, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 9.990, mean reward: 0.454 [0.346, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.617], loss: 0.003869, mae: 0.068817, mean_q: 0.506905
 17761/100000: episode: 267, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 6.383, mean reward: 0.375 [0.307, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.452, 10.454], loss: 0.004639, mae: 0.074756, mean_q: 0.526579
 17774/100000: episode: 268, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 5.280, mean reward: 0.406 [0.371, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.316, 10.100], loss: 0.100865, mae: 0.093889, mean_q: 0.536297
 17795/100000: episode: 269, duration: 0.125s, episode steps: 21, steps per second: 169, episode reward: 9.387, mean reward: 0.447 [0.300, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.470], loss: 0.070449, mae: 0.121704, mean_q: 0.540570
 17820/100000: episode: 270, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 10.684, mean reward: 0.427 [0.316, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.172, 10.501], loss: 0.006435, mae: 0.087971, mean_q: 0.507496
 17838/100000: episode: 271, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 7.412, mean reward: 0.412 [0.202, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.456], loss: 0.004053, mae: 0.071334, mean_q: 0.529342
[RESULT] FALSIFICATION!
 17853/100000: episode: 272, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 18.196, mean reward: 1.213 [0.441, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.138, 10.726], loss: 0.004158, mae: 0.071119, mean_q: 0.551614
 17872/100000: episode: 273, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 8.797, mean reward: 0.463 [0.404, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.801, 10.582], loss: 0.069412, mae: 0.085342, mean_q: 0.534994
 17889/100000: episode: 274, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 8.953, mean reward: 0.527 [0.393, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.679], loss: 0.006582, mae: 0.092576, mean_q: 0.547378
 17911/100000: episode: 275, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 8.855, mean reward: 0.402 [0.318, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.485, 10.562], loss: 0.063309, mae: 0.107661, mean_q: 0.559097
 17924/100000: episode: 276, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 4.452, mean reward: 0.342 [0.240, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.559, 10.100], loss: 0.104303, mae: 0.121472, mean_q: 0.551131
 17946/100000: episode: 277, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 12.670, mean reward: 0.576 [0.462, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.589, 10.610], loss: 0.006222, mae: 0.089344, mean_q: 0.550807
 17965/100000: episode: 278, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 7.431, mean reward: 0.391 [0.248, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.412], loss: 0.003836, mae: 0.067244, mean_q: 0.555570
 17983/100000: episode: 279, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 9.381, mean reward: 0.521 [0.433, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.663], loss: 0.075191, mae: 0.104552, mean_q: 0.560575
 18004/100000: episode: 280, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 9.526, mean reward: 0.454 [0.326, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.082, 10.481], loss: 0.005368, mae: 0.079858, mean_q: 0.558523
 18017/100000: episode: 281, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 5.219, mean reward: 0.401 [0.327, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.623, 10.100], loss: 0.004267, mae: 0.071551, mean_q: 0.541266
 18030/100000: episode: 282, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 6.118, mean reward: 0.471 [0.403, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.535], loss: 0.003860, mae: 0.070954, mean_q: 0.544946
 18055/100000: episode: 283, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 10.311, mean reward: 0.412 [0.209, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.699, 10.457], loss: 0.003909, mae: 0.068561, mean_q: 0.542984
 18068/100000: episode: 284, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 4.737, mean reward: 0.364 [0.222, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.297, 10.100], loss: 0.003347, mae: 0.062327, mean_q: 0.553056
 18089/100000: episode: 285, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 9.422, mean reward: 0.449 [0.370, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.473], loss: 0.003674, mae: 0.065885, mean_q: 0.558478
 18111/100000: episode: 286, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 9.451, mean reward: 0.430 [0.337, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.139, 10.519], loss: 0.003649, mae: 0.065865, mean_q: 0.546994
 18129/100000: episode: 287, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 8.595, mean reward: 0.478 [0.370, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.079, 10.418], loss: 0.003627, mae: 0.067860, mean_q: 0.548798
 18150/100000: episode: 288, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 10.605, mean reward: 0.505 [0.444, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.169, 10.550], loss: 0.007187, mae: 0.095738, mean_q: 0.570450
 18163/100000: episode: 289, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 7.515, mean reward: 0.578 [0.496, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.417, 10.709], loss: 0.102334, mae: 0.106651, mean_q: 0.601627
 18176/100000: episode: 290, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 3.987, mean reward: 0.307 [0.197, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.680, 10.463], loss: 0.004617, mae: 0.073810, mean_q: 0.583358
 18189/100000: episode: 291, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 6.171, mean reward: 0.475 [0.402, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.493, 10.100], loss: 0.004944, mae: 0.076204, mean_q: 0.552975
 18202/100000: episode: 292, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 5.767, mean reward: 0.444 [0.357, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.246, 10.100], loss: 0.003948, mae: 0.069812, mean_q: 0.584164
 18224/100000: episode: 293, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 9.258, mean reward: 0.421 [0.293, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.535], loss: 0.005122, mae: 0.074675, mean_q: 0.574770
 18242/100000: episode: 294, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 7.552, mean reward: 0.420 [0.196, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.491], loss: 0.004466, mae: 0.071111, mean_q: 0.558204
 18259/100000: episode: 295, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 7.896, mean reward: 0.464 [0.316, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.073, 10.461], loss: 0.004637, mae: 0.070849, mean_q: 0.577543
 18272/100000: episode: 296, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 5.937, mean reward: 0.457 [0.414, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.253, 10.494], loss: 0.004273, mae: 0.071259, mean_q: 0.552997
 18294/100000: episode: 297, duration: 0.147s, episode steps: 22, steps per second: 150, episode reward: 10.380, mean reward: 0.472 [0.319, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.237, 10.462], loss: 0.004262, mae: 0.068625, mean_q: 0.574861
 18307/100000: episode: 298, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 7.075, mean reward: 0.544 [0.471, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.685], loss: 0.003802, mae: 0.065556, mean_q: 0.561232
 18320/100000: episode: 299, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 5.782, mean reward: 0.445 [0.356, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.435, 10.100], loss: 0.004711, mae: 0.071100, mean_q: 0.577492
 18338/100000: episode: 300, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 8.860, mean reward: 0.492 [0.416, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.703, 10.606], loss: 0.004071, mae: 0.067857, mean_q: 0.574536
 18363/100000: episode: 301, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 10.644, mean reward: 0.426 [0.223, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.570, 10.359], loss: 0.105180, mae: 0.103898, mean_q: 0.612881
 18380/100000: episode: 302, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 6.949, mean reward: 0.409 [0.331, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.474], loss: 0.004874, mae: 0.071966, mean_q: 0.588281
 18405/100000: episode: 303, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 11.687, mean reward: 0.467 [0.375, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.575], loss: 0.005143, mae: 0.075672, mean_q: 0.585156
 18430/100000: episode: 304, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 7.565, mean reward: 0.303 [0.164, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.311], loss: 0.003832, mae: 0.065900, mean_q: 0.602431
 18448/100000: episode: 305, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 8.400, mean reward: 0.467 [0.412, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.652, 10.391], loss: 0.075256, mae: 0.106612, mean_q: 0.587996
 18467/100000: episode: 306, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 8.320, mean reward: 0.438 [0.322, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.774, 10.487], loss: 0.005299, mae: 0.076264, mean_q: 0.608910
 18484/100000: episode: 307, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 7.772, mean reward: 0.457 [0.369, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.302, 10.383], loss: 0.004596, mae: 0.070904, mean_q: 0.595727
 18501/100000: episode: 308, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 7.386, mean reward: 0.434 [0.377, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.473], loss: 0.004414, mae: 0.072293, mean_q: 0.598497
 18522/100000: episode: 309, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 7.131, mean reward: 0.340 [0.188, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.188, 10.368], loss: 0.003810, mae: 0.066317, mean_q: 0.603086
 18543/100000: episode: 310, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 9.890, mean reward: 0.471 [0.397, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.671, 10.586], loss: 0.004556, mae: 0.071361, mean_q: 0.602997
 18556/100000: episode: 311, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 5.534, mean reward: 0.426 [0.348, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.380, 10.100], loss: 0.004255, mae: 0.069701, mean_q: 0.582134
 18577/100000: episode: 312, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 8.976, mean reward: 0.427 [0.310, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.457], loss: 0.004284, mae: 0.071324, mean_q: 0.595323
 18598/100000: episode: 313, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 6.693, mean reward: 0.319 [0.158, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.237], loss: 0.004135, mae: 0.069807, mean_q: 0.618789
 18611/100000: episode: 314, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 5.867, mean reward: 0.451 [0.405, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.451, 10.100], loss: 0.003706, mae: 0.064401, mean_q: 0.630137
 18636/100000: episode: 315, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 11.019, mean reward: 0.441 [0.336, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.784, 10.452], loss: 0.004213, mae: 0.069620, mean_q: 0.609584
 18655/100000: episode: 316, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 8.953, mean reward: 0.471 [0.435, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.854, 10.583], loss: 0.003962, mae: 0.067067, mean_q: 0.590768
 18677/100000: episode: 317, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 9.741, mean reward: 0.443 [0.309, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.681, 10.578], loss: 0.004058, mae: 0.065364, mean_q: 0.614729
 18690/100000: episode: 318, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 6.126, mean reward: 0.471 [0.394, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.452], loss: 0.004596, mae: 0.071446, mean_q: 0.596389
 18712/100000: episode: 319, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 10.138, mean reward: 0.461 [0.319, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.484, 10.474], loss: 0.004192, mae: 0.067069, mean_q: 0.630316
 18733/100000: episode: 320, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 11.766, mean reward: 0.560 [0.437, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.117, 10.597], loss: 0.003948, mae: 0.066713, mean_q: 0.616964
 18746/100000: episode: 321, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 4.882, mean reward: 0.376 [0.187, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.636, 10.100], loss: 0.004211, mae: 0.068676, mean_q: 0.606356
 18764/100000: episode: 322, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 8.260, mean reward: 0.459 [0.382, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.553], loss: 0.076547, mae: 0.107283, mean_q: 0.635647
 18789/100000: episode: 323, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 11.399, mean reward: 0.456 [0.296, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.104, 10.472], loss: 0.056784, mae: 0.096642, mean_q: 0.630935
 18802/100000: episode: 324, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 5.874, mean reward: 0.452 [0.320, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.793, 10.475], loss: 0.008267, mae: 0.102647, mean_q: 0.664621
 18815/100000: episode: 325, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.563, mean reward: 0.428 [0.361, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.432, 10.466], loss: 0.099203, mae: 0.090872, mean_q: 0.614848
 18837/100000: episode: 326, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 8.450, mean reward: 0.384 [0.303, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.627], loss: 0.006071, mae: 0.087604, mean_q: 0.646151
 18862/100000: episode: 327, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 9.128, mean reward: 0.365 [0.205, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.767, 10.388], loss: 0.054772, mae: 0.091861, mean_q: 0.645421
[RESULT] FALSIFICATION!
 18869/100000: episode: 328, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 13.432, mean reward: 1.919 [0.501, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.828, 10.050], loss: 0.186526, mae: 0.158268, mean_q: 0.702648
 18887/100000: episode: 329, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 9.351, mean reward: 0.519 [0.440, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.147, 10.674], loss: 0.006116, mae: 0.087013, mean_q: 0.625256
[Info] New level: 1.101162075996399 | Considering 10/90 traces
 18900/100000: episode: 330, duration: 4.027s, episode steps: 13, steps per second: 3, episode reward: 5.528, mean reward: 0.425 [0.332, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.603, 10.100], loss: 0.004516, mae: 0.072056, mean_q: 0.623560
 18912/100000: episode: 331, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 5.500, mean reward: 0.458 [0.368, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.962, 10.478], loss: 0.003995, mae: 0.068958, mean_q: 0.641025
 18927/100000: episode: 332, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 7.555, mean reward: 0.504 [0.423, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.097, 10.577], loss: 0.088570, mae: 0.104023, mean_q: 0.656319
 18942/100000: episode: 333, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 8.114, mean reward: 0.541 [0.438, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-1.075, 10.609], loss: 0.004369, mae: 0.070319, mean_q: 0.634623
 18957/100000: episode: 334, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 6.979, mean reward: 0.465 [0.371, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.420], loss: 0.004057, mae: 0.066264, mean_q: 0.658219
 18968/100000: episode: 335, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 6.382, mean reward: 0.580 [0.509, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.040, 10.592], loss: 0.003995, mae: 0.068506, mean_q: 0.669161
 18980/100000: episode: 336, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 6.822, mean reward: 0.569 [0.480, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.684, 10.684], loss: 0.003537, mae: 0.062435, mean_q: 0.645905
 18989/100000: episode: 337, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 5.016, mean reward: 0.557 [0.461, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.351, 10.570], loss: 0.144444, mae: 0.116529, mean_q: 0.670888
 19004/100000: episode: 338, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 7.187, mean reward: 0.479 [0.374, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.454], loss: 0.005710, mae: 0.083870, mean_q: 0.665315
 19017/100000: episode: 339, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 6.676, mean reward: 0.514 [0.431, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.595], loss: 0.101283, mae: 0.108027, mean_q: 0.678283
 19029/100000: episode: 340, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 7.684, mean reward: 0.640 [0.568, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.724], loss: 0.005311, mae: 0.081320, mean_q: 0.641623
 19041/100000: episode: 341, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 5.885, mean reward: 0.490 [0.433, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.568], loss: 0.004551, mae: 0.072712, mean_q: 0.637901
 19052/100000: episode: 342, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 6.269, mean reward: 0.570 [0.511, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.540], loss: 0.118139, mae: 0.113546, mean_q: 0.695021
 19063/100000: episode: 343, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 6.149, mean reward: 0.559 [0.457, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.668], loss: 0.006434, mae: 0.089685, mean_q: 0.680803
 19078/100000: episode: 344, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 7.279, mean reward: 0.485 [0.450, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.109, 10.468], loss: 0.171912, mae: 0.131609, mean_q: 0.667154
 19093/100000: episode: 345, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 8.729, mean reward: 0.582 [0.474, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.628], loss: 0.005924, mae: 0.083515, mean_q: 0.693031
 19111/100000: episode: 346, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 9.319, mean reward: 0.518 [0.430, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.071, 10.512], loss: 0.004546, mae: 0.072235, mean_q: 0.675532
[RESULT] FALSIFICATION!
 19120/100000: episode: 347, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 14.760, mean reward: 1.640 [0.488, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.287, 10.591], loss: 0.003990, mae: 0.068432, mean_q: 0.678862
[RESULT] FALSIFICATION!
 19124/100000: episode: 348, duration: 0.024s, episode steps: 4, steps per second: 163, episode reward: 11.915, mean reward: 2.979 [0.600, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.013, 10.712], loss: 0.006887, mae: 0.101217, mean_q: 0.745414
 19135/100000: episode: 349, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 6.220, mean reward: 0.565 [0.512, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.635], loss: 0.005510, mae: 0.078879, mean_q: 0.687696
 19150/100000: episode: 350, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 6.506, mean reward: 0.434 [0.364, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.524], loss: 0.004090, mae: 0.065670, mean_q: 0.692703
 19159/100000: episode: 351, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 5.329, mean reward: 0.592 [0.492, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.603], loss: 0.008576, mae: 0.097610, mean_q: 0.727328
 19172/100000: episode: 352, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 6.668, mean reward: 0.513 [0.429, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.318, 10.479], loss: 0.099545, mae: 0.103707, mean_q: 0.693337
 19181/100000: episode: 353, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 5.154, mean reward: 0.573 [0.521, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.624], loss: 0.005974, mae: 0.081215, mean_q: 0.681879
[RESULT] FALSIFICATION!
 19186/100000: episode: 354, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 12.674, mean reward: 2.535 [0.647, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.013, 10.701], loss: 0.003565, mae: 0.064941, mean_q: 0.686923
 19195/100000: episode: 355, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 5.209, mean reward: 0.579 [0.510, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.635], loss: 0.005076, mae: 0.071450, mean_q: 0.665467
 19206/100000: episode: 356, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 6.209, mean reward: 0.564 [0.497, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.341, 10.681], loss: 0.004733, mae: 0.073337, mean_q: 0.700907
 19218/100000: episode: 357, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 5.770, mean reward: 0.481 [0.362, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.561], loss: 0.207003, mae: 0.116837, mean_q: 0.691004
 19227/100000: episode: 358, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 5.395, mean reward: 0.599 [0.537, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.358, 10.723], loss: 0.018080, mae: 0.158851, mean_q: 0.717545
 19240/100000: episode: 359, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 6.363, mean reward: 0.489 [0.356, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.576], loss: 0.008313, mae: 0.100240, mean_q: 0.711568
 19252/100000: episode: 360, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 6.154, mean reward: 0.513 [0.463, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.480], loss: 0.213795, mae: 0.147953, mean_q: 0.736506
[RESULT] FALSIFICATION!
 19260/100000: episode: 361, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 14.306, mean reward: 1.788 [0.535, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.246, 10.776], loss: 0.006514, mae: 0.086524, mean_q: 0.704676
 19275/100000: episode: 362, duration: 0.092s, episode steps: 15, steps per second: 162, episode reward: 7.949, mean reward: 0.530 [0.470, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.283, 10.501], loss: 0.005473, mae: 0.079280, mean_q: 0.700273
 19286/100000: episode: 363, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 5.800, mean reward: 0.527 [0.463, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.596], loss: 0.004278, mae: 0.068830, mean_q: 0.725841
 19299/100000: episode: 364, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 6.591, mean reward: 0.507 [0.413, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.552], loss: 0.102913, mae: 0.114100, mean_q: 0.736749
 19308/100000: episode: 365, duration: 0.062s, episode steps: 9, steps per second: 144, episode reward: 5.490, mean reward: 0.610 [0.528, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.071, 10.700], loss: 0.145488, mae: 0.118912, mean_q: 0.735517
 19323/100000: episode: 366, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 8.596, mean reward: 0.573 [0.516, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.742], loss: 0.004917, mae: 0.074976, mean_q: 0.707268
 19336/100000: episode: 367, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 6.690, mean reward: 0.515 [0.434, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.084, 10.507], loss: 0.099541, mae: 0.091194, mean_q: 0.707082
[RESULT] FALSIFICATION!
 19344/100000: episode: 368, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 14.280, mean reward: 1.785 [0.577, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.019, 10.266], loss: 0.007676, mae: 0.094317, mean_q: 0.717951
 19355/100000: episode: 369, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 7.086, mean reward: 0.644 [0.575, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.721], loss: 0.120811, mae: 0.120098, mean_q: 0.740860
 19366/100000: episode: 370, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 6.051, mean reward: 0.550 [0.516, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-2.073, 10.607], loss: 0.007850, mae: 0.094070, mean_q: 0.747489
 19378/100000: episode: 371, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 6.706, mean reward: 0.559 [0.478, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.683], loss: 0.105373, mae: 0.115007, mean_q: 0.737363
[RESULT] FALSIFICATION!
 19391/100000: episode: 372, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 16.743, mean reward: 1.288 [0.433, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.038, 10.799], loss: 0.101631, mae: 0.129532, mean_q: 0.733689
 19400/100000: episode: 373, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 5.230, mean reward: 0.581 [0.469, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.634], loss: 0.006866, mae: 0.091414, mean_q: 0.689698
 19412/100000: episode: 374, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 5.706, mean reward: 0.475 [0.351, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.571], loss: 0.006448, mae: 0.083394, mean_q: 0.712039
 19424/100000: episode: 375, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 6.911, mean reward: 0.576 [0.492, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.580, 10.664], loss: 0.110307, mae: 0.106206, mean_q: 0.734145
 19436/100000: episode: 376, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 6.427, mean reward: 0.536 [0.437, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.632], loss: 0.005436, mae: 0.079703, mean_q: 0.705514
[RESULT] FALSIFICATION!
 19447/100000: episode: 377, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 15.942, mean reward: 1.449 [0.548, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.162, 10.724], loss: 0.003640, mae: 0.068896, mean_q: 0.724023
 19458/100000: episode: 378, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 5.765, mean reward: 0.524 [0.463, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.854, 10.689], loss: 0.120662, mae: 0.099916, mean_q: 0.713567
 19470/100000: episode: 379, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 6.308, mean reward: 0.526 [0.467, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.723], loss: 0.012738, mae: 0.130527, mean_q: 0.730377
 19482/100000: episode: 380, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 6.364, mean reward: 0.530 [0.410, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.580, 10.429], loss: 0.006290, mae: 0.084746, mean_q: 0.734194
 19494/100000: episode: 381, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 5.595, mean reward: 0.466 [0.369, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.304, 10.466], loss: 0.110318, mae: 0.096844, mean_q: 0.731440
 19512/100000: episode: 382, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 9.090, mean reward: 0.505 [0.453, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.954, 10.575], loss: 0.145233, mae: 0.119413, mean_q: 0.766077
[RESULT] FALSIFICATION!
 19521/100000: episode: 383, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 14.517, mean reward: 1.613 [0.521, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.872, 10.486], loss: 0.143205, mae: 0.124949, mean_q: 0.740185
[RESULT] FALSIFICATION!
 19533/100000: episode: 384, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 16.884, mean reward: 1.407 [0.559, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.328, 10.607], loss: 0.111253, mae: 0.150495, mean_q: 0.744382
 19544/100000: episode: 385, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 6.397, mean reward: 0.582 [0.549, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.626], loss: 0.006695, mae: 0.083698, mean_q: 0.762243
 19553/100000: episode: 386, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 5.427, mean reward: 0.603 [0.548, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.776, 10.586], loss: 0.143308, mae: 0.123787, mean_q: 0.797291
[RESULT] FALSIFICATION!
 19568/100000: episode: 387, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 17.622, mean reward: 1.175 [0.435, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.489], loss: 0.091303, mae: 0.109358, mean_q: 0.769277
 19580/100000: episode: 388, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 5.845, mean reward: 0.487 [0.429, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.559], loss: 0.005314, mae: 0.077891, mean_q: 0.738517
 19598/100000: episode: 389, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 9.447, mean reward: 0.525 [0.454, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.677], loss: 0.142221, mae: 0.123273, mean_q: 0.773206
 19611/100000: episode: 390, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 6.480, mean reward: 0.498 [0.418, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.990, 10.381], loss: 0.007575, mae: 0.099695, mean_q: 0.726141
[RESULT] FALSIFICATION!
 19625/100000: episode: 391, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 17.601, mean reward: 1.257 [0.491, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.825, 10.678], loss: 0.097830, mae: 0.111160, mean_q: 0.768537
 19640/100000: episode: 392, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 7.303, mean reward: 0.487 [0.432, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.584], loss: 0.005906, mae: 0.080733, mean_q: 0.750783
 19653/100000: episode: 393, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 6.086, mean reward: 0.468 [0.440, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.232, 10.563], loss: 0.102235, mae: 0.104965, mean_q: 0.760916
 19662/100000: episode: 394, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 5.006, mean reward: 0.556 [0.493, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.579], loss: 0.139575, mae: 0.118920, mean_q: 0.757908
[RESULT] FALSIFICATION!
 19664/100000: episode: 395, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.677, mean reward: 5.339 [0.677, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.018, 10.383], loss: 0.015220, mae: 0.125172, mean_q: 0.851380
 19676/100000: episode: 396, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 5.766, mean reward: 0.480 [0.440, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.581], loss: 0.006830, mae: 0.090131, mean_q: 0.776842
 19687/100000: episode: 397, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 4.675, mean reward: 0.425 [0.364, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.906, 10.492], loss: 0.123423, mae: 0.114963, mean_q: 0.781809
 19698/100000: episode: 398, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 5.682, mean reward: 0.517 [0.476, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.721, 10.593], loss: 0.450501, mae: 0.223478, mean_q: 0.877257
 19713/100000: episode: 399, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 7.858, mean reward: 0.524 [0.421, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.511], loss: 0.093744, mae: 0.131204, mean_q: 0.801233
 19728/100000: episode: 400, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 9.497, mean reward: 0.633 [0.549, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.542, 10.656], loss: 0.005500, mae: 0.075852, mean_q: 0.779998
 19737/100000: episode: 401, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 5.635, mean reward: 0.626 [0.564, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.594], loss: 0.146061, mae: 0.132757, mean_q: 0.811587
 19749/100000: episode: 402, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 6.329, mean reward: 0.527 [0.470, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.586, 10.523], loss: 0.106031, mae: 0.120427, mean_q: 0.770369
 19764/100000: episode: 403, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 8.227, mean reward: 0.548 [0.457, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.639, 10.542], loss: 0.089887, mae: 0.100465, mean_q: 0.799790
 19776/100000: episode: 404, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 6.181, mean reward: 0.515 [0.431, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.438, 10.435], loss: 0.214263, mae: 0.155200, mean_q: 0.866356
 19788/100000: episode: 405, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 6.739, mean reward: 0.562 [0.468, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-1.263, 10.563], loss: 0.214747, mae: 0.179657, mean_q: 0.759223
 19799/100000: episode: 406, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 6.891, mean reward: 0.626 [0.533, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.707], loss: 0.015859, mae: 0.145047, mean_q: 0.768637
 19810/100000: episode: 407, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 6.178, mean reward: 0.562 [0.483, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.485], loss: 0.009068, mae: 0.101781, mean_q: 0.786817
 19825/100000: episode: 408, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 8.134, mean reward: 0.542 [0.466, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.995, 10.597], loss: 0.251966, mae: 0.163164, mean_q: 0.828560
 19837/100000: episode: 409, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 6.364, mean reward: 0.530 [0.485, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.118, 10.589], loss: 0.208172, mae: 0.127408, mean_q: 0.848660
 19849/100000: episode: 410, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 6.719, mean reward: 0.560 [0.477, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.564, 10.713], loss: 0.597856, mae: 0.271538, mean_q: 0.910201
 19860/100000: episode: 411, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 5.661, mean reward: 0.515 [0.473, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.626, 10.580], loss: 0.246162, mae: 0.199039, mean_q: 0.817775
 19871/100000: episode: 412, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 6.531, mean reward: 0.594 [0.557, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.442, 10.625], loss: 0.038091, mae: 0.235632, mean_q: 0.850170
 19884/100000: episode: 413, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 5.962, mean reward: 0.459 [0.373, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.582], loss: 0.011103, mae: 0.109890, mean_q: 0.828960
[RESULT] FALSIFICATION!
 19892/100000: episode: 414, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 14.293, mean reward: 1.787 [0.582, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.516, 10.590], loss: 0.160985, mae: 0.129565, mean_q: 0.782377
[RESULT] FALSIFICATION!
 19896/100000: episode: 415, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 11.847, mean reward: 2.962 [0.590, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.020, 10.311], loss: 0.302444, mae: 0.205912, mean_q: 0.885347
 19908/100000: episode: 416, duration: 0.084s, episode steps: 12, steps per second: 144, episode reward: 6.897, mean reward: 0.575 [0.502, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.687], loss: 0.007736, mae: 0.085670, mean_q: 0.787623
 19919/100000: episode: 417, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 5.618, mean reward: 0.511 [0.474, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.540], loss: 0.119138, mae: 0.115753, mean_q: 0.829229
 19930/100000: episode: 418, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 5.188, mean reward: 0.472 [0.404, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.509], loss: 0.007557, mae: 0.084801, mean_q: 0.822753
 19945/100000: episode: 419, duration: 0.084s, episode steps: 15, steps per second: 180, episode reward: 7.875, mean reward: 0.525 [0.473, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.470, 10.561], loss: 0.008043, mae: 0.085477, mean_q: 0.789231
[Info] New level: 1.3784236907958984 | Considering 10/90 traces
 19963/100000: episode: 420, duration: 4.068s, episode steps: 18, steps per second: 4, episode reward: 8.261, mean reward: 0.459 [0.372, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.047, 10.505], loss: 0.211376, mae: 0.129805, mean_q: 0.837059
[RESULT] FALSIFICATION!
 19964/100000: episode: 421, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.013, 10.590], loss: 0.006284, mae: 0.078988, mean_q: 0.872170
[RESULT] FALSIFICATION!
 19966/100000: episode: 422, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 10.673, mean reward: 5.337 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.008, 10.338], loss: 0.015811, mae: 0.133260, mean_q: 0.893526
 19974/100000: episode: 423, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.853, mean reward: 0.607 [0.559, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.644, 10.687], loss: 0.155584, mae: 0.114663, mean_q: 0.818031
[RESULT] FALSIFICATION!
 19978/100000: episode: 424, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.980, mean reward: 2.995 [0.625, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.190, 10.698], loss: 0.304032, mae: 0.154261, mean_q: 0.883572
[RESULT] FALSIFICATION!
 19980/100000: episode: 425, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.672, mean reward: 5.336 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.011, 10.285], loss: 0.586895, mae: 0.220480, mean_q: 0.799596
 19987/100000: episode: 426, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 4.281, mean reward: 0.612 [0.569, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.687], loss: 0.007915, mae: 0.092391, mean_q: 0.822642
[RESULT] FALSIFICATION!
 19990/100000: episode: 427, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 11.297, mean reward: 3.766 [0.643, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.011, 10.688], loss: 0.008279, mae: 0.099823, mean_q: 0.790857
 19994/100000: episode: 428, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.588, mean reward: 0.647 [0.634, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.567], loss: 0.317834, mae: 0.171654, mean_q: 0.836292
[RESULT] FALSIFICATION!
 19996/100000: episode: 429, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 10.625, mean reward: 5.312 [0.625, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.086, 10.234], loss: 0.010708, mae: 0.106330, mean_q: 0.873610
[RESULT] FALSIFICATION!
 20003/100000: episode: 430, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 13.793, mean reward: 1.970 [0.613, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.012, 10.449], loss: 0.359761, mae: 0.167196, mean_q: 0.765796
[RESULT] FALSIFICATION!
 20006/100000: episode: 431, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 11.374, mean reward: 3.791 [0.687, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.386, 10.570], loss: 0.035076, mae: 0.225369, mean_q: 0.972248
 20014/100000: episode: 432, duration: 0.055s, episode steps: 8, steps per second: 144, episode reward: 5.102, mean reward: 0.638 [0.599, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.620], loss: 0.171628, mae: 0.166244, mean_q: 0.787788
[RESULT] FALSIFICATION!
 20017/100000: episode: 433, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 11.263, mean reward: 3.754 [0.602, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.013, 10.255], loss: 0.401296, mae: 0.190556, mean_q: 0.857948
[RESULT] FALSIFICATION!
 20020/100000: episode: 434, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 11.382, mean reward: 3.794 [0.689, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.013, 10.639], loss: 0.020070, mae: 0.164316, mean_q: 0.935810
 20028/100000: episode: 435, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.775, mean reward: 0.597 [0.571, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.559], loss: 0.310436, mae: 0.145901, mean_q: 0.778262
[RESULT] FALSIFICATION!
 20037/100000: episode: 436, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 15.043, mean reward: 1.671 [0.574, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.181, 10.560], loss: 0.659717, mae: 0.366268, mean_q: 1.014462
 20043/100000: episode: 437, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 3.653, mean reward: 0.609 [0.566, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.102, 10.728], loss: 0.198383, mae: 0.132191, mean_q: 0.853125
 20052/100000: episode: 438, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 5.709, mean reward: 0.634 [0.599, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.751], loss: 0.006393, mae: 0.083168, mean_q: 0.833820
[RESULT] FALSIFICATION!
 20056/100000: episode: 439, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 11.952, mean reward: 2.988 [0.628, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.012, 10.632], loss: 0.005528, mae: 0.077216, mean_q: 0.807028
[RESULT] FALSIFICATION!
 20058/100000: episode: 440, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 10.618, mean reward: 5.309 [0.618, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.011, 10.491], loss: 0.637886, mae: 0.222229, mean_q: 0.786235
 20064/100000: episode: 441, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 4.018, mean reward: 0.670 [0.623, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.973, 10.678], loss: 0.211292, mae: 0.201372, mean_q: 0.923323
[RESULT] FALSIFICATION!
 20065/100000: episode: 442, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.014, 10.607], loss: 0.009085, mae: 0.104764, mean_q: 0.924254
 20069/100000: episode: 443, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 2.601, mean reward: 0.650 [0.646, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.629], loss: 0.626047, mae: 0.234024, mean_q: 0.749503
[RESULT] FALSIFICATION!
 20070/100000: episode: 444, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.014, 10.690], loss: 0.013622, mae: 0.125814, mean_q: 0.912753
 20081/100000: episode: 445, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 6.045, mean reward: 0.550 [0.480, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.381, 10.552], loss: 0.332931, mae: 0.215993, mean_q: 0.931110
 20092/100000: episode: 446, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 6.292, mean reward: 0.572 [0.495, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.580], loss: 0.550104, mae: 0.302341, mean_q: 0.985858
[RESULT] FALSIFICATION!
 20094/100000: episode: 447, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 10.637, mean reward: 5.318 [0.637, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.008, 10.162], loss: 0.006674, mae: 0.087421, mean_q: 0.902243
 20099/100000: episode: 448, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 2.942, mean reward: 0.588 [0.548, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.665], loss: 0.261381, mae: 0.177468, mean_q: 0.795530
 20103/100000: episode: 449, duration: 0.032s, episode steps: 4, steps per second: 123, episode reward: 2.542, mean reward: 0.636 [0.605, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.672], loss: 0.018284, mae: 0.160345, mean_q: 1.017228
 20109/100000: episode: 450, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 3.417, mean reward: 0.570 [0.546, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.292, 10.619], loss: 0.394266, mae: 0.180078, mean_q: 0.846638
 20120/100000: episode: 451, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 6.038, mean reward: 0.549 [0.495, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.587], loss: 0.224778, mae: 0.183098, mean_q: 0.903374
[RESULT] FALSIFICATION!
 20123/100000: episode: 452, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 11.347, mean reward: 3.782 [0.667, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.012, 10.690], loss: 0.014288, mae: 0.150412, mean_q: 0.757504
[RESULT] FALSIFICATION!
 20126/100000: episode: 453, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 11.351, mean reward: 3.784 [0.668, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.140, 10.471], loss: 0.015227, mae: 0.128843, mean_q: 0.932365
[RESULT] FALSIFICATION!
 20129/100000: episode: 454, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 11.298, mean reward: 3.766 [0.640, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.011, 10.624], loss: 0.015202, mae: 0.149397, mean_q: 0.971652
[RESULT] FALSIFICATION!
 20130/100000: episode: 455, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.011, 10.641], loss: 0.005834, mae: 0.071742, mean_q: 0.792503
 20136/100000: episode: 456, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 3.877, mean reward: 0.646 [0.601, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.716], loss: 0.012637, mae: 0.127592, mean_q: 0.761131
 20141/100000: episode: 457, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 3.215, mean reward: 0.643 [0.602, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.710], loss: 0.265361, mae: 0.182925, mean_q: 0.867426
 20149/100000: episode: 458, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 4.723, mean reward: 0.590 [0.567, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.691], loss: 0.312952, mae: 0.169471, mean_q: 0.843420
 20157/100000: episode: 459, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.733, mean reward: 0.592 [0.547, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.630], loss: 0.155158, mae: 0.148875, mean_q: 0.924031
 20165/100000: episode: 460, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.941, mean reward: 0.618 [0.538, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.662], loss: 0.153614, mae: 0.119024, mean_q: 0.847769
 20173/100000: episode: 461, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.671, mean reward: 0.584 [0.542, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.616], loss: 0.297139, mae: 0.153741, mean_q: 0.929155
 20184/100000: episode: 462, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 6.591, mean reward: 0.599 [0.500, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.266, 10.599], loss: 0.439242, mae: 0.211857, mean_q: 0.884338
[RESULT] FALSIFICATION!
 20185/100000: episode: 463, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.011, 10.666], loss: 1.202209, mae: 0.434794, mean_q: 1.015581
 20190/100000: episode: 464, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 3.242, mean reward: 0.648 [0.636, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.354, 10.676], loss: 0.685518, mae: 0.313149, mean_q: 0.976142
 20201/100000: episode: 465, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 7.017, mean reward: 0.638 [0.565, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.854, 10.678], loss: 0.745809, mae: 0.416113, mean_q: 1.069696
[RESULT] FALSIFICATION!
 20203/100000: episode: 466, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.643, mean reward: 5.321 [0.643, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.014, 10.582], loss: 0.015915, mae: 0.161975, mean_q: 0.718341
 20208/100000: episode: 467, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 3.081, mean reward: 0.616 [0.579, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.524], loss: 0.022357, mae: 0.187429, mean_q: 0.715149
[RESULT] FALSIFICATION!
 20216/100000: episode: 468, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 14.295, mean reward: 1.787 [0.565, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.013, 10.672], loss: 0.014295, mae: 0.123754, mean_q: 0.928719
 20223/100000: episode: 469, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 4.125, mean reward: 0.589 [0.481, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.580], loss: 0.008681, mae: 0.086631, mean_q: 0.865014
 20228/100000: episode: 470, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 3.197, mean reward: 0.639 [0.626, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.617], loss: 0.473767, mae: 0.229935, mean_q: 0.962817
 20239/100000: episode: 471, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 6.183, mean reward: 0.562 [0.509, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.080, 10.522], loss: 0.447842, mae: 0.304336, mean_q: 0.962667
 20250/100000: episode: 472, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.912, mean reward: 0.537 [0.490, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.534], loss: 0.339982, mae: 0.229367, mean_q: 0.912153
 20259/100000: episode: 473, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 5.917, mean reward: 0.657 [0.617, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.701], loss: 0.146085, mae: 0.140896, mean_q: 0.827944
[RESULT] FALSIFICATION!
 20260/100000: episode: 474, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.014, 10.725], loss: 0.006763, mae: 0.093971, mean_q: 0.902538
[RESULT] FALSIFICATION!
 20266/100000: episode: 475, duration: 0.047s, episode steps: 6, steps per second: 128, episode reward: 13.308, mean reward: 2.218 [0.626, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.584], loss: 0.222486, mae: 0.275183, mean_q: 1.058666
[RESULT] FALSIFICATION!
 20273/100000: episode: 476, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 13.603, mean reward: 1.943 [0.508, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.016, 10.550], loss: 0.199630, mae: 0.202703, mean_q: 0.753193
 20279/100000: episode: 477, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.987, mean reward: 0.664 [0.643, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.644], loss: 0.203878, mae: 0.182764, mean_q: 0.985024
 20284/100000: episode: 478, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 3.143, mean reward: 0.629 [0.589, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.677], loss: 0.688159, mae: 0.262055, mean_q: 0.939490
 20292/100000: episode: 479, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 4.763, mean reward: 0.595 [0.554, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.627], loss: 0.430598, mae: 0.243962, mean_q: 1.005903
 20301/100000: episode: 480, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 5.692, mean reward: 0.632 [0.590, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.685], loss: 0.009436, mae: 0.104862, mean_q: 0.841885
 20306/100000: episode: 481, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 3.273, mean reward: 0.655 [0.617, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.652], loss: 0.257080, mae: 0.215887, mean_q: 0.988749
[RESULT] FALSIFICATION!
 20307/100000: episode: 482, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.010, 10.494], loss: 0.015506, mae: 0.151707, mean_q: 0.977309
[RESULT] FALSIFICATION!
 20310/100000: episode: 483, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 11.263, mean reward: 3.754 [0.626, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.010, 10.458], loss: 1.190965, mae: 0.357992, mean_q: 0.902332
 20319/100000: episode: 484, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 5.919, mean reward: 0.658 [0.622, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.637], loss: 0.789154, mae: 0.516574, mean_q: 1.192991
 20325/100000: episode: 485, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 3.653, mean reward: 0.609 [0.575, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.530], loss: 0.653187, mae: 0.337260, mean_q: 0.775629
[RESULT] FALSIFICATION!
 20333/100000: episode: 486, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 14.316, mean reward: 1.789 [0.574, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.486], loss: 0.314415, mae: 0.230767, mean_q: 0.973457
[RESULT] FALSIFICATION!
 20337/100000: episode: 487, duration: 0.024s, episode steps: 4, steps per second: 163, episode reward: 11.966, mean reward: 2.991 [0.605, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.011, 10.741], loss: 0.298545, mae: 0.195816, mean_q: 0.956022
[RESULT] FALSIFICATION!
 20345/100000: episode: 488, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 14.360, mean reward: 1.795 [0.564, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.345, 10.620], loss: 0.154292, mae: 0.125260, mean_q: 0.879754
 20356/100000: episode: 489, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 6.110, mean reward: 0.555 [0.444, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.486], loss: 0.645029, mae: 0.367663, mean_q: 1.073442
[RESULT] FALSIFICATION!
 20357/100000: episode: 490, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.015, 10.770], loss: 0.030393, mae: 0.180745, mean_q: 1.001878
[RESULT] FALSIFICATION!
 20360/100000: episode: 491, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 11.385, mean reward: 3.795 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.011, 10.516], loss: 0.800665, mae: 0.325137, mean_q: 0.846818
[RESULT] FALSIFICATION!
 20362/100000: episode: 492, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.577, mean reward: 5.289 [0.577, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.014, 10.695], loss: 0.013347, mae: 0.128157, mean_q: 0.772247
 20373/100000: episode: 493, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 7.111, mean reward: 0.646 [0.606, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.602], loss: 0.336699, mae: 0.244071, mean_q: 0.989187
[RESULT] FALSIFICATION!
 20374/100000: episode: 494, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.009, 10.427], loss: 0.004456, mae: 0.076605, mean_q: 0.961619
[RESULT] FALSIFICATION!
 20383/100000: episode: 495, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 15.156, mean reward: 1.684 [0.559, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.698], loss: 0.139939, mae: 0.129293, mean_q: 0.856076
 20389/100000: episode: 496, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 3.503, mean reward: 0.584 [0.544, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.548], loss: 0.561322, mae: 0.284324, mean_q: 0.986633
 20397/100000: episode: 497, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.007, mean reward: 0.501 [0.429, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.349, 10.646], loss: 0.440061, mae: 0.291234, mean_q: 1.052671
 20404/100000: episode: 498, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 4.491, mean reward: 0.642 [0.624, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.533], loss: 0.502679, mae: 0.265999, mean_q: 0.989041
 20408/100000: episode: 499, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 2.665, mean reward: 0.666 [0.654, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.645], loss: 0.016421, mae: 0.131190, mean_q: 0.906677
[RESULT] FALSIFICATION!
 20409/100000: episode: 500, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.013, 10.680], loss: 1.245040, mae: 0.386674, mean_q: 0.834012
 20413/100000: episode: 501, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 2.582, mean reward: 0.646 [0.632, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.587, 10.691], loss: 0.864559, mae: 0.358862, mean_q: 1.012846
 20419/100000: episode: 502, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.556, mean reward: 0.593 [0.550, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.616], loss: 0.260542, mae: 0.345442, mean_q: 1.162282
 20427/100000: episode: 503, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 5.020, mean reward: 0.628 [0.586, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.838, 10.556], loss: 0.314932, mae: 0.216099, mean_q: 0.878620
[RESULT] FALSIFICATION!
 20436/100000: episode: 504, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 15.026, mean reward: 1.670 [0.553, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.477, 10.584], loss: 0.263127, mae: 0.177629, mean_q: 0.989790
[RESULT] FALSIFICATION!
 20437/100000: episode: 505, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.716, 10.529], loss: 0.009352, mae: 0.093308, mean_q: 0.952585
 20448/100000: episode: 506, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 6.462, mean reward: 0.587 [0.532, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.462, 10.603], loss: 0.317378, mae: 0.191684, mean_q: 0.963789
 20459/100000: episode: 507, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 6.354, mean reward: 0.578 [0.521, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.425], loss: 0.638842, mae: 0.349973, mean_q: 1.118205
[RESULT] FALSIFICATION!
 20462/100000: episode: 508, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 11.302, mean reward: 3.767 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.012, 10.323], loss: 1.539180, mae: 0.462302, mean_q: 1.000421
[RESULT] FALSIFICATION!
 20463/100000: episode: 509, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.010, 10.505], loss: 0.030106, mae: 0.170558, mean_q: 1.117941
[RESULT] FALSIFICATION!
[Info] New level: 1.7473831176757812 | Considering 10/90 traces
 20465/100000: episode: 510, duration: 3.937s, episode steps: 2, steps per second: 1, episode reward: 10.684, mean reward: 5.342 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.010, 10.429], loss: 0.576427, mae: 0.291187, mean_q: 1.066821
[RESULT] FALSIFICATION!
 20469/100000: episode: 511, duration: 0.023s, episode steps: 4, steps per second: 170, episode reward: 11.891, mean reward: 2.973 [0.580, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.016, 10.433], loss: 0.832846, mae: 0.312910, mean_q: 1.001663
 20473/100000: episode: 512, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.513, mean reward: 0.628 [0.601, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.100, 10.708], loss: 0.323063, mae: 0.303122, mean_q: 1.150348
[RESULT] FALSIFICATION!
 20474/100000: episode: 513, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.014, 10.605], loss: 1.179461, mae: 0.424051, mean_q: 1.094889
[RESULT] FALSIFICATION!
 20475/100000: episode: 514, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.763, 10.396], loss: 0.016662, mae: 0.136237, mean_q: 0.955825
[RESULT] FALSIFICATION!
 20477/100000: episode: 515, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.683, mean reward: 5.341 [0.683, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.506], loss: 0.019208, mae: 0.131934, mean_q: 1.028617
 20480/100000: episode: 516, duration: 0.020s, episode steps: 3, steps per second: 154, episode reward: 1.977, mean reward: 0.659 [0.632, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.810, 10.547], loss: 0.802873, mae: 0.295716, mean_q: 0.841451
 20484/100000: episode: 517, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 2.574, mean reward: 0.643 [0.627, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.708], loss: 0.307965, mae: 0.265304, mean_q: 1.095900
[RESULT] FALSIFICATION!
 20485/100000: episode: 518, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.009, 10.603], loss: 0.048392, mae: 0.237397, mean_q: 1.074130
[RESULT] FALSIFICATION!
 20486/100000: episode: 519, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.009, 10.606], loss: 0.018326, mae: 0.152523, mean_q: 1.069855
[RESULT] FALSIFICATION!
 20488/100000: episode: 520, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.679, mean reward: 5.339 [0.679, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.014, 10.630], loss: 0.605942, mae: 0.263206, mean_q: 0.873352
[RESULT] FALSIFICATION!
 20489/100000: episode: 521, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.008, 10.566], loss: 0.007357, mae: 0.103810, mean_q: 0.920808
[RESULT] FALSIFICATION!
 20490/100000: episode: 522, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.011, 10.551], loss: 0.008956, mae: 0.094259, mean_q: 0.908350
 20498/100000: episode: 523, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 5.016, mean reward: 0.627 [0.573, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.573], loss: 0.157114, mae: 0.179197, mean_q: 1.045404
[RESULT] FALSIFICATION!
 20500/100000: episode: 524, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 10.692, mean reward: 5.346 [0.692, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.669, 10.673], loss: 0.016820, mae: 0.104633, mean_q: 0.896983
[RESULT] FALSIFICATION!
 20501/100000: episode: 525, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.012, 10.594], loss: 0.014929, mae: 0.130732, mean_q: 0.762785
[RESULT] FALSIFICATION!
 20502/100000: episode: 526, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.009, 10.608], loss: 0.006940, mae: 0.100915, mean_q: 0.856754
[RESULT] FALSIFICATION!
 20505/100000: episode: 527, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 11.354, mean reward: 3.785 [0.667, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.011, 10.642], loss: 1.473870, mae: 0.474490, mean_q: 0.994650
 20509/100000: episode: 528, duration: 0.035s, episode steps: 4, steps per second: 116, episode reward: 2.646, mean reward: 0.661 [0.649, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.679], loss: 0.913837, mae: 0.688786, mean_q: 1.393567
[RESULT] FALSIFICATION!
 20511/100000: episode: 529, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 10.658, mean reward: 5.329 [0.658, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.011, 10.646], loss: 0.571983, mae: 0.412899, mean_q: 1.149658
 20515/100000: episode: 530, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 2.506, mean reward: 0.626 [0.588, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.690], loss: 0.579577, mae: 0.278379, mean_q: 0.868421
 20517/100000: episode: 531, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.284, mean reward: 0.642 [0.629, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.035, 10.662], loss: 0.011167, mae: 0.119212, mean_q: 0.898628
 20521/100000: episode: 532, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.556, mean reward: 0.639 [0.597, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.668], loss: 0.279528, mae: 0.198214, mean_q: 0.969267
 20530/100000: episode: 533, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 5.470, mean reward: 0.608 [0.574, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.659], loss: 0.407254, mae: 0.379560, mean_q: 1.178209
 20533/100000: episode: 534, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 1.976, mean reward: 0.659 [0.622, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.035, 10.758], loss: 0.421836, mae: 0.306614, mean_q: 0.745503
 20542/100000: episode: 535, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 5.783, mean reward: 0.643 [0.583, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.239, 10.657], loss: 0.873387, mae: 0.453043, mean_q: 1.087873
[RESULT] FALSIFICATION!
 20546/100000: episode: 536, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 11.978, mean reward: 2.994 [0.626, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.734], loss: 1.084207, mae: 0.559292, mean_q: 1.232264
 20550/100000: episode: 537, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 2.512, mean reward: 0.628 [0.609, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.610], loss: 0.855137, mae: 0.324907, mean_q: 0.987093
 20552/100000: episode: 538, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.290, mean reward: 0.645 [0.639, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.562], loss: 0.564073, mae: 0.254133, mean_q: 0.989193
[RESULT] FALSIFICATION!
 20558/100000: episode: 539, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 13.122, mean reward: 2.187 [0.562, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.015, 10.737], loss: 1.111147, mae: 0.587689, mean_q: 1.280171
[RESULT] FALSIFICATION!
 20560/100000: episode: 540, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 10.685, mean reward: 5.343 [0.685, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.010, 10.608], loss: 0.082633, mae: 0.352656, mean_q: 1.340722
 20564/100000: episode: 541, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 2.566, mean reward: 0.642 [0.613, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.596], loss: 0.042739, mae: 0.192215, mean_q: 1.025868
[RESULT] FALSIFICATION!
 20567/100000: episode: 542, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 11.286, mean reward: 3.762 [0.637, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.012, 10.741], loss: 0.731326, mae: 0.317595, mean_q: 0.885224
 20569/100000: episode: 543, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 1.377, mean reward: 0.688 [0.678, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.564], loss: 0.026827, mae: 0.137188, mean_q: 1.002913
[RESULT] FALSIFICATION!
 20572/100000: episode: 544, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 11.297, mean reward: 3.766 [0.644, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.531, 10.611], loss: 0.740652, mae: 0.353582, mean_q: 1.119367
 20576/100000: episode: 545, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 2.739, mean reward: 0.685 [0.661, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.662], loss: 0.324275, mae: 0.247312, mean_q: 1.132184
 20578/100000: episode: 546, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.267, mean reward: 0.633 [0.622, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.539], loss: 0.519136, mae: 0.272627, mean_q: 0.947070
[RESULT] FALSIFICATION!
 20582/100000: episode: 547, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 11.894, mean reward: 2.973 [0.617, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.672], loss: 0.820134, mae: 0.324427, mean_q: 1.013570
 20586/100000: episode: 548, duration: 0.033s, episode steps: 4, steps per second: 121, episode reward: 2.533, mean reward: 0.633 [0.608, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.038, 10.559], loss: 0.543205, mae: 0.332145, mean_q: 1.188212
[RESULT] FALSIFICATION!
 20590/100000: episode: 549, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 11.884, mean reward: 2.971 [0.618, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.682], loss: 0.310287, mae: 0.263439, mean_q: 1.130146
 20594/100000: episode: 550, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 2.570, mean reward: 0.643 [0.605, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.719], loss: 0.288321, mae: 0.208354, mean_q: 1.047692
[RESULT] FALSIFICATION!
 20600/100000: episode: 551, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.039, mean reward: 2.173 [0.572, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.832, 10.473], loss: 0.380715, mae: 0.212087, mean_q: 0.940163
[RESULT] FALSIFICATION!
 20601/100000: episode: 552, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.009, 10.588], loss: 0.062970, mae: 0.258811, mean_q: 1.104623
[RESULT] FALSIFICATION!
 20602/100000: episode: 553, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.009, 10.462], loss: 0.098437, mae: 0.365459, mean_q: 1.309231
[RESULT] FALSIFICATION!
 20607/100000: episode: 554, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 12.566, mean reward: 2.513 [0.617, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.378, 10.380], loss: 0.638019, mae: 0.402041, mean_q: 1.192806
[RESULT] FALSIFICATION!
 20608/100000: episode: 555, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.010, 10.706], loss: 1.058935, mae: 0.403954, mean_q: 1.037691
[RESULT] FALSIFICATION!
 20610/100000: episode: 556, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 10.674, mean reward: 5.337 [0.674, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.753], loss: 0.024092, mae: 0.150684, mean_q: 0.875165
[RESULT] FALSIFICATION!
 20611/100000: episode: 557, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.025, 10.425], loss: 1.029302, mae: 0.402208, mean_q: 0.907940
[RESULT] FALSIFICATION!
 20617/100000: episode: 558, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 13.259, mean reward: 2.210 [0.586, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.108, 10.557], loss: 1.220564, mae: 0.634902, mean_q: 1.263982
 20619/100000: episode: 559, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 1.354, mean reward: 0.677 [0.655, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.555], loss: 1.624782, mae: 0.824802, mean_q: 1.455139
 20628/100000: episode: 560, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 5.746, mean reward: 0.638 [0.590, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.212, 10.691], loss: 0.285000, mae: 0.274299, mean_q: 0.878774
[RESULT] FALSIFICATION!
 20629/100000: episode: 561, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.010, 10.590], loss: 0.012741, mae: 0.124373, mean_q: 1.059280
[RESULT] FALSIFICATION!
 20630/100000: episode: 562, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.340, 10.385], loss: 0.021518, mae: 0.158510, mean_q: 1.002415
[RESULT] FALSIFICATION!
 20631/100000: episode: 563, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.010, 10.599], loss: 0.053386, mae: 0.213897, mean_q: 0.964240
[RESULT] FALSIFICATION!
 20632/100000: episode: 564, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.509, 10.273], loss: 1.043291, mae: 0.408643, mean_q: 1.003141
[RESULT] FALSIFICATION!
 20637/100000: episode: 565, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 12.529, mean reward: 2.506 [0.591, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.013, 10.595], loss: 0.883591, mae: 0.343562, mean_q: 1.034972
[RESULT] FALSIFICATION!
 20638/100000: episode: 566, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.011, 10.713], loss: 0.041809, mae: 0.223284, mean_q: 1.229136
 20640/100000: episode: 567, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.317, mean reward: 0.659 [0.658, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.553], loss: 1.119399, mae: 0.487603, mean_q: 1.166894
 20644/100000: episode: 568, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 2.496, mean reward: 0.624 [0.576, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.719], loss: 0.826532, mae: 0.515244, mean_q: 1.303979
 20646/100000: episode: 569, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.220, mean reward: 0.610 [0.602, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.035, 10.608], loss: 0.049654, mae: 0.253982, mean_q: 1.245113
 20655/100000: episode: 570, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 5.647, mean reward: 0.627 [0.576, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.175, 10.633], loss: 1.059708, mae: 0.467903, mean_q: 1.153714
[RESULT] FALSIFICATION!
 20661/100000: episode: 571, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 13.264, mean reward: 2.211 [0.634, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.474, 10.731], loss: 0.535776, mae: 0.422217, mean_q: 1.282991
 20665/100000: episode: 572, duration: 0.037s, episode steps: 4, steps per second: 109, episode reward: 2.511, mean reward: 0.628 [0.582, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.148, 10.768], loss: 0.028709, mae: 0.184242, mean_q: 0.844455
 20669/100000: episode: 573, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 2.615, mean reward: 0.654 [0.616, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.035, 10.670], loss: 0.540869, mae: 0.331083, mean_q: 1.113024
 20677/100000: episode: 574, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 5.257, mean reward: 0.657 [0.636, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.524, 10.683], loss: 0.675048, mae: 0.418206, mean_q: 1.174096
 20681/100000: episode: 575, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.561, mean reward: 0.640 [0.622, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.551], loss: 0.540079, mae: 0.279190, mean_q: 0.909789
[RESULT] FALSIFICATION!
 20682/100000: episode: 576, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.010, 10.567], loss: 0.023006, mae: 0.159509, mean_q: 1.140048
 20686/100000: episode: 577, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 2.400, mean reward: 0.600 [0.585, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.591], loss: 0.557675, mae: 0.438662, mean_q: 1.294900
[RESULT] FALSIFICATION!
 20690/100000: episode: 578, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 11.927, mean reward: 2.982 [0.612, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.490, 10.762], loss: 1.118030, mae: 0.426502, mean_q: 1.064382
 20693/100000: episode: 579, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.847, mean reward: 0.616 [0.605, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.035, 10.595], loss: 0.986071, mae: 0.420513, mean_q: 1.163535
 20701/100000: episode: 580, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 5.199, mean reward: 0.650 [0.602, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.689], loss: 1.044458, mae: 0.651400, mean_q: 1.397290
 20705/100000: episode: 581, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 2.527, mean reward: 0.632 [0.610, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.253, 10.460], loss: 0.290651, mae: 0.241612, mean_q: 1.064186
[RESULT] FALSIFICATION!
 20707/100000: episode: 582, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 10.667, mean reward: 5.333 [0.667, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.011, 10.713], loss: 1.607284, mae: 0.593790, mean_q: 0.896111
[RESULT] FALSIFICATION!
 20709/100000: episode: 583, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.681, mean reward: 5.340 [0.681, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.684], loss: 0.119010, mae: 0.306115, mean_q: 1.219780
[RESULT] FALSIFICATION!
 20710/100000: episode: 584, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.011, 10.575], loss: 1.053471, mae: 0.614225, mean_q: 1.508904
[RESULT] FALSIFICATION!
 20711/100000: episode: 585, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.010, 10.414], loss: 1.046567, mae: 0.639906, mean_q: 1.520216
[RESULT] FALSIFICATION!
 20713/100000: episode: 586, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.682, mean reward: 5.341 [0.682, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.009, 10.582], loss: 1.042561, mae: 0.465254, mean_q: 1.187890
[RESULT] FALSIFICATION!
 20715/100000: episode: 587, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.688, mean reward: 5.344 [0.688, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.011, 10.592], loss: 0.548429, mae: 0.298068, mean_q: 1.003423
[RESULT] FALSIFICATION!
 20716/100000: episode: 588, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.010, 10.679], loss: 1.046172, mae: 0.380974, mean_q: 0.956458
[RESULT] FALSIFICATION!
 20723/100000: episode: 589, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 13.954, mean reward: 1.993 [0.641, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.015, 10.712], loss: 1.648330, mae: 0.902327, mean_q: 1.540473
[RESULT] FALSIFICATION!
 20724/100000: episode: 590, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.009, 10.562], loss: 1.032888, mae: 0.752212, mean_q: 1.640250
[RESULT] FALSIFICATION!
 20728/100000: episode: 591, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 12.070, mean reward: 3.017 [0.677, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.542], loss: 0.311805, mae: 0.294158, mean_q: 0.949851
 20732/100000: episode: 592, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 2.543, mean reward: 0.636 [0.613, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.572], loss: 0.051159, mae: 0.284143, mean_q: 0.750558
[RESULT] FALSIFICATION!
 20740/100000: episode: 593, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 14.388, mean reward: 1.799 [0.603, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.015, 10.573], loss: 0.991240, mae: 0.637036, mean_q: 1.409731
[RESULT] FALSIFICATION!
 20742/100000: episode: 594, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 10.690, mean reward: 5.345 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.011, 10.513], loss: 0.061467, mae: 0.215340, mean_q: 1.062920
 20746/100000: episode: 595, duration: 0.032s, episode steps: 4, steps per second: 124, episode reward: 2.500, mean reward: 0.625 [0.609, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.697], loss: 1.231390, mae: 0.507634, mean_q: 0.834123
 20755/100000: episode: 596, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 5.622, mean reward: 0.625 [0.528, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.696], loss: 0.387842, mae: 0.278359, mean_q: 1.152699
 20763/100000: episode: 597, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 4.752, mean reward: 0.594 [0.554, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.534], loss: 0.769513, mae: 0.443899, mean_q: 1.246208
 20767/100000: episode: 598, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 2.572, mean reward: 0.643 [0.617, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.636], loss: 0.543659, mae: 0.276273, mean_q: 1.049329
[RESULT] FALSIFICATION!
 20772/100000: episode: 599, duration: 0.041s, episode steps: 5, steps per second: 122, episode reward: 12.483, mean reward: 2.497 [0.601, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.014, 10.629], loss: 1.099635, mae: 0.477995, mean_q: 1.199888
[Info] New level: 2.5755577087402344 | Considering 10/90 traces
 20776/100000: episode: 600, duration: 3.962s, episode steps: 4, steps per second: 1, episode reward: 2.612, mean reward: 0.653 [0.637, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.087, 10.570], loss: 1.026704, mae: 0.634772, mean_q: 1.432425
[RESULT] FALSIFICATION!
 20779/100000: episode: 601, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 11.322, mean reward: 3.774 [0.644, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.633], loss: 1.041573, mae: 0.447160, mean_q: 1.210335
 20782/100000: episode: 602, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.982, mean reward: 0.661 [0.635, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.633], loss: 0.353876, mae: 0.244020, mean_q: 1.056432
[RESULT] FALSIFICATION!
 20783/100000: episode: 603, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.010, 10.475], loss: 0.974343, mae: 0.367334, mean_q: 0.888247
 20785/100000: episode: 604, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 1.315, mean reward: 0.657 [0.632, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.625], loss: 0.487860, mae: 0.285557, mean_q: 1.131686
 20786/100000: episode: 605, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.661, mean reward: 0.661 [0.661, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.459], loss: 3.078211, mae: 0.992538, mean_q: 1.274774
[RESULT] FALSIFICATION!
 20788/100000: episode: 606, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.010, 10.679], loss: 1.398710, mae: 0.857655, mean_q: 1.583742
[RESULT] FALSIFICATION!
 20791/100000: episode: 607, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 11.350, mean reward: 3.783 [0.675, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.594], loss: 0.452218, mae: 0.574444, mean_q: 1.501200
[RESULT] FALSIFICATION!
 20795/100000: episode: 608, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 12.004, mean reward: 3.001 [0.631, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.736, 10.483], loss: 0.561944, mae: 0.357662, mean_q: 0.942470
[RESULT] FALSIFICATION!
 20796/100000: episode: 609, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.012, 10.437], loss: 1.052998, mae: 0.498754, mean_q: 0.938869
[RESULT] FALSIFICATION!
 20798/100000: episode: 610, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.683, mean reward: 5.341 [0.683, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.013, 10.397], loss: 1.041591, mae: 0.430584, mean_q: 0.975874
[RESULT] FALSIFICATION!
 20799/100000: episode: 611, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.516], loss: 0.166146, mae: 0.416308, mean_q: 1.377992
[RESULT] FALSIFICATION!
 20802/100000: episode: 612, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 11.378, mean reward: 3.793 [0.681, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.475, 10.597], loss: 0.777693, mae: 0.563329, mean_q: 1.455494
[RESULT] FALSIFICATION!
 20803/100000: episode: 613, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.013, 10.401], loss: 0.068703, mae: 0.202320, mean_q: 1.035036
[RESULT] FALSIFICATION!
 20804/100000: episode: 614, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.013, 10.471], loss: 1.237263, mae: 0.474484, mean_q: 1.063523
[RESULT] FALSIFICATION!
 20806/100000: episode: 615, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 10.637, mean reward: 5.319 [0.637, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.587], loss: 0.548851, mae: 0.294774, mean_q: 0.832069
[RESULT] FALSIFICATION!
 20807/100000: episode: 616, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.011, 10.546], loss: 1.097876, mae: 0.503014, mean_q: 1.012044
 20808/100000: episode: 617, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.620, mean reward: 0.620 [0.620, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.035, 10.458], loss: 0.012180, mae: 0.113568, mean_q: 0.901551
[RESULT] FALSIFICATION!
 20810/100000: episode: 618, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 10.674, mean reward: 5.337 [0.674, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.015, 10.446], loss: 0.028357, mae: 0.154267, mean_q: 0.963143
[RESULT] FALSIFICATION!
 20811/100000: episode: 619, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.012, 10.658], loss: 0.994340, mae: 0.403546, mean_q: 1.063102
 20812/100000: episode: 620, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.692, mean reward: 0.692 [0.692, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.035, 10.511], loss: 1.946405, mae: 0.679111, mean_q: 1.117297
[RESULT] FALSIFICATION!
 20815/100000: episode: 621, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 11.362, mean reward: 3.787 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.492], loss: 0.554485, mae: 0.700925, mean_q: 1.588924
 20816/100000: episode: 622, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.639, mean reward: 0.639 [0.639, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.035, 10.548], loss: 0.923880, mae: 0.550742, mean_q: 1.299493
[RESULT] FALSIFICATION!
 20818/100000: episode: 623, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 10.688, mean reward: 5.344 [0.688, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.012, 10.417], loss: 0.466911, mae: 0.291390, mean_q: 1.209735
 20819/100000: episode: 624, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.502], loss: 0.014776, mae: 0.137877, mean_q: 0.999148
[RESULT] FALSIFICATION!
 20821/100000: episode: 625, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.695, mean reward: 5.348 [0.695, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.628], loss: 0.043616, mae: 0.243177, mean_q: 0.909140
[RESULT] FALSIFICATION!
 20822/100000: episode: 626, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.010, 10.590], loss: 0.041985, mae: 0.214566, mean_q: 0.846173
[RESULT] FALSIFICATION!
 20829/100000: episode: 627, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 13.978, mean reward: 1.997 [0.621, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.600], loss: 0.743080, mae: 0.396170, mean_q: 1.223767
[RESULT] FALSIFICATION!
 20834/100000: episode: 628, duration: 0.037s, episode steps: 5, steps per second: 137, episode reward: 12.699, mean reward: 2.540 [0.657, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.013, 10.535], loss: 1.518997, mae: 0.813015, mean_q: 1.534416
[RESULT] FALSIFICATION!
 20838/100000: episode: 629, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 11.988, mean reward: 2.997 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.011, 10.658], loss: 1.030450, mae: 0.472495, mean_q: 1.222614
 20840/100000: episode: 630, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.350, mean reward: 0.675 [0.656, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.568], loss: 0.520436, mae: 0.258740, mean_q: 1.035669
[RESULT] FALSIFICATION!
 20841/100000: episode: 631, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.607], loss: 0.045112, mae: 0.167075, mean_q: 1.062145
[RESULT] FALSIFICATION!
 20843/100000: episode: 632, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.649, mean reward: 5.325 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.548], loss: 0.901726, mae: 0.518475, mean_q: 1.264897
[RESULT] FALSIFICATION!
 20844/100000: episode: 633, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.444], loss: 0.200640, mae: 0.526419, mean_q: 1.605262
[RESULT] FALSIFICATION!
 20845/100000: episode: 634, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.009, 10.666], loss: 0.089781, mae: 0.260025, mean_q: 1.242533
 20846/100000: episode: 635, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.035, 10.561], loss: 2.190193, mae: 0.715430, mean_q: 1.324685
 20853/100000: episode: 636, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 4.506, mean reward: 0.644 [0.628, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.504], loss: 1.530281, mae: 0.640524, mean_q: 1.286663
 20854/100000: episode: 637, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.698, mean reward: 0.698 [0.698, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.479], loss: 0.251876, mae: 0.576753, mean_q: 1.535346
[RESULT] FALSIFICATION!
 20855/100000: episode: 638, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.011, 10.563], loss: 0.935866, mae: 0.787307, mean_q: 1.848958
[RESULT] FALSIFICATION!
 20856/100000: episode: 639, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.035, 10.492], loss: 1.685305, mae: 0.772865, mean_q: 1.388959
[RESULT] FALSIFICATION!
 20857/100000: episode: 640, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.421], loss: 1.910911, mae: 0.700766, mean_q: 1.140556
[RESULT] FALSIFICATION!
 20859/100000: episode: 641, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.010, 10.489], loss: 1.008853, mae: 0.528074, mean_q: 1.322723
 20860/100000: episode: 642, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.613, mean reward: 0.613 [0.613, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.035, 10.557], loss: 0.838669, mae: 0.333699, mean_q: 1.012892
 20862/100000: episode: 643, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.274, mean reward: 0.637 [0.625, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.579], loss: 0.523887, mae: 0.399295, mean_q: 1.258490
[RESULT] FALSIFICATION!
 20863/100000: episode: 644, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.014, 10.578], loss: 0.058013, mae: 0.239519, mean_q: 1.256286
[RESULT] FALSIFICATION!
 20864/100000: episode: 645, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.013, 10.494], loss: 0.105066, mae: 0.253466, mean_q: 1.244647
 20866/100000: episode: 646, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.266, mean reward: 0.633 [0.630, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.528], loss: 1.082311, mae: 0.486094, mean_q: 0.953077
[RESULT] FALSIFICATION!
 20868/100000: episode: 647, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.684, mean reward: 5.342 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.010, 10.605], loss: 1.107753, mae: 0.438205, mean_q: 0.961226
[RESULT] FALSIFICATION!
 20870/100000: episode: 648, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.012, 10.673], loss: 0.061416, mae: 0.229047, mean_q: 1.210561
[RESULT] FALSIFICATION!
 20877/100000: episode: 649, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 13.955, mean reward: 1.994 [0.623, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.622], loss: 1.118454, mae: 0.575806, mean_q: 1.302395
[RESULT] FALSIFICATION!
 20878/100000: episode: 650, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.525], loss: 1.038220, mae: 0.761977, mean_q: 1.704779
[RESULT] FALSIFICATION!
 20879/100000: episode: 651, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.402], loss: 0.138901, mae: 0.419287, mean_q: 1.381994
 20881/100000: episode: 652, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 1.222, mean reward: 0.611 [0.606, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.035, 10.509], loss: 0.623616, mae: 0.561630, mean_q: 1.501210
 20886/100000: episode: 653, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 3.313, mean reward: 0.663 [0.648, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.718], loss: 0.665042, mae: 0.329474, mean_q: 1.054536
[RESULT] FALSIFICATION!
 20887/100000: episode: 654, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.393], loss: 0.013968, mae: 0.148528, mean_q: 0.944107
 20888/100000: episode: 655, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.649, mean reward: 0.649 [0.649, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.456], loss: 2.713178, mae: 0.934256, mean_q: 1.293866
[RESULT] FALSIFICATION!
 20892/100000: episode: 656, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 11.986, mean reward: 2.996 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.013, 10.525], loss: 1.032269, mae: 0.873185, mean_q: 1.723595
[RESULT] FALSIFICATION!
 20898/100000: episode: 657, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 13.314, mean reward: 2.219 [0.592, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.102, 10.589], loss: 0.839882, mae: 0.456088, mean_q: 1.196274
 20899/100000: episode: 658, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.511], loss: 0.075087, mae: 0.279579, mean_q: 1.045045
[RESULT] FALSIFICATION!
 20900/100000: episode: 659, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.011, 10.618], loss: 1.152339, mae: 0.487546, mean_q: 0.987232
[RESULT] FALSIFICATION!
 20901/100000: episode: 660, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.385, 10.434], loss: 0.019189, mae: 0.129121, mean_q: 0.997430
[RESULT] FALSIFICATION!
 20903/100000: episode: 661, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.686, mean reward: 5.343 [0.686, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.012, 10.742], loss: 0.494851, mae: 0.394115, mean_q: 1.342691
[RESULT] FALSIFICATION!
 20904/100000: episode: 662, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.010, 10.697], loss: 1.895588, mae: 0.776649, mean_q: 1.385034
[RESULT] FALSIFICATION!
 20905/100000: episode: 663, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.012, 10.614], loss: 0.930760, mae: 0.561250, mean_q: 1.404111
[RESULT] FALSIFICATION!
 20906/100000: episode: 664, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.012, 10.544], loss: 0.887729, mae: 0.577162, mean_q: 1.445571
[RESULT] FALSIFICATION!
 20908/100000: episode: 665, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.655, mean reward: 5.327 [0.655, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.010, 10.721], loss: 0.053729, mae: 0.226594, mean_q: 1.320239
 20909/100000: episode: 666, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.466], loss: 0.929812, mae: 0.373346, mean_q: 1.048288
 20910/100000: episode: 667, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.499], loss: 1.153072, mae: 0.500076, mean_q: 1.283765
[RESULT] FALSIFICATION!
 20911/100000: episode: 668, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.402, 10.473], loss: 0.861683, mae: 0.420288, mean_q: 1.205032
 20913/100000: episode: 669, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.314, mean reward: 0.657 [0.642, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.035, 10.639], loss: 1.475281, mae: 0.731077, mean_q: 1.468773
[RESULT] FALSIFICATION!
 20914/100000: episode: 670, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.014, 10.563], loss: 2.618274, mae: 1.082993, mean_q: 1.640460
[RESULT] FALSIFICATION!
 20915/100000: episode: 671, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.424], loss: 0.180734, mae: 0.500510, mean_q: 1.605106
 20920/100000: episode: 672, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 3.204, mean reward: 0.641 [0.592, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.572], loss: 1.162542, mae: 0.482467, mean_q: 1.187652
[RESULT] FALSIFICATION!
 20921/100000: episode: 673, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.011, 10.489], loss: 0.993660, mae: 0.483919, mean_q: 1.351428
[RESULT] FALSIFICATION!
 20922/100000: episode: 674, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.012, 10.633], loss: 0.107353, mae: 0.223314, mean_q: 1.266007
 20923/100000: episode: 675, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.638, mean reward: 0.638 [0.638, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.035, 10.548], loss: 1.944021, mae: 0.696215, mean_q: 1.170773
 20924/100000: episode: 676, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.697, mean reward: 0.697 [0.697, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.399], loss: 2.746774, mae: 0.914084, mean_q: 1.277548
 20927/100000: episode: 677, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 2.055, mean reward: 0.685 [0.684, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.588], loss: 1.009739, mae: 0.577106, mean_q: 1.394810
[RESULT] FALSIFICATION!
 20928/100000: episode: 678, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.012, 10.453], loss: 0.044970, mae: 0.189577, mean_q: 1.196250
 20930/100000: episode: 679, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.321, mean reward: 0.660 [0.650, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.035, 10.597], loss: 1.337649, mae: 0.587464, mean_q: 1.357545
[RESULT] FALSIFICATION!
 20931/100000: episode: 680, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.012, 10.445], loss: 0.094561, mae: 0.272538, mean_q: 1.293940
 20932/100000: episode: 681, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.035, 10.462], loss: 1.782543, mae: 0.639426, mean_q: 1.175658
[RESULT] FALSIFICATION!
 20935/100000: episode: 682, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 11.341, mean reward: 3.780 [0.658, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.013, 10.538], loss: 0.597132, mae: 0.355252, mean_q: 1.274969
[RESULT] FALSIFICATION!
 20936/100000: episode: 683, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.010, 10.529], loss: 0.205527, mae: 0.411148, mean_q: 1.586789
[RESULT] FALSIFICATION!
 20937/100000: episode: 684, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.011, 10.277], loss: 0.145160, mae: 0.305644, mean_q: 1.285872
 20939/100000: episode: 685, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.315, mean reward: 0.657 [0.638, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.431], loss: 1.882962, mae: 0.760860, mean_q: 1.424037
[RESULT] FALSIFICATION!
 20940/100000: episode: 686, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.012, 10.557], loss: 1.079810, mae: 0.605146, mean_q: 1.495927
 20945/100000: episode: 687, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 3.250, mean reward: 0.650 [0.620, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.648], loss: 1.573259, mae: 0.784432, mean_q: 1.526330
[RESULT] FALSIFICATION!
 20947/100000: episode: 688, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.697, mean reward: 5.348 [0.697, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.014, 10.597], loss: 0.531524, mae: 0.538831, mean_q: 1.490775
[RESULT] FALSIFICATION!
 20953/100000: episode: 689, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 13.330, mean reward: 2.222 [0.644, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.013, 10.406], loss: 1.175949, mae: 0.526767, mean_q: 1.226521
[RESULT] FALSIFICATION!
[Info] New level: 3.8279902935028076 | Considering 10/90 traces
 20954/100000: episode: 690, duration: 3.908s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.010, 10.557], loss: 1.693177, mae: 0.884472, mean_q: 1.710287
[RESULT] FALSIFICATION!
 20955/100000: episode: 691, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.070, 10.722], loss: 0.909698, mae: 0.732070, mean_q: 1.643286
[RESULT] FALSIFICATION!
 20956/100000: episode: 692, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.035, 10.662], loss: 0.286265, mae: 0.533459, mean_q: 1.541034
[RESULT] FALSIFICATION!
 20957/100000: episode: 693, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.537], loss: 0.944203, mae: 0.420496, mean_q: 1.239738
[RESULT] FALSIFICATION!
 20958/100000: episode: 694, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.070, 10.579], loss: 2.122187, mae: 0.762511, mean_q: 1.260807
[RESULT] FALSIFICATION!
 20959/100000: episode: 695, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.012, 10.479], loss: 0.063250, mae: 0.243548, mean_q: 1.196297
[RESULT] FALSIFICATION!
 20960/100000: episode: 696, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.542], loss: 0.812758, mae: 0.487070, mean_q: 1.337243
 20961/100000: episode: 697, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.690, mean reward: 0.690 [0.690, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.624], loss: 0.093708, mae: 0.215215, mean_q: 1.175200
[RESULT] FALSIFICATION!
 20962/100000: episode: 698, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.070, 10.523], loss: 0.896001, mae: 0.569715, mean_q: 1.451710
[RESULT] FALSIFICATION!
 20963/100000: episode: 699, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.455 [-0.070, 10.698], loss: 0.081509, mae: 0.272003, mean_q: 1.299831
[RESULT] FALSIFICATION!
 20964/100000: episode: 700, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.639], loss: 1.056143, mae: 0.630412, mean_q: 1.539794
[RESULT] FALSIFICATION!
 20965/100000: episode: 701, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.459 [-0.070, 10.821], loss: 1.100707, mae: 0.579482, mean_q: 1.314359
[RESULT] FALSIFICATION!
 20966/100000: episode: 702, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.070, 10.643], loss: 1.055035, mae: 0.540832, mean_q: 1.165182
[RESULT] FALSIFICATION!
 20967/100000: episode: 703, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.700], loss: 0.948012, mae: 0.488093, mean_q: 1.126531
[RESULT] FALSIFICATION!
 20968/100000: episode: 704, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.070, 10.459], loss: 2.083394, mae: 0.691687, mean_q: 1.163530
[RESULT] FALSIFICATION!
 20969/100000: episode: 705, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.070, 10.459], loss: 2.930956, mae: 0.922438, mean_q: 1.200170
[RESULT] FALSIFICATION!
 20970/100000: episode: 706, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.035, 10.654], loss: 1.007313, mae: 0.646725, mean_q: 1.545452
[RESULT] FALSIFICATION!
 20971/100000: episode: 707, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.070, 10.578], loss: 0.851417, mae: 0.544453, mean_q: 1.397284
[RESULT] FALSIFICATION!
 20972/100000: episode: 708, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.070, 10.653], loss: 0.814409, mae: 0.538376, mean_q: 1.396763
 20973/100000: episode: 709, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.671, mean reward: 0.671 [0.671, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.070, 10.579], loss: 0.885525, mae: 0.523240, mean_q: 1.393907
[RESULT] FALSIFICATION!
 20974/100000: episode: 710, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.464], loss: 0.064735, mae: 0.251111, mean_q: 1.308755
[RESULT] FALSIFICATION!
 20975/100000: episode: 711, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.628], loss: 1.192809, mae: 0.479042, mean_q: 1.186359
 20976/100000: episode: 712, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.684, mean reward: 0.684 [0.684, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.544], loss: 0.074702, mae: 0.210345, mean_q: 1.248392
[RESULT] FALSIFICATION!
 20977/100000: episode: 713, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.070, 10.411], loss: 0.153121, mae: 0.299423, mean_q: 1.193684
[RESULT] FALSIFICATION!
 20978/100000: episode: 714, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.070, 10.495], loss: 1.038911, mae: 0.452310, mean_q: 1.080036
[RESULT] FALSIFICATION!
 20979/100000: episode: 715, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.035, 10.665], loss: 0.918216, mae: 0.501659, mean_q: 1.410666
[RESULT] FALSIFICATION!
 20980/100000: episode: 716, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.070, 10.497], loss: 0.173272, mae: 0.366168, mean_q: 1.385075
[RESULT] FALSIFICATION!
 20981/100000: episode: 717, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.035, 10.750], loss: 1.055108, mae: 0.709599, mean_q: 1.651253
[RESULT] FALSIFICATION!
 20982/100000: episode: 718, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.422 [-0.035, 10.612], loss: 2.441157, mae: 1.091461, mean_q: 1.656370
[RESULT] FALSIFICATION!
 20983/100000: episode: 719, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.011, 10.638], loss: 0.247467, mae: 0.534308, mean_q: 1.572836
 20984/100000: episode: 720, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.674, mean reward: 0.674 [0.674, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.035, 10.603], loss: 0.910135, mae: 0.712014, mean_q: 1.606512
[RESULT] FALSIFICATION!
 20985/100000: episode: 721, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.646, 10.425], loss: 1.899739, mae: 1.091671, mean_q: 1.910169
[RESULT] FALSIFICATION!
 20986/100000: episode: 722, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.035, 10.544], loss: 0.155059, mae: 0.359899, mean_q: 1.408390
 20987/100000: episode: 723, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.070, 10.530], loss: 0.885242, mae: 0.513665, mean_q: 1.333274
[RESULT] FALSIFICATION!
 20988/100000: episode: 724, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.070, 10.518], loss: 1.725127, mae: 0.710757, mean_q: 1.422941
[RESULT] FALSIFICATION!
 20989/100000: episode: 725, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.706], loss: 1.222590, mae: 0.452872, mean_q: 1.224709
[RESULT] FALSIFICATION!
 20990/100000: episode: 726, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.035, 10.767], loss: 1.646442, mae: 0.746384, mean_q: 1.608191
 20991/100000: episode: 727, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.070, 10.622], loss: 0.116792, mae: 0.287723, mean_q: 1.170666
[RESULT] FALSIFICATION!
 20992/100000: episode: 728, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.035, 10.619], loss: 2.347271, mae: 0.915735, mean_q: 1.587894
[RESULT] FALSIFICATION!
 20993/100000: episode: 729, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.070, 10.479], loss: 1.944467, mae: 0.865714, mean_q: 1.500197
[RESULT] FALSIFICATION!
 20994/100000: episode: 730, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.070, 10.645], loss: 2.075531, mae: 0.857668, mean_q: 1.454852
 20995/100000: episode: 731, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.070, 10.491], loss: 2.028541, mae: 1.035276, mean_q: 1.824393
 20996/100000: episode: 732, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.070, 10.601], loss: 0.651770, mae: 0.800441, mean_q: 1.812659
[RESULT] FALSIFICATION!
 20997/100000: episode: 733, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.070, 10.521], loss: 1.615110, mae: 0.978129, mean_q: 1.811900
[RESULT] FALSIFICATION!
 20998/100000: episode: 734, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.035, 10.684], loss: 4.327311, mae: 1.549498, mean_q: 1.705770
[RESULT] FALSIFICATION!
 20999/100000: episode: 735, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.727], loss: 0.922624, mae: 0.685756, mean_q: 1.543714
[RESULT] FALSIFICATION!
 21000/100000: episode: 736, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.070, 10.599], loss: 3.133239, mae: 1.330743, mean_q: 1.864932
 21001/100000: episode: 737, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.070, 10.596], loss: 2.644053, mae: 1.342031, mean_q: 2.093625
[RESULT] FALSIFICATION!
 21002/100000: episode: 738, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.035, 10.643], loss: 0.915091, mae: 0.749730, mean_q: 1.689589
[RESULT] FALSIFICATION!
 21003/100000: episode: 739, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.011, 10.588], loss: 0.999233, mae: 0.468609, mean_q: 1.147503
[RESULT] FALSIFICATION!
 21004/100000: episode: 740, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.015, 10.429], loss: 0.964823, mae: 0.457832, mean_q: 1.024016
 21005/100000: episode: 741, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.687, mean reward: 0.687 [0.687, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.407 [-0.070, 10.554], loss: 1.690004, mae: 0.662177, mean_q: 1.095246
[RESULT] FALSIFICATION!
 21006/100000: episode: 742, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.011, 10.545], loss: 0.043364, mae: 0.192982, mean_q: 1.297542
[RESULT] FALSIFICATION!
 21007/100000: episode: 743, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.070, 10.560], loss: 0.725990, mae: 0.505196, mean_q: 1.372528
[RESULT] FALSIFICATION!
 21008/100000: episode: 744, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.400], loss: 1.925683, mae: 0.841822, mean_q: 1.441591
 21009/100000: episode: 745, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.679, mean reward: 0.679 [0.679, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.488], loss: 3.877825, mae: 1.497241, mean_q: 1.930669
[RESULT] FALSIFICATION!
 21010/100000: episode: 746, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.523, 10.441], loss: 1.972327, mae: 1.185043, mean_q: 2.154477
[RESULT] FALSIFICATION!
 21011/100000: episode: 747, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.070, 10.626], loss: 0.922354, mae: 0.802036, mean_q: 1.789080
[RESULT] FALSIFICATION!
 21012/100000: episode: 748, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.014, 10.278], loss: 2.280412, mae: 1.179696, mean_q: 1.958278
[RESULT] FALSIFICATION!
 21013/100000: episode: 749, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.014, 10.102], loss: 0.624141, mae: 0.377427, mean_q: 1.139748
[RESULT] FALSIFICATION!
 21016/100000: episode: 750, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 11.360, mean reward: 3.787 [0.676, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.015, 10.608], loss: 0.713635, mae: 0.415315, mean_q: 1.108750
[RESULT] FALSIFICATION!
 21017/100000: episode: 751, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.035, 10.518], loss: 1.512421, mae: 0.586473, mean_q: 1.284052
[RESULT] FALSIFICATION!
 21018/100000: episode: 752, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.689], loss: 1.550719, mae: 0.916303, mean_q: 1.730099
[RESULT] FALSIFICATION!
 21019/100000: episode: 753, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.013, 10.684], loss: 0.957454, mae: 0.831351, mean_q: 1.874460
[RESULT] FALSIFICATION!
 21020/100000: episode: 754, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.070, 10.455], loss: 0.680066, mae: 0.692003, mean_q: 1.713668
[RESULT] FALSIFICATION!
 21021/100000: episode: 755, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.035, 10.696], loss: 1.806176, mae: 0.650045, mean_q: 1.211938
[RESULT] FALSIFICATION!
 21022/100000: episode: 756, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.547], loss: 2.647225, mae: 0.955031, mean_q: 1.437221
[RESULT] FALSIFICATION!
 21023/100000: episode: 757, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.679], loss: 1.457000, mae: 0.786569, mean_q: 1.603990
[RESULT] FALSIFICATION!
 21024/100000: episode: 758, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.012, 10.701], loss: 0.648394, mae: 0.704727, mean_q: 1.737083
[RESULT] FALSIFICATION!
 21025/100000: episode: 759, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.070, 10.525], loss: 0.055930, mae: 0.221542, mean_q: 1.324377
 21026/100000: episode: 760, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.673, mean reward: 0.673 [0.673, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.070, 10.659], loss: 0.871053, mae: 0.485792, mean_q: 1.245147
[RESULT] FALSIFICATION!
 21027/100000: episode: 761, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.070, 10.683], loss: 0.822456, mae: 0.459687, mean_q: 1.130778
[RESULT] FALSIFICATION!
 21028/100000: episode: 762, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.035, 10.697], loss: 1.892167, mae: 0.841185, mean_q: 1.188624
[RESULT] FALSIFICATION!
 21029/100000: episode: 763, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.070, 10.603], loss: 2.039714, mae: 0.689451, mean_q: 1.269362
[RESULT] FALSIFICATION!
 21030/100000: episode: 764, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.070, 10.565], loss: 1.390747, mae: 0.921824, mean_q: 2.011510
 21031/100000: episode: 765, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.646, mean reward: 0.646 [0.646, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.035, 10.484], loss: 2.084926, mae: 1.348562, mean_q: 2.227676
 21032/100000: episode: 766, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.070, 10.596], loss: 2.784389, mae: 1.677083, mean_q: 2.471812
[RESULT] FALSIFICATION!
 21033/100000: episode: 767, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.035, 10.619], loss: 0.628245, mae: 0.782292, mean_q: 2.026505
[RESULT] FALSIFICATION!
 21034/100000: episode: 768, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.612], loss: 1.904612, mae: 0.648740, mean_q: 1.517570
[RESULT] FALSIFICATION!
 21035/100000: episode: 769, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.013, 10.314], loss: 1.087248, mae: 0.514490, mean_q: 1.096090
 21036/100000: episode: 770, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.680, mean reward: 0.680 [0.680, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.035, 10.568], loss: 0.077980, mae: 0.329932, mean_q: 0.928655
[RESULT] FALSIFICATION!
 21037/100000: episode: 771, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.035, 10.708], loss: 0.808349, mae: 0.417165, mean_q: 1.006832
 21038/100000: episode: 772, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.688, mean reward: 0.688 [0.688, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.602], loss: 0.663162, mae: 0.371053, mean_q: 1.420893
 21039/100000: episode: 773, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.689, mean reward: 0.689 [0.689, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.427], loss: 1.634381, mae: 1.074877, mean_q: 1.959127
[RESULT] FALSIFICATION!
 21040/100000: episode: 774, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.070, 10.660], loss: 2.743109, mae: 1.388139, mean_q: 1.926688
[RESULT] FALSIFICATION!
 21041/100000: episode: 775, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.586], loss: 1.577168, mae: 1.233094, mean_q: 2.186812
[RESULT] FALSIFICATION!
 21042/100000: episode: 776, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.486], loss: 1.363048, mae: 0.941578, mean_q: 1.850379
[RESULT] FALSIFICATION!
 21043/100000: episode: 777, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.490], loss: 0.949507, mae: 0.615969, mean_q: 1.596395
[RESULT] FALSIFICATION!
 21044/100000: episode: 778, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.070, 10.583], loss: 2.250209, mae: 0.848832, mean_q: 1.288276
[RESULT] FALSIFICATION!
 21045/100000: episode: 779, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.035, 10.751], loss: 2.229962, mae: 0.747314, mean_q: 1.205733
[Info] New level: 4.717656135559082 | Considering 10/90 traces
 21046/100000: episode: 780, duration: 3.993s, episode steps: 1, steps per second: 0, episode reward: 0.696, mean reward: 0.696 [0.696, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.070, 10.594], loss: 2.268702, mae: 0.874731, mean_q: 1.435793
[RESULT] FALSIFICATION!
 21047/100000: episode: 781, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.070, 10.654], loss: 0.837937, mae: 0.589578, mean_q: 1.593309
 21048/100000: episode: 782, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.690, mean reward: 0.690 [0.690, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.070, 10.584], loss: 1.307965, mae: 0.878550, mean_q: 1.860993
 21049/100000: episode: 783, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.614, mean reward: 0.614 [0.614, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.481], loss: 1.192235, mae: 0.711561, mean_q: 1.593822
[RESULT] FALSIFICATION!
 21050/100000: episode: 784, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.449 [-0.105, 10.660], loss: 1.305046, mae: 0.579244, mean_q: 1.309986
 21051/100000: episode: 785, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.678, mean reward: 0.678 [0.678, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.105, 10.490], loss: 1.188028, mae: 0.659601, mean_q: 1.532284
[RESULT] FALSIFICATION!
 21052/100000: episode: 786, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-1.037, 10.414], loss: 2.818623, mae: 1.071498, mean_q: 1.471385
[RESULT] FALSIFICATION!
 21053/100000: episode: 787, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.105, 10.548], loss: 0.860694, mae: 0.505106, mean_q: 1.272190
[RESULT] FALSIFICATION!
 21054/100000: episode: 788, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.070, 10.690], loss: 2.162373, mae: 0.891993, mean_q: 1.528840
 21055/100000: episode: 789, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.658, mean reward: 0.658 [0.658, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.382], loss: 1.477481, mae: 0.823141, mean_q: 1.641690
[RESULT] FALSIFICATION!
 21056/100000: episode: 790, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.105, 10.570], loss: 2.513094, mae: 1.061468, mean_q: 1.686729
[RESULT] FALSIFICATION!
 21057/100000: episode: 791, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.339, 10.445], loss: 1.628860, mae: 1.103548, mean_q: 2.053632
[RESULT] FALSIFICATION!
 21058/100000: episode: 792, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.776, 10.466], loss: 1.851381, mae: 0.983505, mean_q: 1.871601
[RESULT] FALSIFICATION!
 21059/100000: episode: 793, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.390 [-0.105, 10.522], loss: 1.554048, mae: 0.889866, mean_q: 1.862349
[RESULT] FALSIFICATION!
 21060/100000: episode: 794, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.413 [-0.105, 10.646], loss: 1.818348, mae: 0.657344, mean_q: 1.290553
 21061/100000: episode: 795, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.035, 10.566], loss: 2.203988, mae: 0.906876, mean_q: 1.673104
[RESULT] FALSIFICATION!
 21062/100000: episode: 796, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.105, 10.558], loss: 1.893320, mae: 0.929033, mean_q: 1.740678
[RESULT] FALSIFICATION!
 21063/100000: episode: 797, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.105, 10.770], loss: 2.509046, mae: 1.320849, mean_q: 2.144221
[RESULT] FALSIFICATION!
 21064/100000: episode: 798, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.447 [-0.105, 10.639], loss: 1.261929, mae: 0.807525, mean_q: 1.676411
[RESULT] FALSIFICATION!
 21065/100000: episode: 799, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.105, 10.490], loss: 1.153057, mae: 0.812264, mean_q: 1.864155
[RESULT] FALSIFICATION!
 21066/100000: episode: 800, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.105, 10.464], loss: 0.843827, mae: 0.897051, mean_q: 2.045495
[RESULT] FALSIFICATION!
 21067/100000: episode: 801, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.105, 10.578], loss: 1.054035, mae: 0.499715, mean_q: 1.046831
[RESULT] FALSIFICATION!
 21068/100000: episode: 802, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.424 [-0.105, 10.653], loss: 1.832853, mae: 0.810322, mean_q: 1.321936
[RESULT] FALSIFICATION!
 21069/100000: episode: 803, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.105, 10.604], loss: 0.991542, mae: 0.577165, mean_q: 1.092250
[RESULT] FALSIFICATION!
 21070/100000: episode: 804, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.480 [-0.105, 10.693], loss: 3.033000, mae: 1.075725, mean_q: 1.440144
[RESULT] FALSIFICATION!
 21071/100000: episode: 805, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.105, 10.719], loss: 0.344172, mae: 0.498658, mean_q: 1.668252
 21072/100000: episode: 806, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.692, mean reward: 0.692 [0.692, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.432 [-0.105, 10.669], loss: 1.159429, mae: 0.926232, mean_q: 1.941826
[RESULT] FALSIFICATION!
 21073/100000: episode: 807, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.494 [-0.105, 10.695], loss: 0.697856, mae: 0.759284, mean_q: 1.892869
[RESULT] FALSIFICATION!
 21074/100000: episode: 808, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.105, 10.603], loss: 2.153392, mae: 0.876742, mean_q: 1.679943
[RESULT] FALSIFICATION!
 21075/100000: episode: 809, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.105, 10.562], loss: 1.409889, mae: 0.643436, mean_q: 1.252215
[RESULT] FALSIFICATION!
 21076/100000: episode: 810, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.105, 10.486], loss: 2.324076, mae: 0.933046, mean_q: 1.296864
[RESULT] FALSIFICATION!
 21077/100000: episode: 811, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.070, 10.678], loss: 2.768334, mae: 0.892112, mean_q: 1.133718
[RESULT] FALSIFICATION!
 21078/100000: episode: 812, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.105, 10.605], loss: 1.188771, mae: 0.804525, mean_q: 1.841463
 21079/100000: episode: 813, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.685, mean reward: 0.685 [0.685, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.070, 10.579], loss: 1.524698, mae: 1.114493, mean_q: 2.074345
 21080/100000: episode: 814, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.666, mean reward: 0.666 [0.666, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.419 [-0.105, 10.601], loss: 1.280079, mae: 1.241376, mean_q: 2.501600
 21081/100000: episode: 815, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.676, mean reward: 0.676 [0.676, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.105, 10.455], loss: 1.449133, mae: 0.854087, mean_q: 1.856753
[RESULT] FALSIFICATION!
 21082/100000: episode: 816, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.105, 10.536], loss: 2.431530, mae: 0.899993, mean_q: 1.404624
[RESULT] FALSIFICATION!
 21083/100000: episode: 817, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.552], loss: 3.334771, mae: 0.946777, mean_q: 1.125682
 21084/100000: episode: 818, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.688, mean reward: 0.688 [0.688, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.520], loss: 2.245243, mae: 0.824713, mean_q: 1.475772
[RESULT] FALSIFICATION!
 21085/100000: episode: 819, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.461 [-0.105, 10.686], loss: 2.135208, mae: 0.816599, mean_q: 1.427476
 21086/100000: episode: 820, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.694, mean reward: 0.694 [0.694, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.453 [-0.105, 10.548], loss: 1.471186, mae: 0.941905, mean_q: 1.951963
 21087/100000: episode: 821, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.691, mean reward: 0.691 [0.691, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.105, 10.497], loss: 0.392971, mae: 0.692363, mean_q: 1.904279
[RESULT] FALSIFICATION!
 21088/100000: episode: 822, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.105, 10.653], loss: 1.175740, mae: 0.757160, mean_q: 1.830610
[RESULT] FALSIFICATION!
 21089/100000: episode: 823, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.158, 10.664], loss: 3.038190, mae: 1.289640, mean_q: 1.911944
[RESULT] FALSIFICATION!
 21090/100000: episode: 824, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.105, 10.578], loss: 1.993912, mae: 0.988941, mean_q: 1.868231
[RESULT] FALSIFICATION!
 21091/100000: episode: 825, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.105, 10.706], loss: 1.886254, mae: 1.030506, mean_q: 1.972651
[RESULT] FALSIFICATION!
 21092/100000: episode: 826, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.105, 10.550], loss: 1.196116, mae: 0.827731, mean_q: 1.873328
 21093/100000: episode: 827, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.181, 10.535], loss: 0.129630, mae: 0.313922, mean_q: 1.365882
 21094/100000: episode: 828, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.686, mean reward: 0.686 [0.686, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.105, 10.661], loss: 1.022942, mae: 0.694436, mean_q: 1.485325
 21095/100000: episode: 829, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.683, mean reward: 0.683 [0.683, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.105, 10.593], loss: 1.722983, mae: 0.737702, mean_q: 1.174416
[RESULT] FALSIFICATION!
 21096/100000: episode: 830, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.105, 10.620], loss: 0.854724, mae: 0.515131, mean_q: 1.164309
[RESULT] FALSIFICATION!
 21097/100000: episode: 831, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.105, 10.698], loss: 1.136147, mae: 0.551259, mean_q: 1.370788
[RESULT] FALSIFICATION!
 21098/100000: episode: 832, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.105, 10.476], loss: 2.182880, mae: 0.987650, mean_q: 1.745434
[RESULT] FALSIFICATION!
 21099/100000: episode: 833, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.433 [-0.105, 10.718], loss: 1.507776, mae: 1.095616, mean_q: 2.227459
 21100/100000: episode: 834, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.627, mean reward: 0.627 [0.627, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.035, 10.534], loss: 0.995363, mae: 0.850994, mean_q: 2.166523
[RESULT] FALSIFICATION!
 21101/100000: episode: 835, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.105, 10.723], loss: 0.596642, mae: 0.410218, mean_q: 1.408333
[RESULT] FALSIFICATION!
 21102/100000: episode: 836, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.105, 10.717], loss: 0.751553, mae: 0.461076, mean_q: 1.422883
[RESULT] FALSIFICATION!
 21103/100000: episode: 837, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.105, 10.653], loss: 3.849589, mae: 1.370639, mean_q: 1.634272
 21104/100000: episode: 838, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.654, mean reward: 0.654 [0.654, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.035, 10.468], loss: 2.777278, mae: 1.177379, mean_q: 1.838066
[RESULT] FALSIFICATION!
 21105/100000: episode: 839, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.070, 10.645], loss: 1.313896, mae: 0.931705, mean_q: 1.839458
[RESULT] FALSIFICATION!
 21106/100000: episode: 840, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.105, 10.649], loss: 1.934276, mae: 1.317114, mean_q: 2.330618
 21107/100000: episode: 841, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.105, 10.631], loss: 1.936724, mae: 1.048788, mean_q: 1.942645
[RESULT] FALSIFICATION!
 21108/100000: episode: 842, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.105, 10.586], loss: 2.054603, mae: 0.872536, mean_q: 1.797889
[RESULT] FALSIFICATION!
 21109/100000: episode: 843, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.105, 10.670], loss: 2.176441, mae: 0.842690, mean_q: 1.242666
[RESULT] FALSIFICATION!
 21110/100000: episode: 844, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.105, 10.547], loss: 2.217320, mae: 0.820778, mean_q: 1.300045
 21111/100000: episode: 845, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.693, mean reward: 0.693 [0.693, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.464], loss: 2.251848, mae: 0.993791, mean_q: 1.627761
[RESULT] FALSIFICATION!
 21112/100000: episode: 846, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.105, 10.731], loss: 1.379834, mae: 0.864113, mean_q: 1.705510
[RESULT] FALSIFICATION!
 21113/100000: episode: 847, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.105, 10.570], loss: 1.251619, mae: 0.727480, mean_q: 1.499383
 21114/100000: episode: 848, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.670, mean reward: 0.670 [0.670, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.070, 10.446], loss: 0.700615, mae: 0.474360, mean_q: 1.353453
[RESULT] FALSIFICATION!
 21115/100000: episode: 849, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.105, 10.551], loss: 1.436388, mae: 0.901783, mean_q: 1.853871
[RESULT] FALSIFICATION!
 21116/100000: episode: 850, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.105, 10.587], loss: 1.930892, mae: 1.156868, mean_q: 2.047167
[RESULT] FALSIFICATION!
 21117/100000: episode: 851, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.460 [-0.105, 10.743], loss: 2.553222, mae: 1.685413, mean_q: 2.844616
 21118/100000: episode: 852, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.070, 10.578], loss: 1.541118, mae: 1.116075, mean_q: 2.296653
 21119/100000: episode: 853, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.672, mean reward: 0.672 [0.672, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.105, 10.471], loss: 1.196496, mae: 0.862442, mean_q: 1.797857
[RESULT] FALSIFICATION!
 21120/100000: episode: 854, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.105, 10.822], loss: 1.368650, mae: 0.652721, mean_q: 1.490623
[RESULT] FALSIFICATION!
 21121/100000: episode: 855, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.070, 10.687], loss: 1.578515, mae: 0.845620, mean_q: 1.686236
[RESULT] FALSIFICATION!
 21122/100000: episode: 856, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.105, 10.541], loss: 1.779066, mae: 0.767292, mean_q: 1.415085
[RESULT] FALSIFICATION!
 21123/100000: episode: 857, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.105, 10.536], loss: 1.817093, mae: 0.901109, mean_q: 1.845297
[RESULT] FALSIFICATION!
 21124/100000: episode: 858, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.105, 10.595], loss: 1.600003, mae: 1.069242, mean_q: 2.337080
[RESULT] FALSIFICATION!
 21125/100000: episode: 859, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.105, 10.485], loss: 0.822165, mae: 0.769944, mean_q: 1.923421
[RESULT] FALSIFICATION!
 21126/100000: episode: 860, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.105, 10.558], loss: 2.016411, mae: 1.163673, mean_q: 2.024659
 21127/100000: episode: 861, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.637, mean reward: 0.637 [0.637, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.035, 10.476], loss: 1.475122, mae: 0.934205, mean_q: 1.975819
 21128/100000: episode: 862, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.595, mean reward: 0.595 [0.595, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.035, 10.548], loss: 1.548489, mae: 0.840984, mean_q: 1.832425
[RESULT] FALSIFICATION!
 21129/100000: episode: 863, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.105, 10.505], loss: 1.713485, mae: 0.879073, mean_q: 1.722549
[RESULT] FALSIFICATION!
 21130/100000: episode: 864, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.070, 10.554], loss: 0.863605, mae: 0.589353, mean_q: 1.595247
 21131/100000: episode: 865, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.695, mean reward: 0.695 [0.695, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.105, 10.555], loss: 0.891699, mae: 0.591061, mean_q: 1.643703
[RESULT] FALSIFICATION!
 21132/100000: episode: 866, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.535, 10.449], loss: 1.177566, mae: 0.584443, mean_q: 1.465075
[RESULT] FALSIFICATION!
 21133/100000: episode: 867, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.105, 10.654], loss: 0.594527, mae: 0.416864, mean_q: 1.252005
[RESULT] FALSIFICATION!
 21134/100000: episode: 868, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.105, 10.684], loss: 2.233386, mae: 1.161168, mean_q: 1.992758
[RESULT] FALSIFICATION!
 21135/100000: episode: 869, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.403 [-0.105, 10.634], loss: 2.721485, mae: 1.171001, mean_q: 1.940954
[RESULT] FALSIFICATION!
[Info] New level: 6.311214447021484 | Considering 10/90 traces
 21136/100000: episode: 870, duration: 4.105s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.105, 10.704], loss: 1.520553, mae: 0.861965, mean_q: 1.770706
[RESULT] FALSIFICATION!
 21137/100000: episode: 871, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.140, 10.673], loss: 0.908892, mae: 0.770636, mean_q: 1.988128
[RESULT] FALSIFICATION!
 21138/100000: episode: 872, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.407 [-0.140, 10.631], loss: 2.080772, mae: 0.971440, mean_q: 1.541212
[RESULT] FALSIFICATION!
 21139/100000: episode: 873, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.140, 10.478], loss: 1.387961, mae: 0.620032, mean_q: 1.094177
[RESULT] FALSIFICATION!
 21140/100000: episode: 874, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.140, 10.599], loss: 0.924040, mae: 0.652885, mean_q: 1.659687
[RESULT] FALSIFICATION!
 21141/100000: episode: 875, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.140, 10.582], loss: 1.504865, mae: 0.834570, mean_q: 1.682646
[RESULT] FALSIFICATION!
 21142/100000: episode: 876, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.140, 10.597], loss: 2.368279, mae: 1.486845, mean_q: 2.973936
[RESULT] FALSIFICATION!
 21143/100000: episode: 877, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.441 [-0.140, 10.664], loss: 1.401124, mae: 1.055501, mean_q: 2.495542
[RESULT] FALSIFICATION!
 21144/100000: episode: 878, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.140, 10.579], loss: 0.935309, mae: 0.559564, mean_q: 1.555645
[RESULT] FALSIFICATION!
 21145/100000: episode: 879, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.140, 10.676], loss: 1.163382, mae: 0.672595, mean_q: 1.733960
[RESULT] FALSIFICATION!
 21146/100000: episode: 880, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.140, 10.518], loss: 1.492909, mae: 0.716177, mean_q: 1.536354
[RESULT] FALSIFICATION!
 21147/100000: episode: 881, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.140, 10.488], loss: 0.945928, mae: 0.698907, mean_q: 1.885516
[RESULT] FALSIFICATION!
 21148/100000: episode: 882, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.140, 10.486], loss: 0.823626, mae: 0.790704, mean_q: 1.961165
[RESULT] FALSIFICATION!
 21149/100000: episode: 883, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.140, 10.699], loss: 1.501312, mae: 0.828014, mean_q: 1.868587
[RESULT] FALSIFICATION!
 21150/100000: episode: 884, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.478 [-0.140, 10.625], loss: 0.706859, mae: 0.568291, mean_q: 1.702372
[RESULT] FALSIFICATION!
 21151/100000: episode: 885, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.140, 10.437], loss: 1.592071, mae: 1.143891, mean_q: 2.560709
[RESULT] FALSIFICATION!
 21152/100000: episode: 886, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.140, 10.533], loss: 1.874143, mae: 1.159309, mean_q: 2.144092
[RESULT] FALSIFICATION!
 21153/100000: episode: 887, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.140, 10.483], loss: 1.491910, mae: 0.885388, mean_q: 1.723482
[RESULT] FALSIFICATION!
 21154/100000: episode: 888, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.140, 10.524], loss: 1.892255, mae: 0.906415, mean_q: 1.455914
[RESULT] FALSIFICATION!
 21155/100000: episode: 889, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.140, 10.574], loss: 0.379539, mae: 0.529643, mean_q: 1.467015
[RESULT] FALSIFICATION!
 21156/100000: episode: 890, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.140, 10.475], loss: 0.592406, mae: 0.596119, mean_q: 1.777390
[RESULT] FALSIFICATION!
 21157/100000: episode: 891, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.452 [-0.140, 10.691], loss: 2.027989, mae: 0.940785, mean_q: 1.692903
[RESULT] FALSIFICATION!
 21158/100000: episode: 892, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.140, 10.786], loss: 2.851268, mae: 1.267915, mean_q: 2.256050
[RESULT] FALSIFICATION!
 21159/100000: episode: 893, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-1.127, 10.400], loss: 1.146573, mae: 0.868705, mean_q: 1.978899
[RESULT] FALSIFICATION!
 21160/100000: episode: 894, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.140, 10.414], loss: 3.780869, mae: 1.681530, mean_q: 2.309742
[RESULT] FALSIFICATION!
 21161/100000: episode: 895, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.140, 10.556], loss: 0.881662, mae: 0.673423, mean_q: 1.734750
[RESULT] FALSIFICATION!
 21162/100000: episode: 896, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.140, 10.488], loss: 1.856223, mae: 0.894802, mean_q: 2.032531
[RESULT] FALSIFICATION!
 21163/100000: episode: 897, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.140, 10.418], loss: 0.821648, mae: 0.560244, mean_q: 1.554507
[RESULT] FALSIFICATION!
 21164/100000: episode: 898, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.140, 10.660], loss: 0.820210, mae: 0.692072, mean_q: 1.903946
[RESULT] FALSIFICATION!
 21165/100000: episode: 899, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.140, 10.606], loss: 1.830878, mae: 1.015631, mean_q: 2.097959
[RESULT] FALSIFICATION!
 21166/100000: episode: 900, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.140, 10.509], loss: 1.536808, mae: 0.779943, mean_q: 1.715562
[RESULT] FALSIFICATION!
 21167/100000: episode: 901, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.452 [-0.140, 10.682], loss: 0.202430, mae: 0.306503, mean_q: 1.238778
[RESULT] FALSIFICATION!
 21168/100000: episode: 902, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.140, 10.566], loss: 0.839952, mae: 0.542982, mean_q: 1.333879
 21169/100000: episode: 903, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.632, mean reward: 0.632 [0.632, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.521 [-0.140, 10.589], loss: 3.043775, mae: 1.113555, mean_q: 1.735590
[RESULT] FALSIFICATION!
 21170/100000: episode: 904, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.140, 10.487], loss: 0.851975, mae: 0.649925, mean_q: 1.764667
[RESULT] FALSIFICATION!
 21171/100000: episode: 905, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.140, 10.614], loss: 1.003997, mae: 1.008030, mean_q: 2.306611
[RESULT] FALSIFICATION!
 21172/100000: episode: 906, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.140, 10.517], loss: 2.222740, mae: 1.532579, mean_q: 2.762469
[RESULT] FALSIFICATION!
 21173/100000: episode: 907, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.140, 10.596], loss: 0.968346, mae: 0.951189, mean_q: 2.283227
[RESULT] FALSIFICATION!
 21174/100000: episode: 908, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.140, 10.770], loss: 1.616901, mae: 0.938838, mean_q: 2.191638
[RESULT] FALSIFICATION!
 21175/100000: episode: 909, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.140, 10.605], loss: 1.195524, mae: 0.726578, mean_q: 1.805554
 21176/100000: episode: 910, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.698, mean reward: 0.698 [0.698, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.504 [-0.140, 10.620], loss: 1.144996, mae: 0.731562, mean_q: 1.404628
[RESULT] FALSIFICATION!
 21177/100000: episode: 911, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.140, 10.554], loss: 0.962377, mae: 0.647567, mean_q: 1.200630
[RESULT] FALSIFICATION!
 21178/100000: episode: 912, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.140, 10.591], loss: 1.703423, mae: 0.729081, mean_q: 1.305268
 21179/100000: episode: 913, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.679, mean reward: 0.679 [0.679, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.140, 10.549], loss: 1.874703, mae: 0.859518, mean_q: 1.731685
[RESULT] FALSIFICATION!
 21180/100000: episode: 914, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.463 [-0.140, 10.805], loss: 0.634804, mae: 0.721131, mean_q: 2.118306
[RESULT] FALSIFICATION!
 21181/100000: episode: 915, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.140, 10.516], loss: 1.611324, mae: 1.151377, mean_q: 2.471383
[RESULT] FALSIFICATION!
 21182/100000: episode: 916, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.140, 10.566], loss: 1.000620, mae: 0.544098, mean_q: 1.435855
[RESULT] FALSIFICATION!
 21183/100000: episode: 917, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.140, 10.606], loss: 0.269158, mae: 0.348602, mean_q: 1.670234
[RESULT] FALSIFICATION!
 21184/100000: episode: 918, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.140, 10.444], loss: 0.485267, mae: 0.356267, mean_q: 1.236498
[RESULT] FALSIFICATION!
 21185/100000: episode: 919, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.140, 10.594], loss: 0.378487, mae: 0.433526, mean_q: 1.591550
[RESULT] FALSIFICATION!
 21186/100000: episode: 920, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.140, 10.551], loss: 1.362000, mae: 0.774419, mean_q: 1.810367
[RESULT] FALSIFICATION!
 21187/100000: episode: 921, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.440 [-0.140, 10.636], loss: 1.285051, mae: 0.873397, mean_q: 2.147123
[RESULT] FALSIFICATION!
 21188/100000: episode: 922, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.140, 10.548], loss: 3.331654, mae: 1.796048, mean_q: 2.886924
[RESULT] FALSIFICATION!
 21189/100000: episode: 923, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.140, 10.486], loss: 0.902463, mae: 0.750947, mean_q: 2.078825
[RESULT] FALSIFICATION!
 21190/100000: episode: 924, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.140, 10.555], loss: 1.768673, mae: 0.909202, mean_q: 1.628075
[RESULT] FALSIFICATION!
 21191/100000: episode: 925, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.140, 10.526], loss: 0.968126, mae: 0.762555, mean_q: 1.668174
[RESULT] FALSIFICATION!
 21192/100000: episode: 926, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.140, 10.492], loss: 0.064611, mae: 0.286128, mean_q: 1.038674
[RESULT] FALSIFICATION!
 21193/100000: episode: 927, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.140, 10.487], loss: 0.557788, mae: 0.484263, mean_q: 1.166932
[RESULT] FALSIFICATION!
 21194/100000: episode: 928, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.140, 10.440], loss: 1.606042, mae: 0.796465, mean_q: 1.835705
[RESULT] FALSIFICATION!
 21195/100000: episode: 929, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.463 [-0.140, 10.706], loss: 1.630545, mae: 0.757080, mean_q: 1.586065
 21196/100000: episode: 930, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.659, mean reward: 0.659 [0.659, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.445 [-0.140, 10.527], loss: 2.165859, mae: 1.063563, mean_q: 1.700658
[RESULT] FALSIFICATION!
 21197/100000: episode: 931, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.493 [-0.140, 10.651], loss: 0.999210, mae: 0.884324, mean_q: 2.195547
[RESULT] FALSIFICATION!
 21198/100000: episode: 932, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.140, 10.441], loss: 4.222024, mae: 1.728902, mean_q: 2.199271
[RESULT] FALSIFICATION!
 21199/100000: episode: 933, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.140, 10.477], loss: 1.844664, mae: 0.893850, mean_q: 1.590356
[RESULT] FALSIFICATION!
 21200/100000: episode: 934, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.140, 10.467], loss: 1.088102, mae: 1.041548, mean_q: 2.123867
[RESULT] FALSIFICATION!
 21201/100000: episode: 935, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.140, 10.525], loss: 1.917310, mae: 1.412227, mean_q: 2.925081
[RESULT] FALSIFICATION!
 21202/100000: episode: 936, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.140, 10.616], loss: 1.673011, mae: 0.982062, mean_q: 1.968768
[RESULT] FALSIFICATION!
 21203/100000: episode: 937, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.417 [-0.140, 10.527], loss: 1.362681, mae: 0.837360, mean_q: 1.731065
[RESULT] FALSIFICATION!
 21204/100000: episode: 938, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.140, 10.489], loss: 2.930038, mae: 1.055356, mean_q: 1.248852
[RESULT] FALSIFICATION!
 21205/100000: episode: 939, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.436 [-0.140, 10.531], loss: 1.015683, mae: 0.720537, mean_q: 1.643192
[RESULT] FALSIFICATION!
 21206/100000: episode: 940, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.140, 10.708], loss: 0.724668, mae: 0.665705, mean_q: 1.633198
[RESULT] FALSIFICATION!
 21207/100000: episode: 941, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.140, 10.487], loss: 1.860890, mae: 1.023810, mean_q: 2.025831
[RESULT] FALSIFICATION!
 21208/100000: episode: 942, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.140, 10.570], loss: 0.353450, mae: 0.453961, mean_q: 1.666061
[RESULT] FALSIFICATION!
 21209/100000: episode: 943, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.140, 10.568], loss: 1.275161, mae: 1.039151, mean_q: 2.565575
[RESULT] FALSIFICATION!
 21210/100000: episode: 944, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.140, 10.633], loss: 0.602778, mae: 0.629202, mean_q: 1.887903
[RESULT] FALSIFICATION!
 21211/100000: episode: 945, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.476 [-0.140, 10.730], loss: 1.190495, mae: 0.830770, mean_q: 2.237945
[RESULT] FALSIFICATION!
 21212/100000: episode: 946, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.140, 10.600], loss: 2.485887, mae: 1.045517, mean_q: 1.690891
[RESULT] FALSIFICATION!
 21213/100000: episode: 947, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.140, 10.487], loss: 1.531374, mae: 0.995640, mean_q: 2.119892
[RESULT] FALSIFICATION!
 21214/100000: episode: 948, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.140, 10.458], loss: 3.594112, mae: 1.363851, mean_q: 2.236268
[RESULT] FALSIFICATION!
 21215/100000: episode: 949, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.140, 10.666], loss: 1.099537, mae: 0.772544, mean_q: 2.244349
[RESULT] FALSIFICATION!
 21216/100000: episode: 950, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.383, 10.506], loss: 1.356245, mae: 1.069916, mean_q: 2.688848
[RESULT] FALSIFICATION!
 21217/100000: episode: 951, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.140, 10.481], loss: 0.694909, mae: 0.528175, mean_q: 1.701298
[RESULT] FALSIFICATION!
 21218/100000: episode: 952, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.140, 10.616], loss: 0.520626, mae: 0.460046, mean_q: 1.486313
 21219/100000: episode: 953, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.689, mean reward: 0.689 [0.689, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.140, 10.457], loss: 1.104338, mae: 0.658068, mean_q: 1.620301
[RESULT] FALSIFICATION!
 21220/100000: episode: 954, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.140, 10.493], loss: 1.825337, mae: 1.121182, mean_q: 2.306188
[RESULT] FALSIFICATION!
 21221/100000: episode: 955, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.140, 10.661], loss: 0.607779, mae: 0.669023, mean_q: 2.347755
 21222/100000: episode: 956, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.675, mean reward: 0.675 [0.675, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.426 [-0.140, 10.536], loss: 2.652624, mae: 1.141346, mean_q: 1.958534
[RESULT] FALSIFICATION!
 21223/100000: episode: 957, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.411 [-0.140, 10.477], loss: 2.314109, mae: 1.240708, mean_q: 2.572727
[RESULT] FALSIFICATION!
 21224/100000: episode: 958, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.140, 10.522], loss: 1.017096, mae: 0.869706, mean_q: 2.049215
[RESULT] FALSIFICATION!
 21225/100000: episode: 959, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.140, 10.522], loss: 1.848196, mae: 0.972192, mean_q: 2.118035
[RESULT] FALSIFICATION!
[Info] New level: 8.075307846069336 | Considering 11/89 traces
 21226/100000: episode: 960, duration: 4.176s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.140, 10.551], loss: 1.303272, mae: 0.835256, mean_q: 1.980842
[RESULT] FALSIFICATION!
 21227/100000: episode: 961, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.140, 10.543], loss: 0.584097, mae: 0.548639, mean_q: 1.631883
 21228/100000: episode: 962, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.678, mean reward: 0.678 [0.678, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.140, 10.511], loss: 2.132183, mae: 1.074502, mean_q: 2.032260
[RESULT] FALSIFICATION!
 21229/100000: episode: 963, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.175, 10.588], loss: 0.694637, mae: 0.702024, mean_q: 2.084620
[RESULT] FALSIFICATION!
 21230/100000: episode: 964, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.175, 10.509], loss: 1.165983, mae: 0.891657, mean_q: 2.317730
[RESULT] FALSIFICATION!
 21231/100000: episode: 965, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.140, 10.531], loss: 2.208768, mae: 1.204372, mean_q: 2.495321
[RESULT] FALSIFICATION!
 21232/100000: episode: 966, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.140, 10.530], loss: 2.082314, mae: 0.951635, mean_q: 1.979919
[RESULT] FALSIFICATION!
 21233/100000: episode: 967, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.175, 10.522], loss: 1.596456, mae: 0.897405, mean_q: 1.970039
[RESULT] FALSIFICATION!
 21234/100000: episode: 968, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.140, 10.484], loss: 1.171889, mae: 0.779062, mean_q: 2.195785
[RESULT] FALSIFICATION!
 21235/100000: episode: 969, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.454 [-0.140, 10.619], loss: 2.958721, mae: 1.267136, mean_q: 2.559162
[RESULT] FALSIFICATION!
 21236/100000: episode: 970, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.175, 10.500], loss: 1.381595, mae: 0.878623, mean_q: 2.200661
[RESULT] FALSIFICATION!
 21237/100000: episode: 971, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.225, 10.400], loss: 1.207669, mae: 0.770259, mean_q: 2.024277
[RESULT] FALSIFICATION!
 21238/100000: episode: 972, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.140, 10.528], loss: 0.492805, mae: 0.595587, mean_q: 1.982168
 21239/100000: episode: 973, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.694, mean reward: 0.694 [0.694, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.140, 10.616], loss: 1.213119, mae: 0.473716, mean_q: 1.268550
[RESULT] FALSIFICATION!
 21240/100000: episode: 974, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.140, 10.449], loss: 1.030834, mae: 0.618494, mean_q: 1.557584
[RESULT] FALSIFICATION!
 21241/100000: episode: 975, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.140, 10.426], loss: 1.504621, mae: 0.784579, mean_q: 1.797536
[RESULT] FALSIFICATION!
 21242/100000: episode: 976, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.140, 10.461], loss: 2.109622, mae: 1.061672, mean_q: 1.990526
 21243/100000: episode: 977, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.689, mean reward: 0.689 [0.689, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.140, 10.509], loss: 2.150317, mae: 0.936141, mean_q: 1.865265
[RESULT] FALSIFICATION!
 21244/100000: episode: 978, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.140, 10.536], loss: 1.261663, mae: 0.806467, mean_q: 1.977656
[RESULT] FALSIFICATION!
 21245/100000: episode: 979, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.140, 10.468], loss: 1.823971, mae: 0.949392, mean_q: 2.296753
[RESULT] FALSIFICATION!
 21246/100000: episode: 980, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.140, 10.522], loss: 0.657912, mae: 0.646032, mean_q: 2.114470
[RESULT] FALSIFICATION!
 21247/100000: episode: 981, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.175, 10.679], loss: 2.345340, mae: 1.011419, mean_q: 1.757289
[RESULT] FALSIFICATION!
 21248/100000: episode: 982, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.175, 10.573], loss: 1.633035, mae: 0.832974, mean_q: 1.757733
[RESULT] FALSIFICATION!
 21249/100000: episode: 983, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.140, 10.612], loss: 2.170898, mae: 1.153425, mean_q: 2.220789
[RESULT] FALSIFICATION!
 21250/100000: episode: 984, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.140, 10.584], loss: 2.998371, mae: 1.176560, mean_q: 2.001578
[RESULT] FALSIFICATION!
 21251/100000: episode: 985, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.175, 10.500], loss: 2.710561, mae: 1.181884, mean_q: 2.751670
[RESULT] FALSIFICATION!
 21252/100000: episode: 986, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.175, 10.500], loss: 0.927433, mae: 0.735569, mean_q: 2.174388
[RESULT] FALSIFICATION!
 21253/100000: episode: 987, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.140, 10.530], loss: 1.366858, mae: 0.621101, mean_q: 1.614874
[RESULT] FALSIFICATION!
 21254/100000: episode: 988, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.140, 10.565], loss: 1.151185, mae: 0.679948, mean_q: 1.923739
 21255/100000: episode: 989, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.691, mean reward: 0.691 [0.691, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.140, 10.466], loss: 0.901058, mae: 0.561273, mean_q: 1.679279
[RESULT] FALSIFICATION!
 21256/100000: episode: 990, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.175, 10.571], loss: 1.293032, mae: 0.534894, mean_q: 1.506271
[RESULT] FALSIFICATION!
 21257/100000: episode: 991, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.140, 10.507], loss: 1.291507, mae: 0.702742, mean_q: 1.614620
[RESULT] FALSIFICATION!
 21258/100000: episode: 992, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.140, 10.558], loss: 0.477138, mae: 0.454459, mean_q: 1.741914
[RESULT] FALSIFICATION!
 21259/100000: episode: 993, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.140, 10.485], loss: 2.187260, mae: 1.069172, mean_q: 2.313836
[RESULT] FALSIFICATION!
 21260/100000: episode: 994, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.422 [-0.175, 10.593], loss: 1.442587, mae: 0.800177, mean_q: 1.908303
[RESULT] FALSIFICATION!
 21261/100000: episode: 995, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.140, 10.400], loss: 1.664614, mae: 0.843060, mean_q: 1.861537
[RESULT] FALSIFICATION!
 21262/100000: episode: 996, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.140, 10.556], loss: 0.160089, mae: 0.310786, mean_q: 1.582158
[RESULT] FALSIFICATION!
 21263/100000: episode: 997, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.140, 10.628], loss: 1.625368, mae: 0.971022, mean_q: 2.307534
[RESULT] FALSIFICATION!
 21264/100000: episode: 998, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.175, 10.617], loss: 3.350093, mae: 1.286144, mean_q: 2.161666
[RESULT] FALSIFICATION!
 21265/100000: episode: 999, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.140, 10.465], loss: 1.650902, mae: 0.935142, mean_q: 2.097499
[RESULT] FALSIFICATION!
 21266/100000: episode: 1000, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.197, 10.500], loss: 1.613892, mae: 0.911146, mean_q: 2.301125
[RESULT] FALSIFICATION!
 21267/100000: episode: 1001, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.140, 10.486], loss: 2.490607, mae: 1.159897, mean_q: 2.308311
[RESULT] FALSIFICATION!
 21268/100000: episode: 1002, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.140, 10.512], loss: 2.518352, mae: 1.093821, mean_q: 2.365496
[RESULT] FALSIFICATION!
 21269/100000: episode: 1003, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.140, 10.520], loss: 2.144310, mae: 1.131518, mean_q: 2.548422
[RESULT] FALSIFICATION!
 21270/100000: episode: 1004, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.140, 10.400], loss: 1.454201, mae: 0.837901, mean_q: 1.605805
[RESULT] FALSIFICATION!
 21271/100000: episode: 1005, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.140, 10.529], loss: 1.177808, mae: 0.793341, mean_q: 1.626348
[RESULT] FALSIFICATION!
 21272/100000: episode: 1006, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.140, 10.489], loss: 1.125889, mae: 0.740045, mean_q: 1.753701
[RESULT] FALSIFICATION!
 21273/100000: episode: 1007, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.140, 10.434], loss: 1.619302, mae: 0.847288, mean_q: 2.191594
 21274/100000: episode: 1008, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.648, mean reward: 0.648 [0.648, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.486 [-0.140, 10.566], loss: 0.771613, mae: 0.613148, mean_q: 1.860288
[RESULT] FALSIFICATION!
 21275/100000: episode: 1009, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.140, 10.469], loss: 1.457121, mae: 0.835731, mean_q: 2.153981
[RESULT] FALSIFICATION!
 21276/100000: episode: 1010, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.140, 10.448], loss: 1.752320, mae: 0.963418, mean_q: 2.291891
[RESULT] FALSIFICATION!
 21277/100000: episode: 1011, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.140, 10.479], loss: 0.454219, mae: 0.540017, mean_q: 2.093032
[RESULT] FALSIFICATION!
 21278/100000: episode: 1012, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.140, 10.518], loss: 2.254114, mae: 1.139941, mean_q: 2.258323
[RESULT] FALSIFICATION!
 21279/100000: episode: 1013, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.420 [-0.140, 10.500], loss: 0.630888, mae: 0.500538, mean_q: 1.739926
 21280/100000: episode: 1014, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.669, mean reward: 0.669 [0.669, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.447 [-0.140, 10.497], loss: 1.872153, mae: 1.061247, mean_q: 2.685647
[RESULT] FALSIFICATION!
 21281/100000: episode: 1015, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.140, 10.404], loss: 1.563627, mae: 0.903561, mean_q: 2.317743
[RESULT] FALSIFICATION!
 21282/100000: episode: 1016, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.140, 10.552], loss: 0.991058, mae: 0.653944, mean_q: 2.049888
[RESULT] FALSIFICATION!
 21283/100000: episode: 1017, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.140, 10.466], loss: 0.861132, mae: 0.620503, mean_q: 1.834744
[RESULT] FALSIFICATION!
 21284/100000: episode: 1018, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.140, 10.504], loss: 0.765524, mae: 0.509491, mean_q: 1.692742
[RESULT] FALSIFICATION!
 21285/100000: episode: 1019, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.140, 10.574], loss: 1.283686, mae: 0.704630, mean_q: 1.612581
[RESULT] FALSIFICATION!
 21286/100000: episode: 1020, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.175, 10.670], loss: 1.593958, mae: 0.870402, mean_q: 2.413614
[RESULT] FALSIFICATION!
 21287/100000: episode: 1021, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.140, 10.570], loss: 2.048038, mae: 0.920369, mean_q: 1.674368
[RESULT] FALSIFICATION!
 21288/100000: episode: 1022, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.432 [-0.175, 10.590], loss: 1.372715, mae: 0.917817, mean_q: 2.472039
[RESULT] FALSIFICATION!
 21289/100000: episode: 1023, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.403 [-0.140, 10.566], loss: 1.376325, mae: 1.119594, mean_q: 2.657290
[RESULT] FALSIFICATION!
 21290/100000: episode: 1024, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.140, 10.545], loss: 1.078677, mae: 0.644014, mean_q: 1.826690
 21291/100000: episode: 1025, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.700, mean reward: 0.700 [0.700, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.140, 10.590], loss: 1.913953, mae: 0.843058, mean_q: 1.812149
[RESULT] FALSIFICATION!
 21292/100000: episode: 1026, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.175, 10.558], loss: 0.743398, mae: 0.573373, mean_q: 1.964695
 21293/100000: episode: 1027, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.677, mean reward: 0.677 [0.677, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.487 [-0.140, 10.613], loss: 1.863846, mae: 0.675632, mean_q: 1.570528
[RESULT] FALSIFICATION!
 21294/100000: episode: 1028, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.140, 10.501], loss: 2.237447, mae: 0.915651, mean_q: 1.953022
 21295/100000: episode: 1029, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.667, mean reward: 0.667 [0.667, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.140, 10.451], loss: 0.621200, mae: 0.588582, mean_q: 1.753102
[RESULT] FALSIFICATION!
 21296/100000: episode: 1030, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.175, 10.564], loss: 2.188471, mae: 1.073805, mean_q: 2.564691
[RESULT] FALSIFICATION!
 21297/100000: episode: 1031, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.175, 10.536], loss: 1.386052, mae: 0.919824, mean_q: 2.323199
[RESULT] FALSIFICATION!
 21298/100000: episode: 1032, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.140, 10.400], loss: 3.878653, mae: 1.503576, mean_q: 2.873878
 21299/100000: episode: 1033, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.674, mean reward: 0.674 [0.674, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.140, 10.400], loss: 1.056168, mae: 0.850728, mean_q: 2.592089
[RESULT] FALSIFICATION!
 21300/100000: episode: 1034, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.506, 10.400], loss: 0.128828, mae: 0.259472, mean_q: 1.279728
 21301/100000: episode: 1035, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.682, mean reward: 0.682 [0.682, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.140, 10.642], loss: 1.311936, mae: 0.668660, mean_q: 1.327702
[RESULT] FALSIFICATION!
 21302/100000: episode: 1036, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.140, 10.500], loss: 0.295367, mae: 0.377967, mean_q: 1.404096
[RESULT] FALSIFICATION!
 21303/100000: episode: 1037, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.140, 10.481], loss: 1.864977, mae: 0.978719, mean_q: 1.887635
[RESULT] FALSIFICATION!
 21304/100000: episode: 1038, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.463 [-0.140, 10.597], loss: 0.437371, mae: 0.455688, mean_q: 1.653948
[RESULT] FALSIFICATION!
 21305/100000: episode: 1039, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.175, 10.511], loss: 2.916005, mae: 1.010888, mean_q: 1.545851
[RESULT] FALSIFICATION!
 21306/100000: episode: 1040, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.140, 10.613], loss: 2.550301, mae: 1.102080, mean_q: 2.473210
[RESULT] FALSIFICATION!
 21307/100000: episode: 1041, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.175, 10.500], loss: 0.833059, mae: 0.623853, mean_q: 2.274374
[RESULT] FALSIFICATION!
 21308/100000: episode: 1042, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.466 [-0.175, 10.617], loss: 3.281087, mae: 1.295826, mean_q: 2.495429
[RESULT] FALSIFICATION!
 21309/100000: episode: 1043, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.459 [-0.140, 10.540], loss: 4.439176, mae: 1.426157, mean_q: 1.632002
[RESULT] FALSIFICATION!
 21310/100000: episode: 1044, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.140, 10.420], loss: 0.426347, mae: 0.501012, mean_q: 2.211635
[RESULT] FALSIFICATION!
 21311/100000: episode: 1045, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.460 [-0.140, 10.675], loss: 0.859892, mae: 0.640213, mean_q: 2.014337
[RESULT] FALSIFICATION!
 21312/100000: episode: 1046, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.140, 10.400], loss: 2.142757, mae: 1.034405, mean_q: 2.015556
[RESULT] FALSIFICATION!
 21313/100000: episode: 1047, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.175, 10.574], loss: 1.462839, mae: 1.025482, mean_q: 3.058646
[RESULT] FALSIFICATION!
 21314/100000: episode: 1048, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.175, 10.559], loss: 2.553536, mae: 1.334245, mean_q: 2.913676
[RESULT] FALSIFICATION!
[Info] New level: 9.0943603515625 | Considering 13/87 traces
 21315/100000: episode: 1049, duration: 4.114s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.140, 10.514], loss: 3.342582, mae: 1.427416, mean_q: 2.328911
[RESULT] FALSIFICATION!
 21316/100000: episode: 1050, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.175, 10.500], loss: 0.711486, mae: 0.628343, mean_q: 1.991200
[RESULT] FALSIFICATION!
 21317/100000: episode: 1051, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.175, 10.580], loss: 1.477695, mae: 0.809343, mean_q: 1.780298
[RESULT] FALSIFICATION!
 21318/100000: episode: 1052, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.175, 10.552], loss: 2.536501, mae: 1.048254, mean_q: 2.243325
[RESULT] FALSIFICATION!
 21319/100000: episode: 1053, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.175, 10.587], loss: 4.444989, mae: 1.611923, mean_q: 2.511207
[RESULT] FALSIFICATION!
 21320/100000: episode: 1054, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.175, 10.500], loss: 2.088562, mae: 1.162642, mean_q: 3.114051
[RESULT] FALSIFICATION!
 21321/100000: episode: 1055, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.417 [-0.175, 10.500], loss: 1.252720, mae: 0.707904, mean_q: 1.916324
[RESULT] FALSIFICATION!
 21322/100000: episode: 1056, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.175, 10.555], loss: 0.649731, mae: 0.553405, mean_q: 2.271965
[RESULT] FALSIFICATION!
 21323/100000: episode: 1057, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.461 [-0.210, 10.600], loss: 0.907006, mae: 0.654217, mean_q: 1.916502
[RESULT] FALSIFICATION!
 21324/100000: episode: 1058, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.175, 10.576], loss: 0.843212, mae: 0.637600, mean_q: 1.768283
[RESULT] FALSIFICATION!
 21325/100000: episode: 1059, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.175, 10.500], loss: 1.653288, mae: 1.151090, mean_q: 2.755721
[RESULT] FALSIFICATION!
 21326/100000: episode: 1060, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.493 [-0.175, 10.761], loss: 1.584472, mae: 0.842653, mean_q: 2.462487
[RESULT] FALSIFICATION!
 21327/100000: episode: 1061, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.175, 10.500], loss: 1.211851, mae: 0.569936, mean_q: 2.198682
[RESULT] FALSIFICATION!
 21328/100000: episode: 1062, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.472 [-0.175, 10.627], loss: 1.577519, mae: 0.854926, mean_q: 2.557347
[RESULT] FALSIFICATION!
 21329/100000: episode: 1063, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.457 [-0.210, 10.615], loss: 2.076965, mae: 0.961149, mean_q: 1.778434
[RESULT] FALSIFICATION!
 21330/100000: episode: 1064, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.492 [-0.175, 10.655], loss: 2.548719, mae: 1.087882, mean_q: 1.947453
[RESULT] FALSIFICATION!
 21331/100000: episode: 1065, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.175, 10.500], loss: 1.268449, mae: 0.760860, mean_q: 2.155616
 21332/100000: episode: 1066, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.670, mean reward: 0.670 [0.670, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.458 [-0.175, 10.589], loss: 0.537482, mae: 0.591389, mean_q: 2.011360
[RESULT] FALSIFICATION!
 21333/100000: episode: 1067, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.452 [-0.175, 10.673], loss: 0.625604, mae: 0.562371, mean_q: 2.073537
[RESULT] FALSIFICATION!
 21334/100000: episode: 1068, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.175, 10.500], loss: 3.217403, mae: 1.243335, mean_q: 2.035228
[RESULT] FALSIFICATION!
 21335/100000: episode: 1069, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.175, 10.500], loss: 0.951489, mae: 0.817110, mean_q: 2.704387
[RESULT] FALSIFICATION!
 21336/100000: episode: 1070, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.405 [-0.175, 10.502], loss: 2.035560, mae: 1.317297, mean_q: 3.340303
[RESULT] FALSIFICATION!
 21337/100000: episode: 1071, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.175, 10.561], loss: 1.121691, mae: 0.935816, mean_q: 2.609328
[RESULT] FALSIFICATION!
 21338/100000: episode: 1072, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.462 [-0.175, 10.604], loss: 2.200221, mae: 1.158779, mean_q: 2.555780
[RESULT] FALSIFICATION!
 21339/100000: episode: 1073, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.175, 10.557], loss: 1.965200, mae: 0.931117, mean_q: 1.877219
[RESULT] FALSIFICATION!
 21340/100000: episode: 1074, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.175, 10.551], loss: 1.318347, mae: 0.794663, mean_q: 2.152750
[RESULT] FALSIFICATION!
 21341/100000: episode: 1075, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.175, 10.608], loss: 1.787453, mae: 1.013464, mean_q: 2.194726
[RESULT] FALSIFICATION!
 21342/100000: episode: 1076, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.175, 10.500], loss: 2.714329, mae: 1.030891, mean_q: 1.473077
[RESULT] FALSIFICATION!
 21343/100000: episode: 1077, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.175, 10.500], loss: 1.611886, mae: 0.851738, mean_q: 2.352642
[RESULT] FALSIFICATION!
 21344/100000: episode: 1078, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.210, 10.600], loss: 1.671518, mae: 0.714032, mean_q: 1.881606
[RESULT] FALSIFICATION!
 21345/100000: episode: 1079, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.175, 10.500], loss: 2.402883, mae: 1.205481, mean_q: 2.499237
[RESULT] FALSIFICATION!
 21346/100000: episode: 1080, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.175, 10.562], loss: 2.016516, mae: 1.510957, mean_q: 3.237021
[RESULT] FALSIFICATION!
 21347/100000: episode: 1081, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.175, 10.533], loss: 0.843553, mae: 0.805759, mean_q: 2.841836
[RESULT] FALSIFICATION!
 21348/100000: episode: 1082, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.506 [-0.175, 10.768], loss: 1.029976, mae: 0.793773, mean_q: 2.717383
[RESULT] FALSIFICATION!
 21349/100000: episode: 1083, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.500 [-0.175, 10.665], loss: 2.279352, mae: 1.020371, mean_q: 2.471146
[RESULT] FALSIFICATION!
 21350/100000: episode: 1084, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.175, 10.500], loss: 2.419909, mae: 0.982934, mean_q: 2.232719
[RESULT] FALSIFICATION!
 21351/100000: episode: 1085, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.175, 10.501], loss: 1.266695, mae: 0.815170, mean_q: 2.118236
[RESULT] FALSIFICATION!
 21352/100000: episode: 1086, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.432 [-0.210, 10.600], loss: 2.854699, mae: 1.033612, mean_q: 1.760343
[RESULT] FALSIFICATION!
 21353/100000: episode: 1087, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.390 [-0.175, 10.500], loss: 1.951026, mae: 0.986685, mean_q: 2.056585
[RESULT] FALSIFICATION!
 21354/100000: episode: 1088, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.175, 10.678], loss: 4.746270, mae: 1.897143, mean_q: 3.717739
[RESULT] FALSIFICATION!
 21355/100000: episode: 1089, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.175, 10.541], loss: 2.299114, mae: 1.175182, mean_q: 2.628937
[RESULT] FALSIFICATION!
 21356/100000: episode: 1090, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.175, 10.500], loss: 2.463599, mae: 1.092278, mean_q: 2.409571
[RESULT] FALSIFICATION!
 21357/100000: episode: 1091, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.451 [-0.210, 10.600], loss: 1.936567, mae: 0.819052, mean_q: 1.844107
[RESULT] FALSIFICATION!
 21358/100000: episode: 1092, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.210, 10.600], loss: 1.722722, mae: 0.947956, mean_q: 2.327565
[RESULT] FALSIFICATION!
 21359/100000: episode: 1093, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.413 [-0.175, 10.500], loss: 2.598317, mae: 1.114995, mean_q: 2.061839
[RESULT] FALSIFICATION!
 21360/100000: episode: 1094, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.175, 10.600], loss: 1.116376, mae: 0.893360, mean_q: 2.356647
[RESULT] FALSIFICATION!
 21361/100000: episode: 1095, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.175, 10.500], loss: 1.308251, mae: 0.765784, mean_q: 2.027284
[RESULT] FALSIFICATION!
 21362/100000: episode: 1096, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.175, 10.500], loss: 1.556485, mae: 0.805356, mean_q: 2.045944
[RESULT] FALSIFICATION!
 21363/100000: episode: 1097, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.210, 10.600], loss: 1.394175, mae: 0.781917, mean_q: 2.788986
[RESULT] FALSIFICATION!
 21364/100000: episode: 1098, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.175, 10.580], loss: 1.892543, mae: 0.731497, mean_q: 2.375801
[RESULT] FALSIFICATION!
 21365/100000: episode: 1099, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.175, 10.500], loss: 1.428399, mae: 0.723529, mean_q: 2.257194
[RESULT] FALSIFICATION!
 21366/100000: episode: 1100, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.411 [-0.175, 10.541], loss: 2.013946, mae: 1.017085, mean_q: 2.358732
[RESULT] FALSIFICATION!
 21367/100000: episode: 1101, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.175, 10.504], loss: 2.728522, mae: 1.336264, mean_q: 2.826984
[RESULT] FALSIFICATION!
 21368/100000: episode: 1102, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.175, 10.500], loss: 1.107271, mae: 0.755189, mean_q: 2.305106
[RESULT] FALSIFICATION!
 21369/100000: episode: 1103, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.175, 10.604], loss: 2.026215, mae: 1.208439, mean_q: 2.890608
[RESULT] FALSIFICATION!
 21370/100000: episode: 1104, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.175, 10.551], loss: 2.127485, mae: 1.086082, mean_q: 2.523193
[RESULT] FALSIFICATION!
 21371/100000: episode: 1105, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.462 [-0.175, 10.683], loss: 3.503463, mae: 1.546840, mean_q: 2.721563
[RESULT] FALSIFICATION!
 21372/100000: episode: 1106, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.175, 10.526], loss: 2.746889, mae: 1.453248, mean_q: 3.150290
[RESULT] FALSIFICATION!
 21373/100000: episode: 1107, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.447 [-0.175, 10.510], loss: 1.674966, mae: 0.914590, mean_q: 1.951863
[RESULT] FALSIFICATION!
 21374/100000: episode: 1108, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.175, 10.572], loss: 3.322355, mae: 1.114523, mean_q: 1.791702
[RESULT] FALSIFICATION!
 21375/100000: episode: 1109, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.424 [-0.175, 10.556], loss: 2.211588, mae: 0.971111, mean_q: 1.921216
[RESULT] FALSIFICATION!
 21376/100000: episode: 1110, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.175, 10.601], loss: 4.492179, mae: 1.503298, mean_q: 1.979293
[RESULT] FALSIFICATION!
 21377/100000: episode: 1111, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.175, 10.564], loss: 2.010956, mae: 1.213225, mean_q: 3.267678
[RESULT] FALSIFICATION!
 21378/100000: episode: 1112, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.175, 10.500], loss: 1.703055, mae: 1.041544, mean_q: 3.680317
[RESULT] FALSIFICATION!
 21379/100000: episode: 1113, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.439 [-0.175, 10.643], loss: 1.980747, mae: 1.365810, mean_q: 3.008117
[RESULT] FALSIFICATION!
 21380/100000: episode: 1114, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.175, 10.579], loss: 0.898949, mae: 0.633393, mean_q: 2.232123
[RESULT] FALSIFICATION!
 21381/100000: episode: 1115, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.420, 10.500], loss: 3.223570, mae: 1.307181, mean_q: 2.259336
[RESULT] FALSIFICATION!
 21382/100000: episode: 1116, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.210, 10.600], loss: 3.410172, mae: 1.101019, mean_q: 1.619840
[RESULT] FALSIFICATION!
 21383/100000: episode: 1117, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.175, 10.509], loss: 1.491055, mae: 0.809897, mean_q: 2.037874
[RESULT] FALSIFICATION!
 21384/100000: episode: 1118, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.443 [-0.175, 10.674], loss: 3.375321, mae: 1.223077, mean_q: 2.713218
[RESULT] FALSIFICATION!
 21385/100000: episode: 1119, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.175, 10.586], loss: 1.790291, mae: 0.978843, mean_q: 2.647469
[RESULT] FALSIFICATION!
 21386/100000: episode: 1120, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.175, 10.528], loss: 0.569650, mae: 0.558780, mean_q: 2.331644
[RESULT] FALSIFICATION!
 21387/100000: episode: 1121, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.477 [-0.175, 10.677], loss: 2.181925, mae: 1.025931, mean_q: 2.715275
[RESULT] FALSIFICATION!
 21388/100000: episode: 1122, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.175, 10.520], loss: 1.820412, mae: 1.000204, mean_q: 3.019295
[RESULT] FALSIFICATION!
 21389/100000: episode: 1123, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.175, 10.520], loss: 0.586526, mae: 0.618264, mean_q: 2.324794
[RESULT] FALSIFICATION!
 21390/100000: episode: 1124, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.175, 10.512], loss: 1.323907, mae: 0.730642, mean_q: 3.345465
[RESULT] FALSIFICATION!
 21391/100000: episode: 1125, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.175, 10.605], loss: 1.128848, mae: 0.650763, mean_q: 2.424686
[RESULT] FALSIFICATION!
 21392/100000: episode: 1126, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.175, 10.500], loss: 1.713363, mae: 1.046815, mean_q: 2.914009
[RESULT] FALSIFICATION!
 21393/100000: episode: 1127, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.210, 10.600], loss: 0.676911, mae: 0.539927, mean_q: 2.153788
[RESULT] FALSIFICATION!
 21394/100000: episode: 1128, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.175, 10.500], loss: 4.059420, mae: 1.940827, mean_q: 3.760016
[RESULT] FALSIFICATION!
 21395/100000: episode: 1129, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.175, 10.528], loss: 1.984854, mae: 0.870363, mean_q: 2.977922
[RESULT] FALSIFICATION!
 21396/100000: episode: 1130, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.175, 10.500], loss: 2.014875, mae: 0.983823, mean_q: 2.402677
[RESULT] FALSIFICATION!
 21397/100000: episode: 1131, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.446 [-0.175, 10.577], loss: 1.925552, mae: 0.948421, mean_q: 2.225046
[RESULT] FALSIFICATION!
 21398/100000: episode: 1132, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.443 [-0.175, 10.564], loss: 1.484222, mae: 0.774862, mean_q: 1.688359
[RESULT] FALSIFICATION!
 21399/100000: episode: 1133, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.411 [-0.175, 10.591], loss: 0.702450, mae: 0.502316, mean_q: 1.452482
[RESULT] FALSIFICATION!
 21400/100000: episode: 1134, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.175, 10.500], loss: 1.830767, mae: 0.959547, mean_q: 2.385143
[RESULT] FALSIFICATION!
 21401/100000: episode: 1135, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.175, 10.524], loss: 3.029535, mae: 1.292293, mean_q: 2.414835
[RESULT] FALSIFICATION!
[Info] New level: 9.948262214660645 | Considering 10/90 traces
 21402/100000: episode: 1136, duration: 4.121s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.390 [-0.175, 10.585], loss: 2.372475, mae: 1.137286, mean_q: 2.341689
[RESULT] FALSIFICATION!
 21403/100000: episode: 1137, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.471 [-0.245, 10.700], loss: 0.596582, mae: 0.674717, mean_q: 2.257827
[RESULT] FALSIFICATION!
 21404/100000: episode: 1138, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.541, 10.600], loss: 1.512660, mae: 0.960300, mean_q: 2.625674
[RESULT] FALSIFICATION!
 21405/100000: episode: 1139, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.210, 10.600], loss: 0.386092, mae: 0.494663, mean_q: 2.595775
[RESULT] FALSIFICATION!
 21406/100000: episode: 1140, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.210, 10.600], loss: 3.253437, mae: 1.325233, mean_q: 2.496551
[RESULT] FALSIFICATION!
 21407/100000: episode: 1141, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.501 [-0.210, 10.606], loss: 0.722576, mae: 0.688385, mean_q: 2.786578
[RESULT] FALSIFICATION!
 21408/100000: episode: 1142, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.433 [-0.210, 10.600], loss: 0.748612, mae: 0.558448, mean_q: 2.112631
[RESULT] FALSIFICATION!
 21409/100000: episode: 1143, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.466 [-0.245, 10.700], loss: 1.069186, mae: 0.867447, mean_q: 2.367839
[RESULT] FALSIFICATION!
 21410/100000: episode: 1144, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.436 [-0.210, 10.601], loss: 2.780820, mae: 1.216382, mean_q: 2.365169
[RESULT] FALSIFICATION!
 21411/100000: episode: 1145, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.210, 10.600], loss: 1.218248, mae: 0.658936, mean_q: 1.593728
[RESULT] FALSIFICATION!
 21412/100000: episode: 1146, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.482 [-0.210, 10.600], loss: 0.349871, mae: 0.424799, mean_q: 1.707935
[RESULT] FALSIFICATION!
 21413/100000: episode: 1147, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.210, 10.600], loss: 1.388155, mae: 0.712000, mean_q: 2.205374
[RESULT] FALSIFICATION!
 21414/100000: episode: 1148, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.461 [-0.245, 10.700], loss: 3.648520, mae: 1.374000, mean_q: 2.495060
[RESULT] FALSIFICATION!
 21415/100000: episode: 1149, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.245, 10.700], loss: 0.548257, mae: 0.431582, mean_q: 1.674473
[RESULT] FALSIFICATION!
 21416/100000: episode: 1150, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.245, 10.700], loss: 0.018363, mae: 0.131555, mean_q: 1.482997
[RESULT] FALSIFICATION!
 21417/100000: episode: 1151, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.245, 10.700], loss: 0.648735, mae: 0.571251, mean_q: 2.310728
[RESULT] FALSIFICATION!
 21418/100000: episode: 1152, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.210, 10.600], loss: 1.764595, mae: 0.817720, mean_q: 2.016489
[RESULT] FALSIFICATION!
 21419/100000: episode: 1153, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.210, 10.600], loss: 2.663159, mae: 1.030551, mean_q: 2.021125
[RESULT] FALSIFICATION!
 21420/100000: episode: 1154, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.210, 10.600], loss: 1.646501, mae: 0.850115, mean_q: 2.393178
[RESULT] FALSIFICATION!
 21421/100000: episode: 1155, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.210, 10.600], loss: 1.220745, mae: 0.713957, mean_q: 1.917388
[RESULT] FALSIFICATION!
 21422/100000: episode: 1156, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.953, 10.700], loss: 3.409276, mae: 1.358181, mean_q: 2.763988
[RESULT] FALSIFICATION!
 21423/100000: episode: 1157, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.245, 10.700], loss: 1.206523, mae: 0.799621, mean_q: 2.598355
[RESULT] FALSIFICATION!
 21424/100000: episode: 1158, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.444 [-0.210, 10.609], loss: 1.476888, mae: 0.689072, mean_q: 2.701391
[RESULT] FALSIFICATION!
 21425/100000: episode: 1159, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.448 [-0.210, 10.600], loss: 3.825297, mae: 1.496710, mean_q: 2.415355
[RESULT] FALSIFICATION!
 21426/100000: episode: 1160, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.437 [-0.245, 10.700], loss: 0.574108, mae: 0.565677, mean_q: 3.009537
[RESULT] FALSIFICATION!
 21427/100000: episode: 1161, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.210, 10.600], loss: 0.992769, mae: 0.816705, mean_q: 2.959957
[RESULT] FALSIFICATION!
 21428/100000: episode: 1162, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.210, 10.600], loss: 1.522595, mae: 0.897365, mean_q: 3.413605
[RESULT] FALSIFICATION!
 21429/100000: episode: 1163, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.210, 10.600], loss: 2.280045, mae: 1.197775, mean_q: 2.184716
[RESULT] FALSIFICATION!
 21430/100000: episode: 1164, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.437 [-0.210, 10.600], loss: 3.820828, mae: 1.555516, mean_q: 3.072813
[RESULT] FALSIFICATION!
 21431/100000: episode: 1165, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.659, 10.600], loss: 1.426934, mae: 0.752439, mean_q: 2.277465
[RESULT] FALSIFICATION!
 21432/100000: episode: 1166, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.245, 10.700], loss: 3.198950, mae: 1.367950, mean_q: 2.524955
[RESULT] FALSIFICATION!
 21433/100000: episode: 1167, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.210, 10.600], loss: 1.405744, mae: 0.673346, mean_q: 1.782438
[RESULT] FALSIFICATION!
 21434/100000: episode: 1168, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.450 [-0.210, 10.607], loss: 0.970487, mae: 0.775499, mean_q: 3.443866
[RESULT] FALSIFICATION!
 21435/100000: episode: 1169, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.245, 10.700], loss: 3.138473, mae: 1.079822, mean_q: 3.232798
[RESULT] FALSIFICATION!
 21436/100000: episode: 1170, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.457 [-0.245, 10.700], loss: 0.590488, mae: 0.544459, mean_q: 1.935585
[RESULT] FALSIFICATION!
 21437/100000: episode: 1171, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.210, 10.600], loss: 2.704893, mae: 0.956764, mean_q: 1.857926
[RESULT] FALSIFICATION!
 21438/100000: episode: 1172, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.210, 10.600], loss: 2.542749, mae: 0.915287, mean_q: 1.678820
[RESULT] FALSIFICATION!
 21439/100000: episode: 1173, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.420 [-0.210, 10.600], loss: 0.365069, mae: 0.479991, mean_q: 1.910309
[RESULT] FALSIFICATION!
 21440/100000: episode: 1174, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.245, 10.700], loss: 2.497034, mae: 1.059411, mean_q: 2.439093
[RESULT] FALSIFICATION!
 21441/100000: episode: 1175, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.210, 10.600], loss: 1.278454, mae: 0.742009, mean_q: 2.786793
[RESULT] FALSIFICATION!
 21442/100000: episode: 1176, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.436 [-0.210, 10.600], loss: 0.919634, mae: 0.754368, mean_q: 2.544034
[RESULT] FALSIFICATION!
 21443/100000: episode: 1177, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.210, 10.600], loss: 2.640164, mae: 1.183033, mean_q: 2.697273
[RESULT] FALSIFICATION!
 21444/100000: episode: 1178, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.532 [-0.210, 10.650], loss: 1.379322, mae: 0.779760, mean_q: 2.358981
[RESULT] FALSIFICATION!
 21445/100000: episode: 1179, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.439 [-0.210, 10.600], loss: 0.331447, mae: 0.310119, mean_q: 1.611753
[RESULT] FALSIFICATION!
 21446/100000: episode: 1180, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.475 [-0.210, 10.600], loss: 0.912721, mae: 0.646681, mean_q: 2.086973
[RESULT] FALSIFICATION!
 21447/100000: episode: 1181, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.210, 10.600], loss: 2.414225, mae: 0.781156, mean_q: 1.107683
[RESULT] FALSIFICATION!
 21448/100000: episode: 1182, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.210, 10.600], loss: 0.337057, mae: 0.381097, mean_q: 1.497815
[RESULT] FALSIFICATION!
 21449/100000: episode: 1183, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.245, 10.700], loss: 1.408245, mae: 0.830432, mean_q: 2.217591
[RESULT] FALSIFICATION!
 21450/100000: episode: 1184, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.419 [-0.245, 10.700], loss: 4.749026, mae: 1.639590, mean_q: 1.857150
[RESULT] FALSIFICATION!
 21451/100000: episode: 1185, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.210, 10.600], loss: 0.531024, mae: 0.824092, mean_q: 3.141727
[RESULT] FALSIFICATION!
 21452/100000: episode: 1186, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.245, 10.700], loss: 3.646092, mae: 1.933961, mean_q: 3.638958
[RESULT] FALSIFICATION!
 21453/100000: episode: 1187, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.446 [-0.210, 10.600], loss: 3.537054, mae: 1.931013, mean_q: 3.724525
[RESULT] FALSIFICATION!
 21454/100000: episode: 1188, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.484 [-0.245, 10.700], loss: 1.082932, mae: 1.056259, mean_q: 3.589175
[RESULT] FALSIFICATION!
 21455/100000: episode: 1189, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.500 [-0.245, 10.700], loss: 2.859340, mae: 1.227980, mean_q: 2.576580
[RESULT] FALSIFICATION!
 21456/100000: episode: 1190, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.245, 10.700], loss: 0.678555, mae: 0.611283, mean_q: 2.296057
[RESULT] FALSIFICATION!
 21457/100000: episode: 1191, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.210, 10.600], loss: 0.816390, mae: 0.483567, mean_q: 2.155324
[RESULT] FALSIFICATION!
 21458/100000: episode: 1192, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.245, 10.700], loss: 2.362250, mae: 1.089884, mean_q: 3.165196
[RESULT] FALSIFICATION!
 21459/100000: episode: 1193, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.210, 10.600], loss: 0.626362, mae: 0.742499, mean_q: 3.072450
[RESULT] FALSIFICATION!
 21460/100000: episode: 1194, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.210, 10.600], loss: 1.819446, mae: 1.078124, mean_q: 3.166090
[RESULT] FALSIFICATION!
 21461/100000: episode: 1195, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.210, 10.600], loss: 1.092834, mae: 0.635594, mean_q: 1.809550
[RESULT] FALSIFICATION!
 21462/100000: episode: 1196, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.440 [-0.210, 10.600], loss: 1.693633, mae: 0.927042, mean_q: 2.342333
[RESULT] FALSIFICATION!
 21463/100000: episode: 1197, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.422 [-0.210, 10.600], loss: 0.526536, mae: 0.555123, mean_q: 2.514973
[RESULT] FALSIFICATION!
 21464/100000: episode: 1198, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.245, 10.700], loss: 2.129448, mae: 1.137982, mean_q: 2.118126
[RESULT] FALSIFICATION!
 21465/100000: episode: 1199, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-1.262, 10.600], loss: 0.724541, mae: 0.631570, mean_q: 2.895907
[RESULT] FALSIFICATION!
 21466/100000: episode: 1200, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.472 [-0.210, 10.600], loss: 2.873499, mae: 1.128784, mean_q: 2.269997
[RESULT] FALSIFICATION!
 21467/100000: episode: 1201, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.464 [-0.245, 10.701], loss: 2.289241, mae: 0.927657, mean_q: 2.422478
[RESULT] FALSIFICATION!
 21468/100000: episode: 1202, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.210, 10.600], loss: 0.247379, mae: 0.339754, mean_q: 2.145793
[RESULT] FALSIFICATION!
 21469/100000: episode: 1203, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.245, 10.700], loss: 1.502577, mae: 1.006007, mean_q: 2.614114
[RESULT] FALSIFICATION!
 21470/100000: episode: 1204, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.445 [-0.245, 10.700], loss: 2.497782, mae: 1.199251, mean_q: 3.914903
[RESULT] FALSIFICATION!
 21471/100000: episode: 1205, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.210, 10.600], loss: 0.488676, mae: 0.425008, mean_q: 2.072524
[RESULT] FALSIFICATION!
 21472/100000: episode: 1206, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.210, 10.600], loss: 2.282733, mae: 1.040871, mean_q: 2.666913
[RESULT] FALSIFICATION!
 21473/100000: episode: 1207, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.210, 10.600], loss: 1.720096, mae: 0.853608, mean_q: 2.189109
[RESULT] FALSIFICATION!
 21474/100000: episode: 1208, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.419 [-0.210, 10.600], loss: 1.828571, mae: 0.906503, mean_q: 3.197181
[RESULT] FALSIFICATION!
 21475/100000: episode: 1209, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.501 [-0.210, 10.608], loss: 0.652723, mae: 0.518174, mean_q: 2.086898
[RESULT] FALSIFICATION!
 21476/100000: episode: 1210, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.488 [-0.210, 10.628], loss: 1.047794, mae: 0.917124, mean_q: 3.191559
[RESULT] FALSIFICATION!
 21477/100000: episode: 1211, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.454 [-0.210, 10.672], loss: 2.596490, mae: 0.953548, mean_q: 2.460159
[RESULT] FALSIFICATION!
 21478/100000: episode: 1212, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.467 [-0.245, 10.709], loss: 1.997765, mae: 1.020535, mean_q: 3.362069
[RESULT] FALSIFICATION!
 21479/100000: episode: 1213, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.210, 10.600], loss: 1.986726, mae: 1.263409, mean_q: 3.324619
[RESULT] FALSIFICATION!
 21480/100000: episode: 1214, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.409 [-0.210, 10.605], loss: 1.114172, mae: 0.746241, mean_q: 2.122478
[RESULT] FALSIFICATION!
 21481/100000: episode: 1215, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.210, 10.600], loss: 0.762304, mae: 0.563186, mean_q: 2.527454
[RESULT] FALSIFICATION!
 21482/100000: episode: 1216, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.543 [-0.245, 10.718], loss: 1.827551, mae: 0.862186, mean_q: 1.881488
[RESULT] FALSIFICATION!
 21483/100000: episode: 1217, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.481 [-0.245, 10.703], loss: 1.553343, mae: 0.741341, mean_q: 2.635269
[RESULT] FALSIFICATION!
 21484/100000: episode: 1218, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.412 [-0.245, 10.700], loss: 1.079364, mae: 0.750385, mean_q: 2.193029
[RESULT] FALSIFICATION!
 21485/100000: episode: 1219, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.443 [-0.210, 10.600], loss: 1.812198, mae: 0.843512, mean_q: 2.548720
[RESULT] FALSIFICATION!
 21486/100000: episode: 1220, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.210, 10.600], loss: 0.976783, mae: 0.809032, mean_q: 2.738590
[RESULT] FALSIFICATION!
 21487/100000: episode: 1221, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.245, 10.700], loss: 0.394261, mae: 0.435017, mean_q: 2.034356
[RESULT] FALSIFICATION!
 21488/100000: episode: 1222, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.446 [-0.245, 10.700], loss: 0.584152, mae: 0.583453, mean_q: 2.634838
[RESULT] FALSIFICATION!
 21489/100000: episode: 1223, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.459 [-0.210, 10.600], loss: 0.120532, mae: 0.307401, mean_q: 2.115419
[RESULT] FALSIFICATION!
 21490/100000: episode: 1224, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.479 [-0.210, 10.600], loss: 1.423699, mae: 0.600290, mean_q: 1.759577
[RESULT] FALSIFICATION!
 21491/100000: episode: 1225, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.478 [-0.245, 10.700], loss: 1.400878, mae: 0.705907, mean_q: 2.195468
[RESULT] FALSIFICATION!
[Info] New level: 9.990148544311523 | Considering 17/83 traces
 21492/100000: episode: 1226, duration: 4.216s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.245, 10.700], loss: 2.499608, mae: 0.968284, mean_q: 3.251801
[RESULT] FALSIFICATION!
 21493/100000: episode: 1227, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.210, 10.600], loss: 3.236629, mae: 1.244738, mean_q: 3.015538
[RESULT] FALSIFICATION!
 21494/100000: episode: 1228, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.245, 10.700], loss: 0.994925, mae: 0.896245, mean_q: 2.796807
[RESULT] FALSIFICATION!
 21495/100000: episode: 1229, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.210, 10.600], loss: 3.282816, mae: 1.348697, mean_q: 3.052107
[RESULT] FALSIFICATION!
 21496/100000: episode: 1230, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.979, 10.600], loss: 0.889441, mae: 0.731711, mean_q: 3.466087
[RESULT] FALSIFICATION!
 21497/100000: episode: 1231, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.421 [-0.280, 10.800], loss: 3.849709, mae: 1.548288, mean_q: 3.281051
[RESULT] FALSIFICATION!
 21498/100000: episode: 1232, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.432 [-0.210, 10.600], loss: 4.416871, mae: 1.455989, mean_q: 2.881176
[RESULT] FALSIFICATION!
 21499/100000: episode: 1233, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.455 [-0.245, 10.700], loss: 0.254295, mae: 0.299270, mean_q: 1.736936
[RESULT] FALSIFICATION!
 21500/100000: episode: 1234, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.288, 10.600], loss: 1.542057, mae: 0.899176, mean_q: 3.163262
[RESULT] FALSIFICATION!
 21501/100000: episode: 1235, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.245, 10.700], loss: 3.102940, mae: 1.283012, mean_q: 3.234741
[RESULT] FALSIFICATION!
 21502/100000: episode: 1236, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.449 [-0.280, 10.800], loss: 2.021147, mae: 0.886424, mean_q: 2.660764
[RESULT] FALSIFICATION!
 21503/100000: episode: 1237, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.444 [-0.210, 10.600], loss: 2.180218, mae: 1.084422, mean_q: 2.257277
[RESULT] FALSIFICATION!
 21504/100000: episode: 1238, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.452 [-0.245, 10.700], loss: 1.339012, mae: 1.092410, mean_q: 3.223390
[RESULT] FALSIFICATION!
 21505/100000: episode: 1239, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.464 [-0.280, 10.800], loss: 0.879804, mae: 0.554290, mean_q: 2.811697
[RESULT] FALSIFICATION!
 21506/100000: episode: 1240, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.210, 10.600], loss: 2.436908, mae: 0.966048, mean_q: 2.417525
[RESULT] FALSIFICATION!
 21507/100000: episode: 1241, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.463 [-0.245, 10.700], loss: 1.941072, mae: 0.770878, mean_q: 2.143763
[RESULT] FALSIFICATION!
 21508/100000: episode: 1242, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.439 [-0.210, 10.600], loss: 4.093977, mae: 1.565148, mean_q: 3.357473
[RESULT] FALSIFICATION!
 21509/100000: episode: 1243, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.484 [-0.245, 10.700], loss: 3.562527, mae: 1.121587, mean_q: 2.492555
[RESULT] FALSIFICATION!
 21510/100000: episode: 1244, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.210, 10.600], loss: 1.119884, mae: 0.738224, mean_q: 2.176393
[RESULT] FALSIFICATION!
 21511/100000: episode: 1245, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.210, 10.600], loss: 3.295091, mae: 1.319967, mean_q: 2.512107
[RESULT] FALSIFICATION!
 21512/100000: episode: 1246, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.210, 10.600], loss: 2.247228, mae: 1.189580, mean_q: 2.677909
[RESULT] FALSIFICATION!
 21513/100000: episode: 1247, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.444 [-0.245, 10.700], loss: 1.375905, mae: 0.945451, mean_q: 2.656813
[RESULT] FALSIFICATION!
 21514/100000: episode: 1248, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.210, 10.603], loss: 1.045736, mae: 0.976102, mean_q: 2.964165
[RESULT] FALSIFICATION!
 21515/100000: episode: 1249, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.454 [-0.245, 10.700], loss: 0.851179, mae: 0.766823, mean_q: 3.327131
[RESULT] FALSIFICATION!
 21516/100000: episode: 1250, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.456 [-0.210, 10.600], loss: 1.452327, mae: 0.904536, mean_q: 3.993880
[RESULT] FALSIFICATION!
 21517/100000: episode: 1251, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.473 [-0.210, 10.600], loss: 2.921593, mae: 1.162288, mean_q: 2.421415
[RESULT] FALSIFICATION!
 21518/100000: episode: 1252, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.210, 10.600], loss: 1.993128, mae: 0.926015, mean_q: 2.339368
[RESULT] FALSIFICATION!
 21519/100000: episode: 1253, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.210, 10.600], loss: 1.457184, mae: 0.574170, mean_q: 2.128230
[RESULT] FALSIFICATION!
 21520/100000: episode: 1254, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.210, 10.600], loss: 0.264089, mae: 0.383385, mean_q: 2.349170
[RESULT] FALSIFICATION!
 21521/100000: episode: 1255, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.210, 10.600], loss: 3.513286, mae: 1.355293, mean_q: 2.434975
[RESULT] FALSIFICATION!
 21522/100000: episode: 1256, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.448 [-0.280, 10.800], loss: 1.225333, mae: 0.597581, mean_q: 2.886177
[RESULT] FALSIFICATION!
 21523/100000: episode: 1257, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.411 [-0.280, 10.800], loss: 1.479302, mae: 0.790496, mean_q: 2.928964
[RESULT] FALSIFICATION!
 21524/100000: episode: 1258, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.210, 10.600], loss: 4.064545, mae: 1.615548, mean_q: 3.135793
[RESULT] FALSIFICATION!
 21525/100000: episode: 1259, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.440, 10.600], loss: 0.269887, mae: 0.446012, mean_q: 3.103412
[RESULT] FALSIFICATION!
 21526/100000: episode: 1260, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.210, 10.600], loss: 1.870117, mae: 0.884621, mean_q: 3.302463
[RESULT] FALSIFICATION!
 21527/100000: episode: 1261, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.245, 10.700], loss: 1.779888, mae: 0.715076, mean_q: 1.594775
[RESULT] FALSIFICATION!
 21528/100000: episode: 1262, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.210, 10.618], loss: 2.739911, mae: 0.946744, mean_q: 2.385448
[RESULT] FALSIFICATION!
 21529/100000: episode: 1263, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.468 [-0.210, 10.600], loss: 1.775397, mae: 0.819892, mean_q: 2.746011
[RESULT] FALSIFICATION!
 21530/100000: episode: 1264, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.245, 10.700], loss: 2.674735, mae: 1.316099, mean_q: 2.875799
[RESULT] FALSIFICATION!
 21531/100000: episode: 1265, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.458 [-0.280, 10.800], loss: 2.507839, mae: 1.104834, mean_q: 3.129569
[RESULT] FALSIFICATION!
 21532/100000: episode: 1266, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.210, 10.600], loss: 2.682106, mae: 1.189955, mean_q: 2.318232
[RESULT] FALSIFICATION!
 21533/100000: episode: 1267, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.462 [-0.280, 10.800], loss: 1.416887, mae: 1.058087, mean_q: 2.951278
[RESULT] FALSIFICATION!
 21534/100000: episode: 1268, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.426 [-0.245, 10.700], loss: 2.388674, mae: 1.114974, mean_q: 2.260468
[RESULT] FALSIFICATION!
 21535/100000: episode: 1269, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.460 [-0.245, 10.700], loss: 0.987128, mae: 0.815133, mean_q: 3.019923
[RESULT] FALSIFICATION!
 21536/100000: episode: 1270, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.426 [-0.245, 10.700], loss: 4.725399, mae: 1.695963, mean_q: 3.389185
[RESULT] FALSIFICATION!
 21537/100000: episode: 1271, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.210, 10.600], loss: 4.091466, mae: 1.527273, mean_q: 3.519939
[RESULT] FALSIFICATION!
 21538/100000: episode: 1272, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.245, 10.700], loss: 0.157955, mae: 0.309280, mean_q: 2.035889
[RESULT] FALSIFICATION!
 21539/100000: episode: 1273, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.458 [-0.245, 10.700], loss: 3.298440, mae: 1.295639, mean_q: 1.676270
[RESULT] FALSIFICATION!
 21540/100000: episode: 1274, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.436 [-0.210, 10.600], loss: 1.402774, mae: 0.801653, mean_q: 1.231118
[RESULT] FALSIFICATION!
 21541/100000: episode: 1275, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.245, 10.700], loss: 1.487937, mae: 0.777791, mean_q: 1.949162
[RESULT] FALSIFICATION!
 21542/100000: episode: 1276, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.448 [-0.280, 10.800], loss: 1.765307, mae: 0.864925, mean_q: 2.448596
[RESULT] FALSIFICATION!
 21543/100000: episode: 1277, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.421 [-0.210, 10.600], loss: 1.020991, mae: 0.663025, mean_q: 2.402559
[RESULT] FALSIFICATION!
 21544/100000: episode: 1278, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.504 [-0.280, 10.800], loss: 0.408633, mae: 0.522544, mean_q: 3.453083
[RESULT] FALSIFICATION!
 21545/100000: episode: 1279, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.280, 10.800], loss: 1.847369, mae: 1.025819, mean_q: 3.570294
[RESULT] FALSIFICATION!
 21546/100000: episode: 1280, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.482 [-0.280, 10.800], loss: 3.831523, mae: 1.502221, mean_q: 3.901338
[RESULT] FALSIFICATION!
 21547/100000: episode: 1281, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.426 [-0.280, 10.800], loss: 2.169280, mae: 1.096588, mean_q: 3.141309
[RESULT] FALSIFICATION!
 21548/100000: episode: 1282, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.245, 10.700], loss: 2.032687, mae: 0.932201, mean_q: 2.629157
[RESULT] FALSIFICATION!
 21549/100000: episode: 1283, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.210, 10.600], loss: 3.702110, mae: 1.515335, mean_q: 2.564433
[RESULT] FALSIFICATION!
 21550/100000: episode: 1284, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.479 [-0.245, 10.700], loss: 1.434267, mae: 0.757436, mean_q: 2.884536
[RESULT] FALSIFICATION!
 21551/100000: episode: 1285, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.210, 10.600], loss: 1.037769, mae: 0.814015, mean_q: 2.608513
[RESULT] FALSIFICATION!
 21552/100000: episode: 1286, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.491 [-0.280, 10.800], loss: 0.334832, mae: 0.449510, mean_q: 2.523900
[RESULT] FALSIFICATION!
 21553/100000: episode: 1287, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.483 [-0.210, 10.600], loss: 1.621076, mae: 0.884209, mean_q: 3.528970
[RESULT] FALSIFICATION!
 21554/100000: episode: 1288, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.455 [-0.210, 10.600], loss: 0.312976, mae: 0.448919, mean_q: 2.572507
[RESULT] FALSIFICATION!
 21555/100000: episode: 1289, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.210, 10.600], loss: 1.632088, mae: 0.852468, mean_q: 2.004429
[RESULT] FALSIFICATION!
 21556/100000: episode: 1290, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.210, 10.600], loss: 1.813983, mae: 0.829102, mean_q: 2.722460
[RESULT] FALSIFICATION!
 21557/100000: episode: 1291, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.510 [-0.280, 10.800], loss: 0.461671, mae: 0.488487, mean_q: 2.767435
[RESULT] FALSIFICATION!
 21558/100000: episode: 1292, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.210, 10.600], loss: 1.955133, mae: 0.895288, mean_q: 3.501363
[RESULT] FALSIFICATION!
 21559/100000: episode: 1293, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.210, 10.600], loss: 3.153327, mae: 1.321589, mean_q: 3.678798
[RESULT] FALSIFICATION!
 21560/100000: episode: 1294, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.454 [-0.210, 10.600], loss: 3.959101, mae: 1.501363, mean_q: 3.972174
[RESULT] FALSIFICATION!
 21561/100000: episode: 1295, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.280, 10.800], loss: 3.534338, mae: 1.198696, mean_q: 3.686910
[RESULT] FALSIFICATION!
 21562/100000: episode: 1296, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.480 [-0.210, 10.600], loss: 2.014794, mae: 1.138712, mean_q: 4.058910
[RESULT] FALSIFICATION!
 21563/100000: episode: 1297, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.436 [-0.210, 10.600], loss: 0.990700, mae: 0.665006, mean_q: 2.625836
[RESULT] FALSIFICATION!
 21564/100000: episode: 1298, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.245, 10.700], loss: 2.033739, mae: 1.168043, mean_q: 2.141001
[RESULT] FALSIFICATION!
 21565/100000: episode: 1299, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.468 [-0.210, 10.600], loss: 3.082106, mae: 1.152410, mean_q: 1.944233
[RESULT] FALSIFICATION!
 21566/100000: episode: 1300, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.475 [-0.210, 10.614], loss: 2.279562, mae: 1.130606, mean_q: 2.448603
[RESULT] FALSIFICATION!
 21567/100000: episode: 1301, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.459 [-0.245, 10.736], loss: 2.589426, mae: 1.134847, mean_q: 3.440629
[RESULT] FALSIFICATION!
 21568/100000: episode: 1302, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.469 [-0.245, 10.700], loss: 1.345878, mae: 0.805419, mean_q: 2.672229
[RESULT] FALSIFICATION!
 21569/100000: episode: 1303, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.245, 10.700], loss: 2.351269, mae: 1.277964, mean_q: 3.854600
[RESULT] FALSIFICATION!
 21570/100000: episode: 1304, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.510 [-0.280, 10.800], loss: 0.617619, mae: 0.776278, mean_q: 3.012092
[RESULT] FALSIFICATION!
 21571/100000: episode: 1305, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.430 [-0.210, 10.600], loss: 2.615108, mae: 1.218807, mean_q: 3.827303
[RESULT] FALSIFICATION!
 21572/100000: episode: 1306, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.210, 10.600], loss: 1.283659, mae: 0.622968, mean_q: 3.020232
[RESULT] FALSIFICATION!
 21573/100000: episode: 1307, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.280, 10.800], loss: 1.804143, mae: 1.129117, mean_q: 3.742303
[RESULT] FALSIFICATION!
 21574/100000: episode: 1308, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.450 [-0.245, 10.700], loss: 0.521635, mae: 0.478840, mean_q: 2.760161
[RESULT] FALSIFICATION!
[Info] New level: 10.167445182800293 | Considering 11/89 traces
 21575/100000: episode: 1309, duration: 4.240s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.425 [-0.210, 10.600], loss: 2.222049, mae: 1.122792, mean_q: 2.851691
[RESULT] FALSIFICATION!
 21576/100000: episode: 1310, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.489 [-0.245, 10.700], loss: 1.644843, mae: 0.685132, mean_q: 1.959377
[RESULT] FALSIFICATION!
 21577/100000: episode: 1311, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.475 [-0.245, 10.700], loss: 0.262526, mae: 0.327689, mean_q: 2.948898
[RESULT] FALSIFICATION!
 21578/100000: episode: 1312, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.450 [-0.245, 10.700], loss: 1.409349, mae: 0.870832, mean_q: 2.406931
[RESULT] FALSIFICATION!
 21579/100000: episode: 1313, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.245, 10.700], loss: 2.531362, mae: 1.126351, mean_q: 3.927565
[RESULT] FALSIFICATION!
 21580/100000: episode: 1314, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.419 [-0.280, 10.800], loss: 1.273932, mae: 1.047550, mean_q: 3.358821
[RESULT] FALSIFICATION!
 21581/100000: episode: 1315, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.245, 10.700], loss: 0.788858, mae: 0.687409, mean_q: 3.434275
[RESULT] FALSIFICATION!
 21582/100000: episode: 1316, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.444 [-0.245, 10.700], loss: 1.520708, mae: 0.813261, mean_q: 2.683406
[RESULT] FALSIFICATION!
 21583/100000: episode: 1317, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.467 [-0.245, 10.700], loss: 0.290112, mae: 0.402822, mean_q: 3.007631
[RESULT] FALSIFICATION!
 21584/100000: episode: 1318, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.245, 10.700], loss: 1.451061, mae: 0.771779, mean_q: 3.106804
[RESULT] FALSIFICATION!
 21585/100000: episode: 1319, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.442 [-0.245, 10.700], loss: 1.884205, mae: 0.904355, mean_q: 2.953643
[RESULT] FALSIFICATION!
 21586/100000: episode: 1320, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.325, 10.700], loss: 1.420662, mae: 0.770738, mean_q: 2.926299
[RESULT] FALSIFICATION!
 21587/100000: episode: 1321, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.280, 10.800], loss: 0.684610, mae: 0.450409, mean_q: 2.385641
[RESULT] FALSIFICATION!
 21588/100000: episode: 1322, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.245, 10.700], loss: 4.783046, mae: 1.634089, mean_q: 3.125086
[RESULT] FALSIFICATION!
 21589/100000: episode: 1323, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.435 [-0.245, 10.700], loss: 0.910998, mae: 0.557913, mean_q: 3.175071
[RESULT] FALSIFICATION!
 21590/100000: episode: 1324, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.495 [-0.245, 10.706], loss: 2.544428, mae: 1.022811, mean_q: 2.249849
[RESULT] FALSIFICATION!
 21591/100000: episode: 1325, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.397 [-0.245, 10.700], loss: 0.800009, mae: 0.672873, mean_q: 3.641155
[RESULT] FALSIFICATION!
 21592/100000: episode: 1326, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.245, 10.700], loss: 1.704843, mae: 0.983376, mean_q: 3.917847
[RESULT] FALSIFICATION!
 21593/100000: episode: 1327, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.506 [-0.245, 10.700], loss: 0.638555, mae: 0.439295, mean_q: 2.177706
[RESULT] FALSIFICATION!
 21594/100000: episode: 1328, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.522 [-0.245, 10.700], loss: 1.841613, mae: 0.770886, mean_q: 3.191810
[RESULT] FALSIFICATION!
 21595/100000: episode: 1329, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.245, 10.700], loss: 1.681923, mae: 0.675971, mean_q: 2.093651
[RESULT] FALSIFICATION!
 21596/100000: episode: 1330, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.472 [-0.245, 10.700], loss: 2.132566, mae: 0.902219, mean_q: 1.936744
[RESULT] FALSIFICATION!
 21597/100000: episode: 1331, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.402 [-0.245, 10.700], loss: 0.634800, mae: 0.535871, mean_q: 2.327752
[RESULT] FALSIFICATION!
 21598/100000: episode: 1332, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.245, 10.700], loss: 0.645431, mae: 0.528816, mean_q: 2.513926
[RESULT] FALSIFICATION!
 21599/100000: episode: 1333, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.245, 10.700], loss: 0.693392, mae: 0.528773, mean_q: 2.382109
[RESULT] FALSIFICATION!
 21600/100000: episode: 1334, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.280, 10.800], loss: 2.104461, mae: 0.883602, mean_q: 2.323493
[RESULT] FALSIFICATION!
 21601/100000: episode: 1335, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.423 [-0.280, 10.800], loss: 0.851009, mae: 0.631270, mean_q: 2.126104
[RESULT] FALSIFICATION!
 21602/100000: episode: 1336, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.505 [-0.245, 10.700], loss: 2.459965, mae: 0.938608, mean_q: 2.151688
[RESULT] FALSIFICATION!
 21603/100000: episode: 1337, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.518 [-0.245, 10.700], loss: 2.391535, mae: 1.036861, mean_q: 2.685604
[RESULT] FALSIFICATION!
 21604/100000: episode: 1338, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.280, 10.800], loss: 3.770746, mae: 1.377601, mean_q: 2.358181
[RESULT] FALSIFICATION!
 21605/100000: episode: 1339, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.245, 10.700], loss: 1.319323, mae: 0.735839, mean_q: 2.528790
[RESULT] FALSIFICATION!
 21606/100000: episode: 1340, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.245, 10.700], loss: 2.721830, mae: 1.364492, mean_q: 4.254480
[RESULT] FALSIFICATION!
 21607/100000: episode: 1341, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.495 [-0.245, 10.712], loss: 0.640106, mae: 0.716907, mean_q: 2.794847
[RESULT] FALSIFICATION!
 21608/100000: episode: 1342, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.464 [-0.245, 10.700], loss: 3.177757, mae: 1.134389, mean_q: 3.816440
[RESULT] FALSIFICATION!
 21609/100000: episode: 1343, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.245, 10.700], loss: 2.582560, mae: 1.007173, mean_q: 3.657228
[RESULT] FALSIFICATION!
 21610/100000: episode: 1344, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.477 [-0.245, 10.700], loss: 1.746569, mae: 0.843245, mean_q: 2.481100
[RESULT] FALSIFICATION!
 21611/100000: episode: 1345, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.461 [-0.245, 10.700], loss: 0.552763, mae: 0.526185, mean_q: 2.509597
[RESULT] FALSIFICATION!
 21612/100000: episode: 1346, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.245, 10.700], loss: 2.268336, mae: 0.860028, mean_q: 2.392663
[RESULT] FALSIFICATION!
 21613/100000: episode: 1347, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.468 [-0.245, 10.700], loss: 0.911097, mae: 0.553514, mean_q: 2.160414
[RESULT] FALSIFICATION!
 21614/100000: episode: 1348, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.422 [-0.245, 10.700], loss: 1.010790, mae: 0.613353, mean_q: 3.974223
[RESULT] FALSIFICATION!
 21615/100000: episode: 1349, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.245, 10.700], loss: 0.634497, mae: 0.558664, mean_q: 2.765859
[RESULT] FALSIFICATION!
 21616/100000: episode: 1350, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.448 [-0.245, 10.700], loss: 3.519842, mae: 1.314160, mean_q: 3.600047
[RESULT] FALSIFICATION!
 21617/100000: episode: 1351, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.451 [-0.280, 10.800], loss: 1.488706, mae: 0.717314, mean_q: 2.713798
[RESULT] FALSIFICATION!
 21618/100000: episode: 1352, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.245, 10.700], loss: 2.041295, mae: 0.689342, mean_q: 2.499935
[RESULT] FALSIFICATION!
 21619/100000: episode: 1353, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.245, 10.700], loss: 0.301794, mae: 0.410365, mean_q: 1.874238
[RESULT] FALSIFICATION!
 21620/100000: episode: 1354, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.245, 10.700], loss: 3.965234, mae: 1.338368, mean_q: 3.044979
[RESULT] FALSIFICATION!
 21621/100000: episode: 1355, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.480 [-0.245, 10.700], loss: 2.572598, mae: 1.090063, mean_q: 3.973800
[RESULT] FALSIFICATION!
 21622/100000: episode: 1356, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.245, 10.700], loss: 0.320099, mae: 0.375423, mean_q: 3.365021
[RESULT] FALSIFICATION!
 21623/100000: episode: 1357, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.509 [-0.280, 10.800], loss: 2.003939, mae: 1.027888, mean_q: 2.486388
[RESULT] FALSIFICATION!
 21624/100000: episode: 1358, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.245, 10.700], loss: 1.682289, mae: 0.755972, mean_q: 3.643885
[RESULT] FALSIFICATION!
 21625/100000: episode: 1359, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.462 [-0.245, 10.700], loss: 1.411867, mae: 0.770997, mean_q: 3.406675
[RESULT] FALSIFICATION!
 21626/100000: episode: 1360, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.428 [-0.245, 10.700], loss: 1.625366, mae: 0.709458, mean_q: 2.386064
[RESULT] FALSIFICATION!
 21627/100000: episode: 1361, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.454 [-0.245, 10.700], loss: 4.149556, mae: 1.410010, mean_q: 4.511913
[RESULT] FALSIFICATION!
 21628/100000: episode: 1362, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.414 [-0.245, 10.700], loss: 1.104436, mae: 0.730178, mean_q: 2.932629
[RESULT] FALSIFICATION!
 21629/100000: episode: 1363, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.446 [-0.245, 10.700], loss: 1.370592, mae: 0.867324, mean_q: 3.385144
[RESULT] FALSIFICATION!
 21630/100000: episode: 1364, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.245, 10.700], loss: 2.048578, mae: 1.007929, mean_q: 3.803740
[RESULT] FALSIFICATION!
 21631/100000: episode: 1365, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.431 [-0.245, 10.700], loss: 1.000716, mae: 0.778348, mean_q: 2.990271
[RESULT] FALSIFICATION!
 21632/100000: episode: 1366, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.493 [-0.280, 10.800], loss: 0.748830, mae: 0.664418, mean_q: 3.295446
[RESULT] FALSIFICATION!
 21633/100000: episode: 1367, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.453 [-0.245, 10.700], loss: 2.548083, mae: 1.147273, mean_q: 2.768243
[RESULT] FALSIFICATION!
 21634/100000: episode: 1368, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.245, 10.700], loss: 1.617389, mae: 1.045942, mean_q: 4.249571
[RESULT] FALSIFICATION!
 21635/100000: episode: 1369, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.481 [-0.280, 10.800], loss: 1.610187, mae: 0.798217, mean_q: 2.242836
[RESULT] FALSIFICATION!
 21636/100000: episode: 1370, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.418 [-0.245, 10.700], loss: 0.418641, mae: 0.571362, mean_q: 3.112151
[RESULT] FALSIFICATION!
 21637/100000: episode: 1371, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.457 [-0.280, 10.800], loss: 2.326767, mae: 0.831870, mean_q: 3.124824
[RESULT] FALSIFICATION!
 21638/100000: episode: 1372, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.245, 10.700], loss: 1.483289, mae: 0.896156, mean_q: 2.758754
[RESULT] FALSIFICATION!
 21639/100000: episode: 1373, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.245, 10.700], loss: 1.309841, mae: 0.602854, mean_q: 2.714906
[RESULT] FALSIFICATION!
 21640/100000: episode: 1374, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.455 [-0.245, 10.712], loss: 1.419146, mae: 0.674811, mean_q: 2.205632
[RESULT] FALSIFICATION!
 21641/100000: episode: 1375, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.464 [-0.245, 10.700], loss: 2.212452, mae: 0.977226, mean_q: 2.565569
[RESULT] FALSIFICATION!
 21642/100000: episode: 1376, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.435 [-0.280, 10.800], loss: 1.512484, mae: 1.088828, mean_q: 2.883259
[RESULT] FALSIFICATION!
 21643/100000: episode: 1377, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.396 [-0.245, 10.700], loss: 4.537092, mae: 1.450285, mean_q: 2.155169
[RESULT] FALSIFICATION!
 21644/100000: episode: 1378, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.433 [-0.245, 10.700], loss: 3.011918, mae: 1.056918, mean_q: 1.525780
[RESULT] FALSIFICATION!
 21645/100000: episode: 1379, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.514 [-0.245, 10.700], loss: 3.539430, mae: 1.363117, mean_q: 2.580268
[RESULT] FALSIFICATION!
 21646/100000: episode: 1380, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.424 [-0.245, 10.700], loss: 1.407869, mae: 0.765047, mean_q: 2.281391
[RESULT] FALSIFICATION!
 21647/100000: episode: 1381, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.477 [-0.280, 10.800], loss: 1.304158, mae: 0.824359, mean_q: 2.965157
[RESULT] FALSIFICATION!
 21648/100000: episode: 1382, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.488 [-0.245, 10.700], loss: 3.009398, mae: 1.213949, mean_q: 3.782952
[RESULT] FALSIFICATION!
 21649/100000: episode: 1383, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.245, 10.700], loss: 1.263766, mae: 1.099050, mean_q: 3.172408
[RESULT] FALSIFICATION!
 21650/100000: episode: 1384, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.466 [-0.245, 10.700], loss: 1.750745, mae: 1.164115, mean_q: 3.076092
[RESULT] FALSIFICATION!
 21651/100000: episode: 1385, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.245, 10.700], loss: 0.450119, mae: 0.594300, mean_q: 3.516302
[RESULT] FALSIFICATION!
 21652/100000: episode: 1386, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.427 [-0.245, 10.700], loss: 0.391656, mae: 0.561866, mean_q: 2.849427
[RESULT] FALSIFICATION!
 21653/100000: episode: 1387, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.245, 10.700], loss: 0.943073, mae: 0.795228, mean_q: 3.084637
[RESULT] FALSIFICATION!
 21654/100000: episode: 1388, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.571, 10.700], loss: 0.686303, mae: 0.659414, mean_q: 2.836162
[RESULT] FALSIFICATION!
 21655/100000: episode: 1389, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.440 [-0.245, 10.700], loss: 0.162629, mae: 0.350536, mean_q: 3.385204
[RESULT] FALSIFICATION!
 21656/100000: episode: 1390, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.446 [-0.245, 10.700], loss: 1.832773, mae: 0.955870, mean_q: 3.465034
[RESULT] FALSIFICATION!
 21657/100000: episode: 1391, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.466 [-0.280, 10.800], loss: 0.115517, mae: 0.374838, mean_q: 1.955850
[RESULT] FALSIFICATION!
 21658/100000: episode: 1392, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.434 [-0.280, 10.800], loss: 0.977656, mae: 0.833123, mean_q: 2.343567
[RESULT] FALSIFICATION!
 21659/100000: episode: 1393, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.462 [-0.245, 10.700], loss: 1.699227, mae: 0.918645, mean_q: 3.693407
[RESULT] FALSIFICATION!
 21660/100000: episode: 1394, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.465 [-0.245, 10.700], loss: 2.625760, mae: 1.324275, mean_q: 2.922315
[RESULT] FALSIFICATION!
 21661/100000: episode: 1395, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.422 [-0.245, 10.700], loss: 2.067828, mae: 1.255745, mean_q: 5.121262
[RESULT] FALSIFICATION!
 21662/100000: episode: 1396, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.481 [-0.245, 10.700], loss: 3.178495, mae: 1.528759, mean_q: 3.780127
[RESULT] FALSIFICATION!
 21663/100000: episode: 1397, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.245, 10.700], loss: 0.670973, mae: 0.720494, mean_q: 3.474392
[RESULT] FALSIFICATION!
[Info] Not found new level, current best level reached = 10.167445182800293
 21664/100000: episode: 1398, duration: 4.208s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.448 [-0.245, 10.700], loss: 1.514109, mae: 0.850944, mean_q: 3.101302
 21764/100000: episode: 1399, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 17.148, mean reward: 0.171 [0.013, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.180, 10.212], loss: 1.709085, mae: 0.868608, mean_q: 2.999003
 21864/100000: episode: 1400, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 13.436, mean reward: 0.134 [0.007, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.196, 10.098], loss: 1.877966, mae: 0.919542, mean_q: 3.067070
 21964/100000: episode: 1401, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.481, mean reward: 0.175 [0.013, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.987, 10.293], loss: 1.833972, mae: 0.946907, mean_q: 3.084449
 22064/100000: episode: 1402, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.740, mean reward: 0.157 [0.029, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.210], loss: 1.712656, mae: 0.855226, mean_q: 3.011642
 22164/100000: episode: 1403, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 20.634, mean reward: 0.206 [0.016, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.737, 10.098], loss: 1.792951, mae: 0.895568, mean_q: 2.984893
 22264/100000: episode: 1404, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 14.961, mean reward: 0.150 [0.009, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.879, 10.242], loss: 1.806512, mae: 0.899110, mean_q: 3.096116
 22364/100000: episode: 1405, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 20.129, mean reward: 0.201 [0.017, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.517, 10.098], loss: 1.955348, mae: 0.951161, mean_q: 3.018565
 22464/100000: episode: 1406, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 15.739, mean reward: 0.157 [0.046, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.483, 10.166], loss: 1.778649, mae: 0.919555, mean_q: 3.080547
 22564/100000: episode: 1407, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 13.897, mean reward: 0.139 [0.005, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.909, 10.243], loss: 1.869413, mae: 0.913761, mean_q: 2.942151
 22664/100000: episode: 1408, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.262, mean reward: 0.163 [0.017, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.891, 10.098], loss: 1.667897, mae: 0.870580, mean_q: 2.919919
 22764/100000: episode: 1409, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 15.848, mean reward: 0.158 [0.013, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.683, 10.098], loss: 1.735288, mae: 0.850423, mean_q: 2.835502
 22864/100000: episode: 1410, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 19.091, mean reward: 0.191 [0.031, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.524, 10.098], loss: 1.885778, mae: 0.945597, mean_q: 3.000460
 22964/100000: episode: 1411, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 14.559, mean reward: 0.146 [0.007, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.107], loss: 1.719241, mae: 0.867755, mean_q: 2.754097
 23064/100000: episode: 1412, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 21.697, mean reward: 0.217 [0.027, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.018, 10.318], loss: 1.773893, mae: 0.856932, mean_q: 2.727871
 23164/100000: episode: 1413, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 16.510, mean reward: 0.165 [0.026, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.152], loss: 1.691967, mae: 0.874769, mean_q: 2.953150
 23264/100000: episode: 1414, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 20.231, mean reward: 0.202 [0.038, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.481, 10.250], loss: 1.599877, mae: 0.814667, mean_q: 2.724237
 23364/100000: episode: 1415, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 17.574, mean reward: 0.176 [0.015, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.191], loss: 1.677001, mae: 0.842995, mean_q: 2.722336
 23464/100000: episode: 1416, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.962, mean reward: 0.180 [0.024, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.255, 10.098], loss: 1.587108, mae: 0.810173, mean_q: 2.741346
 23564/100000: episode: 1417, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 15.432, mean reward: 0.154 [0.007, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.596, 10.354], loss: 1.832193, mae: 0.887188, mean_q: 2.699305
 23664/100000: episode: 1418, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 14.893, mean reward: 0.149 [0.024, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.288, 10.109], loss: 1.771740, mae: 0.890440, mean_q: 2.838633
 23764/100000: episode: 1419, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 20.779, mean reward: 0.208 [0.033, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.054, 10.098], loss: 1.551250, mae: 0.814414, mean_q: 2.653200
 23864/100000: episode: 1420, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 19.244, mean reward: 0.192 [0.019, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.777, 10.346], loss: 1.422437, mae: 0.741183, mean_q: 2.641032
 23964/100000: episode: 1421, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 17.385, mean reward: 0.174 [0.025, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.059, 10.098], loss: 1.536671, mae: 0.773405, mean_q: 2.573780
 24064/100000: episode: 1422, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 13.188, mean reward: 0.132 [0.004, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.350, 10.098], loss: 1.559204, mae: 0.786169, mean_q: 2.630533
 24164/100000: episode: 1423, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.689, mean reward: 0.147 [0.018, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.681, 10.098], loss: 1.499619, mae: 0.780514, mean_q: 2.667892
 24264/100000: episode: 1424, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 16.364, mean reward: 0.164 [0.031, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.277, 10.098], loss: 1.189710, mae: 0.638272, mean_q: 2.379496
 24364/100000: episode: 1425, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 13.664, mean reward: 0.137 [0.004, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.597, 10.098], loss: 1.544318, mae: 0.757604, mean_q: 2.592056
 24464/100000: episode: 1426, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 13.297, mean reward: 0.133 [0.010, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.173, 10.098], loss: 1.133917, mae: 0.622894, mean_q: 2.296285
 24564/100000: episode: 1427, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 13.852, mean reward: 0.139 [0.018, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.502, 10.123], loss: 1.346410, mae: 0.670840, mean_q: 2.380947
 24664/100000: episode: 1428, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 14.622, mean reward: 0.146 [0.020, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.383, 10.098], loss: 1.203507, mae: 0.619882, mean_q: 2.278707
 24764/100000: episode: 1429, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 14.110, mean reward: 0.141 [0.012, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.356, 10.197], loss: 0.893802, mae: 0.490778, mean_q: 2.160057
 24864/100000: episode: 1430, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 16.245, mean reward: 0.162 [0.019, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.476, 10.160], loss: 0.910293, mae: 0.490042, mean_q: 2.172035
 24964/100000: episode: 1431, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 17.724, mean reward: 0.177 [0.005, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.389, 10.104], loss: 0.859561, mae: 0.481267, mean_q: 2.121610
 25064/100000: episode: 1432, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.276, mean reward: 0.133 [0.002, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.356, 10.109], loss: 0.711954, mae: 0.384734, mean_q: 1.882493
 25164/100000: episode: 1433, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 13.744, mean reward: 0.137 [0.009, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.631, 10.112], loss: 0.709905, mae: 0.371906, mean_q: 1.906982
 25264/100000: episode: 1434, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 23.444, mean reward: 0.234 [0.044, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.390, 10.098], loss: 0.561785, mae: 0.318021, mean_q: 1.841604
 25364/100000: episode: 1435, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.609, mean reward: 0.156 [0.014, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.195, 10.188], loss: 0.401969, mae: 0.251419, mean_q: 1.725133
 25464/100000: episode: 1436, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 14.012, mean reward: 0.140 [0.010, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.381, 10.234], loss: 0.329108, mae: 0.214280, mean_q: 1.646446
 25564/100000: episode: 1437, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 12.616, mean reward: 0.126 [0.015, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.169], loss: 0.247664, mae: 0.195742, mean_q: 1.494040
 25664/100000: episode: 1438, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.590, mean reward: 0.166 [0.003, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.279, 10.295], loss: 0.131196, mae: 0.135065, mean_q: 1.449185
 25764/100000: episode: 1439, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.598, mean reward: 0.176 [0.021, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.162, 10.098], loss: 0.044748, mae: 0.100640, mean_q: 1.198717
 25864/100000: episode: 1440, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 17.359, mean reward: 0.174 [0.018, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.719, 10.273], loss: 0.078494, mae: 0.110553, mean_q: 1.152855
 25964/100000: episode: 1441, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 20.327, mean reward: 0.203 [0.028, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.326, 10.098], loss: 0.029224, mae: 0.086023, mean_q: 1.062094
 26064/100000: episode: 1442, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 24.199, mean reward: 0.242 [0.003, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.098], loss: 0.005307, mae: 0.077979, mean_q: 0.964324
 26164/100000: episode: 1443, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 15.709, mean reward: 0.157 [0.015, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.371, 10.148], loss: 0.004642, mae: 0.075150, mean_q: 0.872023
 26264/100000: episode: 1444, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 21.915, mean reward: 0.219 [0.036, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.098], loss: 0.004300, mae: 0.073055, mean_q: 0.742127
 26364/100000: episode: 1445, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 14.463, mean reward: 0.145 [0.017, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.352, 10.374], loss: 0.004486, mae: 0.074155, mean_q: 0.588838
 26464/100000: episode: 1446, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 14.426, mean reward: 0.144 [0.014, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.775, 10.189], loss: 0.004378, mae: 0.073411, mean_q: 0.563133
 26564/100000: episode: 1447, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 17.160, mean reward: 0.172 [0.021, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.318, 10.225], loss: 0.004332, mae: 0.074188, mean_q: 0.466154
 26664/100000: episode: 1448, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 16.536, mean reward: 0.165 [0.021, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.322, 10.098], loss: 0.004119, mae: 0.072286, mean_q: 0.343464
 26764/100000: episode: 1449, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.262, mean reward: 0.173 [0.016, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.115, 10.255], loss: 0.004269, mae: 0.072822, mean_q: 0.329903
 26864/100000: episode: 1450, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.621, mean reward: 0.166 [0.025, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.301, 10.153], loss: 0.004232, mae: 0.072450, mean_q: 0.330180
 26964/100000: episode: 1451, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 14.684, mean reward: 0.147 [0.002, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.594, 10.110], loss: 0.004323, mae: 0.072529, mean_q: 0.329611
 27064/100000: episode: 1452, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 15.065, mean reward: 0.151 [0.029, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.789, 10.098], loss: 0.004087, mae: 0.071423, mean_q: 0.331634
 27164/100000: episode: 1453, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 17.981, mean reward: 0.180 [0.022, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.098], loss: 0.004184, mae: 0.071953, mean_q: 0.325936
 27264/100000: episode: 1454, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 14.025, mean reward: 0.140 [0.007, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.624, 10.116], loss: 0.003831, mae: 0.070038, mean_q: 0.330344
 27364/100000: episode: 1455, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 16.938, mean reward: 0.169 [0.009, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.098], loss: 0.003898, mae: 0.068880, mean_q: 0.326755
 27464/100000: episode: 1456, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 16.750, mean reward: 0.168 [0.011, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.786, 10.492], loss: 0.004124, mae: 0.071552, mean_q: 0.329620
 27564/100000: episode: 1457, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.983, mean reward: 0.150 [0.022, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.750, 10.098], loss: 0.003854, mae: 0.070380, mean_q: 0.330546
 27664/100000: episode: 1458, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 17.155, mean reward: 0.172 [0.013, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.819, 10.276], loss: 0.003925, mae: 0.070321, mean_q: 0.327198
 27764/100000: episode: 1459, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.704, mean reward: 0.157 [0.017, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.575, 10.098], loss: 0.003742, mae: 0.068819, mean_q: 0.323854
 27864/100000: episode: 1460, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 16.686, mean reward: 0.167 [0.005, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.301, 10.143], loss: 0.004078, mae: 0.071411, mean_q: 0.327332
 27964/100000: episode: 1461, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 16.224, mean reward: 0.162 [0.013, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.167, 10.098], loss: 0.003840, mae: 0.069932, mean_q: 0.324487
 28064/100000: episode: 1462, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 18.053, mean reward: 0.181 [0.018, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.270], loss: 0.003922, mae: 0.070141, mean_q: 0.325188
 28164/100000: episode: 1463, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 15.633, mean reward: 0.156 [0.007, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.836, 10.098], loss: 0.003919, mae: 0.069599, mean_q: 0.326514
 28264/100000: episode: 1464, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 22.936, mean reward: 0.229 [0.028, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.935, 10.377], loss: 0.004068, mae: 0.071110, mean_q: 0.325567
 28364/100000: episode: 1465, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.322, mean reward: 0.183 [0.017, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.860, 10.098], loss: 0.003907, mae: 0.069687, mean_q: 0.329761
 28464/100000: episode: 1466, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 18.720, mean reward: 0.187 [0.014, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.098], loss: 0.004010, mae: 0.070739, mean_q: 0.331778
 28564/100000: episode: 1467, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 15.191, mean reward: 0.152 [0.010, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.921, 10.098], loss: 0.004025, mae: 0.070104, mean_q: 0.329374
 28664/100000: episode: 1468, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 22.957, mean reward: 0.230 [0.045, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.842, 10.098], loss: 0.003750, mae: 0.068488, mean_q: 0.329006
 28764/100000: episode: 1469, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 18.655, mean reward: 0.187 [0.015, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.794, 10.098], loss: 0.003891, mae: 0.069676, mean_q: 0.329664
 28864/100000: episode: 1470, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 12.595, mean reward: 0.126 [0.022, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.588, 10.098], loss: 0.003773, mae: 0.068454, mean_q: 0.327230
 28964/100000: episode: 1471, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 17.353, mean reward: 0.174 [0.006, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.307, 10.345], loss: 0.003843, mae: 0.069283, mean_q: 0.327197
 29064/100000: episode: 1472, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.859, mean reward: 0.169 [0.005, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.132], loss: 0.003778, mae: 0.068934, mean_q: 0.326661
 29164/100000: episode: 1473, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 17.124, mean reward: 0.171 [0.015, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.409, 10.098], loss: 0.003920, mae: 0.070048, mean_q: 0.327916
 29264/100000: episode: 1474, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 15.773, mean reward: 0.158 [0.009, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.098], loss: 0.003620, mae: 0.066888, mean_q: 0.328475
 29364/100000: episode: 1475, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 14.155, mean reward: 0.142 [0.002, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.098], loss: 0.004268, mae: 0.073411, mean_q: 0.329749
 29464/100000: episode: 1476, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 18.444, mean reward: 0.184 [0.003, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.248, 10.383], loss: 0.003930, mae: 0.070297, mean_q: 0.330892
 29564/100000: episode: 1477, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.205, mean reward: 0.162 [0.018, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.243, 10.273], loss: 0.003944, mae: 0.070044, mean_q: 0.332385
 29664/100000: episode: 1478, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 15.015, mean reward: 0.150 [0.013, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.784, 10.098], loss: 0.003899, mae: 0.069365, mean_q: 0.328847
 29764/100000: episode: 1479, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 21.098, mean reward: 0.211 [0.022, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.601, 10.098], loss: 0.004125, mae: 0.072185, mean_q: 0.334776
 29864/100000: episode: 1480, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 14.961, mean reward: 0.150 [0.020, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.413, 10.253], loss: 0.004099, mae: 0.071245, mean_q: 0.338201
 29964/100000: episode: 1481, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.428, mean reward: 0.144 [0.032, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.073, 10.260], loss: 0.003869, mae: 0.069301, mean_q: 0.333593
 30064/100000: episode: 1482, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 18.256, mean reward: 0.183 [0.009, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.585, 10.318], loss: 0.003676, mae: 0.067184, mean_q: 0.334196
 30164/100000: episode: 1483, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 18.079, mean reward: 0.181 [0.028, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.431, 10.325], loss: 0.003929, mae: 0.069306, mean_q: 0.334208
 30264/100000: episode: 1484, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.795, mean reward: 0.178 [0.000, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.131], loss: 0.003776, mae: 0.068304, mean_q: 0.333289
 30364/100000: episode: 1485, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 18.882, mean reward: 0.189 [0.041, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.790, 10.167], loss: 0.003828, mae: 0.069613, mean_q: 0.340029
 30464/100000: episode: 1486, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 20.362, mean reward: 0.204 [0.016, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.460, 10.313], loss: 0.003905, mae: 0.069777, mean_q: 0.334510
 30564/100000: episode: 1487, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 16.553, mean reward: 0.166 [0.015, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.711, 10.098], loss: 0.003634, mae: 0.066946, mean_q: 0.335232
 30664/100000: episode: 1488, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.756, mean reward: 0.168 [0.027, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.885, 10.201], loss: 0.003831, mae: 0.068808, mean_q: 0.338739
 30764/100000: episode: 1489, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 20.691, mean reward: 0.207 [0.020, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.098], loss: 0.003888, mae: 0.069259, mean_q: 0.342755
 30864/100000: episode: 1490, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 14.050, mean reward: 0.140 [0.016, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.643, 10.144], loss: 0.003849, mae: 0.068781, mean_q: 0.342612
 30964/100000: episode: 1491, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 19.945, mean reward: 0.199 [0.041, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.961, 10.337], loss: 0.004075, mae: 0.071402, mean_q: 0.340162
 31064/100000: episode: 1492, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 21.208, mean reward: 0.212 [0.008, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.500, 10.355], loss: 0.003921, mae: 0.069842, mean_q: 0.337579
 31164/100000: episode: 1493, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 19.410, mean reward: 0.194 [0.009, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.818, 10.098], loss: 0.003644, mae: 0.067215, mean_q: 0.339394
 31264/100000: episode: 1494, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 15.508, mean reward: 0.155 [0.013, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.052, 10.098], loss: 0.003770, mae: 0.068635, mean_q: 0.340868
 31364/100000: episode: 1495, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 19.778, mean reward: 0.198 [0.024, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.536, 10.393], loss: 0.003915, mae: 0.070298, mean_q: 0.339114
 31464/100000: episode: 1496, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 17.992, mean reward: 0.180 [0.030, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.582, 10.098], loss: 0.003887, mae: 0.069021, mean_q: 0.342857
 31564/100000: episode: 1497, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 18.930, mean reward: 0.189 [0.020, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.907, 10.098], loss: 0.003630, mae: 0.066668, mean_q: 0.340327
[Info] New level: 0.6076881885528564 | Considering 10/90 traces
 31664/100000: episode: 1498, duration: 4.518s, episode steps: 100, steps per second: 22, episode reward: 18.096, mean reward: 0.181 [0.010, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.261, 10.098], loss: 0.003743, mae: 0.068286, mean_q: 0.340763
 31676/100000: episode: 1499, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 3.943, mean reward: 0.329 [0.288, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.389, 10.100], loss: 0.003425, mae: 0.064058, mean_q: 0.328794
 31692/100000: episode: 1500, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 5.343, mean reward: 0.334 [0.290, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.222, 10.100], loss: 0.003703, mae: 0.067554, mean_q: 0.337594
 31704/100000: episode: 1501, duration: 0.079s, episode steps: 12, steps per second: 153, episode reward: 3.874, mean reward: 0.323 [0.222, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.351, 10.100], loss: 0.003751, mae: 0.069161, mean_q: 0.339282
 31714/100000: episode: 1502, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 3.075, mean reward: 0.307 [0.252, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.292, 10.100], loss: 0.004358, mae: 0.074044, mean_q: 0.352871
 31732/100000: episode: 1503, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 6.006, mean reward: 0.334 [0.193, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.632, 10.317], loss: 0.003569, mae: 0.067052, mean_q: 0.349878
 31741/100000: episode: 1504, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 2.384, mean reward: 0.265 [0.193, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.206, 10.100], loss: 0.003124, mae: 0.063425, mean_q: 0.337844
 31751/100000: episode: 1505, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 3.107, mean reward: 0.311 [0.219, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.199, 10.100], loss: 0.003648, mae: 0.067117, mean_q: 0.348827
 31767/100000: episode: 1506, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 5.522, mean reward: 0.345 [0.266, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.342, 10.100], loss: 0.003880, mae: 0.067738, mean_q: 0.346662
 31779/100000: episode: 1507, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 3.950, mean reward: 0.329 [0.248, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.337, 10.100], loss: 0.004122, mae: 0.070464, mean_q: 0.347939
 31797/100000: episode: 1508, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 6.394, mean reward: 0.355 [0.205, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.363, 10.412], loss: 0.003788, mae: 0.070565, mean_q: 0.356897
 31806/100000: episode: 1509, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 2.330, mean reward: 0.259 [0.220, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.439, 10.100], loss: 0.003451, mae: 0.066611, mean_q: 0.339720
 31905/100000: episode: 1510, duration: 0.539s, episode steps: 99, steps per second: 184, episode reward: 17.426, mean reward: 0.176 [0.006, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.829, 10.134], loss: 0.003649, mae: 0.067156, mean_q: 0.347378
 31921/100000: episode: 1511, duration: 0.107s, episode steps: 16, steps per second: 149, episode reward: 5.947, mean reward: 0.372 [0.290, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.386, 10.100], loss: 0.003778, mae: 0.068297, mean_q: 0.356013
 31974/100000: episode: 1512, duration: 0.294s, episode steps: 53, steps per second: 180, episode reward: 11.881, mean reward: 0.224 [0.114, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.307, 10.100], loss: 0.003819, mae: 0.068800, mean_q: 0.350256
 32027/100000: episode: 1513, duration: 0.312s, episode steps: 53, steps per second: 170, episode reward: 13.619, mean reward: 0.257 [0.086, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.693, 10.100], loss: 0.004109, mae: 0.069826, mean_q: 0.353968
 32049/100000: episode: 1514, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 8.078, mean reward: 0.367 [0.263, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.497, 10.100], loss: 0.003536, mae: 0.067123, mean_q: 0.357316
 32061/100000: episode: 1515, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 3.767, mean reward: 0.314 [0.218, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.246, 10.100], loss: 0.003693, mae: 0.068352, mean_q: 0.371408
 32083/100000: episode: 1516, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 8.240, mean reward: 0.375 [0.183, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.155, 10.100], loss: 0.003457, mae: 0.066869, mean_q: 0.355354
 32105/100000: episode: 1517, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 6.911, mean reward: 0.314 [0.186, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.336, 10.100], loss: 0.003709, mae: 0.067164, mean_q: 0.352778
 32204/100000: episode: 1518, duration: 0.528s, episode steps: 99, steps per second: 188, episode reward: 16.342, mean reward: 0.165 [0.021, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.593, 10.100], loss: 0.003761, mae: 0.068019, mean_q: 0.362337
 32216/100000: episode: 1519, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 3.958, mean reward: 0.330 [0.205, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.151, 10.100], loss: 0.004043, mae: 0.068587, mean_q: 0.364622
 32234/100000: episode: 1520, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 6.515, mean reward: 0.362 [0.302, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.637, 10.467], loss: 0.003548, mae: 0.065706, mean_q: 0.363304
 32333/100000: episode: 1521, duration: 0.507s, episode steps: 99, steps per second: 195, episode reward: 17.197, mean reward: 0.174 [0.035, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.336, 10.100], loss: 0.004201, mae: 0.072431, mean_q: 0.368587
 32352/100000: episode: 1522, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 6.806, mean reward: 0.358 [0.227, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.053, 10.495], loss: 0.003624, mae: 0.067005, mean_q: 0.369320
 32374/100000: episode: 1523, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 8.326, mean reward: 0.378 [0.268, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.434, 10.100], loss: 0.003800, mae: 0.067900, mean_q: 0.358706
 32384/100000: episode: 1524, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 3.378, mean reward: 0.338 [0.259, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.342, 10.100], loss: 0.003731, mae: 0.067951, mean_q: 0.363663
 32396/100000: episode: 1525, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 4.637, mean reward: 0.386 [0.266, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.829, 10.100], loss: 0.004604, mae: 0.069266, mean_q: 0.388096
 32418/100000: episode: 1526, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 5.842, mean reward: 0.266 [0.149, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.293, 10.100], loss: 0.003182, mae: 0.064049, mean_q: 0.362594
 32430/100000: episode: 1527, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 3.668, mean reward: 0.306 [0.202, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.294, 10.100], loss: 0.004794, mae: 0.075826, mean_q: 0.370203
 32483/100000: episode: 1528, duration: 0.292s, episode steps: 53, steps per second: 181, episode reward: 12.754, mean reward: 0.241 [0.044, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.441, 10.292], loss: 0.003948, mae: 0.069497, mean_q: 0.377416
 32495/100000: episode: 1529, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 3.547, mean reward: 0.296 [0.229, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.343, 10.100], loss: 0.003788, mae: 0.068399, mean_q: 0.378409
 32517/100000: episode: 1530, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 6.264, mean reward: 0.285 [0.127, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.557, 10.100], loss: 0.003491, mae: 0.065913, mean_q: 0.367691
 32527/100000: episode: 1531, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 3.261, mean reward: 0.326 [0.217, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.330, 10.100], loss: 0.003457, mae: 0.067198, mean_q: 0.369721
 32536/100000: episode: 1532, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 3.096, mean reward: 0.344 [0.266, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.265, 10.100], loss: 0.004002, mae: 0.071837, mean_q: 0.369229
 32554/100000: episode: 1533, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 6.732, mean reward: 0.374 [0.274, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.315, 10.340], loss: 0.003836, mae: 0.070062, mean_q: 0.375688
 32653/100000: episode: 1534, duration: 0.537s, episode steps: 99, steps per second: 184, episode reward: 16.331, mean reward: 0.165 [0.021, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.731, 10.187], loss: 0.003762, mae: 0.068520, mean_q: 0.376914
 32752/100000: episode: 1535, duration: 0.545s, episode steps: 99, steps per second: 182, episode reward: 15.929, mean reward: 0.161 [0.025, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.439, 10.100], loss: 0.003642, mae: 0.066927, mean_q: 0.373230
 32764/100000: episode: 1536, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 4.048, mean reward: 0.337 [0.216, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.346, 10.100], loss: 0.004110, mae: 0.066592, mean_q: 0.380983
 32773/100000: episode: 1537, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 1.531, mean reward: 0.170 [0.073, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.219, 10.100], loss: 0.003923, mae: 0.072275, mean_q: 0.373314
 32785/100000: episode: 1538, duration: 0.077s, episode steps: 12, steps per second: 155, episode reward: 3.631, mean reward: 0.303 [0.174, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.259, 10.100], loss: 0.003832, mae: 0.069566, mean_q: 0.380268
 32801/100000: episode: 1539, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 4.929, mean reward: 0.308 [0.171, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.582, 10.100], loss: 0.003499, mae: 0.066616, mean_q: 0.377405
 32817/100000: episode: 1540, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 3.393, mean reward: 0.212 [0.078, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.144, 10.100], loss: 0.004097, mae: 0.070438, mean_q: 0.377526
 32916/100000: episode: 1541, duration: 0.537s, episode steps: 99, steps per second: 184, episode reward: 15.661, mean reward: 0.158 [0.030, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.955, 10.153], loss: 0.004129, mae: 0.070755, mean_q: 0.376618
 32925/100000: episode: 1542, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 1.962, mean reward: 0.218 [0.173, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.094, 10.100], loss: 0.003138, mae: 0.063625, mean_q: 0.392488
 32935/100000: episode: 1543, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 3.771, mean reward: 0.377 [0.298, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.306, 10.100], loss: 0.003439, mae: 0.065234, mean_q: 0.377139
 32947/100000: episode: 1544, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 3.417, mean reward: 0.285 [0.222, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.580, 10.100], loss: 0.003381, mae: 0.066850, mean_q: 0.379141
 32957/100000: episode: 1545, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 2.646, mean reward: 0.265 [0.224, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.394, 10.100], loss: 0.004290, mae: 0.073897, mean_q: 0.378424
 33010/100000: episode: 1546, duration: 0.306s, episode steps: 53, steps per second: 173, episode reward: 16.224, mean reward: 0.306 [0.033, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.912, 10.100], loss: 0.003769, mae: 0.068369, mean_q: 0.378961
 33022/100000: episode: 1547, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 3.398, mean reward: 0.283 [0.199, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.394, 10.100], loss: 0.003926, mae: 0.071959, mean_q: 0.389369
 33040/100000: episode: 1548, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 5.261, mean reward: 0.292 [0.226, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.356], loss: 0.003296, mae: 0.061998, mean_q: 0.376604
 33058/100000: episode: 1549, duration: 0.111s, episode steps: 18, steps per second: 161, episode reward: 6.804, mean reward: 0.378 [0.306, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.541, 10.570], loss: 0.004228, mae: 0.070871, mean_q: 0.378493
 33070/100000: episode: 1550, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 3.595, mean reward: 0.300 [0.198, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.241, 10.100], loss: 0.003816, mae: 0.068786, mean_q: 0.378375
 33089/100000: episode: 1551, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 6.303, mean reward: 0.332 [0.261, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.763, 10.475], loss: 0.004141, mae: 0.072516, mean_q: 0.386960
 33101/100000: episode: 1552, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 3.349, mean reward: 0.279 [0.206, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.333, 10.100], loss: 0.004140, mae: 0.070050, mean_q: 0.396483
 33113/100000: episode: 1553, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 2.608, mean reward: 0.217 [0.096, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.271, 10.100], loss: 0.004589, mae: 0.073839, mean_q: 0.382669
 33132/100000: episode: 1554, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 5.836, mean reward: 0.307 [0.182, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.352], loss: 0.003872, mae: 0.068840, mean_q: 0.389822
 33144/100000: episode: 1555, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 3.130, mean reward: 0.261 [0.205, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.060, 10.100], loss: 0.003285, mae: 0.061674, mean_q: 0.375614
 33197/100000: episode: 1556, duration: 0.294s, episode steps: 53, steps per second: 180, episode reward: 12.378, mean reward: 0.234 [0.033, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.338, 10.102], loss: 0.003992, mae: 0.070240, mean_q: 0.385844
 33209/100000: episode: 1557, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 3.017, mean reward: 0.251 [0.168, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.248, 10.100], loss: 0.004291, mae: 0.072193, mean_q: 0.381606
 33221/100000: episode: 1558, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 3.608, mean reward: 0.301 [0.191, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.726, 10.100], loss: 0.004099, mae: 0.072207, mean_q: 0.388783
 33240/100000: episode: 1559, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 4.444, mean reward: 0.234 [0.161, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.337, 10.360], loss: 0.003946, mae: 0.069647, mean_q: 0.377228
 33258/100000: episode: 1560, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 6.533, mean reward: 0.363 [0.260, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.730, 10.409], loss: 0.004251, mae: 0.071475, mean_q: 0.397056
 33267/100000: episode: 1561, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 2.614, mean reward: 0.290 [0.194, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.232, 10.100], loss: 0.004117, mae: 0.068807, mean_q: 0.387026
 33366/100000: episode: 1562, duration: 0.530s, episode steps: 99, steps per second: 187, episode reward: 17.579, mean reward: 0.178 [0.018, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.945, 10.100], loss: 0.003914, mae: 0.069645, mean_q: 0.389091
 33376/100000: episode: 1563, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 3.468, mean reward: 0.347 [0.284, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.237, 10.100], loss: 0.004171, mae: 0.071856, mean_q: 0.406674
 33388/100000: episode: 1564, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 2.437, mean reward: 0.203 [0.120, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.042, 10.100], loss: 0.004743, mae: 0.076780, mean_q: 0.384274
 33404/100000: episode: 1565, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 4.891, mean reward: 0.306 [0.255, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.819, 10.100], loss: 0.003975, mae: 0.072592, mean_q: 0.404002
 33416/100000: episode: 1566, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 3.614, mean reward: 0.301 [0.195, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.304, 10.100], loss: 0.003876, mae: 0.066770, mean_q: 0.392107
 33428/100000: episode: 1567, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 3.838, mean reward: 0.320 [0.258, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.234, 10.100], loss: 0.004615, mae: 0.077394, mean_q: 0.392174
 33450/100000: episode: 1568, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 6.818, mean reward: 0.310 [0.207, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.428, 10.100], loss: 0.004072, mae: 0.071400, mean_q: 0.391346
 33462/100000: episode: 1569, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 2.737, mean reward: 0.228 [0.180, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.277, 10.100], loss: 0.003182, mae: 0.063333, mean_q: 0.389757
 33471/100000: episode: 1570, duration: 0.063s, episode steps: 9, steps per second: 144, episode reward: 1.949, mean reward: 0.217 [0.171, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.344, 10.100], loss: 0.003362, mae: 0.064160, mean_q: 0.385592
 33490/100000: episode: 1571, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 7.586, mean reward: 0.399 [0.346, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.066, 10.459], loss: 0.004299, mae: 0.070263, mean_q: 0.391441
 33502/100000: episode: 1572, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 3.221, mean reward: 0.268 [0.176, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.326, 10.100], loss: 0.004215, mae: 0.071146, mean_q: 0.407756
 33520/100000: episode: 1573, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 6.474, mean reward: 0.360 [0.251, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.839, 10.515], loss: 0.003658, mae: 0.067393, mean_q: 0.389877
 33542/100000: episode: 1574, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 7.374, mean reward: 0.335 [0.229, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.101, 10.100], loss: 0.004383, mae: 0.073154, mean_q: 0.412297
 33552/100000: episode: 1575, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 3.457, mean reward: 0.346 [0.265, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.788, 10.100], loss: 0.003773, mae: 0.068359, mean_q: 0.392998
 33561/100000: episode: 1576, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 3.396, mean reward: 0.377 [0.280, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.934, 10.100], loss: 0.003912, mae: 0.071862, mean_q: 0.388723
 33580/100000: episode: 1577, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 7.047, mean reward: 0.371 [0.275, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.542], loss: 0.004183, mae: 0.071632, mean_q: 0.400869
 33602/100000: episode: 1578, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 9.156, mean reward: 0.416 [0.318, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.724, 10.100], loss: 0.004102, mae: 0.070574, mean_q: 0.397541
 33614/100000: episode: 1579, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 3.314, mean reward: 0.276 [0.231, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.810, 10.100], loss: 0.004135, mae: 0.073449, mean_q: 0.410863
 33636/100000: episode: 1580, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 9.527, mean reward: 0.433 [0.351, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.604, 10.100], loss: 0.004198, mae: 0.071220, mean_q: 0.411126
 33654/100000: episode: 1581, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 5.129, mean reward: 0.285 [0.197, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.526, 10.400], loss: 0.004203, mae: 0.071231, mean_q: 0.414818
 33673/100000: episode: 1582, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 6.545, mean reward: 0.344 [0.267, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.295, 10.426], loss: 0.003935, mae: 0.069067, mean_q: 0.401146
 33683/100000: episode: 1583, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 2.829, mean reward: 0.283 [0.253, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.388, 10.100], loss: 0.003356, mae: 0.062553, mean_q: 0.398268
 33701/100000: episode: 1584, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 4.956, mean reward: 0.275 [0.196, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.336], loss: 0.003978, mae: 0.070134, mean_q: 0.406752
 33723/100000: episode: 1585, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 8.125, mean reward: 0.369 [0.269, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.740, 10.100], loss: 0.004053, mae: 0.070439, mean_q: 0.397034
 33735/100000: episode: 1586, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 3.401, mean reward: 0.283 [0.205, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.342, 10.100], loss: 0.004162, mae: 0.069718, mean_q: 0.400464
 33747/100000: episode: 1587, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 4.085, mean reward: 0.340 [0.176, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.114, 10.100], loss: 0.003991, mae: 0.068389, mean_q: 0.424738
[Info] New level: 0.8048567771911621 | Considering 10/90 traces
 33757/100000: episode: 1588, duration: 4.048s, episode steps: 10, steps per second: 2, episode reward: 3.080, mean reward: 0.308 [0.272, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.869, 10.100], loss: 0.004117, mae: 0.071894, mean_q: 0.404123
 33771/100000: episode: 1589, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 6.296, mean reward: 0.450 [0.381, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.509], loss: 0.003886, mae: 0.069402, mean_q: 0.407053
 33785/100000: episode: 1590, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 6.513, mean reward: 0.465 [0.389, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.495], loss: 0.004115, mae: 0.072447, mean_q: 0.410727
 33797/100000: episode: 1591, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 4.535, mean reward: 0.378 [0.295, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.321], loss: 0.003644, mae: 0.067126, mean_q: 0.418716
 33815/100000: episode: 1592, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 6.099, mean reward: 0.339 [0.218, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.413], loss: 0.003439, mae: 0.066174, mean_q: 0.418219
[RESULT] FALSIFICATION!
 33819/100000: episode: 1593, duration: 0.030s, episode steps: 4, steps per second: 135, episode reward: 11.494, mean reward: 2.873 [0.436, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.019, 7.880], loss: 0.003776, mae: 0.068179, mean_q: 0.398296
 33833/100000: episode: 1594, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 6.143, mean reward: 0.439 [0.357, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.542], loss: 0.003848, mae: 0.069116, mean_q: 0.407212
 33845/100000: episode: 1595, duration: 0.085s, episode steps: 12, steps per second: 140, episode reward: 5.030, mean reward: 0.419 [0.305, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.307, 10.550], loss: 0.116016, mae: 0.132140, mean_q: 0.435322
 33846/100000: episode: 1596, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.070, 10.366], loss: 0.006650, mae: 0.098778, mean_q: 0.476586
 33863/100000: episode: 1597, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 6.293, mean reward: 0.370 [0.271, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.534, 10.392], loss: 0.005100, mae: 0.080238, mean_q: 0.409968
 33879/100000: episode: 1598, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 5.506, mean reward: 0.344 [0.210, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.294, 10.343], loss: 0.004382, mae: 0.073296, mean_q: 0.434814
 33893/100000: episode: 1599, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 5.746, mean reward: 0.410 [0.271, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.937, 10.458], loss: 0.004625, mae: 0.073115, mean_q: 0.425027
 33909/100000: episode: 1600, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.445, mean reward: 0.403 [0.256, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.634, 10.418], loss: 0.004136, mae: 0.072418, mean_q: 0.422441
 33923/100000: episode: 1601, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 5.350, mean reward: 0.382 [0.250, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.358], loss: 0.003571, mae: 0.066888, mean_q: 0.412701
 33940/100000: episode: 1602, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 6.631, mean reward: 0.390 [0.306, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.383], loss: 0.003707, mae: 0.068411, mean_q: 0.422388
 33956/100000: episode: 1603, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 8.189, mean reward: 0.512 [0.371, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.579], loss: 0.091653, mae: 0.140082, mean_q: 0.457678
 33970/100000: episode: 1604, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 6.478, mean reward: 0.463 [0.256, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.112, 10.561], loss: 0.005481, mae: 0.083969, mean_q: 0.430468
 33986/100000: episode: 1605, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 7.037, mean reward: 0.440 [0.342, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-1.018, 10.575], loss: 0.004283, mae: 0.071966, mean_q: 0.425735
 34000/100000: episode: 1606, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 5.300, mean reward: 0.379 [0.274, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.265, 10.480], loss: 0.003797, mae: 0.069744, mean_q: 0.431439
 34018/100000: episode: 1607, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 7.593, mean reward: 0.422 [0.375, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.634, 10.484], loss: 0.004193, mae: 0.071037, mean_q: 0.428948
 34034/100000: episode: 1608, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 6.645, mean reward: 0.415 [0.349, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.216, 10.516], loss: 0.004436, mae: 0.073700, mean_q: 0.431479
 34050/100000: episode: 1609, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 6.076, mean reward: 0.380 [0.325, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.144, 10.485], loss: 0.003568, mae: 0.065316, mean_q: 0.436834
 34051/100000: episode: 1610, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.070, 10.437], loss: 0.003000, mae: 0.060814, mean_q: 0.386617
 34069/100000: episode: 1611, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 8.831, mean reward: 0.491 [0.357, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.279, 10.670], loss: 0.004154, mae: 0.073732, mean_q: 0.447439
 34104/100000: episode: 1612, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 10.137, mean reward: 0.290 [0.191, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.395, 10.100], loss: 0.004150, mae: 0.071772, mean_q: 0.444778
 34139/100000: episode: 1613, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 13.332, mean reward: 0.381 [0.302, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.788, 10.100], loss: 0.003803, mae: 0.068693, mean_q: 0.434528
 34151/100000: episode: 1614, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 4.512, mean reward: 0.376 [0.312, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.358], loss: 0.003413, mae: 0.064931, mean_q: 0.459071
 34169/100000: episode: 1615, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 7.689, mean reward: 0.427 [0.223, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.803, 10.435], loss: 0.003655, mae: 0.064836, mean_q: 0.450188
 34181/100000: episode: 1616, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 3.317, mean reward: 0.276 [0.238, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.229, 10.381], loss: 0.004431, mae: 0.073669, mean_q: 0.452464
 34199/100000: episode: 1617, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 7.713, mean reward: 0.429 [0.372, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.358, 10.570], loss: 0.003952, mae: 0.068602, mean_q: 0.443616
 34211/100000: episode: 1618, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 4.295, mean reward: 0.358 [0.297, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.443, 10.505], loss: 0.003851, mae: 0.068651, mean_q: 0.445255
 34227/100000: episode: 1619, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 5.370, mean reward: 0.336 [0.271, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.485], loss: 0.004112, mae: 0.070913, mean_q: 0.468670
 34241/100000: episode: 1620, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 4.630, mean reward: 0.331 [0.272, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.545, 10.303], loss: 0.003581, mae: 0.066421, mean_q: 0.455304
 34253/100000: episode: 1621, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 5.416, mean reward: 0.451 [0.381, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.479], loss: 0.003934, mae: 0.068467, mean_q: 0.468428
 34270/100000: episode: 1622, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 6.010, mean reward: 0.354 [0.236, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.672, 10.384], loss: 0.003771, mae: 0.067324, mean_q: 0.440496
 34282/100000: episode: 1623, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 4.659, mean reward: 0.388 [0.317, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-1.690, 10.517], loss: 0.003450, mae: 0.065028, mean_q: 0.460166
 34317/100000: episode: 1624, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 13.029, mean reward: 0.372 [0.208, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.266, 10.100], loss: 0.003886, mae: 0.069478, mean_q: 0.452493
 34332/100000: episode: 1625, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 6.012, mean reward: 0.401 [0.365, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.543], loss: 0.004361, mae: 0.072721, mean_q: 0.457933
 34346/100000: episode: 1626, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 4.757, mean reward: 0.340 [0.283, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.442], loss: 0.003894, mae: 0.069053, mean_q: 0.448672
 34347/100000: episode: 1627, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.548, mean reward: 0.548 [0.548, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.478, 10.335], loss: 0.002620, mae: 0.056864, mean_q: 0.482234
 34348/100000: episode: 1628, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.324, 10.378], loss: 0.002799, mae: 0.066069, mean_q: 0.436197
 34362/100000: episode: 1629, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 6.694, mean reward: 0.478 [0.374, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.437], loss: 0.004532, mae: 0.072611, mean_q: 0.471754
 34376/100000: episode: 1630, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 4.924, mean reward: 0.352 [0.259, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.275, 10.384], loss: 0.003756, mae: 0.067489, mean_q: 0.467650
 34377/100000: episode: 1631, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.070, 10.358], loss: 0.007287, mae: 0.104276, mean_q: 0.464860
 34391/100000: episode: 1632, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 4.493, mean reward: 0.321 [0.188, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.356], loss: 0.023211, mae: 0.170154, mean_q: 0.493014
 34405/100000: episode: 1633, duration: 0.093s, episode steps: 14, steps per second: 151, episode reward: 4.853, mean reward: 0.347 [0.292, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.482], loss: 0.007947, mae: 0.100316, mean_q: 0.457025
 34421/100000: episode: 1634, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 5.514, mean reward: 0.345 [0.242, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.131, 10.281], loss: 0.004766, mae: 0.078504, mean_q: 0.482058
 34456/100000: episode: 1635, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 15.563, mean reward: 0.445 [0.283, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.542, 10.100], loss: 0.004315, mae: 0.072060, mean_q: 0.468249
 34474/100000: episode: 1636, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 6.378, mean reward: 0.354 [0.240, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.146, 10.310], loss: 0.003371, mae: 0.063434, mean_q: 0.460964
 34490/100000: episode: 1637, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 5.773, mean reward: 0.361 [0.280, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.226, 10.306], loss: 0.004545, mae: 0.074732, mean_q: 0.466702
 34504/100000: episode: 1638, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 4.744, mean reward: 0.339 [0.202, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.356], loss: 0.003427, mae: 0.064555, mean_q: 0.491416
 34518/100000: episode: 1639, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 4.993, mean reward: 0.357 [0.303, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.372, 10.395], loss: 0.004067, mae: 0.070580, mean_q: 0.481275
 34532/100000: episode: 1640, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 4.904, mean reward: 0.350 [0.255, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.434], loss: 0.096808, mae: 0.116229, mean_q: 0.518053
 34544/100000: episode: 1641, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 4.457, mean reward: 0.371 [0.300, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.060, 10.496], loss: 0.005828, mae: 0.086407, mean_q: 0.487982
 34559/100000: episode: 1642, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 5.826, mean reward: 0.388 [0.323, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.662, 10.497], loss: 0.006707, mae: 0.082211, mean_q: 0.467243
 34573/100000: episode: 1643, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 4.624, mean reward: 0.330 [0.242, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.557, 10.426], loss: 0.008680, mae: 0.105607, mean_q: 0.479787
 34587/100000: episode: 1644, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 5.110, mean reward: 0.365 [0.295, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.386], loss: 0.004518, mae: 0.074161, mean_q: 0.479663
 34604/100000: episode: 1645, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 7.045, mean reward: 0.414 [0.348, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.137, 10.455], loss: 0.004086, mae: 0.070805, mean_q: 0.474275
 34622/100000: episode: 1646, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 9.974, mean reward: 0.554 [0.414, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.691], loss: 0.004188, mae: 0.072334, mean_q: 0.479502
 34639/100000: episode: 1647, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 6.963, mean reward: 0.410 [0.345, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.231, 10.549], loss: 0.004351, mae: 0.072810, mean_q: 0.483983
 34653/100000: episode: 1648, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 5.168, mean reward: 0.369 [0.293, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.606, 10.478], loss: 0.004148, mae: 0.070358, mean_q: 0.491603
 34654/100000: episode: 1649, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.070, 10.436], loss: 0.004698, mae: 0.078351, mean_q: 0.565621
 34668/100000: episode: 1650, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 4.372, mean reward: 0.312 [0.216, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.391], loss: 0.005298, mae: 0.079863, mean_q: 0.492085
 34703/100000: episode: 1651, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 11.057, mean reward: 0.316 [0.194, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.803, 10.100], loss: 0.003385, mae: 0.063857, mean_q: 0.483200
 34715/100000: episode: 1652, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 4.771, mean reward: 0.398 [0.324, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.522], loss: 0.003400, mae: 0.063966, mean_q: 0.483474
 34750/100000: episode: 1653, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 12.789, mean reward: 0.365 [0.156, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.426, 10.100], loss: 0.003813, mae: 0.067738, mean_q: 0.487880
 34765/100000: episode: 1654, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 5.443, mean reward: 0.363 [0.164, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.019, 10.302], loss: 0.004035, mae: 0.071552, mean_q: 0.497804
 34800/100000: episode: 1655, duration: 0.202s, episode steps: 35, steps per second: 174, episode reward: 12.259, mean reward: 0.350 [0.234, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.977, 10.100], loss: 0.004201, mae: 0.072071, mean_q: 0.505001
 34814/100000: episode: 1656, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 6.852, mean reward: 0.489 [0.406, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.602], loss: 0.003961, mae: 0.069700, mean_q: 0.502643
 34828/100000: episode: 1657, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 4.270, mean reward: 0.305 [0.231, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.422], loss: 0.003673, mae: 0.066959, mean_q: 0.496020
 34829/100000: episode: 1658, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.481, mean reward: 0.481 [0.481, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.070, 10.289], loss: 0.004294, mae: 0.071938, mean_q: 0.469426
 34843/100000: episode: 1659, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 5.666, mean reward: 0.405 [0.300, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.437], loss: 0.004221, mae: 0.070989, mean_q: 0.492421
 34855/100000: episode: 1660, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 3.915, mean reward: 0.326 [0.267, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.444], loss: 0.114837, mae: 0.128452, mean_q: 0.546285
 34856/100000: episode: 1661, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.394, mean reward: 0.394 [0.394, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.070, 10.375], loss: 0.008925, mae: 0.101840, mean_q: 0.417815
 34870/100000: episode: 1662, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 5.113, mean reward: 0.365 [0.226, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.386], loss: 0.095024, mae: 0.125588, mean_q: 0.546913
 34887/100000: episode: 1663, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 6.400, mean reward: 0.376 [0.296, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.401], loss: 0.007973, mae: 0.100102, mean_q: 0.485593
 34905/100000: episode: 1664, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 7.195, mean reward: 0.400 [0.309, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.417], loss: 0.075960, mae: 0.104634, mean_q: 0.549357
 34919/100000: episode: 1665, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 4.511, mean reward: 0.322 [0.171, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.543, 10.391], loss: 0.006064, mae: 0.087619, mean_q: 0.520163
 34933/100000: episode: 1666, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 4.606, mean reward: 0.329 [0.211, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.376], loss: 0.004626, mae: 0.077754, mean_q: 0.502532
 34951/100000: episode: 1667, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 8.576, mean reward: 0.476 [0.376, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.991, 10.510], loss: 0.004210, mae: 0.071298, mean_q: 0.497372
 34986/100000: episode: 1668, duration: 0.204s, episode steps: 35, steps per second: 171, episode reward: 15.603, mean reward: 0.446 [0.294, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.626, 10.100], loss: 0.003889, mae: 0.068691, mean_q: 0.501972
 34987/100000: episode: 1669, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.070, 10.385], loss: 0.003054, mae: 0.062358, mean_q: 0.562738
 35002/100000: episode: 1670, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 4.651, mean reward: 0.310 [0.244, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.442], loss: 0.004084, mae: 0.069755, mean_q: 0.505447
 35014/100000: episode: 1671, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 3.974, mean reward: 0.331 [0.296, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.447], loss: 0.003374, mae: 0.062824, mean_q: 0.512905
[RESULT] FALSIFICATION!
 35031/100000: episode: 1672, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 18.681, mean reward: 1.099 [0.325, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.015, 10.635], loss: 0.003799, mae: 0.069379, mean_q: 0.497187
 35043/100000: episode: 1673, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 4.689, mean reward: 0.391 [0.344, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.428], loss: 0.003716, mae: 0.066167, mean_q: 0.524527
 35044/100000: episode: 1674, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.428, mean reward: 0.428 [0.428, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.070, 10.437], loss: 0.002508, mae: 0.059569, mean_q: 0.543113
 35079/100000: episode: 1675, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 14.188, mean reward: 0.405 [0.262, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.422, 10.100], loss: 0.003787, mae: 0.066787, mean_q: 0.522272
 35095/100000: episode: 1676, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 5.890, mean reward: 0.368 [0.284, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.372], loss: 0.003537, mae: 0.065158, mean_q: 0.522826
 35111/100000: episode: 1677, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 7.983, mean reward: 0.499 [0.416, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.376, 10.592], loss: 0.003880, mae: 0.069526, mean_q: 0.532362
[Info] New level: 1.0255597829818726 | Considering 10/90 traces
 35125/100000: episode: 1678, duration: 4.062s, episode steps: 14, steps per second: 3, episode reward: 5.530, mean reward: 0.395 [0.325, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.174, 10.546], loss: 0.004057, mae: 0.068527, mean_q: 0.540816
 35156/100000: episode: 1679, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 14.088, mean reward: 0.454 [0.252, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.422, 10.100], loss: 0.003968, mae: 0.069076, mean_q: 0.526077
 35167/100000: episode: 1680, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 5.951, mean reward: 0.541 [0.473, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.299, 10.611], loss: 0.003605, mae: 0.066168, mean_q: 0.517224
 35198/100000: episode: 1681, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 10.060, mean reward: 0.325 [0.171, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.750, 10.100], loss: 0.003624, mae: 0.066247, mean_q: 0.518444
 35230/100000: episode: 1682, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 10.868, mean reward: 0.340 [0.118, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.315, 10.100], loss: 0.046592, mae: 0.093591, mean_q: 0.538280
 35262/100000: episode: 1683, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 13.218, mean reward: 0.413 [0.281, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.304, 10.100], loss: 0.003930, mae: 0.068381, mean_q: 0.536019
 35295/100000: episode: 1684, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 13.392, mean reward: 0.406 [0.225, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.140, 10.100], loss: 0.003806, mae: 0.066018, mean_q: 0.538649
 35328/100000: episode: 1685, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 10.658, mean reward: 0.323 [0.068, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.517, 10.100], loss: 0.003834, mae: 0.067425, mean_q: 0.549239
 35362/100000: episode: 1686, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 11.676, mean reward: 0.343 [0.017, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.841, 10.100], loss: 0.043876, mae: 0.091900, mean_q: 0.549679
 35396/100000: episode: 1687, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 18.206, mean reward: 0.535 [0.389, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.512, 10.100], loss: 0.003954, mae: 0.069562, mean_q: 0.550326
 35429/100000: episode: 1688, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 12.827, mean reward: 0.389 [0.228, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.504, 10.100], loss: 0.003717, mae: 0.065620, mean_q: 0.543685
 35463/100000: episode: 1689, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 12.619, mean reward: 0.371 [0.208, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.454, 10.100], loss: 0.085182, mae: 0.131683, mean_q: 0.573159
 35497/100000: episode: 1690, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 9.726, mean reward: 0.286 [0.173, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.596, 10.100], loss: 0.005755, mae: 0.079079, mean_q: 0.565214
 35530/100000: episode: 1691, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 10.501, mean reward: 0.318 [0.100, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.257, 10.100], loss: 0.050560, mae: 0.108145, mean_q: 0.595919
 35561/100000: episode: 1692, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 13.927, mean reward: 0.449 [0.358, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.497, 10.100], loss: 0.073886, mae: 0.217102, mean_q: 0.550942
 35595/100000: episode: 1693, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 11.510, mean reward: 0.339 [0.098, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.843, 10.100], loss: 0.012607, mae: 0.125715, mean_q: 0.556443
 35626/100000: episode: 1694, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 11.183, mean reward: 0.361 [0.246, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.679, 10.100], loss: 0.007571, mae: 0.094960, mean_q: 0.568723
[RESULT] FALSIFICATION!
 35628/100000: episode: 1695, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.595, mean reward: 5.298 [0.595, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.696 [-0.015, 7.747], loss: 0.007462, mae: 0.097884, mean_q: 0.562981
 35662/100000: episode: 1696, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 15.907, mean reward: 0.468 [0.339, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.893, 10.100], loss: 0.048379, mae: 0.097862, mean_q: 0.591467
 35696/100000: episode: 1697, duration: 0.187s, episode steps: 34, steps per second: 181, episode reward: 12.132, mean reward: 0.357 [0.078, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.143, 10.100], loss: 0.047952, mae: 0.100865, mean_q: 0.613357
 35728/100000: episode: 1698, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 13.098, mean reward: 0.409 [0.286, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.616, 10.100], loss: 0.005712, mae: 0.083195, mean_q: 0.598592
 35761/100000: episode: 1699, duration: 0.196s, episode steps: 33, steps per second: 168, episode reward: 12.484, mean reward: 0.378 [0.160, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.367, 10.100], loss: 0.045946, mae: 0.102987, mean_q: 0.618995
 35795/100000: episode: 1700, duration: 0.212s, episode steps: 34, steps per second: 160, episode reward: 14.373, mean reward: 0.423 [0.226, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.128, 10.100], loss: 0.043307, mae: 0.095071, mean_q: 0.623976
 35828/100000: episode: 1701, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 10.797, mean reward: 0.327 [0.052, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.044, 10.100], loss: 0.085819, mae: 0.103422, mean_q: 0.623814
 35861/100000: episode: 1702, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 11.972, mean reward: 0.363 [0.170, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.395, 10.100], loss: 0.007790, mae: 0.098231, mean_q: 0.589685
 35895/100000: episode: 1703, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 13.186, mean reward: 0.388 [0.223, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.129, 10.100], loss: 0.045352, mae: 0.104939, mean_q: 0.605064
 35906/100000: episode: 1704, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 5.807, mean reward: 0.528 [0.494, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.607], loss: 0.004568, mae: 0.077763, mean_q: 0.589894
 35918/100000: episode: 1705, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 6.937, mean reward: 0.578 [0.535, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.510], loss: 0.003818, mae: 0.069483, mean_q: 0.591340
 35952/100000: episode: 1706, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 15.615, mean reward: 0.459 [0.321, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.301, 10.100], loss: 0.003790, mae: 0.067699, mean_q: 0.591721
 35964/100000: episode: 1707, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 5.989, mean reward: 0.499 [0.431, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.559], loss: 0.116180, mae: 0.106953, mean_q: 0.614086
 35998/100000: episode: 1708, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 12.875, mean reward: 0.379 [0.216, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.586, 10.100], loss: 0.044097, mae: 0.095470, mean_q: 0.611817
 36029/100000: episode: 1709, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 11.467, mean reward: 0.370 [0.251, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.798, 10.100], loss: 0.044630, mae: 0.081622, mean_q: 0.617676
 36063/100000: episode: 1710, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 14.433, mean reward: 0.425 [0.260, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.688, 10.100], loss: 0.043714, mae: 0.085415, mean_q: 0.599248
 36097/100000: episode: 1711, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 13.746, mean reward: 0.404 [0.274, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.490, 10.100], loss: 0.003713, mae: 0.066308, mean_q: 0.611061
 36109/100000: episode: 1712, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 5.572, mean reward: 0.464 [0.397, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.558], loss: 0.003944, mae: 0.069402, mean_q: 0.593731
 36140/100000: episode: 1713, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 13.281, mean reward: 0.428 [0.310, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.692, 10.100], loss: 0.004074, mae: 0.069931, mean_q: 0.620207
 36151/100000: episode: 1714, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 5.150, mean reward: 0.468 [0.416, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.600], loss: 0.125026, mae: 0.117231, mean_q: 0.659472
 36185/100000: episode: 1715, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 12.924, mean reward: 0.380 [0.170, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.849, 10.100], loss: 0.004828, mae: 0.076391, mean_q: 0.608321
 36218/100000: episode: 1716, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 11.898, mean reward: 0.361 [0.227, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.367, 10.100], loss: 0.003618, mae: 0.065172, mean_q: 0.616691
 36252/100000: episode: 1717, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 14.529, mean reward: 0.427 [0.331, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.326, 10.100], loss: 0.041040, mae: 0.078825, mean_q: 0.640507
 36285/100000: episode: 1718, duration: 0.210s, episode steps: 33, steps per second: 157, episode reward: 13.627, mean reward: 0.413 [0.352, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.957, 10.100], loss: 0.044473, mae: 0.102381, mean_q: 0.620434
 36319/100000: episode: 1719, duration: 0.219s, episode steps: 34, steps per second: 155, episode reward: 9.380, mean reward: 0.276 [0.009, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.048, 10.102], loss: 0.006682, mae: 0.090826, mean_q: 0.628433
 36352/100000: episode: 1720, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 11.392, mean reward: 0.345 [0.141, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.435, 10.100], loss: 0.043443, mae: 0.078787, mean_q: 0.635835
 36384/100000: episode: 1721, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 13.947, mean reward: 0.436 [0.254, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.497, 10.100], loss: 0.046650, mae: 0.102163, mean_q: 0.653770
 36415/100000: episode: 1722, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 11.188, mean reward: 0.361 [0.269, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.306, 10.100], loss: 0.045784, mae: 0.090122, mean_q: 0.634097
 36427/100000: episode: 1723, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.987, mean reward: 0.416 [0.298, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.362, 10.442], loss: 0.004460, mae: 0.075329, mean_q: 0.653053
 36461/100000: episode: 1724, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 15.750, mean reward: 0.463 [0.331, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.553, 10.100], loss: 0.003793, mae: 0.067063, mean_q: 0.630332
 36473/100000: episode: 1725, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 5.859, mean reward: 0.488 [0.324, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.506], loss: 0.004246, mae: 0.071486, mean_q: 0.623185
 36507/100000: episode: 1726, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 15.970, mean reward: 0.470 [0.370, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.900, 10.100], loss: 0.003633, mae: 0.066314, mean_q: 0.637095
 36538/100000: episode: 1727, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 12.460, mean reward: 0.402 [0.185, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.146, 10.100], loss: 0.045659, mae: 0.079198, mean_q: 0.653676
[RESULT] FALSIFICATION!
 36542/100000: episode: 1728, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 11.739, mean reward: 2.935 [0.491, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.015, 10.601], loss: 0.008561, mae: 0.105301, mean_q: 0.579613
 36575/100000: episode: 1729, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 9.510, mean reward: 0.288 [0.101, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.681, 10.100], loss: 0.004518, mae: 0.074365, mean_q: 0.630730
 36609/100000: episode: 1730, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 12.783, mean reward: 0.376 [0.200, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.172, 10.100], loss: 0.042029, mae: 0.085430, mean_q: 0.648239
 36640/100000: episode: 1731, duration: 0.190s, episode steps: 31, steps per second: 163, episode reward: 11.697, mean reward: 0.377 [0.276, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.326, 10.100], loss: 0.085593, mae: 0.112514, mean_q: 0.685926
 36673/100000: episode: 1732, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 12.591, mean reward: 0.382 [0.212, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.346, 10.100], loss: 0.044757, mae: 0.095872, mean_q: 0.637928
 36705/100000: episode: 1733, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 12.348, mean reward: 0.386 [0.223, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.358, 10.100], loss: 0.004233, mae: 0.071416, mean_q: 0.656751
 36737/100000: episode: 1734, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 15.116, mean reward: 0.472 [0.339, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.922, 10.100], loss: 0.004191, mae: 0.070934, mean_q: 0.662079
 36769/100000: episode: 1735, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 17.667, mean reward: 0.552 [0.477, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.399, 10.100], loss: 0.120481, mae: 0.121940, mean_q: 0.686225
 36800/100000: episode: 1736, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 10.410, mean reward: 0.336 [0.172, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.264, 10.100], loss: 0.005262, mae: 0.079791, mean_q: 0.661271
 36831/100000: episode: 1737, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 10.880, mean reward: 0.351 [0.178, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.390, 10.100], loss: 0.004566, mae: 0.073190, mean_q: 0.656036
 36865/100000: episode: 1738, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 12.627, mean reward: 0.371 [0.189, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.379, 10.100], loss: 0.004046, mae: 0.068674, mean_q: 0.653625
 36896/100000: episode: 1739, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 9.838, mean reward: 0.317 [0.162, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.298, 10.100], loss: 0.047118, mae: 0.094006, mean_q: 0.666452
 36930/100000: episode: 1740, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 11.487, mean reward: 0.338 [0.144, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.423, 10.100], loss: 0.003904, mae: 0.068744, mean_q: 0.654829
 36961/100000: episode: 1741, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 11.176, mean reward: 0.361 [0.240, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.698, 10.100], loss: 0.003684, mae: 0.066043, mean_q: 0.672321
 36972/100000: episode: 1742, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.130, mean reward: 0.466 [0.390, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.499], loss: 0.003577, mae: 0.064013, mean_q: 0.674555
 36983/100000: episode: 1743, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 5.047, mean reward: 0.459 [0.336, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.462], loss: 0.002988, mae: 0.057549, mean_q: 0.637269
 37015/100000: episode: 1744, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 14.657, mean reward: 0.458 [0.282, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.282, 10.100], loss: 0.083375, mae: 0.101017, mean_q: 0.681686
 37046/100000: episode: 1745, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 15.473, mean reward: 0.499 [0.411, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.900, 10.100], loss: 0.004819, mae: 0.076864, mean_q: 0.676919
 37079/100000: episode: 1746, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 9.697, mean reward: 0.294 [0.103, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.758, 10.100], loss: 0.044000, mae: 0.098091, mean_q: 0.700697
 37113/100000: episode: 1747, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 14.468, mean reward: 0.426 [0.262, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.239, 10.100], loss: 0.041088, mae: 0.090449, mean_q: 0.687758
 37144/100000: episode: 1748, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 12.119, mean reward: 0.391 [0.179, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.339, 10.100], loss: 0.166265, mae: 0.128975, mean_q: 0.704291
 37178/100000: episode: 1749, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 12.184, mean reward: 0.358 [0.155, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.379, 10.100], loss: 0.005175, mae: 0.078045, mean_q: 0.667304
 37212/100000: episode: 1750, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 12.018, mean reward: 0.353 [0.085, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.714, 10.100], loss: 0.003833, mae: 0.067956, mean_q: 0.671740
 37244/100000: episode: 1751, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 11.910, mean reward: 0.372 [0.212, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.295, 10.100], loss: 0.121597, mae: 0.100379, mean_q: 0.701459
 37278/100000: episode: 1752, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 15.093, mean reward: 0.444 [0.353, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.509, 10.100], loss: 0.007383, mae: 0.089987, mean_q: 0.693007
 37309/100000: episode: 1753, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 8.425, mean reward: 0.272 [0.113, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.564, 10.100], loss: 0.084932, mae: 0.090326, mean_q: 0.699049
 37343/100000: episode: 1754, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 10.015, mean reward: 0.295 [0.092, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.307, 10.100], loss: 0.004826, mae: 0.076862, mean_q: 0.692681
 37374/100000: episode: 1755, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 11.279, mean reward: 0.364 [0.246, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.218, 10.100], loss: 0.003689, mae: 0.065907, mean_q: 0.687834
 37385/100000: episode: 1756, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 6.166, mean reward: 0.561 [0.435, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-1.243, 10.640], loss: 0.004621, mae: 0.071550, mean_q: 0.666718
 37397/100000: episode: 1757, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 5.927, mean reward: 0.494 [0.451, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.595], loss: 0.003659, mae: 0.064776, mean_q: 0.694889
 37431/100000: episode: 1758, duration: 0.212s, episode steps: 34, steps per second: 161, episode reward: 7.569, mean reward: 0.223 [0.061, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.248, 10.100], loss: 0.041035, mae: 0.080940, mean_q: 0.699816
 37462/100000: episode: 1759, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 11.629, mean reward: 0.375 [0.232, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.134, 10.100], loss: 0.003396, mae: 0.063111, mean_q: 0.693446
 37474/100000: episode: 1760, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 6.333, mean reward: 0.528 [0.510, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.626], loss: 0.003336, mae: 0.062958, mean_q: 0.700228
 37486/100000: episode: 1761, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 6.506, mean reward: 0.542 [0.512, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.953, 10.580], loss: 0.003884, mae: 0.067639, mean_q: 0.692856
 37498/100000: episode: 1762, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 6.480, mean reward: 0.540 [0.468, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.590], loss: 0.004486, mae: 0.069312, mean_q: 0.691417
 37510/100000: episode: 1763, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 6.263, mean reward: 0.522 [0.451, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-1.164, 10.452], loss: 0.106803, mae: 0.086411, mean_q: 0.704256
 37521/100000: episode: 1764, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 6.001, mean reward: 0.546 [0.495, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.036, 10.614], loss: 0.006583, mae: 0.088222, mean_q: 0.709521
 37555/100000: episode: 1765, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 13.774, mean reward: 0.405 [0.231, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-1.320, 10.100], loss: 0.003625, mae: 0.064501, mean_q: 0.708978
 37567/100000: episode: 1766, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 6.052, mean reward: 0.504 [0.452, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.046, 10.652], loss: 0.003708, mae: 0.065694, mean_q: 0.722104
 37601/100000: episode: 1767, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 10.877, mean reward: 0.320 [0.217, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.282, 10.100], loss: 0.076106, mae: 0.087384, mean_q: 0.701244
[Info] New level: 1.1771385669708252 | Considering 10/90 traces
 37635/100000: episode: 1768, duration: 4.154s, episode steps: 34, steps per second: 8, episode reward: 10.710, mean reward: 0.315 [0.201, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.312, 10.100], loss: 0.042991, mae: 0.086383, mean_q: 0.715917
 37666/100000: episode: 1769, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 13.537, mean reward: 0.437 [0.214, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.940, 10.100], loss: 0.003686, mae: 0.065599, mean_q: 0.711656
 37676/100000: episode: 1770, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 5.057, mean reward: 0.506 [0.451, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.464], loss: 0.379807, mae: 0.239049, mean_q: 0.833581
 37682/100000: episode: 1771, duration: 0.045s, episode steps: 6, steps per second: 135, episode reward: 3.370, mean reward: 0.562 [0.545, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.636], loss: 0.014735, mae: 0.138782, mean_q: 0.653575
 37691/100000: episode: 1772, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 5.562, mean reward: 0.618 [0.582, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.181, 10.558], loss: 0.147467, mae: 0.144385, mean_q: 0.746259
 37701/100000: episode: 1773, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.690, mean reward: 0.469 [0.416, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.564], loss: 0.005713, mae: 0.083678, mean_q: 0.761701
 37711/100000: episode: 1774, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 5.586, mean reward: 0.559 [0.509, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.536, 10.596], loss: 0.004442, mae: 0.071897, mean_q: 0.695688
 37720/100000: episode: 1775, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 5.529, mean reward: 0.614 [0.563, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.564], loss: 0.003475, mae: 0.062771, mean_q: 0.735236
 37731/100000: episode: 1776, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 6.168, mean reward: 0.561 [0.465, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.756], loss: 0.117816, mae: 0.097796, mean_q: 0.751462
 37740/100000: episode: 1777, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 5.258, mean reward: 0.584 [0.546, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.608], loss: 0.147156, mae: 0.130763, mean_q: 0.759725
 37747/100000: episode: 1778, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 3.267, mean reward: 0.467 [0.410, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.455], loss: 0.005055, mae: 0.077228, mean_q: 0.709382
 37756/100000: episode: 1779, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 5.873, mean reward: 0.653 [0.632, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.692], loss: 0.004578, mae: 0.071091, mean_q: 0.699658
 37762/100000: episode: 1780, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 2.627, mean reward: 0.438 [0.333, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.509], loss: 0.204771, mae: 0.124685, mean_q: 0.781712
 37772/100000: episode: 1781, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 5.650, mean reward: 0.565 [0.492, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-1.157, 10.573], loss: 0.127047, mae: 0.116470, mean_q: 0.754304
 37781/100000: episode: 1782, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 5.616, mean reward: 0.624 [0.601, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.650], loss: 0.138221, mae: 0.108231, mean_q: 0.758850
[RESULT] FALSIFICATION!
 37782/100000: episode: 1783, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.012, 10.573], loss: 0.003285, mae: 0.070817, mean_q: 0.732110
 37793/100000: episode: 1784, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 5.906, mean reward: 0.537 [0.465, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.687], loss: 0.006096, mae: 0.077680, mean_q: 0.728379
 37803/100000: episode: 1785, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 5.101, mean reward: 0.510 [0.455, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.611], loss: 0.005020, mae: 0.071173, mean_q: 0.716125
 37809/100000: episode: 1786, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 3.224, mean reward: 0.537 [0.515, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.253, 10.582], loss: 0.004348, mae: 0.067431, mean_q: 0.734107
[RESULT] FALSIFICATION!
 37810/100000: episode: 1787, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.014, 10.595], loss: 0.002239, mae: 0.049430, mean_q: 0.733289
 37817/100000: episode: 1788, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 3.586, mean reward: 0.512 [0.469, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.623], loss: 0.004078, mae: 0.071120, mean_q: 0.735661
[RESULT] FALSIFICATION!
 37824/100000: episode: 1789, duration: 0.055s, episode steps: 7, steps per second: 127, episode reward: 13.591, mean reward: 1.942 [0.503, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.668], loss: 0.003583, mae: 0.064706, mean_q: 0.740981
 37831/100000: episode: 1790, duration: 0.047s, episode steps: 7, steps per second: 151, episode reward: 3.467, mean reward: 0.495 [0.459, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.575], loss: 0.003409, mae: 0.061537, mean_q: 0.710596
 37838/100000: episode: 1791, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 4.129, mean reward: 0.590 [0.552, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.914, 10.674], loss: 0.003141, mae: 0.062114, mean_q: 0.713971
 37847/100000: episode: 1792, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.929, mean reward: 0.548 [0.511, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.669], loss: 0.003538, mae: 0.062741, mean_q: 0.743332
 37857/100000: episode: 1793, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 5.128, mean reward: 0.513 [0.487, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.544], loss: 0.003333, mae: 0.062622, mean_q: 0.693475
 37889/100000: episode: 1794, duration: 0.199s, episode steps: 32, steps per second: 161, episode reward: 13.499, mean reward: 0.422 [0.337, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.348, 10.100], loss: 0.003340, mae: 0.063260, mean_q: 0.737584
 37898/100000: episode: 1795, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 5.821, mean reward: 0.647 [0.584, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.347, 10.708], loss: 0.003964, mae: 0.065511, mean_q: 0.727435
 37929/100000: episode: 1796, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 11.498, mean reward: 0.371 [0.117, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.711, 10.100], loss: 0.003567, mae: 0.063758, mean_q: 0.733346
 37961/100000: episode: 1797, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 9.812, mean reward: 0.307 [0.021, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.100], loss: 0.119721, mae: 0.109307, mean_q: 0.767033
 37967/100000: episode: 1798, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 3.379, mean reward: 0.563 [0.521, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.067, 10.557], loss: 0.011618, mae: 0.124644, mean_q: 0.693007
 37998/100000: episode: 1799, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 13.399, mean reward: 0.432 [0.216, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.044, 10.100], loss: 0.164620, mae: 0.149205, mean_q: 0.787044
 38009/100000: episode: 1800, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 6.807, mean reward: 0.619 [0.551, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.534], loss: 0.004585, mae: 0.074951, mean_q: 0.757462
 38020/100000: episode: 1801, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 6.238, mean reward: 0.567 [0.500, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.526], loss: 0.003799, mae: 0.069438, mean_q: 0.748807
 38030/100000: episode: 1802, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 5.607, mean reward: 0.561 [0.513, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.737], loss: 0.003577, mae: 0.062010, mean_q: 0.734993
[RESULT] FALSIFICATION!
 38031/100000: episode: 1803, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.015, 10.553], loss: 0.002014, mae: 0.050465, mean_q: 0.737514
 38038/100000: episode: 1804, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 3.856, mean reward: 0.551 [0.488, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.653], loss: 0.003668, mae: 0.067266, mean_q: 0.769749
 38044/100000: episode: 1805, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 3.858, mean reward: 0.643 [0.623, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.661], loss: 0.003840, mae: 0.068016, mean_q: 0.743350
[RESULT] FALSIFICATION!
 38050/100000: episode: 1806, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 13.088, mean reward: 2.181 [0.579, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.340, 10.556], loss: 0.203299, mae: 0.120630, mean_q: 0.798170
 38057/100000: episode: 1807, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 3.249, mean reward: 0.464 [0.390, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.040, 10.561], loss: 0.004000, mae: 0.071916, mean_q: 0.742973
 38067/100000: episode: 1808, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 5.686, mean reward: 0.569 [0.507, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.644], loss: 0.124762, mae: 0.097134, mean_q: 0.760153
 38076/100000: episode: 1809, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 5.066, mean reward: 0.563 [0.444, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.632], loss: 0.008777, mae: 0.089745, mean_q: 0.764151
 38108/100000: episode: 1810, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 12.635, mean reward: 0.395 [0.217, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.196, 10.100], loss: 0.081875, mae: 0.098432, mean_q: 0.781490
 38114/100000: episode: 1811, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 3.677, mean reward: 0.613 [0.591, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.151, 10.614], loss: 0.005203, mae: 0.082822, mean_q: 0.736970
 38124/100000: episode: 1812, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 5.518, mean reward: 0.552 [0.449, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.321, 10.603], loss: 0.129439, mae: 0.095029, mean_q: 0.749842
 38131/100000: episode: 1813, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 3.587, mean reward: 0.512 [0.476, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.569], loss: 0.176188, mae: 0.136976, mean_q: 0.824556
 38162/100000: episode: 1814, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 13.088, mean reward: 0.422 [0.271, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.326, 10.100], loss: 0.044720, mae: 0.097608, mean_q: 0.767455
 38168/100000: episode: 1815, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 3.470, mean reward: 0.578 [0.525, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-1.139, 10.611], loss: 0.005275, mae: 0.077277, mean_q: 0.751994
 38200/100000: episode: 1816, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 13.332, mean reward: 0.417 [0.299, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.578, 10.100], loss: 0.044048, mae: 0.082813, mean_q: 0.768874
 38209/100000: episode: 1817, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 5.758, mean reward: 0.640 [0.574, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.636], loss: 0.274039, mae: 0.154293, mean_q: 0.812596
 38241/100000: episode: 1818, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 11.486, mean reward: 0.359 [0.204, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.697, 10.100], loss: 0.044263, mae: 0.087979, mean_q: 0.769481
 38248/100000: episode: 1819, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.706, mean reward: 0.529 [0.492, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.657], loss: 0.005112, mae: 0.076846, mean_q: 0.786785
 38255/100000: episode: 1820, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 3.840, mean reward: 0.549 [0.507, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.255, 10.523], loss: 0.007126, mae: 0.082993, mean_q: 0.793641
[RESULT] FALSIFICATION!
 38261/100000: episode: 1821, duration: 0.045s, episode steps: 6, steps per second: 133, episode reward: 13.258, mean reward: 2.210 [0.604, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.065, 10.735], loss: 0.198814, mae: 0.109916, mean_q: 0.750789
 38268/100000: episode: 1822, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 3.431, mean reward: 0.490 [0.419, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.505], loss: 0.013138, mae: 0.111640, mean_q: 0.802682
 38278/100000: episode: 1823, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 5.240, mean reward: 0.524 [0.472, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.580], loss: 0.005391, mae: 0.080458, mean_q: 0.766909
 38309/100000: episode: 1824, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 13.835, mean reward: 0.446 [0.203, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.239, 10.100], loss: 0.081802, mae: 0.089724, mean_q: 0.782282
 38316/100000: episode: 1825, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 3.680, mean reward: 0.526 [0.494, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.515], loss: 0.336197, mae: 0.169962, mean_q: 0.853802
 38323/100000: episode: 1826, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 3.653, mean reward: 0.522 [0.446, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.601], loss: 0.031682, mae: 0.183586, mean_q: 0.891959
 38334/100000: episode: 1827, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 6.153, mean reward: 0.559 [0.462, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.268, 10.578], loss: 0.010072, mae: 0.113257, mean_q: 0.746297
[RESULT] FALSIFICATION!
 38335/100000: episode: 1828, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-0.012, 7.880], loss: 1.208623, mae: 0.352380, mean_q: 0.813579
 38346/100000: episode: 1829, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 6.123, mean reward: 0.557 [0.477, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.204, 10.574], loss: 0.010466, mae: 0.104710, mean_q: 0.814319
[RESULT] FALSIFICATION!
 38347/100000: episode: 1830, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.698 [-0.011, 7.880], loss: 0.001960, mae: 0.047568, mean_q: 0.805860
 38354/100000: episode: 1831, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 3.913, mean reward: 0.559 [0.519, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.530], loss: 0.175566, mae: 0.126464, mean_q: 0.761846
 38386/100000: episode: 1832, duration: 0.179s, episode steps: 32, steps per second: 178, episode reward: 14.581, mean reward: 0.456 [0.283, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.525, 10.100], loss: 0.158600, mae: 0.141001, mean_q: 0.815172
 38417/100000: episode: 1833, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 12.864, mean reward: 0.415 [0.234, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.616, 10.100], loss: 0.162414, mae: 0.143275, mean_q: 0.816167
 38424/100000: episode: 1834, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 3.422, mean reward: 0.489 [0.452, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.662, 10.531], loss: 0.342379, mae: 0.194239, mean_q: 0.862040
 38455/100000: episode: 1835, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 14.947, mean reward: 0.482 [0.210, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.159, 10.100], loss: 0.193890, mae: 0.158430, mean_q: 0.832386
 38466/100000: episode: 1836, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 6.430, mean reward: 0.585 [0.549, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-1.111, 10.669], loss: 0.107698, mae: 0.127998, mean_q: 0.814196
 38475/100000: episode: 1837, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 5.312, mean reward: 0.590 [0.508, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.961, 10.403], loss: 0.007181, mae: 0.093323, mean_q: 0.745738
 38485/100000: episode: 1838, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 4.567, mean reward: 0.457 [0.322, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.481], loss: 0.005106, mae: 0.074738, mean_q: 0.788800
[RESULT] FALSIFICATION!
 38488/100000: episode: 1839, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 11.311, mean reward: 3.770 [0.637, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.013, 10.575], loss: 0.005187, mae: 0.077008, mean_q: 0.779408
[RESULT] FALSIFICATION!
 38493/100000: episode: 1840, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 12.541, mean reward: 2.508 [0.624, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.211, 8.502], loss: 0.004709, mae: 0.073031, mean_q: 0.776765
 38502/100000: episode: 1841, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 5.668, mean reward: 0.630 [0.584, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.161, 10.708], loss: 0.136667, mae: 0.116647, mean_q: 0.805448
[RESULT] FALSIFICATION!
 38507/100000: episode: 1842, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 12.667, mean reward: 2.533 [0.640, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.384, 10.375], loss: 0.004298, mae: 0.070886, mean_q: 0.790822
 38517/100000: episode: 1843, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 5.663, mean reward: 0.566 [0.512, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.697, 10.574], loss: 0.124856, mae: 0.099360, mean_q: 0.810142
 38548/100000: episode: 1844, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 13.995, mean reward: 0.451 [0.342, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.356, 10.100], loss: 0.168473, mae: 0.144028, mean_q: 0.859283
 38555/100000: episode: 1845, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 4.035, mean reward: 0.576 [0.469, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.503, 10.676], loss: 0.014379, mae: 0.141506, mean_q: 0.706867
 38562/100000: episode: 1846, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 4.324, mean reward: 0.618 [0.554, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.657], loss: 0.006906, mae: 0.089775, mean_q: 0.830987
 38573/100000: episode: 1847, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 7.184, mean reward: 0.653 [0.537, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.648], loss: 0.111506, mae: 0.105338, mean_q: 0.799746
[RESULT] FALSIFICATION!
 38576/100000: episode: 1848, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 11.327, mean reward: 3.776 [0.642, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.014, 10.767], loss: 0.005433, mae: 0.084769, mean_q: 0.819884
 38608/100000: episode: 1849, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 15.601, mean reward: 0.488 [0.353, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.465, 10.100], loss: 0.153893, mae: 0.119083, mean_q: 0.835419
 38617/100000: episode: 1850, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 5.493, mean reward: 0.610 [0.567, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.569], loss: 0.135458, mae: 0.107537, mean_q: 0.787445
 38623/100000: episode: 1851, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 3.427, mean reward: 0.571 [0.518, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.679], loss: 0.197695, mae: 0.134123, mean_q: 0.818364
[RESULT] FALSIFICATION!
 38625/100000: episode: 1852, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 10.655, mean reward: 5.328 [0.655, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.014, 10.528], loss: 0.005856, mae: 0.079716, mean_q: 0.777147
[RESULT] FALSIFICATION!
 38628/100000: episode: 1853, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 11.251, mean reward: 3.750 [0.602, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.609, 10.274], loss: 0.008981, mae: 0.088767, mean_q: 0.805203
[RESULT] FALSIFICATION!
 38635/100000: episode: 1854, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 13.156, mean reward: 1.879 [0.462, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.218, 10.818], loss: 0.004137, mae: 0.063902, mean_q: 0.840084
 38667/100000: episode: 1855, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 13.617, mean reward: 0.426 [0.263, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.165, 10.100], loss: 0.123575, mae: 0.131544, mean_q: 0.820508
[RESULT] FALSIFICATION!
 38668/100000: episode: 1856, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.013, 10.489], loss: 0.006511, mae: 0.087023, mean_q: 0.723761
 38675/100000: episode: 1857, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 3.762, mean reward: 0.537 [0.493, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.508], loss: 0.329667, mae: 0.180231, mean_q: 0.895645
[Info] New level: 1.5610249042510986 | Considering 10/90 traces
 38685/100000: episode: 1858, duration: 3.944s, episode steps: 10, steps per second: 3, episode reward: 5.016, mean reward: 0.502 [0.466, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.558], loss: 0.251427, mae: 0.171833, mean_q: 0.850127
[RESULT] FALSIFICATION!
 38689/100000: episode: 1859, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 11.975, mean reward: 2.994 [0.605, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.012, 10.510], loss: 0.007012, mae: 0.082930, mean_q: 0.821688
 38698/100000: episode: 1860, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 5.069, mean reward: 0.563 [0.480, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.582], loss: 0.398029, mae: 0.200173, mean_q: 0.876310
[RESULT] FALSIFICATION!
 38704/100000: episode: 1861, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.309, mean reward: 2.218 [0.634, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.015, 10.663], loss: 0.196279, mae: 0.158881, mean_q: 0.854930
[RESULT] FALSIFICATION!
 38708/100000: episode: 1862, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.988, mean reward: 2.997 [0.618, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.075, 10.325], loss: 0.007418, mae: 0.089607, mean_q: 0.818594
 38717/100000: episode: 1863, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 5.236, mean reward: 0.582 [0.473, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.627], loss: 0.007163, mae: 0.084129, mean_q: 0.800432
[RESULT] FALSIFICATION!
 38719/100000: episode: 1864, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 10.662, mean reward: 5.331 [0.662, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.013, 10.471], loss: 0.006333, mae: 0.079417, mean_q: 0.795953
 38728/100000: episode: 1865, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 5.409, mean reward: 0.601 [0.556, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.621], loss: 0.135649, mae: 0.121369, mean_q: 0.869904
 38736/100000: episode: 1866, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 5.049, mean reward: 0.631 [0.588, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.731], loss: 0.291528, mae: 0.148543, mean_q: 0.845241
 38744/100000: episode: 1867, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 4.866, mean reward: 0.608 [0.569, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.613], loss: 0.450597, mae: 0.252395, mean_q: 0.953239
[RESULT] FALSIFICATION!
 38746/100000: episode: 1868, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 10.644, mean reward: 5.322 [0.644, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.065, 10.100], loss: 0.007963, mae: 0.090950, mean_q: 0.879341
[RESULT] FALSIFICATION!
 38748/100000: episode: 1869, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.017, 10.669], loss: 0.586603, mae: 0.234308, mean_q: 0.744502
 38755/100000: episode: 1870, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 3.804, mean reward: 0.543 [0.469, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.549], loss: 0.005497, mae: 0.077944, mean_q: 0.789856
 38762/100000: episode: 1871, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.577, mean reward: 0.654 [0.635, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.664], loss: 0.005418, mae: 0.076740, mean_q: 0.829907
[RESULT] FALSIFICATION!
 38764/100000: episode: 1872, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 10.697, mean reward: 5.349 [0.697, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.011, 10.342], loss: 0.004049, mae: 0.069681, mean_q: 0.769407
[RESULT] FALSIFICATION!
 38767/100000: episode: 1873, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 11.386, mean reward: 3.795 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.037, 10.536], loss: 0.005552, mae: 0.086134, mean_q: 0.889982
 38775/100000: episode: 1874, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.893, mean reward: 0.612 [0.578, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.668], loss: 0.004681, mae: 0.073930, mean_q: 0.815679
 38782/100000: episode: 1875, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.546, mean reward: 0.649 [0.618, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.651], loss: 0.348372, mae: 0.185971, mean_q: 0.909948
 38791/100000: episode: 1876, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 5.599, mean reward: 0.622 [0.544, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.666], loss: 0.135442, mae: 0.135677, mean_q: 0.793864
 38798/100000: episode: 1877, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 4.062, mean reward: 0.580 [0.532, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.134, 10.614], loss: 0.007569, mae: 0.088510, mean_q: 0.867709
[RESULT] FALSIFICATION!
 38799/100000: episode: 1878, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.455, 10.253], loss: 0.004257, mae: 0.070590, mean_q: 0.748667
 38808/100000: episode: 1879, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 6.034, mean reward: 0.670 [0.639, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.606], loss: 0.651385, mae: 0.281381, mean_q: 0.910493
[RESULT] FALSIFICATION!
 38811/100000: episode: 1880, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 11.322, mean reward: 3.774 [0.661, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.013, 10.670], loss: 0.083505, mae: 0.272980, mean_q: 0.993537
[RESULT] FALSIFICATION!
 38813/100000: episode: 1881, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.683, mean reward: 5.342 [0.683, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.012, 10.368], loss: 0.047538, mae: 0.212533, mean_q: 0.973505
 38821/100000: episode: 1882, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 5.173, mean reward: 0.647 [0.602, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.692], loss: 0.307747, mae: 0.187864, mean_q: 0.790193
 38829/100000: episode: 1883, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 4.913, mean reward: 0.614 [0.576, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-1.771, 10.671], loss: 0.430142, mae: 0.191950, mean_q: 0.852713
 38837/100000: episode: 1884, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 5.057, mean reward: 0.632 [0.581, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.698], loss: 0.298568, mae: 0.290481, mean_q: 0.987791
[RESULT] FALSIFICATION!
 38838/100000: episode: 1885, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.011, 10.262], loss: 0.012828, mae: 0.124867, mean_q: 0.907127
 38848/100000: episode: 1886, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 5.945, mean reward: 0.594 [0.545, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.535, 10.690], loss: 0.364575, mae: 0.177356, mean_q: 0.809922
[RESULT] FALSIFICATION!
 38853/100000: episode: 1887, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 12.577, mean reward: 2.515 [0.601, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.011, 10.802], loss: 0.224990, mae: 0.187908, mean_q: 0.945066
 38861/100000: episode: 1888, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 4.619, mean reward: 0.577 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.633], loss: 0.287737, mae: 0.257369, mean_q: 0.904552
[RESULT] FALSIFICATION!
 38863/100000: episode: 1889, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.699, mean reward: 5.349 [0.699, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.013, 10.666], loss: 0.567052, mae: 0.226254, mean_q: 0.863585
[RESULT] FALSIFICATION!
 38866/100000: episode: 1890, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 11.379, mean reward: 3.793 [0.680, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.011, 10.455], loss: 0.369309, mae: 0.178729, mean_q: 0.918904
 38873/100000: episode: 1891, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.344, mean reward: 0.621 [0.577, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.693], loss: 0.177234, mae: 0.182605, mean_q: 0.908483
[RESULT] FALSIFICATION!
 38876/100000: episode: 1892, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 11.367, mean reward: 3.789 [0.676, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.016, 10.615], loss: 0.039011, mae: 0.192864, mean_q: 0.866933
 38885/100000: episode: 1893, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 5.335, mean reward: 0.593 [0.496, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.578], loss: 0.252573, mae: 0.160570, mean_q: 0.847436
 38894/100000: episode: 1894, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 5.797, mean reward: 0.644 [0.609, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.376, 10.464], loss: 0.008595, mae: 0.094819, mean_q: 0.813530
 38904/100000: episode: 1895, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 5.791, mean reward: 0.579 [0.540, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.677], loss: 0.227399, mae: 0.138276, mean_q: 0.867600
 38911/100000: episode: 1896, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.052, mean reward: 0.579 [0.544, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.616], loss: 0.314837, mae: 0.191856, mean_q: 0.868459
[RESULT] FALSIFICATION!
 38917/100000: episode: 1897, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 13.179, mean reward: 2.196 [0.597, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.012, 10.520], loss: 0.197145, mae: 0.157573, mean_q: 0.888717
 38926/100000: episode: 1898, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 5.858, mean reward: 0.651 [0.630, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.661], loss: 0.132234, mae: 0.145816, mean_q: 0.895888
[RESULT] FALSIFICATION!
 38928/100000: episode: 1899, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.661, mean reward: 5.331 [0.661, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.014, 10.645], loss: 0.012289, mae: 0.130041, mean_q: 0.732937
 38935/100000: episode: 1900, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 3.998, mean reward: 0.571 [0.547, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.573], loss: 0.339228, mae: 0.219504, mean_q: 0.946399
 38944/100000: episode: 1901, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.614, mean reward: 0.513 [0.413, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.472], loss: 0.497712, mae: 0.264424, mean_q: 0.889145
 38952/100000: episode: 1902, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.707, mean reward: 0.588 [0.497, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.517], loss: 0.169998, mae: 0.194370, mean_q: 0.935384
 38962/100000: episode: 1903, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 4.837, mean reward: 0.484 [0.411, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.448], loss: 0.349034, mae: 0.226344, mean_q: 0.941952
 38970/100000: episode: 1904, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 5.167, mean reward: 0.646 [0.611, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.246, 10.791], loss: 0.281858, mae: 0.194682, mean_q: 0.882359
 38979/100000: episode: 1905, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 5.573, mean reward: 0.619 [0.575, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.632], loss: 0.371380, mae: 0.223154, mean_q: 0.944010
 38987/100000: episode: 1906, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.534, mean reward: 0.567 [0.500, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.593], loss: 0.423812, mae: 0.218824, mean_q: 0.928918
[RESULT] FALSIFICATION!
 38988/100000: episode: 1907, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.011, 10.154], loss: 1.065448, mae: 0.361898, mean_q: 0.900375
 38998/100000: episode: 1908, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 5.952, mean reward: 0.595 [0.548, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.688], loss: 0.552893, mae: 0.300046, mean_q: 0.975779
 39008/100000: episode: 1909, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 5.996, mean reward: 0.600 [0.527, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.750], loss: 0.351769, mae: 0.224865, mean_q: 0.954383
 39015/100000: episode: 1910, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 4.381, mean reward: 0.626 [0.606, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.700], loss: 0.320757, mae: 0.202569, mean_q: 0.922008
 39024/100000: episode: 1911, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 5.664, mean reward: 0.629 [0.566, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.720], loss: 0.149496, mae: 0.143443, mean_q: 0.840444
 39032/100000: episode: 1912, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 5.238, mean reward: 0.655 [0.615, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.431, 10.592], loss: 0.289673, mae: 0.216504, mean_q: 1.021580
 39042/100000: episode: 1913, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 6.187, mean reward: 0.619 [0.578, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.622], loss: 0.121162, mae: 0.150630, mean_q: 0.891049
 39051/100000: episode: 1914, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 4.989, mean reward: 0.554 [0.497, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.582], loss: 0.258456, mae: 0.148101, mean_q: 0.895793
 39058/100000: episode: 1915, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.000, mean reward: 0.571 [0.547, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.363, 10.696], loss: 0.310435, mae: 0.178524, mean_q: 0.881753
[RESULT] FALSIFICATION!
 39064/100000: episode: 1916, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.400, mean reward: 2.233 [0.642, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.013, 10.521], loss: 0.015911, mae: 0.122550, mean_q: 0.916913
[RESULT] FALSIFICATION!
 39065/100000: episode: 1917, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.011, 10.311], loss: 0.006214, mae: 0.077865, mean_q: 0.836384
 39072/100000: episode: 1918, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 4.507, mean reward: 0.644 [0.606, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.639], loss: 0.333962, mae: 0.161251, mean_q: 0.890561
 39079/100000: episode: 1919, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 4.311, mean reward: 0.616 [0.586, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.272, 10.640], loss: 0.165782, mae: 0.147927, mean_q: 0.883587
 39086/100000: episode: 1920, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 4.349, mean reward: 0.621 [0.551, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.475], loss: 0.010840, mae: 0.106461, mean_q: 0.940078
[RESULT] FALSIFICATION!
 39089/100000: episode: 1921, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 11.285, mean reward: 3.762 [0.614, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.709, 10.329], loss: 1.137885, mae: 0.344776, mean_q: 0.895151
 39099/100000: episode: 1922, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 5.491, mean reward: 0.549 [0.494, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.589], loss: 0.670205, mae: 0.380121, mean_q: 1.085463
 39108/100000: episode: 1923, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 5.286, mean reward: 0.587 [0.558, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.509, 10.643], loss: 0.539728, mae: 0.302501, mean_q: 0.913139
[RESULT] FALSIFICATION!
 39110/100000: episode: 1924, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 10.685, mean reward: 5.343 [0.685, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.014, 10.500], loss: 0.049144, mae: 0.277773, mean_q: 1.045375
 39119/100000: episode: 1925, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 5.581, mean reward: 0.620 [0.570, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.638], loss: 0.263199, mae: 0.199451, mean_q: 0.817055
[RESULT] FALSIFICATION!
 39126/100000: episode: 1926, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 13.993, mean reward: 1.999 [0.651, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.013, 10.710], loss: 0.652566, mae: 0.305368, mean_q: 1.006069
[RESULT] FALSIFICATION!
 39133/100000: episode: 1927, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 13.841, mean reward: 1.977 [0.593, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.406, 10.582], loss: 0.371684, mae: 0.273628, mean_q: 1.005441
 39142/100000: episode: 1928, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 5.543, mean reward: 0.616 [0.565, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.639, 10.632], loss: 0.265966, mae: 0.167572, mean_q: 0.904079
[RESULT] FALSIFICATION!
 39143/100000: episode: 1929, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.295, 10.116], loss: 0.004004, mae: 0.067672, mean_q: 0.887483
 39150/100000: episode: 1930, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 3.803, mean reward: 0.543 [0.508, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.964, 10.611], loss: 0.661965, mae: 0.322234, mean_q: 1.003077
[RESULT] FALSIFICATION!
 39153/100000: episode: 1931, duration: 0.027s, episode steps: 3, steps per second: 111, episode reward: 11.288, mean reward: 3.763 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.013, 10.698], loss: 0.409192, mae: 0.241786, mean_q: 0.947999
[RESULT] FALSIFICATION!
 39157/100000: episode: 1932, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.912, mean reward: 2.978 [0.622, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.522, 10.431], loss: 0.037657, mae: 0.211525, mean_q: 0.908464
 39164/100000: episode: 1933, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 4.281, mean reward: 0.612 [0.589, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.506], loss: 0.485706, mae: 0.248533, mean_q: 0.938923
 39172/100000: episode: 1934, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 5.042, mean reward: 0.630 [0.600, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.556], loss: 0.283673, mae: 0.193769, mean_q: 0.968907
[RESULT] FALSIFICATION!
 39173/100000: episode: 1935, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.012, 10.412], loss: 0.019512, mae: 0.132001, mean_q: 0.929506
 39183/100000: episode: 1936, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 6.038, mean reward: 0.604 [0.565, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.118, 10.719], loss: 0.805898, mae: 0.392810, mean_q: 1.070235
[RESULT] FALSIFICATION!
 39185/100000: episode: 1937, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 10.649, mean reward: 5.324 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.011, 10.259], loss: 0.521367, mae: 0.283532, mean_q: 0.919040
 39193/100000: episode: 1938, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 4.919, mean reward: 0.615 [0.562, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.704], loss: 0.547560, mae: 0.310999, mean_q: 0.956124
[RESULT] FALSIFICATION!
 39196/100000: episode: 1939, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 11.323, mean reward: 3.774 [0.652, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.016, 10.553], loss: 0.011930, mae: 0.105105, mean_q: 0.924828
 39203/100000: episode: 1940, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 4.383, mean reward: 0.626 [0.597, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.627, 10.536], loss: 0.328783, mae: 0.186502, mean_q: 0.962129
 39210/100000: episode: 1941, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 4.209, mean reward: 0.601 [0.575, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-1.589, 10.603], loss: 0.321424, mae: 0.194056, mean_q: 0.972820
[RESULT] FALSIFICATION!
 39214/100000: episode: 1942, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.991, mean reward: 2.998 [0.642, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.013, 10.787], loss: 0.529823, mae: 0.253719, mean_q: 1.011200
 39221/100000: episode: 1943, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 4.223, mean reward: 0.603 [0.568, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.627], loss: 0.780310, mae: 0.335640, mean_q: 1.015003
[RESULT] FALSIFICATION!
 39228/100000: episode: 1944, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 13.827, mean reward: 1.975 [0.612, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.387, 10.621], loss: 0.495407, mae: 0.296706, mean_q: 1.035262
[RESULT] FALSIFICATION!
 39231/100000: episode: 1945, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 11.346, mean reward: 3.782 [0.661, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.012, 10.484], loss: 0.754035, mae: 0.343807, mean_q: 1.013505
 39239/100000: episode: 1946, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.777, mean reward: 0.597 [0.551, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.724], loss: 0.552953, mae: 0.280768, mean_q: 0.990104
[RESULT] FALSIFICATION!
 39240/100000: episode: 1947, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.011, 10.311], loss: 0.019146, mae: 0.126976, mean_q: 0.888077
[Info] New level: 1.8599680662155151 | Considering 10/90 traces
 39249/100000: episode: 1948, duration: 3.971s, episode steps: 9, steps per second: 2, episode reward: 5.303, mean reward: 0.589 [0.529, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.597], loss: 0.248898, mae: 0.172694, mean_q: 0.922701
[RESULT] FALSIFICATION!
 39250/100000: episode: 1949, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.010, 10.337], loss: 0.004797, mae: 0.074756, mean_q: 0.903185
 39257/100000: episode: 1950, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 4.766, mean reward: 0.681 [0.657, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.100, 10.648], loss: 0.472153, mae: 0.221808, mean_q: 0.931212
 39264/100000: episode: 1951, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.226, mean reward: 0.604 [0.573, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.658], loss: 0.624961, mae: 0.316499, mean_q: 1.002780
[RESULT] FALSIFICATION!
 39266/100000: episode: 1952, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 10.683, mean reward: 5.342 [0.683, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.010, 10.478], loss: 0.038640, mae: 0.156905, mean_q: 0.941388
 39273/100000: episode: 1953, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 4.528, mean reward: 0.647 [0.594, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.671], loss: 0.182581, mae: 0.186130, mean_q: 0.969731
[RESULT] FALSIFICATION!
 39274/100000: episode: 1954, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.010, 10.216], loss: 0.007305, mae: 0.095795, mean_q: 1.015001
[RESULT] FALSIFICATION!
 39275/100000: episode: 1955, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.012, 10.408], loss: 0.004558, mae: 0.082537, mean_q: 0.781435
[RESULT] FALSIFICATION!
 39276/100000: episode: 1956, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.011, 10.354], loss: 0.013402, mae: 0.130263, mean_q: 0.830115
 39282/100000: episode: 1957, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.660, mean reward: 0.610 [0.581, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.547], loss: 0.790127, mae: 0.285094, mean_q: 0.860826
[RESULT] FALSIFICATION!
 39287/100000: episode: 1958, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 12.700, mean reward: 2.540 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.416], loss: 0.259998, mae: 0.396424, mean_q: 1.218040
[RESULT] FALSIFICATION!
 39293/100000: episode: 1959, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.176, mean reward: 2.196 [0.605, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.014, 10.597], loss: 0.738582, mae: 0.409189, mean_q: 1.006322
[RESULT] FALSIFICATION!
 39295/100000: episode: 1960, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.685, mean reward: 5.343 [0.685, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.012, 10.583], loss: 1.662172, mae: 0.582250, mean_q: 1.013335
[RESULT] FALSIFICATION!
 39297/100000: episode: 1961, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 10.696, mean reward: 5.348 [0.696, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.014, 10.576], loss: 0.043110, mae: 0.243003, mean_q: 1.178739
[RESULT] FALSIFICATION!
 39299/100000: episode: 1962, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.696, mean reward: 5.348 [0.696, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.010, 10.463], loss: 0.560622, mae: 0.346056, mean_q: 1.128546
[RESULT] FALSIFICATION!
 39302/100000: episode: 1963, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 11.363, mean reward: 3.788 [0.664, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.098, 10.466], loss: 0.357796, mae: 0.211936, mean_q: 0.909749
[RESULT] FALSIFICATION!
 39303/100000: episode: 1964, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.009, 10.244], loss: 0.027629, mae: 0.159981, mean_q: 0.946903
[RESULT] FALSIFICATION!
 39306/100000: episode: 1965, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 11.333, mean reward: 3.778 [0.660, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.010, 10.716], loss: 0.751065, mae: 0.308976, mean_q: 0.812027
 39312/100000: episode: 1966, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.690, mean reward: 0.615 [0.592, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.610], loss: 0.349446, mae: 0.235400, mean_q: 1.026272
[RESULT] FALSIFICATION!
 39313/100000: episode: 1967, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.011, 10.292], loss: 0.031319, mae: 0.186222, mean_q: 1.070248
 39321/100000: episode: 1968, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 4.886, mean reward: 0.611 [0.567, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.625], loss: 0.423135, mae: 0.262514, mean_q: 0.975691
[RESULT] FALSIFICATION!
 39322/100000: episode: 1969, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.010, 10.423], loss: 0.030987, mae: 0.190097, mean_q: 1.062597
[RESULT] FALSIFICATION!
 39323/100000: episode: 1970, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.011, 10.240], loss: 1.254721, mae: 0.416468, mean_q: 1.085597
[RESULT] FALSIFICATION!
 39324/100000: episode: 1971, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.008, 10.369], loss: 2.239282, mae: 0.696753, mean_q: 1.082394
[RESULT] FALSIFICATION!
 39325/100000: episode: 1972, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.011, 10.543], loss: 0.054933, mae: 0.289802, mean_q: 1.144289
[RESULT] FALSIFICATION!
 39328/100000: episode: 1973, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 11.355, mean reward: 3.785 [0.665, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.308, 10.420], loss: 0.050049, mae: 0.253414, mean_q: 1.138503
[RESULT] FALSIFICATION!
 39329/100000: episode: 1974, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.013, 10.535], loss: 0.040621, mae: 0.165587, mean_q: 0.927548
[RESULT] FALSIFICATION!
 39334/100000: episode: 1975, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 12.757, mean reward: 2.551 [0.671, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.011, 10.543], loss: 1.095074, mae: 0.431965, mean_q: 0.916254
[RESULT] FALSIFICATION!
 39336/100000: episode: 1976, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.654, mean reward: 5.327 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.010, 10.426], loss: 0.538062, mae: 0.413333, mean_q: 1.107052
[RESULT] FALSIFICATION!
 39338/100000: episode: 1977, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.661, mean reward: 5.330 [0.661, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.011, 10.513], loss: 0.979754, mae: 0.485905, mean_q: 1.127067
[RESULT] FALSIFICATION!
 39343/100000: episode: 1978, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 12.610, mean reward: 2.522 [0.632, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.015, 10.748], loss: 0.277216, mae: 0.320448, mean_q: 1.166229
[RESULT] FALSIFICATION!
 39344/100000: episode: 1979, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.009, 10.529], loss: 1.031919, mae: 0.361520, mean_q: 0.923600
[RESULT] FALSIFICATION!
 39345/100000: episode: 1980, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.011, 10.348], loss: 0.028150, mae: 0.133723, mean_q: 0.875592
 39352/100000: episode: 1981, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 4.192, mean reward: 0.599 [0.526, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.433], loss: 0.666381, mae: 0.282398, mean_q: 0.934425
[RESULT] FALSIFICATION!
 39353/100000: episode: 1982, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.010, 10.185], loss: 0.017002, mae: 0.160762, mean_q: 1.053777
[RESULT] FALSIFICATION!
 39354/100000: episode: 1983, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.009, 10.202], loss: 0.014869, mae: 0.136866, mean_q: 1.010461
[RESULT] FALSIFICATION!
 39357/100000: episode: 1984, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 11.287, mean reward: 3.762 [0.632, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.011, 10.411], loss: 1.054494, mae: 0.416114, mean_q: 1.046159
 39365/100000: episode: 1985, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 4.939, mean reward: 0.617 [0.559, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.574], loss: 0.916470, mae: 0.513338, mean_q: 1.163045
 39371/100000: episode: 1986, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 3.907, mean reward: 0.651 [0.615, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.751], loss: 1.010840, mae: 0.584846, mean_q: 1.340420
[RESULT] FALSIFICATION!
 39372/100000: episode: 1987, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.011, 10.286], loss: 1.851821, mae: 0.668586, mean_q: 1.075264
[RESULT] FALSIFICATION!
 39376/100000: episode: 1988, duration: 0.026s, episode steps: 4, steps per second: 156, episode reward: 12.054, mean reward: 3.013 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.758], loss: 1.651562, mae: 0.617122, mean_q: 1.020090
[RESULT] FALSIFICATION!
 39377/100000: episode: 1989, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.010, 10.475], loss: 0.861553, mae: 0.501572, mean_q: 1.157202
[RESULT] FALSIFICATION!
 39383/100000: episode: 1990, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 13.336, mean reward: 2.223 [0.642, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.013, 10.677], loss: 0.298542, mae: 0.399602, mean_q: 1.278651
[RESULT] FALSIFICATION!
 39384/100000: episode: 1991, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.009, 10.319], loss: 1.046739, mae: 0.395802, mean_q: 0.972882
 39391/100000: episode: 1992, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 4.380, mean reward: 0.626 [0.615, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.159, 10.580], loss: 1.416359, mae: 0.576998, mean_q: 1.096583
[RESULT] FALSIFICATION!
 39393/100000: episode: 1993, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 10.662, mean reward: 5.331 [0.662, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.013, 10.506], loss: 0.608816, mae: 0.596951, mean_q: 1.486548
[RESULT] FALSIFICATION!
 39394/100000: episode: 1994, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.011, 10.494], loss: 0.171913, mae: 0.415553, mean_q: 1.414292
 39401/100000: episode: 1995, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.598, mean reward: 0.657 [0.606, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.651], loss: 0.355119, mae: 0.356744, mean_q: 0.939582
[RESULT] FALSIFICATION!
 39402/100000: episode: 1996, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.013, 10.480], loss: 0.006563, mae: 0.086736, mean_q: 0.900095
[RESULT] FALSIFICATION!
 39403/100000: episode: 1997, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.011, 10.298], loss: 1.000433, mae: 0.374826, mean_q: 1.012978
 39411/100000: episode: 1998, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 4.787, mean reward: 0.598 [0.543, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-1.693, 10.717], loss: 0.438179, mae: 0.392380, mean_q: 1.273953
[RESULT] FALSIFICATION!
 39412/100000: episode: 1999, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.013, 10.491], loss: 0.051385, mae: 0.260031, mean_q: 1.103043
[RESULT] FALSIFICATION!
 39414/100000: episode: 2000, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.682, mean reward: 5.341 [0.682, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.013, 10.603], loss: 0.642661, mae: 0.368674, mean_q: 1.050940
[RESULT] FALSIFICATION!
 39415/100000: episode: 2001, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.083, 10.129], loss: 0.012271, mae: 0.123807, mean_q: 0.859756
 39422/100000: episode: 2002, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 4.342, mean reward: 0.620 [0.591, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.539], loss: 0.817391, mae: 0.313911, mean_q: 1.050264
[RESULT] FALSIFICATION!
 39423/100000: episode: 2003, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.011, 10.238], loss: 0.036542, mae: 0.247655, mean_q: 1.070284
[RESULT] FALSIFICATION!
 39424/100000: episode: 2004, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.009, 10.120], loss: 0.026346, mae: 0.201580, mean_q: 1.156773
[RESULT] FALSIFICATION!
 39425/100000: episode: 2005, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.011, 10.390], loss: 1.046671, mae: 0.419378, mean_q: 1.105281
[RESULT] FALSIFICATION!
 39426/100000: episode: 2006, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.012, 10.585], loss: 0.023775, mae: 0.171953, mean_q: 1.121394
 39433/100000: episode: 2007, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 4.170, mean reward: 0.596 [0.542, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.636], loss: 0.744194, mae: 0.416133, mean_q: 1.075412
[RESULT] FALSIFICATION!
 39437/100000: episode: 2008, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 12.052, mean reward: 3.013 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.048, 10.571], loss: 0.295654, mae: 0.321355, mean_q: 1.123267
[RESULT] FALSIFICATION!
 39439/100000: episode: 2009, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.699, mean reward: 5.349 [0.699, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.084, 10.262], loss: 1.701419, mae: 0.576458, mean_q: 1.201075
[RESULT] FALSIFICATION!
 39441/100000: episode: 2010, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 10.699, mean reward: 5.349 [0.699, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.011, 10.486], loss: 0.022907, mae: 0.164226, mean_q: 1.019831
[RESULT] FALSIFICATION!
 39443/100000: episode: 2011, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 10.691, mean reward: 5.346 [0.691, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.359], loss: 0.483422, mae: 0.307214, mean_q: 1.133817
 39450/100000: episode: 2012, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.103, mean reward: 0.586 [0.526, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.595], loss: 0.845028, mae: 0.424611, mean_q: 1.154616
 39458/100000: episode: 2013, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 4.705, mean reward: 0.588 [0.509, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.525], loss: 0.873267, mae: 0.491548, mean_q: 1.157516
[RESULT] FALSIFICATION!
 39462/100000: episode: 2014, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 11.981, mean reward: 2.995 [0.646, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.011, 10.706], loss: 0.473436, mae: 0.393426, mean_q: 1.215315
[RESULT] FALSIFICATION!
 39465/100000: episode: 2015, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 11.383, mean reward: 3.794 [0.689, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.014, 10.597], loss: 0.675557, mae: 0.329802, mean_q: 1.029351
[RESULT] FALSIFICATION!
 39468/100000: episode: 2016, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 11.368, mean reward: 3.789 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.012, 10.607], loss: 0.685463, mae: 0.286420, mean_q: 0.868720
 39475/100000: episode: 2017, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 4.607, mean reward: 0.658 [0.612, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.665], loss: 0.434599, mae: 0.249789, mean_q: 0.994766
[RESULT] FALSIFICATION!
 39479/100000: episode: 2018, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 12.022, mean reward: 3.005 [0.658, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.015, 10.612], loss: 0.502397, mae: 0.340758, mean_q: 1.177560
 39486/100000: episode: 2019, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.515, mean reward: 0.645 [0.594, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.557, 10.695], loss: 0.469510, mae: 0.320264, mean_q: 1.073446
[RESULT] FALSIFICATION!
 39488/100000: episode: 2020, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.686, mean reward: 5.343 [0.686, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.016, 10.629], loss: 1.008853, mae: 0.362902, mean_q: 0.912480
 39495/100000: episode: 2021, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 4.452, mean reward: 0.636 [0.585, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.036, 10.585], loss: 0.769123, mae: 0.431786, mean_q: 1.155808
[RESULT] FALSIFICATION!
 39496/100000: episode: 2022, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.011, 10.262], loss: 3.327995, mae: 1.347086, mean_q: 1.759575
[RESULT] FALSIFICATION!
 39497/100000: episode: 2023, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.015, 10.614], loss: 0.244410, mae: 0.457323, mean_q: 1.413203
 39505/100000: episode: 2024, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.984, mean reward: 0.623 [0.577, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.633], loss: 0.993039, mae: 0.573706, mean_q: 1.275845
[RESULT] FALSIFICATION!
 39510/100000: episode: 2025, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 12.727, mean reward: 2.545 [0.675, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.678], loss: 0.596479, mae: 0.297554, mean_q: 1.058392
[RESULT] FALSIFICATION!
 39511/100000: episode: 2026, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.012, 10.207], loss: 2.127293, mae: 0.627629, mean_q: 1.060870
 39517/100000: episode: 2027, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 3.721, mean reward: 0.620 [0.575, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.035, 10.715], loss: 0.373938, mae: 0.298044, mean_q: 1.160114
 39524/100000: episode: 2028, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 4.540, mean reward: 0.649 [0.611, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.707], loss: 0.719880, mae: 0.388491, mean_q: 1.104716
[RESULT] FALSIFICATION!
 39525/100000: episode: 2029, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.012, 10.361], loss: 0.864227, mae: 0.447635, mean_q: 1.226466
 39532/100000: episode: 2030, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.624, mean reward: 0.661 [0.623, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.756], loss: 0.321216, mae: 0.326572, mean_q: 1.196827
[RESULT] FALSIFICATION!
 39537/100000: episode: 2031, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 12.654, mean reward: 2.531 [0.656, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.659], loss: 0.828726, mae: 0.380010, mean_q: 1.056136
[RESULT] FALSIFICATION!
 39538/100000: episode: 2032, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.011, 10.359], loss: 1.810624, mae: 0.597519, mean_q: 1.177394
[RESULT] FALSIFICATION!
 39540/100000: episode: 2033, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.692, mean reward: 5.346 [0.692, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.012, 10.424], loss: 0.502381, mae: 0.405841, mean_q: 1.290725
[RESULT] FALSIFICATION!
 39541/100000: episode: 2034, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.011, 10.554], loss: 0.888039, mae: 0.568820, mean_q: 1.344268
[RESULT] FALSIFICATION!
 39542/100000: episode: 2035, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.009, 10.238], loss: 1.040249, mae: 0.784281, mean_q: 1.505614
 39547/100000: episode: 2036, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 3.388, mean reward: 0.678 [0.651, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.698], loss: 0.435386, mae: 0.392295, mean_q: 1.201184
[RESULT] FALSIFICATION!
 39549/100000: episode: 2037, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.673, mean reward: 5.336 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.012, 10.581], loss: 1.055259, mae: 0.473803, mean_q: 1.056299
[RESULT] FALSIFICATION!
[Info] New level: 2.695786714553833 | Considering 15/85 traces
 39550/100000: episode: 2038, duration: 3.847s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.014, 10.528], loss: 1.143334, mae: 0.466489, mean_q: 0.954345
[RESULT] FALSIFICATION!
 39551/100000: episode: 2039, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.012, 10.493], loss: 0.904844, mae: 0.399654, mean_q: 1.229500
[RESULT] FALSIFICATION!
 39552/100000: episode: 2040, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.013, 10.531], loss: 1.951730, mae: 0.753508, mean_q: 1.217706
[RESULT] FALSIFICATION!
 39553/100000: episode: 2041, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.011, 10.321], loss: 0.130007, mae: 0.418265, mean_q: 1.409930
[RESULT] FALSIFICATION!
 39554/100000: episode: 2042, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.013, 10.389], loss: 2.813569, mae: 1.192426, mean_q: 1.628864
[RESULT] FALSIFICATION!
 39557/100000: episode: 2043, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.389, mean reward: 3.796 [0.693, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.012, 10.431], loss: 0.646201, mae: 0.470754, mean_q: 1.294955
[RESULT] FALSIFICATION!
 39559/100000: episode: 2044, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.697, mean reward: 5.348 [0.697, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.014, 10.361], loss: 0.654204, mae: 0.418084, mean_q: 1.094713
[RESULT] FALSIFICATION!
 39561/100000: episode: 2045, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.674, mean reward: 5.337 [0.674, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.036, 10.355], loss: 0.921135, mae: 0.514088, mean_q: 1.029103
[RESULT] FALSIFICATION!
 39564/100000: episode: 2046, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 11.324, mean reward: 3.775 [0.658, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.015, 10.519], loss: 0.647943, mae: 0.334374, mean_q: 1.003643
[RESULT] FALSIFICATION!
 39568/100000: episode: 2047, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 12.018, mean reward: 3.005 [0.668, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.013, 10.649], loss: 0.529631, mae: 0.336256, mean_q: 1.189239
[RESULT] FALSIFICATION!
 39570/100000: episode: 2048, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.678, mean reward: 5.339 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.015, 10.682], loss: 0.075253, mae: 0.273777, mean_q: 1.286544
[RESULT] FALSIFICATION!
 39571/100000: episode: 2049, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.011, 10.435], loss: 0.044230, mae: 0.203035, mean_q: 1.155518
[RESULT] FALSIFICATION!
 39574/100000: episode: 2050, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 11.319, mean reward: 3.773 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.014, 10.493], loss: 0.859618, mae: 0.413767, mean_q: 1.086276
[RESULT] FALSIFICATION!
 39575/100000: episode: 2051, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.013, 10.638], loss: 1.779479, mae: 0.611253, mean_q: 1.031966
[RESULT] FALSIFICATION!
 39576/100000: episode: 2052, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.012, 10.384], loss: 1.662055, mae: 0.673517, mean_q: 1.226932
[RESULT] FALSIFICATION!
 39577/100000: episode: 2053, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.013, 10.357], loss: 0.949175, mae: 0.615977, mean_q: 1.430961
[RESULT] FALSIFICATION!
 39578/100000: episode: 2054, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.011, 10.246], loss: 1.329149, mae: 0.793839, mean_q: 1.604379
[RESULT] FALSIFICATION!
 39579/100000: episode: 2055, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.012, 10.516], loss: 0.165840, mae: 0.379066, mean_q: 1.316081
[RESULT] FALSIFICATION!
 39580/100000: episode: 2056, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.011, 10.320], loss: 0.854806, mae: 0.468921, mean_q: 1.161247
[RESULT] FALSIFICATION!
 39581/100000: episode: 2057, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.010, 10.263], loss: 0.047549, mae: 0.207411, mean_q: 1.134978
[RESULT] FALSIFICATION!
 39584/100000: episode: 2058, duration: 0.020s, episode steps: 3, steps per second: 154, episode reward: 11.385, mean reward: 3.795 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.014, 10.706], loss: 0.020912, mae: 0.161574, mean_q: 0.841783
[RESULT] FALSIFICATION!
 39585/100000: episode: 2059, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.012, 10.274], loss: 0.014329, mae: 0.118181, mean_q: 0.808846
[RESULT] FALSIFICATION!
 39586/100000: episode: 2060, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.011, 10.442], loss: 0.008544, mae: 0.103106, mean_q: 0.849511
[RESULT] FALSIFICATION!
 39588/100000: episode: 2061, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.666, mean reward: 5.333 [0.666, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.011, 10.376], loss: 0.528321, mae: 0.254458, mean_q: 1.019672
[RESULT] FALSIFICATION!
 39589/100000: episode: 2062, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.491, 10.290], loss: 0.955873, mae: 0.414472, mean_q: 1.180852
[RESULT] FALSIFICATION!
 39590/100000: episode: 2063, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.013, 10.565], loss: 0.906065, mae: 0.450495, mean_q: 1.085050
[RESULT] FALSIFICATION!
 39593/100000: episode: 2064, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.350, mean reward: 3.783 [0.660, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.014, 10.554], loss: 0.694558, mae: 0.608842, mean_q: 1.495010
[RESULT] FALSIFICATION!
 39598/100000: episode: 2065, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 12.640, mean reward: 2.528 [0.623, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.013, 10.766], loss: 1.410839, mae: 0.779467, mean_q: 1.412708
[RESULT] FALSIFICATION!
 39606/100000: episode: 2066, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 14.429, mean reward: 1.804 [0.606, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.116, 10.579], loss: 1.196060, mae: 0.614744, mean_q: 1.240383
[RESULT] FALSIFICATION!
 39607/100000: episode: 2067, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.011, 10.492], loss: 0.120746, mae: 0.372157, mean_q: 1.370359
[RESULT] FALSIFICATION!
 39608/100000: episode: 2068, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.011, 10.374], loss: 1.767888, mae: 0.775425, mean_q: 1.365089
[RESULT] FALSIFICATION!
 39609/100000: episode: 2069, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.013, 10.507], loss: 0.870905, mae: 0.673469, mean_q: 1.529956
[RESULT] FALSIFICATION!
 39610/100000: episode: 2070, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.011, 10.299], loss: 2.529612, mae: 1.100134, mean_q: 1.565830
[RESULT] FALSIFICATION!
 39611/100000: episode: 2071, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.011, 10.300], loss: 0.264803, mae: 0.490233, mean_q: 1.430105
[RESULT] FALSIFICATION!
 39612/100000: episode: 2072, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.012, 10.433], loss: 0.175367, mae: 0.409675, mean_q: 1.265404
[RESULT] FALSIFICATION!
 39613/100000: episode: 2073, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.010, 10.304], loss: 0.899912, mae: 0.564676, mean_q: 1.154111
[RESULT] FALSIFICATION!
 39614/100000: episode: 2074, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.011, 10.290], loss: 0.883553, mae: 0.710510, mean_q: 1.678381
[RESULT] FALSIFICATION!
 39615/100000: episode: 2075, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.013, 10.253], loss: 0.901660, mae: 0.601034, mean_q: 1.324336
[RESULT] FALSIFICATION!
 39616/100000: episode: 2076, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.012, 10.293], loss: 2.027054, mae: 0.784426, mean_q: 1.231809
[RESULT] FALSIFICATION!
 39617/100000: episode: 2077, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.011, 10.369], loss: 1.598294, mae: 0.802256, mean_q: 1.489583
[RESULT] FALSIFICATION!
 39619/100000: episode: 2078, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 10.690, mean reward: 5.345 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.011, 10.411], loss: 1.331252, mae: 0.905847, mean_q: 1.786343
 39627/100000: episode: 2079, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 5.114, mean reward: 0.639 [0.591, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.196, 10.660], loss: 0.687012, mae: 0.480377, mean_q: 1.270603
[RESULT] FALSIFICATION!
 39628/100000: episode: 2080, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.309], loss: 0.903176, mae: 0.435893, mean_q: 1.131191
[RESULT] FALSIFICATION!
 39629/100000: episode: 2081, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.012, 10.321], loss: 1.804178, mae: 0.659277, mean_q: 1.136147
[RESULT] FALSIFICATION!
 39630/100000: episode: 2082, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.373, 10.145], loss: 0.962348, mae: 0.399247, mean_q: 1.219792
[RESULT] FALSIFICATION!
 39632/100000: episode: 2083, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 10.649, mean reward: 5.324 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.010, 10.343], loss: 0.039916, mae: 0.207380, mean_q: 1.079443
[RESULT] FALSIFICATION!
 39633/100000: episode: 2084, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.011, 10.448], loss: 1.020711, mae: 0.482429, mean_q: 1.188248
[RESULT] FALSIFICATION!
 39634/100000: episode: 2085, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.011, 10.388], loss: 0.863417, mae: 0.496157, mean_q: 1.276284
[RESULT] FALSIFICATION!
 39635/100000: episode: 2086, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.012, 10.336], loss: 1.658715, mae: 0.697622, mean_q: 1.379497
[RESULT] FALSIFICATION!
 39636/100000: episode: 2087, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.010, 10.363], loss: 1.649349, mae: 0.796327, mean_q: 1.443979
[RESULT] FALSIFICATION!
 39638/100000: episode: 2088, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.011, 10.493], loss: 2.945903, mae: 1.096786, mean_q: 1.407145
[RESULT] FALSIFICATION!
 39639/100000: episode: 2089, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.011, 10.378], loss: 1.434327, mae: 0.731024, mean_q: 1.269213
[RESULT] FALSIFICATION!
 39640/100000: episode: 2090, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.012, 10.464], loss: 2.282132, mae: 1.127398, mean_q: 1.710493
[RESULT] FALSIFICATION!
 39641/100000: episode: 2091, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.011, 10.457], loss: 3.532609, mae: 1.661117, mean_q: 2.273257
 39649/100000: episode: 2092, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 5.084, mean reward: 0.636 [0.594, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.672], loss: 1.053192, mae: 0.650926, mean_q: 1.392162
[RESULT] FALSIFICATION!
 39651/100000: episode: 2093, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 10.678, mean reward: 5.339 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.013, 10.587], loss: 1.765182, mae: 0.678766, mean_q: 1.209769
[RESULT] FALSIFICATION!
 39653/100000: episode: 2094, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.679, mean reward: 5.340 [0.679, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.012, 10.493], loss: 0.935991, mae: 0.382481, mean_q: 0.990438
[RESULT] FALSIFICATION!
 39656/100000: episode: 2095, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 11.329, mean reward: 3.776 [0.657, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.013, 10.492], loss: 0.602113, mae: 0.423915, mean_q: 1.419980
[RESULT] FALSIFICATION!
 39657/100000: episode: 2096, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.012, 10.509], loss: 0.042209, mae: 0.189902, mean_q: 1.191103
[RESULT] FALSIFICATION!
 39659/100000: episode: 2097, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 10.697, mean reward: 5.349 [0.697, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.192, 10.414], loss: 0.823473, mae: 0.490228, mean_q: 1.421344
[RESULT] FALSIFICATION!
 39661/100000: episode: 2098, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 10.672, mean reward: 5.336 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.012, 10.618], loss: 1.577904, mae: 0.636828, mean_q: 1.158299
[RESULT] FALSIFICATION!
 39662/100000: episode: 2099, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.011, 10.403], loss: 0.168127, mae: 0.379465, mean_q: 1.372266
[RESULT] FALSIFICATION!
 39663/100000: episode: 2100, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.013, 10.375], loss: 0.763669, mae: 0.553754, mean_q: 1.453440
[RESULT] FALSIFICATION!
 39664/100000: episode: 2101, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.012, 10.355], loss: 3.543532, mae: 1.374125, mean_q: 1.632103
 39672/100000: episode: 2102, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 4.951, mean reward: 0.619 [0.521, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.679, 10.550], loss: 1.037643, mae: 0.730060, mean_q: 1.599627
[RESULT] FALSIFICATION!
 39673/100000: episode: 2103, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.012, 10.273], loss: 1.489954, mae: 0.687072, mean_q: 1.283813
[RESULT] FALSIFICATION!
 39674/100000: episode: 2104, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.010, 10.276], loss: 2.775599, mae: 0.893909, mean_q: 0.998200
[RESULT] FALSIFICATION!
 39675/100000: episode: 2105, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.011, 10.375], loss: 1.350548, mae: 0.696647, mean_q: 1.448984
[RESULT] FALSIFICATION!
 39677/100000: episode: 2106, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 10.679, mean reward: 5.339 [0.679, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.013, 10.525], loss: 0.262141, mae: 0.514478, mean_q: 1.446687
[RESULT] FALSIFICATION!
 39678/100000: episode: 2107, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.803, 10.149], loss: 0.819359, mae: 0.638119, mean_q: 1.470220
[RESULT] FALSIFICATION!
 39679/100000: episode: 2108, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.010, 10.310], loss: 0.751232, mae: 0.500953, mean_q: 1.429642
[RESULT] FALSIFICATION!
 39680/100000: episode: 2109, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.013, 10.320], loss: 0.919792, mae: 0.590668, mean_q: 1.229936
 39688/100000: episode: 2110, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 5.061, mean reward: 0.633 [0.552, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.598], loss: 1.417388, mae: 0.700407, mean_q: 1.337863
[RESULT] FALSIFICATION!
 39689/100000: episode: 2111, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.012, 10.362], loss: 0.394294, mae: 0.554466, mean_q: 1.528393
[RESULT] FALSIFICATION!
 39694/100000: episode: 2112, duration: 0.032s, episode steps: 5, steps per second: 159, episode reward: 12.658, mean reward: 2.532 [0.627, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.014, 10.702], loss: 0.713439, mae: 0.523166, mean_q: 1.297958
[RESULT] FALSIFICATION!
 39696/100000: episode: 2113, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 10.672, mean reward: 5.336 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.422, 10.452], loss: 1.721955, mae: 0.662762, mean_q: 1.128810
[RESULT] FALSIFICATION!
 39697/100000: episode: 2114, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.791, 10.277], loss: 0.743899, mae: 0.390507, mean_q: 1.073230
[RESULT] FALSIFICATION!
 39698/100000: episode: 2115, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.011, 10.378], loss: 1.516613, mae: 0.699876, mean_q: 1.305835
[RESULT] FALSIFICATION!
 39699/100000: episode: 2116, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.112, 10.294], loss: 0.294330, mae: 0.584902, mean_q: 1.794232
 39707/100000: episode: 2117, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 5.304, mean reward: 0.663 [0.623, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.703], loss: 1.029013, mae: 0.554615, mean_q: 1.367043
[RESULT] FALSIFICATION!
 39708/100000: episode: 2118, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.482], loss: 0.033705, mae: 0.154957, mean_q: 0.979186
[RESULT] FALSIFICATION!
 39709/100000: episode: 2119, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.010, 10.283], loss: 0.788724, mae: 0.481688, mean_q: 1.299536
[RESULT] FALSIFICATION!
 39710/100000: episode: 2120, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.013, 10.468], loss: 0.892591, mae: 0.422477, mean_q: 1.257148
[RESULT] FALSIFICATION!
 39712/100000: episode: 2121, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 10.675, mean reward: 5.337 [0.675, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.015, 10.663], loss: 1.716907, mae: 0.670686, mean_q: 1.212679
 39720/100000: episode: 2122, duration: 0.057s, episode steps: 8, steps per second: 139, episode reward: 4.842, mean reward: 0.605 [0.561, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.275, 10.593], loss: 1.219816, mae: 0.779947, mean_q: 1.596699
[RESULT] FALSIFICATION!
[Info] New level: 4.1249918937683105 | Considering 13/87 traces
 39721/100000: episode: 2123, duration: 3.816s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.508, 10.266], loss: 0.322210, mae: 0.540895, mean_q: 1.498333
[RESULT] FALSIFICATION!
 39722/100000: episode: 2124, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.013, 10.507], loss: 1.330883, mae: 0.658813, mean_q: 1.215290
[RESULT] FALSIFICATION!
 39723/100000: episode: 2125, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.012, 10.383], loss: 0.871652, mae: 0.547279, mean_q: 1.339823
[RESULT] FALSIFICATION!
 39729/100000: episode: 2126, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 13.277, mean reward: 2.213 [0.625, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.012, 10.754], loss: 1.355596, mae: 0.655189, mean_q: 1.437909
[RESULT] FALSIFICATION!
 39730/100000: episode: 2127, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.013, 10.598], loss: 0.726842, mae: 0.514717, mean_q: 1.417605
[RESULT] FALSIFICATION!
 39731/100000: episode: 2128, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.013, 10.596], loss: 1.602413, mae: 0.820585, mean_q: 1.496707
[RESULT] FALSIFICATION!
 39732/100000: episode: 2129, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.013, 10.424], loss: 3.585060, mae: 1.472142, mean_q: 1.879625
[RESULT] FALSIFICATION!
 39733/100000: episode: 2130, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.012, 10.508], loss: 0.305795, mae: 0.454723, mean_q: 1.280509
[RESULT] FALSIFICATION!
 39736/100000: episode: 2131, duration: 0.027s, episode steps: 3, steps per second: 113, episode reward: 11.305, mean reward: 3.768 [0.639, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.011, 10.579], loss: 1.438296, mae: 0.883331, mean_q: 1.695447
[RESULT] FALSIFICATION!
 39737/100000: episode: 2132, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.012, 10.508], loss: 0.819963, mae: 0.659272, mean_q: 1.499153
[RESULT] FALSIFICATION!
 39738/100000: episode: 2133, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.014, 10.494], loss: 0.624539, mae: 0.395084, mean_q: 1.041367
[RESULT] FALSIFICATION!
 39740/100000: episode: 2134, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.682, mean reward: 5.341 [0.682, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.014, 10.542], loss: 0.831282, mae: 0.597939, mean_q: 1.492136
[RESULT] FALSIFICATION!
 39741/100000: episode: 2135, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.014, 10.658], loss: 0.056681, mae: 0.237109, mean_q: 1.041835
[RESULT] FALSIFICATION!
 39742/100000: episode: 2136, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.013, 10.532], loss: 0.078611, mae: 0.259219, mean_q: 1.252257
[RESULT] FALSIFICATION!
 39743/100000: episode: 2137, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.500], loss: 0.190593, mae: 0.394104, mean_q: 1.272480
[RESULT] FALSIFICATION!
 39744/100000: episode: 2138, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.012, 10.513], loss: 1.403504, mae: 0.835596, mean_q: 1.744867
[RESULT] FALSIFICATION!
 39745/100000: episode: 2139, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.013, 10.535], loss: 1.167979, mae: 0.632784, mean_q: 1.496536
[RESULT] FALSIFICATION!
 39746/100000: episode: 2140, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.013, 10.485], loss: 3.941453, mae: 1.454362, mean_q: 1.746892
[RESULT] FALSIFICATION!
 39747/100000: episode: 2141, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.012, 10.611], loss: 0.892675, mae: 0.721314, mean_q: 1.633375
[RESULT] FALSIFICATION!
 39749/100000: episode: 2142, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 10.652, mean reward: 5.326 [0.652, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.013, 10.478], loss: 0.143169, mae: 0.316901, mean_q: 1.286719
[RESULT] FALSIFICATION!
 39750/100000: episode: 2143, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.013, 10.581], loss: 2.610956, mae: 0.945159, mean_q: 1.421853
[RESULT] FALSIFICATION!
 39751/100000: episode: 2144, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.010, 10.176], loss: 0.772386, mae: 0.461883, mean_q: 1.348034
[RESULT] FALSIFICATION!
 39752/100000: episode: 2145, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.011, 10.454], loss: 1.207414, mae: 0.574278, mean_q: 1.401690
[RESULT] FALSIFICATION!
 39753/100000: episode: 2146, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.012, 10.337], loss: 1.790630, mae: 0.647000, mean_q: 1.167832
[RESULT] FALSIFICATION!
 39754/100000: episode: 2147, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.402], loss: 0.961990, mae: 0.569866, mean_q: 1.352259
[RESULT] FALSIFICATION!
 39755/100000: episode: 2148, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.013, 10.502], loss: 1.290439, mae: 0.640866, mean_q: 1.341792
[RESULT] FALSIFICATION!
 39756/100000: episode: 2149, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.012, 10.419], loss: 0.828862, mae: 0.459064, mean_q: 1.234221
[RESULT] FALSIFICATION!
 39758/100000: episode: 2150, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.681, mean reward: 5.340 [0.681, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.012, 10.584], loss: 1.822872, mae: 0.771132, mean_q: 1.483346
[RESULT] FALSIFICATION!
 39759/100000: episode: 2151, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.013, 10.507], loss: 0.744632, mae: 0.564705, mean_q: 1.557037
[RESULT] FALSIFICATION!
 39760/100000: episode: 2152, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.013, 10.578], loss: 0.211521, mae: 0.414857, mean_q: 1.386353
[RESULT] FALSIFICATION!
 39761/100000: episode: 2153, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.015, 10.631], loss: 2.127643, mae: 1.027044, mean_q: 1.831496
[RESULT] FALSIFICATION!
 39762/100000: episode: 2154, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.012, 10.449], loss: 1.366202, mae: 0.676060, mean_q: 1.351696
[RESULT] FALSIFICATION!
 39763/100000: episode: 2155, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.012, 10.232], loss: 0.732565, mae: 0.548563, mean_q: 1.411789
[RESULT] FALSIFICATION!
 39764/100000: episode: 2156, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.011, 10.304], loss: 0.145950, mae: 0.351503, mean_q: 1.340338
[RESULT] FALSIFICATION!
 39765/100000: episode: 2157, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.013, 10.464], loss: 0.052552, mae: 0.217325, mean_q: 1.168931
[RESULT] FALSIFICATION!
 39766/100000: episode: 2158, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.012, 10.506], loss: 2.674757, mae: 0.891649, mean_q: 1.379588
[RESULT] FALSIFICATION!
 39767/100000: episode: 2159, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.014, 10.538], loss: 4.300584, mae: 1.421004, mean_q: 1.646171
[RESULT] FALSIFICATION!
 39768/100000: episode: 2160, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.010, 10.197], loss: 0.098872, mae: 0.322224, mean_q: 1.184101
[RESULT] FALSIFICATION!
 39769/100000: episode: 2161, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.014, 10.688], loss: 2.236191, mae: 1.275320, mean_q: 2.237630
[RESULT] FALSIFICATION!
 39770/100000: episode: 2162, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.012, 10.608], loss: 0.703723, mae: 0.496216, mean_q: 1.272333
[RESULT] FALSIFICATION!
 39771/100000: episode: 2163, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.010, 10.258], loss: 0.810598, mae: 0.671788, mean_q: 1.662135
[RESULT] FALSIFICATION!
 39773/100000: episode: 2164, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.684, mean reward: 5.342 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.011, 10.321], loss: 1.854458, mae: 0.979595, mean_q: 1.560652
[RESULT] FALSIFICATION!
 39776/100000: episode: 2165, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 11.350, mean reward: 3.783 [0.650, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.015, 10.673], loss: 1.909314, mae: 0.891875, mean_q: 1.502184
[RESULT] FALSIFICATION!
 39777/100000: episode: 2166, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.012, 10.471], loss: 2.546673, mae: 1.201127, mean_q: 1.859177
[RESULT] FALSIFICATION!
 39778/100000: episode: 2167, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.014, 10.527], loss: 0.561925, mae: 0.425181, mean_q: 1.244837
[RESULT] FALSIFICATION!
 39779/100000: episode: 2168, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.639], loss: 1.822749, mae: 0.896490, mean_q: 1.613623
[RESULT] FALSIFICATION!
 39780/100000: episode: 2169, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.015, 10.578], loss: 0.246828, mae: 0.433876, mean_q: 1.448161
[RESULT] FALSIFICATION!
 39781/100000: episode: 2170, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.013, 10.585], loss: 3.303522, mae: 1.433309, mean_q: 1.961317
[RESULT] FALSIFICATION!
 39782/100000: episode: 2171, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.013, 10.572], loss: 1.837709, mae: 1.070564, mean_q: 1.941607
[RESULT] FALSIFICATION!
 39783/100000: episode: 2172, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.013, 10.512], loss: 1.454453, mae: 0.788781, mean_q: 1.512044
[RESULT] FALSIFICATION!
 39784/100000: episode: 2173, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.012, 10.619], loss: 1.411445, mae: 0.858696, mean_q: 1.786599
[RESULT] FALSIFICATION!
 39785/100000: episode: 2174, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.013, 10.513], loss: 1.049909, mae: 0.786855, mean_q: 1.846136
[RESULT] FALSIFICATION!
 39786/100000: episode: 2175, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.011, 10.495], loss: 0.358797, mae: 0.519967, mean_q: 1.524351
[RESULT] FALSIFICATION!
 39787/100000: episode: 2176, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.012, 10.558], loss: 0.669065, mae: 0.465409, mean_q: 1.218146
[RESULT] FALSIFICATION!
 39790/100000: episode: 2177, duration: 0.019s, episode steps: 3, steps per second: 154, episode reward: 11.316, mean reward: 3.772 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.012, 10.638], loss: 2.069932, mae: 0.900753, mean_q: 1.417965
[RESULT] FALSIFICATION!
 39792/100000: episode: 2178, duration: 0.023s, episode steps: 2, steps per second: 87, episode reward: 10.689, mean reward: 5.345 [0.689, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.012, 10.618], loss: 1.864503, mae: 0.950439, mean_q: 1.849534
[RESULT] FALSIFICATION!
 39794/100000: episode: 2179, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 10.691, mean reward: 5.345 [0.691, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.014, 10.462], loss: 1.027332, mae: 0.840857, mean_q: 1.745690
[RESULT] FALSIFICATION!
 39795/100000: episode: 2180, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.010, 10.348], loss: 1.736500, mae: 1.148898, mean_q: 2.207554
[RESULT] FALSIFICATION!
 39796/100000: episode: 2181, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.014, 10.544], loss: 1.162148, mae: 0.785398, mean_q: 1.626370
[RESULT] FALSIFICATION!
 39797/100000: episode: 2182, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.013, 10.637], loss: 0.282291, mae: 0.475143, mean_q: 1.420852
[RESULT] FALSIFICATION!
 39798/100000: episode: 2183, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.326, 10.189], loss: 1.281937, mae: 0.655435, mean_q: 0.974019
[RESULT] FALSIFICATION!
 39799/100000: episode: 2184, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.013, 10.535], loss: 0.666259, mae: 0.500684, mean_q: 0.887977
[RESULT] FALSIFICATION!
 39800/100000: episode: 2185, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.013, 10.497], loss: 0.706656, mae: 0.455615, mean_q: 1.234152
[RESULT] FALSIFICATION!
 39803/100000: episode: 2186, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 11.384, mean reward: 3.795 [0.686, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.011, 10.429], loss: 1.288903, mae: 0.574632, mean_q: 1.331841
[RESULT] FALSIFICATION!
 39804/100000: episode: 2187, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.319], loss: 0.640345, mae: 0.486515, mean_q: 1.384302
[RESULT] FALSIFICATION!
 39806/100000: episode: 2188, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 10.684, mean reward: 5.342 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.013, 10.614], loss: 1.542989, mae: 0.820624, mean_q: 1.585770
[RESULT] FALSIFICATION!
 39807/100000: episode: 2189, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.013, 10.422], loss: 1.004092, mae: 0.652095, mean_q: 1.451588
[RESULT] FALSIFICATION!
 39808/100000: episode: 2190, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.013, 10.434], loss: 2.493367, mae: 1.308970, mean_q: 2.128180
[RESULT] FALSIFICATION!
 39809/100000: episode: 2191, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.012, 10.494], loss: 1.246005, mae: 0.836582, mean_q: 1.522981
[RESULT] FALSIFICATION!
 39810/100000: episode: 2192, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.012, 10.519], loss: 1.525233, mae: 1.060741, mean_q: 1.909264
[RESULT] FALSIFICATION!
 39811/100000: episode: 2193, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.013, 10.436], loss: 0.793574, mae: 0.787580, mean_q: 1.633134
[RESULT] FALSIFICATION!
 39812/100000: episode: 2194, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.557], loss: 0.635733, mae: 0.547897, mean_q: 1.110010
[RESULT] FALSIFICATION!
 39813/100000: episode: 2195, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.010, 10.311], loss: 0.661139, mae: 0.481362, mean_q: 1.045145
[RESULT] FALSIFICATION!
 39814/100000: episode: 2196, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.013, 10.659], loss: 3.314753, mae: 1.045351, mean_q: 1.307535
[RESULT] FALSIFICATION!
 39815/100000: episode: 2197, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.010, 10.247], loss: 2.153101, mae: 0.883378, mean_q: 1.485979
[RESULT] FALSIFICATION!
 39816/100000: episode: 2198, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.011, 10.442], loss: 1.489245, mae: 1.074565, mean_q: 2.040766
[RESULT] FALSIFICATION!
 39817/100000: episode: 2199, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.012, 10.378], loss: 1.017751, mae: 0.951871, mean_q: 1.988881
[RESULT] FALSIFICATION!
 39818/100000: episode: 2200, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.011, 10.398], loss: 0.554084, mae: 0.775654, mean_q: 1.891084
[RESULT] FALSIFICATION!
 39820/100000: episode: 2201, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.684, mean reward: 5.342 [0.684, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.012, 10.566], loss: 1.126478, mae: 0.799794, mean_q: 1.734433
[RESULT] FALSIFICATION!
 39821/100000: episode: 2202, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.013, 10.516], loss: 0.182559, mae: 0.449727, mean_q: 1.102108
[RESULT] FALSIFICATION!
 39822/100000: episode: 2203, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.014, 10.571], loss: 1.246374, mae: 0.819298, mean_q: 1.301561
[RESULT] FALSIFICATION!
 39823/100000: episode: 2204, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.012, 10.366], loss: 0.653410, mae: 0.581592, mean_q: 1.247630
[RESULT] FALSIFICATION!
 39824/100000: episode: 2205, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.014, 10.609], loss: 1.816888, mae: 1.008648, mean_q: 1.743956
[RESULT] FALSIFICATION!
 39825/100000: episode: 2206, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.013, 10.448], loss: 0.341722, mae: 0.561583, mean_q: 1.678514
[RESULT] FALSIFICATION!
 39826/100000: episode: 2207, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.012, 10.407], loss: 1.319041, mae: 0.771218, mean_q: 1.785696
[RESULT] FALSIFICATION!
 39827/100000: episode: 2208, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.012, 10.543], loss: 1.806138, mae: 0.812394, mean_q: 1.556475
[RESULT] FALSIFICATION!
 39828/100000: episode: 2209, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.011, 10.190], loss: 1.986254, mae: 0.894218, mean_q: 1.433984
[RESULT] FALSIFICATION!
[Info] New level: 4.936394214630127 | Considering 13/87 traces
 39829/100000: episode: 2210, duration: 4.004s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.011, 10.208], loss: 2.620883, mae: 1.080163, mean_q: 1.691182
[RESULT] FALSIFICATION!
 39830/100000: episode: 2211, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.012, 10.449], loss: 2.521005, mae: 1.267261, mean_q: 1.947870
[RESULT] FALSIFICATION!
 39833/100000: episode: 2212, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 11.375, mean reward: 3.792 [0.675, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.013, 10.603], loss: 1.525183, mae: 1.049023, mean_q: 1.982821
[RESULT] FALSIFICATION!
 39834/100000: episode: 2213, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.011, 10.276], loss: 1.514671, mae: 0.989135, mean_q: 1.752877
[RESULT] FALSIFICATION!
 39835/100000: episode: 2214, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.011, 10.444], loss: 1.882534, mae: 1.082837, mean_q: 1.649423
[RESULT] FALSIFICATION!
 39836/100000: episode: 2215, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.012, 10.478], loss: 1.762112, mae: 0.898835, mean_q: 1.421125
[RESULT] FALSIFICATION!
 39837/100000: episode: 2216, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.011, 10.370], loss: 0.691270, mae: 0.565183, mean_q: 1.276159
[RESULT] FALSIFICATION!
 39838/100000: episode: 2217, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.299], loss: 3.028723, mae: 1.271734, mean_q: 1.855542
[RESULT] FALSIFICATION!
 39840/100000: episode: 2218, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.674, mean reward: 5.337 [0.674, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.012, 10.320], loss: 1.310664, mae: 0.817898, mean_q: 1.762207
[RESULT] FALSIFICATION!
 39841/100000: episode: 2219, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.014, 10.607], loss: 1.106817, mae: 0.834006, mean_q: 1.773812
[RESULT] FALSIFICATION!
 39842/100000: episode: 2220, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.012, 10.316], loss: 1.186417, mae: 0.824439, mean_q: 1.736643
[RESULT] FALSIFICATION!
 39843/100000: episode: 2221, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.388], loss: 1.283288, mae: 0.990667, mean_q: 1.959963
[RESULT] FALSIFICATION!
 39844/100000: episode: 2222, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.012, 10.739], loss: 0.861308, mae: 0.778424, mean_q: 1.781443
[RESULT] FALSIFICATION!
 39845/100000: episode: 2223, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.011, 10.487], loss: 2.001365, mae: 1.173823, mean_q: 1.972445
[RESULT] FALSIFICATION!
 39846/100000: episode: 2224, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.014, 10.576], loss: 1.408406, mae: 0.776307, mean_q: 1.397849
[RESULT] FALSIFICATION!
 39847/100000: episode: 2225, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.014, 10.637], loss: 1.716371, mae: 0.958549, mean_q: 1.686172
[RESULT] FALSIFICATION!
 39848/100000: episode: 2226, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.010, 10.248], loss: 0.781714, mae: 0.586349, mean_q: 1.369166
[RESULT] FALSIFICATION!
 39849/100000: episode: 2227, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.012, 10.406], loss: 0.782099, mae: 0.747029, mean_q: 1.876915
[RESULT] FALSIFICATION!
 39850/100000: episode: 2228, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.012, 10.415], loss: 0.200121, mae: 0.340341, mean_q: 1.282993
[RESULT] FALSIFICATION!
 39851/100000: episode: 2229, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.012, 10.283], loss: 1.642700, mae: 1.065376, mean_q: 2.234261
[RESULT] FALSIFICATION!
 39852/100000: episode: 2230, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.012, 10.516], loss: 1.223032, mae: 0.644940, mean_q: 1.470779
[RESULT] FALSIFICATION!
 39853/100000: episode: 2231, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.013, 10.479], loss: 0.127996, mae: 0.314106, mean_q: 1.416703
[RESULT] FALSIFICATION!
 39854/100000: episode: 2232, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.012, 10.342], loss: 0.593706, mae: 0.456537, mean_q: 1.422049
[RESULT] FALSIFICATION!
 39855/100000: episode: 2233, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.010, 10.337], loss: 0.171353, mae: 0.364793, mean_q: 1.366011
[RESULT] FALSIFICATION!
 39856/100000: episode: 2234, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.012, 10.435], loss: 0.612720, mae: 0.508785, mean_q: 1.461311
[RESULT] FALSIFICATION!
 39857/100000: episode: 2235, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.518], loss: 1.071079, mae: 0.738534, mean_q: 1.731677
[RESULT] FALSIFICATION!
 39858/100000: episode: 2236, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.012, 10.519], loss: 2.577534, mae: 1.367763, mean_q: 2.270562
[RESULT] FALSIFICATION!
 39859/100000: episode: 2237, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.012, 10.370], loss: 0.540524, mae: 0.644855, mean_q: 1.821700
[RESULT] FALSIFICATION!
 39860/100000: episode: 2238, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.672, 10.210], loss: 0.281335, mae: 0.429635, mean_q: 1.371281
[RESULT] FALSIFICATION!
 39861/100000: episode: 2239, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.012, 10.312], loss: 0.692175, mae: 0.626014, mean_q: 1.585372
[RESULT] FALSIFICATION!
 39862/100000: episode: 2240, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.011, 10.225], loss: 1.636800, mae: 0.919372, mean_q: 1.675344
[RESULT] FALSIFICATION!
 39863/100000: episode: 2241, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.010, 10.203], loss: 0.292135, mae: 0.484384, mean_q: 1.537234
[RESULT] FALSIFICATION!
 39864/100000: episode: 2242, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.013, 10.436], loss: 1.294046, mae: 0.736700, mean_q: 1.582053
[RESULT] FALSIFICATION!
 39865/100000: episode: 2243, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.013, 10.646], loss: 1.202631, mae: 0.685375, mean_q: 1.487443
[RESULT] FALSIFICATION!
 39866/100000: episode: 2244, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.013, 10.579], loss: 0.577694, mae: 0.479846, mean_q: 1.277257
[RESULT] FALSIFICATION!
 39867/100000: episode: 2245, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.012, 10.379], loss: 0.684234, mae: 0.581589, mean_q: 1.577012
[RESULT] FALSIFICATION!
 39868/100000: episode: 2246, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.294], loss: 1.510742, mae: 0.893499, mean_q: 1.759652
[RESULT] FALSIFICATION!
 39869/100000: episode: 2247, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.014, 10.486], loss: 0.910812, mae: 0.781296, mean_q: 1.825734
[RESULT] FALSIFICATION!
 39870/100000: episode: 2248, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.012, 10.459], loss: 1.626337, mae: 0.966550, mean_q: 1.912015
[RESULT] FALSIFICATION!
 39871/100000: episode: 2249, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.011, 10.254], loss: 2.970314, mae: 1.454262, mean_q: 2.228748
[RESULT] FALSIFICATION!
 39874/100000: episode: 2250, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 11.391, mean reward: 3.797 [0.692, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.224, 10.478], loss: 1.386438, mae: 0.852560, mean_q: 1.755001
[RESULT] FALSIFICATION!
 39875/100000: episode: 2251, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.011, 10.236], loss: 0.937288, mae: 0.596290, mean_q: 1.431944
[RESULT] FALSIFICATION!
 39876/100000: episode: 2252, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.013, 10.408], loss: 1.782746, mae: 1.113453, mean_q: 2.076818
[RESULT] FALSIFICATION!
 39877/100000: episode: 2253, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.012, 10.422], loss: 1.829786, mae: 1.089252, mean_q: 2.017201
[RESULT] FALSIFICATION!
 39878/100000: episode: 2254, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.011, 10.623], loss: 2.871227, mae: 1.271335, mean_q: 1.807794
[RESULT] FALSIFICATION!
 39879/100000: episode: 2255, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.011, 10.307], loss: 2.575020, mae: 1.415069, mean_q: 2.209402
[RESULT] FALSIFICATION!
 39880/100000: episode: 2256, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.011, 10.386], loss: 1.058366, mae: 0.907521, mean_q: 1.972047
[RESULT] FALSIFICATION!
 39881/100000: episode: 2257, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.011, 10.433], loss: 1.010588, mae: 0.826925, mean_q: 1.456347
[RESULT] FALSIFICATION!
 39882/100000: episode: 2258, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.012, 10.361], loss: 2.496688, mae: 1.120356, mean_q: 1.409722
[RESULT] FALSIFICATION!
 39883/100000: episode: 2259, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.236, 10.522], loss: 1.350853, mae: 0.911408, mean_q: 1.743731
[RESULT] FALSIFICATION!
 39884/100000: episode: 2260, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.011, 10.309], loss: 1.418851, mae: 0.825728, mean_q: 1.723629
[RESULT] FALSIFICATION!
 39885/100000: episode: 2261, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.012, 10.269], loss: 2.838325, mae: 1.508592, mean_q: 2.478103
[RESULT] FALSIFICATION!
 39887/100000: episode: 2262, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 10.675, mean reward: 5.338 [0.675, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.013, 10.552], loss: 0.755576, mae: 0.705510, mean_q: 1.729847
[RESULT] FALSIFICATION!
 39888/100000: episode: 2263, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.013, 10.545], loss: 1.184241, mae: 0.956106, mean_q: 2.235523
[RESULT] FALSIFICATION!
 39889/100000: episode: 2264, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.013, 10.441], loss: 1.394523, mae: 0.734059, mean_q: 1.208092
[RESULT] FALSIFICATION!
 39890/100000: episode: 2265, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.012, 10.339], loss: 2.084613, mae: 0.920731, mean_q: 1.356258
[RESULT] FALSIFICATION!
 39891/100000: episode: 2266, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.012, 10.389], loss: 0.904589, mae: 0.590639, mean_q: 1.333480
[RESULT] FALSIFICATION!
 39892/100000: episode: 2267, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.011, 10.227], loss: 0.484546, mae: 0.513401, mean_q: 1.443139
[RESULT] FALSIFICATION!
 39893/100000: episode: 2268, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.136, 10.125], loss: 0.839968, mae: 0.645765, mean_q: 1.706187
[RESULT] FALSIFICATION!
 39894/100000: episode: 2269, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.012, 10.555], loss: 1.486199, mae: 1.007149, mean_q: 2.274377
[RESULT] FALSIFICATION!
 39895/100000: episode: 2270, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.011, 10.337], loss: 2.042386, mae: 1.208136, mean_q: 2.205935
[RESULT] FALSIFICATION!
 39896/100000: episode: 2271, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.013, 10.514], loss: 1.456500, mae: 1.026044, mean_q: 2.089519
[RESULT] FALSIFICATION!
 39898/100000: episode: 2272, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 10.688, mean reward: 5.344 [0.688, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.014, 10.628], loss: 0.755706, mae: 0.660336, mean_q: 1.620380
[RESULT] FALSIFICATION!
 39899/100000: episode: 2273, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.010, 10.305], loss: 1.600793, mae: 1.008205, mean_q: 2.040784
[RESULT] FALSIFICATION!
 39900/100000: episode: 2274, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.010, 10.229], loss: 1.038717, mae: 0.754099, mean_q: 1.537342
[RESULT] FALSIFICATION!
 39901/100000: episode: 2275, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.011, 10.278], loss: 1.415035, mae: 0.618271, mean_q: 1.273327
[RESULT] FALSIFICATION!
 39902/100000: episode: 2276, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.011, 10.274], loss: 1.088076, mae: 0.824673, mean_q: 2.023917
[RESULT] FALSIFICATION!
 39904/100000: episode: 2277, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.690, mean reward: 5.345 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.010, 10.429], loss: 0.981980, mae: 0.850179, mean_q: 1.977084
[RESULT] FALSIFICATION!
 39905/100000: episode: 2278, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.014, 10.593], loss: 2.713199, mae: 1.174932, mean_q: 1.789407
[RESULT] FALSIFICATION!
 39906/100000: episode: 2279, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.011, 10.477], loss: 1.172006, mae: 0.836802, mean_q: 1.914419
[RESULT] FALSIFICATION!
 39907/100000: episode: 2280, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.012, 10.404], loss: 1.909249, mae: 1.212832, mean_q: 2.240885
[RESULT] FALSIFICATION!
 39908/100000: episode: 2281, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.011, 10.493], loss: 0.885672, mae: 0.782448, mean_q: 1.892583
[RESULT] FALSIFICATION!
 39909/100000: episode: 2282, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.012, 10.402], loss: 2.102157, mae: 1.189138, mean_q: 2.202395
[RESULT] FALSIFICATION!
 39910/100000: episode: 2283, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.011, 10.420], loss: 0.759970, mae: 0.761307, mean_q: 1.841403
[RESULT] FALSIFICATION!
 39911/100000: episode: 2284, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.011, 10.286], loss: 1.556632, mae: 0.908919, mean_q: 1.774499
[RESULT] FALSIFICATION!
 39913/100000: episode: 2285, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 10.693, mean reward: 5.347 [0.693, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.012, 10.361], loss: 0.906645, mae: 0.609901, mean_q: 1.344714
[RESULT] FALSIFICATION!
 39914/100000: episode: 2286, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.011, 10.447], loss: 1.527645, mae: 0.848467, mean_q: 1.678239
[RESULT] FALSIFICATION!
 39915/100000: episode: 2287, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.326], loss: 0.772804, mae: 0.766028, mean_q: 1.867310
[RESULT] FALSIFICATION!
 39916/100000: episode: 2288, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.609, 10.281], loss: 0.677777, mae: 0.653740, mean_q: 1.679614
[RESULT] FALSIFICATION!
 39917/100000: episode: 2289, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.013, 10.540], loss: 2.067911, mae: 1.037740, mean_q: 1.818762
[RESULT] FALSIFICATION!
 39918/100000: episode: 2290, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.013, 10.339], loss: 1.081222, mae: 0.713668, mean_q: 1.806268
[RESULT] FALSIFICATION!
 39919/100000: episode: 2291, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.436, 10.207], loss: 1.638056, mae: 0.966297, mean_q: 1.987669
[RESULT] FALSIFICATION!
 39920/100000: episode: 2292, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.454, 10.257], loss: 0.194195, mae: 0.338833, mean_q: 1.280676
[RESULT] FALSIFICATION!
 39921/100000: episode: 2293, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.011, 10.248], loss: 1.230646, mae: 0.841305, mean_q: 1.761869
[RESULT] FALSIFICATION!
 39922/100000: episode: 2294, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.013, 10.467], loss: 0.472590, mae: 0.460269, mean_q: 1.275611
[RESULT] FALSIFICATION!
 39923/100000: episode: 2295, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.010, 10.291], loss: 2.127760, mae: 1.180199, mean_q: 2.167391
[RESULT] FALSIFICATION!
 39924/100000: episode: 2296, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.013, 10.614], loss: 0.272240, mae: 0.466486, mean_q: 1.334156
[RESULT] FALSIFICATION!
[Info] New level: 6.517909049987793 | Considering 14/86 traces
 39925/100000: episode: 2297, duration: 3.817s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.012, 10.558], loss: 2.974781, mae: 1.273050, mean_q: 1.885294
[RESULT] FALSIFICATION!
 39926/100000: episode: 2298, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.011, 10.494], loss: 1.562225, mae: 0.917808, mean_q: 1.766278
[RESULT] FALSIFICATION!
 39927/100000: episode: 2299, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.015, 10.531], loss: 1.717414, mae: 1.134745, mean_q: 2.210182
[RESULT] FALSIFICATION!
 39928/100000: episode: 2300, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.012, 10.374], loss: 3.114582, mae: 1.613397, mean_q: 2.721989
[RESULT] FALSIFICATION!
 39929/100000: episode: 2301, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.013, 10.567], loss: 0.978334, mae: 0.831020, mean_q: 2.100904
[RESULT] FALSIFICATION!
 39930/100000: episode: 2302, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.014, 10.574], loss: 0.752326, mae: 0.582710, mean_q: 1.470716
[RESULT] FALSIFICATION!
 39931/100000: episode: 2303, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.013, 10.577], loss: 1.399031, mae: 0.755195, mean_q: 1.489083
[RESULT] FALSIFICATION!
 39932/100000: episode: 2304, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.013, 10.455], loss: 1.557255, mae: 0.775530, mean_q: 1.389627
[RESULT] FALSIFICATION!
 39933/100000: episode: 2305, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.015, 10.685], loss: 1.694192, mae: 1.053114, mean_q: 2.101177
[RESULT] FALSIFICATION!
 39934/100000: episode: 2306, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.013, 10.531], loss: 1.210864, mae: 1.139812, mean_q: 2.435154
[RESULT] FALSIFICATION!
 39935/100000: episode: 2307, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.013, 10.582], loss: 2.017191, mae: 1.251184, mean_q: 2.358144
[RESULT] FALSIFICATION!
 39936/100000: episode: 2308, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.013, 10.600], loss: 1.534698, mae: 1.133170, mean_q: 2.129951
[RESULT] FALSIFICATION!
 39937/100000: episode: 2309, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.012, 10.460], loss: 0.720187, mae: 0.675252, mean_q: 1.444234
[RESULT] FALSIFICATION!
 39938/100000: episode: 2310, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.014, 10.582], loss: 2.107298, mae: 1.022247, mean_q: 1.512987
[RESULT] FALSIFICATION!
 39939/100000: episode: 2311, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.013, 10.561], loss: 1.443793, mae: 1.036592, mean_q: 2.145129
[RESULT] FALSIFICATION!
 39940/100000: episode: 2312, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.544], loss: 1.306360, mae: 0.908536, mean_q: 1.906781
[RESULT] FALSIFICATION!
 39941/100000: episode: 2313, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.015, 10.535], loss: 1.111673, mae: 0.949363, mean_q: 2.293836
[RESULT] FALSIFICATION!
 39942/100000: episode: 2314, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.012, 10.568], loss: 1.052672, mae: 0.747622, mean_q: 1.609208
[RESULT] FALSIFICATION!
 39943/100000: episode: 2315, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.011, 10.407], loss: 0.560432, mae: 0.631493, mean_q: 1.734181
[RESULT] FALSIFICATION!
 39944/100000: episode: 2316, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.012, 10.553], loss: 2.824678, mae: 1.518862, mean_q: 2.445483
[RESULT] FALSIFICATION!
 39945/100000: episode: 2317, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.014, 10.602], loss: 1.753147, mae: 1.140799, mean_q: 2.353828
[RESULT] FALSIFICATION!
 39946/100000: episode: 2318, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.014, 10.567], loss: 0.754399, mae: 0.646428, mean_q: 1.747062
[RESULT] FALSIFICATION!
 39947/100000: episode: 2319, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.012, 10.455], loss: 1.933155, mae: 1.373342, mean_q: 2.971590
[RESULT] FALSIFICATION!
 39948/100000: episode: 2320, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.812, 10.291], loss: 1.638879, mae: 1.177045, mean_q: 2.514920
[RESULT] FALSIFICATION!
 39949/100000: episode: 2321, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.014, 10.545], loss: 1.373821, mae: 1.000981, mean_q: 2.030558
[RESULT] FALSIFICATION!
 39950/100000: episode: 2322, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.011, 10.550], loss: 0.439021, mae: 0.678459, mean_q: 1.630589
[RESULT] FALSIFICATION!
 39951/100000: episode: 2323, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.014, 10.477], loss: 0.206868, mae: 0.369417, mean_q: 1.351341
[RESULT] FALSIFICATION!
 39952/100000: episode: 2324, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.014, 10.564], loss: 0.577044, mae: 0.522374, mean_q: 1.599726
[RESULT] FALSIFICATION!
 39953/100000: episode: 2325, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.586], loss: 1.199510, mae: 0.770658, mean_q: 1.523064
[RESULT] FALSIFICATION!
 39955/100000: episode: 2326, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 10.686, mean reward: 5.343 [0.686, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.014, 10.562], loss: 1.743191, mae: 0.910132, mean_q: 1.626142
[RESULT] FALSIFICATION!
 39956/100000: episode: 2327, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.014, 10.529], loss: 1.983454, mae: 1.066561, mean_q: 1.965204
[RESULT] FALSIFICATION!
 39957/100000: episode: 2328, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.361 [-0.015, 10.644], loss: 1.046922, mae: 0.765150, mean_q: 1.788798
[RESULT] FALSIFICATION!
 39958/100000: episode: 2329, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.013, 10.546], loss: 1.589019, mae: 1.064631, mean_q: 1.976795
[RESULT] FALSIFICATION!
 39959/100000: episode: 2330, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.013, 10.480], loss: 1.983028, mae: 1.408952, mean_q: 2.721510
[RESULT] FALSIFICATION!
 39960/100000: episode: 2331, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.013, 10.585], loss: 1.101710, mae: 0.953587, mean_q: 1.771268
[RESULT] FALSIFICATION!
 39961/100000: episode: 2332, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.013, 10.573], loss: 0.295191, mae: 0.485565, mean_q: 1.037580
[RESULT] FALSIFICATION!
 39962/100000: episode: 2333, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.013, 10.642], loss: 1.317959, mae: 1.047891, mean_q: 2.241474
[RESULT] FALSIFICATION!
 39963/100000: episode: 2334, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.249, 10.358], loss: 1.906541, mae: 1.055360, mean_q: 2.064673
[RESULT] FALSIFICATION!
 39964/100000: episode: 2335, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.014, 10.669], loss: 0.793740, mae: 0.901171, mean_q: 2.429779
[RESULT] FALSIFICATION!
 39965/100000: episode: 2336, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.420 [-0.016, 10.659], loss: 0.998968, mae: 0.652089, mean_q: 1.499135
[RESULT] FALSIFICATION!
 39966/100000: episode: 2337, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.013, 10.626], loss: 0.495961, mae: 0.496931, mean_q: 1.531254
[RESULT] FALSIFICATION!
 39967/100000: episode: 2338, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.021, 10.388], loss: 1.086906, mae: 0.617935, mean_q: 1.380073
[RESULT] FALSIFICATION!
 39968/100000: episode: 2339, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.014, 10.619], loss: 2.643497, mae: 1.239902, mean_q: 2.079347
[RESULT] FALSIFICATION!
 39969/100000: episode: 2340, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.010, 10.446], loss: 0.851222, mae: 0.696294, mean_q: 1.705380
[RESULT] FALSIFICATION!
 39970/100000: episode: 2341, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.014, 10.561], loss: 0.697329, mae: 0.768695, mean_q: 1.773423
[RESULT] FALSIFICATION!
 39971/100000: episode: 2342, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.595], loss: 1.092229, mae: 0.831069, mean_q: 1.850670
[RESULT] FALSIFICATION!
 39972/100000: episode: 2343, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.482], loss: 1.901270, mae: 1.155032, mean_q: 2.204387
[RESULT] FALSIFICATION!
 39973/100000: episode: 2344, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.013, 10.379], loss: 0.956641, mae: 0.651983, mean_q: 1.492820
[RESULT] FALSIFICATION!
 39974/100000: episode: 2345, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.012, 10.415], loss: 3.410186, mae: 1.480495, mean_q: 2.297593
[RESULT] FALSIFICATION!
 39975/100000: episode: 2346, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.013, 10.509], loss: 1.704970, mae: 1.087953, mean_q: 2.335639
[RESULT] FALSIFICATION!
 39976/100000: episode: 2347, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.012, 10.463], loss: 1.166386, mae: 0.898650, mean_q: 2.134179
[RESULT] FALSIFICATION!
 39977/100000: episode: 2348, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.013, 10.506], loss: 1.080433, mae: 0.926434, mean_q: 2.115058
[RESULT] FALSIFICATION!
 39978/100000: episode: 2349, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.012, 10.512], loss: 1.706285, mae: 1.079678, mean_q: 2.128767
[RESULT] FALSIFICATION!
 39979/100000: episode: 2350, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.012, 10.334], loss: 1.538161, mae: 1.009167, mean_q: 2.132984
[RESULT] FALSIFICATION!
 39980/100000: episode: 2351, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.012, 10.524], loss: 0.558817, mae: 0.636760, mean_q: 1.826121
[RESULT] FALSIFICATION!
 39981/100000: episode: 2352, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.014, 10.655], loss: 1.184685, mae: 1.024127, mean_q: 2.429736
[RESULT] FALSIFICATION!
 39982/100000: episode: 2353, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.013, 10.358], loss: 1.069800, mae: 0.744143, mean_q: 1.746665
[RESULT] FALSIFICATION!
 39983/100000: episode: 2354, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.012, 10.387], loss: 0.746513, mae: 0.653673, mean_q: 1.836874
[RESULT] FALSIFICATION!
 39984/100000: episode: 2355, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.013, 10.446], loss: 1.144082, mae: 0.802365, mean_q: 1.751751
[RESULT] FALSIFICATION!
 39985/100000: episode: 2356, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.014, 10.522], loss: 1.657931, mae: 1.293767, mean_q: 2.994498
[RESULT] FALSIFICATION!
 39986/100000: episode: 2357, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.014, 10.545], loss: 0.398846, mae: 0.493220, mean_q: 1.716677
[RESULT] FALSIFICATION!
 39987/100000: episode: 2358, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.014, 10.734], loss: 0.743528, mae: 0.580418, mean_q: 1.540496
[RESULT] FALSIFICATION!
 39988/100000: episode: 2359, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.013, 10.485], loss: 0.091103, mae: 0.280008, mean_q: 1.120589
[RESULT] FALSIFICATION!
 39989/100000: episode: 2360, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.012, 10.507], loss: 0.798234, mae: 0.674696, mean_q: 1.922348
[RESULT] FALSIFICATION!
 39990/100000: episode: 2361, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.013, 10.495], loss: 0.720666, mae: 0.725908, mean_q: 1.785687
[RESULT] FALSIFICATION!
 39991/100000: episode: 2362, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.014, 10.414], loss: 1.127170, mae: 1.003519, mean_q: 2.400727
[RESULT] FALSIFICATION!
 39992/100000: episode: 2363, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.012, 10.428], loss: 1.114752, mae: 0.893165, mean_q: 2.165861
[RESULT] FALSIFICATION!
 39993/100000: episode: 2364, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.356 [-0.014, 10.654], loss: 1.076144, mae: 0.876876, mean_q: 2.217760
[RESULT] FALSIFICATION!
 39994/100000: episode: 2365, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.371], loss: 1.316832, mae: 1.109212, mean_q: 2.565408
[RESULT] FALSIFICATION!
 39995/100000: episode: 2366, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.013, 10.500], loss: 1.307879, mae: 1.094835, mean_q: 2.418744
[RESULT] FALSIFICATION!
 39996/100000: episode: 2367, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.013, 10.585], loss: 1.063534, mae: 0.697899, mean_q: 1.288178
[RESULT] FALSIFICATION!
 39997/100000: episode: 2368, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.013, 10.469], loss: 1.124298, mae: 0.883740, mean_q: 1.781682
[RESULT] FALSIFICATION!
 39998/100000: episode: 2369, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.012, 10.475], loss: 1.509150, mae: 1.177158, mean_q: 2.387745
[RESULT] FALSIFICATION!
 39999/100000: episode: 2370, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.015, 10.582], loss: 0.618850, mae: 0.489170, mean_q: 1.288207
[RESULT] FALSIFICATION!
 40000/100000: episode: 2371, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.012, 10.575], loss: 0.646881, mae: 0.585511, mean_q: 1.729524
[RESULT] FALSIFICATION!
 40001/100000: episode: 2372, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.012, 10.444], loss: 2.439464, mae: 1.076538, mean_q: 1.834993
[RESULT] FALSIFICATION!
 40002/100000: episode: 2373, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.012, 10.352], loss: 1.313945, mae: 0.707428, mean_q: 1.454033
[RESULT] FALSIFICATION!
 40004/100000: episode: 2374, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.649, mean reward: 5.325 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.013, 10.603], loss: 1.171785, mae: 0.868264, mean_q: 2.028370
[RESULT] FALSIFICATION!
 40005/100000: episode: 2375, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.013, 10.552], loss: 1.177527, mae: 0.865523, mean_q: 2.038586
[RESULT] FALSIFICATION!
 40006/100000: episode: 2376, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.013, 10.513], loss: 0.640182, mae: 0.519499, mean_q: 1.179921
[RESULT] FALSIFICATION!
 40007/100000: episode: 2377, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.013, 10.510], loss: 0.712996, mae: 0.703413, mean_q: 1.534541
[RESULT] FALSIFICATION!
 40008/100000: episode: 2378, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.014, 10.536], loss: 0.759130, mae: 0.732720, mean_q: 1.772950
[RESULT] FALSIFICATION!
 40009/100000: episode: 2379, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.012, 10.494], loss: 0.708667, mae: 0.671516, mean_q: 1.728339
[RESULT] FALSIFICATION!
 40010/100000: episode: 2380, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.013, 10.642], loss: 1.682086, mae: 1.227868, mean_q: 2.620311
[RESULT] FALSIFICATION!
 40011/100000: episode: 2381, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.013, 10.603], loss: 0.527223, mae: 0.603106, mean_q: 1.731566
[RESULT] FALSIFICATION!
 40012/100000: episode: 2382, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.014, 10.683], loss: 0.908936, mae: 0.862020, mean_q: 2.093607
[RESULT] FALSIFICATION!
[Info] New level: 7.163121700286865 | Considering 10/90 traces
 40013/100000: episode: 2383, duration: 3.908s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.013, 10.627], loss: 1.151942, mae: 0.828146, mean_q: 2.019539
[RESULT] FALSIFICATION!
 40014/100000: episode: 2384, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.014, 10.571], loss: 0.448821, mae: 0.529441, mean_q: 1.406587
[RESULT] FALSIFICATION!
 40015/100000: episode: 2385, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.387 [-0.016, 10.590], loss: 1.050536, mae: 0.841965, mean_q: 1.871916
[RESULT] FALSIFICATION!
 40016/100000: episode: 2386, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.012, 10.679], loss: 1.635329, mae: 0.950880, mean_q: 1.910596
[RESULT] FALSIFICATION!
 40017/100000: episode: 2387, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.012, 10.387], loss: 0.897438, mae: 0.720991, mean_q: 1.886100
[RESULT] FALSIFICATION!
 40018/100000: episode: 2388, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.013, 10.670], loss: 1.409409, mae: 1.118371, mean_q: 2.659908
[RESULT] FALSIFICATION!
 40019/100000: episode: 2389, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.437 [-0.017, 10.851], loss: 0.929344, mae: 0.830194, mean_q: 2.152710
[RESULT] FALSIFICATION!
 40020/100000: episode: 2390, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.014, 10.530], loss: 1.069193, mae: 0.858685, mean_q: 2.099918
[RESULT] FALSIFICATION!
 40021/100000: episode: 2391, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.012, 10.640], loss: 2.416530, mae: 1.280929, mean_q: 2.431687
[RESULT] FALSIFICATION!
 40022/100000: episode: 2392, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.014, 10.691], loss: 2.195653, mae: 1.151705, mean_q: 2.311771
[RESULT] FALSIFICATION!
 40023/100000: episode: 2393, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.013, 10.483], loss: 2.268569, mae: 1.535413, mean_q: 3.217081
[RESULT] FALSIFICATION!
 40024/100000: episode: 2394, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.013, 10.629], loss: 3.077548, mae: 1.335094, mean_q: 2.223147
[RESULT] FALSIFICATION!
 40025/100000: episode: 2395, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.013, 10.468], loss: 0.787451, mae: 0.777872, mean_q: 2.021385
[RESULT] FALSIFICATION!
 40026/100000: episode: 2396, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.013, 10.681], loss: 0.905430, mae: 0.735147, mean_q: 1.758130
[RESULT] FALSIFICATION!
 40027/100000: episode: 2397, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.694, 10.447], loss: 1.047824, mae: 0.938107, mean_q: 2.394263
[RESULT] FALSIFICATION!
 40028/100000: episode: 2398, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.014, 10.515], loss: 2.744464, mae: 1.443433, mean_q: 2.622631
[RESULT] FALSIFICATION!
 40029/100000: episode: 2399, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.165, 10.536], loss: 1.155456, mae: 0.960029, mean_q: 2.557896
[RESULT] FALSIFICATION!
 40030/100000: episode: 2400, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.014, 10.698], loss: 1.713819, mae: 0.996128, mean_q: 1.789533
[RESULT] FALSIFICATION!
 40031/100000: episode: 2401, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.158, 10.550], loss: 0.819688, mae: 0.744281, mean_q: 1.856180
[RESULT] FALSIFICATION!
 40032/100000: episode: 2402, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.013, 10.617], loss: 1.286664, mae: 0.964120, mean_q: 2.175535
[RESULT] FALSIFICATION!
 40033/100000: episode: 2403, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.015, 10.516], loss: 1.819993, mae: 0.956448, mean_q: 1.741542
[RESULT] FALSIFICATION!
 40034/100000: episode: 2404, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.014, 10.713], loss: 2.381812, mae: 1.278729, mean_q: 2.632010
[RESULT] FALSIFICATION!
 40035/100000: episode: 2405, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.014, 10.547], loss: 1.772165, mae: 1.144217, mean_q: 2.450500
[RESULT] FALSIFICATION!
 40036/100000: episode: 2406, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.014, 10.697], loss: 1.443809, mae: 1.148516, mean_q: 2.353666
[RESULT] FALSIFICATION!
 40037/100000: episode: 2407, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.011, 10.523], loss: 1.696414, mae: 1.259320, mean_q: 3.118821
[RESULT] FALSIFICATION!
 40038/100000: episode: 2408, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.014, 10.502], loss: 0.920152, mae: 0.908972, mean_q: 1.911377
[RESULT] FALSIFICATION!
 40039/100000: episode: 2409, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.268, 10.502], loss: 1.324713, mae: 1.092926, mean_q: 1.390393
[RESULT] FALSIFICATION!
 40040/100000: episode: 2410, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.014, 10.705], loss: 2.240451, mae: 0.917249, mean_q: 1.203389
[RESULT] FALSIFICATION!
 40041/100000: episode: 2411, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.013, 10.658], loss: 1.079783, mae: 0.941766, mean_q: 2.192259
[RESULT] FALSIFICATION!
 40042/100000: episode: 2412, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.014, 10.579], loss: 0.953410, mae: 1.074607, mean_q: 2.534426
[RESULT] FALSIFICATION!
 40043/100000: episode: 2413, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.011, 10.479], loss: 2.558394, mae: 1.570384, mean_q: 3.195099
[RESULT] FALSIFICATION!
 40044/100000: episode: 2414, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.014, 10.600], loss: 1.991264, mae: 0.990452, mean_q: 1.924348
[RESULT] FALSIFICATION!
 40045/100000: episode: 2415, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.013, 10.463], loss: 0.755524, mae: 0.787013, mean_q: 2.030469
[RESULT] FALSIFICATION!
 40046/100000: episode: 2416, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.013, 10.364], loss: 0.732933, mae: 0.722077, mean_q: 1.637192
[RESULT] FALSIFICATION!
 40047/100000: episode: 2417, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.014, 10.450], loss: 0.872499, mae: 0.801383, mean_q: 1.745967
[RESULT] FALSIFICATION!
 40048/100000: episode: 2418, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.013, 10.515], loss: 1.149457, mae: 0.904635, mean_q: 1.944915
[RESULT] FALSIFICATION!
 40049/100000: episode: 2419, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.012, 10.477], loss: 0.839684, mae: 0.837767, mean_q: 2.451842
[RESULT] FALSIFICATION!
 40050/100000: episode: 2420, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.014, 10.619], loss: 2.237131, mae: 1.392122, mean_q: 3.009712
[RESULT] FALSIFICATION!
 40051/100000: episode: 2421, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.013, 10.531], loss: 1.016840, mae: 0.873675, mean_q: 2.125344
[RESULT] FALSIFICATION!
 40052/100000: episode: 2422, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.014, 10.621], loss: 1.084700, mae: 0.879099, mean_q: 2.276280
[RESULT] FALSIFICATION!
 40053/100000: episode: 2423, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.015, 10.646], loss: 1.005238, mae: 0.935053, mean_q: 2.572282
[RESULT] FALSIFICATION!
 40054/100000: episode: 2424, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.063, 10.476], loss: 1.059578, mae: 0.922070, mean_q: 2.448145
[RESULT] FALSIFICATION!
 40055/100000: episode: 2425, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.014, 10.726], loss: 0.621219, mae: 0.674036, mean_q: 1.916690
[RESULT] FALSIFICATION!
 40056/100000: episode: 2426, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.011, 10.585], loss: 2.803476, mae: 1.338282, mean_q: 2.239776
[RESULT] FALSIFICATION!
 40057/100000: episode: 2427, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.014, 10.617], loss: 1.188529, mae: 0.882974, mean_q: 2.046685
[RESULT] FALSIFICATION!
 40058/100000: episode: 2428, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.014, 10.667], loss: 1.564042, mae: 0.934772, mean_q: 2.528944
[RESULT] FALSIFICATION!
 40059/100000: episode: 2429, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.014, 10.599], loss: 1.241805, mae: 0.946770, mean_q: 2.216284
[RESULT] FALSIFICATION!
 40060/100000: episode: 2430, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.012, 10.509], loss: 1.073641, mae: 1.045462, mean_q: 2.850090
[RESULT] FALSIFICATION!
 40061/100000: episode: 2431, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.014, 10.596], loss: 2.202375, mae: 1.416046, mean_q: 2.889600
[RESULT] FALSIFICATION!
 40062/100000: episode: 2432, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.568], loss: 1.825797, mae: 1.094300, mean_q: 1.871239
[RESULT] FALSIFICATION!
 40063/100000: episode: 2433, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.852, 10.454], loss: 1.155757, mae: 1.028973, mean_q: 2.300417
[RESULT] FALSIFICATION!
 40064/100000: episode: 2434, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.015, 10.690], loss: 1.277109, mae: 0.854598, mean_q: 1.802160
[RESULT] FALSIFICATION!
 40065/100000: episode: 2435, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.600], loss: 0.806627, mae: 0.667455, mean_q: 1.735744
[RESULT] FALSIFICATION!
 40066/100000: episode: 2436, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.015, 10.695], loss: 1.727809, mae: 1.271879, mean_q: 2.915918
[RESULT] FALSIFICATION!
 40067/100000: episode: 2437, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.012, 10.648], loss: 0.872624, mae: 0.555756, mean_q: 1.597949
[RESULT] FALSIFICATION!
 40068/100000: episode: 2438, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.012, 10.656], loss: 1.481261, mae: 0.986927, mean_q: 2.366832
[RESULT] FALSIFICATION!
 40069/100000: episode: 2439, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.013, 10.769], loss: 1.235595, mae: 0.987711, mean_q: 2.319176
[RESULT] FALSIFICATION!
 40070/100000: episode: 2440, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.014, 10.514], loss: 1.413762, mae: 1.073405, mean_q: 2.593642
[RESULT] FALSIFICATION!
 40071/100000: episode: 2441, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.015, 10.538], loss: 1.375387, mae: 1.166534, mean_q: 3.177018
[RESULT] FALSIFICATION!
 40072/100000: episode: 2442, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.014, 10.633], loss: 0.824483, mae: 0.589172, mean_q: 1.283310
[RESULT] FALSIFICATION!
 40073/100000: episode: 2443, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.012, 10.411], loss: 1.187232, mae: 0.902770, mean_q: 2.107059
[RESULT] FALSIFICATION!
 40074/100000: episode: 2444, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.014, 10.595], loss: 0.722571, mae: 0.654501, mean_q: 1.963261
[RESULT] FALSIFICATION!
 40075/100000: episode: 2445, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.013, 10.525], loss: 1.609646, mae: 0.881398, mean_q: 1.841234
[RESULT] FALSIFICATION!
 40076/100000: episode: 2446, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.012, 10.610], loss: 0.414019, mae: 0.621545, mean_q: 1.930897
[RESULT] FALSIFICATION!
 40077/100000: episode: 2447, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.013, 10.581], loss: 0.980082, mae: 0.857887, mean_q: 2.091033
[RESULT] FALSIFICATION!
 40078/100000: episode: 2448, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.425, 10.466], loss: 0.788963, mae: 0.901100, mean_q: 2.247779
[RESULT] FALSIFICATION!
 40079/100000: episode: 2449, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.012, 10.393], loss: 2.203707, mae: 1.336865, mean_q: 2.909391
[RESULT] FALSIFICATION!
 40080/100000: episode: 2450, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.015, 10.763], loss: 1.605936, mae: 1.123174, mean_q: 2.264216
[RESULT] FALSIFICATION!
 40081/100000: episode: 2451, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.013, 10.610], loss: 1.235674, mae: 0.943430, mean_q: 2.099993
[RESULT] FALSIFICATION!
 40082/100000: episode: 2452, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.015, 10.647], loss: 0.387908, mae: 0.535271, mean_q: 1.476321
[RESULT] FALSIFICATION!
 40083/100000: episode: 2453, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.012, 10.660], loss: 0.673090, mae: 0.652100, mean_q: 2.093153
[RESULT] FALSIFICATION!
 40084/100000: episode: 2454, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.013, 10.628], loss: 1.132095, mae: 0.984690, mean_q: 2.399339
[RESULT] FALSIFICATION!
 40085/100000: episode: 2455, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.013, 10.590], loss: 3.709194, mae: 1.710272, mean_q: 2.983790
[RESULT] FALSIFICATION!
 40086/100000: episode: 2456, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.017, 10.704], loss: 1.789397, mae: 1.119285, mean_q: 2.041936
[RESULT] FALSIFICATION!
 40087/100000: episode: 2457, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.013, 10.580], loss: 1.054141, mae: 0.969294, mean_q: 2.314750
[RESULT] FALSIFICATION!
 40088/100000: episode: 2458, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.012, 10.537], loss: 1.522361, mae: 1.247335, mean_q: 2.694329
[RESULT] FALSIFICATION!
 40089/100000: episode: 2459, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.013, 10.625], loss: 1.063236, mae: 1.009218, mean_q: 3.016240
[RESULT] FALSIFICATION!
 40090/100000: episode: 2460, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.013, 10.432], loss: 1.280361, mae: 0.982906, mean_q: 2.557635
[RESULT] FALSIFICATION!
 40091/100000: episode: 2461, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.012, 10.431], loss: 0.393948, mae: 0.546277, mean_q: 1.610065
[RESULT] FALSIFICATION!
 40092/100000: episode: 2462, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.013, 10.527], loss: 0.689908, mae: 0.791765, mean_q: 1.764929
[RESULT] FALSIFICATION!
 40093/100000: episode: 2463, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.012, 10.495], loss: 0.748592, mae: 0.815661, mean_q: 2.049593
[RESULT] FALSIFICATION!
 40094/100000: episode: 2464, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.012, 10.636], loss: 1.605614, mae: 1.045009, mean_q: 2.223439
[RESULT] FALSIFICATION!
 40095/100000: episode: 2465, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.013, 10.577], loss: 1.239030, mae: 1.250852, mean_q: 3.219738
[RESULT] FALSIFICATION!
 40096/100000: episode: 2466, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.013, 10.546], loss: 1.817794, mae: 1.493787, mean_q: 3.543257
[RESULT] FALSIFICATION!
 40097/100000: episode: 2467, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.012, 10.577], loss: 0.527861, mae: 0.630982, mean_q: 1.836476
[RESULT] FALSIFICATION!
 40098/100000: episode: 2468, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.013, 10.625], loss: 1.761903, mae: 1.109436, mean_q: 2.135744
[RESULT] FALSIFICATION!
 40099/100000: episode: 2469, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.015, 10.708], loss: 1.653601, mae: 1.022520, mean_q: 2.349107
[RESULT] FALSIFICATION!
 40100/100000: episode: 2470, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.013, 10.581], loss: 1.131653, mae: 0.923354, mean_q: 2.255808
[RESULT] FALSIFICATION!
 40101/100000: episode: 2471, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.014, 10.691], loss: 1.411968, mae: 1.098768, mean_q: 2.829192
[RESULT] FALSIFICATION!
 40102/100000: episode: 2472, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.014, 10.563], loss: 2.017881, mae: 1.113692, mean_q: 2.424034
[RESULT] FALSIFICATION!
[Info] New level: 8.43616771697998 | Considering 11/89 traces
 40103/100000: episode: 2473, duration: 3.864s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.551, 10.473], loss: 0.500682, mae: 0.563820, mean_q: 1.936223
[RESULT] FALSIFICATION!
 40104/100000: episode: 2474, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.013, 10.584], loss: 1.511379, mae: 1.206365, mean_q: 3.060300
[RESULT] FALSIFICATION!
 40105/100000: episode: 2475, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.013, 10.748], loss: 1.223521, mae: 1.025481, mean_q: 2.320945
[RESULT] FALSIFICATION!
 40106/100000: episode: 2476, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.376 [-0.014, 10.643], loss: 1.046090, mae: 1.025737, mean_q: 2.557100
[RESULT] FALSIFICATION!
 40107/100000: episode: 2477, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.013, 10.592], loss: 0.745906, mae: 0.734136, mean_q: 1.942198
[RESULT] FALSIFICATION!
 40108/100000: episode: 2478, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.012, 10.533], loss: 1.509034, mae: 1.105275, mean_q: 2.017679
[RESULT] FALSIFICATION!
 40109/100000: episode: 2479, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.013, 10.692], loss: 1.883527, mae: 0.746792, mean_q: 1.496454
[RESULT] FALSIFICATION!
 40110/100000: episode: 2480, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.013, 10.570], loss: 0.653902, mae: 0.745141, mean_q: 2.063311
[RESULT] FALSIFICATION!
 40111/100000: episode: 2481, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.013, 10.620], loss: 0.859331, mae: 0.838911, mean_q: 2.250629
[RESULT] FALSIFICATION!
 40112/100000: episode: 2482, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.014, 10.702], loss: 1.388986, mae: 0.823861, mean_q: 1.800826
[RESULT] FALSIFICATION!
 40113/100000: episode: 2483, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.014, 10.846], loss: 2.478271, mae: 0.986168, mean_q: 1.432239
[RESULT] FALSIFICATION!
 40114/100000: episode: 2484, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.011, 10.526], loss: 1.587411, mae: 0.808279, mean_q: 1.532378
[RESULT] FALSIFICATION!
 40115/100000: episode: 2485, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.012, 10.522], loss: 1.205600, mae: 0.973435, mean_q: 2.515471
[RESULT] FALSIFICATION!
 40116/100000: episode: 2486, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.012, 10.600], loss: 1.014911, mae: 0.997912, mean_q: 2.728830
[RESULT] FALSIFICATION!
 40117/100000: episode: 2487, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.012, 10.541], loss: 1.192014, mae: 0.855214, mean_q: 2.194826
[RESULT] FALSIFICATION!
 40118/100000: episode: 2488, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.012, 10.458], loss: 1.374535, mae: 1.148857, mean_q: 3.323586
[RESULT] FALSIFICATION!
 40119/100000: episode: 2489, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.011, 10.473], loss: 1.127805, mae: 0.961163, mean_q: 2.310410
[RESULT] FALSIFICATION!
 40120/100000: episode: 2490, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.013, 10.624], loss: 1.308310, mae: 1.097493, mean_q: 2.500654
[RESULT] FALSIFICATION!
 40121/100000: episode: 2491, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.013, 10.570], loss: 0.383858, mae: 0.619420, mean_q: 1.866346
[RESULT] FALSIFICATION!
 40122/100000: episode: 2492, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.013, 10.628], loss: 1.113479, mae: 0.974558, mean_q: 2.711549
[RESULT] FALSIFICATION!
 40123/100000: episode: 2493, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.634], loss: 0.865298, mae: 0.803839, mean_q: 2.042445
[RESULT] FALSIFICATION!
 40124/100000: episode: 2494, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.012, 10.437], loss: 0.424949, mae: 0.587155, mean_q: 1.934185
[RESULT] FALSIFICATION!
 40125/100000: episode: 2495, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.012, 10.678], loss: 0.704534, mae: 0.773742, mean_q: 1.824596
[RESULT] FALSIFICATION!
 40126/100000: episode: 2496, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.013, 10.581], loss: 1.388095, mae: 1.041102, mean_q: 2.443222
[RESULT] FALSIFICATION!
 40127/100000: episode: 2497, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.012, 10.573], loss: 2.973084, mae: 1.523958, mean_q: 2.790013
[RESULT] FALSIFICATION!
 40128/100000: episode: 2498, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.014, 10.491], loss: 1.130477, mae: 0.967712, mean_q: 2.736390
[RESULT] FALSIFICATION!
 40129/100000: episode: 2499, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.012, 10.641], loss: 1.342536, mae: 0.991632, mean_q: 2.612541
[RESULT] FALSIFICATION!
 40130/100000: episode: 2500, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.420 [-0.015, 10.785], loss: 1.012897, mae: 0.908236, mean_q: 2.249862
[RESULT] FALSIFICATION!
 40131/100000: episode: 2501, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.014, 10.725], loss: 0.744193, mae: 0.813035, mean_q: 1.425338
[RESULT] FALSIFICATION!
 40132/100000: episode: 2502, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.013, 10.515], loss: 1.641887, mae: 1.139904, mean_q: 1.719868
[RESULT] FALSIFICATION!
 40133/100000: episode: 2503, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.511], loss: 1.123817, mae: 0.904571, mean_q: 1.641549
[RESULT] FALSIFICATION!
 40134/100000: episode: 2504, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.013, 10.643], loss: 1.446019, mae: 1.139048, mean_q: 3.341508
[RESULT] FALSIFICATION!
 40135/100000: episode: 2505, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.012, 10.669], loss: 0.639568, mae: 0.696602, mean_q: 2.298942
[RESULT] FALSIFICATION!
 40136/100000: episode: 2506, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.596], loss: 0.839112, mae: 0.920370, mean_q: 2.440209
[RESULT] FALSIFICATION!
 40137/100000: episode: 2507, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.012, 10.504], loss: 1.202134, mae: 0.738004, mean_q: 1.482415
[RESULT] FALSIFICATION!
 40138/100000: episode: 2508, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.013, 10.501], loss: 0.578624, mae: 0.722831, mean_q: 2.196085
[RESULT] FALSIFICATION!
 40139/100000: episode: 2509, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.011, 10.456], loss: 1.154707, mae: 0.870914, mean_q: 2.155484
[RESULT] FALSIFICATION!
 40140/100000: episode: 2510, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.013, 10.619], loss: 0.536214, mae: 0.604872, mean_q: 2.247304
[RESULT] FALSIFICATION!
 40141/100000: episode: 2511, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.012, 10.744], loss: 0.740077, mae: 0.685033, mean_q: 2.379938
[RESULT] FALSIFICATION!
 40142/100000: episode: 2512, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.012, 10.714], loss: 1.487479, mae: 1.126930, mean_q: 2.843844
[RESULT] FALSIFICATION!
 40143/100000: episode: 2513, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.011, 10.370], loss: 1.678727, mae: 1.205512, mean_q: 2.805983
[RESULT] FALSIFICATION!
 40144/100000: episode: 2514, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.012, 10.555], loss: 1.282203, mae: 1.026303, mean_q: 2.314643
[RESULT] FALSIFICATION!
 40145/100000: episode: 2515, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.013, 10.694], loss: 0.807906, mae: 0.784867, mean_q: 2.164620
[RESULT] FALSIFICATION!
 40146/100000: episode: 2516, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.013, 10.677], loss: 1.472882, mae: 1.083006, mean_q: 2.906093
[RESULT] FALSIFICATION!
 40147/100000: episode: 2517, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.011, 10.521], loss: 0.783181, mae: 0.781298, mean_q: 2.637170
[RESULT] FALSIFICATION!
 40148/100000: episode: 2518, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.013, 10.563], loss: 0.894903, mae: 0.889272, mean_q: 2.012268
[RESULT] FALSIFICATION!
 40149/100000: episode: 2519, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.012, 10.631], loss: 0.465229, mae: 0.696007, mean_q: 2.434421
[RESULT] FALSIFICATION!
 40150/100000: episode: 2520, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.013, 10.683], loss: 0.863037, mae: 0.850780, mean_q: 2.633023
[RESULT] FALSIFICATION!
 40151/100000: episode: 2521, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.013, 10.758], loss: 2.445187, mae: 1.254800, mean_q: 1.972237
[RESULT] FALSIFICATION!
 40152/100000: episode: 2522, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.013, 10.548], loss: 2.389776, mae: 1.394527, mean_q: 3.237219
[RESULT] FALSIFICATION!
 40153/100000: episode: 2523, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.013, 10.737], loss: 0.869584, mae: 0.879343, mean_q: 2.811612
[RESULT] FALSIFICATION!
 40154/100000: episode: 2524, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.012, 10.512], loss: 0.538720, mae: 0.671969, mean_q: 2.661115
[RESULT] FALSIFICATION!
 40155/100000: episode: 2525, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.010, 10.466], loss: 0.760648, mae: 0.593090, mean_q: 1.660447
[RESULT] FALSIFICATION!
 40156/100000: episode: 2526, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.583], loss: 1.365246, mae: 1.002289, mean_q: 2.611698
[RESULT] FALSIFICATION!
 40157/100000: episode: 2527, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.014, 10.565], loss: 1.038541, mae: 1.054481, mean_q: 2.780545
[RESULT] FALSIFICATION!
 40158/100000: episode: 2528, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.012, 10.522], loss: 0.670555, mae: 0.730235, mean_q: 2.575190
[RESULT] FALSIFICATION!
 40159/100000: episode: 2529, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.013, 10.569], loss: 1.091976, mae: 0.949366, mean_q: 2.733857
[RESULT] FALSIFICATION!
 40160/100000: episode: 2530, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.012, 10.537], loss: 0.734322, mae: 0.829395, mean_q: 2.062038
[RESULT] FALSIFICATION!
 40161/100000: episode: 2531, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.612], loss: 0.748648, mae: 0.767679, mean_q: 2.520449
[RESULT] FALSIFICATION!
 40162/100000: episode: 2532, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.013, 10.729], loss: 1.367782, mae: 1.042147, mean_q: 2.772631
[RESULT] FALSIFICATION!
 40163/100000: episode: 2533, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.343, 10.460], loss: 0.713217, mae: 0.697715, mean_q: 2.985521
[RESULT] FALSIFICATION!
 40164/100000: episode: 2534, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.013, 10.625], loss: 0.939780, mae: 0.892594, mean_q: 2.976810
[RESULT] FALSIFICATION!
 40165/100000: episode: 2535, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.015, 10.749], loss: 0.781843, mae: 0.754084, mean_q: 2.270574
[RESULT] FALSIFICATION!
 40166/100000: episode: 2536, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.012, 10.549], loss: 1.009241, mae: 0.918752, mean_q: 2.563526
[RESULT] FALSIFICATION!
 40167/100000: episode: 2537, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.013, 10.722], loss: 1.618420, mae: 1.075439, mean_q: 1.895890
[RESULT] FALSIFICATION!
 40168/100000: episode: 2538, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.012, 10.617], loss: 2.289141, mae: 1.174133, mean_q: 1.785741
[RESULT] FALSIFICATION!
 40169/100000: episode: 2539, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.014, 10.720], loss: 1.216971, mae: 1.086238, mean_q: 3.047480
[RESULT] FALSIFICATION!
 40170/100000: episode: 2540, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.012, 10.502], loss: 1.090398, mae: 1.085333, mean_q: 2.820988
[RESULT] FALSIFICATION!
 40171/100000: episode: 2541, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.014, 10.760], loss: 1.469043, mae: 0.973587, mean_q: 2.377177
[RESULT] FALSIFICATION!
 40172/100000: episode: 2542, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.013, 10.712], loss: 0.796000, mae: 0.806240, mean_q: 2.774351
[RESULT] FALSIFICATION!
 40173/100000: episode: 2543, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.012, 10.513], loss: 1.543245, mae: 0.905336, mean_q: 2.011702
[RESULT] FALSIFICATION!
 40174/100000: episode: 2544, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.316, 10.405], loss: 0.986098, mae: 0.912241, mean_q: 2.029198
[RESULT] FALSIFICATION!
 40175/100000: episode: 2545, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.014, 10.749], loss: 0.778985, mae: 0.789628, mean_q: 1.877002
[RESULT] FALSIFICATION!
 40176/100000: episode: 2546, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.012, 10.507], loss: 1.125762, mae: 0.799253, mean_q: 2.130033
[RESULT] FALSIFICATION!
 40177/100000: episode: 2547, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.013, 10.636], loss: 0.387904, mae: 0.605612, mean_q: 2.229763
[RESULT] FALSIFICATION!
 40178/100000: episode: 2548, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.013, 10.756], loss: 0.727256, mae: 0.734593, mean_q: 2.558454
[RESULT] FALSIFICATION!
 40179/100000: episode: 2549, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.070, 10.483], loss: 1.090693, mae: 0.812958, mean_q: 2.337745
[RESULT] FALSIFICATION!
 40180/100000: episode: 2550, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.013, 10.641], loss: 0.710633, mae: 0.802125, mean_q: 2.771094
[RESULT] FALSIFICATION!
 40181/100000: episode: 2551, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.753], loss: 0.433292, mae: 0.623973, mean_q: 3.160030
[RESULT] FALSIFICATION!
 40182/100000: episode: 2552, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.014, 10.778], loss: 0.760285, mae: 0.772994, mean_q: 2.179042
[RESULT] FALSIFICATION!
 40183/100000: episode: 2553, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.012, 10.679], loss: 2.576067, mae: 1.295297, mean_q: 2.569978
[RESULT] FALSIFICATION!
 40184/100000: episode: 2554, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.013, 10.594], loss: 2.397574, mae: 1.277815, mean_q: 2.831696
[RESULT] FALSIFICATION!
 40185/100000: episode: 2555, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.014, 10.589], loss: 0.388857, mae: 0.614979, mean_q: 2.300356
[RESULT] FALSIFICATION!
 40186/100000: episode: 2556, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.012, 10.584], loss: 0.676669, mae: 0.783849, mean_q: 2.561831
[RESULT] FALSIFICATION!
 40187/100000: episode: 2557, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.011, 10.537], loss: 1.128269, mae: 0.911078, mean_q: 2.769806
[RESULT] FALSIFICATION!
 40188/100000: episode: 2558, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.012, 10.423], loss: 1.179412, mae: 0.991840, mean_q: 3.393953
[RESULT] FALSIFICATION!
 40189/100000: episode: 2559, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.014, 10.588], loss: 0.579943, mae: 0.663933, mean_q: 2.567154
[RESULT] FALSIFICATION!
 40190/100000: episode: 2560, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.012, 10.398], loss: 0.428800, mae: 0.673623, mean_q: 2.832340
[RESULT] FALSIFICATION!
 40191/100000: episode: 2561, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.012, 10.760], loss: 0.876991, mae: 0.782295, mean_q: 2.181507
[RESULT] FALSIFICATION!
[Info] New level: 9.545731544494629 | Considering 15/85 traces
 40192/100000: episode: 2562, duration: 3.911s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.012, 10.612], loss: 0.777680, mae: 0.880888, mean_q: 2.521952
[RESULT] FALSIFICATION!
 40193/100000: episode: 2563, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.980, 10.396], loss: 1.149164, mae: 0.981666, mean_q: 2.675225
[RESULT] FALSIFICATION!
 40194/100000: episode: 2564, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.013, 10.739], loss: 0.708352, mae: 0.781209, mean_q: 2.660547
[RESULT] FALSIFICATION!
 40195/100000: episode: 2565, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.014, 10.526], loss: 0.922233, mae: 0.905712, mean_q: 2.668020
[RESULT] FALSIFICATION!
 40196/100000: episode: 2566, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.013, 10.518], loss: 0.953422, mae: 0.804767, mean_q: 3.052811
[RESULT] FALSIFICATION!
 40197/100000: episode: 2567, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.015, 10.727], loss: 0.747560, mae: 0.771473, mean_q: 2.141322
[RESULT] FALSIFICATION!
 40198/100000: episode: 2568, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.013, 10.583], loss: 1.329067, mae: 1.029134, mean_q: 3.006757
[RESULT] FALSIFICATION!
 40199/100000: episode: 2569, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.012, 10.584], loss: 1.291938, mae: 0.840250, mean_q: 1.830574
[RESULT] FALSIFICATION!
 40200/100000: episode: 2570, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.014, 10.692], loss: 1.136916, mae: 0.815479, mean_q: 1.800750
[RESULT] FALSIFICATION!
 40201/100000: episode: 2571, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.014, 10.704], loss: 0.384580, mae: 0.562366, mean_q: 2.411065
[RESULT] FALSIFICATION!
 40202/100000: episode: 2572, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.012, 10.604], loss: 0.309171, mae: 0.522044, mean_q: 2.248835
[RESULT] FALSIFICATION!
 40203/100000: episode: 2573, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.395 [-0.015, 10.804], loss: 0.935066, mae: 0.879351, mean_q: 2.485221
[RESULT] FALSIFICATION!
 40204/100000: episode: 2574, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.014, 10.755], loss: 0.684371, mae: 0.646962, mean_q: 1.681618
[RESULT] FALSIFICATION!
 40205/100000: episode: 2575, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.663], loss: 0.887438, mae: 0.681131, mean_q: 1.915050
[RESULT] FALSIFICATION!
 40206/100000: episode: 2576, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.013, 10.619], loss: 1.689770, mae: 0.997285, mean_q: 2.241051
[RESULT] FALSIFICATION!
 40207/100000: episode: 2577, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.013, 10.691], loss: 1.534101, mae: 0.892096, mean_q: 2.203378
[RESULT] FALSIFICATION!
 40208/100000: episode: 2578, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.429 [-0.015, 10.903], loss: 0.710813, mae: 0.746009, mean_q: 2.241814
[RESULT] FALSIFICATION!
 40209/100000: episode: 2579, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.012, 10.542], loss: 1.283773, mae: 1.031700, mean_q: 2.392516
[RESULT] FALSIFICATION!
 40210/100000: episode: 2580, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.804, 10.514], loss: 0.662132, mae: 0.647275, mean_q: 2.591038
[RESULT] FALSIFICATION!
 40211/100000: episode: 2581, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.012, 10.736], loss: 0.835398, mae: 0.682429, mean_q: 1.952220
[RESULT] FALSIFICATION!
 40212/100000: episode: 2582, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.013, 10.862], loss: 1.754589, mae: 1.156991, mean_q: 3.385913
[RESULT] FALSIFICATION!
 40213/100000: episode: 2583, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.014, 10.622], loss: 0.492335, mae: 0.597037, mean_q: 1.852839
[RESULT] FALSIFICATION!
 40214/100000: episode: 2584, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.014, 10.660], loss: 1.966803, mae: 0.942638, mean_q: 2.417185
[RESULT] FALSIFICATION!
 40215/100000: episode: 2585, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.012, 10.678], loss: 1.497275, mae: 0.790698, mean_q: 2.257790
[RESULT] FALSIFICATION!
 40216/100000: episode: 2586, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.015, 10.751], loss: 0.616791, mae: 0.661307, mean_q: 2.283532
[RESULT] FALSIFICATION!
 40217/100000: episode: 2587, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.015, 10.662], loss: 0.697456, mae: 0.648473, mean_q: 1.913175
[RESULT] FALSIFICATION!
 40218/100000: episode: 2588, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.013, 10.781], loss: 2.437989, mae: 1.245283, mean_q: 2.591720
[RESULT] FALSIFICATION!
 40219/100000: episode: 2589, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.012, 10.511], loss: 0.749490, mae: 0.897801, mean_q: 2.995667
[RESULT] FALSIFICATION!
 40220/100000: episode: 2590, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.013, 10.595], loss: 1.439848, mae: 1.015824, mean_q: 2.099884
[RESULT] FALSIFICATION!
 40221/100000: episode: 2591, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.012, 10.576], loss: 0.735368, mae: 0.615273, mean_q: 3.030684
[RESULT] FALSIFICATION!
 40222/100000: episode: 2592, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.013, 10.658], loss: 0.871717, mae: 0.792373, mean_q: 1.752095
[RESULT] FALSIFICATION!
 40223/100000: episode: 2593, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.014, 10.737], loss: 3.470614, mae: 1.536121, mean_q: 2.854342
[RESULT] FALSIFICATION!
 40224/100000: episode: 2594, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.012, 10.550], loss: 0.760959, mae: 0.795253, mean_q: 1.798343
[RESULT] FALSIFICATION!
 40225/100000: episode: 2595, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.015, 10.729], loss: 0.773092, mae: 0.836876, mean_q: 3.121359
[RESULT] FALSIFICATION!
 40226/100000: episode: 2596, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.014, 10.717], loss: 0.432016, mae: 0.607426, mean_q: 2.390877
[RESULT] FALSIFICATION!
 40227/100000: episode: 2597, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.013, 10.567], loss: 1.206413, mae: 0.915676, mean_q: 2.596424
[RESULT] FALSIFICATION!
 40228/100000: episode: 2598, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.534, 10.564], loss: 0.427928, mae: 0.595153, mean_q: 2.466561
[RESULT] FALSIFICATION!
 40229/100000: episode: 2599, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.011, 10.529], loss: 0.608475, mae: 0.728880, mean_q: 2.744315
[RESULT] FALSIFICATION!
 40230/100000: episode: 2600, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.012, 10.727], loss: 1.060884, mae: 0.823373, mean_q: 2.540335
[RESULT] FALSIFICATION!
 40231/100000: episode: 2601, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.014, 10.691], loss: 1.764107, mae: 1.023959, mean_q: 2.450464
[RESULT] FALSIFICATION!
 40232/100000: episode: 2602, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.012, 10.629], loss: 0.381346, mae: 0.569270, mean_q: 2.957752
[RESULT] FALSIFICATION!
 40233/100000: episode: 2603, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.014, 10.748], loss: 1.094113, mae: 0.902855, mean_q: 2.627039
[RESULT] FALSIFICATION!
 40234/100000: episode: 2604, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.064, 10.568], loss: 0.214854, mae: 0.439209, mean_q: 1.650956
[RESULT] FALSIFICATION!
 40235/100000: episode: 2605, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.014, 10.528], loss: 1.709743, mae: 1.031804, mean_q: 2.579153
[RESULT] FALSIFICATION!
 40236/100000: episode: 2606, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.015, 10.759], loss: 1.497790, mae: 1.192584, mean_q: 3.040593
[RESULT] FALSIFICATION!
 40237/100000: episode: 2607, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.013, 10.494], loss: 1.182586, mae: 1.149720, mean_q: 4.134569
[RESULT] FALSIFICATION!
 40238/100000: episode: 2608, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.014, 10.629], loss: 2.024854, mae: 1.298926, mean_q: 3.552145
[RESULT] FALSIFICATION!
 40239/100000: episode: 2609, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.013, 10.594], loss: 0.537727, mae: 0.612283, mean_q: 2.800375
[RESULT] FALSIFICATION!
 40240/100000: episode: 2610, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.013, 10.765], loss: 0.688562, mae: 0.637010, mean_q: 1.290781
[RESULT] FALSIFICATION!
 40241/100000: episode: 2611, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.013, 10.553], loss: 2.008596, mae: 1.153201, mean_q: 2.188850
[RESULT] FALSIFICATION!
 40242/100000: episode: 2612, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.013, 10.793], loss: 0.782901, mae: 0.674736, mean_q: 2.260738
[RESULT] FALSIFICATION!
 40243/100000: episode: 2613, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.013, 10.543], loss: 1.565989, mae: 1.107227, mean_q: 2.640983
[RESULT] FALSIFICATION!
 40244/100000: episode: 2614, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.012, 10.648], loss: 1.332711, mae: 1.126967, mean_q: 3.380014
[RESULT] FALSIFICATION!
 40245/100000: episode: 2615, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.155, 10.582], loss: 2.428592, mae: 1.132456, mean_q: 3.433862
[RESULT] FALSIFICATION!
 40246/100000: episode: 2616, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.013, 10.755], loss: 0.377748, mae: 0.609226, mean_q: 2.459525
[RESULT] FALSIFICATION!
 40247/100000: episode: 2617, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.904, 10.424], loss: 2.281125, mae: 1.013643, mean_q: 1.662059
[RESULT] FALSIFICATION!
 40248/100000: episode: 2618, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.460, 10.666], loss: 2.084467, mae: 1.052532, mean_q: 1.905418
[RESULT] FALSIFICATION!
 40249/100000: episode: 2619, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.012, 10.637], loss: 0.602249, mae: 0.728510, mean_q: 3.279925
[RESULT] FALSIFICATION!
 40250/100000: episode: 2620, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.625, 10.400], loss: 2.946977, mae: 1.353395, mean_q: 3.334753
[RESULT] FALSIFICATION!
 40251/100000: episode: 2621, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.261, 10.463], loss: 1.511001, mae: 1.118473, mean_q: 3.074260
[RESULT] FALSIFICATION!
 40252/100000: episode: 2622, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.129, 10.489], loss: 0.390349, mae: 0.536590, mean_q: 2.256418
[RESULT] FALSIFICATION!
 40253/100000: episode: 2623, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.016, 10.734], loss: 0.600476, mae: 0.642210, mean_q: 2.229119
[RESULT] FALSIFICATION!
 40254/100000: episode: 2624, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.013, 10.804], loss: 0.359807, mae: 0.600230, mean_q: 1.469956
[RESULT] FALSIFICATION!
 40255/100000: episode: 2625, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.011, 10.576], loss: 0.663769, mae: 0.740213, mean_q: 1.773434
[RESULT] FALSIFICATION!
 40256/100000: episode: 2626, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.014, 10.855], loss: 2.541563, mae: 1.084896, mean_q: 2.356057
[RESULT] FALSIFICATION!
 40257/100000: episode: 2627, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.393 [-0.015, 10.842], loss: 2.807415, mae: 1.559836, mean_q: 3.681907
[RESULT] FALSIFICATION!
 40258/100000: episode: 2628, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.013, 10.743], loss: 0.611872, mae: 0.826531, mean_q: 3.300162
[RESULT] FALSIFICATION!
 40259/100000: episode: 2629, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.014, 10.592], loss: 1.010643, mae: 1.005898, mean_q: 3.536052
[RESULT] FALSIFICATION!
 40260/100000: episode: 2630, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.014, 10.640], loss: 1.143957, mae: 0.867327, mean_q: 2.248235
[RESULT] FALSIFICATION!
 40261/100000: episode: 2631, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.014, 10.822], loss: 1.274607, mae: 0.878965, mean_q: 2.870878
[RESULT] FALSIFICATION!
 40262/100000: episode: 2632, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.012, 10.705], loss: 0.652386, mae: 0.673882, mean_q: 2.257017
[RESULT] FALSIFICATION!
 40263/100000: episode: 2633, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.014, 10.572], loss: 1.699799, mae: 1.011874, mean_q: 2.228359
[RESULT] FALSIFICATION!
 40264/100000: episode: 2634, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.281, 10.568], loss: 1.355280, mae: 0.853462, mean_q: 2.049118
[RESULT] FALSIFICATION!
 40265/100000: episode: 2635, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.014, 10.649], loss: 0.991606, mae: 0.929410, mean_q: 3.198375
[RESULT] FALSIFICATION!
 40266/100000: episode: 2636, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.013, 10.674], loss: 1.178024, mae: 0.904003, mean_q: 2.289553
[RESULT] FALSIFICATION!
 40267/100000: episode: 2637, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.584], loss: 2.262069, mae: 1.171835, mean_q: 3.097959
[RESULT] FALSIFICATION!
 40268/100000: episode: 2638, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.014, 10.818], loss: 0.548923, mae: 0.615025, mean_q: 2.264965
[RESULT] FALSIFICATION!
 40269/100000: episode: 2639, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.014, 10.603], loss: 3.291930, mae: 1.514876, mean_q: 2.898896
[RESULT] FALSIFICATION!
 40270/100000: episode: 2640, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.014, 10.654], loss: 1.708836, mae: 1.240779, mean_q: 3.118961
[RESULT] FALSIFICATION!
 40271/100000: episode: 2641, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.015, 10.721], loss: 0.761579, mae: 0.757095, mean_q: 2.739003
[RESULT] FALSIFICATION!
 40272/100000: episode: 2642, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.013, 10.552], loss: 0.627386, mae: 0.790749, mean_q: 2.512404
[RESULT] FALSIFICATION!
 40273/100000: episode: 2643, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.012, 10.543], loss: 1.386345, mae: 1.031126, mean_q: 2.520972
[RESULT] FALSIFICATION!
 40274/100000: episode: 2644, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.013, 10.560], loss: 1.172370, mae: 0.881296, mean_q: 2.212507
[RESULT] FALSIFICATION!
 40275/100000: episode: 2645, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.015, 10.807], loss: 1.194886, mae: 0.863831, mean_q: 2.697325
[RESULT] FALSIFICATION!
 40276/100000: episode: 2646, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.683, 10.579], loss: 1.559220, mae: 1.046399, mean_q: 3.628665
[RESULT] FALSIFICATION!
[Info] New level: 10.473820686340332 | Considering 10/90 traces
 40277/100000: episode: 2647, duration: 4.458s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.014, 10.742], loss: 1.101530, mae: 0.832981, mean_q: 2.531208
[RESULT] FALSIFICATION!
 40278/100000: episode: 2648, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.012, 10.622], loss: 0.875952, mae: 0.722016, mean_q: 2.641306
[RESULT] FALSIFICATION!
 40279/100000: episode: 2649, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.014, 10.628], loss: 1.810664, mae: 1.091353, mean_q: 3.053801
[RESULT] FALSIFICATION!
 40280/100000: episode: 2650, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.015, 10.918], loss: 0.791860, mae: 0.663636, mean_q: 2.747406
[RESULT] FALSIFICATION!
 40281/100000: episode: 2651, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.015, 10.710], loss: 0.760827, mae: 0.566630, mean_q: 2.061808
[RESULT] FALSIFICATION!
 40282/100000: episode: 2652, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.015, 10.694], loss: 0.941172, mae: 0.801202, mean_q: 3.193059
[RESULT] FALSIFICATION!
 40283/100000: episode: 2653, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.416 [-0.016, 10.983], loss: 1.961768, mae: 1.084690, mean_q: 3.265981
[RESULT] FALSIFICATION!
 40284/100000: episode: 2654, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.035, 10.701], loss: 0.799557, mae: 0.570231, mean_q: 2.356751
[RESULT] FALSIFICATION!
 40285/100000: episode: 2655, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.012, 10.591], loss: 1.595926, mae: 1.031022, mean_q: 2.813776
[RESULT] FALSIFICATION!
 40286/100000: episode: 2656, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.013, 10.633], loss: 0.693362, mae: 0.544239, mean_q: 2.447374
[RESULT] FALSIFICATION!
 40287/100000: episode: 2657, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.014, 10.651], loss: 1.037242, mae: 0.906534, mean_q: 2.638052
[RESULT] FALSIFICATION!
 40288/100000: episode: 2658, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.013, 10.599], loss: 0.816914, mae: 0.738023, mean_q: 2.399910
[RESULT] FALSIFICATION!
 40289/100000: episode: 2659, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.014, 10.829], loss: 0.720270, mae: 0.762934, mean_q: 3.825778
[RESULT] FALSIFICATION!
 40290/100000: episode: 2660, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.014, 10.753], loss: 0.747247, mae: 0.677881, mean_q: 2.092584
[RESULT] FALSIFICATION!
 40291/100000: episode: 2661, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.014, 10.805], loss: 0.814028, mae: 0.724302, mean_q: 2.826467
[RESULT] FALSIFICATION!
 40292/100000: episode: 2662, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.014, 10.826], loss: 0.429064, mae: 0.617622, mean_q: 3.040526
[RESULT] FALSIFICATION!
 40293/100000: episode: 2663, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.015, 10.881], loss: 0.807639, mae: 0.896242, mean_q: 2.900813
[RESULT] FALSIFICATION!
 40294/100000: episode: 2664, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.014, 10.679], loss: 0.272864, mae: 0.523220, mean_q: 1.437435
[RESULT] FALSIFICATION!
 40295/100000: episode: 2665, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.014, 10.625], loss: 0.691353, mae: 0.662346, mean_q: 2.232975
[RESULT] FALSIFICATION!
 40296/100000: episode: 2666, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.013, 10.664], loss: 2.649738, mae: 1.172530, mean_q: 2.126488
[RESULT] FALSIFICATION!
 40297/100000: episode: 2667, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.015, 10.627], loss: 2.719500, mae: 1.403625, mean_q: 3.455781
[RESULT] FALSIFICATION!
 40298/100000: episode: 2668, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.015, 10.487], loss: 0.558723, mae: 0.601045, mean_q: 3.891482
[RESULT] FALSIFICATION!
 40299/100000: episode: 2669, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.625], loss: 0.726029, mae: 0.670583, mean_q: 2.769237
[RESULT] FALSIFICATION!
 40300/100000: episode: 2670, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.413 [-0.017, 10.738], loss: 1.159302, mae: 1.040750, mean_q: 3.275739
[RESULT] FALSIFICATION!
 40301/100000: episode: 2671, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.015, 10.721], loss: 1.594497, mae: 1.067657, mean_q: 1.986070
[RESULT] FALSIFICATION!
 40302/100000: episode: 2672, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.015, 10.756], loss: 0.466813, mae: 0.582199, mean_q: 2.801512
[RESULT] FALSIFICATION!
 40303/100000: episode: 2673, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.230, 10.410], loss: 1.185148, mae: 0.943405, mean_q: 3.495488
[RESULT] FALSIFICATION!
 40304/100000: episode: 2674, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.015, 10.975], loss: 0.636352, mae: 0.739935, mean_q: 3.911085
[RESULT] FALSIFICATION!
 40305/100000: episode: 2675, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.014, 10.649], loss: 1.657029, mae: 1.072160, mean_q: 2.746016
[RESULT] FALSIFICATION!
 40306/100000: episode: 2676, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.015, 10.825], loss: 1.736836, mae: 0.931025, mean_q: 2.298434
[RESULT] FALSIFICATION!
 40307/100000: episode: 2677, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.035, 10.679], loss: 0.985794, mae: 0.637911, mean_q: 1.660261
[RESULT] FALSIFICATION!
 40308/100000: episode: 2678, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.014, 10.918], loss: 1.279541, mae: 0.790333, mean_q: 2.324161
[RESULT] FALSIFICATION!
 40309/100000: episode: 2679, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.438 [-0.035, 10.808], loss: 0.785046, mae: 0.755848, mean_q: 2.969613
[RESULT] FALSIFICATION!
 40310/100000: episode: 2680, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.014, 10.770], loss: 0.766007, mae: 0.834074, mean_q: 2.568678
[RESULT] FALSIFICATION!
 40311/100000: episode: 2681, duration: 0.016s, episode steps: 1, steps per second: 63, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.014, 10.658], loss: 0.613933, mae: 0.668267, mean_q: 3.635702
[RESULT] FALSIFICATION!
 40312/100000: episode: 2682, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.622], loss: 1.280078, mae: 0.983937, mean_q: 3.294952
[RESULT] FALSIFICATION!
 40313/100000: episode: 2683, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.015, 10.689], loss: 1.230698, mae: 1.024056, mean_q: 2.978995
[RESULT] FALSIFICATION!
 40314/100000: episode: 2684, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.012, 10.540], loss: 0.604897, mae: 0.757952, mean_q: 2.941071
[RESULT] FALSIFICATION!
 40315/100000: episode: 2685, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.016, 10.870], loss: 2.336215, mae: 1.019133, mean_q: 2.255377
[RESULT] FALSIFICATION!
 40316/100000: episode: 2686, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.014, 10.765], loss: 0.866962, mae: 0.694046, mean_q: 2.964729
[RESULT] FALSIFICATION!
 40317/100000: episode: 2687, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.016, 10.780], loss: 1.202217, mae: 0.821910, mean_q: 2.939024
[RESULT] FALSIFICATION!
 40318/100000: episode: 2688, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.382 [-0.014, 10.840], loss: 0.477508, mae: 0.671338, mean_q: 2.629683
[RESULT] FALSIFICATION!
 40319/100000: episode: 2689, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.015, 10.805], loss: 0.898701, mae: 0.662345, mean_q: 2.484729
[RESULT] FALSIFICATION!
 40320/100000: episode: 2690, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.378 [-0.016, 10.905], loss: 3.327613, mae: 1.221459, mean_q: 2.372747
[RESULT] FALSIFICATION!
 40321/100000: episode: 2691, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.015, 10.602], loss: 0.557405, mae: 0.628112, mean_q: 2.424704
[RESULT] FALSIFICATION!
 40322/100000: episode: 2692, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.014, 10.775], loss: 1.021855, mae: 0.862151, mean_q: 3.999878
[RESULT] FALSIFICATION!
 40323/100000: episode: 2693, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.014, 10.537], loss: 0.603720, mae: 0.576361, mean_q: 2.036582
[RESULT] FALSIFICATION!
 40324/100000: episode: 2694, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.362, 10.470], loss: 1.245202, mae: 0.885260, mean_q: 3.329608
[RESULT] FALSIFICATION!
 40325/100000: episode: 2695, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.480, 10.612], loss: 0.738087, mae: 0.609677, mean_q: 2.225783
[RESULT] FALSIFICATION!
 40326/100000: episode: 2696, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.035, 10.734], loss: 0.577750, mae: 0.637033, mean_q: 2.740890
[RESULT] FALSIFICATION!
 40327/100000: episode: 2697, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.015, 10.666], loss: 0.920649, mae: 0.676800, mean_q: 2.108332
[RESULT] FALSIFICATION!
 40328/100000: episode: 2698, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.439], loss: 0.100758, mae: 0.308737, mean_q: 1.999955
[RESULT] FALSIFICATION!
 40329/100000: episode: 2699, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.015, 10.872], loss: 2.379366, mae: 0.928676, mean_q: 1.661836
[RESULT] FALSIFICATION!
 40330/100000: episode: 2700, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.011, 10.700], loss: 2.325935, mae: 1.223907, mean_q: 3.236755
[RESULT] FALSIFICATION!
 40331/100000: episode: 2701, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.013, 10.815], loss: 0.948347, mae: 0.797146, mean_q: 2.786214
[RESULT] FALSIFICATION!
 40332/100000: episode: 2702, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.417 [-0.015, 10.867], loss: 2.531884, mae: 1.338197, mean_q: 3.745812
[RESULT] FALSIFICATION!
 40333/100000: episode: 2703, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.012, 10.658], loss: 0.719652, mae: 0.767381, mean_q: 1.891831
[RESULT] FALSIFICATION!
 40334/100000: episode: 2704, duration: 0.021s, episode steps: 1, steps per second: 49, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.401 [-0.015, 10.748], loss: 0.599275, mae: 0.621059, mean_q: 2.243949
[RESULT] FALSIFICATION!
 40335/100000: episode: 2705, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.015, 10.891], loss: 1.143550, mae: 1.068749, mean_q: 3.062650
[RESULT] FALSIFICATION!
 40336/100000: episode: 2706, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.252, 10.459], loss: 0.730031, mae: 0.727871, mean_q: 2.693586
[RESULT] FALSIFICATION!
 40337/100000: episode: 2707, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.343 [-0.015, 10.699], loss: 0.347945, mae: 0.523383, mean_q: 1.946877
[RESULT] FALSIFICATION!
 40338/100000: episode: 2708, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.014, 10.714], loss: 0.802930, mae: 0.891452, mean_q: 3.390920
[RESULT] FALSIFICATION!
 40339/100000: episode: 2709, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.014, 10.751], loss: 1.302647, mae: 0.715487, mean_q: 2.456789
[RESULT] FALSIFICATION!
 40340/100000: episode: 2710, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.015, 10.714], loss: 0.806863, mae: 0.616579, mean_q: 2.045474
[RESULT] FALSIFICATION!
 40341/100000: episode: 2711, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.014, 10.927], loss: 0.726054, mae: 0.672810, mean_q: 2.459965
[RESULT] FALSIFICATION!
 40342/100000: episode: 2712, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.389 [-0.014, 10.805], loss: 0.821614, mae: 0.601488, mean_q: 1.584868
[RESULT] FALSIFICATION!
 40343/100000: episode: 2713, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.013, 10.709], loss: 1.003079, mae: 0.859885, mean_q: 3.411015
[RESULT] FALSIFICATION!
 40344/100000: episode: 2714, duration: 0.027s, episode steps: 1, steps per second: 37, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.254, 10.534], loss: 0.793921, mae: 0.784132, mean_q: 3.814106
[RESULT] FALSIFICATION!
 40345/100000: episode: 2715, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.012, 10.652], loss: 0.522213, mae: 0.612768, mean_q: 2.983729
[RESULT] FALSIFICATION!
 40346/100000: episode: 2716, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.015, 10.761], loss: 0.354285, mae: 0.548741, mean_q: 2.969662
[RESULT] FALSIFICATION!
 40347/100000: episode: 2717, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.015, 10.678], loss: 2.079743, mae: 0.921643, mean_q: 2.765399
[RESULT] FALSIFICATION!
 40348/100000: episode: 2718, duration: 0.019s, episode steps: 1, steps per second: 53, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.383 [-0.014, 10.837], loss: 3.452776, mae: 1.397666, mean_q: 1.979370
[RESULT] FALSIFICATION!
 40349/100000: episode: 2719, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.394 [-0.016, 10.842], loss: 1.916291, mae: 1.164221, mean_q: 3.222024
[RESULT] FALSIFICATION!
 40350/100000: episode: 2720, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.013, 10.746], loss: 1.779161, mae: 1.291697, mean_q: 3.726786
[RESULT] FALSIFICATION!
 40351/100000: episode: 2721, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.013, 10.753], loss: 1.995644, mae: 1.369437, mean_q: 4.588798
[RESULT] FALSIFICATION!
 40352/100000: episode: 2722, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.014, 10.756], loss: 2.094737, mae: 1.037707, mean_q: 3.421078
[RESULT] FALSIFICATION!
 40353/100000: episode: 2723, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.013, 10.739], loss: 1.752002, mae: 0.799139, mean_q: 2.087066
[RESULT] FALSIFICATION!
 40354/100000: episode: 2724, duration: 0.021s, episode steps: 1, steps per second: 49, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.014, 10.600], loss: 1.889892, mae: 0.887747, mean_q: 2.518260
[RESULT] FALSIFICATION!
 40355/100000: episode: 2725, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.014, 10.717], loss: 1.038766, mae: 0.932639, mean_q: 3.577887
[RESULT] FALSIFICATION!
 40356/100000: episode: 2726, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.015, 10.869], loss: 1.405858, mae: 1.009051, mean_q: 3.095506
[RESULT] FALSIFICATION!
 40357/100000: episode: 2727, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.014, 10.715], loss: 1.074235, mae: 0.879410, mean_q: 3.519692
[RESULT] FALSIFICATION!
 40358/100000: episode: 2728, duration: 0.027s, episode steps: 1, steps per second: 36, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.014, 10.778], loss: 1.023331, mae: 0.768497, mean_q: 1.873901
[RESULT] FALSIFICATION!
 40359/100000: episode: 2729, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.014, 10.622], loss: 2.628294, mae: 1.165985, mean_q: 2.733997
[RESULT] FALSIFICATION!
 40360/100000: episode: 2730, duration: 0.025s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.014, 10.564], loss: 0.333943, mae: 0.486071, mean_q: 1.779724
[RESULT] FALSIFICATION!
 40361/100000: episode: 2731, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.345 [-0.015, 10.858], loss: 1.268269, mae: 0.907750, mean_q: 3.096944
[RESULT] FALSIFICATION!
 40362/100000: episode: 2732, duration: 0.035s, episode steps: 1, steps per second: 28, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.015, 10.737], loss: 0.510478, mae: 0.557843, mean_q: 2.952327
[RESULT] FALSIFICATION!
 40363/100000: episode: 2733, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.013, 10.571], loss: 0.933954, mae: 0.944531, mean_q: 3.721005
[RESULT] FALSIFICATION!
 40364/100000: episode: 2734, duration: 0.021s, episode steps: 1, steps per second: 47, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.013, 10.693], loss: 0.813666, mae: 0.680319, mean_q: 2.082356
[RESULT] FALSIFICATION!
 40365/100000: episode: 2735, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.014, 10.760], loss: 0.638336, mae: 0.571674, mean_q: 2.415732
[RESULT] FALSIFICATION!
 40366/100000: episode: 2736, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.013, 10.795], loss: 0.687265, mae: 0.520975, mean_q: 2.206419
[RESULT] FALSIFICATION!
[Info] New level: 10.947945594787598 | Considering 10/90 traces
 40367/100000: episode: 2737, duration: 8.676s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.014, 10.757], loss: 1.120389, mae: 0.734637, mean_q: 2.771662
[RESULT] FALSIFICATION!
 40368/100000: episode: 2738, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.015, 10.762], loss: 0.357001, mae: 0.547292, mean_q: 3.176389
[RESULT] FALSIFICATION!
 40369/100000: episode: 2739, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.116, 10.636], loss: 0.588336, mae: 0.768726, mean_q: 3.068075
[RESULT] FALSIFICATION!
 40370/100000: episode: 2740, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.014, 10.745], loss: 0.934664, mae: 0.780425, mean_q: 4.094698
[RESULT] FALSIFICATION!
 40371/100000: episode: 2741, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.381 [-0.035, 10.930], loss: 1.776690, mae: 0.952711, mean_q: 3.248135
[RESULT] FALSIFICATION!
 40372/100000: episode: 2742, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.013, 10.874], loss: 0.795002, mae: 0.805236, mean_q: 3.371463
[RESULT] FALSIFICATION!
 40373/100000: episode: 2743, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.354 [-0.015, 10.923], loss: 0.893250, mae: 0.936467, mean_q: 3.168084
[RESULT] FALSIFICATION!
 40374/100000: episode: 2744, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.015, 10.779], loss: 1.044966, mae: 0.931336, mean_q: 3.239253
[RESULT] FALSIFICATION!
 40375/100000: episode: 2745, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.014, 10.843], loss: 1.203225, mae: 0.924467, mean_q: 3.527357
[RESULT] FALSIFICATION!
 40376/100000: episode: 2746, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.015, 10.887], loss: 1.584884, mae: 0.898479, mean_q: 3.409575
[RESULT] FALSIFICATION!
 40377/100000: episode: 2747, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.016, 10.888], loss: 0.730335, mae: 0.672588, mean_q: 4.258157
[RESULT] FALSIFICATION!
 40378/100000: episode: 2748, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.015, 10.856], loss: 2.820672, mae: 1.420172, mean_q: 3.158823
[RESULT] FALSIFICATION!
 40379/100000: episode: 2749, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.344, 10.668], loss: 0.689993, mae: 0.765148, mean_q: 3.112509
[RESULT] FALSIFICATION!
 40380/100000: episode: 2750, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.015, 10.799], loss: 3.116893, mae: 1.167970, mean_q: 2.781954
[RESULT] FALSIFICATION!
 40381/100000: episode: 2751, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.013, 10.736], loss: 0.454672, mae: 0.498580, mean_q: 2.368673
[RESULT] FALSIFICATION!
 40382/100000: episode: 2752, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.358 [-0.015, 10.818], loss: 0.526160, mae: 0.549492, mean_q: 2.343493
[RESULT] FALSIFICATION!
 40383/100000: episode: 2753, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.388 [-0.035, 10.837], loss: 1.157831, mae: 0.905187, mean_q: 4.543595
[RESULT] FALSIFICATION!
 40384/100000: episode: 2754, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.014, 10.733], loss: 0.759179, mae: 0.733585, mean_q: 3.685519
[RESULT] FALSIFICATION!
 40385/100000: episode: 2755, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.035, 10.762], loss: 0.750865, mae: 0.605746, mean_q: 2.700279
[RESULT] FALSIFICATION!
 40386/100000: episode: 2756, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.080, 10.777], loss: 1.243984, mae: 0.855677, mean_q: 2.272872
[RESULT] FALSIFICATION!
 40387/100000: episode: 2757, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.015, 10.844], loss: 3.158158, mae: 1.221189, mean_q: 2.403101
[RESULT] FALSIFICATION!
 40388/100000: episode: 2758, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.013, 10.848], loss: 1.162811, mae: 0.934241, mean_q: 3.587221
[RESULT] FALSIFICATION!
 40389/100000: episode: 2759, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.384 [-0.016, 10.936], loss: 0.629111, mae: 0.533327, mean_q: 2.262455
[RESULT] FALSIFICATION!
 40390/100000: episode: 2760, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.015, 10.892], loss: 0.492387, mae: 0.655350, mean_q: 2.912367
[RESULT] FALSIFICATION!
 40391/100000: episode: 2761, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.014, 10.778], loss: 1.661441, mae: 0.887722, mean_q: 3.559733
[RESULT] FALSIFICATION!
 40392/100000: episode: 2762, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.014, 10.739], loss: 1.236804, mae: 0.790126, mean_q: 2.741530
[RESULT] FALSIFICATION!
 40393/100000: episode: 2763, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.015, 10.741], loss: 1.177031, mae: 0.943686, mean_q: 3.346507
[RESULT] FALSIFICATION!
 40394/100000: episode: 2764, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.406 [-0.017, 10.853], loss: 0.376235, mae: 0.576590, mean_q: 3.763842
[RESULT] FALSIFICATION!
 40395/100000: episode: 2765, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.022, 10.689], loss: 0.870504, mae: 0.754541, mean_q: 3.237116
[RESULT] FALSIFICATION!
 40396/100000: episode: 2766, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.665], loss: 1.371735, mae: 0.943939, mean_q: 3.724374
[RESULT] FALSIFICATION!
 40397/100000: episode: 2767, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.014, 10.833], loss: 0.576762, mae: 0.682032, mean_q: 3.130610
[RESULT] FALSIFICATION!
 40398/100000: episode: 2768, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.014, 10.838], loss: 1.783015, mae: 0.983595, mean_q: 2.649899
[RESULT] FALSIFICATION!
 40399/100000: episode: 2769, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.014, 10.721], loss: 0.883077, mae: 0.711018, mean_q: 2.311885
[RESULT] FALSIFICATION!
 40400/100000: episode: 2770, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.015, 10.880], loss: 1.853520, mae: 1.145974, mean_q: 3.401484
[RESULT] FALSIFICATION!
 40401/100000: episode: 2771, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.015, 10.936], loss: 0.920260, mae: 0.812250, mean_q: 3.450648
[RESULT] FALSIFICATION!
 40402/100000: episode: 2772, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.014, 10.816], loss: 2.141511, mae: 1.237506, mean_q: 3.377786
[RESULT] FALSIFICATION!
 40403/100000: episode: 2773, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.403, 10.642], loss: 0.736054, mae: 0.626463, mean_q: 1.957435
[RESULT] FALSIFICATION!
 40404/100000: episode: 2774, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.374 [-0.016, 10.798], loss: 1.514110, mae: 0.887103, mean_q: 3.271458
[RESULT] FALSIFICATION!
 40405/100000: episode: 2775, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.480, 10.772], loss: 0.653095, mae: 0.656601, mean_q: 3.227804
[RESULT] FALSIFICATION!
 40406/100000: episode: 2776, duration: 0.022s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.015, 10.825], loss: 1.195519, mae: 0.969987, mean_q: 3.566915
[RESULT] FALSIFICATION!
 40407/100000: episode: 2777, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.424 [-0.017, 10.944], loss: 0.937618, mae: 0.941844, mean_q: 3.392545
[RESULT] FALSIFICATION!
 40408/100000: episode: 2778, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.373 [-0.015, 10.873], loss: 3.516567, mae: 1.214971, mean_q: 2.972171
[RESULT] FALSIFICATION!
 40409/100000: episode: 2779, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.014, 10.879], loss: 0.997339, mae: 0.653059, mean_q: 2.574577
[RESULT] FALSIFICATION!
 40410/100000: episode: 2780, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.815], loss: 0.466668, mae: 0.602510, mean_q: 2.607137
[RESULT] FALSIFICATION!
 40411/100000: episode: 2781, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.015, 10.827], loss: 1.423770, mae: 1.038769, mean_q: 2.970132
[RESULT] FALSIFICATION!
 40412/100000: episode: 2782, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.014, 10.744], loss: 2.069724, mae: 1.246203, mean_q: 4.672217
[RESULT] FALSIFICATION!
 40413/100000: episode: 2783, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.410, 10.827], loss: 0.544558, mae: 0.641176, mean_q: 3.042349
[RESULT] FALSIFICATION!
 40414/100000: episode: 2784, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.410 [-0.017, 10.932], loss: 0.723304, mae: 0.719973, mean_q: 2.746006
[RESULT] FALSIFICATION!
 40415/100000: episode: 2785, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.014, 10.971], loss: 2.367382, mae: 1.314306, mean_q: 2.840040
[RESULT] FALSIFICATION!
 40416/100000: episode: 2786, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.360 [-0.035, 10.819], loss: 0.247170, mae: 0.527852, mean_q: 2.060654
[RESULT] FALSIFICATION!
 40417/100000: episode: 2787, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.400 [-0.016, 10.942], loss: 2.122864, mae: 1.127827, mean_q: 1.991966
[RESULT] FALSIFICATION!
 40418/100000: episode: 2788, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.016, 10.861], loss: 0.744583, mae: 0.682780, mean_q: 3.479531
[RESULT] FALSIFICATION!
 40419/100000: episode: 2789, duration: 0.018s, episode steps: 1, steps per second: 54, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.015, 10.941], loss: 0.541754, mae: 0.582478, mean_q: 2.984306
[RESULT] FALSIFICATION!
 40420/100000: episode: 2790, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.408 [-0.016, 11.014], loss: 1.452094, mae: 0.985757, mean_q: 4.250882
[RESULT] FALSIFICATION!
 40421/100000: episode: 2791, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.016, 10.832], loss: 0.788040, mae: 0.943308, mean_q: 3.758055
[RESULT] FALSIFICATION!
 40422/100000: episode: 2792, duration: 0.021s, episode steps: 1, steps per second: 49, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.015, 10.881], loss: 1.201392, mae: 0.756418, mean_q: 2.806141
[RESULT] FALSIFICATION!
 40423/100000: episode: 2793, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.015, 10.900], loss: 0.724172, mae: 0.612814, mean_q: 3.367442
[RESULT] FALSIFICATION!
 40424/100000: episode: 2794, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.015, 10.743], loss: 1.672254, mae: 0.996514, mean_q: 2.547717
[RESULT] FALSIFICATION!
 40425/100000: episode: 2795, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.014, 10.821], loss: 0.689405, mae: 0.774867, mean_q: 3.605847
[RESULT] FALSIFICATION!
 40426/100000: episode: 2796, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.014, 10.837], loss: 0.687413, mae: 0.672174, mean_q: 2.395238
[RESULT] FALSIFICATION!
 40427/100000: episode: 2797, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.014, 10.955], loss: 1.234492, mae: 0.867184, mean_q: 3.675931
[RESULT] FALSIFICATION!
 40428/100000: episode: 2798, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.013, 10.783], loss: 1.614874, mae: 1.131622, mean_q: 3.203855
[RESULT] FALSIFICATION!
 40429/100000: episode: 2799, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.015, 10.726], loss: 1.289293, mae: 0.797704, mean_q: 2.616445
[RESULT] FALSIFICATION!
 40430/100000: episode: 2800, duration: 0.028s, episode steps: 1, steps per second: 36, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.260, 10.728], loss: 0.821840, mae: 0.731813, mean_q: 2.003211
[RESULT] FALSIFICATION!
 40431/100000: episode: 2801, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.015, 10.865], loss: 0.745591, mae: 0.636736, mean_q: 3.198774
[RESULT] FALSIFICATION!
 40432/100000: episode: 2802, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.346 [-0.015, 10.829], loss: 2.028578, mae: 1.144280, mean_q: 2.796258
[RESULT] FALSIFICATION!
 40433/100000: episode: 2803, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.013, 10.729], loss: 0.128743, mae: 0.296150, mean_q: 2.467495
[RESULT] FALSIFICATION!
 40434/100000: episode: 2804, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.127, 10.558], loss: 1.513167, mae: 1.074025, mean_q: 3.509573
[RESULT] FALSIFICATION!
 40435/100000: episode: 2805, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.391 [-0.016, 10.900], loss: 0.842420, mae: 0.817852, mean_q: 3.626172
[RESULT] FALSIFICATION!
 40436/100000: episode: 2806, duration: 0.027s, episode steps: 1, steps per second: 37, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.014, 10.735], loss: 2.037045, mae: 1.143197, mean_q: 3.984011
[RESULT] FALSIFICATION!
 40437/100000: episode: 2807, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.016, 10.891], loss: 0.205468, mae: 0.398866, mean_q: 1.913460
[RESULT] FALSIFICATION!
 40438/100000: episode: 2808, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.014, 10.787], loss: 2.884799, mae: 1.148595, mean_q: 2.425121
[RESULT] FALSIFICATION!
 40439/100000: episode: 2809, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.014, 10.731], loss: 0.439248, mae: 0.592445, mean_q: 3.124745
[RESULT] FALSIFICATION!
 40440/100000: episode: 2810, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.014, 10.714], loss: 0.524630, mae: 0.756585, mean_q: 3.413766
[RESULT] FALSIFICATION!
 40441/100000: episode: 2811, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.013, 10.883], loss: 0.632555, mae: 0.731257, mean_q: 3.364835
[RESULT] FALSIFICATION!
 40442/100000: episode: 2812, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.015, 10.990], loss: 1.069879, mae: 0.918624, mean_q: 3.202254
[RESULT] FALSIFICATION!
 40443/100000: episode: 2813, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.015, 10.895], loss: 1.338187, mae: 1.045771, mean_q: 3.541728
[RESULT] FALSIFICATION!
 40444/100000: episode: 2814, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.014, 10.841], loss: 0.242884, mae: 0.479111, mean_q: 3.709690
[RESULT] FALSIFICATION!
 40445/100000: episode: 2815, duration: 0.027s, episode steps: 1, steps per second: 37, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.377 [-0.016, 10.937], loss: 0.665062, mae: 0.719286, mean_q: 2.498597
[RESULT] FALSIFICATION!
 40446/100000: episode: 2816, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.014, 10.810], loss: 1.012010, mae: 0.728146, mean_q: 2.022782
[RESULT] FALSIFICATION!
 40447/100000: episode: 2817, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.015, 10.881], loss: 0.614503, mae: 0.586486, mean_q: 2.124569
[RESULT] FALSIFICATION!
 40448/100000: episode: 2818, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.372 [-0.016, 10.885], loss: 0.907017, mae: 0.733625, mean_q: 3.220326
[RESULT] FALSIFICATION!
 40449/100000: episode: 2819, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.392 [-0.016, 10.915], loss: 1.206661, mae: 1.014632, mean_q: 4.579110
[RESULT] FALSIFICATION!
 40450/100000: episode: 2820, duration: 0.022s, episode steps: 1, steps per second: 46, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.014, 10.818], loss: 0.711289, mae: 0.723082, mean_q: 2.379667
[RESULT] FALSIFICATION!
 40451/100000: episode: 2821, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.015, 10.874], loss: 0.834546, mae: 0.645940, mean_q: 1.814916
[RESULT] FALSIFICATION!
 40452/100000: episode: 2822, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.015, 10.882], loss: 3.376076, mae: 1.322381, mean_q: 3.257548
[RESULT] FALSIFICATION!
 40453/100000: episode: 2823, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-1.232, 10.771], loss: 1.737174, mae: 1.256955, mean_q: 4.129241
[RESULT] FALSIFICATION!
 40454/100000: episode: 2824, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.015, 10.868], loss: 1.259473, mae: 0.837695, mean_q: 2.623816
[RESULT] FALSIFICATION!
 40455/100000: episode: 2825, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.368 [-0.016, 10.842], loss: 2.401015, mae: 1.236326, mean_q: 3.831580
[RESULT] FALSIFICATION!
 40456/100000: episode: 2826, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.015, 10.922], loss: 1.238739, mae: 1.080084, mean_q: 3.450642
[RESULT] FALSIFICATION!
[Info] Not found new level, current best level reached = 10.947945594787598
 40457/100000: episode: 2827, duration: 4.684s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.014, 10.776], loss: 1.208058, mae: 0.983865, mean_q: 4.736649
 40557/100000: episode: 2828, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.250, mean reward: 0.163 [0.013, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.271, 10.192], loss: 1.149203, mae: 0.805905, mean_q: 3.175073
 40657/100000: episode: 2829, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 19.672, mean reward: 0.197 [0.011, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.247, 10.098], loss: 1.116085, mae: 0.816780, mean_q: 3.190612
 40757/100000: episode: 2830, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 16.699, mean reward: 0.167 [0.018, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.093, 10.158], loss: 1.183616, mae: 0.831412, mean_q: 3.196660
 40857/100000: episode: 2831, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 14.593, mean reward: 0.146 [0.015, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.866, 10.264], loss: 1.144078, mae: 0.787713, mean_q: 3.182642
 40957/100000: episode: 2832, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 11.637, mean reward: 0.116 [0.006, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.675, 10.098], loss: 1.283043, mae: 0.865715, mean_q: 3.202947
 41057/100000: episode: 2833, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 15.496, mean reward: 0.155 [0.014, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.098], loss: 1.221861, mae: 0.821323, mean_q: 3.085093
 41157/100000: episode: 2834, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.156, mean reward: 0.152 [0.005, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.869, 10.098], loss: 1.154082, mae: 0.807333, mean_q: 3.260520
 41257/100000: episode: 2835, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 21.190, mean reward: 0.212 [0.042, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.288, 10.098], loss: 1.230986, mae: 0.824373, mean_q: 3.093153
 41357/100000: episode: 2836, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 16.296, mean reward: 0.163 [0.011, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.799, 10.160], loss: 1.083265, mae: 0.740972, mean_q: 3.128615
 41457/100000: episode: 2837, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 24.678, mean reward: 0.247 [0.033, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.627, 10.098], loss: 1.295111, mae: 0.825102, mean_q: 3.092104
 41557/100000: episode: 2838, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 13.158, mean reward: 0.132 [0.008, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.281, 10.098], loss: 1.122696, mae: 0.767287, mean_q: 3.079760
 41657/100000: episode: 2839, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 15.542, mean reward: 0.155 [0.015, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.725, 10.098], loss: 1.071944, mae: 0.735327, mean_q: 3.165986
 41757/100000: episode: 2840, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 18.922, mean reward: 0.189 [0.038, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.503, 10.098], loss: 1.002229, mae: 0.711240, mean_q: 3.098349
 41857/100000: episode: 2841, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 15.993, mean reward: 0.160 [0.012, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.159], loss: 1.067798, mae: 0.724667, mean_q: 3.018493
 41957/100000: episode: 2842, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 14.771, mean reward: 0.148 [0.007, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.878, 10.203], loss: 1.016413, mae: 0.707120, mean_q: 2.971504
 42057/100000: episode: 2843, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 14.871, mean reward: 0.149 [0.020, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.959, 10.219], loss: 1.046949, mae: 0.692955, mean_q: 2.884696
 42157/100000: episode: 2844, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 17.221, mean reward: 0.172 [0.008, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.800, 10.098], loss: 1.138685, mae: 0.747747, mean_q: 3.015089
 42257/100000: episode: 2845, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 15.801, mean reward: 0.158 [0.012, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.702, 10.188], loss: 1.154329, mae: 0.768709, mean_q: 2.951801
 42357/100000: episode: 2846, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 14.306, mean reward: 0.143 [0.017, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.209, 10.103], loss: 0.918849, mae: 0.652777, mean_q: 2.776477
 42457/100000: episode: 2847, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 14.428, mean reward: 0.144 [0.005, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.662, 10.120], loss: 1.064558, mae: 0.734944, mean_q: 2.927757
 42557/100000: episode: 2848, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 16.073, mean reward: 0.161 [0.014, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.204, 10.217], loss: 0.999576, mae: 0.708367, mean_q: 2.776952
 42657/100000: episode: 2849, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 20.526, mean reward: 0.205 [0.018, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.421, 10.177], loss: 0.939920, mae: 0.665209, mean_q: 2.900788
 42757/100000: episode: 2850, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 22.927, mean reward: 0.229 [0.008, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.567, 10.498], loss: 0.905712, mae: 0.649007, mean_q: 2.838606
 42857/100000: episode: 2851, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 18.207, mean reward: 0.182 [0.013, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.524, 10.098], loss: 0.865847, mae: 0.632601, mean_q: 2.811835
 42957/100000: episode: 2852, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 14.781, mean reward: 0.148 [0.007, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.554, 10.206], loss: 0.848973, mae: 0.603203, mean_q: 2.640292
 43057/100000: episode: 2853, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 16.600, mean reward: 0.166 [0.009, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.085, 10.098], loss: 0.825041, mae: 0.587334, mean_q: 2.622188
 43157/100000: episode: 2854, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 17.318, mean reward: 0.173 [0.045, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.351, 10.098], loss: 0.740760, mae: 0.568305, mean_q: 2.710494
 43257/100000: episode: 2855, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 16.386, mean reward: 0.164 [0.014, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.098], loss: 0.747333, mae: 0.566650, mean_q: 2.599411
 43357/100000: episode: 2856, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 15.205, mean reward: 0.152 [0.015, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.822, 10.134], loss: 0.680067, mae: 0.509312, mean_q: 2.454131
 43457/100000: episode: 2857, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 18.670, mean reward: 0.187 [0.059, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.165], loss: 0.506691, mae: 0.438874, mean_q: 2.383993
 43557/100000: episode: 2858, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.380, mean reward: 0.174 [0.005, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.342], loss: 0.417290, mae: 0.369825, mean_q: 2.363200
 43657/100000: episode: 2859, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.635, mean reward: 0.166 [0.039, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.069, 10.132], loss: 0.452307, mae: 0.399472, mean_q: 2.446093
 43757/100000: episode: 2860, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.942, mean reward: 0.169 [0.009, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.247], loss: 0.310292, mae: 0.326590, mean_q: 2.392479
 43857/100000: episode: 2861, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 20.845, mean reward: 0.208 [0.001, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.653, 10.513], loss: 0.238028, mae: 0.266675, mean_q: 2.126650
 43957/100000: episode: 2862, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 14.278, mean reward: 0.143 [0.011, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.225], loss: 0.162288, mae: 0.223077, mean_q: 2.008538
 44057/100000: episode: 2863, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 16.308, mean reward: 0.163 [0.007, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.111], loss: 0.080081, mae: 0.174723, mean_q: 1.985706
 44157/100000: episode: 2864, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 15.408, mean reward: 0.154 [0.018, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.098], loss: 0.050316, mae: 0.143771, mean_q: 1.873006
 44257/100000: episode: 2865, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 15.977, mean reward: 0.160 [0.033, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.106], loss: 0.038800, mae: 0.129632, mean_q: 1.670381
 44357/100000: episode: 2866, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 14.369, mean reward: 0.144 [0.013, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.650, 10.098], loss: 0.019709, mae: 0.109723, mean_q: 1.503739
 44457/100000: episode: 2867, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 14.617, mean reward: 0.146 [0.011, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.732, 10.194], loss: 0.009706, mae: 0.100002, mean_q: 1.379112
 44557/100000: episode: 2868, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 12.515, mean reward: 0.125 [0.007, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.166, 10.098], loss: 0.008232, mae: 0.093694, mean_q: 1.283114
 44657/100000: episode: 2869, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.180, mean reward: 0.162 [0.023, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.215, 10.414], loss: 0.007565, mae: 0.090694, mean_q: 1.223941
 44757/100000: episode: 2870, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 20.566, mean reward: 0.206 [0.006, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.634, 10.098], loss: 0.006804, mae: 0.087844, mean_q: 1.031295
 44857/100000: episode: 2871, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.778, mean reward: 0.178 [0.022, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.176], loss: 0.006608, mae: 0.087567, mean_q: 0.941501
 44957/100000: episode: 2872, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 19.427, mean reward: 0.194 [0.017, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.650, 10.098], loss: 0.005610, mae: 0.082121, mean_q: 0.844733
 45057/100000: episode: 2873, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 14.074, mean reward: 0.141 [0.006, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.525, 10.098], loss: 0.006297, mae: 0.085815, mean_q: 0.731201
 45157/100000: episode: 2874, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 18.158, mean reward: 0.182 [0.017, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.368, 10.217], loss: 0.005652, mae: 0.082121, mean_q: 0.569819
 45257/100000: episode: 2875, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 19.557, mean reward: 0.196 [0.006, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.216, 10.098], loss: 0.005860, mae: 0.083055, mean_q: 0.510726
 45357/100000: episode: 2876, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 18.505, mean reward: 0.185 [0.017, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.424, 10.098], loss: 0.006062, mae: 0.085162, mean_q: 0.427617
 45457/100000: episode: 2877, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.381, mean reward: 0.164 [0.034, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.098], loss: 0.005738, mae: 0.084028, mean_q: 0.350297
 45557/100000: episode: 2878, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.680, mean reward: 0.157 [0.019, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.558, 10.202], loss: 0.006045, mae: 0.085273, mean_q: 0.333385
 45657/100000: episode: 2879, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 14.870, mean reward: 0.149 [0.007, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.039, 10.126], loss: 0.005341, mae: 0.081014, mean_q: 0.328751
 45757/100000: episode: 2880, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 13.671, mean reward: 0.137 [0.009, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.617, 10.098], loss: 0.005039, mae: 0.078464, mean_q: 0.327246
 45857/100000: episode: 2881, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 15.456, mean reward: 0.155 [0.031, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.589, 10.098], loss: 0.005034, mae: 0.077980, mean_q: 0.325718
 45957/100000: episode: 2882, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 14.658, mean reward: 0.147 [0.011, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.515, 10.098], loss: 0.005161, mae: 0.080075, mean_q: 0.333063
 46057/100000: episode: 2883, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 16.629, mean reward: 0.166 [0.011, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.326, 10.098], loss: 0.004909, mae: 0.077013, mean_q: 0.327413
 46157/100000: episode: 2884, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.717, mean reward: 0.167 [0.019, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.606, 10.267], loss: 0.005030, mae: 0.078476, mean_q: 0.333001
 46257/100000: episode: 2885, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 19.525, mean reward: 0.195 [0.022, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.969, 10.098], loss: 0.005228, mae: 0.079386, mean_q: 0.332739
 46357/100000: episode: 2886, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 14.100, mean reward: 0.141 [0.015, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.190, 10.171], loss: 0.005238, mae: 0.079469, mean_q: 0.331237
 46457/100000: episode: 2887, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 21.347, mean reward: 0.213 [0.024, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.039, 10.098], loss: 0.005041, mae: 0.077975, mean_q: 0.324939
 46557/100000: episode: 2888, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 18.817, mean reward: 0.188 [0.007, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.936, 10.111], loss: 0.004904, mae: 0.077971, mean_q: 0.326644
 46657/100000: episode: 2889, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 17.051, mean reward: 0.171 [0.018, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.832, 10.098], loss: 0.005109, mae: 0.078107, mean_q: 0.330184
 46757/100000: episode: 2890, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.600, mean reward: 0.176 [0.018, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.486, 10.098], loss: 0.004954, mae: 0.077600, mean_q: 0.328526
 46857/100000: episode: 2891, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 15.838, mean reward: 0.158 [0.019, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.098], loss: 0.005484, mae: 0.082397, mean_q: 0.333606
 46957/100000: episode: 2892, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 14.203, mean reward: 0.142 [0.019, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.504, 10.098], loss: 0.005154, mae: 0.079230, mean_q: 0.331785
 47057/100000: episode: 2893, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 17.098, mean reward: 0.171 [0.045, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.939, 10.098], loss: 0.005368, mae: 0.080090, mean_q: 0.334269
 47157/100000: episode: 2894, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 17.900, mean reward: 0.179 [0.034, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.289], loss: 0.005281, mae: 0.080270, mean_q: 0.332302
 47257/100000: episode: 2895, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 13.581, mean reward: 0.136 [0.010, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.388, 10.119], loss: 0.004893, mae: 0.077683, mean_q: 0.333386
 47357/100000: episode: 2896, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 15.400, mean reward: 0.154 [0.024, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.152], loss: 0.004532, mae: 0.074273, mean_q: 0.335974
 47457/100000: episode: 2897, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 17.303, mean reward: 0.173 [0.037, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.374, 10.098], loss: 0.005326, mae: 0.081073, mean_q: 0.333588
 47557/100000: episode: 2898, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.918, mean reward: 0.169 [0.015, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.711, 10.322], loss: 0.004612, mae: 0.075541, mean_q: 0.334660
 47657/100000: episode: 2899, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 18.753, mean reward: 0.188 [0.004, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.665, 10.098], loss: 0.004865, mae: 0.076991, mean_q: 0.331559
 47757/100000: episode: 2900, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 15.367, mean reward: 0.154 [0.013, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.112, 10.299], loss: 0.005163, mae: 0.079965, mean_q: 0.327947
 47857/100000: episode: 2901, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 22.541, mean reward: 0.225 [0.037, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.952, 10.098], loss: 0.005311, mae: 0.080107, mean_q: 0.328189
 47957/100000: episode: 2902, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 14.357, mean reward: 0.144 [0.015, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.173, 10.290], loss: 0.004505, mae: 0.073937, mean_q: 0.326181
 48057/100000: episode: 2903, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.584, mean reward: 0.166 [0.015, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.420], loss: 0.004392, mae: 0.072679, mean_q: 0.334298
 48157/100000: episode: 2904, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 13.462, mean reward: 0.135 [0.007, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.665, 10.145], loss: 0.004169, mae: 0.071695, mean_q: 0.332920
 48257/100000: episode: 2905, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 14.633, mean reward: 0.146 [0.014, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.516, 10.123], loss: 0.004664, mae: 0.075963, mean_q: 0.332618
 48357/100000: episode: 2906, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 17.930, mean reward: 0.179 [0.029, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.681, 10.098], loss: 0.004665, mae: 0.075416, mean_q: 0.327217
 48457/100000: episode: 2907, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 15.671, mean reward: 0.157 [0.004, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.845, 10.114], loss: 0.004411, mae: 0.073482, mean_q: 0.328006
 48557/100000: episode: 2908, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 13.023, mean reward: 0.130 [0.023, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.250, 10.221], loss: 0.004567, mae: 0.074902, mean_q: 0.328618
 48657/100000: episode: 2909, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 17.126, mean reward: 0.171 [0.027, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.840, 10.221], loss: 0.004812, mae: 0.076545, mean_q: 0.328160
 48757/100000: episode: 2910, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 25.262, mean reward: 0.253 [0.062, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.095, 10.222], loss: 0.005198, mae: 0.080283, mean_q: 0.326831
 48857/100000: episode: 2911, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 17.995, mean reward: 0.180 [0.024, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.164, 10.168], loss: 0.004313, mae: 0.072129, mean_q: 0.329267
 48957/100000: episode: 2912, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 12.968, mean reward: 0.130 [0.003, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.999, 10.098], loss: 0.004364, mae: 0.073602, mean_q: 0.327922
 49057/100000: episode: 2913, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 14.778, mean reward: 0.148 [0.008, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.120, 10.343], loss: 0.004999, mae: 0.078090, mean_q: 0.334326
 49157/100000: episode: 2914, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 16.535, mean reward: 0.165 [0.008, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.367, 10.098], loss: 0.004755, mae: 0.076833, mean_q: 0.327701
 49257/100000: episode: 2915, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 15.509, mean reward: 0.155 [0.036, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.086, 10.342], loss: 0.004832, mae: 0.077227, mean_q: 0.332995
 49357/100000: episode: 2916, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 18.443, mean reward: 0.184 [0.029, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.600, 10.098], loss: 0.004622, mae: 0.075778, mean_q: 0.328782
 49457/100000: episode: 2917, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 13.091, mean reward: 0.131 [0.016, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.209, 10.239], loss: 0.004920, mae: 0.077357, mean_q: 0.328137
 49557/100000: episode: 2918, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.266, mean reward: 0.173 [0.016, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.657, 10.098], loss: 0.004936, mae: 0.077229, mean_q: 0.329738
 49657/100000: episode: 2919, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 13.804, mean reward: 0.138 [0.007, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.312, 10.138], loss: 0.005017, mae: 0.078934, mean_q: 0.328880
 49757/100000: episode: 2920, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 16.425, mean reward: 0.164 [0.027, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.294], loss: 0.004324, mae: 0.072443, mean_q: 0.327844
 49857/100000: episode: 2921, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 27.302, mean reward: 0.273 [0.054, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.728, 10.098], loss: 0.004260, mae: 0.072274, mean_q: 0.327370
 49957/100000: episode: 2922, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 16.587, mean reward: 0.166 [0.029, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.600, 10.098], loss: 0.004731, mae: 0.075995, mean_q: 0.328954
 50057/100000: episode: 2923, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 20.550, mean reward: 0.205 [0.022, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.912, 10.359], loss: 0.004217, mae: 0.071379, mean_q: 0.330966
 50157/100000: episode: 2924, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 13.622, mean reward: 0.136 [0.006, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.098], loss: 0.004390, mae: 0.073642, mean_q: 0.327952
 50257/100000: episode: 2925, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 12.715, mean reward: 0.127 [0.016, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.637, 10.151], loss: 0.004647, mae: 0.075945, mean_q: 0.329524
 50357/100000: episode: 2926, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 16.878, mean reward: 0.169 [0.018, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.498, 10.163], loss: 0.004477, mae: 0.074897, mean_q: 0.328316
[Info] New level: 0.6294599771499634 | Considering 10/90 traces
 50457/100000: episode: 2927, duration: 4.554s, episode steps: 100, steps per second: 22, episode reward: 15.597, mean reward: 0.156 [0.006, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.633, 10.098], loss: 0.004267, mae: 0.072088, mean_q: 0.330541
 50481/100000: episode: 2928, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 10.082, mean reward: 0.420 [0.123, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.072, 10.441], loss: 0.005200, mae: 0.080936, mean_q: 0.327703
 50486/100000: episode: 2929, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 2.054, mean reward: 0.411 [0.279, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.824, 10.396], loss: 0.005902, mae: 0.086665, mean_q: 0.330902
 50524/100000: episode: 2930, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 13.971, mean reward: 0.368 [0.232, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.815, 10.100], loss: 0.005738, mae: 0.082779, mean_q: 0.334738
 50548/100000: episode: 2931, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 7.938, mean reward: 0.331 [0.234, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.204, 10.501], loss: 0.004761, mae: 0.076454, mean_q: 0.332101
 50553/100000: episode: 2932, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 1.149, mean reward: 0.230 [0.214, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.327], loss: 0.004437, mae: 0.075669, mean_q: 0.343018
 50577/100000: episode: 2933, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 10.218, mean reward: 0.426 [0.360, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.481], loss: 0.004641, mae: 0.074220, mean_q: 0.339117
 50603/100000: episode: 2934, duration: 0.139s, episode steps: 26, steps per second: 188, episode reward: 7.363, mean reward: 0.283 [0.087, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.736, 10.100], loss: 0.004523, mae: 0.072813, mean_q: 0.343673
 50620/100000: episode: 2935, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 5.020, mean reward: 0.295 [0.094, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.445, 10.100], loss: 0.004405, mae: 0.074955, mean_q: 0.344673
 50646/100000: episode: 2936, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 6.613, mean reward: 0.254 [0.080, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.095, 10.100], loss: 0.004980, mae: 0.076667, mean_q: 0.336137
 50663/100000: episode: 2937, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 5.039, mean reward: 0.296 [0.097, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.832, 10.100], loss: 0.003893, mae: 0.070104, mean_q: 0.340363
 50689/100000: episode: 2938, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 7.124, mean reward: 0.274 [0.144, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.410, 10.100], loss: 0.004790, mae: 0.076437, mean_q: 0.338138
 50721/100000: episode: 2939, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 6.276, mean reward: 0.196 [0.048, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.234, 10.100], loss: 0.005025, mae: 0.076533, mean_q: 0.346461
 50747/100000: episode: 2940, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 5.525, mean reward: 0.213 [0.039, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.321, 10.239], loss: 0.004502, mae: 0.074114, mean_q: 0.349281
 50806/100000: episode: 2941, duration: 0.339s, episode steps: 59, steps per second: 174, episode reward: 10.934, mean reward: 0.185 [0.010, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.824 [-1.162, 10.123], loss: 0.004525, mae: 0.075533, mean_q: 0.344212
 50830/100000: episode: 2942, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 8.587, mean reward: 0.358 [0.235, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.552, 10.336], loss: 0.004409, mae: 0.074187, mean_q: 0.336923
 50848/100000: episode: 2943, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 6.611, mean reward: 0.367 [0.243, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.334, 10.546], loss: 0.004865, mae: 0.078073, mean_q: 0.344697
 50874/100000: episode: 2944, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 5.079, mean reward: 0.195 [0.047, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.346, 10.203], loss: 0.004712, mae: 0.077135, mean_q: 0.348118
 50912/100000: episode: 2945, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 11.151, mean reward: 0.293 [0.113, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.364, 10.100], loss: 0.004908, mae: 0.078045, mean_q: 0.345988
 50944/100000: episode: 2946, duration: 0.232s, episode steps: 32, steps per second: 138, episode reward: 7.946, mean reward: 0.248 [0.117, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.510, 10.100], loss: 0.005239, mae: 0.080795, mean_q: 0.352235
 50970/100000: episode: 2947, duration: 0.274s, episode steps: 26, steps per second: 95, episode reward: 5.577, mean reward: 0.214 [0.012, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.915, 10.117], loss: 0.005165, mae: 0.081000, mean_q: 0.348239
 51010/100000: episode: 2948, duration: 0.331s, episode steps: 40, steps per second: 121, episode reward: 10.379, mean reward: 0.259 [0.136, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.544, 10.293], loss: 0.004341, mae: 0.072381, mean_q: 0.344800
 51028/100000: episode: 2949, duration: 0.152s, episode steps: 18, steps per second: 119, episode reward: 5.639, mean reward: 0.313 [0.246, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.828, 10.438], loss: 0.004149, mae: 0.070518, mean_q: 0.347232
 51066/100000: episode: 2950, duration: 0.296s, episode steps: 38, steps per second: 128, episode reward: 16.772, mean reward: 0.441 [0.257, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.776, 10.100], loss: 0.004769, mae: 0.077193, mean_q: 0.354379
 51083/100000: episode: 2951, duration: 0.130s, episode steps: 17, steps per second: 131, episode reward: 5.444, mean reward: 0.320 [0.199, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.125, 10.100], loss: 0.005254, mae: 0.082046, mean_q: 0.357074
 51121/100000: episode: 2952, duration: 0.244s, episode steps: 38, steps per second: 156, episode reward: 14.547, mean reward: 0.383 [0.204, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.273, 10.100], loss: 0.005029, mae: 0.077349, mean_q: 0.364918
 51153/100000: episode: 2953, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 7.005, mean reward: 0.219 [0.079, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.223, 10.100], loss: 0.005245, mae: 0.079850, mean_q: 0.361622
 51193/100000: episode: 2954, duration: 0.204s, episode steps: 40, steps per second: 197, episode reward: 6.706, mean reward: 0.168 [0.038, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.186, 10.132], loss: 0.005870, mae: 0.086684, mean_q: 0.367728
 51252/100000: episode: 2955, duration: 0.304s, episode steps: 59, steps per second: 194, episode reward: 20.992, mean reward: 0.356 [0.108, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.438, 10.345], loss: 0.005629, mae: 0.083403, mean_q: 0.359385
 51292/100000: episode: 2956, duration: 0.265s, episode steps: 40, steps per second: 151, episode reward: 14.659, mean reward: 0.366 [0.228, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.093, 10.468], loss: 0.004571, mae: 0.076145, mean_q: 0.360397
 51310/100000: episode: 2957, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 5.628, mean reward: 0.313 [0.236, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.347], loss: 0.004390, mae: 0.073811, mean_q: 0.378077
 51342/100000: episode: 2958, duration: 0.222s, episode steps: 32, steps per second: 144, episode reward: 9.873, mean reward: 0.309 [0.142, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.044, 10.100], loss: 0.005102, mae: 0.078897, mean_q: 0.377163
 51368/100000: episode: 2959, duration: 0.248s, episode steps: 26, steps per second: 105, episode reward: 7.728, mean reward: 0.297 [0.128, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.762, 10.100], loss: 0.005275, mae: 0.079749, mean_q: 0.373185
 51392/100000: episode: 2960, duration: 0.270s, episode steps: 24, steps per second: 89, episode reward: 11.935, mean reward: 0.497 [0.303, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.369, 10.623], loss: 0.004318, mae: 0.073115, mean_q: 0.378678
 51416/100000: episode: 2961, duration: 0.295s, episode steps: 24, steps per second: 81, episode reward: 8.802, mean reward: 0.367 [0.262, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.196, 10.509], loss: 0.004866, mae: 0.075620, mean_q: 0.368584
 51442/100000: episode: 2962, duration: 0.223s, episode steps: 26, steps per second: 117, episode reward: 7.549, mean reward: 0.290 [0.131, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.470, 10.100], loss: 0.004543, mae: 0.074284, mean_q: 0.388264
 51447/100000: episode: 2963, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 1.461, mean reward: 0.292 [0.258, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.484, 10.375], loss: 0.005139, mae: 0.082530, mean_q: 0.415174
 51485/100000: episode: 2964, duration: 0.248s, episode steps: 38, steps per second: 153, episode reward: 7.318, mean reward: 0.193 [0.036, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.869, 10.389], loss: 0.004826, mae: 0.076795, mean_q: 0.381840
 51502/100000: episode: 2965, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 4.807, mean reward: 0.283 [0.159, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.233, 10.100], loss: 0.004536, mae: 0.074260, mean_q: 0.389034
 51528/100000: episode: 2966, duration: 0.218s, episode steps: 26, steps per second: 119, episode reward: 7.915, mean reward: 0.304 [0.150, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.199, 10.100], loss: 0.004536, mae: 0.074679, mean_q: 0.389015
 51566/100000: episode: 2967, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 14.644, mean reward: 0.385 [0.261, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.881, 10.100], loss: 0.004877, mae: 0.077958, mean_q: 0.379544
 51604/100000: episode: 2968, duration: 0.314s, episode steps: 38, steps per second: 121, episode reward: 16.515, mean reward: 0.435 [0.204, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.038, 10.100], loss: 0.004474, mae: 0.073180, mean_q: 0.389425
 51628/100000: episode: 2969, duration: 0.197s, episode steps: 24, steps per second: 122, episode reward: 8.131, mean reward: 0.339 [0.160, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.342], loss: 0.004412, mae: 0.072494, mean_q: 0.386349
 51666/100000: episode: 2970, duration: 0.282s, episode steps: 38, steps per second: 135, episode reward: 12.998, mean reward: 0.342 [0.242, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.227, 10.100], loss: 0.004739, mae: 0.076128, mean_q: 0.394815
 51725/100000: episode: 2971, duration: 0.426s, episode steps: 59, steps per second: 139, episode reward: 12.461, mean reward: 0.211 [0.017, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.816 [-1.202, 10.100], loss: 0.004750, mae: 0.075668, mean_q: 0.396399
 51751/100000: episode: 2972, duration: 0.230s, episode steps: 26, steps per second: 113, episode reward: 7.640, mean reward: 0.294 [0.175, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.227, 10.100], loss: 0.004042, mae: 0.070704, mean_q: 0.392401
 51810/100000: episode: 2973, duration: 0.508s, episode steps: 59, steps per second: 116, episode reward: 11.137, mean reward: 0.189 [0.015, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.827 [-1.328, 10.139], loss: 0.005484, mae: 0.080918, mean_q: 0.396039
 51815/100000: episode: 2974, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 1.656, mean reward: 0.331 [0.280, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.470], loss: 0.004941, mae: 0.076400, mean_q: 0.381155
 51839/100000: episode: 2975, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 7.830, mean reward: 0.326 [0.174, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.419, 10.293], loss: 0.004953, mae: 0.078327, mean_q: 0.388766
 51877/100000: episode: 2976, duration: 0.364s, episode steps: 38, steps per second: 104, episode reward: 13.721, mean reward: 0.361 [0.271, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.634, 10.100], loss: 0.005007, mae: 0.078035, mean_q: 0.402999
 51894/100000: episode: 2977, duration: 0.151s, episode steps: 17, steps per second: 113, episode reward: 4.629, mean reward: 0.272 [0.112, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.186, 10.100], loss: 0.004966, mae: 0.078730, mean_q: 0.404508
 51926/100000: episode: 2978, duration: 0.302s, episode steps: 32, steps per second: 106, episode reward: 13.075, mean reward: 0.409 [0.271, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.587, 10.100], loss: 0.004421, mae: 0.072752, mean_q: 0.408435
 51964/100000: episode: 2979, duration: 0.337s, episode steps: 38, steps per second: 113, episode reward: 18.519, mean reward: 0.487 [0.268, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.413, 10.100], loss: 0.004397, mae: 0.072253, mean_q: 0.404862
 52023/100000: episode: 2980, duration: 0.487s, episode steps: 59, steps per second: 121, episode reward: 10.643, mean reward: 0.180 [0.032, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.819 [-0.965, 10.242], loss: 0.004770, mae: 0.076145, mean_q: 0.411638
 52063/100000: episode: 2981, duration: 0.338s, episode steps: 40, steps per second: 118, episode reward: 10.429, mean reward: 0.261 [0.091, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.308, 10.175], loss: 0.004928, mae: 0.077607, mean_q: 0.404476
 52089/100000: episode: 2982, duration: 0.207s, episode steps: 26, steps per second: 126, episode reward: 7.709, mean reward: 0.296 [0.174, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.804, 10.100], loss: 0.004320, mae: 0.073437, mean_q: 0.436241
 52094/100000: episode: 2983, duration: 0.044s, episode steps: 5, steps per second: 114, episode reward: 1.622, mean reward: 0.324 [0.292, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.455], loss: 0.003144, mae: 0.060168, mean_q: 0.429779
 52153/100000: episode: 2984, duration: 0.482s, episode steps: 59, steps per second: 122, episode reward: 12.149, mean reward: 0.206 [0.040, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.820 [-0.548, 10.100], loss: 0.004623, mae: 0.073633, mean_q: 0.411456
 52177/100000: episode: 2985, duration: 0.203s, episode steps: 24, steps per second: 118, episode reward: 8.273, mean reward: 0.345 [0.243, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.522, 10.374], loss: 0.005097, mae: 0.078784, mean_q: 0.420106
 52215/100000: episode: 2986, duration: 0.335s, episode steps: 38, steps per second: 113, episode reward: 9.290, mean reward: 0.244 [0.064, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.112, 10.100], loss: 0.004313, mae: 0.072876, mean_q: 0.413423
 52239/100000: episode: 2987, duration: 0.241s, episode steps: 24, steps per second: 99, episode reward: 8.565, mean reward: 0.357 [0.107, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.154, 10.312], loss: 0.004383, mae: 0.075282, mean_q: 0.419991
 52298/100000: episode: 2988, duration: 0.474s, episode steps: 59, steps per second: 125, episode reward: 17.733, mean reward: 0.301 [0.055, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.384, 10.241], loss: 0.004115, mae: 0.070378, mean_q: 0.424563
 52336/100000: episode: 2989, duration: 0.337s, episode steps: 38, steps per second: 113, episode reward: 10.177, mean reward: 0.268 [0.066, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.109, 10.100], loss: 0.005375, mae: 0.081913, mean_q: 0.411587
 52362/100000: episode: 2990, duration: 0.245s, episode steps: 26, steps per second: 106, episode reward: 5.311, mean reward: 0.204 [0.040, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.076, 10.109], loss: 0.004123, mae: 0.069792, mean_q: 0.414966
 52402/100000: episode: 2991, duration: 0.335s, episode steps: 40, steps per second: 119, episode reward: 9.425, mean reward: 0.236 [0.093, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.434, 10.307], loss: 0.004438, mae: 0.073501, mean_q: 0.431278
 52420/100000: episode: 2992, duration: 0.151s, episode steps: 18, steps per second: 119, episode reward: 3.389, mean reward: 0.188 [0.105, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.278], loss: 0.004344, mae: 0.071663, mean_q: 0.414394
 52458/100000: episode: 2993, duration: 0.332s, episode steps: 38, steps per second: 114, episode reward: 8.491, mean reward: 0.223 [0.096, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.488, 10.100], loss: 0.004130, mae: 0.070563, mean_q: 0.426587
 52475/100000: episode: 2994, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 4.438, mean reward: 0.261 [0.123, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.104, 10.100], loss: 0.003640, mae: 0.065401, mean_q: 0.412766
 52499/100000: episode: 2995, duration: 0.203s, episode steps: 24, steps per second: 118, episode reward: 7.409, mean reward: 0.309 [0.230, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.256, 10.333], loss: 0.004726, mae: 0.075083, mean_q: 0.426244
 52525/100000: episode: 2996, duration: 0.209s, episode steps: 26, steps per second: 124, episode reward: 7.414, mean reward: 0.285 [0.192, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.633, 10.100], loss: 0.004355, mae: 0.072148, mean_q: 0.437342
 52563/100000: episode: 2997, duration: 0.314s, episode steps: 38, steps per second: 121, episode reward: 14.304, mean reward: 0.376 [0.246, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.182, 10.100], loss: 0.004392, mae: 0.072058, mean_q: 0.434569
 52603/100000: episode: 2998, duration: 0.329s, episode steps: 40, steps per second: 122, episode reward: 11.933, mean reward: 0.298 [0.208, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.515, 10.396], loss: 0.003832, mae: 0.068478, mean_q: 0.439364
 52641/100000: episode: 2999, duration: 0.303s, episode steps: 38, steps per second: 125, episode reward: 8.536, mean reward: 0.225 [0.103, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.436, 10.100], loss: 0.004571, mae: 0.074968, mean_q: 0.440538
 52646/100000: episode: 3000, duration: 0.042s, episode steps: 5, steps per second: 120, episode reward: 1.752, mean reward: 0.350 [0.335, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.437], loss: 0.004022, mae: 0.069719, mean_q: 0.401987
 52664/100000: episode: 3001, duration: 0.154s, episode steps: 18, steps per second: 117, episode reward: 5.510, mean reward: 0.306 [0.212, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.562, 10.355], loss: 0.003609, mae: 0.066273, mean_q: 0.436329
 52702/100000: episode: 3002, duration: 0.303s, episode steps: 38, steps per second: 125, episode reward: 9.338, mean reward: 0.246 [0.083, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.824, 10.100], loss: 0.004158, mae: 0.070345, mean_q: 0.432750
 52707/100000: episode: 3003, duration: 0.044s, episode steps: 5, steps per second: 113, episode reward: 1.955, mean reward: 0.391 [0.336, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.314, 10.360], loss: 0.004189, mae: 0.070245, mean_q: 0.460025
 52739/100000: episode: 3004, duration: 0.260s, episode steps: 32, steps per second: 123, episode reward: 5.807, mean reward: 0.181 [0.044, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.213, 10.100], loss: 0.004086, mae: 0.069834, mean_q: 0.432364
 52757/100000: episode: 3005, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 4.934, mean reward: 0.274 [0.236, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.705, 10.387], loss: 0.003909, mae: 0.069743, mean_q: 0.448269
 52781/100000: episode: 3006, duration: 0.154s, episode steps: 24, steps per second: 156, episode reward: 7.143, mean reward: 0.298 [0.197, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.249, 10.288], loss: 0.006273, mae: 0.087122, mean_q: 0.452959
 52840/100000: episode: 3007, duration: 0.420s, episode steps: 59, steps per second: 140, episode reward: 19.326, mean reward: 0.328 [0.210, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.819 [-0.330, 10.329], loss: 0.004654, mae: 0.074789, mean_q: 0.442461
 52880/100000: episode: 3008, duration: 0.332s, episode steps: 40, steps per second: 120, episode reward: 9.381, mean reward: 0.235 [0.118, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.900, 10.366], loss: 0.003590, mae: 0.065239, mean_q: 0.441606
 52918/100000: episode: 3009, duration: 0.318s, episode steps: 38, steps per second: 119, episode reward: 8.285, mean reward: 0.218 [0.068, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.056, 10.240], loss: 0.004097, mae: 0.070682, mean_q: 0.442467
 52942/100000: episode: 3010, duration: 0.218s, episode steps: 24, steps per second: 110, episode reward: 8.081, mean reward: 0.337 [0.269, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.537, 10.402], loss: 0.005359, mae: 0.082166, mean_q: 0.447539
 52974/100000: episode: 3011, duration: 0.279s, episode steps: 32, steps per second: 115, episode reward: 12.051, mean reward: 0.377 [0.155, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.657, 10.100], loss: 0.003865, mae: 0.067949, mean_q: 0.445750
 53012/100000: episode: 3012, duration: 0.331s, episode steps: 38, steps per second: 115, episode reward: 11.814, mean reward: 0.311 [0.139, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.655, 10.100], loss: 0.004247, mae: 0.071467, mean_q: 0.443223
 53050/100000: episode: 3013, duration: 0.307s, episode steps: 38, steps per second: 124, episode reward: 11.102, mean reward: 0.292 [0.151, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.961, 10.100], loss: 0.004065, mae: 0.069766, mean_q: 0.465858
 53074/100000: episode: 3014, duration: 0.191s, episode steps: 24, steps per second: 125, episode reward: 11.649, mean reward: 0.485 [0.370, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.967, 10.514], loss: 0.004729, mae: 0.075839, mean_q: 0.457959
 53091/100000: episode: 3015, duration: 0.159s, episode steps: 17, steps per second: 107, episode reward: 4.244, mean reward: 0.250 [0.133, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.086, 10.100], loss: 0.005324, mae: 0.080660, mean_q: 0.446786
 53117/100000: episode: 3016, duration: 0.236s, episode steps: 26, steps per second: 110, episode reward: 7.804, mean reward: 0.300 [0.187, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.082, 10.100], loss: 0.004205, mae: 0.070969, mean_q: 0.459423
[Info] New level: 0.9619541168212891 | Considering 10/90 traces
 53155/100000: episode: 3017, duration: 5.278s, episode steps: 38, steps per second: 7, episode reward: 12.244, mean reward: 0.322 [0.005, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.374, 10.198], loss: 0.004372, mae: 0.072572, mean_q: 0.466045
 53169/100000: episode: 3018, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 6.747, mean reward: 0.482 [0.362, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.185, 10.523], loss: 0.004054, mae: 0.067053, mean_q: 0.445488
 53185/100000: episode: 3019, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 8.663, mean reward: 0.541 [0.487, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.475, 10.100], loss: 0.004247, mae: 0.072881, mean_q: 0.457393
 53203/100000: episode: 3020, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 7.712, mean reward: 0.428 [0.354, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.550, 10.477], loss: 0.004976, mae: 0.075309, mean_q: 0.465082
[RESULT] FALSIFICATION!
 53212/100000: episode: 3021, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 14.402, mean reward: 1.600 [0.455, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.608, 10.005], loss: 0.003315, mae: 0.065225, mean_q: 0.437037
 53226/100000: episode: 3022, duration: 0.098s, episode steps: 14, steps per second: 144, episode reward: 7.808, mean reward: 0.558 [0.406, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.091, 10.708], loss: 0.133079, mae: 0.173873, mean_q: 0.480969
 53280/100000: episode: 3023, duration: 0.476s, episode steps: 54, steps per second: 113, episode reward: 11.331, mean reward: 0.210 [0.015, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-1.148, 10.192], loss: 0.007494, mae: 0.093345, mean_q: 0.476071
 53296/100000: episode: 3024, duration: 0.135s, episode steps: 16, steps per second: 119, episode reward: 7.618, mean reward: 0.476 [0.401, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.724, 10.100], loss: 0.005290, mae: 0.080425, mean_q: 0.476463
 53321/100000: episode: 3025, duration: 0.205s, episode steps: 25, steps per second: 122, episode reward: 9.911, mean reward: 0.396 [0.225, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.754, 10.100], loss: 0.004635, mae: 0.074539, mean_q: 0.490882
 53337/100000: episode: 3026, duration: 0.130s, episode steps: 16, steps per second: 123, episode reward: 8.643, mean reward: 0.540 [0.459, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.318, 10.100], loss: 0.004105, mae: 0.070319, mean_q: 0.495836
 53354/100000: episode: 3027, duration: 0.163s, episode steps: 17, steps per second: 104, episode reward: 6.574, mean reward: 0.387 [0.262, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.804, 10.373], loss: 0.004794, mae: 0.077241, mean_q: 0.476764
 53368/100000: episode: 3028, duration: 0.106s, episode steps: 14, steps per second: 132, episode reward: 7.341, mean reward: 0.524 [0.468, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.522], loss: 0.004641, mae: 0.074842, mean_q: 0.486753
 53385/100000: episode: 3029, duration: 0.133s, episode steps: 17, steps per second: 128, episode reward: 8.148, mean reward: 0.479 [0.378, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.090, 10.527], loss: 0.004357, mae: 0.072975, mean_q: 0.480673
 53386/100000: episode: 3030, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.070, 10.348], loss: 0.003614, mae: 0.062778, mean_q: 0.473818
 53404/100000: episode: 3031, duration: 0.138s, episode steps: 18, steps per second: 131, episode reward: 8.039, mean reward: 0.447 [0.388, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.439, 10.460], loss: 0.003972, mae: 0.068348, mean_q: 0.495823
 53420/100000: episode: 3032, duration: 0.138s, episode steps: 16, steps per second: 116, episode reward: 7.677, mean reward: 0.480 [0.423, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.394, 10.100], loss: 0.088447, mae: 0.133350, mean_q: 0.515179
 53437/100000: episode: 3033, duration: 0.147s, episode steps: 17, steps per second: 116, episode reward: 8.264, mean reward: 0.486 [0.429, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.481], loss: 0.007061, mae: 0.095098, mean_q: 0.484106
 53462/100000: episode: 3034, duration: 0.214s, episode steps: 25, steps per second: 117, episode reward: 10.306, mean reward: 0.412 [0.222, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.160, 10.100], loss: 0.004693, mae: 0.073862, mean_q: 0.494000
 53479/100000: episode: 3035, duration: 0.162s, episode steps: 17, steps per second: 105, episode reward: 9.565, mean reward: 0.563 [0.479, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.773, 10.550], loss: 0.005006, mae: 0.076428, mean_q: 0.513517
 53499/100000: episode: 3036, duration: 0.185s, episode steps: 20, steps per second: 108, episode reward: 9.539, mean reward: 0.477 [0.386, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.438], loss: 0.004068, mae: 0.072030, mean_q: 0.499800
 53513/100000: episode: 3037, duration: 0.124s, episode steps: 14, steps per second: 113, episode reward: 5.753, mean reward: 0.411 [0.346, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.195, 10.528], loss: 0.004265, mae: 0.069587, mean_q: 0.500507
 53538/100000: episode: 3038, duration: 0.227s, episode steps: 25, steps per second: 110, episode reward: 12.264, mean reward: 0.491 [0.315, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.299, 10.100], loss: 0.055902, mae: 0.092375, mean_q: 0.509440
 53556/100000: episode: 3039, duration: 0.170s, episode steps: 18, steps per second: 106, episode reward: 8.068, mean reward: 0.448 [0.386, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.791, 10.645], loss: 0.011558, mae: 0.119783, mean_q: 0.481762
 53581/100000: episode: 3040, duration: 0.239s, episode steps: 25, steps per second: 104, episode reward: 10.769, mean reward: 0.431 [0.262, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.593, 10.100], loss: 0.005457, mae: 0.079633, mean_q: 0.527746
 53606/100000: episode: 3041, duration: 0.379s, episode steps: 25, steps per second: 66, episode reward: 13.501, mean reward: 0.540 [0.367, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.767, 10.100], loss: 0.004937, mae: 0.075496, mean_q: 0.512529
[RESULT] FALSIFICATION!
 53618/100000: episode: 3042, duration: 0.098s, episode steps: 12, steps per second: 122, episode reward: 16.780, mean reward: 1.398 [0.561, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.404, 10.067], loss: 0.004001, mae: 0.070403, mean_q: 0.496960
 53634/100000: episode: 3043, duration: 0.138s, episode steps: 16, steps per second: 116, episode reward: 10.059, mean reward: 0.629 [0.591, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.943, 10.100], loss: 0.005494, mae: 0.077789, mean_q: 0.493870
 53650/100000: episode: 3044, duration: 0.158s, episode steps: 16, steps per second: 101, episode reward: 7.180, mean reward: 0.449 [0.279, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.711, 10.100], loss: 0.003882, mae: 0.067476, mean_q: 0.515623
 53675/100000: episode: 3045, duration: 0.286s, episode steps: 25, steps per second: 87, episode reward: 10.311, mean reward: 0.412 [0.296, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.805, 10.100], loss: 0.055934, mae: 0.104295, mean_q: 0.532234
 53729/100000: episode: 3046, duration: 0.559s, episode steps: 54, steps per second: 97, episode reward: 23.300, mean reward: 0.431 [0.256, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.453, 10.397], loss: 0.006708, mae: 0.088058, mean_q: 0.511924
 53749/100000: episode: 3047, duration: 0.219s, episode steps: 20, steps per second: 91, episode reward: 8.733, mean reward: 0.437 [0.391, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.555], loss: 0.075092, mae: 0.137189, mean_q: 0.538830
 53750/100000: episode: 3048, duration: 0.018s, episode steps: 1, steps per second: 55, episode reward: 0.446, mean reward: 0.446 [0.446, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.467 [-0.070, 10.538], loss: 0.008164, mae: 0.111453, mean_q: 0.607185
 53751/100000: episode: 3049, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.070, 10.329], loss: 0.006846, mae: 0.092304, mean_q: 0.553221
 53767/100000: episode: 3050, duration: 0.141s, episode steps: 16, steps per second: 114, episode reward: 8.045, mean reward: 0.503 [0.350, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.353, 10.100], loss: 0.005855, mae: 0.085016, mean_q: 0.519993
 53783/100000: episode: 3051, duration: 0.157s, episode steps: 16, steps per second: 102, episode reward: 9.325, mean reward: 0.583 [0.453, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.640, 10.100], loss: 0.003972, mae: 0.069080, mean_q: 0.528721
 53808/100000: episode: 3052, duration: 0.234s, episode steps: 25, steps per second: 107, episode reward: 12.280, mean reward: 0.491 [0.403, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.788, 10.100], loss: 0.053978, mae: 0.098922, mean_q: 0.556898
 53825/100000: episode: 3053, duration: 0.155s, episode steps: 17, steps per second: 110, episode reward: 7.929, mean reward: 0.466 [0.369, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.516], loss: 0.010085, mae: 0.112727, mean_q: 0.549891
 53826/100000: episode: 3054, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.371 [-0.070, 10.400], loss: 0.006635, mae: 0.078464, mean_q: 0.519637
 53851/100000: episode: 3055, duration: 0.247s, episode steps: 25, steps per second: 101, episode reward: 10.529, mean reward: 0.421 [0.318, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.137, 10.100], loss: 0.004751, mae: 0.071762, mean_q: 0.532157
 53852/100000: episode: 3056, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.070, 10.357], loss: 0.004805, mae: 0.075575, mean_q: 0.549231
 53872/100000: episode: 3057, duration: 0.265s, episode steps: 20, steps per second: 76, episode reward: 10.544, mean reward: 0.527 [0.414, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.003, 10.485], loss: 0.003987, mae: 0.067475, mean_q: 0.530022
 53888/100000: episode: 3058, duration: 0.274s, episode steps: 16, steps per second: 58, episode reward: 9.109, mean reward: 0.569 [0.496, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.336, 10.100], loss: 0.004264, mae: 0.071552, mean_q: 0.568781
 53908/100000: episode: 3059, duration: 0.310s, episode steps: 20, steps per second: 64, episode reward: 9.485, mean reward: 0.474 [0.396, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.597], loss: 0.004089, mae: 0.069126, mean_q: 0.557020
 53962/100000: episode: 3060, duration: 0.770s, episode steps: 54, steps per second: 70, episode reward: 22.026, mean reward: 0.408 [0.253, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-1.111, 10.324], loss: 0.004692, mae: 0.071708, mean_q: 0.551356
 53987/100000: episode: 3061, duration: 0.303s, episode steps: 25, steps per second: 83, episode reward: 10.301, mean reward: 0.412 [0.245, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.314, 10.100], loss: 0.056991, mae: 0.106095, mean_q: 0.579620
 54001/100000: episode: 3062, duration: 0.134s, episode steps: 14, steps per second: 104, episode reward: 7.120, mean reward: 0.509 [0.370, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.700], loss: 0.005702, mae: 0.080024, mean_q: 0.577710
 54018/100000: episode: 3063, duration: 0.136s, episode steps: 17, steps per second: 125, episode reward: 8.168, mean reward: 0.480 [0.395, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.487], loss: 0.004668, mae: 0.074726, mean_q: 0.547731
 54043/100000: episode: 3064, duration: 0.192s, episode steps: 25, steps per second: 130, episode reward: 9.103, mean reward: 0.364 [0.228, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.217, 10.100], loss: 0.054838, mae: 0.094129, mean_q: 0.578875
 54044/100000: episode: 3065, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.398 [-0.070, 10.528], loss: 0.007260, mae: 0.103611, mean_q: 0.512977
 54062/100000: episode: 3066, duration: 0.158s, episode steps: 18, steps per second: 114, episode reward: 7.202, mean reward: 0.400 [0.242, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.470], loss: 0.082705, mae: 0.151641, mean_q: 0.562686
 54080/100000: episode: 3067, duration: 0.132s, episode steps: 18, steps per second: 136, episode reward: 8.706, mean reward: 0.484 [0.412, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.689], loss: 0.006903, mae: 0.089238, mean_q: 0.555187
 54105/100000: episode: 3068, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 12.062, mean reward: 0.482 [0.408, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.372, 10.100], loss: 0.056603, mae: 0.117431, mean_q: 0.588184
 54119/100000: episode: 3069, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 6.634, mean reward: 0.474 [0.378, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.577, 10.453], loss: 0.006919, mae: 0.091964, mean_q: 0.564207
 54137/100000: episode: 3070, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 6.877, mean reward: 0.382 [0.315, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.808, 10.439], loss: 0.005822, mae: 0.086178, mean_q: 0.583320
 54155/100000: episode: 3071, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 9.051, mean reward: 0.503 [0.321, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.511], loss: 0.076248, mae: 0.112470, mean_q: 0.609731
 54209/100000: episode: 3072, duration: 0.370s, episode steps: 54, steps per second: 146, episode reward: 16.011, mean reward: 0.297 [0.118, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.496, 10.230], loss: 0.004984, mae: 0.075002, mean_q: 0.577446
 54234/100000: episode: 3073, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 10.223, mean reward: 0.409 [0.240, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.322, 10.100], loss: 0.004385, mae: 0.071828, mean_q: 0.591467
 54251/100000: episode: 3074, duration: 0.135s, episode steps: 17, steps per second: 126, episode reward: 7.584, mean reward: 0.446 [0.343, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.882, 10.534], loss: 0.005038, mae: 0.073793, mean_q: 0.565003
 54252/100000: episode: 3075, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.441, mean reward: 0.441 [0.441, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.070, 10.390], loss: 0.004172, mae: 0.073321, mean_q: 0.629860
 54270/100000: episode: 3076, duration: 0.148s, episode steps: 18, steps per second: 122, episode reward: 9.691, mean reward: 0.538 [0.459, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.038, 10.666], loss: 0.004232, mae: 0.067494, mean_q: 0.580473
 54295/100000: episode: 3077, duration: 0.229s, episode steps: 25, steps per second: 109, episode reward: 10.569, mean reward: 0.423 [0.335, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.277, 10.100], loss: 0.003628, mae: 0.064481, mean_q: 0.587398
 54320/100000: episode: 3078, duration: 0.183s, episode steps: 25, steps per second: 137, episode reward: 8.127, mean reward: 0.325 [0.151, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.763, 10.100], loss: 0.004246, mae: 0.070427, mean_q: 0.607096
 54321/100000: episode: 3079, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.399 [-0.070, 10.435], loss: 0.004226, mae: 0.075039, mean_q: 0.516067
 54341/100000: episode: 3080, duration: 0.165s, episode steps: 20, steps per second: 121, episode reward: 8.896, mean reward: 0.445 [0.309, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.452], loss: 0.004378, mae: 0.070621, mean_q: 0.582783
 54358/100000: episode: 3081, duration: 0.154s, episode steps: 17, steps per second: 111, episode reward: 9.588, mean reward: 0.564 [0.516, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.937, 10.637], loss: 0.004487, mae: 0.071382, mean_q: 0.595744
 54378/100000: episode: 3082, duration: 0.151s, episode steps: 20, steps per second: 132, episode reward: 9.733, mean reward: 0.487 [0.386, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.172, 10.499], loss: 0.004086, mae: 0.068931, mean_q: 0.588989
 54403/100000: episode: 3083, duration: 0.204s, episode steps: 25, steps per second: 122, episode reward: 13.295, mean reward: 0.532 [0.475, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.805, 10.100], loss: 0.004088, mae: 0.068987, mean_q: 0.597016
 54421/100000: episode: 3084, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 8.399, mean reward: 0.467 [0.417, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.527], loss: 0.004249, mae: 0.068882, mean_q: 0.604865
 54437/100000: episode: 3085, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 8.682, mean reward: 0.543 [0.489, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.649, 10.100], loss: 0.004710, mae: 0.071534, mean_q: 0.597143
 54455/100000: episode: 3086, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 8.861, mean reward: 0.492 [0.387, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.627], loss: 0.004841, mae: 0.075966, mean_q: 0.618186
 54480/100000: episode: 3087, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 10.494, mean reward: 0.420 [0.133, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.074, 10.100], loss: 0.003906, mae: 0.067668, mean_q: 0.612143
 54500/100000: episode: 3088, duration: 0.149s, episode steps: 20, steps per second: 134, episode reward: 10.106, mean reward: 0.505 [0.379, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.548], loss: 0.133474, mae: 0.150853, mean_q: 0.632873
 54517/100000: episode: 3089, duration: 0.158s, episode steps: 17, steps per second: 108, episode reward: 6.222, mean reward: 0.366 [0.287, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.518], loss: 0.088258, mae: 0.166610, mean_q: 0.620975
 54531/100000: episode: 3090, duration: 0.109s, episode steps: 14, steps per second: 128, episode reward: 6.203, mean reward: 0.443 [0.353, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.675], loss: 0.009419, mae: 0.106182, mean_q: 0.599681
 54585/100000: episode: 3091, duration: 0.408s, episode steps: 54, steps per second: 133, episode reward: 11.222, mean reward: 0.208 [0.023, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.558, 10.119], loss: 0.004576, mae: 0.071892, mean_q: 0.615719
 54603/100000: episode: 3092, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 8.470, mean reward: 0.471 [0.255, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.511], loss: 0.075793, mae: 0.111643, mean_q: 0.645782
 54617/100000: episode: 3093, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 6.316, mean reward: 0.451 [0.396, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.538], loss: 0.014517, mae: 0.139760, mean_q: 0.596080
[RESULT] FALSIFICATION!
 54633/100000: episode: 3094, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 19.226, mean reward: 1.202 [0.576, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.338, 9.911], loss: 0.088264, mae: 0.129547, mean_q: 0.630750
 54634/100000: episode: 3095, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.070, 10.435], loss: 0.004026, mae: 0.074999, mean_q: 0.670684
 54650/100000: episode: 3096, duration: 0.128s, episode steps: 16, steps per second: 125, episode reward: 6.848, mean reward: 0.428 [0.358, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.389, 10.100], loss: 0.006402, mae: 0.085152, mean_q: 0.619384
 54668/100000: episode: 3097, duration: 0.146s, episode steps: 18, steps per second: 123, episode reward: 7.527, mean reward: 0.418 [0.345, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.152, 10.483], loss: 0.004096, mae: 0.068851, mean_q: 0.623216
 54693/100000: episode: 3098, duration: 0.191s, episode steps: 25, steps per second: 131, episode reward: 10.201, mean reward: 0.408 [0.264, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.474, 10.100], loss: 0.005389, mae: 0.079622, mean_q: 0.629671
 54711/100000: episode: 3099, duration: 0.136s, episode steps: 18, steps per second: 132, episode reward: 8.456, mean reward: 0.470 [0.351, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.546, 10.478], loss: 0.004978, mae: 0.072611, mean_q: 0.610737
 54731/100000: episode: 3100, duration: 0.132s, episode steps: 20, steps per second: 152, episode reward: 7.463, mean reward: 0.373 [0.278, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.384], loss: 0.004218, mae: 0.068221, mean_q: 0.626361
 54751/100000: episode: 3101, duration: 0.164s, episode steps: 20, steps per second: 122, episode reward: 7.085, mean reward: 0.354 [0.194, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.117, 10.370], loss: 0.065704, mae: 0.097116, mean_q: 0.664631
 54767/100000: episode: 3102, duration: 0.145s, episode steps: 16, steps per second: 110, episode reward: 8.449, mean reward: 0.528 [0.284, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.316, 10.100], loss: 0.086289, mae: 0.134078, mean_q: 0.651678
 54784/100000: episode: 3103, duration: 0.134s, episode steps: 17, steps per second: 127, episode reward: 7.714, mean reward: 0.454 [0.376, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.548], loss: 0.009519, mae: 0.105975, mean_q: 0.639699
 54802/100000: episode: 3104, duration: 0.184s, episode steps: 18, steps per second: 98, episode reward: 10.093, mean reward: 0.561 [0.434, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.984, 10.651], loss: 0.007296, mae: 0.085522, mean_q: 0.620531
 54827/100000: episode: 3105, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 11.293, mean reward: 0.452 [0.376, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.390, 10.100], loss: 0.004750, mae: 0.073872, mean_q: 0.637840
 54843/100000: episode: 3106, duration: 0.143s, episode steps: 16, steps per second: 112, episode reward: 8.174, mean reward: 0.511 [0.448, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.425, 10.100], loss: 0.003708, mae: 0.063181, mean_q: 0.630947
[Info] New level: 1.1343656778335571 | Considering 10/90 traces
 54868/100000: episode: 3107, duration: 4.521s, episode steps: 25, steps per second: 6, episode reward: 12.208, mean reward: 0.488 [0.429, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.374, 10.100], loss: 0.056142, mae: 0.097098, mean_q: 0.638800
 54876/100000: episode: 3108, duration: 0.052s, episode steps: 8, steps per second: 152, episode reward: 4.644, mean reward: 0.581 [0.534, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.699], loss: 0.007045, mae: 0.095943, mean_q: 0.641544
 54889/100000: episode: 3109, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 6.055, mean reward: 0.466 [0.376, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.542], loss: 0.005909, mae: 0.084717, mean_q: 0.631428
 54902/100000: episode: 3110, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 7.507, mean reward: 0.577 [0.499, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.778], loss: 0.100277, mae: 0.125186, mean_q: 0.659487
 54916/100000: episode: 3111, duration: 0.085s, episode steps: 14, steps per second: 166, episode reward: 7.113, mean reward: 0.508 [0.390, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.038, 10.555], loss: 0.182144, mae: 0.185514, mean_q: 0.739772
 54924/100000: episode: 3112, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 4.890, mean reward: 0.611 [0.585, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.682], loss: 0.014132, mae: 0.138268, mean_q: 0.646671
 54937/100000: episode: 3113, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 7.091, mean reward: 0.545 [0.475, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.637], loss: 0.006629, mae: 0.088895, mean_q: 0.653616
 54950/100000: episode: 3114, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 7.624, mean reward: 0.586 [0.483, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.060, 10.567], loss: 0.004804, mae: 0.072835, mean_q: 0.644834
 54964/100000: episode: 3115, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 7.981, mean reward: 0.570 [0.491, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.375, 10.100], loss: 0.005295, mae: 0.072369, mean_q: 0.657097
 54977/100000: episode: 3116, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 6.235, mean reward: 0.480 [0.417, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.938, 10.475], loss: 0.004200, mae: 0.070184, mean_q: 0.674723
[RESULT] FALSIFICATION!
 54981/100000: episode: 3117, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 11.809, mean reward: 2.952 [0.568, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.362, 10.067], loss: 0.005660, mae: 0.074622, mean_q: 0.714573
[RESULT] FALSIFICATION!
 54985/100000: episode: 3118, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 11.639, mean reward: 2.910 [0.519, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.014, 10.583], loss: 0.006548, mae: 0.074609, mean_q: 0.681960
[RESULT] FALSIFICATION!
 54987/100000: episode: 3119, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 10.664, mean reward: 5.332 [0.664, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.017, 10.457], loss: 0.004395, mae: 0.064241, mean_q: 0.591174
 55000/100000: episode: 3120, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 7.157, mean reward: 0.551 [0.455, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.430, 10.100], loss: 0.196095, mae: 0.137226, mean_q: 0.672413
 55013/100000: episode: 3121, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 6.961, mean reward: 0.535 [0.499, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.327, 10.653], loss: 0.204668, mae: 0.217539, mean_q: 0.735128
 55026/100000: episode: 3122, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 6.640, mean reward: 0.511 [0.398, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.539, 10.100], loss: 0.015196, mae: 0.142067, mean_q: 0.643097
 55040/100000: episode: 3123, duration: 0.100s, episode steps: 14, steps per second: 140, episode reward: 8.075, mean reward: 0.577 [0.365, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.431, 10.100], loss: 0.187138, mae: 0.162173, mean_q: 0.713827
 55053/100000: episode: 3124, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 7.830, mean reward: 0.602 [0.559, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.489, 10.100], loss: 0.019304, mae: 0.154376, mean_q: 0.670755
 55066/100000: episode: 3125, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 8.417, mean reward: 0.647 [0.587, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.553, 10.100], loss: 0.009086, mae: 0.094534, mean_q: 0.695091
 55079/100000: episode: 3126, duration: 0.088s, episode steps: 13, steps per second: 148, episode reward: 7.012, mean reward: 0.539 [0.507, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.350, 10.560], loss: 0.005843, mae: 0.083675, mean_q: 0.669602
 55093/100000: episode: 3127, duration: 0.092s, episode steps: 14, steps per second: 153, episode reward: 7.686, mean reward: 0.549 [0.471, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.613], loss: 0.005667, mae: 0.074893, mean_q: 0.690150
 55106/100000: episode: 3128, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 6.776, mean reward: 0.521 [0.460, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.538], loss: 0.193834, mae: 0.156621, mean_q: 0.699022
 55119/100000: episode: 3129, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 7.172, mean reward: 0.552 [0.418, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-1.106, 10.500], loss: 0.016462, mae: 0.143227, mean_q: 0.728461
 55131/100000: episode: 3130, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 6.009, mean reward: 0.501 [0.383, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.648], loss: 0.008018, mae: 0.100473, mean_q: 0.666543
 55144/100000: episode: 3131, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 7.161, mean reward: 0.551 [0.519, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.617], loss: 0.005367, mae: 0.074877, mean_q: 0.704464
 55157/100000: episode: 3132, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 7.877, mean reward: 0.606 [0.531, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.618, 10.100], loss: 0.004936, mae: 0.077188, mean_q: 0.696569
 55171/100000: episode: 3133, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 8.017, mean reward: 0.573 [0.516, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.617, 10.660], loss: 0.182759, mae: 0.148390, mean_q: 0.745843
 55183/100000: episode: 3134, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 6.220, mean reward: 0.518 [0.484, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.794, 10.526], loss: 0.130083, mae: 0.199502, mean_q: 0.703732
 55196/100000: episode: 3135, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 7.172, mean reward: 0.552 [0.495, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.708], loss: 0.009114, mae: 0.104081, mean_q: 0.670826
 55209/100000: episode: 3136, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 7.058, mean reward: 0.543 [0.405, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.424, 10.519], loss: 0.099353, mae: 0.122930, mean_q: 0.729109
 55222/100000: episode: 3137, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 6.262, mean reward: 0.482 [0.327, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.553], loss: 0.098071, mae: 0.111736, mean_q: 0.713663
[RESULT] FALSIFICATION!
 55227/100000: episode: 3138, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 12.679, mean reward: 2.536 [0.639, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.326, 9.911], loss: 0.006357, mae: 0.088784, mean_q: 0.689599
 55239/100000: episode: 3139, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 6.542, mean reward: 0.545 [0.478, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.158, 10.559], loss: 0.005066, mae: 0.074718, mean_q: 0.716647
 55252/100000: episode: 3140, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 7.959, mean reward: 0.612 [0.566, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.039, 10.681], loss: 0.004896, mae: 0.074101, mean_q: 0.735456
 55265/100000: episode: 3141, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 6.512, mean reward: 0.501 [0.428, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.575, 10.545], loss: 0.099867, mae: 0.114248, mean_q: 0.728672
 55277/100000: episode: 3142, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 6.265, mean reward: 0.522 [0.466, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.277, 10.579], loss: 0.104430, mae: 0.106110, mean_q: 0.756525
 55289/100000: episode: 3143, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 7.676, mean reward: 0.640 [0.568, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.437, 10.703], loss: 0.006187, mae: 0.084000, mean_q: 0.675754
 55297/100000: episode: 3144, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 4.081, mean reward: 0.510 [0.436, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.511, 10.100], loss: 0.005406, mae: 0.080485, mean_q: 0.673743
 55311/100000: episode: 3145, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 7.422, mean reward: 0.530 [0.364, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.310, 10.100], loss: 0.004896, mae: 0.072795, mean_q: 0.696162
 55323/100000: episode: 3146, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 7.477, mean reward: 0.623 [0.543, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.753], loss: 0.005063, mae: 0.074337, mean_q: 0.710564
 55335/100000: episode: 3147, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 6.909, mean reward: 0.576 [0.511, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-1.417, 10.509], loss: 0.005013, mae: 0.070724, mean_q: 0.714773
 55347/100000: episode: 3148, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 6.136, mean reward: 0.511 [0.454, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.567, 10.531], loss: 0.003939, mae: 0.068818, mean_q: 0.722123
 55359/100000: episode: 3149, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 6.508, mean reward: 0.542 [0.467, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.682], loss: 0.004927, mae: 0.072289, mean_q: 0.698737
 55372/100000: episode: 3150, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 7.745, mean reward: 0.596 [0.547, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.635], loss: 0.100277, mae: 0.106166, mean_q: 0.718813
[RESULT] FALSIFICATION!
 55382/100000: episode: 3151, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 15.020, mean reward: 1.502 [0.466, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.015, 10.780], loss: 0.369902, mae: 0.209464, mean_q: 0.780744
 55396/100000: episode: 3152, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 7.898, mean reward: 0.564 [0.487, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.320, 10.100], loss: 0.107426, mae: 0.170989, mean_q: 0.762810
 55410/100000: episode: 3153, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 7.546, mean reward: 0.539 [0.437, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.515, 10.100], loss: 0.008735, mae: 0.097329, mean_q: 0.696747
 55424/100000: episode: 3154, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 6.803, mean reward: 0.486 [0.390, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.435, 10.100], loss: 0.266091, mae: 0.202485, mean_q: 0.812162
 55437/100000: episode: 3155, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 7.320, mean reward: 0.563 [0.513, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.363, 10.100], loss: 0.015082, mae: 0.140889, mean_q: 0.709487
 55445/100000: episode: 3156, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 4.774, mean reward: 0.597 [0.550, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.678], loss: 0.006137, mae: 0.087678, mean_q: 0.778606
 55458/100000: episode: 3157, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 7.093, mean reward: 0.546 [0.359, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.371, 10.100], loss: 0.102566, mae: 0.134498, mean_q: 0.742619
 55472/100000: episode: 3158, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 8.421, mean reward: 0.602 [0.536, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.626], loss: 0.008005, mae: 0.091939, mean_q: 0.727369
 55484/100000: episode: 3159, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 5.858, mean reward: 0.488 [0.396, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.319, 10.472], loss: 0.213620, mae: 0.167910, mean_q: 0.773468
 55497/100000: episode: 3160, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 5.805, mean reward: 0.447 [0.348, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.509], loss: 0.006987, mae: 0.090556, mean_q: 0.721980
 55510/100000: episode: 3161, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 7.022, mean reward: 0.540 [0.457, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.494, 10.680], loss: 0.096744, mae: 0.107999, mean_q: 0.744103
 55518/100000: episode: 3162, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 4.517, mean reward: 0.565 [0.395, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.614, 10.100], loss: 0.163107, mae: 0.148327, mean_q: 0.739169
 55532/100000: episode: 3163, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 8.135, mean reward: 0.581 [0.500, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.221, 10.100], loss: 0.012686, mae: 0.126424, mean_q: 0.754913
 55540/100000: episode: 3164, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 4.668, mean reward: 0.584 [0.544, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.515, 10.100], loss: 0.006277, mae: 0.085791, mean_q: 0.691875
 55554/100000: episode: 3165, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 7.447, mean reward: 0.532 [0.449, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.461, 10.100], loss: 0.004584, mae: 0.070023, mean_q: 0.745627
 55566/100000: episode: 3166, duration: 0.089s, episode steps: 12, steps per second: 134, episode reward: 6.965, mean reward: 0.580 [0.530, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.673], loss: 0.206923, mae: 0.150310, mean_q: 0.810020
 55578/100000: episode: 3167, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 6.618, mean reward: 0.552 [0.479, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.656], loss: 0.113946, mae: 0.136617, mean_q: 0.788258
 55591/100000: episode: 3168, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 6.952, mean reward: 0.535 [0.486, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.630], loss: 0.008745, mae: 0.087938, mean_q: 0.723933
 55603/100000: episode: 3169, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 6.312, mean reward: 0.526 [0.468, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.596, 10.546], loss: 0.110249, mae: 0.118538, mean_q: 0.789439
 55616/100000: episode: 3170, duration: 0.097s, episode steps: 13, steps per second: 134, episode reward: 7.190, mean reward: 0.553 [0.480, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.415, 10.100], loss: 0.006667, mae: 0.084000, mean_q: 0.749012
[RESULT] FALSIFICATION!
 55624/100000: episode: 3171, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 14.176, mean reward: 1.772 [0.564, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.461, 10.046], loss: 0.156599, mae: 0.122957, mean_q: 0.800696
[RESULT] FALSIFICATION!
 55629/100000: episode: 3172, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 12.510, mean reward: 2.502 [0.596, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.282, 9.952], loss: 0.008529, mae: 0.101463, mean_q: 0.732963
 55643/100000: episode: 3173, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 8.001, mean reward: 0.572 [0.488, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-1.846, 10.594], loss: 0.006377, mae: 0.084243, mean_q: 0.758361
 55651/100000: episode: 3174, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 5.237, mean reward: 0.655 [0.606, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.524, 10.100], loss: 0.004365, mae: 0.071601, mean_q: 0.765104
 55659/100000: episode: 3175, duration: 0.060s, episode steps: 8, steps per second: 134, episode reward: 4.208, mean reward: 0.526 [0.495, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.251, 10.602], loss: 0.004611, mae: 0.071532, mean_q: 0.766747
[RESULT] FALSIFICATION!
 55665/100000: episode: 3176, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 13.282, mean reward: 2.214 [0.609, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.324, 9.952], loss: 0.004273, mae: 0.066868, mean_q: 0.720797
 55678/100000: episode: 3177, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 7.363, mean reward: 0.566 [0.512, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.433, 10.100], loss: 0.004718, mae: 0.069949, mean_q: 0.743324
 55686/100000: episode: 3178, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 4.356, mean reward: 0.545 [0.475, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.838, 10.100], loss: 0.159066, mae: 0.134699, mean_q: 0.798821
 55699/100000: episode: 3179, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 7.687, mean reward: 0.591 [0.551, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.627, 10.100], loss: 0.190551, mae: 0.148825, mean_q: 0.790481
 55712/100000: episode: 3180, duration: 0.164s, episode steps: 13, steps per second: 79, episode reward: 7.372, mean reward: 0.567 [0.480, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.437, 10.447], loss: 0.102049, mae: 0.136594, mean_q: 0.773444
 55724/100000: episode: 3181, duration: 0.131s, episode steps: 12, steps per second: 92, episode reward: 6.391, mean reward: 0.533 [0.467, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.643, 10.541], loss: 0.110793, mae: 0.116339, mean_q: 0.762722
 55737/100000: episode: 3182, duration: 0.122s, episode steps: 13, steps per second: 106, episode reward: 8.057, mean reward: 0.620 [0.559, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.584], loss: 0.104731, mae: 0.126752, mean_q: 0.806405
 55750/100000: episode: 3183, duration: 0.183s, episode steps: 13, steps per second: 71, episode reward: 6.006, mean reward: 0.462 [0.406, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.661], loss: 0.102449, mae: 0.117612, mean_q: 0.802976
 55764/100000: episode: 3184, duration: 0.163s, episode steps: 14, steps per second: 86, episode reward: 9.024, mean reward: 0.645 [0.596, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.438, 10.100], loss: 0.095206, mae: 0.133229, mean_q: 0.784172
 55772/100000: episode: 3185, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 4.767, mean reward: 0.596 [0.569, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.713], loss: 0.160690, mae: 0.146076, mean_q: 0.818913
[RESULT] FALSIFICATION!
 55785/100000: episode: 3186, duration: 0.125s, episode steps: 13, steps per second: 104, episode reward: 17.364, mean reward: 1.336 [0.555, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.779], loss: 0.105100, mae: 0.141177, mean_q: 0.805110
 55798/100000: episode: 3187, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 7.174, mean reward: 0.552 [0.491, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.221, 10.600], loss: 0.100121, mae: 0.112426, mean_q: 0.754829
 55810/100000: episode: 3188, duration: 0.084s, episode steps: 12, steps per second: 142, episode reward: 5.895, mean reward: 0.491 [0.344, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.792, 10.565], loss: 0.111484, mae: 0.165805, mean_q: 0.765849
[RESULT] FALSIFICATION!
 55815/100000: episode: 3189, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 12.384, mean reward: 2.477 [0.583, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.013, 10.782], loss: 0.009077, mae: 0.105097, mean_q: 0.864557
 55829/100000: episode: 3190, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 7.947, mean reward: 0.568 [0.527, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.654], loss: 0.181629, mae: 0.156195, mean_q: 0.818922
[RESULT] FALSIFICATION!
 55840/100000: episode: 3191, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 16.221, mean reward: 1.475 [0.558, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.048, 10.627], loss: 0.010995, mae: 0.119963, mean_q: 0.717000
 55854/100000: episode: 3192, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 7.979, mean reward: 0.570 [0.520, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.427, 10.100], loss: 0.090496, mae: 0.121408, mean_q: 0.782858
 55867/100000: episode: 3193, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 6.472, mean reward: 0.498 [0.458, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.677], loss: 0.117288, mae: 0.159998, mean_q: 0.785752
 55875/100000: episode: 3194, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 4.079, mean reward: 0.510 [0.480, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.521], loss: 0.013419, mae: 0.131130, mean_q: 0.685304
[RESULT] FALSIFICATION!
 55879/100000: episode: 3195, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 11.883, mean reward: 2.971 [0.604, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.161, 9.911], loss: 0.009148, mae: 0.094930, mean_q: 0.890427
 55891/100000: episode: 3196, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 6.808, mean reward: 0.567 [0.541, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.573], loss: 0.104526, mae: 0.119478, mean_q: 0.776455
[Info] New level: 1.4739625453948975 | Considering 10/90 traces
 55904/100000: episode: 3197, duration: 4.444s, episode steps: 13, steps per second: 3, episode reward: 7.952, mean reward: 0.612 [0.489, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.717], loss: 0.289576, mae: 0.185629, mean_q: 0.821312
[RESULT] FALSIFICATION!
 55905/100000: episode: 3198, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.283, 9.911], loss: 0.008845, mae: 0.106193, mean_q: 0.910049
[RESULT] FALSIFICATION!
 55906/100000: episode: 3199, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.198, 9.864], loss: 0.012741, mae: 0.108842, mean_q: 0.783410
 55915/100000: episode: 3200, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 5.704, mean reward: 0.634 [0.601, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.680], loss: 0.145452, mae: 0.141360, mean_q: 0.810345
 55928/100000: episode: 3201, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 7.156, mean reward: 0.550 [0.448, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.481, 10.100], loss: 0.283678, mae: 0.190150, mean_q: 0.840925
[RESULT] FALSIFICATION!
 55929/100000: episode: 3202, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.380, 9.988], loss: 0.008917, mae: 0.092396, mean_q: 0.865710
[RESULT] FALSIFICATION!
 55932/100000: episode: 3203, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 11.366, mean reward: 3.789 [0.680, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.483, 10.098], loss: 0.010352, mae: 0.113114, mean_q: 0.720051
 55943/100000: episode: 3204, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 6.037, mean reward: 0.549 [0.450, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.340, 10.100], loss: 0.227264, mae: 0.178083, mean_q: 0.885071
[RESULT] FALSIFICATION!
 55946/100000: episode: 3205, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 11.314, mean reward: 3.771 [0.650, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.360, 9.864], loss: 0.010451, mae: 0.093083, mean_q: 0.766080
[RESULT] FALSIFICATION!
 55950/100000: episode: 3206, duration: 0.030s, episode steps: 4, steps per second: 134, episode reward: 11.901, mean reward: 2.975 [0.619, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.014, 10.716], loss: 0.316000, mae: 0.215862, mean_q: 0.903739
[RESULT] FALSIFICATION!
 55952/100000: episode: 3207, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.621, mean reward: 5.311 [0.621, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.270, 9.813], loss: 0.035580, mae: 0.226524, mean_q: 1.019732
 55965/100000: episode: 3208, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 7.133, mean reward: 0.549 [0.483, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.881, 10.100], loss: 0.303688, mae: 0.253361, mean_q: 0.878804
 55974/100000: episode: 3209, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 5.251, mean reward: 0.583 [0.533, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.623], loss: 0.548524, mae: 0.296646, mean_q: 0.863904
[RESULT] FALSIFICATION!
 55976/100000: episode: 3210, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 10.690, mean reward: 5.345 [0.690, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.314, 10.020], loss: 0.015869, mae: 0.135229, mean_q: 0.870097
[RESULT] FALSIFICATION!
 55978/100000: episode: 3211, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 10.650, mean reward: 5.325 [0.650, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.229, 9.911], loss: 0.013401, mae: 0.132392, mean_q: 0.732683
 55987/100000: episode: 3212, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 4.908, mean reward: 0.545 [0.458, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.448, 10.100], loss: 0.018763, mae: 0.147174, mean_q: 0.837333
 55998/100000: episode: 3213, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 7.254, mean reward: 0.659 [0.616, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.666, 10.100], loss: 0.115979, mae: 0.145951, mean_q: 0.826932
[RESULT] FALSIFICATION!
 56000/100000: episode: 3214, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 10.680, mean reward: 5.340 [0.680, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.320, 9.952], loss: 0.014751, mae: 0.139157, mean_q: 0.746735
 56004/100000: episode: 3215, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 2.560, mean reward: 0.640 [0.629, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.498, 10.100], loss: 0.313457, mae: 0.179038, mean_q: 0.810840
[RESULT] FALSIFICATION!
 56005/100000: episode: 3216, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.375, 9.988], loss: 1.183968, mae: 0.407371, mean_q: 0.901286
[RESULT] FALSIFICATION!
 56009/100000: episode: 3217, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 11.996, mean reward: 2.999 [0.643, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.340, 9.911], loss: 0.022317, mae: 0.163681, mean_q: 0.973741
 56017/100000: episode: 3218, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 4.299, mean reward: 0.537 [0.494, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.540], loss: 0.323881, mae: 0.217269, mean_q: 0.819022
 56028/100000: episode: 3219, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 6.974, mean reward: 0.634 [0.598, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.340, 10.100], loss: 0.668462, mae: 0.363366, mean_q: 0.919514
 56037/100000: episode: 3220, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 4.842, mean reward: 0.538 [0.453, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.540, 10.100], loss: 0.187634, mae: 0.294110, mean_q: 0.689072
[RESULT] FALSIFICATION!
 56038/100000: episode: 3221, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.388, 9.988], loss: 1.178069, mae: 0.455763, mean_q: 0.985165
[RESULT] FALSIFICATION!
 56040/100000: episode: 3222, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.672, mean reward: 5.336 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.207, 9.813], loss: 1.165441, mae: 0.631985, mean_q: 1.170041
 56051/100000: episode: 3223, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 6.255, mean reward: 0.569 [0.515, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.416, 10.100], loss: 0.265710, mae: 0.340796, mean_q: 0.857623
[RESULT] FALSIFICATION!
 56052/100000: episode: 3224, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.398, 10.082], loss: 0.031714, mae: 0.175725, mean_q: 0.933742
 56061/100000: episode: 3225, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 5.167, mean reward: 0.574 [0.519, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.681], loss: 0.160597, mae: 0.243990, mean_q: 1.014013
 56069/100000: episode: 3226, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 4.404, mean reward: 0.551 [0.518, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.540], loss: 0.157469, mae: 0.168377, mean_q: 0.774993
 56080/100000: episode: 3227, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 6.748, mean reward: 0.613 [0.556, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.499, 10.100], loss: 0.211827, mae: 0.175659, mean_q: 0.899679
 56093/100000: episode: 3228, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 7.686, mean reward: 0.591 [0.464, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.417, 10.100], loss: 0.625850, mae: 0.313546, mean_q: 0.954743
[RESULT] FALSIFICATION!
 56095/100000: episode: 3229, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.673, mean reward: 5.336 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.219, 9.813], loss: 0.033746, mae: 0.193530, mean_q: 0.930581
[RESULT] FALSIFICATION!
 56096/100000: episode: 3230, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.214, 9.756], loss: 0.024443, mae: 0.178396, mean_q: 0.878677
[RESULT] FALSIFICATION!
 56097/100000: episode: 3231, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.177, 9.756], loss: 0.018792, mae: 0.162051, mean_q: 0.887404
[RESULT] FALSIFICATION!
 56098/100000: episode: 3232, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.262, 9.864], loss: 0.023142, mae: 0.181875, mean_q: 0.862509
 56106/100000: episode: 3233, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 4.226, mean reward: 0.528 [0.474, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.549], loss: 0.014941, mae: 0.130892, mean_q: 0.775319
 56115/100000: episode: 3234, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 5.359, mean reward: 0.595 [0.579, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.528, 10.100], loss: 0.510679, mae: 0.273179, mean_q: 0.947080
[RESULT] FALSIFICATION!
 56116/100000: episode: 3235, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.283, 9.911], loss: 0.030575, mae: 0.221396, mean_q: 1.065112
 56125/100000: episode: 3236, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 5.676, mean reward: 0.631 [0.555, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.153, 10.599], loss: 0.286183, mae: 0.217974, mean_q: 0.813675
[RESULT] FALSIFICATION!
 56127/100000: episode: 3237, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 10.663, mean reward: 5.332 [0.663, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.175, 9.911], loss: 0.562991, mae: 0.268746, mean_q: 0.887514
[RESULT] FALSIFICATION!
 56130/100000: episode: 3238, duration: 0.026s, episode steps: 3, steps per second: 118, episode reward: 11.303, mean reward: 3.768 [0.625, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.014, 10.684], loss: 0.033421, mae: 0.199943, mean_q: 1.026192
 56138/100000: episode: 3239, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.810, mean reward: 0.601 [0.522, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.588], loss: 0.017094, mae: 0.143306, mean_q: 0.803606
 56149/100000: episode: 3240, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 6.269, mean reward: 0.570 [0.493, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.374, 10.100], loss: 0.012275, mae: 0.112766, mean_q: 0.832454
[RESULT] FALSIFICATION!
 56155/100000: episode: 3241, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 12.835, mean reward: 2.139 [0.518, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.217, 10.702], loss: 0.379678, mae: 0.188071, mean_q: 0.828157
[RESULT] FALSIFICATION!
 56156/100000: episode: 3242, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.325, 9.911], loss: 0.017940, mae: 0.158322, mean_q: 1.001506
[RESULT] FALSIFICATION!
 56163/100000: episode: 3243, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 13.778, mean reward: 1.968 [0.602, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.350, 10.020], loss: 0.329373, mae: 0.185781, mean_q: 0.813352
 56172/100000: episode: 3244, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 4.993, mean reward: 0.555 [0.504, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.491, 10.100], loss: 0.629857, mae: 0.399210, mean_q: 1.055790
 56185/100000: episode: 3245, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 7.738, mean reward: 0.595 [0.537, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.463, 10.100], loss: 0.746747, mae: 0.369461, mean_q: 0.977888
[RESULT] FALSIFICATION!
 56188/100000: episode: 3246, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 11.166, mean reward: 3.722 [0.554, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.015, 10.708], loss: 0.019127, mae: 0.158173, mean_q: 0.741679
[RESULT] FALSIFICATION!
 56189/100000: episode: 3247, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.338, 9.911], loss: 0.026257, mae: 0.206699, mean_q: 0.718681
[RESULT] FALSIFICATION!
 56192/100000: episode: 3248, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 11.301, mean reward: 3.767 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.281, 9.864], loss: 0.020915, mae: 0.154981, mean_q: 0.785111
 56202/100000: episode: 3249, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 6.521, mean reward: 0.652 [0.605, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.793, 10.100], loss: 0.560377, mae: 0.318295, mean_q: 0.957059
 56213/100000: episode: 3250, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 6.414, mean reward: 0.583 [0.521, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.606, 10.100], loss: 0.401027, mae: 0.245923, mean_q: 0.930502
 56224/100000: episode: 3251, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 6.362, mean reward: 0.578 [0.507, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.460, 10.100], loss: 0.318122, mae: 0.212489, mean_q: 0.833526
[RESULT] FALSIFICATION!
 56230/100000: episode: 3252, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 12.989, mean reward: 2.165 [0.558, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.341, 10.451], loss: 0.186525, mae: 0.188994, mean_q: 0.924667
 56243/100000: episode: 3253, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 6.960, mean reward: 0.535 [0.372, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.434, 10.100], loss: 0.448251, mae: 0.240208, mean_q: 0.918259
[RESULT] FALSIFICATION!
 56244/100000: episode: 3254, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.318, 9.988], loss: 0.976654, mae: 0.389368, mean_q: 0.949785
[RESULT] FALSIFICATION!
 56245/100000: episode: 3255, duration: 0.016s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.318, 9.988], loss: 0.023454, mae: 0.164031, mean_q: 0.895332
[RESULT] FALSIFICATION!
 56246/100000: episode: 3256, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.409, 10.082], loss: 0.022759, mae: 0.153613, mean_q: 0.991706
 56254/100000: episode: 3257, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 4.892, mean reward: 0.611 [0.561, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.554], loss: 0.153955, mae: 0.175298, mean_q: 0.820340
 56258/100000: episode: 3258, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 2.569, mean reward: 0.642 [0.612, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.526, 10.100], loss: 0.266885, mae: 0.181754, mean_q: 0.902053
 56267/100000: episode: 3259, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 4.892, mean reward: 0.544 [0.477, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.290, 10.100], loss: 0.020298, mae: 0.151703, mean_q: 0.962833
 56276/100000: episode: 3260, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 4.664, mean reward: 0.518 [0.414, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.553, 10.100], loss: 0.381258, mae: 0.234420, mean_q: 0.872992
[RESULT] FALSIFICATION!
 56280/100000: episode: 3261, duration: 0.032s, episode steps: 4, steps per second: 124, episode reward: 11.896, mean reward: 2.974 [0.618, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.165, 9.988], loss: 0.033794, mae: 0.191631, mean_q: 1.039220
[RESULT] FALSIFICATION!
 56281/100000: episode: 3262, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.412, 9.988], loss: 0.011638, mae: 0.125032, mean_q: 0.919708
[RESULT] FALSIFICATION!
 56283/100000: episode: 3263, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 10.666, mean reward: 5.333 [0.666, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.212, 9.813], loss: 0.029691, mae: 0.207587, mean_q: 0.678984
[RESULT] FALSIFICATION!
 56285/100000: episode: 3264, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 10.699, mean reward: 5.349 [0.699, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.285, 9.813], loss: 0.012881, mae: 0.132059, mean_q: 0.733871
[RESULT] FALSIFICATION!
 56287/100000: episode: 3265, duration: 0.022s, episode steps: 2, steps per second: 91, episode reward: 10.692, mean reward: 5.346 [0.692, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.354, 10.020], loss: 0.548173, mae: 0.243940, mean_q: 0.966928
[RESULT] FALSIFICATION!
 56288/100000: episode: 3266, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.422, 10.082], loss: 0.031173, mae: 0.219466, mean_q: 1.026966
 56297/100000: episode: 3267, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 5.180, mean reward: 0.576 [0.489, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.605], loss: 0.022686, mae: 0.155082, mean_q: 0.973218
[RESULT] FALSIFICATION!
 56298/100000: episode: 3268, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.266, 9.864], loss: 0.017484, mae: 0.153550, mean_q: 0.853359
[RESULT] FALSIFICATION!
 56299/100000: episode: 3269, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.287, 9.911], loss: 0.011779, mae: 0.126943, mean_q: 0.695219
 56309/100000: episode: 3270, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 6.199, mean reward: 0.620 [0.548, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.616, 10.100], loss: 0.678256, mae: 0.356864, mean_q: 1.021607
[RESULT] FALSIFICATION!
 56310/100000: episode: 3271, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.339, 9.988], loss: 1.033964, mae: 0.525000, mean_q: 1.182568
 56319/100000: episode: 3272, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 4.645, mean reward: 0.516 [0.487, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.533, 10.100], loss: 0.259918, mae: 0.268791, mean_q: 0.964710
[RESULT] FALSIFICATION!
 56320/100000: episode: 3273, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.453, 10.082], loss: 1.114985, mae: 0.396786, mean_q: 0.685874
 56324/100000: episode: 3274, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 2.304, mean reward: 0.576 [0.519, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.506, 10.100], loss: 0.513106, mae: 0.318842, mean_q: 1.069897
 56337/100000: episode: 3275, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 8.118, mean reward: 0.624 [0.539, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.639, 10.100], loss: 0.547463, mae: 0.289655, mean_q: 0.983024
[RESULT] FALSIFICATION!
 56339/100000: episode: 3276, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 10.682, mean reward: 5.341 [0.682, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.369, 10.020], loss: 0.554829, mae: 0.260994, mean_q: 0.867246
 56348/100000: episode: 3277, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 5.519, mean reward: 0.613 [0.557, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.700], loss: 0.362885, mae: 0.257274, mean_q: 1.034397
[RESULT] FALSIFICATION!
 56356/100000: episode: 3278, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 14.650, mean reward: 1.831 [0.643, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.328, 10.046], loss: 0.515875, mae: 0.358561, mean_q: 1.061846
 56364/100000: episode: 3279, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 4.685, mean reward: 0.586 [0.506, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.090, 10.606], loss: 0.426462, mae: 0.270030, mean_q: 0.957631
[RESULT] FALSIFICATION!
 56365/100000: episode: 3280, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.204, 9.756], loss: 2.208822, mae: 0.683295, mean_q: 1.080241
 56376/100000: episode: 3281, duration: 0.078s, episode steps: 11, steps per second: 142, episode reward: 6.829, mean reward: 0.621 [0.537, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.776, 10.100], loss: 0.293925, mae: 0.259145, mean_q: 1.032854
 56387/100000: episode: 3282, duration: 0.069s, episode steps: 11, steps per second: 158, episode reward: 6.991, mean reward: 0.636 [0.535, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.982, 10.100], loss: 0.317474, mae: 0.228209, mean_q: 0.856719
[RESULT] FALSIFICATION!
 56395/100000: episode: 3283, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 14.578, mean reward: 1.822 [0.627, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.447, 10.082], loss: 0.162042, mae: 0.246562, mean_q: 1.040671
 56406/100000: episode: 3284, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 6.904, mean reward: 0.628 [0.561, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.476, 10.100], loss: 0.575587, mae: 0.296387, mean_q: 0.908788
 56414/100000: episode: 3285, duration: 0.055s, episode steps: 8, steps per second: 145, episode reward: 4.178, mean reward: 0.522 [0.463, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-1.046, 10.564], loss: 0.323350, mae: 0.373466, mean_q: 1.190094
 56423/100000: episode: 3286, duration: 0.061s, episode steps: 9, steps per second: 146, episode reward: 5.270, mean reward: 0.586 [0.519, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.531, 10.100], loss: 0.522083, mae: 0.299214, mean_q: 0.932506
[RESULT] FALSIFICATION!
[Info] New level: 2.3391027450561523 | Considering 10/90 traces
 56424/100000: episode: 3287, duration: 4.090s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.314, 9.911], loss: 0.042628, mae: 0.241740, mean_q: 1.020686
[RESULT] FALSIFICATION!
 56425/100000: episode: 3288, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.232, 9.864], loss: 0.029828, mae: 0.185855, mean_q: 1.061821
[RESULT] FALSIFICATION!
 56426/100000: episode: 3289, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.284, 9.813], loss: 0.025464, mae: 0.160454, mean_q: 0.990145
[RESULT] FALSIFICATION!
 56427/100000: episode: 3290, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.318, 9.864], loss: 0.022925, mae: 0.176660, mean_q: 0.695064
[RESULT] FALSIFICATION!
 56428/100000: episode: 3291, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.417, 10.020], loss: 0.026535, mae: 0.173306, mean_q: 0.935128
[RESULT] FALSIFICATION!
 56429/100000: episode: 3292, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.264, 9.813], loss: 1.178286, mae: 0.427924, mean_q: 0.829812
[RESULT] FALSIFICATION!
 56430/100000: episode: 3293, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.304, 9.952], loss: 0.023570, mae: 0.161817, mean_q: 0.918976
 56442/100000: episode: 3294, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 7.626, mean reward: 0.635 [0.580, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.699, 10.100], loss: 0.462175, mae: 0.268953, mean_q: 1.049647
[RESULT] FALSIFICATION!
 56443/100000: episode: 3295, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.380, 10.020], loss: 0.028221, mae: 0.170098, mean_q: 0.998741
[RESULT] FALSIFICATION!
 56450/100000: episode: 3296, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 14.097, mean reward: 2.014 [0.671, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.450, 10.093], loss: 0.983249, mae: 0.443354, mean_q: 1.094941
[RESULT] FALSIFICATION!
 56456/100000: episode: 3297, duration: 0.047s, episode steps: 6, steps per second: 127, episode reward: 13.104, mean reward: 2.184 [0.583, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.328, 10.020], loss: 1.271689, mae: 0.664213, mean_q: 1.266745
[RESULT] FALSIFICATION!
 56459/100000: episode: 3298, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 11.325, mean reward: 3.775 [0.647, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.329, 9.952], loss: 0.384646, mae: 0.424374, mean_q: 1.249878
 56466/100000: episode: 3299, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 4.215, mean reward: 0.602 [0.535, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.518, 10.100], loss: 0.030420, mae: 0.181626, mean_q: 0.910978
 56478/100000: episode: 3300, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 6.996, mean reward: 0.583 [0.508, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.646, 10.100], loss: 0.643988, mae: 0.443514, mean_q: 1.215334
[RESULT] FALSIFICATION!
 56479/100000: episode: 3301, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.319, 9.864], loss: 0.934668, mae: 0.403403, mean_q: 1.118190
[RESULT] FALSIFICATION!
 56480/100000: episode: 3302, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.285, 9.864], loss: 0.052411, mae: 0.200899, mean_q: 0.913876
 56492/100000: episode: 3303, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 7.335, mean reward: 0.611 [0.507, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.512, 10.100], loss: 0.728908, mae: 0.421966, mean_q: 1.082311
 56503/100000: episode: 3304, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 6.605, mean reward: 0.600 [0.540, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.269, 10.100], loss: 0.392291, mae: 0.299864, mean_q: 0.965557
[RESULT] FALSIFICATION!
 56512/100000: episode: 3305, duration: 0.062s, episode steps: 9, steps per second: 144, episode reward: 15.208, mean reward: 1.690 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.725, 10.082], loss: 0.494355, mae: 0.332034, mean_q: 1.067182
[RESULT] FALSIFICATION!
 56513/100000: episode: 3306, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.253, 9.813], loss: 0.034298, mae: 0.187244, mean_q: 1.043074
 56525/100000: episode: 3307, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 6.770, mean reward: 0.564 [0.475, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.459, 10.100], loss: 0.595963, mae: 0.332838, mean_q: 1.080990
[RESULT] FALSIFICATION!
 56526/100000: episode: 3308, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.349, 9.952], loss: 1.287874, mae: 0.451569, mean_q: 1.046674
[RESULT] FALSIFICATION!
 56529/100000: episode: 3309, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 11.227, mean reward: 3.742 [0.580, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.615, 9.911], loss: 0.026941, mae: 0.166533, mean_q: 0.924787
[RESULT] FALSIFICATION!
 56530/100000: episode: 3310, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.403, 10.020], loss: 1.224291, mae: 0.409942, mean_q: 0.772719
[RESULT] FALSIFICATION!
 56531/100000: episode: 3311, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.274, 9.864], loss: 0.028822, mae: 0.156082, mean_q: 0.957099
[RESULT] FALSIFICATION!
 56537/100000: episode: 3312, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 13.370, mean reward: 2.228 [0.645, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.071, 10.020], loss: 0.233878, mae: 0.233321, mean_q: 1.093714
[RESULT] FALSIFICATION!
 56540/100000: episode: 3313, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 11.377, mean reward: 3.792 [0.681, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.388, 9.952], loss: 0.343869, mae: 0.221887, mean_q: 0.986997
[RESULT] FALSIFICATION!
 56541/100000: episode: 3314, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.212, 9.864], loss: 0.973348, mae: 0.352244, mean_q: 0.905777
[RESULT] FALSIFICATION!
 56542/100000: episode: 3315, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.275, 9.864], loss: 1.247245, mae: 0.469195, mean_q: 1.045449
[RESULT] FALSIFICATION!
 56543/100000: episode: 3316, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.381, 10.020], loss: 0.030236, mae: 0.185546, mean_q: 1.060455
[RESULT] FALSIFICATION!
 56544/100000: episode: 3317, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.183, 9.813], loss: 0.905041, mae: 0.369768, mean_q: 1.061188
[RESULT] FALSIFICATION!
 56545/100000: episode: 3318, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.257, 9.813], loss: 0.015169, mae: 0.132729, mean_q: 0.960000
[RESULT] FALSIFICATION!
 56546/100000: episode: 3319, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.323, 9.911], loss: 1.864886, mae: 0.694012, mean_q: 1.354744
[RESULT] FALSIFICATION!
 56547/100000: episode: 3320, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.308, 9.952], loss: 2.804396, mae: 0.954579, mean_q: 1.273177
[RESULT] FALSIFICATION!
 56548/100000: episode: 3321, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.386, 10.020], loss: 0.078773, mae: 0.312847, mean_q: 1.295341
 56559/100000: episode: 3322, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 6.710, mean reward: 0.610 [0.551, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.496, 10.100], loss: 0.534300, mae: 0.390567, mean_q: 1.155151
[RESULT] FALSIFICATION!
 56560/100000: episode: 3323, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.307, 9.952], loss: 0.023838, mae: 0.188027, mean_q: 0.904544
[RESULT] FALSIFICATION!
 56565/100000: episode: 3324, duration: 0.043s, episode steps: 5, steps per second: 117, episode reward: 12.766, mean reward: 2.553 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.387, 10.020], loss: 0.412867, mae: 0.272698, mean_q: 0.978628
[RESULT] FALSIFICATION!
 56567/100000: episode: 3325, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.698, mean reward: 5.349 [0.698, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.377, 9.911], loss: 0.488744, mae: 0.296805, mean_q: 1.165173
 56576/100000: episode: 3326, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 5.615, mean reward: 0.624 [0.549, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.480, 10.100], loss: 0.469759, mae: 0.317308, mean_q: 1.155101
[RESULT] FALSIFICATION!
 56577/100000: episode: 3327, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.248, 9.813], loss: 1.828120, mae: 0.653071, mean_q: 1.085355
[RESULT] FALSIFICATION!
 56578/100000: episode: 3328, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.439, 10.020], loss: 1.781336, mae: 0.643347, mean_q: 1.206034
 56587/100000: episode: 3329, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 5.956, mean reward: 0.662 [0.638, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.775, 10.100], loss: 0.590124, mae: 0.447206, mean_q: 1.206353
[RESULT] FALSIFICATION!
 56591/100000: episode: 3330, duration: 0.032s, episode steps: 4, steps per second: 124, episode reward: 11.950, mean reward: 2.988 [0.633, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.306, 9.952], loss: 0.999031, mae: 0.417884, mean_q: 1.022088
[RESULT] FALSIFICATION!
 56592/100000: episode: 3331, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.326, 9.864], loss: 0.041011, mae: 0.209835, mean_q: 1.119697
[RESULT] FALSIFICATION!
 56593/100000: episode: 3332, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.346, 9.864], loss: 0.964673, mae: 0.442371, mean_q: 1.057219
[RESULT] FALSIFICATION!
 56594/100000: episode: 3333, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.350, 9.952], loss: 0.058133, mae: 0.272940, mean_q: 1.186634
[RESULT] FALSIFICATION!
 56595/100000: episode: 3334, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.390, 9.952], loss: 0.811418, mae: 0.465411, mean_q: 1.243667
 56607/100000: episode: 3335, duration: 0.086s, episode steps: 12, steps per second: 139, episode reward: 6.633, mean reward: 0.553 [0.461, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.449, 10.100], loss: 0.589589, mae: 0.411412, mean_q: 1.115456
[RESULT] FALSIFICATION!
 56609/100000: episode: 3336, duration: 0.028s, episode steps: 2, steps per second: 71, episode reward: 10.669, mean reward: 5.335 [0.669, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.321, 9.864], loss: 1.318794, mae: 0.588132, mean_q: 1.181834
 56619/100000: episode: 3337, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 6.118, mean reward: 0.612 [0.563, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.446, 10.100], loss: 0.941691, mae: 0.461096, mean_q: 1.180841
 56631/100000: episode: 3338, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 7.483, mean reward: 0.624 [0.539, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.418, 10.100], loss: 0.443357, mae: 0.367620, mean_q: 1.204113
[RESULT] FALSIFICATION!
 56632/100000: episode: 3339, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.388, 10.020], loss: 0.926015, mae: 0.453880, mean_q: 0.871543
[RESULT] FALSIFICATION!
 56633/100000: episode: 3340, duration: 0.019s, episode steps: 1, steps per second: 54, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.326, 9.864], loss: 0.033281, mae: 0.215307, mean_q: 0.993868
[RESULT] FALSIFICATION!
 56635/100000: episode: 3341, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 10.673, mean reward: 5.337 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.227, 9.911], loss: 1.606830, mae: 0.573095, mean_q: 0.855779
[RESULT] FALSIFICATION!
 56636/100000: episode: 3342, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.465, 10.020], loss: 0.063289, mae: 0.245895, mean_q: 1.104288
[RESULT] FALSIFICATION!
 56637/100000: episode: 3343, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.221, 9.813], loss: 0.060144, mae: 0.259135, mean_q: 1.133656
[RESULT] FALSIFICATION!
 56640/100000: episode: 3344, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 11.296, mean reward: 3.765 [0.638, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.314, 10.020], loss: 0.921201, mae: 0.529841, mean_q: 1.334577
 56651/100000: episode: 3345, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 6.459, mean reward: 0.587 [0.509, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.500, 10.100], loss: 0.725681, mae: 0.436250, mean_q: 1.176066
 56660/100000: episode: 3346, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 4.768, mean reward: 0.530 [0.434, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.348, 10.100], loss: 0.634986, mae: 0.415921, mean_q: 1.168511
 56672/100000: episode: 3347, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 6.860, mean reward: 0.572 [0.480, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.599, 10.100], loss: 0.756658, mae: 0.417905, mean_q: 1.175435
[RESULT] FALSIFICATION!
 56673/100000: episode: 3348, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.262, 9.864], loss: 0.809983, mae: 0.521424, mean_q: 1.285391
[RESULT] FALSIFICATION!
 56674/100000: episode: 3349, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.262, 9.864], loss: 1.632408, mae: 0.723300, mean_q: 1.391302
[RESULT] FALSIFICATION!
 56675/100000: episode: 3350, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.292, 9.911], loss: 0.119052, mae: 0.327227, mean_q: 1.253416
[RESULT] FALSIFICATION!
 56676/100000: episode: 3351, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.445, 10.020], loss: 0.182959, mae: 0.464026, mean_q: 1.495380
[RESULT] FALSIFICATION!
 56677/100000: episode: 3352, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.200, 9.952], loss: 0.783845, mae: 0.458340, mean_q: 1.219948
 56686/100000: episode: 3353, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 5.500, mean reward: 0.611 [0.543, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.483, 10.100], loss: 0.414952, mae: 0.285650, mean_q: 0.972062
 56698/100000: episode: 3354, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 7.124, mean reward: 0.594 [0.508, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.430, 10.100], loss: 0.851599, mae: 0.505876, mean_q: 1.283515
 56710/100000: episode: 3355, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 6.934, mean reward: 0.578 [0.533, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.550, 10.100], loss: 0.460304, mae: 0.377435, mean_q: 1.086437
[RESULT] FALSIFICATION!
 56711/100000: episode: 3356, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.234, 9.813], loss: 0.023367, mae: 0.163854, mean_q: 1.029028
 56723/100000: episode: 3357, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 7.127, mean reward: 0.594 [0.513, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.361, 10.100], loss: 0.824483, mae: 0.446681, mean_q: 1.164663
[RESULT] FALSIFICATION!
 56724/100000: episode: 3358, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.209, 9.813], loss: 0.773313, mae: 0.558980, mean_q: 1.425213
[RESULT] FALSIFICATION!
 56725/100000: episode: 3359, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.268, 9.813], loss: 0.129540, mae: 0.357687, mean_q: 1.345383
 56737/100000: episode: 3360, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 7.690, mean reward: 0.641 [0.570, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.411, 10.100], loss: 0.582279, mae: 0.350723, mean_q: 0.938036
[RESULT] FALSIFICATION!
 56739/100000: episode: 3361, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 10.677, mean reward: 5.338 [0.677, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.235, 9.911], loss: 0.460645, mae: 0.419014, mean_q: 1.213573
 56751/100000: episode: 3362, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 7.013, mean reward: 0.584 [0.538, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.813, 10.100], loss: 0.971517, mae: 0.568743, mean_q: 1.375267
[RESULT] FALSIFICATION!
 56755/100000: episode: 3363, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 12.061, mean reward: 3.015 [0.671, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.278, 9.952], loss: 0.658801, mae: 0.425109, mean_q: 1.233094
[RESULT] FALSIFICATION!
 56759/100000: episode: 3364, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: 12.025, mean reward: 3.006 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.445, 9.988], loss: 0.546078, mae: 0.368392, mean_q: 1.162451
[RESULT] FALSIFICATION!
 56760/100000: episode: 3365, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.238, 9.864], loss: 0.048417, mae: 0.215716, mean_q: 1.206448
[RESULT] FALSIFICATION!
 56765/100000: episode: 3366, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 12.748, mean reward: 2.550 [0.651, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.426, 10.067], loss: 0.219537, mae: 0.282680, mean_q: 1.113577
 56775/100000: episode: 3367, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 6.195, mean reward: 0.619 [0.559, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.634, 10.100], loss: 0.684054, mae: 0.398913, mean_q: 1.199143
[RESULT] FALSIFICATION!
 56776/100000: episode: 3368, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.286, 9.911], loss: 1.308617, mae: 0.517570, mean_q: 1.210006
 56788/100000: episode: 3369, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 6.550, mean reward: 0.546 [0.444, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.430, 10.100], loss: 0.662575, mae: 0.370510, mean_q: 1.141615
[RESULT] FALSIFICATION!
 56790/100000: episode: 3370, duration: 0.023s, episode steps: 2, steps per second: 88, episode reward: 10.693, mean reward: 5.347 [0.693, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.192, 9.911], loss: 1.388796, mae: 0.631952, mean_q: 1.463089
[RESULT] FALSIFICATION!
 56792/100000: episode: 3371, duration: 0.022s, episode steps: 2, steps per second: 91, episode reward: 10.673, mean reward: 5.337 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.284, 9.864], loss: 0.620818, mae: 0.511486, mean_q: 1.463771
[RESULT] FALSIFICATION!
 56795/100000: episode: 3372, duration: 0.031s, episode steps: 3, steps per second: 96, episode reward: 11.361, mean reward: 3.787 [0.672, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.243, 9.911], loss: 1.027866, mae: 0.571118, mean_q: 1.429249
 56807/100000: episode: 3373, duration: 0.081s, episode steps: 12, steps per second: 147, episode reward: 7.100, mean reward: 0.592 [0.506, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.484, 10.100], loss: 1.040733, mae: 0.567123, mean_q: 1.323432
[RESULT] FALSIFICATION!
 56810/100000: episode: 3374, duration: 0.027s, episode steps: 3, steps per second: 110, episode reward: 11.347, mean reward: 3.782 [0.656, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.318, 9.988], loss: 0.198874, mae: 0.433163, mean_q: 1.463674
[RESULT] FALSIFICATION!
 56811/100000: episode: 3375, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.288, 9.864], loss: 0.157176, mae: 0.389019, mean_q: 1.477665
[RESULT] FALSIFICATION!
 56812/100000: episode: 3376, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.343, 9.952], loss: 1.815302, mae: 0.692088, mean_q: 1.264261
[Info] New level: 3.5368285179138184 | Considering 12/88 traces
 56824/100000: episode: 3377, duration: 4.166s, episode steps: 12, steps per second: 3, episode reward: 6.256, mean reward: 0.521 [0.403, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.397, 10.100], loss: 0.510846, mae: 0.382273, mean_q: 1.189705
[RESULT] FALSIFICATION!
 56825/100000: episode: 3378, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.331, 9.952], loss: 0.098673, mae: 0.316085, mean_q: 1.118951
[RESULT] FALSIFICATION!
 56826/100000: episode: 3379, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.318, 9.911], loss: 0.129980, mae: 0.329822, mean_q: 1.302792
[RESULT] FALSIFICATION!
 56827/100000: episode: 3380, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.393, 9.952], loss: 0.812150, mae: 0.439227, mean_q: 0.955132
[RESULT] FALSIFICATION!
 56828/100000: episode: 3381, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.332, 9.952], loss: 1.235843, mae: 0.480975, mean_q: 1.015695
 56837/100000: episode: 3382, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 5.815, mean reward: 0.646 [0.560, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.661, 10.100], loss: 0.464331, mae: 0.408795, mean_q: 1.278738
[RESULT] FALSIFICATION!
 56840/100000: episode: 3383, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 11.347, mean reward: 3.782 [0.653, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.433, 10.020], loss: 0.885316, mae: 0.499297, mean_q: 1.310721
 56850/100000: episode: 3384, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 5.997, mean reward: 0.600 [0.536, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.529, 10.100], loss: 0.527450, mae: 0.319666, mean_q: 1.161451
[RESULT] FALSIFICATION!
 56851/100000: episode: 3385, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.316, 9.952], loss: 1.992002, mae: 0.776154, mean_q: 1.448152
[RESULT] FALSIFICATION!
 56857/100000: episode: 3386, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 13.403, mean reward: 2.234 [0.661, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.519, 10.067], loss: 1.020096, mae: 0.514936, mean_q: 1.286506
[RESULT] FALSIFICATION!
 56858/100000: episode: 3387, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.365, 9.911], loss: 1.313945, mae: 0.665852, mean_q: 1.404001
[RESULT] FALSIFICATION!
 56866/100000: episode: 3388, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 14.406, mean reward: 1.801 [0.574, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.480, 10.093], loss: 0.655643, mae: 0.543861, mean_q: 1.443082
[RESULT] FALSIFICATION!
 56868/100000: episode: 3389, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 10.678, mean reward: 5.339 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.380, 9.988], loss: 0.451146, mae: 0.345649, mean_q: 1.079336
 56876/100000: episode: 3390, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 4.455, mean reward: 0.557 [0.481, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.475, 10.100], loss: 0.962182, mae: 0.569785, mean_q: 1.280586
[RESULT] FALSIFICATION!
 56877/100000: episode: 3391, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.338, 9.911], loss: 1.427912, mae: 0.698521, mean_q: 1.409615
 56887/100000: episode: 3392, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 5.303, mean reward: 0.530 [0.489, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.559, 10.100], loss: 0.264932, mae: 0.345985, mean_q: 1.111504
[RESULT] FALSIFICATION!
 56888/100000: episode: 3393, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.330, 9.911], loss: 1.026642, mae: 0.477102, mean_q: 1.073170
[RESULT] FALSIFICATION!
 56895/100000: episode: 3394, duration: 0.059s, episode steps: 7, steps per second: 119, episode reward: 13.760, mean reward: 1.966 [0.546, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.860, 10.082], loss: 0.690277, mae: 0.494743, mean_q: 1.386635
[RESULT] FALSIFICATION!
 56896/100000: episode: 3395, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.345, 9.952], loss: 2.603352, mae: 0.975624, mean_q: 1.575617
[RESULT] FALSIFICATION!
 56897/100000: episode: 3396, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.451, 9.952], loss: 2.550524, mae: 0.920074, mean_q: 1.353622
[RESULT] FALSIFICATION!
 56898/100000: episode: 3397, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.393, 9.952], loss: 0.154716, mae: 0.352479, mean_q: 1.417069
[RESULT] FALSIFICATION!
 56899/100000: episode: 3398, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.308, 9.911], loss: 2.436261, mae: 0.908607, mean_q: 1.412192
 56907/100000: episode: 3399, duration: 0.061s, episode steps: 8, steps per second: 130, episode reward: 4.549, mean reward: 0.569 [0.534, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.509, 10.100], loss: 0.710151, mae: 0.468457, mean_q: 1.316193
[RESULT] FALSIFICATION!
 56908/100000: episode: 3400, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.377, 9.911], loss: 1.391449, mae: 0.709833, mean_q: 1.294825
[RESULT] FALSIFICATION!
 56909/100000: episode: 3401, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.342, 9.952], loss: 0.114417, mae: 0.308031, mean_q: 1.200663
[RESULT] FALSIFICATION!
 56910/100000: episode: 3402, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.364, 9.911], loss: 0.928465, mae: 0.467736, mean_q: 1.080968
[RESULT] FALSIFICATION!
 56911/100000: episode: 3403, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.320, 9.911], loss: 1.153782, mae: 0.502713, mean_q: 1.099806
 56919/100000: episode: 3404, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 4.225, mean reward: 0.528 [0.487, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.666, 10.100], loss: 0.583934, mae: 0.420497, mean_q: 1.218673
[RESULT] FALSIFICATION!
 56920/100000: episode: 3405, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.330, 9.952], loss: 1.526676, mae: 0.615117, mean_q: 1.255925
 56929/100000: episode: 3406, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 5.879, mean reward: 0.653 [0.600, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.554, 10.100], loss: 0.956710, mae: 0.562766, mean_q: 1.398359
[RESULT] FALSIFICATION!
 56930/100000: episode: 3407, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.338, 9.952], loss: 0.704584, mae: 0.514725, mean_q: 1.313352
[RESULT] FALSIFICATION!
 56931/100000: episode: 3408, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.390, 9.952], loss: 0.754753, mae: 0.491807, mean_q: 1.229590
[RESULT] FALSIFICATION!
 56933/100000: episode: 3409, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 10.698, mean reward: 5.349 [0.698, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.415, 9.988], loss: 0.417461, mae: 0.361936, mean_q: 1.120415
[RESULT] FALSIFICATION!
 56934/100000: episode: 3410, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.365, 9.952], loss: 1.426461, mae: 0.691326, mean_q: 1.506754
[RESULT] FALSIFICATION!
 56935/100000: episode: 3411, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.411, 9.952], loss: 0.785552, mae: 0.520773, mean_q: 1.352608
[RESULT] FALSIFICATION!
 56936/100000: episode: 3412, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.332, 9.911], loss: 0.097266, mae: 0.309608, mean_q: 1.339184
[RESULT] FALSIFICATION!
 56937/100000: episode: 3413, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.292, 9.911], loss: 1.374087, mae: 0.666689, mean_q: 1.364989
[RESULT] FALSIFICATION!
 56939/100000: episode: 3414, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.639, mean reward: 5.319 [0.639, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.255, 9.952], loss: 1.332037, mae: 0.677898, mean_q: 1.399295
 56948/100000: episode: 3415, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 4.973, mean reward: 0.553 [0.354, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.451, 10.100], loss: 1.061624, mae: 0.749760, mean_q: 1.605765
[RESULT] FALSIFICATION!
 56949/100000: episode: 3416, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.359, 9.952], loss: 0.715496, mae: 0.541539, mean_q: 1.365731
[RESULT] FALSIFICATION!
 56950/100000: episode: 3417, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.366, 9.911], loss: 0.697519, mae: 0.528650, mean_q: 1.200898
[RESULT] FALSIFICATION!
 56951/100000: episode: 3418, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.337, 9.952], loss: 0.653100, mae: 0.405743, mean_q: 0.958376
[RESULT] FALSIFICATION!
 56952/100000: episode: 3419, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.380, 9.952], loss: 0.773176, mae: 0.509554, mean_q: 1.140706
 56960/100000: episode: 3420, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.512, mean reward: 0.564 [0.476, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.512, 10.100], loss: 0.826442, mae: 0.539733, mean_q: 1.338959
[RESULT] FALSIFICATION!
 56961/100000: episode: 3421, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.344, 9.952], loss: 1.687789, mae: 0.796761, mean_q: 1.524848
[RESULT] FALSIFICATION!
 56962/100000: episode: 3422, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.391, 9.952], loss: 0.696911, mae: 0.524504, mean_q: 1.496366
[RESULT] FALSIFICATION!
 56963/100000: episode: 3423, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.341, 9.952], loss: 0.059341, mae: 0.241293, mean_q: 1.192790
[RESULT] FALSIFICATION!
 56964/100000: episode: 3424, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.352, 9.911], loss: 1.620565, mae: 0.884104, mean_q: 1.728436
[RESULT] FALSIFICATION!
 56965/100000: episode: 3425, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.383, 9.952], loss: 1.287951, mae: 0.674222, mean_q: 1.489854
[RESULT] FALSIFICATION!
 56966/100000: episode: 3426, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.344, 9.952], loss: 2.536231, mae: 0.884713, mean_q: 1.291484
 56976/100000: episode: 3427, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 5.438, mean reward: 0.544 [0.428, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.019, 10.100], loss: 1.029992, mae: 0.625108, mean_q: 1.455346
[RESULT] FALSIFICATION!
 56980/100000: episode: 3428, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 12.031, mean reward: 3.008 [0.654, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.363, 10.046], loss: 1.019507, mae: 0.569734, mean_q: 1.324416
[RESULT] FALSIFICATION!
 56981/100000: episode: 3429, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.394, 9.952], loss: 2.049358, mae: 0.854730, mean_q: 1.129647
[RESULT] FALSIFICATION!
 56982/100000: episode: 3430, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.341, 9.952], loss: 1.279095, mae: 0.635853, mean_q: 1.341031
[RESULT] FALSIFICATION!
 56983/100000: episode: 3431, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.296, 9.952], loss: 2.702684, mae: 1.113044, mean_q: 1.620690
[RESULT] FALSIFICATION!
 56984/100000: episode: 3432, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.338, 9.952], loss: 0.731160, mae: 0.494125, mean_q: 1.230468
 56992/100000: episode: 3433, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.324, mean reward: 0.540 [0.504, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.448, 10.100], loss: 1.134388, mae: 0.729672, mean_q: 1.596822
[RESULT] FALSIFICATION!
 56993/100000: episode: 3434, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.423, 9.952], loss: 1.656498, mae: 0.821402, mean_q: 1.493822
[RESULT] FALSIFICATION!
 56994/100000: episode: 3435, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.350, 9.952], loss: 1.221985, mae: 0.551468, mean_q: 1.096655
[RESULT] FALSIFICATION!
 56996/100000: episode: 3436, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.686, mean reward: 5.343 [0.686, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.413, 9.988], loss: 0.367745, mae: 0.429231, mean_q: 1.181744
[RESULT] FALSIFICATION!
 56997/100000: episode: 3437, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.328, 9.911], loss: 0.116752, mae: 0.345272, mean_q: 1.207218
[RESULT] FALSIFICATION!
 56998/100000: episode: 3438, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.323, 9.911], loss: 0.100227, mae: 0.296205, mean_q: 1.178196
[RESULT] FALSIFICATION!
 56999/100000: episode: 3439, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.377, 9.911], loss: 1.609795, mae: 0.641581, mean_q: 1.157909
 57007/100000: episode: 3440, duration: 0.066s, episode steps: 8, steps per second: 122, episode reward: 5.086, mean reward: 0.636 [0.563, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.495, 10.100], loss: 0.584245, mae: 0.524742, mean_q: 1.448125
[RESULT] FALSIFICATION!
 57008/100000: episode: 3441, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.382, 9.952], loss: 0.125065, mae: 0.324762, mean_q: 1.045758
[RESULT] FALSIFICATION!
 57012/100000: episode: 3442, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 12.041, mean reward: 3.010 [0.677, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.684, 10.046], loss: 0.549970, mae: 0.418484, mean_q: 1.077526
[RESULT] FALSIFICATION!
 57013/100000: episode: 3443, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.375, 9.952], loss: 1.846668, mae: 0.657872, mean_q: 1.068615
 57023/100000: episode: 3444, duration: 0.090s, episode steps: 10, steps per second: 111, episode reward: 6.376, mean reward: 0.638 [0.610, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.569, 10.100], loss: 0.904803, mae: 0.615225, mean_q: 1.499196
[RESULT] FALSIFICATION!
 57025/100000: episode: 3445, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 10.687, mean reward: 5.343 [0.687, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.393, 9.988], loss: 0.501184, mae: 0.438254, mean_q: 1.272566
[RESULT] FALSIFICATION!
 57026/100000: episode: 3446, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.315, 9.911], loss: 0.156614, mae: 0.361386, mean_q: 1.266669
[RESULT] FALSIFICATION!
 57027/100000: episode: 3447, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.361, 9.952], loss: 1.705538, mae: 0.766692, mean_q: 1.324980
[RESULT] FALSIFICATION!
 57029/100000: episode: 3448, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 10.666, mean reward: 5.333 [0.666, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.393, 9.988], loss: 0.993986, mae: 0.532015, mean_q: 1.173790
[RESULT] FALSIFICATION!
 57031/100000: episode: 3449, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 10.691, mean reward: 5.345 [0.691, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.365, 9.988], loss: 0.666065, mae: 0.544749, mean_q: 1.423816
[RESULT] FALSIFICATION!
 57032/100000: episode: 3450, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.343, 9.952], loss: 2.134111, mae: 1.063168, mean_q: 1.759675
 57041/100000: episode: 3451, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 5.509, mean reward: 0.612 [0.502, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.495, 10.100], loss: 1.254893, mae: 0.790590, mean_q: 1.605019
[RESULT] FALSIFICATION!
 57042/100000: episode: 3452, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.385, 9.952], loss: 0.604663, mae: 0.502550, mean_q: 1.403794
[RESULT] FALSIFICATION!
 57043/100000: episode: 3453, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.283, 9.911], loss: 0.535509, mae: 0.642798, mean_q: 1.765076
[RESULT] FALSIFICATION!
 57044/100000: episode: 3454, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.388, 9.911], loss: 1.002793, mae: 0.536470, mean_q: 1.313069
[RESULT] FALSIFICATION!
 57045/100000: episode: 3455, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.308, 9.952], loss: 0.679743, mae: 0.533635, mean_q: 1.242094
[RESULT] FALSIFICATION!
 57046/100000: episode: 3456, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.360, 9.988], loss: 0.126620, mae: 0.299370, mean_q: 1.332246
 57056/100000: episode: 3457, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 5.966, mean reward: 0.597 [0.563, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.560, 10.100], loss: 1.581555, mae: 0.764993, mean_q: 1.409065
[RESULT] FALSIFICATION!
 57057/100000: episode: 3458, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.399, 9.952], loss: 1.247808, mae: 1.058164, mean_q: 2.228317
[RESULT] FALSIFICATION!
 57058/100000: episode: 3459, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.401, 9.952], loss: 2.307093, mae: 1.064453, mean_q: 1.826514
 57066/100000: episode: 3460, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 4.687, mean reward: 0.586 [0.472, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.432, 10.100], loss: 0.941366, mae: 0.556856, mean_q: 1.318774
[RESULT] FALSIFICATION!
 57067/100000: episode: 3461, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.349, 9.952], loss: 0.292345, mae: 0.480877, mean_q: 1.244148
[RESULT] FALSIFICATION!
 57070/100000: episode: 3462, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 11.345, mean reward: 3.782 [0.645, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.453, 9.988], loss: 0.743986, mae: 0.537489, mean_q: 1.528289
[RESULT] FALSIFICATION!
 57072/100000: episode: 3463, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.676, mean reward: 5.338 [0.676, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.388, 9.988], loss: 0.493315, mae: 0.468514, mean_q: 1.505587
 57081/100000: episode: 3464, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 5.448, mean reward: 0.605 [0.534, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.595, 10.100], loss: 0.817826, mae: 0.537322, mean_q: 1.401811
[RESULT] FALSIFICATION!
[Info] New level: 4.835165977478027 | Considering 18/82 traces
 57082/100000: episode: 3465, duration: 3.749s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.362, 9.911], loss: 1.555628, mae: 0.782307, mean_q: 1.357491
[RESULT] FALSIFICATION!
 57084/100000: episode: 3466, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.652, mean reward: 5.326 [0.652, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.454, 10.020], loss: 1.133960, mae: 0.635884, mean_q: 1.414426
[RESULT] FALSIFICATION!
 57085/100000: episode: 3467, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.332, 9.911], loss: 0.221075, mae: 0.417394, mean_q: 1.537944
 57093/100000: episode: 3468, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 4.533, mean reward: 0.567 [0.492, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.422, 10.100], loss: 0.532029, mae: 0.430029, mean_q: 1.215524
 57101/100000: episode: 3469, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.557, mean reward: 0.570 [0.510, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.740, 10.100], loss: 0.574046, mae: 0.452443, mean_q: 1.403552
 57111/100000: episode: 3470, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 6.109, mean reward: 0.611 [0.568, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.446, 10.100], loss: 0.889740, mae: 0.525144, mean_q: 1.364781
[RESULT] FALSIFICATION!
 57113/100000: episode: 3471, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 10.629, mean reward: 5.314 [0.629, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-1.363, 9.952], loss: 2.004035, mae: 0.804731, mean_q: 1.446418
[RESULT] FALSIFICATION!
 57117/100000: episode: 3472, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 11.961, mean reward: 2.990 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.539, 10.067], loss: 1.279556, mae: 0.735434, mean_q: 1.699538
 57127/100000: episode: 3473, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 5.952, mean reward: 0.595 [0.535, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.506, 10.100], loss: 0.936493, mae: 0.612118, mean_q: 1.501264
[RESULT] FALSIFICATION!
 57129/100000: episode: 3474, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 10.700, mean reward: 5.350 [0.700, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.330, 9.952], loss: 2.111454, mae: 0.920704, mean_q: 1.595825
 57137/100000: episode: 3475, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 4.898, mean reward: 0.612 [0.563, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.874, 10.100], loss: 1.122263, mae: 0.760053, mean_q: 1.669470
[RESULT] FALSIFICATION!
 57142/100000: episode: 3476, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 12.553, mean reward: 2.511 [0.597, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.431, 10.082], loss: 0.976054, mae: 0.629248, mean_q: 1.264481
[RESULT] FALSIFICATION!
 57143/100000: episode: 3477, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.375, 9.988], loss: 1.415623, mae: 0.818034, mean_q: 1.679934
[RESULT] FALSIFICATION!
 57145/100000: episode: 3478, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 10.657, mean reward: 5.329 [0.657, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.392, 10.020], loss: 2.388694, mae: 1.036811, mean_q: 1.607721
 57155/100000: episode: 3479, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 5.945, mean reward: 0.594 [0.551, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.505, 10.100], loss: 1.012171, mae: 0.724925, mean_q: 1.705970
[RESULT] FALSIFICATION!
 57156/100000: episode: 3480, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.267, 9.911], loss: 0.083467, mae: 0.246665, mean_q: 1.175643
 57164/100000: episode: 3481, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 4.657, mean reward: 0.582 [0.478, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.381, 10.100], loss: 1.166131, mae: 0.670324, mean_q: 1.514744
[RESULT] FALSIFICATION!
 57165/100000: episode: 3482, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.300, 9.911], loss: 0.784264, mae: 0.560852, mean_q: 1.351387
[RESULT] FALSIFICATION!
 57166/100000: episode: 3483, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.316, 9.911], loss: 1.229321, mae: 0.694209, mean_q: 1.608660
[RESULT] FALSIFICATION!
 57167/100000: episode: 3484, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.311, 9.988], loss: 0.254177, mae: 0.376032, mean_q: 1.331236
[RESULT] FALSIFICATION!
 57168/100000: episode: 3485, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.332, 9.911], loss: 0.091325, mae: 0.288617, mean_q: 0.906279
 57178/100000: episode: 3486, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 6.378, mean reward: 0.638 [0.596, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.995, 10.100], loss: 0.877275, mae: 0.542584, mean_q: 1.405733
 57188/100000: episode: 3487, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 6.434, mean reward: 0.643 [0.579, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.067, 10.100], loss: 1.456114, mae: 0.884201, mean_q: 1.809070
 57196/100000: episode: 3488, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.657, mean reward: 0.582 [0.557, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.572, 10.100], loss: 1.635969, mae: 0.882317, mean_q: 1.737829
[RESULT] FALSIFICATION!
 57197/100000: episode: 3489, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.361, 9.988], loss: 1.452311, mae: 0.704184, mean_q: 1.438213
[RESULT] FALSIFICATION!
 57198/100000: episode: 3490, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.332, 9.911], loss: 1.027855, mae: 0.674662, mean_q: 1.539866
 57208/100000: episode: 3491, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 6.341, mean reward: 0.634 [0.600, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.512, 10.100], loss: 1.248699, mae: 0.755232, mean_q: 1.707272
[RESULT] FALSIFICATION!
 57209/100000: episode: 3492, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.408, 9.988], loss: 2.430231, mae: 0.994514, mean_q: 1.571891
 57217/100000: episode: 3493, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 4.204, mean reward: 0.526 [0.439, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.479, 10.100], loss: 0.740093, mae: 0.530072, mean_q: 1.306585
[RESULT] FALSIFICATION!
 57218/100000: episode: 3494, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.324, 9.988], loss: 0.130545, mae: 0.309729, mean_q: 1.014004
[RESULT] FALSIFICATION!
 57219/100000: episode: 3495, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.317, 9.988], loss: 0.597320, mae: 0.515103, mean_q: 1.196508
[RESULT] FALSIFICATION!
 57222/100000: episode: 3496, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.243, mean reward: 3.748 [0.615, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.866, 10.046], loss: 1.535503, mae: 0.645489, mean_q: 1.171587
 57230/100000: episode: 3497, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 4.517, mean reward: 0.565 [0.528, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.522, 10.100], loss: 1.359540, mae: 0.852221, mean_q: 1.797242
[RESULT] FALSIFICATION!
 57231/100000: episode: 3498, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.314, 9.988], loss: 0.493911, mae: 0.600331, mean_q: 1.705067
 57241/100000: episode: 3499, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 5.388, mean reward: 0.539 [0.455, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.366, 10.100], loss: 1.037436, mae: 0.642300, mean_q: 1.384766
 57251/100000: episode: 3500, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 5.853, mean reward: 0.585 [0.510, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.464, 10.100], loss: 0.997980, mae: 0.667952, mean_q: 1.619893
[RESULT] FALSIFICATION!
 57252/100000: episode: 3501, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.302, 9.911], loss: 0.324746, mae: 0.530799, mean_q: 1.710293
[RESULT] FALSIFICATION!
 57253/100000: episode: 3502, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.466, 9.988], loss: 1.542257, mae: 0.764587, mean_q: 1.794602
 57261/100000: episode: 3503, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 4.820, mean reward: 0.602 [0.506, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.669, 10.100], loss: 0.837089, mae: 0.562782, mean_q: 1.389914
[RESULT] FALSIFICATION!
 57262/100000: episode: 3504, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.289, 9.911], loss: 0.610830, mae: 0.600288, mean_q: 1.697429
 57270/100000: episode: 3505, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.672, mean reward: 0.584 [0.475, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.942, 10.100], loss: 0.778967, mae: 0.656497, mean_q: 1.624266
 57280/100000: episode: 3506, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 6.099, mean reward: 0.610 [0.523, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.813, 10.100], loss: 1.193365, mae: 0.648150, mean_q: 1.380588
[RESULT] FALSIFICATION!
 57281/100000: episode: 3507, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.324, 9.911], loss: 0.905154, mae: 0.579946, mean_q: 1.428353
[RESULT] FALSIFICATION!
 57282/100000: episode: 3508, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.331, 9.988], loss: 0.319214, mae: 0.515516, mean_q: 1.767422
[RESULT] FALSIFICATION!
 57283/100000: episode: 3509, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.302, 9.988], loss: 1.787434, mae: 0.906900, mean_q: 1.693503
[RESULT] FALSIFICATION!
 57284/100000: episode: 3510, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.323, 9.988], loss: 1.296218, mae: 0.544410, mean_q: 1.143590
[RESULT] FALSIFICATION!
 57285/100000: episode: 3511, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.447, 9.988], loss: 0.788802, mae: 0.422088, mean_q: 1.281901
 57293/100000: episode: 3512, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 4.557, mean reward: 0.570 [0.433, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.353, 10.100], loss: 1.255233, mae: 0.707418, mean_q: 1.593681
 57301/100000: episode: 3513, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 4.995, mean reward: 0.624 [0.585, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.763, 10.100], loss: 0.725678, mae: 0.548883, mean_q: 1.496608
[RESULT] FALSIFICATION!
 57302/100000: episode: 3514, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.323, 9.988], loss: 0.881335, mae: 0.481793, mean_q: 1.088957
[RESULT] FALSIFICATION!
 57303/100000: episode: 3515, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.273, 9.988], loss: 0.163023, mae: 0.317498, mean_q: 1.300811
[RESULT] FALSIFICATION!
 57304/100000: episode: 3516, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.319, 9.911], loss: 2.126908, mae: 0.917158, mean_q: 1.727882
 57312/100000: episode: 3517, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 4.640, mean reward: 0.580 [0.500, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.478, 10.100], loss: 1.255025, mae: 0.812387, mean_q: 1.807062
[RESULT] FALSIFICATION!
 57313/100000: episode: 3518, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.433, 9.988], loss: 0.201698, mae: 0.370204, mean_q: 1.446227
 57321/100000: episode: 3519, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 4.708, mean reward: 0.588 [0.542, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.586, 10.100], loss: 1.128526, mae: 0.656124, mean_q: 1.195190
 57329/100000: episode: 3520, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 5.042, mean reward: 0.630 [0.573, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.818, 10.100], loss: 0.765654, mae: 0.629787, mean_q: 1.614315
 57337/100000: episode: 3521, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 4.048, mean reward: 0.506 [0.335, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.543, 10.100], loss: 1.049705, mae: 0.633685, mean_q: 1.509947
[RESULT] FALSIFICATION!
 57338/100000: episode: 3522, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.299, 9.911], loss: 0.930468, mae: 0.573734, mean_q: 1.465762
 57348/100000: episode: 3523, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 6.315, mean reward: 0.631 [0.582, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.586, 10.100], loss: 1.032019, mae: 0.650190, mean_q: 1.536321
[RESULT] FALSIFICATION!
 57349/100000: episode: 3524, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.269, 9.988], loss: 1.617401, mae: 0.923136, mean_q: 1.926628
 57357/100000: episode: 3525, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 4.437, mean reward: 0.555 [0.471, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.437, 10.100], loss: 1.089555, mae: 0.762681, mean_q: 1.794641
[RESULT] FALSIFICATION!
 57359/100000: episode: 3526, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 10.674, mean reward: 5.337 [0.674, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.412, 10.020], loss: 1.081398, mae: 0.630215, mean_q: 1.488640
 57369/100000: episode: 3527, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 5.717, mean reward: 0.572 [0.503, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.509, 10.100], loss: 1.231206, mae: 0.652457, mean_q: 1.448055
[RESULT] FALSIFICATION!
 57371/100000: episode: 3528, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 10.679, mean reward: 5.339 [0.679, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.345, 9.952], loss: 2.180021, mae: 1.223090, mean_q: 2.110178
[RESULT] FALSIFICATION!
 57372/100000: episode: 3529, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.434, 9.988], loss: 1.298275, mae: 0.826315, mean_q: 1.853422
 57380/100000: episode: 3530, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 4.578, mean reward: 0.572 [0.539, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.536, 10.100], loss: 1.120100, mae: 0.700419, mean_q: 1.502193
[RESULT] FALSIFICATION!
 57381/100000: episode: 3531, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.343, 9.988], loss: 0.992198, mae: 0.524884, mean_q: 1.294869
[RESULT] FALSIFICATION!
 57382/100000: episode: 3532, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.331, 9.911], loss: 1.812673, mae: 0.736014, mean_q: 1.424075
 57392/100000: episode: 3533, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 5.191, mean reward: 0.519 [0.401, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.390, 10.100], loss: 1.378311, mae: 0.902795, mean_q: 1.881989
[RESULT] FALSIFICATION!
 57394/100000: episode: 3534, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 10.664, mean reward: 5.332 [0.664, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.392, 9.952], loss: 2.835356, mae: 1.207222, mean_q: 1.911720
[RESULT] FALSIFICATION!
 57395/100000: episode: 3535, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.301, 9.911], loss: 0.535942, mae: 0.453339, mean_q: 1.174121
 57405/100000: episode: 3536, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 6.309, mean reward: 0.631 [0.560, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.487, 10.100], loss: 0.829897, mae: 0.616693, mean_q: 1.600730
 57415/100000: episode: 3537, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 6.086, mean reward: 0.609 [0.509, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.422, 10.100], loss: 1.338326, mae: 0.774149, mean_q: 1.674233
 57423/100000: episode: 3538, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 4.871, mean reward: 0.609 [0.585, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.528, 10.100], loss: 1.143447, mae: 0.792091, mean_q: 1.758206
 57433/100000: episode: 3539, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 5.839, mean reward: 0.584 [0.507, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.457, 10.100], loss: 1.185649, mae: 0.677025, mean_q: 1.390435
[RESULT] FALSIFICATION!
 57440/100000: episode: 3540, duration: 0.053s, episode steps: 7, steps per second: 133, episode reward: 13.959, mean reward: 1.994 [0.648, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.579, 10.082], loss: 1.187153, mae: 0.828976, mean_q: 1.882130
[RESULT] FALSIFICATION!
 57441/100000: episode: 3541, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.341, 9.988], loss: 1.171503, mae: 0.744642, mean_q: 1.628489
[RESULT] FALSIFICATION!
 57442/100000: episode: 3542, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.283, 9.911], loss: 0.763640, mae: 0.454402, mean_q: 1.246868
 57450/100000: episode: 3543, duration: 0.060s, episode steps: 8, steps per second: 133, episode reward: 4.735, mean reward: 0.592 [0.552, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.630, 10.100], loss: 0.941550, mae: 0.563206, mean_q: 1.362765
[RESULT] FALSIFICATION!
 57451/100000: episode: 3544, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.296, 9.911], loss: 1.295150, mae: 0.530107, mean_q: 1.146979
 57459/100000: episode: 3545, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 4.228, mean reward: 0.528 [0.481, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.496, 10.100], loss: 1.008556, mae: 0.725121, mean_q: 1.681173
[RESULT] FALSIFICATION!
 57460/100000: episode: 3546, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.319, 9.911], loss: 0.250974, mae: 0.375323, mean_q: 1.564864
[RESULT] FALSIFICATION!
[Info] New level: 5.142922878265381 | Considering 11/89 traces
 57461/100000: episode: 3547, duration: 4.139s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.334, 9.911], loss: 5.537214, mae: 1.902401, mean_q: 2.113365
[RESULT] FALSIFICATION!
 57462/100000: episode: 3548, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.358, 9.988], loss: 0.607520, mae: 0.517026, mean_q: 1.562275
[RESULT] FALSIFICATION!
 57463/100000: episode: 3549, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.374, 9.988], loss: 1.923924, mae: 0.877403, mean_q: 1.522961
[RESULT] FALSIFICATION!
 57464/100000: episode: 3550, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.374, 9.988], loss: 1.319310, mae: 0.906790, mean_q: 1.898844
[RESULT] FALSIFICATION!
 57465/100000: episode: 3551, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.318, 9.988], loss: 1.652952, mae: 0.863952, mean_q: 1.671890
[RESULT] FALSIFICATION!
 57471/100000: episode: 3552, duration: 0.050s, episode steps: 6, steps per second: 120, episode reward: 12.971, mean reward: 2.162 [0.563, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.605, 10.093], loss: 1.567004, mae: 0.849027, mean_q: 1.720116
[RESULT] FALSIFICATION!
 57472/100000: episode: 3553, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.442, 9.988], loss: 0.993302, mae: 0.773554, mean_q: 1.880161
[RESULT] FALSIFICATION!
 57473/100000: episode: 3554, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.388, 9.988], loss: 0.661593, mae: 0.560351, mean_q: 1.617514
[RESULT] FALSIFICATION!
 57474/100000: episode: 3555, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.430, 9.988], loss: 2.060264, mae: 0.835236, mean_q: 1.542797
 57482/100000: episode: 3556, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 5.360, mean reward: 0.670 [0.621, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.684, 10.100], loss: 1.388439, mae: 0.749228, mean_q: 1.635450
 57490/100000: episode: 3557, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.570, mean reward: 0.571 [0.528, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.640, 10.100], loss: 1.555288, mae: 0.852759, mean_q: 1.771942
[RESULT] FALSIFICATION!
 57491/100000: episode: 3558, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.421, 9.988], loss: 2.243124, mae: 1.025008, mean_q: 1.923920
 57499/100000: episode: 3559, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.775, mean reward: 0.597 [0.477, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.569, 10.100], loss: 1.116521, mae: 0.665207, mean_q: 1.611412
 57507/100000: episode: 3560, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 4.960, mean reward: 0.620 [0.508, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.585, 10.100], loss: 1.713503, mae: 0.830028, mean_q: 1.633793
[RESULT] FALSIFICATION!
 57508/100000: episode: 3561, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.390, 9.988], loss: 2.635502, mae: 1.098026, mean_q: 1.650263
[RESULT] FALSIFICATION!
 57516/100000: episode: 3562, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 14.254, mean reward: 1.782 [0.539, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.722, 10.100], loss: 0.941285, mae: 0.658823, mean_q: 1.620902
[RESULT] FALSIFICATION!
 57518/100000: episode: 3563, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 10.641, mean reward: 5.321 [0.641, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.469, 10.020], loss: 1.171584, mae: 0.652653, mean_q: 1.448108
[RESULT] FALSIFICATION!
 57519/100000: episode: 3564, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.429, 9.988], loss: 0.766518, mae: 0.510406, mean_q: 1.406090
[RESULT] FALSIFICATION!
 57520/100000: episode: 3565, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.429, 9.988], loss: 1.085438, mae: 0.650329, mean_q: 1.547502
[RESULT] FALSIFICATION!
 57521/100000: episode: 3566, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.441, 9.988], loss: 1.259584, mae: 0.834697, mean_q: 1.864285
[RESULT] FALSIFICATION!
 57522/100000: episode: 3567, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.447, 9.988], loss: 1.118210, mae: 0.808186, mean_q: 1.767368
[RESULT] FALSIFICATION!
 57523/100000: episode: 3568, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.401, 9.988], loss: 2.275782, mae: 1.222978, mean_q: 2.203106
 57531/100000: episode: 3569, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 5.190, mean reward: 0.649 [0.615, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.588, 10.100], loss: 1.086280, mae: 0.713527, mean_q: 1.545683
[RESULT] FALSIFICATION!
 57534/100000: episode: 3570, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 11.264, mean reward: 3.755 [0.631, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.522, 10.046], loss: 0.638848, mae: 0.553417, mean_q: 1.272986
[RESULT] FALSIFICATION!
 57542/100000: episode: 3571, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 14.539, mean reward: 1.817 [0.603, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.547, 10.100], loss: 0.792923, mae: 0.562492, mean_q: 1.534886
[RESULT] FALSIFICATION!
 57543/100000: episode: 3572, duration: 0.020s, episode steps: 1, steps per second: 50, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.453, 9.988], loss: 0.601542, mae: 0.504219, mean_q: 1.270313
 57551/100000: episode: 3573, duration: 0.067s, episode steps: 8, steps per second: 119, episode reward: 5.263, mean reward: 0.658 [0.619, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.891, 10.100], loss: 0.938815, mae: 0.606031, mean_q: 1.454645
 57559/100000: episode: 3574, duration: 0.063s, episode steps: 8, steps per second: 126, episode reward: 4.784, mean reward: 0.598 [0.546, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.690, 10.100], loss: 1.045915, mae: 0.772772, mean_q: 1.840540
[RESULT] FALSIFICATION!
 57560/100000: episode: 3575, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.379, 9.988], loss: 2.957864, mae: 1.245514, mean_q: 1.844717
[RESULT] FALSIFICATION!
 57561/100000: episode: 3576, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.388, 9.988], loss: 2.498607, mae: 1.243352, mean_q: 2.169932
[RESULT] FALSIFICATION!
 57562/100000: episode: 3577, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.367, 9.988], loss: 0.235284, mae: 0.343341, mean_q: 1.245359
[RESULT] FALSIFICATION!
 57563/100000: episode: 3578, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.462, 9.988], loss: 1.609039, mae: 0.814204, mean_q: 1.692279
 57571/100000: episode: 3579, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 4.690, mean reward: 0.586 [0.484, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.531, 10.100], loss: 0.709633, mae: 0.540247, mean_q: 1.427894
[RESULT] FALSIFICATION!
 57572/100000: episode: 3580, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.326, 9.988], loss: 0.273253, mae: 0.449119, mean_q: 1.418025
 57580/100000: episode: 3581, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 4.559, mean reward: 0.570 [0.526, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.659, 10.100], loss: 1.323592, mae: 0.827791, mean_q: 1.828396
 57588/100000: episode: 3582, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.862, mean reward: 0.608 [0.569, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.618, 10.100], loss: 1.226794, mae: 0.738281, mean_q: 1.655249
[RESULT] FALSIFICATION!
 57589/100000: episode: 3583, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.317, 9.988], loss: 0.994846, mae: 0.537622, mean_q: 1.392145
[RESULT] FALSIFICATION!
 57590/100000: episode: 3584, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.338, 9.988], loss: 0.822194, mae: 0.538113, mean_q: 1.508959
[RESULT] FALSIFICATION!
 57597/100000: episode: 3585, duration: 0.051s, episode steps: 7, steps per second: 136, episode reward: 13.906, mean reward: 1.987 [0.600, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.716, 10.098], loss: 1.250953, mae: 0.671140, mean_q: 1.579920
[RESULT] FALSIFICATION!
 57599/100000: episode: 3586, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.696, mean reward: 5.348 [0.696, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.472, 10.020], loss: 2.374299, mae: 1.065034, mean_q: 1.698780
[RESULT] FALSIFICATION!
 57600/100000: episode: 3587, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.400, 9.988], loss: 0.185506, mae: 0.382972, mean_q: 1.415285
[RESULT] FALSIFICATION!
 57602/100000: episode: 3588, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.683, mean reward: 5.342 [0.683, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.457, 10.020], loss: 0.637539, mae: 0.541518, mean_q: 1.628077
 57610/100000: episode: 3589, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 5.151, mean reward: 0.644 [0.594, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.678, 10.100], loss: 1.587815, mae: 0.790850, mean_q: 1.589641
[RESULT] FALSIFICATION!
 57611/100000: episode: 3590, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.398, 9.988], loss: 0.328427, mae: 0.520609, mean_q: 1.745980
[RESULT] FALSIFICATION!
 57612/100000: episode: 3591, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.418, 9.988], loss: 2.216872, mae: 1.111734, mean_q: 1.973618
[RESULT] FALSIFICATION!
 57613/100000: episode: 3592, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.428, 9.988], loss: 0.312986, mae: 0.517469, mean_q: 1.533845
 57621/100000: episode: 3593, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 4.944, mean reward: 0.618 [0.556, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.536, 10.100], loss: 1.492567, mae: 0.894976, mean_q: 1.895432
 57629/100000: episode: 3594, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.442, mean reward: 0.555 [0.486, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.532, 10.100], loss: 1.212976, mae: 0.663647, mean_q: 1.427077
[RESULT] FALSIFICATION!
 57630/100000: episode: 3595, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.368, 9.988], loss: 1.711907, mae: 0.901827, mean_q: 1.779416
[RESULT] FALSIFICATION!
 57631/100000: episode: 3596, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.351, 9.988], loss: 1.052502, mae: 0.848537, mean_q: 2.044882
[RESULT] FALSIFICATION!
 57632/100000: episode: 3597, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.343, 9.988], loss: 1.717806, mae: 1.034315, mean_q: 2.064358
[RESULT] FALSIFICATION!
 57633/100000: episode: 3598, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.335, 9.988], loss: 2.013762, mae: 1.084106, mean_q: 2.106667
[RESULT] FALSIFICATION!
 57634/100000: episode: 3599, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.464, 9.988], loss: 0.644376, mae: 0.509773, mean_q: 1.550778
[RESULT] FALSIFICATION!
 57635/100000: episode: 3600, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.369, 9.988], loss: 0.266088, mae: 0.381932, mean_q: 1.331534
 57643/100000: episode: 3601, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 4.950, mean reward: 0.619 [0.516, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.638, 10.100], loss: 1.359326, mae: 0.798770, mean_q: 1.654204
[RESULT] FALSIFICATION!
 57644/100000: episode: 3602, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.360, 9.988], loss: 0.286685, mae: 0.483870, mean_q: 1.723633
[RESULT] FALSIFICATION!
 57650/100000: episode: 3603, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 13.081, mean reward: 2.180 [0.592, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.492, 10.093], loss: 1.146728, mae: 0.744195, mean_q: 1.722218
[RESULT] FALSIFICATION!
 57651/100000: episode: 3604, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.467, 9.988], loss: 0.973779, mae: 0.786928, mean_q: 1.973940
[RESULT] FALSIFICATION!
 57652/100000: episode: 3605, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.470, 9.988], loss: 3.203502, mae: 1.265966, mean_q: 1.801512
[RESULT] FALSIFICATION!
 57653/100000: episode: 3606, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.336, 9.988], loss: 1.647159, mae: 0.947456, mean_q: 1.900440
[RESULT] FALSIFICATION!
 57654/100000: episode: 3607, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.367, 9.988], loss: 2.102832, mae: 1.031296, mean_q: 1.952426
[RESULT] FALSIFICATION!
 57655/100000: episode: 3608, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.427, 9.988], loss: 2.318975, mae: 1.247947, mean_q: 2.210955
[RESULT] FALSIFICATION!
 57656/100000: episode: 3609, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.506, 9.988], loss: 1.313378, mae: 0.522413, mean_q: 1.192125
 57664/100000: episode: 3610, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 5.145, mean reward: 0.643 [0.600, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.546, 10.100], loss: 0.790065, mae: 0.591618, mean_q: 1.590998
[RESULT] FALSIFICATION!
 57671/100000: episode: 3611, duration: 0.069s, episode steps: 7, steps per second: 101, episode reward: 13.912, mean reward: 1.987 [0.630, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.575, 10.098], loss: 1.960644, mae: 1.033951, mean_q: 1.979608
[RESULT] FALSIFICATION!
 57672/100000: episode: 3612, duration: 0.016s, episode steps: 1, steps per second: 61, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.450, 9.988], loss: 1.931770, mae: 0.963364, mean_q: 1.648666
[RESULT] FALSIFICATION!
 57677/100000: episode: 3613, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 12.637, mean reward: 2.527 [0.624, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.865, 10.082], loss: 1.526505, mae: 0.958603, mean_q: 2.000707
[RESULT] FALSIFICATION!
 57678/100000: episode: 3614, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.402, 9.988], loss: 2.034946, mae: 1.209480, mean_q: 2.194181
 57686/100000: episode: 3615, duration: 0.061s, episode steps: 8, steps per second: 131, episode reward: 4.895, mean reward: 0.612 [0.548, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.602, 10.100], loss: 1.443820, mae: 0.889190, mean_q: 1.844151
 57694/100000: episode: 3616, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 4.767, mean reward: 0.596 [0.570, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.554, 10.100], loss: 0.988917, mae: 0.655794, mean_q: 1.336279
 57702/100000: episode: 3617, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 4.694, mean reward: 0.587 [0.556, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.716, 10.100], loss: 1.208953, mae: 0.687862, mean_q: 1.511905
[RESULT] FALSIFICATION!
 57704/100000: episode: 3618, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 10.685, mean reward: 5.342 [0.685, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.427, 10.020], loss: 3.035396, mae: 1.442591, mean_q: 2.197757
[RESULT] FALSIFICATION!
 57705/100000: episode: 3619, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.317, 9.988], loss: 1.340047, mae: 1.126964, mean_q: 2.410064
[RESULT] FALSIFICATION!
 57706/100000: episode: 3620, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.375, 9.988], loss: 1.671190, mae: 1.187686, mean_q: 2.363225
 57714/100000: episode: 3621, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 5.118, mean reward: 0.640 [0.603, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.566, 10.100], loss: 1.484281, mae: 0.952489, mean_q: 1.988426
[RESULT] FALSIFICATION!
 57715/100000: episode: 3622, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.477, 9.988], loss: 0.669089, mae: 0.577302, mean_q: 1.373166
[RESULT] FALSIFICATION!
 57717/100000: episode: 3623, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 10.673, mean reward: 5.336 [0.673, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.456, 10.020], loss: 1.077048, mae: 0.797129, mean_q: 1.643432
[RESULT] FALSIFICATION!
 57718/100000: episode: 3624, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.451, 9.988], loss: 0.525441, mae: 0.439305, mean_q: 1.097491
 57726/100000: episode: 3625, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.955, mean reward: 0.619 [0.485, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.543, 10.100], loss: 1.073536, mae: 0.695412, mean_q: 1.657942
[RESULT] FALSIFICATION!
 57727/100000: episode: 3626, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.354, 9.988], loss: 1.366482, mae: 0.987555, mean_q: 2.177807
 57735/100000: episode: 3627, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 4.762, mean reward: 0.595 [0.557, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.542, 10.100], loss: 1.271569, mae: 0.746219, mean_q: 1.649103
[RESULT] FALSIFICATION!
 57740/100000: episode: 3628, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 12.671, mean reward: 2.534 [0.649, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.525, 10.082], loss: 1.846776, mae: 1.014473, mean_q: 1.942270
[RESULT] FALSIFICATION!
 57741/100000: episode: 3629, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.395, 9.988], loss: 0.683711, mae: 0.804380, mean_q: 2.009912
[RESULT] FALSIFICATION!
 57742/100000: episode: 3630, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.360, 9.988], loss: 1.425543, mae: 0.999683, mean_q: 2.163383
[RESULT] FALSIFICATION!
 57743/100000: episode: 3631, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.407, 9.988], loss: 1.270989, mae: 0.744996, mean_q: 1.770559
[RESULT] FALSIFICATION!
 57744/100000: episode: 3632, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.346, 9.988], loss: 0.610294, mae: 0.629361, mean_q: 1.672866
[RESULT] FALSIFICATION!
 57745/100000: episode: 3633, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.378, 9.988], loss: 1.239273, mae: 0.822931, mean_q: 1.869507
 57753/100000: episode: 3634, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 5.101, mean reward: 0.638 [0.594, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.298, 10.100], loss: 1.167924, mae: 0.789015, mean_q: 1.651647
 57761/100000: episode: 3635, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 5.279, mean reward: 0.660 [0.565, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.876, 10.100], loss: 1.880964, mae: 0.950455, mean_q: 1.744438
[Info] New level: 5.526605606079102 | Considering 71/29 traces
 57769/100000: episode: 3636, duration: 4.530s, episode steps: 8, steps per second: 2, episode reward: 5.085, mean reward: 0.636 [0.610, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.007, 10.100], loss: 1.515497, mae: 1.016050, mean_q: 2.138361
 57777/100000: episode: 3637, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 4.436, mean reward: 0.555 [0.455, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.583, 10.100], loss: 1.611282, mae: 0.883253, mean_q: 1.813033
[RESULT] FALSIFICATION!
 57778/100000: episode: 3638, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.414, 9.988], loss: 0.731120, mae: 0.722948, mean_q: 2.015635
[RESULT] FALSIFICATION!
 57779/100000: episode: 3639, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.445, 9.988], loss: 2.921732, mae: 1.393533, mean_q: 2.308314
[RESULT] FALSIFICATION!
 57782/100000: episode: 3640, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 11.288, mean reward: 3.763 [0.637, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.522, 10.046], loss: 1.706565, mae: 0.872380, mean_q: 1.715789
[RESULT] FALSIFICATION!
 57783/100000: episode: 3641, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.464, 9.988], loss: 1.428277, mae: 0.853484, mean_q: 1.738102
 57791/100000: episode: 3642, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 4.728, mean reward: 0.591 [0.568, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.596, 10.100], loss: 1.706198, mae: 1.015649, mean_q: 2.138870
[RESULT] FALSIFICATION!
 57792/100000: episode: 3643, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.410, 9.988], loss: 1.251956, mae: 0.725050, mean_q: 1.526165
[RESULT] FALSIFICATION!
 57793/100000: episode: 3644, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.443, 9.988], loss: 1.093078, mae: 0.731588, mean_q: 1.557556
 57801/100000: episode: 3645, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.917, mean reward: 0.615 [0.561, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.938, 10.100], loss: 1.814620, mae: 1.022075, mean_q: 2.022124
[RESULT] FALSIFICATION!
 57802/100000: episode: 3646, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.473, 9.988], loss: 0.759883, mae: 0.723688, mean_q: 1.898837
[RESULT] FALSIFICATION!
 57810/100000: episode: 3647, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 14.556, mean reward: 1.820 [0.581, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.691, 10.100], loss: 1.219826, mae: 0.661467, mean_q: 1.565870
[RESULT] FALSIFICATION!
 57811/100000: episode: 3648, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.436, 9.988], loss: 4.209548, mae: 1.503475, mean_q: 1.804784
[RESULT] FALSIFICATION!
 57812/100000: episode: 3649, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.437, 9.988], loss: 2.123581, mae: 1.220283, mean_q: 2.342594
[RESULT] FALSIFICATION!
 57813/100000: episode: 3650, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.355, 9.988], loss: 1.159639, mae: 1.006853, mean_q: 2.307173
[RESULT] FALSIFICATION!
 57814/100000: episode: 3651, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.419, 9.988], loss: 0.837773, mae: 0.970427, mean_q: 2.264036
 57822/100000: episode: 3652, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 5.088, mean reward: 0.636 [0.603, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.662, 10.100], loss: 2.079239, mae: 1.059987, mean_q: 1.915401
[RESULT] FALSIFICATION!
 57823/100000: episode: 3653, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.380, 9.988], loss: 0.195820, mae: 0.385222, mean_q: 1.157735
 57831/100000: episode: 3654, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 4.847, mean reward: 0.606 [0.527, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.577, 10.100], loss: 1.496520, mae: 0.989618, mean_q: 2.098539
[RESULT] FALSIFICATION!
 57832/100000: episode: 3655, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.434, 9.988], loss: 0.270005, mae: 0.396729, mean_q: 1.497533
[RESULT] FALSIFICATION!
 57833/100000: episode: 3656, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.441, 9.988], loss: 2.592275, mae: 1.201473, mean_q: 1.939690
[RESULT] FALSIFICATION!
 57835/100000: episode: 3657, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 10.659, mean reward: 5.330 [0.659, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.501, 10.020], loss: 1.570085, mae: 0.905708, mean_q: 1.812025
[RESULT] FALSIFICATION!
 57836/100000: episode: 3658, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.366, 9.988], loss: 0.698659, mae: 0.484097, mean_q: 1.334920
[RESULT] FALSIFICATION!
 57837/100000: episode: 3659, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.413, 9.988], loss: 1.993964, mae: 0.989305, mean_q: 1.924634
 57845/100000: episode: 3660, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.766, mean reward: 0.596 [0.540, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.952, 10.100], loss: 1.498488, mae: 0.884393, mean_q: 1.897681
[RESULT] FALSIFICATION!
 57846/100000: episode: 3661, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.464, 9.988], loss: 2.536778, mae: 1.162014, mean_q: 2.069845
[RESULT] FALSIFICATION!
 57847/100000: episode: 3662, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.423, 9.988], loss: 0.972956, mae: 0.675419, mean_q: 1.609511
 57855/100000: episode: 3663, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 5.158, mean reward: 0.645 [0.587, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.418, 10.100], loss: 1.948107, mae: 1.077773, mean_q: 2.135455
 57863/100000: episode: 3664, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 5.085, mean reward: 0.636 [0.596, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.631, 10.100], loss: 1.260734, mae: 0.769079, mean_q: 1.803767
[RESULT] FALSIFICATION!
[Info] New level: 6.44075345993042 | Considering 100/0 traces
 57864/100000: episode: 3665, duration: 3.977s, episode steps: 1, steps per second: 0, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.414, 9.988], loss: 3.251342, mae: 1.339810, mean_q: 2.082209
[Info] Not found new level, current best level reached = 6.44075345993042
 57872/100000: episode: 3666, duration: 3.986s, episode steps: 8, steps per second: 2, episode reward: 4.892, mean reward: 0.612 [0.553, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.718, 10.100], loss: 1.489993, mae: 0.957824, mean_q: 2.098849
 57972/100000: episode: 3667, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 16.424, mean reward: 0.164 [0.045, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.857, 10.135], loss: 1.601716, mae: 0.950381, mean_q: 1.984039
 58072/100000: episode: 3668, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 14.664, mean reward: 0.147 [0.016, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.631, 10.251], loss: 1.569751, mae: 0.912869, mean_q: 1.878784
 58172/100000: episode: 3669, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 11.668, mean reward: 0.117 [0.024, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.521, 10.098], loss: 1.496057, mae: 0.873900, mean_q: 1.795117
 58272/100000: episode: 3670, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 13.276, mean reward: 0.133 [0.008, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.208, 10.114], loss: 1.575709, mae: 0.916068, mean_q: 1.877994
 58372/100000: episode: 3671, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 14.717, mean reward: 0.147 [0.008, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.292, 10.098], loss: 1.521972, mae: 0.895384, mean_q: 1.829610
 58472/100000: episode: 3672, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 16.524, mean reward: 0.165 [0.016, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.975, 10.098], loss: 1.610610, mae: 0.904744, mean_q: 1.738578
 58572/100000: episode: 3673, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 19.674, mean reward: 0.197 [0.030, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.158, 10.394], loss: 1.576045, mae: 0.909456, mean_q: 1.815826
 58672/100000: episode: 3674, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 14.565, mean reward: 0.146 [0.006, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.806, 10.098], loss: 1.639811, mae: 0.921637, mean_q: 1.781980
 58772/100000: episode: 3675, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 23.059, mean reward: 0.231 [0.042, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.110], loss: 1.496288, mae: 0.881639, mean_q: 1.747085
 58872/100000: episode: 3676, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 18.003, mean reward: 0.180 [0.038, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.554, 10.301], loss: 1.476033, mae: 0.878030, mean_q: 1.752826
 58972/100000: episode: 3677, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 24.049, mean reward: 0.240 [0.021, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.901, 10.388], loss: 1.511645, mae: 0.874357, mean_q: 1.698364
 59072/100000: episode: 3678, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 18.644, mean reward: 0.186 [0.020, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.000, 10.327], loss: 1.530887, mae: 0.882378, mean_q: 1.684078
 59172/100000: episode: 3679, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 22.285, mean reward: 0.223 [0.052, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.589, 10.423], loss: 1.510271, mae: 0.879101, mean_q: 1.709977
 59272/100000: episode: 3680, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 13.632, mean reward: 0.136 [0.023, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.477, 10.098], loss: 1.687133, mae: 0.968654, mean_q: 1.854629
 59372/100000: episode: 3681, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 16.092, mean reward: 0.161 [0.023, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.861, 10.098], loss: 1.461455, mae: 0.834444, mean_q: 1.628488
 59472/100000: episode: 3682, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 18.967, mean reward: 0.190 [0.024, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.119, 10.316], loss: 1.561418, mae: 0.882637, mean_q: 1.679056
 59572/100000: episode: 3683, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 14.574, mean reward: 0.146 [0.024, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.971, 10.235], loss: 1.467446, mae: 0.869162, mean_q: 1.699907
 59672/100000: episode: 3684, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 17.727, mean reward: 0.177 [0.039, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.098], loss: 1.454731, mae: 0.847608, mean_q: 1.610638
 59772/100000: episode: 3685, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 16.758, mean reward: 0.168 [0.014, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.078, 10.246], loss: 1.384217, mae: 0.805773, mean_q: 1.559716
 59872/100000: episode: 3686, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 17.558, mean reward: 0.176 [0.019, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.793, 10.098], loss: 1.480327, mae: 0.877732, mean_q: 1.641374
 59972/100000: episode: 3687, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 13.855, mean reward: 0.139 [0.009, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.543, 10.164], loss: 1.327877, mae: 0.784599, mean_q: 1.504512
 60072/100000: episode: 3688, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.434, mean reward: 0.154 [0.026, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.592, 10.109], loss: 1.347443, mae: 0.796974, mean_q: 1.477022
 60172/100000: episode: 3689, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 13.701, mean reward: 0.137 [0.018, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.098], loss: 1.446534, mae: 0.844545, mean_q: 1.561365
 60272/100000: episode: 3690, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 17.600, mean reward: 0.176 [0.006, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.054, 10.098], loss: 1.330037, mae: 0.767322, mean_q: 1.415230
 60372/100000: episode: 3691, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 16.489, mean reward: 0.165 [0.016, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.254, 10.098], loss: 1.291208, mae: 0.768499, mean_q: 1.421850
 60472/100000: episode: 3692, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.293, mean reward: 0.183 [0.019, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.979, 10.098], loss: 1.349890, mae: 0.774945, mean_q: 1.416690
 60572/100000: episode: 3693, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 22.747, mean reward: 0.227 [0.012, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.479, 10.098], loss: 1.171430, mae: 0.707478, mean_q: 1.352626
 60672/100000: episode: 3694, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.170, mean reward: 0.162 [0.013, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.207, 10.098], loss: 1.318798, mae: 0.770716, mean_q: 1.375721
 60772/100000: episode: 3695, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 18.909, mean reward: 0.189 [0.039, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.155, 10.327], loss: 1.262808, mae: 0.740395, mean_q: 1.374291
 60872/100000: episode: 3696, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 13.928, mean reward: 0.139 [0.011, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.120], loss: 1.193112, mae: 0.694017, mean_q: 1.277118
 60972/100000: episode: 3697, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 21.318, mean reward: 0.213 [0.067, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.325, 10.208], loss: 1.031870, mae: 0.634389, mean_q: 1.218946
 61072/100000: episode: 3698, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 16.210, mean reward: 0.162 [0.017, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.098], loss: 1.046719, mae: 0.642059, mean_q: 1.246393
 61172/100000: episode: 3699, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 13.993, mean reward: 0.140 [0.010, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.076, 10.098], loss: 1.010306, mae: 0.654194, mean_q: 1.162390
 61272/100000: episode: 3700, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 19.899, mean reward: 0.199 [0.006, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.478, 10.500], loss: 0.761926, mae: 0.520518, mean_q: 1.063670
 61372/100000: episode: 3701, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 14.381, mean reward: 0.144 [0.006, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.166], loss: 0.919335, mae: 0.580053, mean_q: 1.071042
 61472/100000: episode: 3702, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 14.837, mean reward: 0.148 [0.006, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.684, 10.120], loss: 0.782652, mae: 0.501981, mean_q: 1.004385
 61572/100000: episode: 3703, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 16.759, mean reward: 0.168 [0.003, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.666, 10.189], loss: 0.805638, mae: 0.506175, mean_q: 0.965705
 61672/100000: episode: 3704, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 14.975, mean reward: 0.150 [0.010, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.397, 10.454], loss: 0.739854, mae: 0.452478, mean_q: 0.865645
 61772/100000: episode: 3705, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.789, mean reward: 0.178 [0.027, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.278, 10.307], loss: 0.639469, mae: 0.418493, mean_q: 0.832274
 61872/100000: episode: 3706, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 12.840, mean reward: 0.128 [0.014, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.348, 10.218], loss: 0.563942, mae: 0.363549, mean_q: 0.762071
 61972/100000: episode: 3707, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 18.969, mean reward: 0.190 [0.047, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.847, 10.145], loss: 0.418565, mae: 0.308305, mean_q: 0.693431
 62072/100000: episode: 3708, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 11.798, mean reward: 0.118 [0.008, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.217, 10.098], loss: 0.440967, mae: 0.315393, mean_q: 0.696438
 62172/100000: episode: 3709, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 16.113, mean reward: 0.161 [0.011, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.736, 10.098], loss: 0.264345, mae: 0.229773, mean_q: 0.596735
 62272/100000: episode: 3710, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 14.412, mean reward: 0.144 [0.010, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.851, 10.229], loss: 0.350921, mae: 0.253947, mean_q: 0.602901
 62372/100000: episode: 3711, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 18.892, mean reward: 0.189 [0.025, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.860, 10.098], loss: 0.324266, mae: 0.240258, mean_q: 0.584623
 62472/100000: episode: 3712, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.380, mean reward: 0.184 [0.029, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.350, 10.098], loss: 0.179240, mae: 0.171168, mean_q: 0.475228
 62572/100000: episode: 3713, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 17.458, mean reward: 0.175 [0.038, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.704, 10.119], loss: 0.179005, mae: 0.173784, mean_q: 0.467956
 62672/100000: episode: 3714, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 14.990, mean reward: 0.150 [0.016, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.494, 10.275], loss: 0.094207, mae: 0.130660, mean_q: 0.413015
 62772/100000: episode: 3715, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 15.845, mean reward: 0.158 [0.027, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.572, 10.191], loss: 0.073796, mae: 0.113223, mean_q: 0.374956
 62872/100000: episode: 3716, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 14.395, mean reward: 0.144 [0.012, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.967, 10.318], loss: 0.009869, mae: 0.083559, mean_q: 0.336266
 62972/100000: episode: 3717, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 20.817, mean reward: 0.208 [0.026, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.270, 10.098], loss: 0.005416, mae: 0.079776, mean_q: 0.327909
 63072/100000: episode: 3718, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 17.004, mean reward: 0.170 [0.001, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.014, 10.176], loss: 0.005197, mae: 0.077869, mean_q: 0.335691
 63172/100000: episode: 3719, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 16.974, mean reward: 0.170 [0.015, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.912, 10.098], loss: 0.005800, mae: 0.080721, mean_q: 0.336206
 63272/100000: episode: 3720, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 13.440, mean reward: 0.134 [0.019, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.196], loss: 0.005861, mae: 0.082735, mean_q: 0.339717
 63372/100000: episode: 3721, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 14.700, mean reward: 0.147 [0.021, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.902, 10.218], loss: 0.005493, mae: 0.080303, mean_q: 0.338588
 63472/100000: episode: 3722, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 18.405, mean reward: 0.184 [0.022, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.546, 10.318], loss: 0.005488, mae: 0.079963, mean_q: 0.340285
 63572/100000: episode: 3723, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 12.471, mean reward: 0.125 [0.006, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.153, 10.211], loss: 0.005416, mae: 0.080384, mean_q: 0.333090
 63672/100000: episode: 3724, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 24.503, mean reward: 0.245 [0.045, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.661, 10.473], loss: 0.005199, mae: 0.079263, mean_q: 0.339326
 63772/100000: episode: 3725, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 15.661, mean reward: 0.157 [0.016, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.280, 10.191], loss: 0.004701, mae: 0.075993, mean_q: 0.336987
 63872/100000: episode: 3726, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 17.719, mean reward: 0.177 [0.008, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.108, 10.230], loss: 0.005329, mae: 0.079346, mean_q: 0.337513
 63972/100000: episode: 3727, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.772, mean reward: 0.168 [0.018, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.211, 10.098], loss: 0.005140, mae: 0.077899, mean_q: 0.333998
 64072/100000: episode: 3728, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.638, mean reward: 0.156 [0.022, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.337], loss: 0.004969, mae: 0.077337, mean_q: 0.330272
 64172/100000: episode: 3729, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.012, mean reward: 0.150 [0.019, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.134], loss: 0.004929, mae: 0.076633, mean_q: 0.330564
 64272/100000: episode: 3730, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.226, mean reward: 0.162 [0.015, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.132, 10.098], loss: 0.004943, mae: 0.076477, mean_q: 0.330090
 64372/100000: episode: 3731, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 14.430, mean reward: 0.144 [0.027, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.098], loss: 0.004816, mae: 0.075822, mean_q: 0.327918
 64472/100000: episode: 3732, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 18.101, mean reward: 0.181 [0.039, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.430], loss: 0.005241, mae: 0.079378, mean_q: 0.336786
 64572/100000: episode: 3733, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 19.706, mean reward: 0.197 [0.018, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.743, 10.098], loss: 0.004993, mae: 0.077561, mean_q: 0.330498
 64672/100000: episode: 3734, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 15.996, mean reward: 0.160 [0.017, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.974, 10.224], loss: 0.004819, mae: 0.075828, mean_q: 0.329560
 64772/100000: episode: 3735, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.459, mean reward: 0.155 [0.009, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.161], loss: 0.005133, mae: 0.079035, mean_q: 0.329432
 64872/100000: episode: 3736, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 18.423, mean reward: 0.184 [0.010, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.755, 10.098], loss: 0.005095, mae: 0.077728, mean_q: 0.329561
 64972/100000: episode: 3737, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.648, mean reward: 0.156 [0.016, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.711, 10.098], loss: 0.004589, mae: 0.075347, mean_q: 0.331921
 65072/100000: episode: 3738, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.796, mean reward: 0.168 [0.009, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.124, 10.173], loss: 0.004540, mae: 0.074451, mean_q: 0.332130
 65172/100000: episode: 3739, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 21.011, mean reward: 0.210 [0.039, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.016, 10.297], loss: 0.005220, mae: 0.078960, mean_q: 0.336742
 65272/100000: episode: 3740, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 13.309, mean reward: 0.133 [0.014, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.564, 10.147], loss: 0.005341, mae: 0.081002, mean_q: 0.336608
 65372/100000: episode: 3741, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 14.970, mean reward: 0.150 [0.019, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.163, 10.160], loss: 0.005058, mae: 0.078378, mean_q: 0.332439
 65472/100000: episode: 3742, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.512, mean reward: 0.165 [0.011, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.263, 10.285], loss: 0.004693, mae: 0.076226, mean_q: 0.330641
 65572/100000: episode: 3743, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 17.029, mean reward: 0.170 [0.005, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.373], loss: 0.004372, mae: 0.073505, mean_q: 0.327751
 65672/100000: episode: 3744, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 15.660, mean reward: 0.157 [0.013, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.592, 10.145], loss: 0.004484, mae: 0.075493, mean_q: 0.329336
 65772/100000: episode: 3745, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 16.862, mean reward: 0.169 [0.009, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.686, 10.149], loss: 0.004795, mae: 0.076639, mean_q: 0.329613
 65872/100000: episode: 3746, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.705, mean reward: 0.137 [0.021, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.417, 10.098], loss: 0.004524, mae: 0.074379, mean_q: 0.324314
 65972/100000: episode: 3747, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 17.689, mean reward: 0.177 [0.012, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.560, 10.253], loss: 0.004684, mae: 0.075654, mean_q: 0.325937
 66072/100000: episode: 3748, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.679, mean reward: 0.167 [0.020, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.052, 10.381], loss: 0.004497, mae: 0.074101, mean_q: 0.324746
 66172/100000: episode: 3749, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 14.447, mean reward: 0.144 [0.007, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.268], loss: 0.004262, mae: 0.072751, mean_q: 0.328365
 66272/100000: episode: 3750, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 15.662, mean reward: 0.157 [0.035, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.066, 10.098], loss: 0.004623, mae: 0.075648, mean_q: 0.328264
 66372/100000: episode: 3751, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 15.473, mean reward: 0.155 [0.021, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.307, 10.098], loss: 0.004040, mae: 0.070177, mean_q: 0.322684
 66472/100000: episode: 3752, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 20.595, mean reward: 0.206 [0.018, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.593, 10.243], loss: 0.004333, mae: 0.072897, mean_q: 0.325573
 66572/100000: episode: 3753, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 14.817, mean reward: 0.148 [0.014, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.536, 10.098], loss: 0.004601, mae: 0.075003, mean_q: 0.327089
 66672/100000: episode: 3754, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.356, mean reward: 0.154 [0.004, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.449, 10.155], loss: 0.004802, mae: 0.075944, mean_q: 0.327460
 66772/100000: episode: 3755, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 12.798, mean reward: 0.128 [0.015, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.242, 10.158], loss: 0.004476, mae: 0.073927, mean_q: 0.327892
 66872/100000: episode: 3756, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 15.662, mean reward: 0.157 [0.004, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.026, 10.150], loss: 0.004187, mae: 0.071613, mean_q: 0.324691
 66972/100000: episode: 3757, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 14.986, mean reward: 0.150 [0.028, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.189, 10.113], loss: 0.004383, mae: 0.073533, mean_q: 0.321585
 67072/100000: episode: 3758, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 15.910, mean reward: 0.159 [0.024, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.222], loss: 0.004267, mae: 0.072542, mean_q: 0.324134
 67172/100000: episode: 3759, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 15.585, mean reward: 0.156 [0.018, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.935, 10.098], loss: 0.004254, mae: 0.072916, mean_q: 0.324216
 67272/100000: episode: 3760, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 17.816, mean reward: 0.178 [0.041, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.242], loss: 0.004494, mae: 0.074653, mean_q: 0.324527
 67372/100000: episode: 3761, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 16.399, mean reward: 0.164 [0.008, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.533, 10.098], loss: 0.004674, mae: 0.075512, mean_q: 0.322361
 67472/100000: episode: 3762, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 19.518, mean reward: 0.195 [0.034, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.554, 10.229], loss: 0.004172, mae: 0.071868, mean_q: 0.324720
 67572/100000: episode: 3763, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.138, mean reward: 0.161 [0.034, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.377, 10.098], loss: 0.003987, mae: 0.070560, mean_q: 0.325053
 67672/100000: episode: 3764, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 21.615, mean reward: 0.216 [0.031, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.000, 10.098], loss: 0.004504, mae: 0.074544, mean_q: 0.327235
 67772/100000: episode: 3765, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 16.167, mean reward: 0.162 [0.016, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.622, 10.247], loss: 0.004297, mae: 0.073310, mean_q: 0.324216
[Info] New level: 0.6976120471954346 | Considering 10/90 traces
 67872/100000: episode: 3766, duration: 4.471s, episode steps: 100, steps per second: 22, episode reward: 15.884, mean reward: 0.159 [0.009, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.739, 10.098], loss: 0.003809, mae: 0.069480, mean_q: 0.323436
 67893/100000: episode: 3767, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 5.812, mean reward: 0.277 [0.143, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.279, 10.393], loss: 0.004155, mae: 0.072261, mean_q: 0.329327
 67923/100000: episode: 3768, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 6.540, mean reward: 0.218 [0.045, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.992, 10.100], loss: 0.004733, mae: 0.076631, mean_q: 0.332216
 67945/100000: episode: 3769, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 7.418, mean reward: 0.337 [0.240, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.396], loss: 0.004144, mae: 0.071199, mean_q: 0.331515
 67967/100000: episode: 3770, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 8.372, mean reward: 0.381 [0.275, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.089, 10.413], loss: 0.004300, mae: 0.073542, mean_q: 0.331515
 68052/100000: episode: 3771, duration: 0.437s, episode steps: 85, steps per second: 195, episode reward: 19.917, mean reward: 0.234 [0.029, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-1.198, 10.100], loss: 0.004211, mae: 0.072865, mean_q: 0.334413
 68073/100000: episode: 3772, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 6.589, mean reward: 0.314 [0.202, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.425, 10.331], loss: 0.004741, mae: 0.077129, mean_q: 0.334215
 68164/100000: episode: 3773, duration: 0.480s, episode steps: 91, steps per second: 190, episode reward: 13.840, mean reward: 0.152 [0.014, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.201, 10.198], loss: 0.004370, mae: 0.073053, mean_q: 0.338400
 68255/100000: episode: 3774, duration: 0.472s, episode steps: 91, steps per second: 193, episode reward: 17.405, mean reward: 0.191 [0.032, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-1.489, 10.266], loss: 0.003958, mae: 0.070576, mean_q: 0.335456
 68284/100000: episode: 3775, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 12.304, mean reward: 0.424 [0.283, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.928, 10.478], loss: 0.004148, mae: 0.072636, mean_q: 0.337468
 68358/100000: episode: 3776, duration: 0.389s, episode steps: 74, steps per second: 190, episode reward: 17.337, mean reward: 0.234 [0.043, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.705 [-0.274, 10.136], loss: 0.004124, mae: 0.071060, mean_q: 0.340618
 68449/100000: episode: 3777, duration: 0.468s, episode steps: 91, steps per second: 195, episode reward: 15.440, mean reward: 0.170 [0.023, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.376, 10.286], loss: 0.004021, mae: 0.070291, mean_q: 0.343523
 68523/100000: episode: 3778, duration: 0.388s, episode steps: 74, steps per second: 191, episode reward: 11.419, mean reward: 0.154 [0.021, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-1.234, 10.100], loss: 0.004581, mae: 0.076272, mean_q: 0.343460
 68614/100000: episode: 3779, duration: 0.451s, episode steps: 91, steps per second: 202, episode reward: 14.956, mean reward: 0.164 [0.015, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.647, 10.100], loss: 0.004182, mae: 0.071432, mean_q: 0.337606
 68699/100000: episode: 3780, duration: 0.468s, episode steps: 85, steps per second: 181, episode reward: 13.532, mean reward: 0.159 [0.028, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-0.630, 10.263], loss: 0.004131, mae: 0.072809, mean_q: 0.343733
 68728/100000: episode: 3781, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 9.065, mean reward: 0.313 [0.143, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.298], loss: 0.003861, mae: 0.069669, mean_q: 0.334956
 68819/100000: episode: 3782, duration: 0.495s, episode steps: 91, steps per second: 184, episode reward: 10.926, mean reward: 0.120 [0.012, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.353, 10.162], loss: 0.004122, mae: 0.071244, mean_q: 0.344220
 68838/100000: episode: 3783, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 8.459, mean reward: 0.445 [0.310, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.685, 10.100], loss: 0.003958, mae: 0.069891, mean_q: 0.337006
 68860/100000: episode: 3784, duration: 0.119s, episode steps: 22, steps per second: 186, episode reward: 7.147, mean reward: 0.325 [0.213, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.064, 10.381], loss: 0.003842, mae: 0.070040, mean_q: 0.343555
 68879/100000: episode: 3785, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 7.645, mean reward: 0.402 [0.273, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.478, 10.100], loss: 0.004289, mae: 0.072960, mean_q: 0.343784
 68908/100000: episode: 3786, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 10.753, mean reward: 0.371 [0.236, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.456], loss: 0.004347, mae: 0.072572, mean_q: 0.352625
 68930/100000: episode: 3787, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 8.865, mean reward: 0.403 [0.297, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.564, 10.496], loss: 0.003847, mae: 0.068106, mean_q: 0.346504
 68959/100000: episode: 3788, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 5.677, mean reward: 0.196 [0.083, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.927, 10.100], loss: 0.004217, mae: 0.071137, mean_q: 0.350034
 69050/100000: episode: 3789, duration: 0.476s, episode steps: 91, steps per second: 191, episode reward: 17.070, mean reward: 0.188 [0.011, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.264, 10.311], loss: 0.004267, mae: 0.071993, mean_q: 0.353749
 69071/100000: episode: 3790, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 7.817, mean reward: 0.372 [0.241, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.743, 10.584], loss: 0.003859, mae: 0.068306, mean_q: 0.335659
 69145/100000: episode: 3791, duration: 0.368s, episode steps: 74, steps per second: 201, episode reward: 14.219, mean reward: 0.192 [0.027, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.660, 10.313], loss: 0.004256, mae: 0.072125, mean_q: 0.350363
 69175/100000: episode: 3792, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 10.427, mean reward: 0.348 [0.268, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.349, 10.442], loss: 0.004440, mae: 0.072681, mean_q: 0.357417
 69254/100000: episode: 3793, duration: 0.400s, episode steps: 79, steps per second: 197, episode reward: 15.015, mean reward: 0.190 [0.068, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-1.145, 10.100], loss: 0.004345, mae: 0.073244, mean_q: 0.352207
 69273/100000: episode: 3794, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 7.276, mean reward: 0.383 [0.301, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.357, 10.100], loss: 0.004291, mae: 0.072888, mean_q: 0.358324
 69302/100000: episode: 3795, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 7.161, mean reward: 0.247 [0.089, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.293], loss: 0.004150, mae: 0.071518, mean_q: 0.360126
 69387/100000: episode: 3796, duration: 0.428s, episode steps: 85, steps per second: 199, episode reward: 14.860, mean reward: 0.175 [0.051, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.588 [-0.738, 10.197], loss: 0.004322, mae: 0.072986, mean_q: 0.357523
 69417/100000: episode: 3797, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 9.551, mean reward: 0.318 [0.200, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.297, 10.358], loss: 0.004319, mae: 0.074057, mean_q: 0.361858
 69436/100000: episode: 3798, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 7.840, mean reward: 0.413 [0.285, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.361, 10.100], loss: 0.003820, mae: 0.067188, mean_q: 0.358335
 69515/100000: episode: 3799, duration: 0.402s, episode steps: 79, steps per second: 197, episode reward: 22.404, mean reward: 0.284 [0.126, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.649 [-1.303, 10.100], loss: 0.004789, mae: 0.076132, mean_q: 0.353756
 69593/100000: episode: 3800, duration: 0.392s, episode steps: 78, steps per second: 199, episode reward: 17.016, mean reward: 0.218 [0.032, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-1.221, 10.203], loss: 0.003912, mae: 0.069761, mean_q: 0.357801
 69622/100000: episode: 3801, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 6.196, mean reward: 0.214 [0.058, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.078, 10.151], loss: 0.004704, mae: 0.076222, mean_q: 0.364626
 69713/100000: episode: 3802, duration: 0.486s, episode steps: 91, steps per second: 187, episode reward: 12.873, mean reward: 0.141 [0.015, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.898, 10.332], loss: 0.004290, mae: 0.072700, mean_q: 0.363535
 69791/100000: episode: 3803, duration: 0.412s, episode steps: 78, steps per second: 189, episode reward: 12.277, mean reward: 0.157 [0.031, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.659 [-0.813, 10.100], loss: 0.004508, mae: 0.074567, mean_q: 0.363318
 69820/100000: episode: 3804, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 9.229, mean reward: 0.318 [0.163, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.057, 10.291], loss: 0.003748, mae: 0.068202, mean_q: 0.361214
 69841/100000: episode: 3805, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 5.735, mean reward: 0.273 [0.169, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.042, 10.342], loss: 0.004050, mae: 0.070254, mean_q: 0.371637
 69915/100000: episode: 3806, duration: 0.378s, episode steps: 74, steps per second: 196, episode reward: 12.204, mean reward: 0.165 [0.027, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.242, 10.241], loss: 0.004427, mae: 0.074617, mean_q: 0.367249
 69993/100000: episode: 3807, duration: 0.398s, episode steps: 78, steps per second: 196, episode reward: 15.388, mean reward: 0.197 [0.052, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-0.594, 10.186], loss: 0.004053, mae: 0.070799, mean_q: 0.368545
 70012/100000: episode: 3808, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 8.537, mean reward: 0.449 [0.311, 0.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.855, 10.100], loss: 0.003820, mae: 0.068679, mean_q: 0.359705
 70031/100000: episode: 3809, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 9.368, mean reward: 0.493 [0.301, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.543, 10.100], loss: 0.003976, mae: 0.068987, mean_q: 0.379242
 70105/100000: episode: 3810, duration: 0.393s, episode steps: 74, steps per second: 188, episode reward: 13.788, mean reward: 0.186 [0.027, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.709 [-0.904, 10.100], loss: 0.004030, mae: 0.070132, mean_q: 0.370774
 70124/100000: episode: 3811, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 8.141, mean reward: 0.428 [0.340, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.674, 10.100], loss: 0.004185, mae: 0.071339, mean_q: 0.381760
 70153/100000: episode: 3812, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 10.766, mean reward: 0.371 [0.246, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.393, 10.530], loss: 0.004473, mae: 0.075284, mean_q: 0.385680
 70227/100000: episode: 3813, duration: 0.373s, episode steps: 74, steps per second: 198, episode reward: 9.758, mean reward: 0.132 [0.007, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.194, 10.225], loss: 0.003909, mae: 0.069087, mean_q: 0.374796
 70318/100000: episode: 3814, duration: 0.471s, episode steps: 91, steps per second: 193, episode reward: 14.023, mean reward: 0.154 [0.016, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.749, 10.150], loss: 0.004315, mae: 0.072040, mean_q: 0.377566
 70409/100000: episode: 3815, duration: 0.455s, episode steps: 91, steps per second: 200, episode reward: 21.892, mean reward: 0.241 [0.019, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.486, 10.520], loss: 0.004186, mae: 0.072333, mean_q: 0.374984
 70439/100000: episode: 3816, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 7.728, mean reward: 0.258 [0.146, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.242], loss: 0.004904, mae: 0.078295, mean_q: 0.384871
 70518/100000: episode: 3817, duration: 0.408s, episode steps: 79, steps per second: 194, episode reward: 12.781, mean reward: 0.162 [0.021, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.653 [-0.276, 10.190], loss: 0.003849, mae: 0.068219, mean_q: 0.378121
 70539/100000: episode: 3818, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 6.410, mean reward: 0.305 [0.202, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.400, 10.469], loss: 0.003806, mae: 0.068276, mean_q: 0.368190
 70569/100000: episode: 3819, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 11.789, mean reward: 0.393 [0.301, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.680, 10.509], loss: 0.004521, mae: 0.072487, mean_q: 0.377119
 70648/100000: episode: 3820, duration: 0.409s, episode steps: 79, steps per second: 193, episode reward: 11.694, mean reward: 0.148 [0.022, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.654 [-1.150, 10.147], loss: 0.003916, mae: 0.068822, mean_q: 0.387560
 70669/100000: episode: 3821, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 4.518, mean reward: 0.215 [0.102, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.066, 10.223], loss: 0.004508, mae: 0.072900, mean_q: 0.391542
 70688/100000: episode: 3822, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 8.221, mean reward: 0.433 [0.283, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.614, 10.100], loss: 0.003454, mae: 0.064762, mean_q: 0.375493
 70718/100000: episode: 3823, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 9.436, mean reward: 0.315 [0.137, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.388, 10.224], loss: 0.004048, mae: 0.071568, mean_q: 0.384162
 70797/100000: episode: 3824, duration: 0.442s, episode steps: 79, steps per second: 179, episode reward: 20.691, mean reward: 0.262 [0.105, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.655 [-1.030, 10.100], loss: 0.003900, mae: 0.069291, mean_q: 0.390196
 70818/100000: episode: 3825, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 4.203, mean reward: 0.200 [0.039, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.170], loss: 0.004209, mae: 0.071601, mean_q: 0.384905
 70892/100000: episode: 3826, duration: 0.364s, episode steps: 74, steps per second: 204, episode reward: 9.985, mean reward: 0.135 [0.024, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.706 [-0.499, 10.191], loss: 0.004207, mae: 0.072278, mean_q: 0.384998
 70922/100000: episode: 3827, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 11.644, mean reward: 0.388 [0.251, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.039, 10.378], loss: 0.004152, mae: 0.072244, mean_q: 0.389206
 70943/100000: episode: 3828, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 6.815, mean reward: 0.325 [0.237, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.646, 10.477], loss: 0.004081, mae: 0.070834, mean_q: 0.394895
 71034/100000: episode: 3829, duration: 0.465s, episode steps: 91, steps per second: 196, episode reward: 16.467, mean reward: 0.181 [0.015, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.583, 10.100], loss: 0.003817, mae: 0.068437, mean_q: 0.387314
 71064/100000: episode: 3830, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 5.716, mean reward: 0.191 [0.009, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.615, 10.100], loss: 0.003610, mae: 0.066089, mean_q: 0.381864
 71083/100000: episode: 3831, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 7.730, mean reward: 0.407 [0.290, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.050, 10.100], loss: 0.004173, mae: 0.071613, mean_q: 0.397918
 71113/100000: episode: 3832, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 6.503, mean reward: 0.217 [0.101, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.083, 10.332], loss: 0.004102, mae: 0.071766, mean_q: 0.393116
 71192/100000: episode: 3833, duration: 0.403s, episode steps: 79, steps per second: 196, episode reward: 12.043, mean reward: 0.152 [0.020, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.660 [-0.928, 10.216], loss: 0.003732, mae: 0.068410, mean_q: 0.390049
 71271/100000: episode: 3834, duration: 0.411s, episode steps: 79, steps per second: 192, episode reward: 11.646, mean reward: 0.147 [0.021, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.651 [-1.142, 10.100], loss: 0.003927, mae: 0.069344, mean_q: 0.401734
 71349/100000: episode: 3835, duration: 0.458s, episode steps: 78, steps per second: 170, episode reward: 18.295, mean reward: 0.235 [0.054, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-0.789, 10.402], loss: 0.004031, mae: 0.071164, mean_q: 0.397895
 71371/100000: episode: 3836, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 6.246, mean reward: 0.284 [0.204, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.441, 10.374], loss: 0.003986, mae: 0.069136, mean_q: 0.393821
 71445/100000: episode: 3837, duration: 0.378s, episode steps: 74, steps per second: 196, episode reward: 9.460, mean reward: 0.128 [0.030, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.699 [-0.859, 10.100], loss: 0.004115, mae: 0.071774, mean_q: 0.396264
 71467/100000: episode: 3838, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 8.729, mean reward: 0.397 [0.323, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.867, 10.490], loss: 0.003707, mae: 0.068364, mean_q: 0.406226
 71546/100000: episode: 3839, duration: 0.391s, episode steps: 79, steps per second: 202, episode reward: 15.418, mean reward: 0.195 [0.007, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.662 [-0.541, 10.100], loss: 0.004171, mae: 0.071240, mean_q: 0.400765
 71575/100000: episode: 3840, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 7.393, mean reward: 0.255 [0.151, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.292], loss: 0.004005, mae: 0.070589, mean_q: 0.393245
 71666/100000: episode: 3841, duration: 0.475s, episode steps: 91, steps per second: 191, episode reward: 16.184, mean reward: 0.178 [0.011, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-1.222, 10.100], loss: 0.004113, mae: 0.071529, mean_q: 0.403307
 71757/100000: episode: 3842, duration: 0.472s, episode steps: 91, steps per second: 193, episode reward: 15.890, mean reward: 0.175 [0.031, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.644, 10.179], loss: 0.003738, mae: 0.067844, mean_q: 0.401841
 71835/100000: episode: 3843, duration: 0.495s, episode steps: 78, steps per second: 158, episode reward: 15.786, mean reward: 0.202 [0.045, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-0.490, 10.100], loss: 0.003954, mae: 0.069999, mean_q: 0.403292
 71857/100000: episode: 3844, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 5.083, mean reward: 0.231 [0.137, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.789, 10.334], loss: 0.003976, mae: 0.069630, mean_q: 0.398475
 71879/100000: episode: 3845, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 6.769, mean reward: 0.308 [0.168, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.123, 10.264], loss: 0.003964, mae: 0.071082, mean_q: 0.423842
 71898/100000: episode: 3846, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 8.504, mean reward: 0.448 [0.223, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.324, 10.100], loss: 0.004505, mae: 0.075999, mean_q: 0.412545
 71928/100000: episode: 3847, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 5.652, mean reward: 0.188 [0.042, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.732, 10.100], loss: 0.004088, mae: 0.070731, mean_q: 0.417077
 71958/100000: episode: 3848, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 9.751, mean reward: 0.325 [0.228, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.216, 10.434], loss: 0.003706, mae: 0.067495, mean_q: 0.415413
 72036/100000: episode: 3849, duration: 0.407s, episode steps: 78, steps per second: 192, episode reward: 11.727, mean reward: 0.150 [0.015, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.668 [-0.615, 10.100], loss: 0.004136, mae: 0.071277, mean_q: 0.411334
 72115/100000: episode: 3850, duration: 0.415s, episode steps: 79, steps per second: 190, episode reward: 11.570, mean reward: 0.146 [0.008, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.655 [-0.497, 10.100], loss: 0.003902, mae: 0.070213, mean_q: 0.415166
 72136/100000: episode: 3851, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 5.585, mean reward: 0.266 [0.172, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-2.097, 10.363], loss: 0.004054, mae: 0.071960, mean_q: 0.409758
 72155/100000: episode: 3852, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 8.520, mean reward: 0.448 [0.277, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.433, 10.100], loss: 0.004218, mae: 0.071654, mean_q: 0.409219
 72185/100000: episode: 3853, duration: 0.153s, episode steps: 30, steps per second: 197, episode reward: 9.488, mean reward: 0.316 [0.131, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.722, 10.245], loss: 0.003381, mae: 0.065795, mean_q: 0.413848
 72263/100000: episode: 3854, duration: 0.425s, episode steps: 78, steps per second: 183, episode reward: 13.617, mean reward: 0.175 [0.031, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.660 [-0.836, 10.197], loss: 0.003748, mae: 0.068403, mean_q: 0.415751
 72342/100000: episode: 3855, duration: 0.398s, episode steps: 79, steps per second: 198, episode reward: 15.940, mean reward: 0.202 [0.020, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.664 [-0.118, 10.322], loss: 0.004002, mae: 0.070709, mean_q: 0.421645
[Info] New level: 0.9242534041404724 | Considering 10/90 traces
 72420/100000: episode: 3856, duration: 4.414s, episode steps: 78, steps per second: 18, episode reward: 18.439, mean reward: 0.236 [0.021, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-0.670, 10.322], loss: 0.003860, mae: 0.069986, mean_q: 0.414799
 72433/100000: episode: 3857, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 5.980, mean reward: 0.460 [0.405, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.546, 10.100], loss: 0.003896, mae: 0.069304, mean_q: 0.420869
 72446/100000: episode: 3858, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 5.114, mean reward: 0.393 [0.283, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.323, 10.100], loss: 0.003319, mae: 0.064340, mean_q: 0.410540
 72456/100000: episode: 3859, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 4.925, mean reward: 0.492 [0.410, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.361, 10.100], loss: 0.003679, mae: 0.067662, mean_q: 0.408759
 72469/100000: episode: 3860, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 6.149, mean reward: 0.473 [0.394, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.515, 10.100], loss: 0.003661, mae: 0.068177, mean_q: 0.428182
 72484/100000: episode: 3861, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 8.078, mean reward: 0.539 [0.351, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.629, 10.100], loss: 0.003683, mae: 0.067170, mean_q: 0.422587
 72495/100000: episode: 3862, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 4.692, mean reward: 0.427 [0.281, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.399, 10.100], loss: 0.003599, mae: 0.067358, mean_q: 0.430459
 72507/100000: episode: 3863, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 4.945, mean reward: 0.412 [0.309, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.402, 10.100], loss: 0.003482, mae: 0.066325, mean_q: 0.424244
 72519/100000: episode: 3864, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 5.533, mean reward: 0.461 [0.318, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.891, 10.100], loss: 0.004465, mae: 0.075319, mean_q: 0.435776
 72531/100000: episode: 3865, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 4.875, mean reward: 0.406 [0.337, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.419, 10.100], loss: 0.003886, mae: 0.066651, mean_q: 0.437596
 72542/100000: episode: 3866, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 5.691, mean reward: 0.517 [0.477, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.586, 10.100], loss: 0.004109, mae: 0.068744, mean_q: 0.433207
[RESULT] FALSIFICATION!
 72554/100000: episode: 3867, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 15.438, mean reward: 1.286 [0.356, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.567, 10.082], loss: 0.003348, mae: 0.066200, mean_q: 0.435377
 72566/100000: episode: 3868, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 5.468, mean reward: 0.456 [0.422, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.377, 10.100], loss: 0.003953, mae: 0.067715, mean_q: 0.430199
 72581/100000: episode: 3869, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 6.506, mean reward: 0.434 [0.334, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.527, 10.100], loss: 0.003967, mae: 0.069797, mean_q: 0.430897
 72593/100000: episode: 3870, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 5.213, mean reward: 0.434 [0.368, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.436, 10.100], loss: 0.004055, mae: 0.070321, mean_q: 0.421978
 72603/100000: episode: 3871, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 5.058, mean reward: 0.506 [0.427, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.437, 10.100], loss: 0.003405, mae: 0.065410, mean_q: 0.460937
 72616/100000: episode: 3872, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.594, mean reward: 0.430 [0.372, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.535, 10.100], loss: 0.003392, mae: 0.066628, mean_q: 0.442898
 72627/100000: episode: 3873, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 4.653, mean reward: 0.423 [0.365, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.414, 10.100], loss: 0.003978, mae: 0.067971, mean_q: 0.440843
 72638/100000: episode: 3874, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 6.199, mean reward: 0.564 [0.481, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.116, 10.100], loss: 0.004083, mae: 0.072811, mean_q: 0.445994
 72649/100000: episode: 3875, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 4.384, mean reward: 0.399 [0.337, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.229, 10.100], loss: 0.004017, mae: 0.070512, mean_q: 0.446994
 72664/100000: episode: 3876, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 6.693, mean reward: 0.446 [0.379, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.399, 10.100], loss: 0.003653, mae: 0.067887, mean_q: 0.445458
 72675/100000: episode: 3877, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 5.689, mean reward: 0.517 [0.480, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.465, 10.100], loss: 0.003492, mae: 0.066138, mean_q: 0.442826
 72687/100000: episode: 3878, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 6.976, mean reward: 0.581 [0.476, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.479, 10.100], loss: 0.003395, mae: 0.064335, mean_q: 0.464032
 72699/100000: episode: 3879, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 6.219, mean reward: 0.518 [0.364, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.430, 10.100], loss: 0.009082, mae: 0.106384, mean_q: 0.456732
 72712/100000: episode: 3880, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 7.309, mean reward: 0.562 [0.421, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.468, 10.100], loss: 0.004662, mae: 0.076359, mean_q: 0.451677
 72724/100000: episode: 3881, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 5.084, mean reward: 0.424 [0.360, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.512, 10.100], loss: 0.004821, mae: 0.075426, mean_q: 0.450876
 72735/100000: episode: 3882, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 4.186, mean reward: 0.381 [0.334, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.447, 10.100], loss: 0.004356, mae: 0.073342, mean_q: 0.460806
 72746/100000: episode: 3883, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 5.374, mean reward: 0.489 [0.398, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.688, 10.100], loss: 0.004091, mae: 0.070117, mean_q: 0.455649
 72758/100000: episode: 3884, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 5.965, mean reward: 0.497 [0.432, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.388, 10.100], loss: 0.004250, mae: 0.073591, mean_q: 0.459473
 72770/100000: episode: 3885, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 6.523, mean reward: 0.544 [0.408, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.462, 10.100], loss: 0.004288, mae: 0.072714, mean_q: 0.445846
[RESULT] FALSIFICATION!
 72778/100000: episode: 3886, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 14.371, mean reward: 1.796 [0.524, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.309, 10.082], loss: 0.003616, mae: 0.067803, mean_q: 0.457400
 72791/100000: episode: 3887, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 7.038, mean reward: 0.541 [0.487, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.835, 10.100], loss: 0.004839, mae: 0.077211, mean_q: 0.458824
 72801/100000: episode: 3888, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 4.778, mean reward: 0.478 [0.429, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.357, 10.100], loss: 0.004065, mae: 0.069405, mean_q: 0.440913
 72814/100000: episode: 3889, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 6.596, mean reward: 0.507 [0.411, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.454, 10.100], loss: 0.003866, mae: 0.070639, mean_q: 0.457909
 72827/100000: episode: 3890, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 6.559, mean reward: 0.505 [0.357, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.443, 10.100], loss: 0.004403, mae: 0.074918, mean_q: 0.475745
 72839/100000: episode: 3891, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 5.350, mean reward: 0.446 [0.363, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.386, 10.100], loss: 0.004089, mae: 0.071454, mean_q: 0.463442
 72852/100000: episode: 3892, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 5.838, mean reward: 0.449 [0.366, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.425, 10.100], loss: 0.103590, mae: 0.105008, mean_q: 0.484826
 72864/100000: episode: 3893, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 5.206, mean reward: 0.434 [0.355, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.253, 10.100], loss: 0.009350, mae: 0.106869, mean_q: 0.461501
 72875/100000: episode: 3894, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 5.200, mean reward: 0.473 [0.380, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.392, 10.100], loss: 0.006017, mae: 0.084855, mean_q: 0.493014
 72888/100000: episode: 3895, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 6.119, mean reward: 0.471 [0.388, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.061, 10.100], loss: 0.007083, mae: 0.082948, mean_q: 0.485740
 72901/100000: episode: 3896, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 6.173, mean reward: 0.475 [0.363, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.243, 10.100], loss: 0.005839, mae: 0.078960, mean_q: 0.484124
 72914/100000: episode: 3897, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 6.381, mean reward: 0.491 [0.370, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.505, 10.100], loss: 0.005327, mae: 0.076592, mean_q: 0.483087
 72925/100000: episode: 3898, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 5.287, mean reward: 0.481 [0.420, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.528, 10.100], loss: 0.005492, mae: 0.076776, mean_q: 0.457525
 72938/100000: episode: 3899, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 5.772, mean reward: 0.444 [0.359, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.587, 10.100], loss: 0.006079, mae: 0.079549, mean_q: 0.484724
 72948/100000: episode: 3900, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 5.168, mean reward: 0.517 [0.401, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.549, 10.100], loss: 0.004885, mae: 0.077946, mean_q: 0.477267
 72959/100000: episode: 3901, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 5.486, mean reward: 0.499 [0.446, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.552, 10.100], loss: 0.004781, mae: 0.071789, mean_q: 0.460421
 72969/100000: episode: 3902, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 4.122, mean reward: 0.412 [0.376, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.453, 10.100], loss: 0.004483, mae: 0.069923, mean_q: 0.481226
 72982/100000: episode: 3903, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 5.041, mean reward: 0.388 [0.343, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.370, 10.100], loss: 0.004363, mae: 0.071663, mean_q: 0.508720
 72992/100000: episode: 3904, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 4.161, mean reward: 0.416 [0.330, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.306, 10.100], loss: 0.004981, mae: 0.069923, mean_q: 0.466048
 73005/100000: episode: 3905, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 6.249, mean reward: 0.481 [0.391, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.574, 10.100], loss: 0.005851, mae: 0.079558, mean_q: 0.491485
 73015/100000: episode: 3906, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 4.917, mean reward: 0.492 [0.416, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.507, 10.100], loss: 0.004784, mae: 0.073417, mean_q: 0.489777
 73026/100000: episode: 3907, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.761, mean reward: 0.433 [0.332, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.551, 10.100], loss: 0.005411, mae: 0.074856, mean_q: 0.499283
 73037/100000: episode: 3908, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 6.004, mean reward: 0.546 [0.500, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.535, 10.100], loss: 0.005394, mae: 0.077665, mean_q: 0.476606
 73049/100000: episode: 3909, duration: 0.066s, episode steps: 12, steps per second: 180, episode reward: 5.967, mean reward: 0.497 [0.404, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.392, 10.100], loss: 0.004219, mae: 0.070131, mean_q: 0.490198
 73060/100000: episode: 3910, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 5.323, mean reward: 0.484 [0.449, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.489, 10.100], loss: 0.004441, mae: 0.073158, mean_q: 0.475420
 73070/100000: episode: 3911, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 4.633, mean reward: 0.463 [0.330, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.357, 10.100], loss: 0.005133, mae: 0.076616, mean_q: 0.488094
 73083/100000: episode: 3912, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 7.221, mean reward: 0.555 [0.383, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.515, 10.100], loss: 0.005202, mae: 0.075426, mean_q: 0.487946
 73096/100000: episode: 3913, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 7.483, mean reward: 0.576 [0.421, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.005, 10.100], loss: 0.004544, mae: 0.071511, mean_q: 0.475427
 73108/100000: episode: 3914, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 5.365, mean reward: 0.447 [0.365, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.658, 10.100], loss: 0.004551, mae: 0.072709, mean_q: 0.502384
 73119/100000: episode: 3915, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 5.113, mean reward: 0.465 [0.371, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.441, 10.100], loss: 0.125882, mae: 0.140932, mean_q: 0.503326
 73131/100000: episode: 3916, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.482, mean reward: 0.457 [0.336, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.298, 10.100], loss: 0.008330, mae: 0.106045, mean_q: 0.498638
 73142/100000: episode: 3917, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 5.819, mean reward: 0.529 [0.494, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.180, 10.100], loss: 0.007316, mae: 0.095644, mean_q: 0.512754
 73153/100000: episode: 3918, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 4.330, mean reward: 0.394 [0.347, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.507, 10.100], loss: 0.005399, mae: 0.083006, mean_q: 0.514757
 73166/100000: episode: 3919, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 5.898, mean reward: 0.454 [0.318, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.696, 10.100], loss: 0.004474, mae: 0.075444, mean_q: 0.495516
 73177/100000: episode: 3920, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 5.319, mean reward: 0.484 [0.438, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.574, 10.100], loss: 0.120577, mae: 0.093875, mean_q: 0.480301
 73190/100000: episode: 3921, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 4.925, mean reward: 0.379 [0.273, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.269, 10.100], loss: 0.011570, mae: 0.115884, mean_q: 0.532997
 73203/100000: episode: 3922, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 5.752, mean reward: 0.442 [0.344, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.451, 10.100], loss: 0.102318, mae: 0.096985, mean_q: 0.503148
 73215/100000: episode: 3923, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 6.234, mean reward: 0.520 [0.452, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.447, 10.100], loss: 0.011106, mae: 0.109506, mean_q: 0.481244
 73227/100000: episode: 3924, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 5.601, mean reward: 0.467 [0.333, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.496, 10.100], loss: 0.007714, mae: 0.091092, mean_q: 0.503863
 73239/100000: episode: 3925, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 6.064, mean reward: 0.505 [0.415, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.322, 10.100], loss: 0.007245, mae: 0.088995, mean_q: 0.523611
 73252/100000: episode: 3926, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 6.264, mean reward: 0.482 [0.411, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.534, 10.100], loss: 0.004874, mae: 0.073792, mean_q: 0.510340
 73263/100000: episode: 3927, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 5.160, mean reward: 0.469 [0.386, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.492, 10.100], loss: 0.004222, mae: 0.072864, mean_q: 0.511109
 73276/100000: episode: 3928, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 5.505, mean reward: 0.423 [0.378, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.356, 10.100], loss: 0.004882, mae: 0.075591, mean_q: 0.510619
[RESULT] FALSIFICATION!
 73288/100000: episode: 3929, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 16.151, mean reward: 1.346 [0.484, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.895, 10.098], loss: 0.005770, mae: 0.082407, mean_q: 0.518385
 73301/100000: episode: 3930, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.589, mean reward: 0.430 [0.364, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.348, 10.100], loss: 0.004651, mae: 0.073514, mean_q: 0.505829
 73312/100000: episode: 3931, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 4.940, mean reward: 0.449 [0.376, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.138, 10.100], loss: 0.004745, mae: 0.074021, mean_q: 0.545259
 73325/100000: episode: 3932, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 6.526, mean reward: 0.502 [0.375, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.393, 10.100], loss: 0.004188, mae: 0.071352, mean_q: 0.518371
 73335/100000: episode: 3933, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 4.593, mean reward: 0.459 [0.389, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.491, 10.100], loss: 0.004182, mae: 0.072461, mean_q: 0.495083
 73348/100000: episode: 3934, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 5.196, mean reward: 0.400 [0.362, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.303, 10.100], loss: 0.003734, mae: 0.068429, mean_q: 0.520484
 73361/100000: episode: 3935, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 5.136, mean reward: 0.395 [0.266, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.498, 10.100], loss: 0.003411, mae: 0.064455, mean_q: 0.520182
 73373/100000: episode: 3936, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 6.507, mean reward: 0.542 [0.473, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.163, 10.100], loss: 0.003830, mae: 0.068615, mean_q: 0.515087
 73383/100000: episode: 3937, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 4.403, mean reward: 0.440 [0.403, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.432, 10.100], loss: 0.142513, mae: 0.118628, mean_q: 0.571832
 73395/100000: episode: 3938, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 4.421, mean reward: 0.368 [0.321, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.630, 10.100], loss: 0.113144, mae: 0.122744, mean_q: 0.553474
 73410/100000: episode: 3939, duration: 0.106s, episode steps: 15, steps per second: 141, episode reward: 7.407, mean reward: 0.494 [0.385, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.425, 10.100], loss: 0.091664, mae: 0.118429, mean_q: 0.535064
 73425/100000: episode: 3940, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 6.380, mean reward: 0.425 [0.364, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.799, 10.100], loss: 0.006990, mae: 0.086542, mean_q: 0.537078
 73436/100000: episode: 3941, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 5.068, mean reward: 0.461 [0.398, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.929, 10.100], loss: 0.006065, mae: 0.082390, mean_q: 0.535169
 73447/100000: episode: 3942, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 4.904, mean reward: 0.446 [0.352, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.664, 10.100], loss: 0.121467, mae: 0.110678, mean_q: 0.548053
 73457/100000: episode: 3943, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 4.452, mean reward: 0.445 [0.412, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.648, 10.100], loss: 0.137418, mae: 0.133378, mean_q: 0.511656
 73470/100000: episode: 3944, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.839, mean reward: 0.449 [0.401, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.403, 10.100], loss: 0.010477, mae: 0.110397, mean_q: 0.521665
 73480/100000: episode: 3945, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 4.556, mean reward: 0.456 [0.407, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.385, 10.100], loss: 0.009318, mae: 0.099244, mean_q: 0.556946
[Info] New level: 1.0159863233566284 | Considering 10/90 traces
 73491/100000: episode: 3946, duration: 4.061s, episode steps: 11, steps per second: 3, episode reward: 5.661, mean reward: 0.515 [0.475, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.393, 10.100], loss: 0.005253, mae: 0.074894, mean_q: 0.514506
 73503/100000: episode: 3947, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 6.304, mean reward: 0.525 [0.432, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.492, 10.100], loss: 0.005464, mae: 0.075701, mean_q: 0.552146
 73511/100000: episode: 3948, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 3.834, mean reward: 0.479 [0.428, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.587, 10.100], loss: 0.005406, mae: 0.071668, mean_q: 0.534176
 73514/100000: episode: 3949, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 1.696, mean reward: 0.565 [0.530, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.544, 10.100], loss: 0.004809, mae: 0.075121, mean_q: 0.577677
 73524/100000: episode: 3950, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 6.119, mean reward: 0.612 [0.504, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.583, 10.100], loss: 0.006212, mae: 0.079514, mean_q: 0.549440
 73527/100000: episode: 3951, duration: 0.027s, episode steps: 3, steps per second: 111, episode reward: 1.569, mean reward: 0.523 [0.514, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.518, 10.100], loss: 0.005365, mae: 0.077112, mean_q: 0.582805
 73535/100000: episode: 3952, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.541, mean reward: 0.568 [0.542, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.575, 10.100], loss: 0.005898, mae: 0.078431, mean_q: 0.541227
 73541/100000: episode: 3953, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.889, mean reward: 0.481 [0.400, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.477, 10.100], loss: 0.004824, mae: 0.076407, mean_q: 0.527595
 73544/100000: episode: 3954, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 1.589, mean reward: 0.530 [0.511, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.497, 10.100], loss: 0.007245, mae: 0.091001, mean_q: 0.558957
 73550/100000: episode: 3955, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 2.726, mean reward: 0.454 [0.414, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.502, 10.100], loss: 0.006560, mae: 0.078505, mean_q: 0.536217
 73559/100000: episode: 3956, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 4.943, mean reward: 0.549 [0.477, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.392, 10.100], loss: 0.006191, mae: 0.084314, mean_q: 0.530275
 73567/100000: episode: 3957, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 4.117, mean reward: 0.515 [0.487, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.521, 10.100], loss: 0.004179, mae: 0.072841, mean_q: 0.518904
 73570/100000: episode: 3958, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 1.586, mean reward: 0.529 [0.521, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.508, 10.100], loss: 0.424461, mae: 0.177667, mean_q: 0.540243
 73571/100000: episode: 3959, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.545, mean reward: 0.545 [0.545, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.699, 10.100], loss: 0.011171, mae: 0.109662, mean_q: 0.583619
 73572/100000: episode: 3960, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.581, mean reward: 0.581 [0.581, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.636, 10.100], loss: 0.021586, mae: 0.147961, mean_q: 0.645948
 73575/100000: episode: 3961, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 1.597, mean reward: 0.532 [0.497, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.504, 10.100], loss: 0.007971, mae: 0.088619, mean_q: 0.508545
 73581/100000: episode: 3962, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 2.709, mean reward: 0.452 [0.395, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.690, 10.100], loss: 0.007596, mae: 0.093549, mean_q: 0.528864
 73589/100000: episode: 3963, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.223, mean reward: 0.528 [0.463, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.582, 10.100], loss: 0.008342, mae: 0.096565, mean_q: 0.540401
 73592/100000: episode: 3964, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.663, mean reward: 0.554 [0.504, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.576, 10.100], loss: 0.007088, mae: 0.091699, mean_q: 0.551771
 73598/100000: episode: 3965, duration: 0.033s, episode steps: 6, steps per second: 185, episode reward: 2.973, mean reward: 0.495 [0.448, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.667, 10.100], loss: 0.006394, mae: 0.084864, mean_q: 0.615150
 73601/100000: episode: 3966, duration: 0.028s, episode steps: 3, steps per second: 109, episode reward: 1.681, mean reward: 0.560 [0.527, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.644, 10.100], loss: 0.005570, mae: 0.079818, mean_q: 0.563889
 73611/100000: episode: 3967, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 4.579, mean reward: 0.458 [0.394, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.346, 10.100], loss: 0.005486, mae: 0.075367, mean_q: 0.559009
 73614/100000: episode: 3968, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.528, mean reward: 0.509 [0.462, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.616, 10.100], loss: 0.006512, mae: 0.083864, mean_q: 0.555502
 73626/100000: episode: 3969, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 6.094, mean reward: 0.508 [0.381, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.644, 10.100], loss: 0.004961, mae: 0.075199, mean_q: 0.546457
[RESULT] FALSIFICATION!
 73633/100000: episode: 3970, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 13.813, mean reward: 1.973 [0.619, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.633, 10.098], loss: 0.006004, mae: 0.081262, mean_q: 0.522987
 73643/100000: episode: 3971, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 6.032, mean reward: 0.603 [0.513, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.540, 10.100], loss: 0.268167, mae: 0.163805, mean_q: 0.641161
 73651/100000: episode: 3972, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 4.394, mean reward: 0.549 [0.515, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.490, 10.100], loss: 0.009189, mae: 0.102988, mean_q: 0.528650
 73654/100000: episode: 3973, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 1.659, mean reward: 0.553 [0.537, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.569, 10.100], loss: 0.007096, mae: 0.086662, mean_q: 0.590319
 73660/100000: episode: 3974, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 2.835, mean reward: 0.472 [0.375, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.586, 10.100], loss: 0.217589, mae: 0.131699, mean_q: 0.576609
 73668/100000: episode: 3975, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 4.183, mean reward: 0.523 [0.449, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.512, 10.100], loss: 0.010222, mae: 0.099685, mean_q: 0.585730
 73676/100000: episode: 3976, duration: 0.042s, episode steps: 8, steps per second: 188, episode reward: 4.416, mean reward: 0.552 [0.524, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.554, 10.100], loss: 0.005571, mae: 0.080208, mean_q: 0.546238
 73688/100000: episode: 3977, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 6.793, mean reward: 0.566 [0.485, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.550, 10.100], loss: 0.006632, mae: 0.079724, mean_q: 0.546893
[RESULT] FALSIFICATION!
 73695/100000: episode: 3978, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 13.588, mean reward: 1.941 [0.536, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.324, 10.046], loss: 0.005775, mae: 0.078547, mean_q: 0.546584
 73698/100000: episode: 3979, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.706, mean reward: 0.569 [0.547, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.511, 10.100], loss: 0.005692, mae: 0.081032, mean_q: 0.586000
 73708/100000: episode: 3980, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 4.802, mean reward: 0.480 [0.424, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.360, 10.100], loss: 0.005998, mae: 0.082519, mean_q: 0.553845
 73711/100000: episode: 3981, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.626, mean reward: 0.542 [0.516, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.571, 10.100], loss: 0.007005, mae: 0.081979, mean_q: 0.500125
 73719/100000: episode: 3982, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 4.839, mean reward: 0.605 [0.559, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.657, 10.100], loss: 0.006731, mae: 0.086104, mean_q: 0.550873
 73728/100000: episode: 3983, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 4.149, mean reward: 0.461 [0.438, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.426, 10.100], loss: 0.005244, mae: 0.077412, mean_q: 0.546395
 73740/100000: episode: 3984, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 5.194, mean reward: 0.433 [0.361, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.599, 10.100], loss: 0.111358, mae: 0.111939, mean_q: 0.590955
 73743/100000: episode: 3985, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 1.618, mean reward: 0.539 [0.499, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.709, 10.100], loss: 0.005870, mae: 0.079004, mean_q: 0.538350
 73755/100000: episode: 3986, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 5.680, mean reward: 0.473 [0.439, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.563, 10.100], loss: 0.006509, mae: 0.085527, mean_q: 0.555825
 73756/100000: episode: 3987, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.582, mean reward: 0.582 [0.582, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.673, 10.100], loss: 0.008075, mae: 0.097877, mean_q: 0.526674
 73766/100000: episode: 3988, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 4.856, mean reward: 0.486 [0.363, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.378, 10.100], loss: 0.006035, mae: 0.081887, mean_q: 0.535582
[RESULT] FALSIFICATION!
 73772/100000: episode: 3989, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 13.209, mean reward: 2.201 [0.627, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.503, 10.093], loss: 0.006589, mae: 0.091086, mean_q: 0.543622
 73780/100000: episode: 3990, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.401, mean reward: 0.550 [0.507, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.474, 10.100], loss: 0.009056, mae: 0.099761, mean_q: 0.621920
 73789/100000: episode: 3991, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 4.407, mean reward: 0.490 [0.462, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.512, 10.100], loss: 0.006632, mae: 0.089271, mean_q: 0.538835
 73792/100000: episode: 3992, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.647, mean reward: 0.549 [0.530, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.583, 10.100], loss: 0.006124, mae: 0.085964, mean_q: 0.580631
 73798/100000: episode: 3993, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 2.956, mean reward: 0.493 [0.351, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.623, 10.100], loss: 0.004857, mae: 0.074731, mean_q: 0.545088
 73801/100000: episode: 3994, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.572, mean reward: 0.524 [0.481, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.170, 10.100], loss: 0.007962, mae: 0.092408, mean_q: 0.588813
 73813/100000: episode: 3995, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 4.575, mean reward: 0.381 [0.292, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.520, 10.100], loss: 0.223112, mae: 0.155792, mean_q: 0.632553
 73814/100000: episode: 3996, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.595, mean reward: 0.595 [0.595, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.648, 10.100], loss: 0.010086, mae: 0.116682, mean_q: 0.644612
 73822/100000: episode: 3997, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.685, mean reward: 0.586 [0.549, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.516, 10.100], loss: 0.164951, mae: 0.120397, mean_q: 0.558004
 73834/100000: episode: 3998, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 5.300, mean reward: 0.442 [0.346, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.424, 10.100], loss: 0.009229, mae: 0.104157, mean_q: 0.566303
 73843/100000: episode: 3999, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 5.302, mean reward: 0.589 [0.502, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.988, 10.100], loss: 0.006102, mae: 0.082493, mean_q: 0.606443
 73853/100000: episode: 4000, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 4.962, mean reward: 0.496 [0.469, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.483, 10.100], loss: 0.007036, mae: 0.093642, mean_q: 0.518906
 73854/100000: episode: 4001, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.578, mean reward: 0.578 [0.578, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.643, 10.100], loss: 0.004670, mae: 0.079592, mean_q: 0.616876
 73862/100000: episode: 4002, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 5.070, mean reward: 0.634 [0.609, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.529, 10.100], loss: 0.167549, mae: 0.129815, mean_q: 0.613978
 73870/100000: episode: 4003, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 3.367, mean reward: 0.421 [0.368, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.546, 10.100], loss: 0.166010, mae: 0.119907, mean_q: 0.528427
 73880/100000: episode: 4004, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 5.764, mean reward: 0.576 [0.539, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.527, 10.100], loss: 0.264933, mae: 0.190269, mean_q: 0.669529
 73888/100000: episode: 4005, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 4.362, mean reward: 0.545 [0.505, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.463, 10.100], loss: 0.008080, mae: 0.098794, mean_q: 0.514789
 73897/100000: episode: 4006, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 4.738, mean reward: 0.526 [0.467, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.436, 10.100], loss: 0.007591, mae: 0.091755, mean_q: 0.604555
 73905/100000: episode: 4007, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 4.758, mean reward: 0.595 [0.558, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.672, 10.100], loss: 0.006322, mae: 0.085466, mean_q: 0.522590
 73911/100000: episode: 4008, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 2.911, mean reward: 0.485 [0.394, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.589, 10.100], loss: 0.005956, mae: 0.080101, mean_q: 0.588687
 73921/100000: episode: 4009, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 5.704, mean reward: 0.570 [0.485, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.559, 10.100], loss: 0.005189, mae: 0.076759, mean_q: 0.581821
 73922/100000: episode: 4010, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.566, mean reward: 0.566 [0.566, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.668, 10.100], loss: 0.006237, mae: 0.090788, mean_q: 0.550706
 73931/100000: episode: 4011, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 4.723, mean reward: 0.525 [0.479, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.529, 10.100], loss: 0.005190, mae: 0.076170, mean_q: 0.610895
 73943/100000: episode: 4012, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 5.353, mean reward: 0.446 [0.393, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.409, 10.100], loss: 0.004235, mae: 0.070901, mean_q: 0.582887
 73951/100000: episode: 4013, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 4.387, mean reward: 0.548 [0.513, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.685, 10.100], loss: 0.003817, mae: 0.068018, mean_q: 0.577027
 73960/100000: episode: 4014, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 4.597, mean reward: 0.511 [0.465, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.514, 10.100], loss: 0.150417, mae: 0.121441, mean_q: 0.615449
 73961/100000: episode: 4015, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.643, mean reward: 0.643 [0.643, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.706, 10.100], loss: 0.006148, mae: 0.091483, mean_q: 0.593565
 73967/100000: episode: 4016, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 2.939, mean reward: 0.490 [0.356, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.673, 10.100], loss: 0.004933, mae: 0.081012, mean_q: 0.529848
 73977/100000: episode: 4017, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 4.833, mean reward: 0.483 [0.459, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.890, 10.100], loss: 0.007268, mae: 0.088206, mean_q: 0.585479
 73989/100000: episode: 4018, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 6.004, mean reward: 0.500 [0.449, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.391, 10.100], loss: 0.005804, mae: 0.080833, mean_q: 0.569118
 73999/100000: episode: 4019, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 5.481, mean reward: 0.548 [0.466, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.532, 10.100], loss: 0.005020, mae: 0.075911, mean_q: 0.604648
 74009/100000: episode: 4020, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 4.322, mean reward: 0.432 [0.395, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.465, 10.100], loss: 0.004480, mae: 0.075483, mean_q: 0.575782
 74021/100000: episode: 4021, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 6.820, mean reward: 0.568 [0.467, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.901, 10.100], loss: 0.005581, mae: 0.081157, mean_q: 0.572044
 74033/100000: episode: 4022, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 5.898, mean reward: 0.491 [0.444, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.514, 10.100], loss: 0.005172, mae: 0.075208, mean_q: 0.565754
 74043/100000: episode: 4023, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 5.251, mean reward: 0.525 [0.467, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.685, 10.100], loss: 0.003398, mae: 0.063507, mean_q: 0.583434
 74044/100000: episode: 4024, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.574, mean reward: 0.574 [0.574, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.637, 10.100], loss: 0.006961, mae: 0.078505, mean_q: 0.616440
 74053/100000: episode: 4025, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 5.004, mean reward: 0.556 [0.462, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.467, 10.100], loss: 0.009389, mae: 0.103045, mean_q: 0.592099
 74056/100000: episode: 4026, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 1.474, mean reward: 0.491 [0.484, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.541, 10.100], loss: 0.008459, mae: 0.102175, mean_q: 0.577371
 74066/100000: episode: 4027, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 5.708, mean reward: 0.571 [0.518, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.390, 10.100], loss: 0.006655, mae: 0.090189, mean_q: 0.617054
 74075/100000: episode: 4028, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 4.140, mean reward: 0.460 [0.394, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.427, 10.100], loss: 0.152874, mae: 0.155285, mean_q: 0.675987
 74085/100000: episode: 4029, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 4.606, mean reward: 0.461 [0.376, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.143, 10.100], loss: 0.010315, mae: 0.106809, mean_q: 0.556069
 74093/100000: episode: 4030, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 4.806, mean reward: 0.601 [0.551, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.553, 10.100], loss: 0.168295, mae: 0.120485, mean_q: 0.608661
 74101/100000: episode: 4031, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 4.656, mean reward: 0.582 [0.463, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.454, 10.100], loss: 0.166232, mae: 0.137744, mean_q: 0.651895
 74111/100000: episode: 4032, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 5.819, mean reward: 0.582 [0.486, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.553, 10.100], loss: 0.010046, mae: 0.110463, mean_q: 0.588152
 74112/100000: episode: 4033, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.555, mean reward: 0.555 [0.555, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.717, 10.100], loss: 0.005769, mae: 0.080405, mean_q: 0.636740
 74118/100000: episode: 4034, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 2.993, mean reward: 0.499 [0.442, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.616, 10.100], loss: 0.224850, mae: 0.149940, mean_q: 0.649957
 74126/100000: episode: 4035, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 4.025, mean reward: 0.503 [0.465, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.451, 10.100], loss: 0.167454, mae: 0.134979, mean_q: 0.556130
[Info] New level: 1.0268728733062744 | Considering 15/85 traces
 74138/100000: episode: 4036, duration: 4.053s, episode steps: 12, steps per second: 3, episode reward: 5.411, mean reward: 0.451 [0.384, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.471, 10.100], loss: 0.008550, mae: 0.094816, mean_q: 0.635733
 74144/100000: episode: 4037, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 2.867, mean reward: 0.478 [0.412, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.607, 10.100], loss: 0.007962, mae: 0.100083, mean_q: 0.559537
 74151/100000: episode: 4038, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 3.531, mean reward: 0.504 [0.426, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.463, 10.100], loss: 0.006902, mae: 0.091177, mean_q: 0.634481
[RESULT] FALSIFICATION!
 74157/100000: episode: 4039, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 12.814, mean reward: 2.136 [0.505, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.625, 10.100], loss: 0.007111, mae: 0.087853, mean_q: 0.596127
 74164/100000: episode: 4040, duration: 0.046s, episode steps: 7, steps per second: 154, episode reward: 4.257, mean reward: 0.608 [0.540, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.562, 10.100], loss: 0.006340, mae: 0.084942, mean_q: 0.653468
[RESULT] FALSIFICATION!
 74166/100000: episode: 4041, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 10.699, mean reward: 5.349 [0.699, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.440, 10.046], loss: 0.005256, mae: 0.075877, mean_q: 0.640031
 74172/100000: episode: 4042, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.820, mean reward: 0.470 [0.398, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.681, 10.100], loss: 0.006759, mae: 0.090960, mean_q: 0.547756
 74178/100000: episode: 4043, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 3.310, mean reward: 0.552 [0.406, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.798, 10.100], loss: 0.004955, mae: 0.077971, mean_q: 0.566306
 74184/100000: episode: 4044, duration: 0.033s, episode steps: 6, steps per second: 185, episode reward: 2.726, mean reward: 0.454 [0.406, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.572, 10.100], loss: 0.006734, mae: 0.082161, mean_q: 0.621412
 74191/100000: episode: 4045, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 3.816, mean reward: 0.545 [0.500, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.686, 10.100], loss: 0.191989, mae: 0.124743, mean_q: 0.629934
 74197/100000: episode: 4046, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 2.728, mean reward: 0.455 [0.396, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.596, 10.100], loss: 0.011085, mae: 0.099652, mean_q: 0.589249
 74203/100000: episode: 4047, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 2.497, mean reward: 0.416 [0.353, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.638, 10.100], loss: 0.004985, mae: 0.078780, mean_q: 0.549824
 74209/100000: episode: 4048, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 2.461, mean reward: 0.410 [0.373, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.580, 10.100], loss: 0.225017, mae: 0.141806, mean_q: 0.637036
 74215/100000: episode: 4049, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 2.528, mean reward: 0.421 [0.372, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.659, 10.100], loss: 0.008309, mae: 0.094911, mean_q: 0.598064
 74221/100000: episode: 4050, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 3.560, mean reward: 0.593 [0.503, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.638, 10.100], loss: 0.006443, mae: 0.087026, mean_q: 0.617465
 74227/100000: episode: 4051, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.697, mean reward: 0.449 [0.376, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.547, 10.100], loss: 0.004770, mae: 0.073565, mean_q: 0.659981
 74234/100000: episode: 4052, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 4.004, mean reward: 0.572 [0.530, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.431, 10.100], loss: 0.006852, mae: 0.084477, mean_q: 0.546610
 74240/100000: episode: 4053, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 2.713, mean reward: 0.452 [0.338, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.026, 10.100], loss: 0.005681, mae: 0.084687, mean_q: 0.595505
 74247/100000: episode: 4054, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 3.322, mean reward: 0.475 [0.435, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.635, 10.100], loss: 0.006461, mae: 0.087567, mean_q: 0.629695
 74253/100000: episode: 4055, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 3.153, mean reward: 0.526 [0.399, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.656, 10.100], loss: 0.004850, mae: 0.075054, mean_q: 0.646330
 74259/100000: episode: 4056, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 2.729, mean reward: 0.455 [0.430, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.473, 10.100], loss: 0.217892, mae: 0.142156, mean_q: 0.640576
 74265/100000: episode: 4057, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.796, mean reward: 0.466 [0.383, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.572, 10.100], loss: 0.006506, mae: 0.086137, mean_q: 0.631196
 74271/100000: episode: 4058, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 3.315, mean reward: 0.553 [0.538, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.682, 10.100], loss: 0.226744, mae: 0.144759, mean_q: 0.582893
 74277/100000: episode: 4059, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 2.614, mean reward: 0.436 [0.390, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.652, 10.100], loss: 0.222791, mae: 0.175351, mean_q: 0.726263
 74283/100000: episode: 4060, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 3.146, mean reward: 0.524 [0.491, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.530, 10.100], loss: 0.231201, mae: 0.153103, mean_q: 0.611825
 74289/100000: episode: 4061, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 3.298, mean reward: 0.550 [0.426, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.611, 10.100], loss: 0.008390, mae: 0.088359, mean_q: 0.625134
 74295/100000: episode: 4062, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 2.941, mean reward: 0.490 [0.377, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.628, 10.100], loss: 0.005750, mae: 0.077807, mean_q: 0.634391
 74302/100000: episode: 4063, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.904, mean reward: 0.558 [0.510, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.609, 10.100], loss: 0.006327, mae: 0.087946, mean_q: 0.591042
 74309/100000: episode: 4064, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 3.926, mean reward: 0.561 [0.537, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.659, 10.100], loss: 0.007370, mae: 0.092354, mean_q: 0.622062
 74315/100000: episode: 4065, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 2.864, mean reward: 0.477 [0.418, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.641, 10.100], loss: 0.006056, mae: 0.082059, mean_q: 0.613424
 74322/100000: episode: 4066, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.281, mean reward: 0.612 [0.569, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.553, 10.100], loss: 0.006116, mae: 0.079906, mean_q: 0.613032
 74328/100000: episode: 4067, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 3.448, mean reward: 0.575 [0.499, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.165, 10.100], loss: 0.222064, mae: 0.124805, mean_q: 0.591235
 74334/100000: episode: 4068, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 2.761, mean reward: 0.460 [0.405, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.937, 10.100], loss: 0.007941, mae: 0.090038, mean_q: 0.665409
 74342/100000: episode: 4069, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 4.027, mean reward: 0.503 [0.440, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.496, 10.100], loss: 0.005561, mae: 0.077730, mean_q: 0.611446
 74349/100000: episode: 4070, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 3.859, mean reward: 0.551 [0.517, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.294, 10.100], loss: 0.194605, mae: 0.130505, mean_q: 0.647964
 74355/100000: episode: 4071, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 3.093, mean reward: 0.516 [0.409, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.602, 10.100], loss: 0.006375, mae: 0.087584, mean_q: 0.643613
 74361/100000: episode: 4072, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.411, mean reward: 0.402 [0.344, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.541, 10.100], loss: 0.007883, mae: 0.098415, mean_q: 0.580047
 74367/100000: episode: 4073, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 3.793, mean reward: 0.632 [0.607, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.092, 10.100], loss: 0.432963, mae: 0.190361, mean_q: 0.656457
 74375/100000: episode: 4074, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 4.148, mean reward: 0.519 [0.435, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.521, 10.100], loss: 0.009714, mae: 0.107396, mean_q: 0.686854
 74381/100000: episode: 4075, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.463, mean reward: 0.577 [0.537, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.573, 10.100], loss: 0.012509, mae: 0.128428, mean_q: 0.555929
 74387/100000: episode: 4076, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 2.721, mean reward: 0.453 [0.409, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.589, 10.100], loss: 0.220121, mae: 0.147367, mean_q: 0.704798
 74393/100000: episode: 4077, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 2.553, mean reward: 0.425 [0.377, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.696, 10.100], loss: 0.011831, mae: 0.114425, mean_q: 0.704777
 74401/100000: episode: 4078, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 3.677, mean reward: 0.460 [0.404, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.447, 10.100], loss: 0.172912, mae: 0.139973, mean_q: 0.590029
 74408/100000: episode: 4079, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 3.810, mean reward: 0.544 [0.472, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.582, 10.100], loss: 0.008561, mae: 0.097391, mean_q: 0.676662
[RESULT] FALSIFICATION!
 74415/100000: episode: 4080, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 13.419, mean reward: 1.917 [0.498, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.631, 10.100], loss: 0.006783, mae: 0.086855, mean_q: 0.590210
 74421/100000: episode: 4081, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 2.424, mean reward: 0.404 [0.337, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.676, 10.100], loss: 0.005895, mae: 0.082761, mean_q: 0.626846
 74427/100000: episode: 4082, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 2.917, mean reward: 0.486 [0.422, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.572, 10.100], loss: 0.221515, mae: 0.122625, mean_q: 0.622041
 74433/100000: episode: 4083, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 3.087, mean reward: 0.515 [0.422, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.721, 10.100], loss: 0.013479, mae: 0.126925, mean_q: 0.723059
 74440/100000: episode: 4084, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 3.799, mean reward: 0.543 [0.525, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.550, 10.100], loss: 0.010537, mae: 0.116059, mean_q: 0.609965
 74446/100000: episode: 4085, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.486, mean reward: 0.414 [0.358, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.681, 10.100], loss: 0.007822, mae: 0.101196, mean_q: 0.612662
 74452/100000: episode: 4086, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 2.815, mean reward: 0.469 [0.410, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.564, 10.100], loss: 0.007638, mae: 0.090496, mean_q: 0.675926
 74459/100000: episode: 4087, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 4.132, mean reward: 0.590 [0.553, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.516, 10.100], loss: 0.006454, mae: 0.091253, mean_q: 0.591591
 74465/100000: episode: 4088, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 2.596, mean reward: 0.433 [0.398, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.533, 10.100], loss: 0.006911, mae: 0.087420, mean_q: 0.602332
 74471/100000: episode: 4089, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 2.894, mean reward: 0.482 [0.417, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.554, 10.100], loss: 0.222800, mae: 0.141839, mean_q: 0.679599
 74477/100000: episode: 4090, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 2.577, mean reward: 0.429 [0.401, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.580, 10.100], loss: 0.007174, mae: 0.086380, mean_q: 0.584973
 74483/100000: episode: 4091, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 2.927, mean reward: 0.488 [0.393, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.567, 10.100], loss: 0.224694, mae: 0.154709, mean_q: 0.692428
 74489/100000: episode: 4092, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 2.883, mean reward: 0.480 [0.388, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.620, 10.100], loss: 0.227182, mae: 0.145703, mean_q: 0.678759
 74495/100000: episode: 4093, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 2.538, mean reward: 0.423 [0.388, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.723, 10.100], loss: 0.007356, mae: 0.088751, mean_q: 0.666522
 74501/100000: episode: 4094, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 2.570, mean reward: 0.428 [0.410, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.714, 10.100], loss: 0.221079, mae: 0.138842, mean_q: 0.648640
 74508/100000: episode: 4095, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.054, mean reward: 0.579 [0.481, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.500, 10.100], loss: 0.189963, mae: 0.134106, mean_q: 0.680623
 74514/100000: episode: 4096, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.044, mean reward: 0.507 [0.441, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.646, 10.100], loss: 0.007662, mae: 0.083537, mean_q: 0.646887
 74520/100000: episode: 4097, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 2.755, mean reward: 0.459 [0.379, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.614, 10.100], loss: 0.224286, mae: 0.129195, mean_q: 0.630853
 74526/100000: episode: 4098, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 2.790, mean reward: 0.465 [0.411, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.255, 10.100], loss: 0.009396, mae: 0.109147, mean_q: 0.709593
 74532/100000: episode: 4099, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 2.725, mean reward: 0.454 [0.392, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.530, 10.100], loss: 0.007925, mae: 0.098475, mean_q: 0.607202
 74538/100000: episode: 4100, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 2.427, mean reward: 0.404 [0.334, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.526, 10.100], loss: 0.007519, mae: 0.091380, mean_q: 0.604517
 74544/100000: episode: 4101, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 2.719, mean reward: 0.453 [0.384, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.513, 10.100], loss: 0.006704, mae: 0.088671, mean_q: 0.672582
 74551/100000: episode: 4102, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 3.959, mean reward: 0.566 [0.547, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.674, 10.100], loss: 0.007099, mae: 0.091423, mean_q: 0.631486
 74557/100000: episode: 4103, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 3.632, mean reward: 0.605 [0.556, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.701, 10.100], loss: 0.005525, mae: 0.080618, mean_q: 0.647593
 74564/100000: episode: 4104, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 3.844, mean reward: 0.549 [0.468, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.607, 10.100], loss: 0.191938, mae: 0.135639, mean_q: 0.694247
 74570/100000: episode: 4105, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 2.943, mean reward: 0.491 [0.421, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.642, 10.100], loss: 0.644214, mae: 0.244780, mean_q: 0.709824
 74577/100000: episode: 4106, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 3.556, mean reward: 0.508 [0.490, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.534, 10.100], loss: 0.014093, mae: 0.126906, mean_q: 0.742971
 74584/100000: episode: 4107, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 4.197, mean reward: 0.600 [0.530, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.505, 10.100], loss: 0.011155, mae: 0.118348, mean_q: 0.634996
 74590/100000: episode: 4108, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 3.723, mean reward: 0.621 [0.566, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.610, 10.100], loss: 0.222807, mae: 0.152865, mean_q: 0.687124
 74596/100000: episode: 4109, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 3.248, mean reward: 0.541 [0.452, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.665, 10.100], loss: 0.426305, mae: 0.210814, mean_q: 0.746370
 74603/100000: episode: 4110, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 4.276, mean reward: 0.611 [0.558, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.567, 10.100], loss: 0.012542, mae: 0.114404, mean_q: 0.653819
 74609/100000: episode: 4111, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 2.958, mean reward: 0.493 [0.429, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.647, 10.100], loss: 0.011798, mae: 0.117026, mean_q: 0.699659
 74615/100000: episode: 4112, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 2.289, mean reward: 0.382 [0.318, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.065, 10.100], loss: 0.008764, mae: 0.095023, mean_q: 0.669755
 74621/100000: episode: 4113, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 2.081, mean reward: 0.347 [0.295, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.605, 10.100], loss: 0.218243, mae: 0.133818, mean_q: 0.638333
[RESULT] FALSIFICATION!
 74626/100000: episode: 4114, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 12.627, mean reward: 2.525 [0.631, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.532, 10.093], loss: 0.010084, mae: 0.094878, mean_q: 0.685983
 74632/100000: episode: 4115, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 2.379, mean reward: 0.397 [0.352, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.667, 10.100], loss: 0.219374, mae: 0.131114, mean_q: 0.686366
 74639/100000: episode: 4116, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 3.999, mean reward: 0.571 [0.525, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.483, 10.100], loss: 0.188538, mae: 0.133217, mean_q: 0.714883
 74646/100000: episode: 4117, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.639, mean reward: 0.520 [0.457, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.628, 10.100], loss: 0.007498, mae: 0.087266, mean_q: 0.689111
 74652/100000: episode: 4118, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 2.553, mean reward: 0.425 [0.387, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.486, 10.100], loss: 0.224705, mae: 0.141543, mean_q: 0.651861
 74658/100000: episode: 4119, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 2.468, mean reward: 0.411 [0.370, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.569, 10.100], loss: 0.012083, mae: 0.108468, mean_q: 0.691568
 74664/100000: episode: 4120, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 2.693, mean reward: 0.449 [0.362, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.552, 10.100], loss: 0.007598, mae: 0.084831, mean_q: 0.669015
[Info] New level: 1.0943608283996582 | Considering 14/86 traces
 74670/100000: episode: 4121, duration: 4.016s, episode steps: 6, steps per second: 1, episode reward: 2.394, mean reward: 0.399 [0.357, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.619, 10.100], loss: 0.008215, mae: 0.094975, mean_q: 0.615096
 74677/100000: episode: 4122, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 3.567, mean reward: 0.510 [0.471, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.652, 10.100], loss: 0.007264, mae: 0.091544, mean_q: 0.682840
 74685/100000: episode: 4123, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 4.925, mean reward: 0.616 [0.569, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.616, 10.100], loss: 0.005555, mae: 0.079808, mean_q: 0.658483
 74692/100000: episode: 4124, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 3.765, mean reward: 0.538 [0.499, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.554, 10.100], loss: 0.007788, mae: 0.092411, mean_q: 0.681007
 74699/100000: episode: 4125, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 3.700, mean reward: 0.529 [0.415, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.641, 10.100], loss: 0.008234, mae: 0.095374, mean_q: 0.730327
 74707/100000: episode: 4126, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 4.684, mean reward: 0.585 [0.492, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.680, 10.100], loss: 0.006774, mae: 0.089827, mean_q: 0.632679
 74714/100000: episode: 4127, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 3.634, mean reward: 0.519 [0.476, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.572, 10.100], loss: 0.192274, mae: 0.111541, mean_q: 0.633044
 74722/100000: episode: 4128, duration: 0.050s, episode steps: 8, steps per second: 158, episode reward: 4.103, mean reward: 0.513 [0.421, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.489, 10.100], loss: 0.166108, mae: 0.131756, mean_q: 0.713342
 74730/100000: episode: 4129, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 3.858, mean reward: 0.482 [0.361, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.467, 10.100], loss: 0.007129, mae: 0.089267, mean_q: 0.662785
 74738/100000: episode: 4130, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 4.511, mean reward: 0.564 [0.523, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.566, 10.100], loss: 0.164742, mae: 0.114773, mean_q: 0.687563
 74746/100000: episode: 4131, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 4.457, mean reward: 0.557 [0.507, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.557, 10.100], loss: 0.008218, mae: 0.091585, mean_q: 0.740534
 74753/100000: episode: 4132, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 3.714, mean reward: 0.531 [0.499, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.553, 10.100], loss: 0.008832, mae: 0.104330, mean_q: 0.651239
 74760/100000: episode: 4133, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 3.405, mean reward: 0.486 [0.447, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.548, 10.100], loss: 0.006923, mae: 0.087531, mean_q: 0.663517
[RESULT] FALSIFICATION!
 74765/100000: episode: 4134, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 12.300, mean reward: 2.460 [0.529, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.590, 10.082], loss: 0.005337, mae: 0.074149, mean_q: 0.652060
 74772/100000: episode: 4135, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 3.430, mean reward: 0.490 [0.417, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.612, 10.100], loss: 0.194250, mae: 0.134436, mean_q: 0.682936
 74779/100000: episode: 4136, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.222, mean reward: 0.603 [0.529, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.754, 10.100], loss: 0.007335, mae: 0.088577, mean_q: 0.670831
 74787/100000: episode: 4137, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 4.078, mean reward: 0.510 [0.462, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.524, 10.100], loss: 0.006947, mae: 0.090586, mean_q: 0.651080
 74794/100000: episode: 4138, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 3.986, mean reward: 0.569 [0.509, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.577, 10.100], loss: 0.188779, mae: 0.139691, mean_q: 0.735185
 74801/100000: episode: 4139, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.817, mean reward: 0.545 [0.464, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.901, 10.100], loss: 0.007223, mae: 0.088741, mean_q: 0.652777
 74808/100000: episode: 4140, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.528, mean reward: 0.504 [0.469, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.071, 10.100], loss: 0.008157, mae: 0.095063, mean_q: 0.634530
 74816/100000: episode: 4141, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 4.482, mean reward: 0.560 [0.526, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.605, 10.100], loss: 0.007434, mae: 0.086691, mean_q: 0.713809
 74823/100000: episode: 4142, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 3.367, mean reward: 0.481 [0.446, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.507, 10.100], loss: 0.194846, mae: 0.126196, mean_q: 0.714018
 74830/100000: episode: 4143, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.007, mean reward: 0.572 [0.494, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.755, 10.100], loss: 0.373821, mae: 0.180791, mean_q: 0.673902
 74837/100000: episode: 4144, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 2.980, mean reward: 0.426 [0.395, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.659, 10.100], loss: 0.009577, mae: 0.098486, mean_q: 0.727427
 74845/100000: episode: 4145, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 3.806, mean reward: 0.476 [0.409, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.692, 10.100], loss: 0.171430, mae: 0.123840, mean_q: 0.641714
 74852/100000: episode: 4146, duration: 0.039s, episode steps: 7, steps per second: 182, episode reward: 3.419, mean reward: 0.488 [0.421, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.804, 10.100], loss: 0.009426, mae: 0.104848, mean_q: 0.718639
 74860/100000: episode: 4147, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 4.201, mean reward: 0.525 [0.466, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.798, 10.100], loss: 0.166638, mae: 0.113091, mean_q: 0.638040
 74867/100000: episode: 4148, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 3.947, mean reward: 0.564 [0.469, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.699, 10.100], loss: 0.187726, mae: 0.140399, mean_q: 0.698404
 74875/100000: episode: 4149, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 3.861, mean reward: 0.483 [0.441, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.541, 10.100], loss: 0.007704, mae: 0.090158, mean_q: 0.721586
 74882/100000: episode: 4150, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 3.462, mean reward: 0.495 [0.467, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.649, 10.100], loss: 0.007860, mae: 0.097126, mean_q: 0.666208
 74889/100000: episode: 4151, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 3.077, mean reward: 0.440 [0.405, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.639, 10.100], loss: 0.190812, mae: 0.147748, mean_q: 0.756514
 74896/100000: episode: 4152, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 3.672, mean reward: 0.525 [0.471, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.504, 10.100], loss: 0.004034, mae: 0.066554, mean_q: 0.641474
 74903/100000: episode: 4153, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 4.213, mean reward: 0.602 [0.538, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.827, 10.100], loss: 0.186566, mae: 0.133182, mean_q: 0.698237
 74911/100000: episode: 4154, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.675, mean reward: 0.584 [0.553, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.510, 10.100], loss: 0.013083, mae: 0.123776, mean_q: 0.745462
[RESULT] FALSIFICATION!
 74919/100000: episode: 4155, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 13.728, mean reward: 1.716 [0.439, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.606, 10.100], loss: 0.176089, mae: 0.139821, mean_q: 0.627783
 74926/100000: episode: 4156, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.545, mean reward: 0.506 [0.458, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.479, 10.100], loss: 0.008364, mae: 0.094929, mean_q: 0.772707
[RESULT] FALSIFICATION!
 74929/100000: episode: 4157, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 11.155, mean reward: 3.718 [0.555, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.436, 10.046], loss: 0.008084, mae: 0.085663, mean_q: 0.718792
 74936/100000: episode: 4158, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 3.340, mean reward: 0.477 [0.413, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.464, 10.100], loss: 0.189593, mae: 0.133550, mean_q: 0.657839
 74943/100000: episode: 4159, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 3.743, mean reward: 0.535 [0.454, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.693, 10.100], loss: 0.189116, mae: 0.162514, mean_q: 0.795686
 74950/100000: episode: 4160, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 4.250, mean reward: 0.607 [0.496, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.767, 10.100], loss: 0.008541, mae: 0.102148, mean_q: 0.675014
 74957/100000: episode: 4161, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.466, mean reward: 0.495 [0.452, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.637, 10.100], loss: 0.007284, mae: 0.091295, mean_q: 0.655780
 74964/100000: episode: 4162, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 3.975, mean reward: 0.568 [0.507, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.621, 10.100], loss: 0.190815, mae: 0.162793, mean_q: 0.792196
[RESULT] FALSIFICATION!
 74966/100000: episode: 4163, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 10.678, mean reward: 5.339 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.489, 10.020], loss: 0.005182, mae: 0.072590, mean_q: 0.658612
 74973/100000: episode: 4164, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.646, mean reward: 0.521 [0.426, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.580, 10.100], loss: 0.199071, mae: 0.125361, mean_q: 0.648064
 74980/100000: episode: 4165, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 3.693, mean reward: 0.528 [0.485, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.578, 10.100], loss: 0.195502, mae: 0.132464, mean_q: 0.712774
 74987/100000: episode: 4166, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 3.968, mean reward: 0.567 [0.522, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.636, 10.100], loss: 0.008850, mae: 0.096644, mean_q: 0.748532
 74994/100000: episode: 4167, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 4.261, mean reward: 0.609 [0.553, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.657, 10.100], loss: 0.364907, mae: 0.166767, mean_q: 0.728314
 75002/100000: episode: 4168, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.807, mean reward: 0.601 [0.560, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.768, 10.100], loss: 0.013656, mae: 0.118489, mean_q: 0.742963
 75009/100000: episode: 4169, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 4.037, mean reward: 0.577 [0.511, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.648, 10.100], loss: 0.364786, mae: 0.178955, mean_q: 0.749620
 75017/100000: episode: 4170, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.395, mean reward: 0.549 [0.500, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.611, 10.100], loss: 0.326523, mae: 0.192734, mean_q: 0.807664
 75024/100000: episode: 4171, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 3.861, mean reward: 0.552 [0.504, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.536, 10.100], loss: 0.549037, mae: 0.230955, mean_q: 0.814332
 75032/100000: episode: 4172, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 4.081, mean reward: 0.510 [0.439, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.530, 10.100], loss: 0.009328, mae: 0.096826, mean_q: 0.780630
 75039/100000: episode: 4173, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 3.745, mean reward: 0.535 [0.460, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.699, 10.100], loss: 0.007910, mae: 0.099656, mean_q: 0.688572
 75047/100000: episode: 4174, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.202, mean reward: 0.525 [0.472, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.871, 10.100], loss: 0.006158, mae: 0.081940, mean_q: 0.751334
 75055/100000: episode: 4175, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 4.088, mean reward: 0.511 [0.456, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.527, 10.100], loss: 0.170613, mae: 0.134782, mean_q: 0.736758
 75062/100000: episode: 4176, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 3.527, mean reward: 0.504 [0.470, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.551, 10.100], loss: 0.009105, mae: 0.092876, mean_q: 0.717922
 75070/100000: episode: 4177, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 4.279, mean reward: 0.535 [0.469, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.636, 10.100], loss: 0.007426, mae: 0.093216, mean_q: 0.660886
[RESULT] FALSIFICATION!
 75073/100000: episode: 4178, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 11.143, mean reward: 3.714 [0.540, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.531, 10.067], loss: 0.005567, mae: 0.078959, mean_q: 0.711610
 75080/100000: episode: 4179, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 3.910, mean reward: 0.559 [0.500, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.622, 10.100], loss: 0.007130, mae: 0.090382, mean_q: 0.714231
 75088/100000: episode: 4180, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 4.901, mean reward: 0.613 [0.587, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.555, 10.100], loss: 0.163708, mae: 0.122525, mean_q: 0.670305
 75095/100000: episode: 4181, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 3.697, mean reward: 0.528 [0.450, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.602, 10.100], loss: 0.552673, mae: 0.248818, mean_q: 0.796234
 75102/100000: episode: 4182, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 3.527, mean reward: 0.504 [0.447, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.664, 10.100], loss: 0.013993, mae: 0.123888, mean_q: 0.773318
 75109/100000: episode: 4183, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 3.834, mean reward: 0.548 [0.459, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.977, 10.100], loss: 0.008807, mae: 0.103859, mean_q: 0.628307
 75116/100000: episode: 4184, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 3.841, mean reward: 0.549 [0.488, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.725, 10.100], loss: 0.189069, mae: 0.149695, mean_q: 0.774902
 75124/100000: episode: 4185, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 3.960, mean reward: 0.495 [0.469, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.995, 10.100], loss: 0.166220, mae: 0.132747, mean_q: 0.752982
 75131/100000: episode: 4186, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 3.912, mean reward: 0.559 [0.507, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.566, 10.100], loss: 0.185112, mae: 0.142786, mean_q: 0.706486
 75138/100000: episode: 4187, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 3.545, mean reward: 0.506 [0.466, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.572, 10.100], loss: 0.190223, mae: 0.154721, mean_q: 0.818990
 75146/100000: episode: 4188, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 4.084, mean reward: 0.511 [0.470, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.685, 10.100], loss: 0.009745, mae: 0.101173, mean_q: 0.748271
 75153/100000: episode: 4189, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 3.872, mean reward: 0.553 [0.518, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.997, 10.100], loss: 0.008643, mae: 0.104818, mean_q: 0.679748
 75160/100000: episode: 4190, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 3.401, mean reward: 0.486 [0.437, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.612, 10.100], loss: 0.192240, mae: 0.134166, mean_q: 0.738771
 75167/100000: episode: 4191, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 3.754, mean reward: 0.536 [0.444, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.633, 10.100], loss: 0.357339, mae: 0.190624, mean_q: 0.816979
 75174/100000: episode: 4192, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.069, mean reward: 0.581 [0.550, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.916, 10.100], loss: 0.012171, mae: 0.117431, mean_q: 0.758031
 75181/100000: episode: 4193, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 4.119, mean reward: 0.588 [0.543, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.536, 10.100], loss: 0.194697, mae: 0.132802, mean_q: 0.734914
 75188/100000: episode: 4194, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.025, mean reward: 0.575 [0.468, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.668, 10.100], loss: 0.009141, mae: 0.094530, mean_q: 0.788321
 75195/100000: episode: 4195, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 3.839, mean reward: 0.548 [0.464, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.048, 10.100], loss: 0.196805, mae: 0.143559, mean_q: 0.669100
 75202/100000: episode: 4196, duration: 0.043s, episode steps: 7, steps per second: 165, episode reward: 3.511, mean reward: 0.502 [0.437, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.601, 10.100], loss: 0.008407, mae: 0.087411, mean_q: 0.802651
 75209/100000: episode: 4197, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 3.444, mean reward: 0.492 [0.452, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.682, 10.100], loss: 0.194503, mae: 0.135642, mean_q: 0.766637
 75216/100000: episode: 4198, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 3.643, mean reward: 0.520 [0.472, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.617, 10.100], loss: 0.007732, mae: 0.094430, mean_q: 0.719234
 75223/100000: episode: 4199, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 3.968, mean reward: 0.567 [0.492, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.707, 10.100], loss: 0.006723, mae: 0.086127, mean_q: 0.717410
 75230/100000: episode: 4200, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 3.620, mean reward: 0.517 [0.466, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.553, 10.100], loss: 0.366014, mae: 0.184980, mean_q: 0.740534
 75237/100000: episode: 4201, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 3.607, mean reward: 0.515 [0.440, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.407, 10.100], loss: 0.016824, mae: 0.139672, mean_q: 0.851472
 75244/100000: episode: 4202, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 3.839, mean reward: 0.548 [0.455, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.797, 10.100], loss: 0.009781, mae: 0.112775, mean_q: 0.673436
 75252/100000: episode: 4203, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 4.469, mean reward: 0.559 [0.503, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.615, 10.100], loss: 0.334486, mae: 0.182294, mean_q: 0.778020
 75260/100000: episode: 4204, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 4.271, mean reward: 0.534 [0.458, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.566, 10.100], loss: 0.332044, mae: 0.206245, mean_q: 0.852815
 75267/100000: episode: 4205, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 3.856, mean reward: 0.551 [0.449, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.637, 10.100], loss: 0.186137, mae: 0.147069, mean_q: 0.721161
 75274/100000: episode: 4206, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 3.723, mean reward: 0.532 [0.464, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.585, 10.100], loss: 0.184524, mae: 0.140563, mean_q: 0.784174
[Info] Not found new level, current best level reached = 1.0943608283996582
 75281/100000: episode: 4207, duration: 4.007s, episode steps: 7, steps per second: 2, episode reward: 3.786, mean reward: 0.541 [0.457, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.479, 10.100], loss: 0.009864, mae: 0.103252, mean_q: 0.744707
 75381/100000: episode: 4208, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 13.949, mean reward: 0.139 [0.014, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.861, 10.169], loss: 0.162736, mae: 0.136543, mean_q: 0.757010
 75481/100000: episode: 4209, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.552, mean reward: 0.156 [0.009, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.409, 10.122], loss: 0.174976, mae: 0.146552, mean_q: 0.769393
 75581/100000: episode: 4210, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 22.952, mean reward: 0.230 [0.037, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.734, 10.098], loss: 0.124891, mae: 0.119134, mean_q: 0.748507
 75681/100000: episode: 4211, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.235, mean reward: 0.162 [0.018, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.894, 10.152], loss: 0.200793, mae: 0.143659, mean_q: 0.759568
 75781/100000: episode: 4212, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 15.013, mean reward: 0.150 [0.015, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.467, 10.098], loss: 0.122547, mae: 0.124215, mean_q: 0.747174
 75881/100000: episode: 4213, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 16.025, mean reward: 0.160 [0.024, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.952, 10.098], loss: 0.149907, mae: 0.134355, mean_q: 0.746765
 75981/100000: episode: 4214, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 13.609, mean reward: 0.136 [0.010, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.649, 10.109], loss: 0.187044, mae: 0.146757, mean_q: 0.769127
 76081/100000: episode: 4215, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 20.008, mean reward: 0.200 [0.053, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.296, 10.098], loss: 0.136389, mae: 0.127739, mean_q: 0.747544
 76181/100000: episode: 4216, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 16.686, mean reward: 0.167 [0.028, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.081, 10.214], loss: 0.172447, mae: 0.141319, mean_q: 0.760054
 76281/100000: episode: 4217, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 15.445, mean reward: 0.154 [0.021, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.242, 10.098], loss: 0.159385, mae: 0.131982, mean_q: 0.736485
 76381/100000: episode: 4218, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 16.774, mean reward: 0.168 [0.007, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.651, 10.098], loss: 0.176154, mae: 0.160057, mean_q: 0.750664
 76481/100000: episode: 4219, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 14.179, mean reward: 0.142 [0.005, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.181], loss: 0.124177, mae: 0.128983, mean_q: 0.738251
 76581/100000: episode: 4220, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 19.463, mean reward: 0.195 [0.018, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.900, 10.261], loss: 0.148873, mae: 0.130424, mean_q: 0.731372
 76681/100000: episode: 4221, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.328, mean reward: 0.143 [0.007, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.649, 10.098], loss: 0.124665, mae: 0.127541, mean_q: 0.733849
 76781/100000: episode: 4222, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 14.941, mean reward: 0.149 [0.009, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.098], loss: 0.111238, mae: 0.120427, mean_q: 0.727648
 76881/100000: episode: 4223, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 13.161, mean reward: 0.132 [0.009, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.112], loss: 0.172411, mae: 0.137546, mean_q: 0.743411
 76981/100000: episode: 4224, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 13.765, mean reward: 0.138 [0.004, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.229], loss: 0.123639, mae: 0.125429, mean_q: 0.727586
 77081/100000: episode: 4225, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 19.265, mean reward: 0.193 [0.014, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.906, 10.098], loss: 0.161349, mae: 0.144268, mean_q: 0.734411
 77181/100000: episode: 4226, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 16.857, mean reward: 0.169 [0.011, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.990, 10.098], loss: 0.110942, mae: 0.115771, mean_q: 0.711705
 77281/100000: episode: 4227, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 17.517, mean reward: 0.175 [0.043, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.502, 10.243], loss: 0.134318, mae: 0.125743, mean_q: 0.696349
 77381/100000: episode: 4228, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 15.987, mean reward: 0.160 [0.008, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.471, 10.098], loss: 0.108592, mae: 0.117269, mean_q: 0.686213
 77481/100000: episode: 4229, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 17.162, mean reward: 0.172 [0.021, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.269, 10.098], loss: 0.161349, mae: 0.136737, mean_q: 0.684740
 77581/100000: episode: 4230, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 22.029, mean reward: 0.220 [0.011, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.328], loss: 0.122667, mae: 0.120948, mean_q: 0.660379
 77681/100000: episode: 4231, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 16.084, mean reward: 0.161 [0.015, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.585, 10.129], loss: 0.135327, mae: 0.122801, mean_q: 0.656805
 77781/100000: episode: 4232, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 18.056, mean reward: 0.181 [0.016, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.893, 10.248], loss: 0.107023, mae: 0.123689, mean_q: 0.645806
 77881/100000: episode: 4233, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 16.523, mean reward: 0.165 [0.031, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.264, 10.098], loss: 0.122453, mae: 0.128453, mean_q: 0.646552
 77981/100000: episode: 4234, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 17.521, mean reward: 0.175 [0.012, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.287, 10.098], loss: 0.108615, mae: 0.111272, mean_q: 0.606793
 78081/100000: episode: 4235, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.746, mean reward: 0.157 [0.013, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.703, 10.098], loss: 0.083051, mae: 0.107777, mean_q: 0.601505
 78181/100000: episode: 4236, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.439, mean reward: 0.164 [0.006, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.925, 10.098], loss: 0.083944, mae: 0.114752, mean_q: 0.583212
 78281/100000: episode: 4237, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.658, mean reward: 0.147 [0.014, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.902, 10.176], loss: 0.119323, mae: 0.114699, mean_q: 0.582515
 78381/100000: episode: 4238, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 17.991, mean reward: 0.180 [0.033, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.481, 10.098], loss: 0.156497, mae: 0.132949, mean_q: 0.585402
 78481/100000: episode: 4239, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 16.774, mean reward: 0.168 [0.030, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.343, 10.098], loss: 0.070745, mae: 0.102744, mean_q: 0.550716
 78581/100000: episode: 4240, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 17.096, mean reward: 0.171 [0.023, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.122, 10.098], loss: 0.032031, mae: 0.093618, mean_q: 0.519402
 78681/100000: episode: 4241, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 18.809, mean reward: 0.188 [0.011, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.098], loss: 0.095390, mae: 0.112265, mean_q: 0.529287
 78781/100000: episode: 4242, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 19.142, mean reward: 0.191 [0.027, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.412, 10.102], loss: 0.057511, mae: 0.099575, mean_q: 0.505426
 78881/100000: episode: 4243, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 16.078, mean reward: 0.161 [0.021, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.185], loss: 0.018839, mae: 0.085334, mean_q: 0.486704
 78981/100000: episode: 4244, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 13.088, mean reward: 0.131 [0.008, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.098], loss: 0.082151, mae: 0.102015, mean_q: 0.483014
 79081/100000: episode: 4245, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 16.364, mean reward: 0.164 [0.009, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.647, 10.098], loss: 0.068596, mae: 0.096556, mean_q: 0.476062
 79181/100000: episode: 4246, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.596, mean reward: 0.156 [0.015, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.499, 10.103], loss: 0.042715, mae: 0.089053, mean_q: 0.462279
 79281/100000: episode: 4247, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 16.051, mean reward: 0.161 [0.013, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.363, 10.226], loss: 0.077886, mae: 0.098604, mean_q: 0.446030
 79381/100000: episode: 4248, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.597, mean reward: 0.146 [0.013, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.098], loss: 0.059275, mae: 0.104957, mean_q: 0.442830
 79481/100000: episode: 4249, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 14.073, mean reward: 0.141 [0.024, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.379, 10.115], loss: 0.067766, mae: 0.094072, mean_q: 0.424871
 79581/100000: episode: 4250, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 17.044, mean reward: 0.170 [0.016, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.471, 10.098], loss: 0.030592, mae: 0.088420, mean_q: 0.422427
 79681/100000: episode: 4251, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 15.715, mean reward: 0.157 [0.019, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.188, 10.098], loss: 0.004781, mae: 0.075163, mean_q: 0.400996
 79781/100000: episode: 4252, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 21.477, mean reward: 0.215 [0.019, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.072, 10.347], loss: 0.017045, mae: 0.076752, mean_q: 0.384928
 79881/100000: episode: 4253, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 12.613, mean reward: 0.126 [0.009, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.100], loss: 0.042406, mae: 0.085955, mean_q: 0.392245
 79981/100000: episode: 4254, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 22.231, mean reward: 0.222 [0.009, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.135, 10.111], loss: 0.004330, mae: 0.071966, mean_q: 0.360182
 80081/100000: episode: 4255, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 21.100, mean reward: 0.211 [0.046, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.170, 10.098], loss: 0.003960, mae: 0.069394, mean_q: 0.351048
 80181/100000: episode: 4256, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 20.594, mean reward: 0.206 [0.028, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.549, 10.098], loss: 0.003883, mae: 0.069558, mean_q: 0.343274
 80281/100000: episode: 4257, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 15.698, mean reward: 0.157 [0.014, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.098], loss: 0.003885, mae: 0.069483, mean_q: 0.332094
 80381/100000: episode: 4258, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 13.623, mean reward: 0.136 [0.002, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.368, 10.098], loss: 0.004059, mae: 0.071066, mean_q: 0.329253
 80481/100000: episode: 4259, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 14.081, mean reward: 0.141 [0.016, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.946, 10.098], loss: 0.004154, mae: 0.071416, mean_q: 0.332178
 80581/100000: episode: 4260, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 16.029, mean reward: 0.160 [0.012, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.539, 10.107], loss: 0.003741, mae: 0.067606, mean_q: 0.331067
 80681/100000: episode: 4261, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 15.079, mean reward: 0.151 [0.026, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.820, 10.098], loss: 0.003894, mae: 0.069136, mean_q: 0.330636
 80781/100000: episode: 4262, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 18.587, mean reward: 0.186 [0.024, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.276, 10.158], loss: 0.003590, mae: 0.066938, mean_q: 0.328692
 80881/100000: episode: 4263, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 15.675, mean reward: 0.157 [0.028, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.098], loss: 0.003845, mae: 0.068399, mean_q: 0.328831
 80981/100000: episode: 4264, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 15.850, mean reward: 0.159 [0.026, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.407, 10.098], loss: 0.003607, mae: 0.067586, mean_q: 0.323725
 81081/100000: episode: 4265, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 17.059, mean reward: 0.171 [0.010, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.192, 10.098], loss: 0.003766, mae: 0.068643, mean_q: 0.327383
 81181/100000: episode: 4266, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 20.067, mean reward: 0.201 [0.021, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.681, 10.338], loss: 0.003818, mae: 0.068601, mean_q: 0.329229
 81281/100000: episode: 4267, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 19.361, mean reward: 0.194 [0.020, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.726, 10.261], loss: 0.003625, mae: 0.067483, mean_q: 0.330953
 81381/100000: episode: 4268, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 16.388, mean reward: 0.164 [0.005, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.318, 10.252], loss: 0.003569, mae: 0.066876, mean_q: 0.332657
 81481/100000: episode: 4269, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 11.306, mean reward: 0.113 [0.007, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.140], loss: 0.003672, mae: 0.068017, mean_q: 0.333645
 81581/100000: episode: 4270, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 14.993, mean reward: 0.150 [0.017, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.406, 10.098], loss: 0.003589, mae: 0.066638, mean_q: 0.331511
 81681/100000: episode: 4271, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 14.451, mean reward: 0.145 [0.002, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.427, 10.098], loss: 0.003682, mae: 0.067858, mean_q: 0.335422
 81781/100000: episode: 4272, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 17.315, mean reward: 0.173 [0.023, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.366, 10.098], loss: 0.003667, mae: 0.068295, mean_q: 0.328382
 81881/100000: episode: 4273, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 17.699, mean reward: 0.177 [0.013, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.212], loss: 0.003715, mae: 0.068130, mean_q: 0.334252
 81981/100000: episode: 4274, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 15.577, mean reward: 0.156 [0.031, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.604, 10.103], loss: 0.003603, mae: 0.066529, mean_q: 0.333380
 82081/100000: episode: 4275, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 16.887, mean reward: 0.169 [0.021, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.763, 10.098], loss: 0.003677, mae: 0.067099, mean_q: 0.330951
 82181/100000: episode: 4276, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 20.455, mean reward: 0.205 [0.032, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.544, 10.098], loss: 0.003640, mae: 0.067311, mean_q: 0.332691
 82281/100000: episode: 4277, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 16.901, mean reward: 0.169 [0.003, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.894, 10.098], loss: 0.003762, mae: 0.068333, mean_q: 0.330943
 82381/100000: episode: 4278, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 19.589, mean reward: 0.196 [0.025, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.781, 10.397], loss: 0.003882, mae: 0.069899, mean_q: 0.335044
 82481/100000: episode: 4279, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 16.607, mean reward: 0.166 [0.010, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.955, 10.322], loss: 0.003979, mae: 0.069972, mean_q: 0.332564
 82581/100000: episode: 4280, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 16.288, mean reward: 0.163 [0.030, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.644, 10.098], loss: 0.003817, mae: 0.068695, mean_q: 0.330170
 82681/100000: episode: 4281, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 19.196, mean reward: 0.192 [0.008, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.788, 10.316], loss: 0.003684, mae: 0.067031, mean_q: 0.330047
 82781/100000: episode: 4282, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 16.691, mean reward: 0.167 [0.010, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.158, 10.098], loss: 0.003669, mae: 0.067229, mean_q: 0.330795
 82881/100000: episode: 4283, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 18.484, mean reward: 0.185 [0.034, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.642, 10.098], loss: 0.003877, mae: 0.068887, mean_q: 0.329563
 82981/100000: episode: 4284, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.485, mean reward: 0.175 [0.029, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.098], loss: 0.003942, mae: 0.069537, mean_q: 0.332524
 83081/100000: episode: 4285, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 19.580, mean reward: 0.196 [0.018, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.129, 10.280], loss: 0.003718, mae: 0.067518, mean_q: 0.333831
 83181/100000: episode: 4286, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 17.782, mean reward: 0.178 [0.020, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.098], loss: 0.003607, mae: 0.066628, mean_q: 0.336008
 83281/100000: episode: 4287, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 15.164, mean reward: 0.152 [0.010, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.098], loss: 0.003559, mae: 0.065871, mean_q: 0.333975
 83381/100000: episode: 4288, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 22.589, mean reward: 0.226 [0.044, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.591, 10.098], loss: 0.003677, mae: 0.066431, mean_q: 0.334082
 83481/100000: episode: 4289, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 16.761, mean reward: 0.168 [0.020, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.793, 10.106], loss: 0.003779, mae: 0.068097, mean_q: 0.336930
 83581/100000: episode: 4290, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 20.145, mean reward: 0.201 [0.050, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.747, 10.098], loss: 0.003861, mae: 0.068863, mean_q: 0.336378
 83681/100000: episode: 4291, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 16.982, mean reward: 0.170 [0.014, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.935, 10.327], loss: 0.003620, mae: 0.067382, mean_q: 0.336694
 83781/100000: episode: 4292, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 18.607, mean reward: 0.186 [0.035, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.098], loss: 0.003871, mae: 0.069253, mean_q: 0.335729
 83881/100000: episode: 4293, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 20.103, mean reward: 0.201 [0.011, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.803, 10.214], loss: 0.003902, mae: 0.069476, mean_q: 0.337373
 83981/100000: episode: 4294, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 20.665, mean reward: 0.207 [0.031, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.043, 10.098], loss: 0.003560, mae: 0.067098, mean_q: 0.342380
 84081/100000: episode: 4295, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 20.679, mean reward: 0.207 [0.030, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.516, 10.303], loss: 0.003731, mae: 0.068486, mean_q: 0.342438
 84181/100000: episode: 4296, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 16.833, mean reward: 0.168 [0.024, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.532, 10.098], loss: 0.003923, mae: 0.069174, mean_q: 0.343220
 84281/100000: episode: 4297, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 13.858, mean reward: 0.139 [0.011, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.176], loss: 0.003540, mae: 0.065697, mean_q: 0.344643
 84381/100000: episode: 4298, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 20.046, mean reward: 0.200 [0.010, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.575, 10.410], loss: 0.003445, mae: 0.064806, mean_q: 0.344377
 84481/100000: episode: 4299, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 19.812, mean reward: 0.198 [0.018, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.105], loss: 0.003463, mae: 0.065600, mean_q: 0.347104
 84581/100000: episode: 4300, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 18.379, mean reward: 0.184 [0.028, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.216, 10.098], loss: 0.003555, mae: 0.066297, mean_q: 0.345888
 84681/100000: episode: 4301, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 13.735, mean reward: 0.137 [0.018, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.134, 10.213], loss: 0.003499, mae: 0.066338, mean_q: 0.349193
 84781/100000: episode: 4302, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 13.956, mean reward: 0.140 [0.019, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.510, 10.148], loss: 0.003506, mae: 0.066141, mean_q: 0.346265
 84881/100000: episode: 4303, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 15.696, mean reward: 0.157 [0.038, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.585, 10.098], loss: 0.003597, mae: 0.066592, mean_q: 0.343665
 84981/100000: episode: 4304, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.147, mean reward: 0.141 [0.012, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.497, 10.308], loss: 0.003496, mae: 0.065006, mean_q: 0.342383
 85081/100000: episode: 4305, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 17.136, mean reward: 0.171 [0.030, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.267, 10.159], loss: 0.003648, mae: 0.067120, mean_q: 0.340534
 85181/100000: episode: 4306, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 18.047, mean reward: 0.180 [0.014, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.383, 10.098], loss: 0.003565, mae: 0.066322, mean_q: 0.338542
[Info] New level: 0.5994099378585815 | Considering 10/90 traces
 85281/100000: episode: 4307, duration: 4.483s, episode steps: 100, steps per second: 22, episode reward: 12.697, mean reward: 0.127 [0.003, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.368, 10.221], loss: 0.003506, mae: 0.065663, mean_q: 0.337575
 85318/100000: episode: 4308, duration: 0.218s, episode steps: 37, steps per second: 170, episode reward: 16.351, mean reward: 0.442 [0.309, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.606, 10.405], loss: 0.003453, mae: 0.065191, mean_q: 0.343126
 85355/100000: episode: 4309, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 13.271, mean reward: 0.359 [0.190, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.908, 10.409], loss: 0.003260, mae: 0.063196, mean_q: 0.339862
 85373/100000: episode: 4310, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 5.922, mean reward: 0.329 [0.246, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.284, 10.100], loss: 0.003125, mae: 0.062461, mean_q: 0.355222
 85420/100000: episode: 4311, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 8.036, mean reward: 0.171 [0.021, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.574, 10.152], loss: 0.003647, mae: 0.066723, mean_q: 0.346988
 85457/100000: episode: 4312, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 12.485, mean reward: 0.337 [0.231, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.428, 10.433], loss: 0.003683, mae: 0.068388, mean_q: 0.346180
 85520/100000: episode: 4313, duration: 0.318s, episode steps: 63, steps per second: 198, episode reward: 15.127, mean reward: 0.240 [0.140, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-1.026, 10.100], loss: 0.003741, mae: 0.068045, mean_q: 0.351294
 85583/100000: episode: 4314, duration: 0.337s, episode steps: 63, steps per second: 187, episode reward: 10.846, mean reward: 0.172 [0.023, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.803 [-0.469, 10.139], loss: 0.003756, mae: 0.067330, mean_q: 0.356920
 85646/100000: episode: 4315, duration: 0.343s, episode steps: 63, steps per second: 184, episode reward: 14.333, mean reward: 0.228 [0.034, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.630, 10.100], loss: 0.003821, mae: 0.069907, mean_q: 0.361689
 85693/100000: episode: 4316, duration: 0.273s, episode steps: 47, steps per second: 172, episode reward: 10.651, mean reward: 0.227 [0.098, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.299, 10.379], loss: 0.003583, mae: 0.066497, mean_q: 0.353414
 85711/100000: episode: 4317, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 5.817, mean reward: 0.323 [0.243, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.325, 10.100], loss: 0.003623, mae: 0.066330, mean_q: 0.367621
 85722/100000: episode: 4318, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 2.871, mean reward: 0.261 [0.175, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.141, 10.100], loss: 0.003277, mae: 0.064961, mean_q: 0.364811
 85733/100000: episode: 4319, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 3.011, mean reward: 0.274 [0.218, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.226, 10.100], loss: 0.003708, mae: 0.069290, mean_q: 0.357187
 85743/100000: episode: 4320, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 3.761, mean reward: 0.376 [0.307, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.608, 10.100], loss: 0.003238, mae: 0.062073, mean_q: 0.346659
 85761/100000: episode: 4321, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 5.359, mean reward: 0.298 [0.192, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.497, 10.100], loss: 0.003005, mae: 0.059983, mean_q: 0.369280
 85808/100000: episode: 4322, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 10.619, mean reward: 0.226 [0.094, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.334, 10.199], loss: 0.003286, mae: 0.064161, mean_q: 0.365018
 85819/100000: episode: 4323, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 3.867, mean reward: 0.352 [0.249, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.347, 10.100], loss: 0.004161, mae: 0.070911, mean_q: 0.362402
 85876/100000: episode: 4324, duration: 0.285s, episode steps: 57, steps per second: 200, episode reward: 11.747, mean reward: 0.206 [0.034, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.811, 10.100], loss: 0.003484, mae: 0.065240, mean_q: 0.362216
 85933/100000: episode: 4325, duration: 0.292s, episode steps: 57, steps per second: 195, episode reward: 13.213, mean reward: 0.232 [0.058, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.606, 10.100], loss: 0.003638, mae: 0.067560, mean_q: 0.363767
 85996/100000: episode: 4326, duration: 0.328s, episode steps: 63, steps per second: 192, episode reward: 15.920, mean reward: 0.253 [0.070, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.790 [-0.931, 10.100], loss: 0.003727, mae: 0.067603, mean_q: 0.365785
 86043/100000: episode: 4327, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 9.749, mean reward: 0.207 [0.035, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.606, 10.100], loss: 0.003553, mae: 0.065206, mean_q: 0.363340
 86061/100000: episode: 4328, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 7.387, mean reward: 0.410 [0.323, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.416, 10.100], loss: 0.003629, mae: 0.066937, mean_q: 0.374380
 86098/100000: episode: 4329, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 10.005, mean reward: 0.270 [0.078, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.088, 10.350], loss: 0.003653, mae: 0.066576, mean_q: 0.369309
 86152/100000: episode: 4330, duration: 0.267s, episode steps: 54, steps per second: 202, episode reward: 12.459, mean reward: 0.231 [0.019, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.498, 10.100], loss: 0.003541, mae: 0.066890, mean_q: 0.371488
 86189/100000: episode: 4331, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 13.700, mean reward: 0.370 [0.242, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.238, 10.401], loss: 0.003747, mae: 0.067798, mean_q: 0.369465
 86248/100000: episode: 4332, duration: 0.300s, episode steps: 59, steps per second: 197, episode reward: 10.545, mean reward: 0.179 [0.039, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-1.145, 10.159], loss: 0.003354, mae: 0.065078, mean_q: 0.368101
 86311/100000: episode: 4333, duration: 0.313s, episode steps: 63, steps per second: 202, episode reward: 14.951, mean reward: 0.237 [0.052, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.781 [-1.130, 10.100], loss: 0.003194, mae: 0.061936, mean_q: 0.372717
 86348/100000: episode: 4334, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 11.657, mean reward: 0.315 [0.089, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.276, 10.256], loss: 0.003824, mae: 0.067689, mean_q: 0.375921
 86366/100000: episode: 4335, duration: 0.110s, episode steps: 18, steps per second: 163, episode reward: 8.041, mean reward: 0.447 [0.291, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.471, 10.100], loss: 0.003436, mae: 0.063554, mean_q: 0.381631
 86376/100000: episode: 4336, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 2.524, mean reward: 0.252 [0.210, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.004, 10.100], loss: 0.003196, mae: 0.063224, mean_q: 0.369601
 86439/100000: episode: 4337, duration: 0.319s, episode steps: 63, steps per second: 198, episode reward: 15.986, mean reward: 0.254 [0.038, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.778 [-0.973, 10.100], loss: 0.003827, mae: 0.068557, mean_q: 0.383970
 86450/100000: episode: 4338, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 3.676, mean reward: 0.334 [0.296, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.361, 10.100], loss: 0.003407, mae: 0.066996, mean_q: 0.378782
 86507/100000: episode: 4339, duration: 0.308s, episode steps: 57, steps per second: 185, episode reward: 9.348, mean reward: 0.164 [0.029, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.484, 10.308], loss: 0.003230, mae: 0.063425, mean_q: 0.382316
 86570/100000: episode: 4340, duration: 0.323s, episode steps: 63, steps per second: 195, episode reward: 10.294, mean reward: 0.163 [0.034, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.736, 10.100], loss: 0.003659, mae: 0.066661, mean_q: 0.389097
 86581/100000: episode: 4341, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 2.175, mean reward: 0.198 [0.139, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.512, 10.100], loss: 0.003025, mae: 0.061248, mean_q: 0.366238
 86618/100000: episode: 4342, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 12.822, mean reward: 0.347 [0.185, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.847, 10.312], loss: 0.003300, mae: 0.064055, mean_q: 0.378458
 86675/100000: episode: 4343, duration: 0.287s, episode steps: 57, steps per second: 199, episode reward: 14.548, mean reward: 0.255 [0.032, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.197, 10.124], loss: 0.003411, mae: 0.064594, mean_q: 0.383676
 86734/100000: episode: 4344, duration: 0.306s, episode steps: 59, steps per second: 193, episode reward: 10.299, mean reward: 0.175 [0.016, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.568, 10.127], loss: 0.003545, mae: 0.065128, mean_q: 0.383056
 86745/100000: episode: 4345, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 4.415, mean reward: 0.401 [0.330, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.885, 10.100], loss: 0.003699, mae: 0.069312, mean_q: 0.379008
 86782/100000: episode: 4346, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 15.107, mean reward: 0.408 [0.276, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.686, 10.478], loss: 0.003109, mae: 0.062209, mean_q: 0.386091
 86793/100000: episode: 4347, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 2.354, mean reward: 0.214 [0.133, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.295, 10.100], loss: 0.003647, mae: 0.066902, mean_q: 0.383380
 86856/100000: episode: 4348, duration: 0.344s, episode steps: 63, steps per second: 183, episode reward: 14.275, mean reward: 0.227 [0.042, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.809 [-0.187, 10.249], loss: 0.003409, mae: 0.064905, mean_q: 0.391151
 86915/100000: episode: 4349, duration: 0.303s, episode steps: 59, steps per second: 195, episode reward: 11.284, mean reward: 0.191 [0.013, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.370, 10.100], loss: 0.003676, mae: 0.067246, mean_q: 0.395367
 86972/100000: episode: 4350, duration: 0.304s, episode steps: 57, steps per second: 187, episode reward: 19.510, mean reward: 0.342 [0.158, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.404, 10.100], loss: 0.003948, mae: 0.070759, mean_q: 0.398317
 87031/100000: episode: 4351, duration: 0.290s, episode steps: 59, steps per second: 203, episode reward: 21.887, mean reward: 0.371 [0.147, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.897, 10.100], loss: 0.003444, mae: 0.064795, mean_q: 0.400598
 87068/100000: episode: 4352, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 12.658, mean reward: 0.342 [0.126, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.350, 10.248], loss: 0.003979, mae: 0.069878, mean_q: 0.407812
 87079/100000: episode: 4353, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 3.170, mean reward: 0.288 [0.180, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.630, 10.100], loss: 0.003886, mae: 0.069019, mean_q: 0.386507
 87133/100000: episode: 4354, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 13.927, mean reward: 0.258 [0.084, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.482, 10.196], loss: 0.003508, mae: 0.065613, mean_q: 0.395220
 87170/100000: episode: 4355, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 13.771, mean reward: 0.372 [0.214, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.569, 10.285], loss: 0.003949, mae: 0.069798, mean_q: 0.405403
 87207/100000: episode: 4356, duration: 0.195s, episode steps: 37, steps per second: 189, episode reward: 10.372, mean reward: 0.280 [0.033, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.817, 10.212], loss: 0.003408, mae: 0.064907, mean_q: 0.406278
 87225/100000: episode: 4357, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 6.094, mean reward: 0.339 [0.283, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.291, 10.100], loss: 0.003564, mae: 0.064924, mean_q: 0.422223
 87236/100000: episode: 4358, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 2.627, mean reward: 0.239 [0.147, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.234, 10.100], loss: 0.003259, mae: 0.062499, mean_q: 0.423130
 87293/100000: episode: 4359, duration: 0.283s, episode steps: 57, steps per second: 202, episode reward: 10.474, mean reward: 0.184 [0.007, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.411, 10.174], loss: 0.003651, mae: 0.066848, mean_q: 0.414821
 87304/100000: episode: 4360, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 3.298, mean reward: 0.300 [0.253, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.383, 10.100], loss: 0.003511, mae: 0.062411, mean_q: 0.406220
 87358/100000: episode: 4361, duration: 0.299s, episode steps: 54, steps per second: 181, episode reward: 11.818, mean reward: 0.219 [0.035, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.344, 10.100], loss: 0.003724, mae: 0.067699, mean_q: 0.406561
 87417/100000: episode: 4362, duration: 0.304s, episode steps: 59, steps per second: 194, episode reward: 15.267, mean reward: 0.259 [0.086, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.397, 10.100], loss: 0.003815, mae: 0.069188, mean_q: 0.412434
 87474/100000: episode: 4363, duration: 0.290s, episode steps: 57, steps per second: 197, episode reward: 10.456, mean reward: 0.183 [0.039, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.839 [-0.569, 10.100], loss: 0.003365, mae: 0.064857, mean_q: 0.412080
 87521/100000: episode: 4364, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 14.868, mean reward: 0.316 [0.097, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.463, 10.292], loss: 0.003568, mae: 0.066473, mean_q: 0.416895
 87539/100000: episode: 4365, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 5.547, mean reward: 0.308 [0.217, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.386, 10.100], loss: 0.002932, mae: 0.062264, mean_q: 0.426442
 87557/100000: episode: 4366, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 5.495, mean reward: 0.305 [0.179, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.306, 10.100], loss: 0.003619, mae: 0.067016, mean_q: 0.419186
 87594/100000: episode: 4367, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 12.796, mean reward: 0.346 [0.131, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.035, 10.306], loss: 0.003321, mae: 0.064086, mean_q: 0.422681
 87631/100000: episode: 4368, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 15.555, mean reward: 0.420 [0.309, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.127, 10.479], loss: 0.003224, mae: 0.063157, mean_q: 0.425982
 87678/100000: episode: 4369, duration: 0.254s, episode steps: 47, steps per second: 185, episode reward: 10.785, mean reward: 0.229 [0.045, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.437, 10.282], loss: 0.003251, mae: 0.063708, mean_q: 0.424649
 87688/100000: episode: 4370, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 2.985, mean reward: 0.299 [0.231, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.221, 10.100], loss: 0.004195, mae: 0.069478, mean_q: 0.429058
 87725/100000: episode: 4371, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 13.431, mean reward: 0.363 [0.296, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.213, 10.364], loss: 0.003545, mae: 0.066736, mean_q: 0.428962
 87772/100000: episode: 4372, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 16.606, mean reward: 0.353 [0.240, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.299, 10.354], loss: 0.003802, mae: 0.067495, mean_q: 0.430826
 87835/100000: episode: 4373, duration: 0.320s, episode steps: 63, steps per second: 197, episode reward: 11.974, mean reward: 0.190 [0.025, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.567, 10.337], loss: 0.003790, mae: 0.068655, mean_q: 0.430605
 87872/100000: episode: 4374, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 9.444, mean reward: 0.255 [0.120, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.205, 10.212], loss: 0.003759, mae: 0.068780, mean_q: 0.438833
 87890/100000: episode: 4375, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 7.281, mean reward: 0.404 [0.294, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.438, 10.100], loss: 0.003279, mae: 0.063204, mean_q: 0.431782
 87953/100000: episode: 4376, duration: 0.332s, episode steps: 63, steps per second: 190, episode reward: 14.669, mean reward: 0.233 [0.100, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.786 [-1.202, 10.100], loss: 0.003739, mae: 0.067229, mean_q: 0.427503
 87964/100000: episode: 4377, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 3.800, mean reward: 0.345 [0.299, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.164, 10.100], loss: 0.003127, mae: 0.062838, mean_q: 0.420356
 88021/100000: episode: 4378, duration: 0.293s, episode steps: 57, steps per second: 194, episode reward: 12.476, mean reward: 0.219 [0.027, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.738, 10.342], loss: 0.003292, mae: 0.063408, mean_q: 0.441126
 88078/100000: episode: 4379, duration: 0.308s, episode steps: 57, steps per second: 185, episode reward: 16.642, mean reward: 0.292 [0.158, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.286, 10.100], loss: 0.003329, mae: 0.064363, mean_q: 0.438604
 88135/100000: episode: 4380, duration: 0.291s, episode steps: 57, steps per second: 196, episode reward: 16.947, mean reward: 0.297 [0.171, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-1.393, 10.100], loss: 0.003464, mae: 0.065105, mean_q: 0.433667
 88145/100000: episode: 4381, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 2.573, mean reward: 0.257 [0.201, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.323, 10.100], loss: 0.002970, mae: 0.061188, mean_q: 0.450472
 88156/100000: episode: 4382, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 2.788, mean reward: 0.253 [0.172, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.767, 10.100], loss: 0.002997, mae: 0.059844, mean_q: 0.446103
 88203/100000: episode: 4383, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 14.987, mean reward: 0.319 [0.166, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.323, 10.359], loss: 0.003427, mae: 0.065372, mean_q: 0.448334
 88257/100000: episode: 4384, duration: 0.307s, episode steps: 54, steps per second: 176, episode reward: 16.011, mean reward: 0.297 [0.134, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.369, 10.360], loss: 0.003550, mae: 0.065865, mean_q: 0.440473
 88314/100000: episode: 4385, duration: 0.301s, episode steps: 57, steps per second: 189, episode reward: 10.542, mean reward: 0.185 [0.028, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.540, 10.171], loss: 0.003256, mae: 0.063498, mean_q: 0.445686
 88377/100000: episode: 4386, duration: 0.343s, episode steps: 63, steps per second: 184, episode reward: 9.999, mean reward: 0.159 [0.020, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.795 [-1.689, 10.111], loss: 0.003425, mae: 0.065377, mean_q: 0.453598
 88431/100000: episode: 4387, duration: 0.277s, episode steps: 54, steps per second: 195, episode reward: 13.663, mean reward: 0.253 [0.031, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.683, 10.100], loss: 0.003623, mae: 0.067136, mean_q: 0.451857
 88468/100000: episode: 4388, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 10.281, mean reward: 0.278 [0.139, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.750, 10.358], loss: 0.003336, mae: 0.064649, mean_q: 0.465265
 88478/100000: episode: 4389, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 2.924, mean reward: 0.292 [0.194, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.320, 10.100], loss: 0.003352, mae: 0.063147, mean_q: 0.451627
 88489/100000: episode: 4390, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 2.210, mean reward: 0.201 [0.149, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.499, 10.100], loss: 0.003870, mae: 0.069641, mean_q: 0.426520
 88548/100000: episode: 4391, duration: 0.303s, episode steps: 59, steps per second: 194, episode reward: 11.568, mean reward: 0.196 [0.025, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.267, 10.100], loss: 0.003205, mae: 0.063688, mean_q: 0.451488
 88607/100000: episode: 4392, duration: 0.312s, episode steps: 59, steps per second: 189, episode reward: 11.599, mean reward: 0.197 [0.041, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-1.239, 10.100], loss: 0.003548, mae: 0.065882, mean_q: 0.450589
 88654/100000: episode: 4393, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 11.196, mean reward: 0.238 [0.067, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.852, 10.209], loss: 0.003243, mae: 0.063074, mean_q: 0.449551
 88672/100000: episode: 4394, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 6.376, mean reward: 0.354 [0.256, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.249, 10.100], loss: 0.003544, mae: 0.065968, mean_q: 0.457206
 88690/100000: episode: 4395, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 7.957, mean reward: 0.442 [0.310, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.580, 10.100], loss: 0.003473, mae: 0.066127, mean_q: 0.444049
 88727/100000: episode: 4396, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 12.993, mean reward: 0.351 [0.243, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.508, 10.533], loss: 0.003263, mae: 0.061284, mean_q: 0.453093
[Info] New level: 0.8934298753738403 | Considering 10/90 traces
 88786/100000: episode: 4397, duration: 4.362s, episode steps: 59, steps per second: 14, episode reward: 16.669, mean reward: 0.283 [0.103, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-0.839, 10.100], loss: 0.003384, mae: 0.063954, mean_q: 0.454810
 88806/100000: episode: 4398, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 6.505, mean reward: 0.325 [0.246, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.267, 10.408], loss: 0.003571, mae: 0.065282, mean_q: 0.471872
 88841/100000: episode: 4399, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 12.827, mean reward: 0.366 [0.219, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.684, 10.356], loss: 0.003695, mae: 0.066600, mean_q: 0.454757
 88867/100000: episode: 4400, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 9.019, mean reward: 0.347 [0.067, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.296], loss: 0.003409, mae: 0.064163, mean_q: 0.455630
 88887/100000: episode: 4401, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 7.964, mean reward: 0.398 [0.287, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.561], loss: 0.002994, mae: 0.059941, mean_q: 0.448882
 88920/100000: episode: 4402, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 12.158, mean reward: 0.368 [0.243, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.029, 10.376], loss: 0.003457, mae: 0.065111, mean_q: 0.469780
 88935/100000: episode: 4403, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 6.528, mean reward: 0.435 [0.353, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.407, 10.100], loss: 0.002962, mae: 0.061068, mean_q: 0.463560
 88970/100000: episode: 4404, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 12.801, mean reward: 0.366 [0.283, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.412], loss: 0.003493, mae: 0.066334, mean_q: 0.470833
 88996/100000: episode: 4405, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 12.530, mean reward: 0.482 [0.335, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.194, 10.523], loss: 0.003432, mae: 0.064675, mean_q: 0.466545
 89011/100000: episode: 4406, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 5.021, mean reward: 0.335 [0.289, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.236, 10.425], loss: 0.003300, mae: 0.063063, mean_q: 0.451852
 89045/100000: episode: 4407, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 12.567, mean reward: 0.370 [0.146, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.046, 10.329], loss: 0.003296, mae: 0.062937, mean_q: 0.475623
 89080/100000: episode: 4408, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 10.916, mean reward: 0.312 [0.153, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.509, 10.338], loss: 0.003243, mae: 0.061891, mean_q: 0.476356
 89114/100000: episode: 4409, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 9.227, mean reward: 0.271 [0.049, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.241, 10.157], loss: 0.003503, mae: 0.065545, mean_q: 0.486462
 89147/100000: episode: 4410, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 9.505, mean reward: 0.288 [0.102, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.257, 10.206], loss: 0.003231, mae: 0.062165, mean_q: 0.471725
 89181/100000: episode: 4411, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 16.212, mean reward: 0.477 [0.427, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.260, 10.537], loss: 0.003306, mae: 0.063150, mean_q: 0.485776
 89216/100000: episode: 4412, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 14.349, mean reward: 0.410 [0.178, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.173, 10.348], loss: 0.003111, mae: 0.062417, mean_q: 0.478238
 89240/100000: episode: 4413, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 9.033, mean reward: 0.376 [0.272, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.401], loss: 0.003404, mae: 0.064412, mean_q: 0.475197
 89255/100000: episode: 4414, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 6.971, mean reward: 0.465 [0.372, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.278, 10.100], loss: 0.003437, mae: 0.064097, mean_q: 0.500325
 89284/100000: episode: 4415, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 10.856, mean reward: 0.374 [0.241, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.105, 10.389], loss: 0.003189, mae: 0.062724, mean_q: 0.496777
 89304/100000: episode: 4416, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 8.024, mean reward: 0.401 [0.334, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.444], loss: 0.003050, mae: 0.061151, mean_q: 0.497622
 89333/100000: episode: 4417, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 11.389, mean reward: 0.393 [0.302, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.142, 10.429], loss: 0.003210, mae: 0.061373, mean_q: 0.503042
 89368/100000: episode: 4418, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 13.170, mean reward: 0.376 [0.269, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.065, 10.393], loss: 0.003639, mae: 0.067073, mean_q: 0.507387
 89401/100000: episode: 4419, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 11.515, mean reward: 0.349 [0.202, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.438, 10.386], loss: 0.003007, mae: 0.060886, mean_q: 0.489882
 89436/100000: episode: 4420, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 13.791, mean reward: 0.394 [0.305, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.590, 10.517], loss: 0.003189, mae: 0.062605, mean_q: 0.505578
 89456/100000: episode: 4421, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 4.843, mean reward: 0.242 [0.075, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.257, 10.290], loss: 0.003295, mae: 0.063984, mean_q: 0.502731
 89482/100000: episode: 4422, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 11.659, mean reward: 0.448 [0.354, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.626, 10.611], loss: 0.003137, mae: 0.061962, mean_q: 0.480199
 89511/100000: episode: 4423, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 7.972, mean reward: 0.275 [0.098, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.696, 10.175], loss: 0.003169, mae: 0.062450, mean_q: 0.507772
 89535/100000: episode: 4424, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 9.510, mean reward: 0.396 [0.246, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.244, 10.415], loss: 0.002606, mae: 0.056686, mean_q: 0.506277
 89570/100000: episode: 4425, duration: 0.194s, episode steps: 35, steps per second: 181, episode reward: 13.551, mean reward: 0.387 [0.273, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.554, 10.405], loss: 0.002944, mae: 0.059944, mean_q: 0.516146
 89605/100000: episode: 4426, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 10.958, mean reward: 0.313 [0.127, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.500, 10.302], loss: 0.002834, mae: 0.059558, mean_q: 0.510167
 89620/100000: episode: 4427, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 6.137, mean reward: 0.409 [0.341, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.133, 10.524], loss: 0.003002, mae: 0.059361, mean_q: 0.503203
 89640/100000: episode: 4428, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 6.920, mean reward: 0.346 [0.236, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.094, 10.383], loss: 0.003135, mae: 0.061863, mean_q: 0.511397
 89664/100000: episode: 4429, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 11.444, mean reward: 0.477 [0.336, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.697, 10.522], loss: 0.003161, mae: 0.063178, mean_q: 0.531211
 89690/100000: episode: 4430, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 9.012, mean reward: 0.347 [0.035, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.074, 10.242], loss: 0.003571, mae: 0.066011, mean_q: 0.534884
 89716/100000: episode: 4431, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 9.485, mean reward: 0.365 [0.164, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.079, 10.352], loss: 0.003391, mae: 0.064431, mean_q: 0.529504
 89751/100000: episode: 4432, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 10.544, mean reward: 0.301 [0.178, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.803, 10.408], loss: 0.003414, mae: 0.064973, mean_q: 0.535093
 89785/100000: episode: 4433, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 10.289, mean reward: 0.303 [0.062, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.162, 10.310], loss: 0.003303, mae: 0.064003, mean_q: 0.528289
 89820/100000: episode: 4434, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 11.427, mean reward: 0.326 [0.199, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.981, 10.320], loss: 0.003049, mae: 0.062375, mean_q: 0.531490
 89835/100000: episode: 4435, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 5.010, mean reward: 0.334 [0.265, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.229, 10.100], loss: 0.002885, mae: 0.059504, mean_q: 0.540676
 89855/100000: episode: 4436, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 9.035, mean reward: 0.452 [0.369, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.149, 10.658], loss: 0.002881, mae: 0.059800, mean_q: 0.538272
 89890/100000: episode: 4437, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 15.151, mean reward: 0.433 [0.313, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.574, 10.462], loss: 0.003335, mae: 0.064240, mean_q: 0.542003
 89905/100000: episode: 4438, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 5.785, mean reward: 0.386 [0.275, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.537, 10.100], loss: 0.002854, mae: 0.059071, mean_q: 0.538653
 89938/100000: episode: 4439, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 8.613, mean reward: 0.261 [0.094, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.787, 10.237], loss: 0.003540, mae: 0.065905, mean_q: 0.543005
 89973/100000: episode: 4440, duration: 0.217s, episode steps: 35, steps per second: 162, episode reward: 9.769, mean reward: 0.279 [0.038, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.073, 10.156], loss: 0.003197, mae: 0.062395, mean_q: 0.539854
 89988/100000: episode: 4441, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 6.015, mean reward: 0.401 [0.316, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.220, 10.100], loss: 0.003023, mae: 0.060251, mean_q: 0.550580
 90003/100000: episode: 4442, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 6.766, mean reward: 0.451 [0.354, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-1.046, 10.381], loss: 0.003256, mae: 0.064661, mean_q: 0.534593
 90032/100000: episode: 4443, duration: 0.155s, episode steps: 29, steps per second: 188, episode reward: 7.451, mean reward: 0.257 [0.033, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.762, 10.152], loss: 0.003258, mae: 0.063396, mean_q: 0.550756
 90067/100000: episode: 4444, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 10.847, mean reward: 0.310 [0.161, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.723, 10.408], loss: 0.003178, mae: 0.062867, mean_q: 0.544466
 90100/100000: episode: 4445, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 7.080, mean reward: 0.215 [0.036, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.348, 10.100], loss: 0.002948, mae: 0.060132, mean_q: 0.538791
 90133/100000: episode: 4446, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 10.862, mean reward: 0.329 [0.109, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.175, 10.316], loss: 0.003108, mae: 0.061885, mean_q: 0.558911
 90159/100000: episode: 4447, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 11.948, mean reward: 0.460 [0.349, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.543], loss: 0.003158, mae: 0.061103, mean_q: 0.551109
 90174/100000: episode: 4448, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 6.601, mean reward: 0.440 [0.320, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.588], loss: 0.002994, mae: 0.059255, mean_q: 0.563820
 90194/100000: episode: 4449, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 7.803, mean reward: 0.390 [0.318, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.510], loss: 0.003005, mae: 0.061079, mean_q: 0.557614
 90229/100000: episode: 4450, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 13.097, mean reward: 0.374 [0.102, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.479, 10.259], loss: 0.003054, mae: 0.061389, mean_q: 0.558956
 90244/100000: episode: 4451, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 5.869, mean reward: 0.391 [0.291, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.334, 10.100], loss: 0.002908, mae: 0.060885, mean_q: 0.543310
 90277/100000: episode: 4452, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 11.799, mean reward: 0.358 [0.222, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.456, 10.340], loss: 0.003023, mae: 0.060851, mean_q: 0.557600
 90311/100000: episode: 4453, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 11.389, mean reward: 0.335 [0.182, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.241], loss: 0.003052, mae: 0.061779, mean_q: 0.556575
 90335/100000: episode: 4454, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 7.160, mean reward: 0.298 [0.090, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.286], loss: 0.003294, mae: 0.061293, mean_q: 0.557159
 90350/100000: episode: 4455, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 6.137, mean reward: 0.409 [0.310, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.762, 10.100], loss: 0.003132, mae: 0.060813, mean_q: 0.553320
 90384/100000: episode: 4456, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 12.570, mean reward: 0.370 [0.183, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.310, 10.276], loss: 0.003052, mae: 0.059663, mean_q: 0.562924
 90408/100000: episode: 4457, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 9.391, mean reward: 0.391 [0.186, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.373], loss: 0.002907, mae: 0.059583, mean_q: 0.561113
 90437/100000: episode: 4458, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 12.122, mean reward: 0.418 [0.340, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.759, 10.567], loss: 0.003667, mae: 0.065790, mean_q: 0.572965
 90452/100000: episode: 4459, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 6.352, mean reward: 0.423 [0.334, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.523, 10.100], loss: 0.003143, mae: 0.060208, mean_q: 0.556571
 90486/100000: episode: 4460, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 11.087, mean reward: 0.326 [0.239, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.251, 10.424], loss: 0.003327, mae: 0.062121, mean_q: 0.577337
 90521/100000: episode: 4461, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 9.117, mean reward: 0.260 [0.043, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.124, 10.164], loss: 0.003179, mae: 0.062727, mean_q: 0.584125
 90556/100000: episode: 4462, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 10.428, mean reward: 0.298 [0.180, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.335], loss: 0.002993, mae: 0.060022, mean_q: 0.568569
 90576/100000: episode: 4463, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 7.467, mean reward: 0.373 [0.172, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.412], loss: 0.003951, mae: 0.070826, mean_q: 0.585764
 90591/100000: episode: 4464, duration: 0.074s, episode steps: 15, steps per second: 201, episode reward: 6.350, mean reward: 0.423 [0.331, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.772, 10.671], loss: 0.003778, mae: 0.069078, mean_q: 0.567272
 90624/100000: episode: 4465, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 11.026, mean reward: 0.334 [0.197, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.720, 10.511], loss: 0.003064, mae: 0.061465, mean_q: 0.571752
 90650/100000: episode: 4466, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 11.481, mean reward: 0.442 [0.353, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.238, 10.591], loss: 0.003359, mae: 0.063921, mean_q: 0.557222
 90684/100000: episode: 4467, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 9.490, mean reward: 0.279 [0.096, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.256], loss: 0.003029, mae: 0.060323, mean_q: 0.583402
 90718/100000: episode: 4468, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 9.882, mean reward: 0.291 [0.111, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.248, 10.281], loss: 0.003127, mae: 0.061056, mean_q: 0.586005
 90733/100000: episode: 4469, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 6.690, mean reward: 0.446 [0.345, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.763, 10.100], loss: 0.002801, mae: 0.056804, mean_q: 0.582854
 90757/100000: episode: 4470, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 11.661, mean reward: 0.486 [0.365, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.697, 10.529], loss: 0.003000, mae: 0.059565, mean_q: 0.590591
 90792/100000: episode: 4471, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 9.319, mean reward: 0.266 [0.125, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.422, 10.339], loss: 0.002975, mae: 0.058714, mean_q: 0.575388
 90812/100000: episode: 4472, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 6.688, mean reward: 0.334 [0.260, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.128, 10.391], loss: 0.003283, mae: 0.062890, mean_q: 0.576557
 90845/100000: episode: 4473, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 8.849, mean reward: 0.268 [0.050, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.335, 10.100], loss: 0.003205, mae: 0.061542, mean_q: 0.578081
 90871/100000: episode: 4474, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 11.612, mean reward: 0.447 [0.292, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.155, 10.419], loss: 0.003425, mae: 0.063448, mean_q: 0.586586
 90886/100000: episode: 4475, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 7.445, mean reward: 0.496 [0.385, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.958, 10.100], loss: 0.003297, mae: 0.063918, mean_q: 0.586378
 90915/100000: episode: 4476, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 8.747, mean reward: 0.302 [0.112, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.163, 10.190], loss: 0.003506, mae: 0.064440, mean_q: 0.567347
 90935/100000: episode: 4477, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 8.140, mean reward: 0.407 [0.326, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.489], loss: 0.003259, mae: 0.062163, mean_q: 0.585635
 90950/100000: episode: 4478, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 6.286, mean reward: 0.419 [0.278, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.877, 10.100], loss: 0.003155, mae: 0.060385, mean_q: 0.586988
 90974/100000: episode: 4479, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 8.624, mean reward: 0.359 [0.216, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.356], loss: 0.003004, mae: 0.060142, mean_q: 0.585651
 91003/100000: episode: 4480, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 8.446, mean reward: 0.291 [0.053, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.420, 10.169], loss: 0.002820, mae: 0.059135, mean_q: 0.586285
 91038/100000: episode: 4481, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 8.170, mean reward: 0.233 [0.044, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.195], loss: 0.002980, mae: 0.059498, mean_q: 0.588158
 91067/100000: episode: 4482, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 5.684, mean reward: 0.196 [0.046, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.335, 10.100], loss: 0.002952, mae: 0.059472, mean_q: 0.588004
 91082/100000: episode: 4483, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 6.205, mean reward: 0.414 [0.370, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.415, 10.100], loss: 0.003363, mae: 0.062701, mean_q: 0.601549
 91115/100000: episode: 4484, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 9.056, mean reward: 0.274 [0.120, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.313], loss: 0.003021, mae: 0.061063, mean_q: 0.584358
 91141/100000: episode: 4485, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 9.309, mean reward: 0.358 [0.159, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.837, 10.356], loss: 0.003009, mae: 0.059130, mean_q: 0.596656
 91156/100000: episode: 4486, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 6.651, mean reward: 0.443 [0.363, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.199, 10.100], loss: 0.002677, mae: 0.057775, mean_q: 0.583730
[Info] New level: 0.9584413766860962 | Considering 10/90 traces
 91191/100000: episode: 4487, duration: 4.171s, episode steps: 35, steps per second: 8, episode reward: 13.158, mean reward: 0.376 [0.281, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.731, 10.431], loss: 0.003217, mae: 0.062329, mean_q: 0.592240
 91219/100000: episode: 4488, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 9.425, mean reward: 0.337 [0.081, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.319, 10.254], loss: 0.003314, mae: 0.062769, mean_q: 0.592634
 91248/100000: episode: 4489, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 10.581, mean reward: 0.365 [0.256, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.236, 10.357], loss: 0.003043, mae: 0.060736, mean_q: 0.598388
 91281/100000: episode: 4490, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 13.047, mean reward: 0.395 [0.181, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.968, 10.368], loss: 0.002848, mae: 0.057939, mean_q: 0.600163
 91299/100000: episode: 4491, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 7.541, mean reward: 0.419 [0.344, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.551, 10.424], loss: 0.003064, mae: 0.061155, mean_q: 0.613032
 91308/100000: episode: 4492, duration: 0.047s, episode steps: 9, steps per second: 194, episode reward: 3.902, mean reward: 0.434 [0.396, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.479, 10.100], loss: 0.002942, mae: 0.057979, mean_q: 0.599279
 91321/100000: episode: 4493, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 6.048, mean reward: 0.465 [0.408, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.429, 10.100], loss: 0.002602, mae: 0.055170, mean_q: 0.593254
 91354/100000: episode: 4494, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 11.353, mean reward: 0.344 [0.108, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.349, 10.208], loss: 0.002950, mae: 0.059285, mean_q: 0.599043
 91373/100000: episode: 4495, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 7.758, mean reward: 0.408 [0.284, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.359, 10.437], loss: 0.002807, mae: 0.059067, mean_q: 0.601585
 91391/100000: episode: 4496, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 8.951, mean reward: 0.497 [0.385, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.130, 10.548], loss: 0.002925, mae: 0.057587, mean_q: 0.608482
 91423/100000: episode: 4497, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 11.124, mean reward: 0.348 [0.161, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.384, 10.287], loss: 0.002824, mae: 0.058830, mean_q: 0.613096
 91432/100000: episode: 4498, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 4.538, mean reward: 0.504 [0.471, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.419, 10.100], loss: 0.002980, mae: 0.060063, mean_q: 0.600286
 91464/100000: episode: 4499, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 10.517, mean reward: 0.329 [0.012, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.063, 10.204], loss: 0.003023, mae: 0.060346, mean_q: 0.604918
 91482/100000: episode: 4500, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 7.561, mean reward: 0.420 [0.272, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.410, 10.343], loss: 0.002808, mae: 0.056665, mean_q: 0.617534
 91491/100000: episode: 4501, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 4.882, mean reward: 0.542 [0.456, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.468, 10.100], loss: 0.003157, mae: 0.060576, mean_q: 0.616841
 91510/100000: episode: 4502, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 9.058, mean reward: 0.477 [0.362, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.540], loss: 0.003075, mae: 0.060715, mean_q: 0.604178
 91519/100000: episode: 4503, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 4.865, mean reward: 0.541 [0.457, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.369, 10.100], loss: 0.003109, mae: 0.060293, mean_q: 0.604622
 91536/100000: episode: 4504, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 7.785, mean reward: 0.458 [0.381, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.136, 10.509], loss: 0.003332, mae: 0.060539, mean_q: 0.613108
 91565/100000: episode: 4505, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 9.709, mean reward: 0.335 [0.138, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.248], loss: 0.003016, mae: 0.060988, mean_q: 0.618967
 91578/100000: episode: 4506, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 5.065, mean reward: 0.390 [0.318, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.408, 10.100], loss: 0.002945, mae: 0.059769, mean_q: 0.606183
 91609/100000: episode: 4507, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 9.240, mean reward: 0.298 [0.077, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.158, 10.175], loss: 0.002917, mae: 0.059508, mean_q: 0.610076
 91641/100000: episode: 4508, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 12.402, mean reward: 0.388 [0.219, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.470, 10.375], loss: 0.002791, mae: 0.057615, mean_q: 0.613536
 91650/100000: episode: 4509, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 4.953, mean reward: 0.550 [0.486, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.465, 10.100], loss: 0.003470, mae: 0.061351, mean_q: 0.600952
 91681/100000: episode: 4510, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 13.532, mean reward: 0.437 [0.260, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.873, 10.446], loss: 0.003238, mae: 0.063519, mean_q: 0.611038
 91698/100000: episode: 4511, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 8.012, mean reward: 0.471 [0.384, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.727, 10.544], loss: 0.002651, mae: 0.057397, mean_q: 0.624729
 91726/100000: episode: 4512, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 10.617, mean reward: 0.379 [0.199, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.071, 10.323], loss: 0.002753, mae: 0.058493, mean_q: 0.617038
 91735/100000: episode: 4513, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 4.758, mean reward: 0.529 [0.430, 0.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.591, 10.100], loss: 0.002876, mae: 0.057296, mean_q: 0.617664
 91767/100000: episode: 4514, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 12.278, mean reward: 0.384 [0.244, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.853, 10.447], loss: 0.002754, mae: 0.058664, mean_q: 0.620329
 91800/100000: episode: 4515, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 11.917, mean reward: 0.361 [0.217, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.380], loss: 0.003014, mae: 0.059398, mean_q: 0.620385
 91817/100000: episode: 4516, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 6.400, mean reward: 0.376 [0.222, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.352], loss: 0.003403, mae: 0.063220, mean_q: 0.639690
 91846/100000: episode: 4517, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 13.159, mean reward: 0.454 [0.354, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.269, 10.656], loss: 0.003265, mae: 0.063147, mean_q: 0.620019
 91878/100000: episode: 4518, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 14.187, mean reward: 0.443 [0.289, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.440], loss: 0.002760, mae: 0.058342, mean_q: 0.624079
 91896/100000: episode: 4519, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 7.052, mean reward: 0.392 [0.262, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.411], loss: 0.002993, mae: 0.058993, mean_q: 0.620125
 91913/100000: episode: 4520, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 7.953, mean reward: 0.468 [0.382, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.819, 10.475], loss: 0.003736, mae: 0.066149, mean_q: 0.619522
 91944/100000: episode: 4521, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 13.007, mean reward: 0.420 [0.299, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.325, 10.439], loss: 0.003043, mae: 0.059989, mean_q: 0.624547
 91973/100000: episode: 4522, duration: 0.151s, episode steps: 29, steps per second: 191, episode reward: 9.432, mean reward: 0.325 [0.173, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.399, 10.286], loss: 0.003107, mae: 0.060571, mean_q: 0.627043
 92002/100000: episode: 4523, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 11.998, mean reward: 0.414 [0.260, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.851, 10.332], loss: 0.002745, mae: 0.056832, mean_q: 0.630657
 92035/100000: episode: 4524, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 11.345, mean reward: 0.344 [0.198, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.784, 10.328], loss: 0.002576, mae: 0.056143, mean_q: 0.629557
 92066/100000: episode: 4525, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 16.938, mean reward: 0.546 [0.406, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.569], loss: 0.002845, mae: 0.058567, mean_q: 0.623156
 92084/100000: episode: 4526, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 7.969, mean reward: 0.443 [0.356, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.579], loss: 0.002703, mae: 0.058844, mean_q: 0.655662
 92117/100000: episode: 4527, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 11.867, mean reward: 0.360 [0.200, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.899, 10.300], loss: 0.002762, mae: 0.056762, mean_q: 0.623977
 92126/100000: episode: 4528, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 5.482, mean reward: 0.609 [0.500, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.486, 10.100], loss: 0.002857, mae: 0.054910, mean_q: 0.643979
 92143/100000: episode: 4529, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 7.716, mean reward: 0.454 [0.384, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.457], loss: 0.003052, mae: 0.060851, mean_q: 0.650064
 92174/100000: episode: 4530, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 15.990, mean reward: 0.516 [0.387, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.173, 10.561], loss: 0.002879, mae: 0.058929, mean_q: 0.648740
 92193/100000: episode: 4531, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 9.630, mean reward: 0.507 [0.296, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.861, 10.498], loss: 0.002986, mae: 0.058792, mean_q: 0.646203
[RESULT] FALSIFICATION!
 92203/100000: episode: 4532, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 15.267, mean reward: 1.527 [0.527, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.422, 10.082], loss: 0.002973, mae: 0.059759, mean_q: 0.690135
 92222/100000: episode: 4533, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 9.637, mean reward: 0.507 [0.421, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.535], loss: 0.003168, mae: 0.061407, mean_q: 0.653945
 92239/100000: episode: 4534, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 8.227, mean reward: 0.484 [0.420, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.264, 10.528], loss: 0.003314, mae: 0.063755, mean_q: 0.647454
 92272/100000: episode: 4535, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 10.883, mean reward: 0.330 [0.164, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.406, 10.479], loss: 0.042661, mae: 0.071702, mean_q: 0.650332
 92285/100000: episode: 4536, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 5.446, mean reward: 0.419 [0.318, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.128, 10.100], loss: 0.008191, mae: 0.090921, mean_q: 0.635894
 92298/100000: episode: 4537, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 5.825, mean reward: 0.448 [0.363, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.616, 10.100], loss: 0.003564, mae: 0.065126, mean_q: 0.687508
 92329/100000: episode: 4538, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 11.521, mean reward: 0.372 [0.165, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.432, 10.303], loss: 0.003509, mae: 0.063703, mean_q: 0.652433
 92357/100000: episode: 4539, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 8.052, mean reward: 0.288 [0.143, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.194, 10.235], loss: 0.002877, mae: 0.059492, mean_q: 0.666664
 92386/100000: episode: 4540, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 11.599, mean reward: 0.400 [0.307, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.340, 10.535], loss: 0.003025, mae: 0.059655, mean_q: 0.661045
 92404/100000: episode: 4541, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 8.208, mean reward: 0.456 [0.347, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.146, 10.500], loss: 0.003011, mae: 0.059926, mean_q: 0.655238
 92433/100000: episode: 4542, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 8.076, mean reward: 0.278 [0.098, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.135, 10.201], loss: 0.003269, mae: 0.062906, mean_q: 0.660216
 92451/100000: episode: 4543, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 8.255, mean reward: 0.459 [0.308, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.633, 10.497], loss: 0.003122, mae: 0.060173, mean_q: 0.653645
 92479/100000: episode: 4544, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 10.673, mean reward: 0.381 [0.223, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.384], loss: 0.003176, mae: 0.061359, mean_q: 0.645220
 92497/100000: episode: 4545, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 7.717, mean reward: 0.429 [0.317, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.484], loss: 0.003022, mae: 0.060285, mean_q: 0.650198
 92528/100000: episode: 4546, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 13.257, mean reward: 0.428 [0.294, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.723, 10.485], loss: 0.002693, mae: 0.057759, mean_q: 0.654186
 92557/100000: episode: 4547, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 13.022, mean reward: 0.449 [0.361, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.581, 10.421], loss: 0.048091, mae: 0.076831, mean_q: 0.672823
 92576/100000: episode: 4548, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 7.879, mean reward: 0.415 [0.272, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.390], loss: 0.004404, mae: 0.069484, mean_q: 0.667914
 92595/100000: episode: 4549, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 8.989, mean reward: 0.473 [0.391, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.529], loss: 0.003776, mae: 0.065306, mean_q: 0.681655
 92627/100000: episode: 4550, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 11.667, mean reward: 0.365 [0.155, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.256, 10.273], loss: 0.003188, mae: 0.062066, mean_q: 0.675931
 92659/100000: episode: 4551, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 14.874, mean reward: 0.465 [0.369, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.606], loss: 0.044430, mae: 0.082503, mean_q: 0.689671
 92676/100000: episode: 4552, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 8.679, mean reward: 0.511 [0.465, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.608, 10.515], loss: 0.003967, mae: 0.064833, mean_q: 0.665872
 92693/100000: episode: 4553, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 6.912, mean reward: 0.407 [0.290, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.150, 10.454], loss: 0.003743, mae: 0.064343, mean_q: 0.672333
 92722/100000: episode: 4554, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 9.388, mean reward: 0.324 [0.155, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.317, 10.264], loss: 0.045936, mae: 0.071766, mean_q: 0.677586
 92754/100000: episode: 4555, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 13.753, mean reward: 0.430 [0.356, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.169, 10.539], loss: 0.005869, mae: 0.075409, mean_q: 0.677558
 92763/100000: episode: 4556, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 5.125, mean reward: 0.569 [0.471, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.527, 10.100], loss: 0.003616, mae: 0.066275, mean_q: 0.664417
 92782/100000: episode: 4557, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 8.818, mean reward: 0.464 [0.321, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.510], loss: 0.003155, mae: 0.061210, mean_q: 0.673711
 92815/100000: episode: 4558, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 12.206, mean reward: 0.370 [0.207, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.394], loss: 0.003183, mae: 0.059640, mean_q: 0.680897
 92846/100000: episode: 4559, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 12.149, mean reward: 0.392 [0.198, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.092, 10.297], loss: 0.002867, mae: 0.059012, mean_q: 0.676080
 92877/100000: episode: 4560, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 10.314, mean reward: 0.333 [0.143, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.334], loss: 0.046104, mae: 0.084257, mean_q: 0.697302
 92906/100000: episode: 4561, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 7.700, mean reward: 0.266 [0.105, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.316], loss: 0.047694, mae: 0.085689, mean_q: 0.702653
 92925/100000: episode: 4562, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 7.491, mean reward: 0.394 [0.270, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.492], loss: 0.004415, mae: 0.067788, mean_q: 0.703054
 92956/100000: episode: 4563, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 15.286, mean reward: 0.493 [0.415, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.617], loss: 0.003595, mae: 0.062877, mean_q: 0.689675
 92974/100000: episode: 4564, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 8.162, mean reward: 0.453 [0.355, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.365, 10.466], loss: 0.003836, mae: 0.065285, mean_q: 0.678824
 93007/100000: episode: 4565, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 10.877, mean reward: 0.330 [0.147, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.470, 10.249], loss: 0.003412, mae: 0.061122, mean_q: 0.702198
 93036/100000: episode: 4566, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 12.373, mean reward: 0.427 [0.344, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.388, 10.480], loss: 0.003215, mae: 0.062093, mean_q: 0.681610
 93053/100000: episode: 4567, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 7.927, mean reward: 0.466 [0.364, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.313, 10.558], loss: 0.003226, mae: 0.061040, mean_q: 0.672227
 93085/100000: episode: 4568, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 10.518, mean reward: 0.329 [0.156, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.233, 10.324], loss: 0.003049, mae: 0.060570, mean_q: 0.705392
 93118/100000: episode: 4569, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 11.815, mean reward: 0.358 [0.198, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.338, 10.338], loss: 0.041378, mae: 0.069292, mean_q: 0.697904
 93151/100000: episode: 4570, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 12.894, mean reward: 0.391 [0.162, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.549, 10.474], loss: 0.003455, mae: 0.061392, mean_q: 0.681684
 93179/100000: episode: 4571, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 9.928, mean reward: 0.355 [0.095, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.967, 10.347], loss: 0.002901, mae: 0.058349, mean_q: 0.709015
 93211/100000: episode: 4572, duration: 0.174s, episode steps: 32, steps per second: 183, episode reward: 11.780, mean reward: 0.368 [0.205, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.344, 10.360], loss: 0.002628, mae: 0.056260, mean_q: 0.695131
 93243/100000: episode: 4573, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 11.977, mean reward: 0.374 [0.217, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.992, 10.463], loss: 0.002916, mae: 0.058403, mean_q: 0.688551
 93275/100000: episode: 4574, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 10.518, mean reward: 0.329 [0.233, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.230, 10.419], loss: 0.042616, mae: 0.070468, mean_q: 0.710045
 93292/100000: episode: 4575, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 7.932, mean reward: 0.467 [0.386, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.508], loss: 0.006751, mae: 0.080615, mean_q: 0.704617
 93324/100000: episode: 4576, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 9.704, mean reward: 0.303 [0.114, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.123, 10.351], loss: 0.003986, mae: 0.063229, mean_q: 0.717449
[Info] New level: 1.0738306045532227 | Considering 10/90 traces
 93356/100000: episode: 4577, duration: 4.230s, episode steps: 32, steps per second: 8, episode reward: 14.974, mean reward: 0.468 [0.331, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.093, 10.496], loss: 0.003047, mae: 0.060018, mean_q: 0.701785
 93383/100000: episode: 4578, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 12.448, mean reward: 0.461 [0.318, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.688, 10.437], loss: 0.049846, mae: 0.074031, mean_q: 0.723432
 93397/100000: episode: 4579, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 6.797, mean reward: 0.485 [0.417, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.512, 10.598], loss: 0.005538, mae: 0.074504, mean_q: 0.708822
 93416/100000: episode: 4580, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 8.429, mean reward: 0.444 [0.318, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.434], loss: 0.003340, mae: 0.059229, mean_q: 0.709499
 93424/100000: episode: 4581, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 4.279, mean reward: 0.535 [0.484, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.540, 10.100], loss: 0.003321, mae: 0.062786, mean_q: 0.680862
 93439/100000: episode: 4582, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 6.815, mean reward: 0.454 [0.390, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.553], loss: 0.003595, mae: 0.061132, mean_q: 0.708453
 93454/100000: episode: 4583, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 6.300, mean reward: 0.420 [0.320, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.619], loss: 0.003184, mae: 0.058823, mean_q: 0.689542
 93468/100000: episode: 4584, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 7.548, mean reward: 0.539 [0.426, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.113, 10.602], loss: 0.002581, mae: 0.055276, mean_q: 0.710890
 93495/100000: episode: 4585, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 12.255, mean reward: 0.454 [0.340, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.382, 10.477], loss: 0.003226, mae: 0.059775, mean_q: 0.709674
 93510/100000: episode: 4586, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 7.047, mean reward: 0.470 [0.412, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.058, 10.548], loss: 0.002763, mae: 0.057373, mean_q: 0.728898
 93528/100000: episode: 4587, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 8.186, mean reward: 0.455 [0.360, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.707, 10.500], loss: 0.003164, mae: 0.057784, mean_q: 0.712260
 93547/100000: episode: 4588, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 10.558, mean reward: 0.556 [0.346, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.473], loss: 0.003121, mae: 0.060451, mean_q: 0.726006
 93562/100000: episode: 4589, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 8.036, mean reward: 0.536 [0.456, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.525], loss: 0.003467, mae: 0.060784, mean_q: 0.739624
 93589/100000: episode: 4590, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 12.148, mean reward: 0.450 [0.355, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.851, 10.502], loss: 0.002649, mae: 0.056148, mean_q: 0.717727
 93616/100000: episode: 4591, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 12.177, mean reward: 0.451 [0.279, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.507, 10.416], loss: 0.002847, mae: 0.057477, mean_q: 0.732424
 93634/100000: episode: 4592, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 9.924, mean reward: 0.551 [0.457, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.650], loss: 0.002717, mae: 0.057136, mean_q: 0.733502
 93649/100000: episode: 4593, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 6.650, mean reward: 0.443 [0.372, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.535], loss: 0.002697, mae: 0.055900, mean_q: 0.719520
[RESULT] FALSIFICATION!
 93653/100000: episode: 4594, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 11.852, mean reward: 2.963 [0.604, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.379, 10.067], loss: 0.002109, mae: 0.052608, mean_q: 0.729455
[RESULT] FALSIFICATION!
 93657/100000: episode: 4595, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 11.863, mean reward: 2.966 [0.600, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.347, 10.067], loss: 0.002816, mae: 0.057338, mean_q: 0.720162
 93675/100000: episode: 4596, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 7.704, mean reward: 0.428 [0.342, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.514], loss: 0.002616, mae: 0.056219, mean_q: 0.715872
 93690/100000: episode: 4597, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 8.396, mean reward: 0.560 [0.467, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.279, 10.581], loss: 0.002793, mae: 0.057110, mean_q: 0.721418
 93709/100000: episode: 4598, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 8.041, mean reward: 0.423 [0.236, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.167, 10.389], loss: 0.003358, mae: 0.060710, mean_q: 0.728377
 93724/100000: episode: 4599, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 7.392, mean reward: 0.493 [0.432, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.499, 10.605], loss: 0.086432, mae: 0.077678, mean_q: 0.744958
 93732/100000: episode: 4600, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 4.137, mean reward: 0.517 [0.456, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.421, 10.100], loss: 0.002470, mae: 0.056231, mean_q: 0.728168
 93740/100000: episode: 4601, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 4.301, mean reward: 0.538 [0.493, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.459, 10.100], loss: 0.003502, mae: 0.066729, mean_q: 0.719170
 93770/100000: episode: 4602, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 13.329, mean reward: 0.444 [0.218, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.100, 10.361], loss: 0.002594, mae: 0.055984, mean_q: 0.722699
 93788/100000: episode: 4603, duration: 0.111s, episode steps: 18, steps per second: 163, episode reward: 10.281, mean reward: 0.571 [0.522, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.643], loss: 0.002670, mae: 0.054181, mean_q: 0.742399
 93799/100000: episode: 4604, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 4.382, mean reward: 0.398 [0.250, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.484, 10.100], loss: 0.002556, mae: 0.054678, mean_q: 0.750526
 93807/100000: episode: 4605, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 4.021, mean reward: 0.503 [0.435, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.507, 10.100], loss: 0.003070, mae: 0.062373, mean_q: 0.716329
 93834/100000: episode: 4606, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 13.115, mean reward: 0.486 [0.190, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.234, 10.307], loss: 0.002347, mae: 0.053622, mean_q: 0.727215
 93849/100000: episode: 4607, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 7.411, mean reward: 0.494 [0.441, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.142, 10.543], loss: 0.003124, mae: 0.058907, mean_q: 0.741801
 93857/100000: episode: 4608, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.322, mean reward: 0.540 [0.482, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.555, 10.100], loss: 0.003480, mae: 0.060515, mean_q: 0.738227
 93865/100000: episode: 4609, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 4.383, mean reward: 0.548 [0.460, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.475, 10.100], loss: 0.002699, mae: 0.057601, mean_q: 0.734590
 93876/100000: episode: 4610, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 4.537, mean reward: 0.412 [0.360, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.003, 10.100], loss: 0.119228, mae: 0.085322, mean_q: 0.757393
 93884/100000: episode: 4611, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.352, mean reward: 0.544 [0.475, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.681, 10.100], loss: 0.008930, mae: 0.085723, mean_q: 0.786004
 93899/100000: episode: 4612, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 6.834, mean reward: 0.456 [0.367, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.590], loss: 0.003619, mae: 0.063885, mean_q: 0.740620
[RESULT] FALSIFICATION!
 93902/100000: episode: 4613, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 11.293, mean reward: 3.764 [0.610, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.360, 10.046], loss: 0.005390, mae: 0.079162, mean_q: 0.754047
 93913/100000: episode: 4614, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 6.568, mean reward: 0.597 [0.559, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.394, 10.100], loss: 0.004597, mae: 0.065375, mean_q: 0.770481
 93928/100000: episode: 4615, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 7.084, mean reward: 0.472 [0.381, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.431, 10.570], loss: 0.093867, mae: 0.121442, mean_q: 0.783914
 93939/100000: episode: 4616, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 5.131, mean reward: 0.466 [0.390, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.328, 10.100], loss: 0.004934, mae: 0.072527, mean_q: 0.733199
 93947/100000: episode: 4617, duration: 0.042s, episode steps: 8, steps per second: 188, episode reward: 3.782, mean reward: 0.473 [0.438, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.499, 10.100], loss: 0.004760, mae: 0.066251, mean_q: 0.743381
 93955/100000: episode: 4618, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 4.402, mean reward: 0.550 [0.500, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.944, 10.100], loss: 0.005352, mae: 0.065777, mean_q: 0.744792
[RESULT] FALSIFICATION!
 93956/100000: episode: 4619, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.016, 9.986], loss: 0.004616, mae: 0.066732, mean_q: 0.753009
 93975/100000: episode: 4620, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 9.595, mean reward: 0.505 [0.401, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.276, 10.562], loss: 0.071689, mae: 0.083462, mean_q: 0.771776
 93983/100000: episode: 4621, duration: 0.056s, episode steps: 8, steps per second: 144, episode reward: 4.613, mean reward: 0.577 [0.531, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.524, 10.100], loss: 0.009965, mae: 0.090099, mean_q: 0.761501
 93991/100000: episode: 4622, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.915, mean reward: 0.614 [0.550, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.456, 10.100], loss: 0.003412, mae: 0.066254, mean_q: 0.754032
 94006/100000: episode: 4623, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 7.932, mean reward: 0.529 [0.471, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.255, 10.715], loss: 0.003686, mae: 0.061678, mean_q: 0.749496
 94033/100000: episode: 4624, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 12.954, mean reward: 0.480 [0.396, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.141, 10.736], loss: 0.049638, mae: 0.072203, mean_q: 0.748737
 94041/100000: episode: 4625, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 4.940, mean reward: 0.618 [0.554, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.430, 10.100], loss: 0.159981, mae: 0.133138, mean_q: 0.784667
 94052/100000: episode: 4626, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 4.404, mean reward: 0.400 [0.343, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.611, 10.100], loss: 0.121255, mae: 0.131232, mean_q: 0.784109
 94063/100000: episode: 4627, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.424, mean reward: 0.493 [0.422, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.519, 10.100], loss: 0.005734, mae: 0.082828, mean_q: 0.750100
 94074/100000: episode: 4628, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 5.069, mean reward: 0.461 [0.389, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.385, 10.100], loss: 0.113039, mae: 0.094132, mean_q: 0.761069
[RESULT] FALSIFICATION!
 94076/100000: episode: 4629, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 10.636, mean reward: 5.318 [0.636, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.396, 10.020], loss: 0.008316, mae: 0.084796, mean_q: 0.729255
 94084/100000: episode: 4630, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 4.553, mean reward: 0.569 [0.525, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.477, 10.100], loss: 0.161894, mae: 0.111899, mean_q: 0.777117
[RESULT] FALSIFICATION!
 94087/100000: episode: 4631, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.090, mean reward: 3.697 [0.541, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.336, 10.046], loss: 0.005217, mae: 0.074539, mean_q: 0.696092
 94114/100000: episode: 4632, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 11.685, mean reward: 0.433 [0.336, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.469], loss: 0.182456, mae: 0.130664, mean_q: 0.787868
 94128/100000: episode: 4633, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 7.376, mean reward: 0.527 [0.406, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.543], loss: 0.105285, mae: 0.132308, mean_q: 0.800903
 94158/100000: episode: 4634, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 12.291, mean reward: 0.410 [0.208, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.890, 10.383], loss: 0.121270, mae: 0.107928, mean_q: 0.789278
 94185/100000: episode: 4635, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 13.442, mean reward: 0.498 [0.409, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.618], loss: 0.004985, mae: 0.071269, mean_q: 0.762303
 94212/100000: episode: 4636, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 11.543, mean reward: 0.428 [0.128, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.441, 10.323], loss: 0.091780, mae: 0.090881, mean_q: 0.770577
 94230/100000: episode: 4637, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 10.372, mean reward: 0.576 [0.457, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-2.304, 10.526], loss: 0.078814, mae: 0.111585, mean_q: 0.806830
[RESULT] FALSIFICATION!
 94237/100000: episode: 4638, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 13.807, mean reward: 1.972 [0.566, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.488, 9.899], loss: 0.006464, mae: 0.088767, mean_q: 0.695090
 94251/100000: episode: 4639, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 6.944, mean reward: 0.496 [0.394, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.000, 10.480], loss: 0.092816, mae: 0.086856, mean_q: 0.766634
 94266/100000: episode: 4640, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 6.119, mean reward: 0.408 [0.342, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.449], loss: 0.007656, mae: 0.076511, mean_q: 0.775832
 94280/100000: episode: 4641, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 7.543, mean reward: 0.539 [0.448, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.557], loss: 0.005679, mae: 0.078205, mean_q: 0.754039
 94294/100000: episode: 4642, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 8.043, mean reward: 0.575 [0.513, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.277, 10.715], loss: 0.003675, mae: 0.061231, mean_q: 0.782703
 94312/100000: episode: 4643, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 9.445, mean reward: 0.525 [0.454, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.474], loss: 0.070795, mae: 0.081878, mean_q: 0.783159
 94323/100000: episode: 4644, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 4.661, mean reward: 0.424 [0.391, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.487, 10.100], loss: 0.003802, mae: 0.061583, mean_q: 0.784540
 94334/100000: episode: 4645, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 5.337, mean reward: 0.485 [0.428, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.446, 10.100], loss: 0.116478, mae: 0.095211, mean_q: 0.775273
 94364/100000: episode: 4646, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 12.920, mean reward: 0.431 [0.326, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.682, 10.495], loss: 0.085511, mae: 0.099284, mean_q: 0.798070
 94394/100000: episode: 4647, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 9.176, mean reward: 0.306 [0.120, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.540, 10.276], loss: 0.004174, mae: 0.066916, mean_q: 0.770748
 94402/100000: episode: 4648, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 5.168, mean reward: 0.646 [0.549, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.536, 10.100], loss: 0.003093, mae: 0.057045, mean_q: 0.770335
 94432/100000: episode: 4649, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 13.672, mean reward: 0.456 [0.310, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.412, 10.472], loss: 0.047586, mae: 0.083477, mean_q: 0.778877
 94440/100000: episode: 4650, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 4.412, mean reward: 0.551 [0.525, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.091, 10.100], loss: 0.003876, mae: 0.063563, mean_q: 0.766078
 94455/100000: episode: 4651, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 8.865, mean reward: 0.591 [0.515, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.529, 10.623], loss: 0.004142, mae: 0.064613, mean_q: 0.755918
 94469/100000: episode: 4652, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 6.998, mean reward: 0.500 [0.387, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.467, 10.581], loss: 0.003585, mae: 0.059405, mean_q: 0.771386
[RESULT] FALSIFICATION!
 94470/100000: episode: 4653, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.018, 10.055], loss: 0.003397, mae: 0.070437, mean_q: 0.778908
 94481/100000: episode: 4654, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 4.014, mean reward: 0.365 [0.317, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.492, 10.100], loss: 0.232397, mae: 0.120893, mean_q: 0.784096
 94495/100000: episode: 4655, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 6.635, mean reward: 0.474 [0.395, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.457], loss: 0.005601, mae: 0.081627, mean_q: 0.789893
 94503/100000: episode: 4656, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.154, mean reward: 0.519 [0.434, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.831, 10.100], loss: 0.004486, mae: 0.071455, mean_q: 0.795046
 94522/100000: episode: 4657, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 11.016, mean reward: 0.580 [0.404, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.043, 10.505], loss: 0.067445, mae: 0.076381, mean_q: 0.774187
 94530/100000: episode: 4658, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 4.312, mean reward: 0.539 [0.483, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.508, 10.100], loss: 0.008758, mae: 0.079072, mean_q: 0.801715
 94544/100000: episode: 4659, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 8.127, mean reward: 0.580 [0.487, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.528, 10.659], loss: 0.095911, mae: 0.092623, mean_q: 0.781686
 94552/100000: episode: 4660, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 4.890, mean reward: 0.611 [0.547, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.597, 10.100], loss: 0.004354, mae: 0.067816, mean_q: 0.785155
 94570/100000: episode: 4661, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 9.715, mean reward: 0.540 [0.303, 0.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.615], loss: 0.144314, mae: 0.118182, mean_q: 0.808190
 94578/100000: episode: 4662, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 4.099, mean reward: 0.512 [0.457, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.465, 10.100], loss: 0.004332, mae: 0.067204, mean_q: 0.826967
 94586/100000: episode: 4663, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 4.303, mean reward: 0.538 [0.490, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.544, 10.100], loss: 0.004777, mae: 0.072234, mean_q: 0.773589
 94597/100000: episode: 4664, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 5.262, mean reward: 0.478 [0.375, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.304, 10.100], loss: 0.234391, mae: 0.128927, mean_q: 0.786397
 94627/100000: episode: 4665, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 12.964, mean reward: 0.432 [0.323, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.133, 10.590], loss: 0.131499, mae: 0.102674, mean_q: 0.794149
 94635/100000: episode: 4666, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 4.133, mean reward: 0.517 [0.435, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.403, 10.100], loss: 0.011676, mae: 0.124073, mean_q: 0.908096
[Info] New level: 1.1787207126617432 | Considering 10/90 traces
 94649/100000: episode: 4667, duration: 4.060s, episode steps: 14, steps per second: 3, episode reward: 8.548, mean reward: 0.611 [0.566, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.404, 10.583], loss: 0.006296, mae: 0.084382, mean_q: 0.795716
 94654/100000: episode: 4668, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 2.932, mean reward: 0.586 [0.546, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.489, 10.100], loss: 0.004823, mae: 0.075568, mean_q: 0.781094
 94658/100000: episode: 4669, duration: 0.032s, episode steps: 4, steps per second: 124, episode reward: 2.202, mean reward: 0.550 [0.510, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.390, 10.100], loss: 0.004848, mae: 0.081335, mean_q: 0.828487
 94662/100000: episode: 4670, duration: 0.032s, episode steps: 4, steps per second: 125, episode reward: 2.363, mean reward: 0.591 [0.516, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.512, 10.100], loss: 0.004500, mae: 0.077296, mean_q: 0.811266
 94666/100000: episode: 4671, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.053, mean reward: 0.513 [0.491, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.141, 10.100], loss: 0.005346, mae: 0.082036, mean_q: 0.742704
 94671/100000: episode: 4672, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 3.090, mean reward: 0.618 [0.581, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.600, 10.100], loss: 0.003196, mae: 0.060085, mean_q: 0.780866
[RESULT] FALSIFICATION!
 94672/100000: episode: 4673, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.429, 10.067], loss: 0.002663, mae: 0.061104, mean_q: 0.825852
 94676/100000: episode: 4674, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 1.961, mean reward: 0.490 [0.478, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.666, 10.100], loss: 0.003988, mae: 0.060835, mean_q: 0.801962
[RESULT] FALSIFICATION!
 94679/100000: episode: 4675, duration: 0.027s, episode steps: 3, steps per second: 110, episode reward: 11.307, mean reward: 3.769 [0.626, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.469, 10.082], loss: 0.004274, mae: 0.065226, mean_q: 0.787290
 94685/100000: episode: 4676, duration: 0.050s, episode steps: 6, steps per second: 119, episode reward: 3.492, mean reward: 0.582 [0.532, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.527, 10.100], loss: 0.003035, mae: 0.062235, mean_q: 0.789436
 94691/100000: episode: 4677, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.061, mean reward: 0.510 [0.454, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.536, 10.100], loss: 0.007817, mae: 0.072890, mean_q: 0.809844
 94696/100000: episode: 4678, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 3.093, mean reward: 0.619 [0.579, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.596, 10.100], loss: 0.251466, mae: 0.117722, mean_q: 0.824987
 94702/100000: episode: 4679, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 3.535, mean reward: 0.589 [0.553, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.628, 10.100], loss: 0.006851, mae: 0.093355, mean_q: 0.827375
 94706/100000: episode: 4680, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 1.913, mean reward: 0.478 [0.335, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.717, 10.100], loss: 0.005931, mae: 0.079050, mean_q: 0.800231
 94712/100000: episode: 4681, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 2.977, mean reward: 0.496 [0.402, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.648, 10.100], loss: 0.006097, mae: 0.078482, mean_q: 0.731181
 94716/100000: episode: 4682, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 2.365, mean reward: 0.591 [0.542, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.600, 10.100], loss: 0.004577, mae: 0.070929, mean_q: 0.790457
 94720/100000: episode: 4683, duration: 0.028s, episode steps: 4, steps per second: 145, episode reward: 1.998, mean reward: 0.500 [0.459, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.483, 10.100], loss: 0.311332, mae: 0.134868, mean_q: 0.781259
[RESULT] FALSIFICATION!
 94721/100000: episode: 4684, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.388, 10.093], loss: 0.009999, mae: 0.078455, mean_q: 0.756071
 94726/100000: episode: 4685, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 3.061, mean reward: 0.612 [0.531, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.451, 10.100], loss: 0.012598, mae: 0.103751, mean_q: 0.849873
[RESULT] FALSIFICATION!
 94728/100000: episode: 4686, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.604, mean reward: 5.302 [0.604, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.462, 10.067], loss: 0.016591, mae: 0.119129, mean_q: 0.875451
 94733/100000: episode: 4687, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 2.838, mean reward: 0.568 [0.525, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.615, 10.100], loss: 0.009521, mae: 0.110272, mean_q: 0.902797
 94739/100000: episode: 4688, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 3.112, mean reward: 0.519 [0.450, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.530, 10.100], loss: 0.005429, mae: 0.079685, mean_q: 0.812178
 94743/100000: episode: 4689, duration: 0.027s, episode steps: 4, steps per second: 151, episode reward: 1.947, mean reward: 0.487 [0.384, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.596, 10.100], loss: 0.319920, mae: 0.149240, mean_q: 0.763833
 94748/100000: episode: 4690, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 2.970, mean reward: 0.594 [0.560, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.586, 10.100], loss: 0.005255, mae: 0.075196, mean_q: 0.850225
[RESULT] FALSIFICATION!
 94751/100000: episode: 4691, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 11.239, mean reward: 3.746 [0.611, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.397, 10.093], loss: 0.005243, mae: 0.081382, mean_q: 0.843747
 94757/100000: episode: 4692, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 3.253, mean reward: 0.542 [0.445, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.511, 10.100], loss: 0.202845, mae: 0.131639, mean_q: 0.832938
 94762/100000: episode: 4693, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 3.016, mean reward: 0.603 [0.559, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.616, 10.100], loss: 0.010771, mae: 0.094395, mean_q: 0.832972
 94767/100000: episode: 4694, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 2.622, mean reward: 0.524 [0.442, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.431, 10.100], loss: 0.007163, mae: 0.075889, mean_q: 0.776995
 94772/100000: episode: 4695, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 3.032, mean reward: 0.606 [0.566, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.316, 10.100], loss: 0.004755, mae: 0.068002, mean_q: 0.816714
 94776/100000: episode: 4696, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 2.029, mean reward: 0.507 [0.496, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.491, 10.100], loss: 0.006846, mae: 0.086018, mean_q: 0.771097
 94780/100000: episode: 4697, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 2.084, mean reward: 0.521 [0.491, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.454, 10.100], loss: 0.312301, mae: 0.134777, mean_q: 0.803735
 94785/100000: episode: 4698, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 2.759, mean reward: 0.552 [0.448, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.436, 10.100], loss: 0.008973, mae: 0.092771, mean_q: 0.836538
 94791/100000: episode: 4699, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.338, mean reward: 0.556 [0.520, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.545, 10.100], loss: 0.411754, mae: 0.188335, mean_q: 0.833756
 94796/100000: episode: 4700, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 2.677, mean reward: 0.535 [0.484, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.582, 10.100], loss: 0.661753, mae: 0.281577, mean_q: 0.922258
 94801/100000: episode: 4701, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 2.854, mean reward: 0.571 [0.492, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.459, 10.100], loss: 0.064524, mae: 0.202325, mean_q: 0.913657
 94805/100000: episode: 4702, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 2.269, mean reward: 0.567 [0.426, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.411, 10.100], loss: 0.039052, mae: 0.228635, mean_q: 0.840420
 94809/100000: episode: 4703, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 2.181, mean reward: 0.545 [0.529, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.541, 10.100], loss: 0.026874, mae: 0.159648, mean_q: 0.839810
 94814/100000: episode: 4704, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 2.739, mean reward: 0.548 [0.514, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.469, 10.100], loss: 0.031796, mae: 0.207605, mean_q: 0.777262
 94818/100000: episode: 4705, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 2.393, mean reward: 0.598 [0.566, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.533, 10.100], loss: 0.009956, mae: 0.107169, mean_q: 0.883781
 94824/100000: episode: 4706, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.434, mean reward: 0.572 [0.473, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.473, 10.100], loss: 0.012377, mae: 0.112459, mean_q: 0.856566
[RESULT] FALSIFICATION!
 94825/100000: episode: 4707, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.442, 10.093], loss: 0.010227, mae: 0.100669, mean_q: 0.839973
[RESULT] FALSIFICATION!
 94826/100000: episode: 4708, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.460, 10.093], loss: 0.003494, mae: 0.072988, mean_q: 0.766540
 94832/100000: episode: 4709, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 3.134, mean reward: 0.522 [0.488, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.979, 10.100], loss: 0.199817, mae: 0.136329, mean_q: 0.811446
 94837/100000: episode: 4710, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 3.041, mean reward: 0.608 [0.565, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.516, 10.100], loss: 0.010483, mae: 0.091032, mean_q: 0.878687
 94843/100000: episode: 4711, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 3.495, mean reward: 0.583 [0.515, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.614, 10.100], loss: 0.010632, mae: 0.114143, mean_q: 0.944120
[RESULT] FALSIFICATION!
 94844/100000: episode: 4712, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.400, 10.046], loss: 0.010269, mae: 0.087247, mean_q: 0.864289
 94850/100000: episode: 4713, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 3.465, mean reward: 0.577 [0.483, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.519, 10.100], loss: 0.409110, mae: 0.198190, mean_q: 0.799672
 94853/100000: episode: 4714, duration: 0.028s, episode steps: 3, steps per second: 108, episode reward: 1.842, mean reward: 0.614 [0.545, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.515, 10.100], loss: 0.015041, mae: 0.124235, mean_q: 0.939688
 94857/100000: episode: 4715, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 2.240, mean reward: 0.560 [0.549, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.527, 10.100], loss: 0.022506, mae: 0.159025, mean_q: 0.954611
[RESULT] FALSIFICATION!
 94858/100000: episode: 4716, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.368, 10.067], loss: 0.006562, mae: 0.090229, mean_q: 0.966399
 94861/100000: episode: 4717, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 1.724, mean reward: 0.575 [0.466, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.475, 10.100], loss: 0.400009, mae: 0.174631, mean_q: 0.826093
 94865/100000: episode: 4718, duration: 0.025s, episode steps: 4, steps per second: 163, episode reward: 1.955, mean reward: 0.489 [0.449, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.489, 10.100], loss: 0.010408, mae: 0.095021, mean_q: 0.835534
 94871/100000: episode: 4719, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 3.454, mean reward: 0.576 [0.550, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.515, 10.100], loss: 0.006127, mae: 0.083093, mean_q: 0.873680
 94877/100000: episode: 4720, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 3.339, mean reward: 0.556 [0.495, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.634, 10.100], loss: 0.010006, mae: 0.094011, mean_q: 0.818227
 94883/100000: episode: 4721, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 3.605, mean reward: 0.601 [0.580, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.555, 10.100], loss: 0.008303, mae: 0.090117, mean_q: 0.795943
 94887/100000: episode: 4722, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 2.050, mean reward: 0.512 [0.492, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.585, 10.100], loss: 0.299959, mae: 0.140799, mean_q: 0.857723
[RESULT] FALSIFICATION!
 94888/100000: episode: 4723, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.374, 10.067], loss: 0.007425, mae: 0.074160, mean_q: 0.851916
 94894/100000: episode: 4724, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 3.336, mean reward: 0.556 [0.510, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.601, 10.100], loss: 0.015082, mae: 0.102985, mean_q: 0.838181
 94899/100000: episode: 4725, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 3.126, mean reward: 0.625 [0.566, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.631, 10.100], loss: 0.252237, mae: 0.135823, mean_q: 0.898423
 94904/100000: episode: 4726, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 2.670, mean reward: 0.534 [0.444, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.474, 10.100], loss: 0.011073, mae: 0.091319, mean_q: 0.832290
[RESULT] FALSIFICATION!
 94905/100000: episode: 4727, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.425, 10.093], loss: 0.018548, mae: 0.118018, mean_q: 0.894309
 94911/100000: episode: 4728, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 3.466, mean reward: 0.578 [0.541, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.908, 10.100], loss: 0.006723, mae: 0.080457, mean_q: 0.864047
 94917/100000: episode: 4729, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 3.122, mean reward: 0.520 [0.455, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.519, 10.100], loss: 0.007395, mae: 0.085664, mean_q: 0.815631
 94921/100000: episode: 4730, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 2.225, mean reward: 0.556 [0.485, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.660, 10.100], loss: 0.297865, mae: 0.156013, mean_q: 0.784377
 94927/100000: episode: 4731, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 3.426, mean reward: 0.571 [0.540, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.560, 10.100], loss: 0.008372, mae: 0.083993, mean_q: 0.879745
 94930/100000: episode: 4732, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.620, mean reward: 0.540 [0.450, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.535, 10.100], loss: 0.386564, mae: 0.168098, mean_q: 0.849433
 94936/100000: episode: 4733, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 3.470, mean reward: 0.578 [0.539, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.596, 10.100], loss: 0.020582, mae: 0.129345, mean_q: 0.892407
[RESULT] FALSIFICATION!
 94941/100000: episode: 4734, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 12.607, mean reward: 2.521 [0.634, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.592, 10.100], loss: 0.252221, mae: 0.149707, mean_q: 0.883336
[RESULT] FALSIFICATION!
 94942/100000: episode: 4735, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.445, 10.067], loss: 0.006153, mae: 0.081113, mean_q: 0.885212
 94946/100000: episode: 4736, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 2.363, mean reward: 0.591 [0.570, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.495, 10.100], loss: 0.005626, mae: 0.076751, mean_q: 0.847449
 94951/100000: episode: 4737, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 2.857, mean reward: 0.571 [0.514, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.515, 10.100], loss: 0.006226, mae: 0.075975, mean_q: 0.827680
 94955/100000: episode: 4738, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.308, mean reward: 0.577 [0.537, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.572, 10.100], loss: 0.323657, mae: 0.165146, mean_q: 0.875188
 94961/100000: episode: 4739, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 3.432, mean reward: 0.572 [0.549, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.535, 10.100], loss: 0.202550, mae: 0.130617, mean_q: 0.849051
 94965/100000: episode: 4740, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 2.059, mean reward: 0.515 [0.471, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.565, 10.100], loss: 0.301909, mae: 0.191222, mean_q: 0.816000
 94970/100000: episode: 4741, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 3.000, mean reward: 0.600 [0.564, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.635, 10.100], loss: 0.234458, mae: 0.162985, mean_q: 0.859089
 94974/100000: episode: 4742, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 1.981, mean reward: 0.495 [0.432, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.559, 10.100], loss: 0.020428, mae: 0.130439, mean_q: 0.904157
 94978/100000: episode: 4743, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 2.266, mean reward: 0.567 [0.523, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.495, 10.100], loss: 0.006271, mae: 0.087009, mean_q: 0.853486
 94984/100000: episode: 4744, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 3.463, mean reward: 0.577 [0.537, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.575, 10.100], loss: 0.201288, mae: 0.159828, mean_q: 0.795089
 94988/100000: episode: 4745, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 2.060, mean reward: 0.515 [0.455, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.565, 10.100], loss: 0.022390, mae: 0.134132, mean_q: 0.851178
 94994/100000: episode: 4746, duration: 0.046s, episode steps: 6, steps per second: 131, episode reward: 3.276, mean reward: 0.546 [0.509, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.552, 10.100], loss: 0.010708, mae: 0.105748, mean_q: 0.917919
 94998/100000: episode: 4747, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 1.888, mean reward: 0.472 [0.446, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.606, 10.100], loss: 0.009214, mae: 0.096900, mean_q: 0.831692
 95004/100000: episode: 4748, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.430, mean reward: 0.572 [0.543, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.856, 10.100], loss: 0.008915, mae: 0.099231, mean_q: 0.757649
 95010/100000: episode: 4749, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 3.459, mean reward: 0.577 [0.552, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.590, 10.100], loss: 0.216989, mae: 0.139106, mean_q: 0.833391
 95016/100000: episode: 4750, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 3.752, mean reward: 0.625 [0.570, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.579, 10.100], loss: 0.010154, mae: 0.108357, mean_q: 0.920000
 95020/100000: episode: 4751, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 2.258, mean reward: 0.564 [0.484, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.511, 10.100], loss: 0.010657, mae: 0.098326, mean_q: 0.854362
 95024/100000: episode: 4752, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 1.975, mean reward: 0.494 [0.467, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.550, 10.100], loss: 0.291966, mae: 0.185339, mean_q: 0.832886
 95028/100000: episode: 4753, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 1.937, mean reward: 0.484 [0.467, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.393, 10.100], loss: 0.009133, mae: 0.089087, mean_q: 0.875036
 95034/100000: episode: 4754, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 3.550, mean reward: 0.592 [0.559, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.556, 10.100], loss: 0.214624, mae: 0.133778, mean_q: 0.894860
 95040/100000: episode: 4755, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 2.969, mean reward: 0.495 [0.412, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.547, 10.100], loss: 0.007656, mae: 0.082436, mean_q: 0.858869
 95046/100000: episode: 4756, duration: 0.033s, episode steps: 6, steps per second: 179, episode reward: 3.000, mean reward: 0.500 [0.452, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.479, 10.100], loss: 0.008172, mae: 0.089866, mean_q: 0.804352
[Info] New level: 1.4387367963790894 | Considering 10/90 traces
 95052/100000: episode: 4757, duration: 4.029s, episode steps: 6, steps per second: 1, episode reward: 3.512, mean reward: 0.585 [0.507, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.611, 10.100], loss: 0.010956, mae: 0.094540, mean_q: 0.833555
 95055/100000: episode: 4758, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.773, mean reward: 0.591 [0.500, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.506, 10.100], loss: 0.812654, mae: 0.283139, mean_q: 0.814867
 95059/100000: episode: 4759, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 2.202, mean reward: 0.551 [0.501, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.613, 10.100], loss: 0.568867, mae: 0.288849, mean_q: 0.930890
 95062/100000: episode: 4760, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 1.762, mean reward: 0.587 [0.532, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.875, 10.100], loss: 0.030100, mae: 0.131750, mean_q: 0.872211
 95065/100000: episode: 4761, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.655, mean reward: 0.552 [0.511, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.789, 10.100], loss: 0.043614, mae: 0.172724, mean_q: 0.963013
 95068/100000: episode: 4762, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 1.701, mean reward: 0.567 [0.543, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.483, 10.100], loss: 0.383058, mae: 0.241349, mean_q: 0.938478
 95071/100000: episode: 4763, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 1.816, mean reward: 0.605 [0.525, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.486, 10.100], loss: 0.011788, mae: 0.099841, mean_q: 0.897021
 95074/100000: episode: 4764, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 1.647, mean reward: 0.549 [0.500, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.409, 10.100], loss: 0.009194, mae: 0.095776, mean_q: 0.811683
 95077/100000: episode: 4765, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 1.699, mean reward: 0.566 [0.507, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.710, 10.100], loss: 1.224833, mae: 0.367740, mean_q: 0.767810
 95080/100000: episode: 4766, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 1.721, mean reward: 0.574 [0.523, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.509, 10.100], loss: 0.032401, mae: 0.203661, mean_q: 0.774939
 95083/100000: episode: 4767, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 1.425, mean reward: 0.475 [0.451, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.516, 10.100], loss: 0.049295, mae: 0.206619, mean_q: 0.858529
 95086/100000: episode: 4768, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 1.992, mean reward: 0.664 [0.639, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.488, 10.100], loss: 0.043087, mae: 0.167141, mean_q: 0.977004
 95089/100000: episode: 4769, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.812, mean reward: 0.604 [0.546, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.424, 10.100], loss: 0.799211, mae: 0.358140, mean_q: 0.998949
 95093/100000: episode: 4770, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 2.125, mean reward: 0.531 [0.484, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.587, 10.100], loss: 0.300074, mae: 0.237508, mean_q: 0.998223
 95096/100000: episode: 4771, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 1.898, mean reward: 0.633 [0.604, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.502, 10.100], loss: 0.814568, mae: 0.294493, mean_q: 0.912504
 95099/100000: episode: 4772, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.679, mean reward: 0.560 [0.489, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.431, 10.100], loss: 0.019923, mae: 0.118026, mean_q: 0.894183
 95103/100000: episode: 4773, duration: 0.029s, episode steps: 4, steps per second: 137, episode reward: 2.522, mean reward: 0.630 [0.565, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.573, 10.100], loss: 0.288146, mae: 0.161832, mean_q: 0.866814
 95106/100000: episode: 4774, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 1.842, mean reward: 0.614 [0.572, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.453, 10.100], loss: 0.018561, mae: 0.114603, mean_q: 0.904521
 95110/100000: episode: 4775, duration: 0.030s, episode steps: 4, steps per second: 132, episode reward: 2.175, mean reward: 0.544 [0.510, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.594, 10.100], loss: 0.008493, mae: 0.101434, mean_q: 0.901405
[RESULT] FALSIFICATION!
 95111/100000: episode: 4776, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.422, 10.093], loss: 0.009241, mae: 0.106818, mean_q: 0.935743
 95114/100000: episode: 4777, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 1.678, mean reward: 0.559 [0.512, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.418, 10.100], loss: 0.401834, mae: 0.189478, mean_q: 0.848089
 95118/100000: episode: 4778, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 2.311, mean reward: 0.578 [0.534, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.569, 10.100], loss: 0.278814, mae: 0.193753, mean_q: 0.809393
 95121/100000: episode: 4779, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 2.003, mean reward: 0.668 [0.623, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.482, 10.100], loss: 0.034147, mae: 0.171487, mean_q: 0.881279
[RESULT] FALSIFICATION!
 95122/100000: episode: 4780, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.458, 10.093], loss: 1.271379, mae: 0.404265, mean_q: 0.829991
 95125/100000: episode: 4781, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 1.822, mean reward: 0.607 [0.558, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.458, 10.100], loss: 0.018615, mae: 0.143915, mean_q: 0.981874
 95128/100000: episode: 4782, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 1.707, mean reward: 0.569 [0.546, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.622, 10.100], loss: 0.014057, mae: 0.143284, mean_q: 0.942522
[RESULT] FALSIFICATION!
 95129/100000: episode: 4783, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.469, 10.093], loss: 1.273810, mae: 0.422166, mean_q: 0.911337
[RESULT] FALSIFICATION!
 95130/100000: episode: 4784, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.485, 10.082], loss: 0.016850, mae: 0.157776, mean_q: 0.975902
 95133/100000: episode: 4785, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 1.760, mean reward: 0.587 [0.481, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.745, 10.100], loss: 0.011068, mae: 0.110165, mean_q: 0.863293
[RESULT] FALSIFICATION!
 95134/100000: episode: 4786, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.420, 10.093], loss: 1.271201, mae: 0.395246, mean_q: 0.927561
[RESULT] FALSIFICATION!
 95135/100000: episode: 4787, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.449, 10.093], loss: 0.010914, mae: 0.106808, mean_q: 0.752186
 95138/100000: episode: 4788, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 1.594, mean reward: 0.531 [0.438, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.476, 10.100], loss: 0.416533, mae: 0.169430, mean_q: 0.842629
 95141/100000: episode: 4789, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 1.766, mean reward: 0.589 [0.549, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.546, 10.100], loss: 0.014452, mae: 0.114295, mean_q: 0.876317
 95144/100000: episode: 4790, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 1.891, mean reward: 0.630 [0.617, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.517, 10.100], loss: 0.011391, mae: 0.111392, mean_q: 0.830374
 95147/100000: episode: 4791, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 1.780, mean reward: 0.593 [0.544, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.670, 10.100], loss: 0.748639, mae: 0.272552, mean_q: 0.812806
[RESULT] FALSIFICATION!
 95148/100000: episode: 4792, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.402, 10.093], loss: 0.013541, mae: 0.122114, mean_q: 0.901311
 95151/100000: episode: 4793, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.895, mean reward: 0.632 [0.626, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.436, 10.100], loss: 0.011964, mae: 0.111233, mean_q: 0.896588
[RESULT] FALSIFICATION!
 95152/100000: episode: 4794, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.452, 10.093], loss: 0.010771, mae: 0.110675, mean_q: 0.853897
 95156/100000: episode: 4795, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 2.585, mean reward: 0.646 [0.627, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.586, 10.100], loss: 0.015030, mae: 0.118104, mean_q: 0.829547
[RESULT] FALSIFICATION!
 95157/100000: episode: 4796, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.517, 10.093], loss: 0.004923, mae: 0.070835, mean_q: 0.859645
 95161/100000: episode: 4797, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 2.336, mean reward: 0.584 [0.504, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.520, 10.100], loss: 0.296505, mae: 0.244870, mean_q: 0.921471
 95164/100000: episode: 4798, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 1.851, mean reward: 0.617 [0.605, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.543, 10.100], loss: 0.030425, mae: 0.160967, mean_q: 0.903977
 95167/100000: episode: 4799, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 1.893, mean reward: 0.631 [0.604, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.466, 10.100], loss: 1.219306, mae: 0.407218, mean_q: 0.910999
 95171/100000: episode: 4800, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 2.371, mean reward: 0.593 [0.556, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.592, 10.100], loss: 0.290907, mae: 0.206550, mean_q: 0.961560
[RESULT] FALSIFICATION!
 95172/100000: episode: 4801, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.465, 10.093], loss: 0.036688, mae: 0.161440, mean_q: 0.995681
[RESULT] FALSIFICATION!
 95173/100000: episode: 4802, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.420, 10.093], loss: 0.048683, mae: 0.181395, mean_q: 0.994582
 95177/100000: episode: 4803, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 2.309, mean reward: 0.577 [0.550, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.562, 10.100], loss: 0.026032, mae: 0.136980, mean_q: 0.929888
[RESULT] FALSIFICATION!
 95178/100000: episode: 4804, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.424, 10.093], loss: 1.148425, mae: 0.362169, mean_q: 0.960499
 95181/100000: episode: 4805, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.760, mean reward: 0.587 [0.524, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.587, 10.100], loss: 0.015517, mae: 0.127766, mean_q: 0.885107
 95184/100000: episode: 4806, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.842, mean reward: 0.614 [0.605, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.477, 10.100], loss: 0.018010, mae: 0.120506, mean_q: 0.908998
 95187/100000: episode: 4807, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.787, mean reward: 0.596 [0.575, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.496, 10.100], loss: 0.013386, mae: 0.110865, mean_q: 0.860469
[RESULT] FALSIFICATION!
 95188/100000: episode: 4808, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.436, 10.093], loss: 0.010696, mae: 0.086852, mean_q: 0.936162
 95191/100000: episode: 4809, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 1.610, mean reward: 0.537 [0.502, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.509, 10.100], loss: 0.012107, mae: 0.104999, mean_q: 0.874602
 95194/100000: episode: 4810, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 1.729, mean reward: 0.576 [0.537, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.521, 10.100], loss: 0.355485, mae: 0.180992, mean_q: 0.907075
[RESULT] FALSIFICATION!
 95195/100000: episode: 4811, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.474, 10.093], loss: 0.026373, mae: 0.164401, mean_q: 0.970111
 95198/100000: episode: 4812, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.898, mean reward: 0.633 [0.554, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.704, 10.100], loss: 0.737840, mae: 0.295173, mean_q: 0.964916
[RESULT] FALSIFICATION!
 95199/100000: episode: 4813, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.418, 10.093], loss: 0.016953, mae: 0.123211, mean_q: 0.919848
 95203/100000: episode: 4814, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 2.202, mean reward: 0.550 [0.515, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.573, 10.100], loss: 0.023323, mae: 0.134154, mean_q: 0.941940
 95206/100000: episode: 4815, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.844, mean reward: 0.615 [0.568, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.527, 10.100], loss: 0.748745, mae: 0.272912, mean_q: 0.893001
 95209/100000: episode: 4816, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 1.821, mean reward: 0.607 [0.550, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.505, 10.100], loss: 0.014990, mae: 0.110540, mean_q: 0.872729
[RESULT] FALSIFICATION!
 95210/100000: episode: 4817, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.440, 10.093], loss: 0.020216, mae: 0.119160, mean_q: 0.844160
[RESULT] FALSIFICATION!
 95212/100000: episode: 4818, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 10.678, mean reward: 5.339 [0.678, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.459, 10.098], loss: 0.034212, mae: 0.163338, mean_q: 0.938704
 95216/100000: episode: 4819, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 2.473, mean reward: 0.618 [0.543, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.595, 10.100], loss: 0.021321, mae: 0.125920, mean_q: 0.924558
 95219/100000: episode: 4820, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.700, mean reward: 0.567 [0.478, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.798, 10.100], loss: 0.011066, mae: 0.106580, mean_q: 0.889144
 95223/100000: episode: 4821, duration: 0.030s, episode steps: 4, steps per second: 133, episode reward: 2.551, mean reward: 0.638 [0.629, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.595, 10.100], loss: 0.010292, mae: 0.106214, mean_q: 0.890172
[RESULT] FALSIFICATION!
 95224/100000: episode: 4822, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.476, 10.093], loss: 1.181907, mae: 0.363284, mean_q: 0.853726
 95227/100000: episode: 4823, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 1.676, mean reward: 0.559 [0.520, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.405, 10.100], loss: 0.722058, mae: 0.278879, mean_q: 0.841896
[RESULT] FALSIFICATION!
 95228/100000: episode: 4824, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.442, 10.093], loss: 0.014188, mae: 0.104657, mean_q: 0.921062
[RESULT] FALSIFICATION!
 95229/100000: episode: 4825, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.450, 10.093], loss: 0.042519, mae: 0.171014, mean_q: 0.892606
 95232/100000: episode: 4826, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.835, mean reward: 0.612 [0.593, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.140, 10.100], loss: 0.715947, mae: 0.284097, mean_q: 0.931139
 95235/100000: episode: 4827, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 1.825, mean reward: 0.608 [0.548, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.493, 10.100], loss: 0.040301, mae: 0.178319, mean_q: 0.985447
 95238/100000: episode: 4828, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.778, mean reward: 0.593 [0.549, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.453, 10.100], loss: 0.013693, mae: 0.118576, mean_q: 0.864577
 95241/100000: episode: 4829, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 1.474, mean reward: 0.491 [0.472, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.502, 10.100], loss: 0.022361, mae: 0.148265, mean_q: 0.824493
[RESULT] FALSIFICATION!
 95244/100000: episode: 4830, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 11.299, mean reward: 3.766 [0.641, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.579, 10.098], loss: 0.352271, mae: 0.189270, mean_q: 0.929196
 95248/100000: episode: 4831, duration: 0.032s, episode steps: 4, steps per second: 123, episode reward: 2.567, mean reward: 0.642 [0.630, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.603, 10.100], loss: 0.304699, mae: 0.227863, mean_q: 0.938052
[RESULT] FALSIFICATION!
 95249/100000: episode: 4832, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.443, 10.093], loss: 0.026791, mae: 0.148901, mean_q: 0.830675
 95252/100000: episode: 4833, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.815, mean reward: 0.605 [0.581, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.448, 10.100], loss: 0.041088, mae: 0.179470, mean_q: 0.974633
 95255/100000: episode: 4834, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 1.580, mean reward: 0.527 [0.449, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.445, 10.100], loss: 0.744464, mae: 0.334492, mean_q: 0.969842
 95258/100000: episode: 4835, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.727, mean reward: 0.576 [0.482, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.611, 10.100], loss: 0.031711, mae: 0.157363, mean_q: 0.972928
 95261/100000: episode: 4836, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.972, mean reward: 0.657 [0.641, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.548, 10.100], loss: 0.366249, mae: 0.213973, mean_q: 0.942676
 95264/100000: episode: 4837, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.471, mean reward: 0.490 [0.435, 0.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.459, 10.100], loss: 0.009277, mae: 0.091427, mean_q: 0.892749
 95267/100000: episode: 4838, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.766, mean reward: 0.589 [0.543, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.521, 10.100], loss: 0.362196, mae: 0.233454, mean_q: 0.937108
 95270/100000: episode: 4839, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.732, mean reward: 0.577 [0.534, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.488, 10.100], loss: 0.460560, mae: 0.271430, mean_q: 0.925552
[RESULT] FALSIFICATION!
 95271/100000: episode: 4840, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.488, 10.093], loss: 1.032097, mae: 0.364664, mean_q: 0.954638
[RESULT] FALSIFICATION!
 95272/100000: episode: 4841, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.446, 10.093], loss: 0.009695, mae: 0.096836, mean_q: 0.921616
 95275/100000: episode: 4842, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.559, mean reward: 0.520 [0.453, 0.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.738, 10.100], loss: 0.444040, mae: 0.275181, mean_q: 0.971427
 95278/100000: episode: 4843, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 1.791, mean reward: 0.597 [0.593, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.624, 10.100], loss: 0.693185, mae: 0.338453, mean_q: 1.044970
 95281/100000: episode: 4844, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 1.694, mean reward: 0.565 [0.550, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.616, 10.100], loss: 0.018202, mae: 0.140459, mean_q: 0.965173
[RESULT] FALSIFICATION!
 95282/100000: episode: 4845, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.411, 10.093], loss: 0.009722, mae: 0.111331, mean_q: 0.840788
 95285/100000: episode: 4846, duration: 0.018s, episode steps: 3, steps per second: 162, episode reward: 1.996, mean reward: 0.665 [0.662, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.623, 10.100], loss: 0.365151, mae: 0.205922, mean_q: 0.837485
[Info] New level: 2.0574421882629395 | Considering 80/20 traces
 95288/100000: episode: 4847, duration: 3.988s, episode steps: 3, steps per second: 1, episode reward: 1.723, mean reward: 0.574 [0.523, 0.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.450, 10.100], loss: 0.038755, mae: 0.170696, mean_q: 0.937325
 95291/100000: episode: 4848, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.806, mean reward: 0.602 [0.549, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.434, 10.100], loss: 0.374868, mae: 0.208056, mean_q: 0.906670
 95294/100000: episode: 4849, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.761, mean reward: 0.587 [0.558, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.477, 10.100], loss: 0.355094, mae: 0.206251, mean_q: 0.940889
[RESULT] FALSIFICATION!
 95295/100000: episode: 4850, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.446, 10.093], loss: 0.029207, mae: 0.159017, mean_q: 1.032233
 95298/100000: episode: 4851, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.612, mean reward: 0.537 [0.494, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.472, 10.100], loss: 0.014463, mae: 0.111284, mean_q: 0.947529
 95301/100000: episode: 4852, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.706, mean reward: 0.569 [0.551, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.519, 10.100], loss: 0.021375, mae: 0.128192, mean_q: 0.849396
[RESULT] FALSIFICATION!
 95304/100000: episode: 4853, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 11.205, mean reward: 3.735 [0.588, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.545, 10.100], loss: 0.437809, mae: 0.208893, mean_q: 0.914893
[RESULT] FALSIFICATION!
 95305/100000: episode: 4854, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.472, 10.093], loss: 1.022081, mae: 0.344925, mean_q: 0.836221
 95308/100000: episode: 4855, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.829, mean reward: 0.610 [0.588, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.461, 10.100], loss: 0.019286, mae: 0.127755, mean_q: 0.908599
 95311/100000: episode: 4856, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.614, mean reward: 0.538 [0.488, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.500, 10.100], loss: 0.349030, mae: 0.204680, mean_q: 0.902651
 95314/100000: episode: 4857, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.763, mean reward: 0.588 [0.562, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.546, 10.100], loss: 0.773540, mae: 0.338097, mean_q: 0.973088
[RESULT] FALSIFICATION!
 95316/100000: episode: 4858, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 10.651, mean reward: 5.325 [0.651, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.520, 10.098], loss: 0.051722, mae: 0.194650, mean_q: 0.928570
 95319/100000: episode: 4859, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 1.703, mean reward: 0.568 [0.518, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.602, 10.100], loss: 0.407612, mae: 0.288440, mean_q: 0.938413
 95322/100000: episode: 4860, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.612, mean reward: 0.537 [0.420, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.621, 10.100], loss: 1.491089, mae: 0.515243, mean_q: 0.999228
[RESULT] FALSIFICATION!
 95323/100000: episode: 4861, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.435, 10.093], loss: 1.084140, mae: 0.528283, mean_q: 1.249808
 95326/100000: episode: 4862, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.816, mean reward: 0.605 [0.583, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.532, 10.100], loss: 0.117238, mae: 0.314272, mean_q: 1.162090
[RESULT] FALSIFICATION!
 95327/100000: episode: 4863, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.427, 10.093], loss: 1.962220, mae: 0.756319, mean_q: 1.199588
[RESULT] FALSIFICATION!
 95328/100000: episode: 4864, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.473, 10.093], loss: 0.976846, mae: 0.526286, mean_q: 1.215868
[RESULT] FALSIFICATION!
 95329/100000: episode: 4865, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.435, 10.093], loss: 0.072305, mae: 0.241383, mean_q: 1.079586
 95332/100000: episode: 4866, duration: 0.026s, episode steps: 3, steps per second: 115, episode reward: 1.880, mean reward: 0.627 [0.600, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.568, 10.100], loss: 0.359627, mae: 0.269544, mean_q: 0.976954
[Info] New level: 2.2080917358398438 | Considering 100/0 traces
 95335/100000: episode: 4867, duration: 3.992s, episode steps: 3, steps per second: 1, episode reward: 1.781, mean reward: 0.594 [0.515, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.526, 10.100], loss: 0.375420, mae: 0.258176, mean_q: 0.970044
[Info] Not found new level, current best level reached = 2.2080917358398438
 95347/100000: episode: 4868, duration: 4.029s, episode steps: 12, steps per second: 3, episode reward: 4.916, mean reward: 0.410 [0.316, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.533, 10.100], loss: 0.200522, mae: 0.195576, mean_q: 0.925313
 95447/100000: episode: 4869, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 20.557, mean reward: 0.206 [0.001, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.273, 10.277], loss: 0.426167, mae: 0.268866, mean_q: 0.963392
 95547/100000: episode: 4870, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.250, mean reward: 0.163 [0.029, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.098], loss: 0.310267, mae: 0.230336, mean_q: 0.927808
 95647/100000: episode: 4871, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 16.710, mean reward: 0.167 [0.004, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.095, 10.293], loss: 0.514122, mae: 0.307314, mean_q: 0.969348
 95747/100000: episode: 4872, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 19.071, mean reward: 0.191 [0.022, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.427, 10.098], loss: 0.368580, mae: 0.257163, mean_q: 0.937067
 95847/100000: episode: 4873, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 14.580, mean reward: 0.146 [0.011, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.736, 10.098], loss: 0.416792, mae: 0.273840, mean_q: 0.938812
 95947/100000: episode: 4874, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 17.916, mean reward: 0.179 [0.022, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.187, 10.139], loss: 0.367969, mae: 0.252667, mean_q: 0.919788
 96047/100000: episode: 4875, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 18.813, mean reward: 0.188 [0.011, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.164], loss: 0.438560, mae: 0.272145, mean_q: 0.911983
 96147/100000: episode: 4876, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 18.812, mean reward: 0.188 [0.009, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.532, 10.098], loss: 0.312571, mae: 0.239304, mean_q: 0.872817
 96247/100000: episode: 4877, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 14.581, mean reward: 0.146 [0.018, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.232, 10.218], loss: 0.512175, mae: 0.308781, mean_q: 0.926047
 96347/100000: episode: 4878, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 20.413, mean reward: 0.204 [0.041, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.466, 10.098], loss: 0.319268, mae: 0.238423, mean_q: 0.854814
 96447/100000: episode: 4879, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 16.653, mean reward: 0.167 [0.011, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.435, 10.121], loss: 0.501678, mae: 0.302963, mean_q: 0.895449
 96547/100000: episode: 4880, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 17.079, mean reward: 0.171 [0.014, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.206, 10.125], loss: 0.426491, mae: 0.286417, mean_q: 0.882911
 96647/100000: episode: 4881, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 15.058, mean reward: 0.151 [0.012, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.267, 10.098], loss: 0.442456, mae: 0.285318, mean_q: 0.846022
 96747/100000: episode: 4882, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 16.101, mean reward: 0.161 [0.022, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.600, 10.098], loss: 0.517905, mae: 0.322802, mean_q: 0.871199
 96847/100000: episode: 4883, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 21.611, mean reward: 0.216 [0.008, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.950, 10.098], loss: 0.408295, mae: 0.281931, mean_q: 0.824959
 96947/100000: episode: 4884, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 23.973, mean reward: 0.240 [0.038, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.786, 10.098], loss: 0.602574, mae: 0.364716, mean_q: 0.884199
 97047/100000: episode: 4885, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 20.921, mean reward: 0.209 [0.014, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.723, 10.324], loss: 0.463807, mae: 0.305093, mean_q: 0.828835
 97147/100000: episode: 4886, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 14.669, mean reward: 0.147 [0.011, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.175, 10.098], loss: 0.442523, mae: 0.293604, mean_q: 0.799943
 97247/100000: episode: 4887, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 14.276, mean reward: 0.143 [0.025, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.326, 10.143], loss: 0.375264, mae: 0.266764, mean_q: 0.771895
 97347/100000: episode: 4888, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 16.538, mean reward: 0.165 [0.028, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.417, 10.098], loss: 0.454520, mae: 0.292860, mean_q: 0.787661
 97447/100000: episode: 4889, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 18.057, mean reward: 0.181 [0.024, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.532, 10.098], loss: 0.348309, mae: 0.247455, mean_q: 0.731607
 97547/100000: episode: 4890, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 20.949, mean reward: 0.209 [0.012, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.229, 10.373], loss: 0.377773, mae: 0.245303, mean_q: 0.733082
 97647/100000: episode: 4891, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.140, mean reward: 0.161 [0.042, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.098], loss: 0.377903, mae: 0.255279, mean_q: 0.753738
 97747/100000: episode: 4892, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 18.021, mean reward: 0.180 [0.022, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.098], loss: 0.312560, mae: 0.225886, mean_q: 0.690369
 97847/100000: episode: 4893, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 15.611, mean reward: 0.156 [0.012, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.524, 10.098], loss: 0.418134, mae: 0.267737, mean_q: 0.731255
 97947/100000: episode: 4894, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 17.578, mean reward: 0.176 [0.017, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.439], loss: 0.490601, mae: 0.297577, mean_q: 0.749277
 98047/100000: episode: 4895, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 13.937, mean reward: 0.139 [0.011, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.895, 10.098], loss: 0.319610, mae: 0.242470, mean_q: 0.702647
 98147/100000: episode: 4896, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 16.369, mean reward: 0.164 [0.021, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.277], loss: 0.325434, mae: 0.238912, mean_q: 0.674774
 98247/100000: episode: 4897, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 12.564, mean reward: 0.126 [0.010, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.240, 10.301], loss: 0.385181, mae: 0.250983, mean_q: 0.655212
 98347/100000: episode: 4898, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 17.148, mean reward: 0.171 [0.031, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.335], loss: 0.312427, mae: 0.226193, mean_q: 0.635952
 98447/100000: episode: 4899, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 16.957, mean reward: 0.170 [0.018, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.656, 10.098], loss: 0.452743, mae: 0.277459, mean_q: 0.682823
 98547/100000: episode: 4900, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 20.047, mean reward: 0.200 [0.051, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.404, 10.098], loss: 0.278289, mae: 0.215392, mean_q: 0.607996
 98647/100000: episode: 4901, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 13.904, mean reward: 0.139 [0.022, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.828, 10.134], loss: 0.385736, mae: 0.251345, mean_q: 0.632700
 98747/100000: episode: 4902, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 18.688, mean reward: 0.187 [0.028, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.444, 10.098], loss: 0.481941, mae: 0.295497, mean_q: 0.656890
 98847/100000: episode: 4903, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 18.443, mean reward: 0.184 [0.035, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.509, 10.098], loss: 0.276644, mae: 0.205319, mean_q: 0.566875
 98947/100000: episode: 4904, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 14.312, mean reward: 0.143 [0.012, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.098], loss: 0.279146, mae: 0.210309, mean_q: 0.555343
 99047/100000: episode: 4905, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 17.300, mean reward: 0.173 [0.023, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.025, 10.098], loss: 0.349969, mae: 0.240598, mean_q: 0.571484
 99147/100000: episode: 4906, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 16.495, mean reward: 0.165 [0.023, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.054, 10.107], loss: 0.234083, mae: 0.201885, mean_q: 0.532498
 99247/100000: episode: 4907, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 16.574, mean reward: 0.166 [0.013, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.368, 10.098], loss: 0.430826, mae: 0.254502, mean_q: 0.557640
 99347/100000: episode: 4908, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 18.391, mean reward: 0.184 [0.007, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.241, 10.209], loss: 0.395755, mae: 0.257013, mean_q: 0.564737
 99447/100000: episode: 4909, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 14.693, mean reward: 0.147 [0.005, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.137], loss: 0.305436, mae: 0.220401, mean_q: 0.525616
 99547/100000: episode: 4910, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 14.698, mean reward: 0.147 [0.017, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.791, 10.098], loss: 0.192791, mae: 0.170597, mean_q: 0.467865
 99647/100000: episode: 4911, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 15.270, mean reward: 0.153 [0.011, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.485, 10.098], loss: 0.255959, mae: 0.183157, mean_q: 0.464417
 99747/100000: episode: 4912, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 16.307, mean reward: 0.163 [0.010, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.077, 10.098], loss: 0.224747, mae: 0.175197, mean_q: 0.458322
 99847/100000: episode: 4913, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 22.501, mean reward: 0.225 [0.036, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.337], loss: 0.159130, mae: 0.147876, mean_q: 0.427791
 99947/100000: episode: 4914, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 20.421, mean reward: 0.204 [0.031, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.582, 10.098], loss: 0.261586, mae: 0.178506, mean_q: 0.445346
done, took 818.697 seconds
[Info] End Importance Splitting.
