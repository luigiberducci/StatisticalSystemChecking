Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 1000000 steps ...
    100/1000000: episode: 1, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 59.360, mean reward: 0.594 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.752, 10.098], loss: --, mae: --, mean_q: --
    200/1000000: episode: 2, duration: 0.273s, episode steps: 100, steps per second: 366, episode reward: 58.264, mean reward: 0.583 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.805, 10.098], loss: --, mae: --, mean_q: --
    300/1000000: episode: 3, duration: 0.202s, episode steps: 100, steps per second: 494, episode reward: 60.530, mean reward: 0.605 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.447, 10.218], loss: --, mae: --, mean_q: --
    400/1000000: episode: 4, duration: 0.184s, episode steps: 100, steps per second: 542, episode reward: 62.292, mean reward: 0.623 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.748, 10.201], loss: --, mae: --, mean_q: --
    500/1000000: episode: 5, duration: 0.184s, episode steps: 100, steps per second: 543, episode reward: 59.253, mean reward: 0.593 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.273, 10.098], loss: --, mae: --, mean_q: --
    600/1000000: episode: 6, duration: 0.186s, episode steps: 100, steps per second: 539, episode reward: 58.728, mean reward: 0.587 [0.506, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.315, 10.378], loss: --, mae: --, mean_q: --
    700/1000000: episode: 7, duration: 0.185s, episode steps: 100, steps per second: 541, episode reward: 58.410, mean reward: 0.584 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.148], loss: --, mae: --, mean_q: --
    800/1000000: episode: 8, duration: 0.184s, episode steps: 100, steps per second: 543, episode reward: 61.503, mean reward: 0.615 [0.511, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.271, 10.247], loss: --, mae: --, mean_q: --
    900/1000000: episode: 9, duration: 0.184s, episode steps: 100, steps per second: 542, episode reward: 59.971, mean reward: 0.600 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.667, 10.098], loss: --, mae: --, mean_q: --
   1000/1000000: episode: 10, duration: 0.185s, episode steps: 100, steps per second: 539, episode reward: 59.212, mean reward: 0.592 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.479, 10.282], loss: --, mae: --, mean_q: --
   1100/1000000: episode: 11, duration: 0.186s, episode steps: 100, steps per second: 537, episode reward: 57.501, mean reward: 0.575 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.103], loss: --, mae: --, mean_q: --
   1200/1000000: episode: 12, duration: 0.184s, episode steps: 100, steps per second: 543, episode reward: 60.753, mean reward: 0.608 [0.507, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.407, 10.452], loss: --, mae: --, mean_q: --
   1300/1000000: episode: 13, duration: 0.191s, episode steps: 100, steps per second: 523, episode reward: 60.877, mean reward: 0.609 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.436, 10.314], loss: --, mae: --, mean_q: --
   1400/1000000: episode: 14, duration: 0.187s, episode steps: 100, steps per second: 536, episode reward: 63.848, mean reward: 0.638 [0.511, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.648, 10.098], loss: --, mae: --, mean_q: --
   1500/1000000: episode: 15, duration: 0.185s, episode steps: 100, steps per second: 541, episode reward: 56.375, mean reward: 0.564 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.100, 10.098], loss: --, mae: --, mean_q: --
   1600/1000000: episode: 16, duration: 0.190s, episode steps: 100, steps per second: 526, episode reward: 65.295, mean reward: 0.653 [0.525, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.414, 10.323], loss: --, mae: --, mean_q: --
   1700/1000000: episode: 17, duration: 0.186s, episode steps: 100, steps per second: 539, episode reward: 59.371, mean reward: 0.594 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.370, 10.215], loss: --, mae: --, mean_q: --
   1800/1000000: episode: 18, duration: 0.186s, episode steps: 100, steps per second: 539, episode reward: 58.469, mean reward: 0.585 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.688, 10.154], loss: --, mae: --, mean_q: --
   1900/1000000: episode: 19, duration: 0.188s, episode steps: 100, steps per second: 531, episode reward: 57.948, mean reward: 0.579 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.763, 10.485], loss: --, mae: --, mean_q: --
   2000/1000000: episode: 20, duration: 0.186s, episode steps: 100, steps per second: 537, episode reward: 57.810, mean reward: 0.578 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.762, 10.098], loss: --, mae: --, mean_q: --
   2100/1000000: episode: 21, duration: 0.186s, episode steps: 100, steps per second: 539, episode reward: 58.139, mean reward: 0.581 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.610, 10.269], loss: --, mae: --, mean_q: --
   2200/1000000: episode: 22, duration: 0.188s, episode steps: 100, steps per second: 531, episode reward: 57.942, mean reward: 0.579 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.514, 10.098], loss: --, mae: --, mean_q: --
   2300/1000000: episode: 23, duration: 0.186s, episode steps: 100, steps per second: 537, episode reward: 57.969, mean reward: 0.580 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.402, 10.239], loss: --, mae: --, mean_q: --
   2400/1000000: episode: 24, duration: 0.186s, episode steps: 100, steps per second: 537, episode reward: 59.473, mean reward: 0.595 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.193, 10.098], loss: --, mae: --, mean_q: --
   2500/1000000: episode: 25, duration: 0.189s, episode steps: 100, steps per second: 530, episode reward: 58.251, mean reward: 0.583 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.145, 10.232], loss: --, mae: --, mean_q: --
   2600/1000000: episode: 26, duration: 0.189s, episode steps: 100, steps per second: 528, episode reward: 57.829, mean reward: 0.578 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.102, 10.098], loss: --, mae: --, mean_q: --
   2700/1000000: episode: 27, duration: 0.187s, episode steps: 100, steps per second: 534, episode reward: 57.907, mean reward: 0.579 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.916, 10.193], loss: --, mae: --, mean_q: --
   2800/1000000: episode: 28, duration: 0.261s, episode steps: 100, steps per second: 383, episode reward: 60.107, mean reward: 0.601 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.345, 10.098], loss: --, mae: --, mean_q: --
   2900/1000000: episode: 29, duration: 0.264s, episode steps: 100, steps per second: 379, episode reward: 64.597, mean reward: 0.646 [0.519, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.945, 10.098], loss: --, mae: --, mean_q: --
   3000/1000000: episode: 30, duration: 0.190s, episode steps: 100, steps per second: 527, episode reward: 59.143, mean reward: 0.591 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.567, 10.098], loss: --, mae: --, mean_q: --
   3100/1000000: episode: 31, duration: 0.186s, episode steps: 100, steps per second: 538, episode reward: 58.388, mean reward: 0.584 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.820, 10.372], loss: --, mae: --, mean_q: --
   3200/1000000: episode: 32, duration: 0.187s, episode steps: 100, steps per second: 536, episode reward: 57.576, mean reward: 0.576 [0.499, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.548, 10.098], loss: --, mae: --, mean_q: --
   3300/1000000: episode: 33, duration: 0.186s, episode steps: 100, steps per second: 538, episode reward: 58.563, mean reward: 0.586 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.079, 10.147], loss: --, mae: --, mean_q: --
   3400/1000000: episode: 34, duration: 0.187s, episode steps: 100, steps per second: 534, episode reward: 57.941, mean reward: 0.579 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.566, 10.215], loss: --, mae: --, mean_q: --
   3500/1000000: episode: 35, duration: 0.187s, episode steps: 100, steps per second: 534, episode reward: 58.102, mean reward: 0.581 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.444, 10.209], loss: --, mae: --, mean_q: --
   3600/1000000: episode: 36, duration: 0.262s, episode steps: 100, steps per second: 382, episode reward: 58.367, mean reward: 0.584 [0.511, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.498, 10.275], loss: --, mae: --, mean_q: --
   3700/1000000: episode: 37, duration: 0.255s, episode steps: 100, steps per second: 393, episode reward: 58.126, mean reward: 0.581 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.787, 10.197], loss: --, mae: --, mean_q: --
   3800/1000000: episode: 38, duration: 0.247s, episode steps: 100, steps per second: 405, episode reward: 57.791, mean reward: 0.578 [0.506, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.366, 10.266], loss: --, mae: --, mean_q: --
   3900/1000000: episode: 39, duration: 0.258s, episode steps: 100, steps per second: 388, episode reward: 58.917, mean reward: 0.589 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.143, 10.278], loss: --, mae: --, mean_q: --
   4000/1000000: episode: 40, duration: 0.256s, episode steps: 100, steps per second: 390, episode reward: 57.709, mean reward: 0.577 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.893, 10.123], loss: --, mae: --, mean_q: --
   4100/1000000: episode: 41, duration: 0.256s, episode steps: 100, steps per second: 391, episode reward: 67.057, mean reward: 0.671 [0.514, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.828, 10.098], loss: --, mae: --, mean_q: --
   4200/1000000: episode: 42, duration: 0.257s, episode steps: 100, steps per second: 390, episode reward: 57.454, mean reward: 0.575 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.332, 10.313], loss: --, mae: --, mean_q: --
   4300/1000000: episode: 43, duration: 0.257s, episode steps: 100, steps per second: 389, episode reward: 63.759, mean reward: 0.638 [0.502, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.110], loss: --, mae: --, mean_q: --
   4400/1000000: episode: 44, duration: 0.264s, episode steps: 100, steps per second: 378, episode reward: 63.891, mean reward: 0.639 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.485, 10.098], loss: --, mae: --, mean_q: --
   4500/1000000: episode: 45, duration: 0.255s, episode steps: 100, steps per second: 393, episode reward: 59.895, mean reward: 0.599 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.081, 10.098], loss: --, mae: --, mean_q: --
   4600/1000000: episode: 46, duration: 0.263s, episode steps: 100, steps per second: 380, episode reward: 58.469, mean reward: 0.585 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.312, 10.239], loss: --, mae: --, mean_q: --
   4700/1000000: episode: 47, duration: 0.258s, episode steps: 100, steps per second: 388, episode reward: 57.426, mean reward: 0.574 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.176], loss: --, mae: --, mean_q: --
   4800/1000000: episode: 48, duration: 0.269s, episode steps: 100, steps per second: 372, episode reward: 58.098, mean reward: 0.581 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.385, 10.121], loss: --, mae: --, mean_q: --
   4900/1000000: episode: 49, duration: 0.255s, episode steps: 100, steps per second: 392, episode reward: 57.166, mean reward: 0.572 [0.502, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.391, 10.326], loss: --, mae: --, mean_q: --
   5000/1000000: episode: 50, duration: 0.269s, episode steps: 100, steps per second: 372, episode reward: 59.200, mean reward: 0.592 [0.508, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.259], loss: --, mae: --, mean_q: --
   5100/1000000: episode: 51, duration: 1.485s, episode steps: 100, steps per second: 67, episode reward: 61.468, mean reward: 0.615 [0.512, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.296, 10.334], loss: 0.019859, mae: 0.128004, mean_q: 0.530063
   5200/1000000: episode: 52, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 58.939, mean reward: 0.589 [0.511, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.295, 10.225], loss: 0.002474, mae: 0.053267, mean_q: 0.790274
   5300/1000000: episode: 53, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 58.464, mean reward: 0.585 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.697, 10.098], loss: 0.002425, mae: 0.052509, mean_q: 0.926856
   5400/1000000: episode: 54, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 59.994, mean reward: 0.600 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.223, 10.098], loss: 0.002844, mae: 0.054453, mean_q: 1.015966
   5500/1000000: episode: 55, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 60.497, mean reward: 0.605 [0.517, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.954, 10.333], loss: 0.002569, mae: 0.052268, mean_q: 1.082540
   5600/1000000: episode: 56, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 58.245, mean reward: 0.582 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.275], loss: 0.003109, mae: 0.055151, mean_q: 1.113348
   5700/1000000: episode: 57, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 61.444, mean reward: 0.614 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.430], loss: 0.003050, mae: 0.054520, mean_q: 1.140461
   5800/1000000: episode: 58, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 58.908, mean reward: 0.589 [0.499, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.177], loss: 0.003124, mae: 0.055148, mean_q: 1.153416
   5900/1000000: episode: 59, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.392, mean reward: 0.584 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.352, 10.098], loss: 0.003139, mae: 0.054660, mean_q: 1.160171
   6000/1000000: episode: 60, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.683, mean reward: 0.587 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.847, 10.098], loss: 0.003161, mae: 0.055537, mean_q: 1.165711
   6100/1000000: episode: 61, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 59.864, mean reward: 0.599 [0.516, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.669, 10.304], loss: 0.003148, mae: 0.054694, mean_q: 1.169528
   6200/1000000: episode: 62, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 63.348, mean reward: 0.633 [0.509, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.317, 10.317], loss: 0.003149, mae: 0.054915, mean_q: 1.171661
   6300/1000000: episode: 63, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 59.284, mean reward: 0.593 [0.513, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.098], loss: 0.003451, mae: 0.059177, mean_q: 1.178562
   6400/1000000: episode: 64, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 60.117, mean reward: 0.601 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.944, 10.098], loss: 0.003374, mae: 0.055917, mean_q: 1.174167
   6500/1000000: episode: 65, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 57.414, mean reward: 0.574 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.402, 10.137], loss: 0.002784, mae: 0.052217, mean_q: 1.175481
   6600/1000000: episode: 66, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 57.939, mean reward: 0.579 [0.499, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.981, 10.149], loss: 0.003008, mae: 0.054755, mean_q: 1.176994
   6700/1000000: episode: 67, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 57.926, mean reward: 0.579 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.983, 10.098], loss: 0.003278, mae: 0.055677, mean_q: 1.174330
   6800/1000000: episode: 68, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: 57.995, mean reward: 0.580 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.524, 10.098], loss: 0.002837, mae: 0.053329, mean_q: 1.175977
   6900/1000000: episode: 69, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 64.385, mean reward: 0.644 [0.505, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.567, 10.574], loss: 0.002863, mae: 0.052515, mean_q: 1.173717
   7000/1000000: episode: 70, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 60.450, mean reward: 0.604 [0.519, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.202, 10.211], loss: 0.002711, mae: 0.050464, mean_q: 1.175312
   7100/1000000: episode: 71, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 60.464, mean reward: 0.605 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.986, 10.098], loss: 0.002849, mae: 0.053804, mean_q: 1.177751
   7200/1000000: episode: 72, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 58.977, mean reward: 0.590 [0.509, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.515, 10.098], loss: 0.002919, mae: 0.052590, mean_q: 1.177283
   7300/1000000: episode: 73, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 60.766, mean reward: 0.608 [0.510, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.800, 10.098], loss: 0.003516, mae: 0.057518, mean_q: 1.176533
   7400/1000000: episode: 74, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 59.350, mean reward: 0.594 [0.511, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.098], loss: 0.002965, mae: 0.054638, mean_q: 1.178144
   7500/1000000: episode: 75, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 59.147, mean reward: 0.591 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.264, 10.098], loss: 0.003096, mae: 0.054589, mean_q: 1.178143
   7600/1000000: episode: 76, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 61.434, mean reward: 0.614 [0.516, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.103, 10.110], loss: 0.003094, mae: 0.054062, mean_q: 1.179493
   7700/1000000: episode: 77, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 57.681, mean reward: 0.577 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.783, 10.234], loss: 0.002971, mae: 0.053759, mean_q: 1.184084
   7800/1000000: episode: 78, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.122, mean reward: 0.581 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.609, 10.140], loss: 0.002904, mae: 0.052727, mean_q: 1.182575
   7900/1000000: episode: 79, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 61.923, mean reward: 0.619 [0.504, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.765, 10.098], loss: 0.003103, mae: 0.053711, mean_q: 1.178203
   8000/1000000: episode: 80, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: 58.740, mean reward: 0.587 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.540, 10.334], loss: 0.003011, mae: 0.053468, mean_q: 1.179398
   8100/1000000: episode: 81, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 57.613, mean reward: 0.576 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.993, 10.218], loss: 0.003217, mae: 0.055977, mean_q: 1.178448
   8200/1000000: episode: 82, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 59.470, mean reward: 0.595 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.785, 10.191], loss: 0.002752, mae: 0.051764, mean_q: 1.179399
   8300/1000000: episode: 83, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 60.467, mean reward: 0.605 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.026, 10.116], loss: 0.003010, mae: 0.053683, mean_q: 1.178238
   8400/1000000: episode: 84, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 62.321, mean reward: 0.623 [0.520, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.067, 10.250], loss: 0.002963, mae: 0.053613, mean_q: 1.179980
   8500/1000000: episode: 85, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 58.637, mean reward: 0.586 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.247], loss: 0.003012, mae: 0.054545, mean_q: 1.178760
   8600/1000000: episode: 86, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 60.905, mean reward: 0.609 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.500, 10.098], loss: 0.003211, mae: 0.055525, mean_q: 1.181727
   8700/1000000: episode: 87, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 57.994, mean reward: 0.580 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.418, 10.098], loss: 0.002994, mae: 0.053862, mean_q: 1.180049
   8800/1000000: episode: 88, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 56.747, mean reward: 0.567 [0.497, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.569, 10.143], loss: 0.003088, mae: 0.054571, mean_q: 1.181438
   8900/1000000: episode: 89, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 61.871, mean reward: 0.619 [0.503, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.529, 10.098], loss: 0.002896, mae: 0.053594, mean_q: 1.183244
   9000/1000000: episode: 90, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 63.206, mean reward: 0.632 [0.509, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.478, 10.318], loss: 0.002865, mae: 0.053513, mean_q: 1.182288
   9100/1000000: episode: 91, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.087, mean reward: 0.581 [0.500, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.223], loss: 0.002776, mae: 0.053118, mean_q: 1.182164
   9200/1000000: episode: 92, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 60.879, mean reward: 0.609 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.986, 10.405], loss: 0.002903, mae: 0.053459, mean_q: 1.181068
   9300/1000000: episode: 93, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 57.294, mean reward: 0.573 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.221], loss: 0.002849, mae: 0.052926, mean_q: 1.181271
   9400/1000000: episode: 94, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 59.270, mean reward: 0.593 [0.507, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.106, 10.098], loss: 0.002919, mae: 0.053225, mean_q: 1.178208
   9500/1000000: episode: 95, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 58.520, mean reward: 0.585 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.594, 10.098], loss: 0.002538, mae: 0.050381, mean_q: 1.178071
   9600/1000000: episode: 96, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 62.398, mean reward: 0.624 [0.516, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.492], loss: 0.002482, mae: 0.050566, mean_q: 1.178998
   9700/1000000: episode: 97, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 57.718, mean reward: 0.577 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.114], loss: 0.003374, mae: 0.058347, mean_q: 1.176836
   9800/1000000: episode: 98, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 56.892, mean reward: 0.569 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.555, 10.168], loss: 0.002634, mae: 0.052074, mean_q: 1.177388
   9900/1000000: episode: 99, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 64.308, mean reward: 0.643 [0.505, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.731, 10.291], loss: 0.002916, mae: 0.054063, mean_q: 1.177207
  10000/1000000: episode: 100, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 58.569, mean reward: 0.586 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.795, 10.268], loss: 0.002992, mae: 0.055361, mean_q: 1.179253
  10100/1000000: episode: 101, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: 57.484, mean reward: 0.575 [0.512, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.362, 10.165], loss: 0.002763, mae: 0.052889, mean_q: 1.177902
  10200/1000000: episode: 102, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 60.545, mean reward: 0.605 [0.525, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.098], loss: 0.002475, mae: 0.050335, mean_q: 1.179028
  10300/1000000: episode: 103, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 61.863, mean reward: 0.619 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.643, 10.098], loss: 0.002526, mae: 0.051026, mean_q: 1.179897
  10400/1000000: episode: 104, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 58.413, mean reward: 0.584 [0.500, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.541, 10.098], loss: 0.002393, mae: 0.049812, mean_q: 1.183586
  10500/1000000: episode: 105, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 58.139, mean reward: 0.581 [0.509, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.129], loss: 0.002679, mae: 0.051524, mean_q: 1.181233
  10600/1000000: episode: 106, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 58.380, mean reward: 0.584 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.098], loss: 0.002468, mae: 0.051258, mean_q: 1.182060
  10700/1000000: episode: 107, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 57.357, mean reward: 0.574 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.669, 10.098], loss: 0.002722, mae: 0.052944, mean_q: 1.178611
  10800/1000000: episode: 108, duration: 1.347s, episode steps: 100, steps per second: 74, episode reward: 57.761, mean reward: 0.578 [0.500, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.083, 10.168], loss: 0.002313, mae: 0.049862, mean_q: 1.181190
  10900/1000000: episode: 109, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 61.632, mean reward: 0.616 [0.510, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.581, 10.098], loss: 0.002528, mae: 0.051806, mean_q: 1.181784
  11000/1000000: episode: 110, duration: 1.412s, episode steps: 100, steps per second: 71, episode reward: 58.452, mean reward: 0.585 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.350], loss: 0.002579, mae: 0.051958, mean_q: 1.180545
  11100/1000000: episode: 111, duration: 1.622s, episode steps: 100, steps per second: 62, episode reward: 57.897, mean reward: 0.579 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.164, 10.429], loss: 0.002560, mae: 0.052299, mean_q: 1.180176
  11200/1000000: episode: 112, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: 60.298, mean reward: 0.603 [0.509, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.861, 10.381], loss: 0.002451, mae: 0.050817, mean_q: 1.177469
  11300/1000000: episode: 113, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 63.399, mean reward: 0.634 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.804, 10.296], loss: 0.002384, mae: 0.050127, mean_q: 1.178915
  11400/1000000: episode: 114, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.305, mean reward: 0.583 [0.498, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.794, 10.098], loss: 0.002459, mae: 0.051367, mean_q: 1.178274
  11500/1000000: episode: 115, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 59.283, mean reward: 0.593 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.033, 10.313], loss: 0.002420, mae: 0.051137, mean_q: 1.176907
  11600/1000000: episode: 116, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 57.442, mean reward: 0.574 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.791, 10.098], loss: 0.002452, mae: 0.051461, mean_q: 1.174709
  11700/1000000: episode: 117, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 60.402, mean reward: 0.604 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.567, 10.098], loss: 0.002289, mae: 0.049842, mean_q: 1.179573
  11800/1000000: episode: 118, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 56.456, mean reward: 0.565 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.670, 10.144], loss: 0.002372, mae: 0.051315, mean_q: 1.178402
  11900/1000000: episode: 119, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 60.847, mean reward: 0.608 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.069, 10.098], loss: 0.002313, mae: 0.050881, mean_q: 1.180467
  12000/1000000: episode: 120, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 60.489, mean reward: 0.605 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.080, 10.098], loss: 0.002184, mae: 0.049551, mean_q: 1.174787
  12100/1000000: episode: 121, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 56.584, mean reward: 0.566 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.813, 10.197], loss: 0.002293, mae: 0.050664, mean_q: 1.178224
  12200/1000000: episode: 122, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 56.679, mean reward: 0.567 [0.503, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.115], loss: 0.002035, mae: 0.048110, mean_q: 1.175037
  12300/1000000: episode: 123, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 61.033, mean reward: 0.610 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.900, 10.098], loss: 0.002025, mae: 0.047728, mean_q: 1.175462
  12400/1000000: episode: 124, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 57.727, mean reward: 0.577 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.133], loss: 0.002134, mae: 0.048884, mean_q: 1.175120
  12500/1000000: episode: 125, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 61.155, mean reward: 0.612 [0.506, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.572, 10.098], loss: 0.002255, mae: 0.050586, mean_q: 1.174190
  12600/1000000: episode: 126, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 58.212, mean reward: 0.582 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.276, 10.098], loss: 0.002149, mae: 0.049131, mean_q: 1.174102
  12700/1000000: episode: 127, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 58.221, mean reward: 0.582 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.844, 10.101], loss: 0.002168, mae: 0.048963, mean_q: 1.172175
  12800/1000000: episode: 128, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 61.097, mean reward: 0.611 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.098], loss: 0.002005, mae: 0.047808, mean_q: 1.174604
  12900/1000000: episode: 129, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 61.151, mean reward: 0.612 [0.511, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.010, 10.098], loss: 0.002092, mae: 0.048583, mean_q: 1.174711
  13000/1000000: episode: 130, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 58.050, mean reward: 0.581 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.154, 10.098], loss: 0.002067, mae: 0.048912, mean_q: 1.175736
  13100/1000000: episode: 131, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 57.728, mean reward: 0.577 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.425, 10.098], loss: 0.002061, mae: 0.048938, mean_q: 1.176213
  13200/1000000: episode: 132, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.273, mean reward: 0.593 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.709, 10.187], loss: 0.001876, mae: 0.047101, mean_q: 1.176034
  13300/1000000: episode: 133, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 58.385, mean reward: 0.584 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.098], loss: 0.002307, mae: 0.050723, mean_q: 1.170911
  13400/1000000: episode: 134, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 58.860, mean reward: 0.589 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.480, 10.216], loss: 0.001956, mae: 0.047874, mean_q: 1.174360
  13500/1000000: episode: 135, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 58.867, mean reward: 0.589 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.048, 10.098], loss: 0.001885, mae: 0.047472, mean_q: 1.171121
  13600/1000000: episode: 136, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 61.274, mean reward: 0.613 [0.506, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.078, 10.255], loss: 0.001941, mae: 0.047865, mean_q: 1.173022
  13700/1000000: episode: 137, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 58.318, mean reward: 0.583 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.483, 10.156], loss: 0.001900, mae: 0.047387, mean_q: 1.176530
  13800/1000000: episode: 138, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 58.917, mean reward: 0.589 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.367, 10.098], loss: 0.001931, mae: 0.048097, mean_q: 1.172979
  13900/1000000: episode: 139, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 58.423, mean reward: 0.584 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.353, 10.132], loss: 0.001879, mae: 0.047246, mean_q: 1.172215
  14000/1000000: episode: 140, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 58.628, mean reward: 0.586 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.567, 10.098], loss: 0.001798, mae: 0.046734, mean_q: 1.173053
  14100/1000000: episode: 141, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 58.441, mean reward: 0.584 [0.501, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.165], loss: 0.001822, mae: 0.046590, mean_q: 1.171288
  14200/1000000: episode: 142, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 59.291, mean reward: 0.593 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.098], loss: 0.001813, mae: 0.045979, mean_q: 1.170717
  14300/1000000: episode: 143, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 57.966, mean reward: 0.580 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.630, 10.098], loss: 0.001878, mae: 0.047367, mean_q: 1.173218
  14400/1000000: episode: 144, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 57.859, mean reward: 0.579 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.488, 10.098], loss: 0.001797, mae: 0.046385, mean_q: 1.169597
  14500/1000000: episode: 145, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 58.519, mean reward: 0.585 [0.514, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.867, 10.184], loss: 0.001994, mae: 0.048614, mean_q: 1.169881
  14600/1000000: episode: 146, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 57.024, mean reward: 0.570 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.369, 10.163], loss: 0.001788, mae: 0.045641, mean_q: 1.168778
  14700/1000000: episode: 147, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 57.545, mean reward: 0.575 [0.503, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.448, 10.200], loss: 0.001863, mae: 0.046425, mean_q: 1.168670
  14800/1000000: episode: 148, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 60.156, mean reward: 0.602 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.098], loss: 0.001913, mae: 0.047233, mean_q: 1.173113
  14900/1000000: episode: 149, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 57.220, mean reward: 0.572 [0.508, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.500, 10.098], loss: 0.001747, mae: 0.045935, mean_q: 1.171488
  15000/1000000: episode: 150, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 57.583, mean reward: 0.576 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.725, 10.178], loss: 0.002025, mae: 0.048583, mean_q: 1.170857
  15100/1000000: episode: 151, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 59.890, mean reward: 0.599 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.642, 10.098], loss: 0.001801, mae: 0.045463, mean_q: 1.166963
  15200/1000000: episode: 152, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 57.312, mean reward: 0.573 [0.500, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.098], loss: 0.001884, mae: 0.046847, mean_q: 1.166272
  15300/1000000: episode: 153, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 59.380, mean reward: 0.594 [0.517, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.528, 10.098], loss: 0.001684, mae: 0.045102, mean_q: 1.167352
  15400/1000000: episode: 154, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 62.447, mean reward: 0.624 [0.506, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.773, 10.234], loss: 0.001700, mae: 0.045363, mean_q: 1.167710
  15500/1000000: episode: 155, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 58.531, mean reward: 0.585 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.803, 10.098], loss: 0.001682, mae: 0.044988, mean_q: 1.168607
  15600/1000000: episode: 156, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 59.041, mean reward: 0.590 [0.508, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.693, 10.217], loss: 0.002048, mae: 0.048906, mean_q: 1.168393
  15700/1000000: episode: 157, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 59.725, mean reward: 0.597 [0.517, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.650, 10.233], loss: 0.001816, mae: 0.045944, mean_q: 1.167958
  15800/1000000: episode: 158, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 58.326, mean reward: 0.583 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.098], loss: 0.001829, mae: 0.047363, mean_q: 1.168003
  15900/1000000: episode: 159, duration: 0.727s, episode steps: 100, steps per second: 137, episode reward: 58.823, mean reward: 0.588 [0.508, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.860, 10.098], loss: 0.001731, mae: 0.045274, mean_q: 1.166732
  16000/1000000: episode: 160, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 61.191, mean reward: 0.612 [0.513, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.222, 10.098], loss: 0.001752, mae: 0.046360, mean_q: 1.168806
  16100/1000000: episode: 161, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 60.665, mean reward: 0.607 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.361, 10.098], loss: 0.001744, mae: 0.045996, mean_q: 1.170520
  16200/1000000: episode: 162, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 60.926, mean reward: 0.609 [0.506, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.098], loss: 0.001749, mae: 0.045493, mean_q: 1.171220
  16300/1000000: episode: 163, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 58.802, mean reward: 0.588 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.119, 10.098], loss: 0.001715, mae: 0.045476, mean_q: 1.167148
  16400/1000000: episode: 164, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 57.488, mean reward: 0.575 [0.502, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.536, 10.098], loss: 0.001757, mae: 0.045419, mean_q: 1.166853
  16500/1000000: episode: 165, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 57.975, mean reward: 0.580 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.413, 10.266], loss: 0.001740, mae: 0.046261, mean_q: 1.166912
  16600/1000000: episode: 166, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 61.409, mean reward: 0.614 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.944, 10.151], loss: 0.001829, mae: 0.046186, mean_q: 1.167635
  16700/1000000: episode: 167, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 61.483, mean reward: 0.615 [0.506, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.274], loss: 0.001616, mae: 0.043700, mean_q: 1.162947
  16800/1000000: episode: 168, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.564, mean reward: 0.586 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.711, 10.259], loss: 0.001851, mae: 0.046855, mean_q: 1.170488
  16900/1000000: episode: 169, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 56.413, mean reward: 0.564 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.098], loss: 0.001749, mae: 0.045921, mean_q: 1.169695
  17000/1000000: episode: 170, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 58.184, mean reward: 0.582 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.211, 10.157], loss: 0.001804, mae: 0.046462, mean_q: 1.167941
  17100/1000000: episode: 171, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 56.987, mean reward: 0.570 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.718, 10.118], loss: 0.001716, mae: 0.044988, mean_q: 1.167809
  17200/1000000: episode: 172, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 59.023, mean reward: 0.590 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.798, 10.207], loss: 0.001777, mae: 0.045643, mean_q: 1.168738
  17300/1000000: episode: 173, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 56.946, mean reward: 0.569 [0.497, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.137], loss: 0.001691, mae: 0.044942, mean_q: 1.167547
  17400/1000000: episode: 174, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 60.941, mean reward: 0.609 [0.516, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.350, 10.181], loss: 0.001765, mae: 0.046287, mean_q: 1.169342
  17500/1000000: episode: 175, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 59.588, mean reward: 0.596 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.030, 10.253], loss: 0.001663, mae: 0.044410, mean_q: 1.168556
  17600/1000000: episode: 176, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 61.387, mean reward: 0.614 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.682, 10.381], loss: 0.001639, mae: 0.044061, mean_q: 1.166236
  17700/1000000: episode: 177, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 58.171, mean reward: 0.582 [0.506, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.098], loss: 0.001896, mae: 0.047592, mean_q: 1.167036
  17800/1000000: episode: 178, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.788, mean reward: 0.608 [0.511, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.896, 10.145], loss: 0.001764, mae: 0.045884, mean_q: 1.166914
  17900/1000000: episode: 179, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 59.189, mean reward: 0.592 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.739, 10.156], loss: 0.001706, mae: 0.045217, mean_q: 1.165715
  18000/1000000: episode: 180, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 60.768, mean reward: 0.608 [0.509, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.484, 10.238], loss: 0.001642, mae: 0.044503, mean_q: 1.170112
  18100/1000000: episode: 181, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 59.892, mean reward: 0.599 [0.519, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.024, 10.098], loss: 0.001721, mae: 0.044969, mean_q: 1.168328
  18200/1000000: episode: 182, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 60.325, mean reward: 0.603 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.433, 10.098], loss: 0.001688, mae: 0.045154, mean_q: 1.170832
  18300/1000000: episode: 183, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 58.083, mean reward: 0.581 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.116, 10.143], loss: 0.001666, mae: 0.044693, mean_q: 1.169647
  18400/1000000: episode: 184, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 59.529, mean reward: 0.595 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.807, 10.099], loss: 0.001830, mae: 0.046231, mean_q: 1.167645
  18500/1000000: episode: 185, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 57.726, mean reward: 0.577 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.654, 10.130], loss: 0.001855, mae: 0.046493, mean_q: 1.166725
  18600/1000000: episode: 186, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 57.652, mean reward: 0.577 [0.510, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.573, 10.098], loss: 0.001604, mae: 0.044079, mean_q: 1.170422
  18700/1000000: episode: 187, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 58.255, mean reward: 0.583 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.354, 10.193], loss: 0.001677, mae: 0.045107, mean_q: 1.168000
  18800/1000000: episode: 188, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 57.001, mean reward: 0.570 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.162], loss: 0.001649, mae: 0.044486, mean_q: 1.166348
  18900/1000000: episode: 189, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 59.761, mean reward: 0.598 [0.513, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 0.001877, mae: 0.047068, mean_q: 1.166330
  19000/1000000: episode: 190, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 57.028, mean reward: 0.570 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.623, 10.098], loss: 0.001623, mae: 0.043899, mean_q: 1.164399
  19100/1000000: episode: 191, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 60.128, mean reward: 0.601 [0.512, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.738, 10.098], loss: 0.001617, mae: 0.044214, mean_q: 1.164322
  19200/1000000: episode: 192, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 61.112, mean reward: 0.611 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.120, 10.098], loss: 0.001689, mae: 0.045536, mean_q: 1.169715
  19300/1000000: episode: 193, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 57.543, mean reward: 0.575 [0.503, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.457, 10.101], loss: 0.001613, mae: 0.044389, mean_q: 1.166198
  19400/1000000: episode: 194, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.824, mean reward: 0.578 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.208], loss: 0.001709, mae: 0.044707, mean_q: 1.166281
  19500/1000000: episode: 195, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.236, mean reward: 0.592 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.127], loss: 0.001582, mae: 0.043290, mean_q: 1.170486
  19600/1000000: episode: 196, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 59.413, mean reward: 0.594 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.953, 10.207], loss: 0.001577, mae: 0.043806, mean_q: 1.167802
  19700/1000000: episode: 197, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 58.277, mean reward: 0.583 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.612, 10.201], loss: 0.001646, mae: 0.044669, mean_q: 1.168349
  19800/1000000: episode: 198, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 60.691, mean reward: 0.607 [0.520, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.225], loss: 0.001675, mae: 0.044271, mean_q: 1.167795
  19900/1000000: episode: 199, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 60.614, mean reward: 0.606 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.909, 10.098], loss: 0.001637, mae: 0.044849, mean_q: 1.173244
  20000/1000000: episode: 200, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 59.050, mean reward: 0.591 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.484, 10.105], loss: 0.001682, mae: 0.045038, mean_q: 1.168464
  20100/1000000: episode: 201, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 59.484, mean reward: 0.595 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.233, 10.289], loss: 0.001575, mae: 0.044193, mean_q: 1.169739
  20200/1000000: episode: 202, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 62.833, mean reward: 0.628 [0.501, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.176, 10.289], loss: 0.001489, mae: 0.043051, mean_q: 1.169268
  20300/1000000: episode: 203, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 61.756, mean reward: 0.618 [0.509, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.444, 10.488], loss: 0.001629, mae: 0.044194, mean_q: 1.172714
  20400/1000000: episode: 204, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 58.412, mean reward: 0.584 [0.510, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.212, 10.284], loss: 0.001844, mae: 0.046511, mean_q: 1.172920
  20500/1000000: episode: 205, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 61.642, mean reward: 0.616 [0.498, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.154, 10.098], loss: 0.001782, mae: 0.046318, mean_q: 1.172573
  20600/1000000: episode: 206, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 57.938, mean reward: 0.579 [0.514, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.122], loss: 0.001803, mae: 0.046533, mean_q: 1.176927
  20700/1000000: episode: 207, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 60.108, mean reward: 0.601 [0.502, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.517, 10.107], loss: 0.001801, mae: 0.046203, mean_q: 1.172705
  20800/1000000: episode: 208, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 59.610, mean reward: 0.596 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.230, 10.273], loss: 0.001751, mae: 0.046258, mean_q: 1.173334
  20900/1000000: episode: 209, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: 62.213, mean reward: 0.622 [0.510, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.574, 10.203], loss: 0.001633, mae: 0.044286, mean_q: 1.175198
  21000/1000000: episode: 210, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 58.074, mean reward: 0.581 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.306, 10.098], loss: 0.001656, mae: 0.044942, mean_q: 1.174137
  21100/1000000: episode: 211, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 56.902, mean reward: 0.569 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.499, 10.129], loss: 0.001697, mae: 0.045512, mean_q: 1.172566
  21200/1000000: episode: 212, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 62.752, mean reward: 0.628 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.967, 10.245], loss: 0.001581, mae: 0.044517, mean_q: 1.174093
  21300/1000000: episode: 213, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 61.344, mean reward: 0.613 [0.506, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.543], loss: 0.001654, mae: 0.045538, mean_q: 1.173982
  21400/1000000: episode: 214, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 60.640, mean reward: 0.606 [0.508, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.794, 10.264], loss: 0.001730, mae: 0.045781, mean_q: 1.175042
  21500/1000000: episode: 215, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.407, mean reward: 0.584 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.656, 10.098], loss: 0.001851, mae: 0.047090, mean_q: 1.176060
  21600/1000000: episode: 216, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 60.592, mean reward: 0.606 [0.514, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.605, 10.098], loss: 0.001928, mae: 0.047870, mean_q: 1.174984
  21700/1000000: episode: 217, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 57.433, mean reward: 0.574 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.092, 10.283], loss: 0.002031, mae: 0.048776, mean_q: 1.178614
  21800/1000000: episode: 218, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 59.731, mean reward: 0.597 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.113], loss: 0.001938, mae: 0.047986, mean_q: 1.174297
  21900/1000000: episode: 219, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 57.933, mean reward: 0.579 [0.509, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.789, 10.098], loss: 0.001853, mae: 0.046707, mean_q: 1.180087
  22000/1000000: episode: 220, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 59.037, mean reward: 0.590 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.202, 10.098], loss: 0.001860, mae: 0.047503, mean_q: 1.176894
  22100/1000000: episode: 221, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 56.279, mean reward: 0.563 [0.509, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.098], loss: 0.001977, mae: 0.048279, mean_q: 1.176956
  22200/1000000: episode: 222, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 58.051, mean reward: 0.581 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.201, 10.098], loss: 0.001805, mae: 0.046635, mean_q: 1.176283
  22300/1000000: episode: 223, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 58.059, mean reward: 0.581 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.277, 10.201], loss: 0.001752, mae: 0.046290, mean_q: 1.174403
  22400/1000000: episode: 224, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 62.089, mean reward: 0.621 [0.515, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.578, 10.451], loss: 0.001697, mae: 0.045143, mean_q: 1.173642
  22500/1000000: episode: 225, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 58.370, mean reward: 0.584 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.098], loss: 0.001933, mae: 0.048017, mean_q: 1.177396
  22600/1000000: episode: 226, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 60.121, mean reward: 0.601 [0.513, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.930, 10.142], loss: 0.001946, mae: 0.048099, mean_q: 1.179027
  22700/1000000: episode: 227, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 59.179, mean reward: 0.592 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.114, 10.098], loss: 0.001858, mae: 0.045910, mean_q: 1.177409
  22800/1000000: episode: 228, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 58.444, mean reward: 0.584 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.219], loss: 0.001836, mae: 0.045945, mean_q: 1.176995
  22900/1000000: episode: 229, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 57.924, mean reward: 0.579 [0.509, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.098], loss: 0.001850, mae: 0.046295, mean_q: 1.176077
  23000/1000000: episode: 230, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 60.409, mean reward: 0.604 [0.514, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.671, 10.342], loss: 0.001682, mae: 0.044867, mean_q: 1.173231
  23100/1000000: episode: 231, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 56.839, mean reward: 0.568 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.234, 10.098], loss: 0.001736, mae: 0.045840, mean_q: 1.176630
  23200/1000000: episode: 232, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 58.697, mean reward: 0.587 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.996, 10.284], loss: 0.001750, mae: 0.045897, mean_q: 1.177337
  23300/1000000: episode: 233, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: 58.236, mean reward: 0.582 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.876, 10.098], loss: 0.001573, mae: 0.043626, mean_q: 1.172034
  23400/1000000: episode: 234, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 58.060, mean reward: 0.581 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.656, 10.260], loss: 0.001792, mae: 0.046034, mean_q: 1.172776
  23500/1000000: episode: 235, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.557, mean reward: 0.606 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.752, 10.445], loss: 0.001624, mae: 0.044124, mean_q: 1.169726
  23600/1000000: episode: 236, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 62.891, mean reward: 0.629 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.493, 10.297], loss: 0.001564, mae: 0.043450, mean_q: 1.173949
  23700/1000000: episode: 237, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.724, mean reward: 0.587 [0.502, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.579, 10.107], loss: 0.001774, mae: 0.046599, mean_q: 1.175672
  23800/1000000: episode: 238, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 60.407, mean reward: 0.604 [0.503, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.650, 10.098], loss: 0.001712, mae: 0.045887, mean_q: 1.175080
  23900/1000000: episode: 239, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 58.294, mean reward: 0.583 [0.498, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.553, 10.186], loss: 0.001670, mae: 0.045079, mean_q: 1.173636
  24000/1000000: episode: 240, duration: 0.727s, episode steps: 100, steps per second: 137, episode reward: 57.452, mean reward: 0.575 [0.503, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.117], loss: 0.001657, mae: 0.044955, mean_q: 1.177586
  24100/1000000: episode: 241, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: 57.931, mean reward: 0.579 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.098], loss: 0.001726, mae: 0.045638, mean_q: 1.175288
  24200/1000000: episode: 242, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 60.122, mean reward: 0.601 [0.516, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.910, 10.176], loss: 0.001709, mae: 0.045404, mean_q: 1.177092
  24300/1000000: episode: 243, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 59.217, mean reward: 0.592 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.098], loss: 0.001981, mae: 0.046230, mean_q: 1.175969
  24400/1000000: episode: 244, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 62.415, mean reward: 0.624 [0.513, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.303, 10.098], loss: 0.001692, mae: 0.044924, mean_q: 1.176705
  24500/1000000: episode: 245, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 57.515, mean reward: 0.575 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.211, 10.098], loss: 0.001609, mae: 0.043651, mean_q: 1.176683
  24600/1000000: episode: 246, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 58.629, mean reward: 0.586 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.554, 10.200], loss: 0.001786, mae: 0.046217, mean_q: 1.176865
  24700/1000000: episode: 247, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 56.608, mean reward: 0.566 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.098], loss: 0.001893, mae: 0.046290, mean_q: 1.174855
  24800/1000000: episode: 248, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 57.644, mean reward: 0.576 [0.504, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.494, 10.234], loss: 0.001807, mae: 0.046079, mean_q: 1.173971
  24900/1000000: episode: 249, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 57.902, mean reward: 0.579 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.280, 10.098], loss: 0.001752, mae: 0.045690, mean_q: 1.173627
  25000/1000000: episode: 250, duration: 0.687s, episode steps: 100, steps per second: 145, episode reward: 58.546, mean reward: 0.585 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.172, 10.098], loss: 0.001718, mae: 0.044683, mean_q: 1.173175
  25100/1000000: episode: 251, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 57.702, mean reward: 0.577 [0.519, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.737, 10.126], loss: 0.001977, mae: 0.047598, mean_q: 1.177582
  25200/1000000: episode: 252, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 61.156, mean reward: 0.612 [0.501, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.098], loss: 0.001736, mae: 0.044775, mean_q: 1.171641
  25300/1000000: episode: 253, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 57.513, mean reward: 0.575 [0.515, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.785, 10.168], loss: 0.001881, mae: 0.045527, mean_q: 1.169517
  25400/1000000: episode: 254, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 58.191, mean reward: 0.582 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.575, 10.135], loss: 0.001760, mae: 0.045109, mean_q: 1.169214
  25500/1000000: episode: 255, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.459, mean reward: 0.585 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.378, 10.098], loss: 0.001673, mae: 0.044551, mean_q: 1.170977
  25600/1000000: episode: 256, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 61.278, mean reward: 0.613 [0.519, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.098], loss: 0.001649, mae: 0.044442, mean_q: 1.169341
  25700/1000000: episode: 257, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 58.380, mean reward: 0.584 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.169, 10.098], loss: 0.001530, mae: 0.042685, mean_q: 1.166476
  25800/1000000: episode: 258, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 57.157, mean reward: 0.572 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.204, 10.098], loss: 0.001659, mae: 0.044333, mean_q: 1.170261
  25900/1000000: episode: 259, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 57.821, mean reward: 0.578 [0.498, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.195], loss: 0.001703, mae: 0.044709, mean_q: 1.166557
  26000/1000000: episode: 260, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.871, mean reward: 0.589 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.142], loss: 0.001618, mae: 0.043635, mean_q: 1.167181
  26100/1000000: episode: 261, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 60.000, mean reward: 0.600 [0.510, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.591, 10.292], loss: 0.001639, mae: 0.044229, mean_q: 1.168822
  26200/1000000: episode: 262, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.140, mean reward: 0.571 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.250, 10.174], loss: 0.001659, mae: 0.043942, mean_q: 1.165918
  26300/1000000: episode: 263, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 63.053, mean reward: 0.631 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.440, 10.098], loss: 0.001639, mae: 0.044272, mean_q: 1.165775
  26400/1000000: episode: 264, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 60.571, mean reward: 0.606 [0.507, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.837, 10.389], loss: 0.001848, mae: 0.045175, mean_q: 1.164119
  26500/1000000: episode: 265, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 59.864, mean reward: 0.599 [0.509, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.238, 10.098], loss: 0.001587, mae: 0.042597, mean_q: 1.163463
  26600/1000000: episode: 266, duration: 0.816s, episode steps: 100, steps per second: 122, episode reward: 57.009, mean reward: 0.570 [0.508, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.405, 10.098], loss: 0.001576, mae: 0.043410, mean_q: 1.167498
  26700/1000000: episode: 267, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 58.338, mean reward: 0.583 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.202, 10.253], loss: 0.001539, mae: 0.042630, mean_q: 1.160698
  26800/1000000: episode: 268, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 58.607, mean reward: 0.586 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.661, 10.098], loss: 0.001553, mae: 0.042861, mean_q: 1.165666
  26900/1000000: episode: 269, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 59.556, mean reward: 0.596 [0.512, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.021, 10.098], loss: 0.001641, mae: 0.043939, mean_q: 1.166424
  27000/1000000: episode: 270, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 58.364, mean reward: 0.584 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.689, 10.163], loss: 0.001610, mae: 0.043425, mean_q: 1.163918
  27100/1000000: episode: 271, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.514, mean reward: 0.575 [0.498, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.331, 10.098], loss: 0.001529, mae: 0.042344, mean_q: 1.163152
  27200/1000000: episode: 272, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 58.385, mean reward: 0.584 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.377, 10.098], loss: 0.001574, mae: 0.043160, mean_q: 1.162802
  27300/1000000: episode: 273, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.956, mean reward: 0.580 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.660, 10.119], loss: 0.001659, mae: 0.044190, mean_q: 1.165017
  27400/1000000: episode: 274, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.821, mean reward: 0.588 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.233, 10.233], loss: 0.001479, mae: 0.042662, mean_q: 1.166296
  27500/1000000: episode: 275, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 59.108, mean reward: 0.591 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.655, 10.308], loss: 0.001571, mae: 0.042973, mean_q: 1.164517
  27600/1000000: episode: 276, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.448, mean reward: 0.584 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.041, 10.098], loss: 0.001523, mae: 0.042458, mean_q: 1.166248
  27700/1000000: episode: 277, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 57.896, mean reward: 0.579 [0.507, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.098], loss: 0.001550, mae: 0.043242, mean_q: 1.161307
  27800/1000000: episode: 278, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.600, mean reward: 0.586 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.025, 10.226], loss: 0.001714, mae: 0.043941, mean_q: 1.163928
  27900/1000000: episode: 279, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 59.219, mean reward: 0.592 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.403, 10.211], loss: 0.001566, mae: 0.043611, mean_q: 1.167365
  28000/1000000: episode: 280, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 64.391, mean reward: 0.644 [0.513, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.411], loss: 0.001674, mae: 0.044415, mean_q: 1.164945
  28100/1000000: episode: 281, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 59.380, mean reward: 0.594 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.969, 10.098], loss: 0.001665, mae: 0.044610, mean_q: 1.166831
  28200/1000000: episode: 282, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 58.901, mean reward: 0.589 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.992, 10.098], loss: 0.001538, mae: 0.043110, mean_q: 1.165776
  28300/1000000: episode: 283, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 60.211, mean reward: 0.602 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.098], loss: 0.001602, mae: 0.044045, mean_q: 1.168589
  28400/1000000: episode: 284, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 61.329, mean reward: 0.613 [0.499, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.858, 10.098], loss: 0.001527, mae: 0.043359, mean_q: 1.167003
  28500/1000000: episode: 285, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 60.454, mean reward: 0.605 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.168, 10.292], loss: 0.001617, mae: 0.043934, mean_q: 1.169785
  28600/1000000: episode: 286, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.930, mean reward: 0.579 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.310, 10.197], loss: 0.001798, mae: 0.045300, mean_q: 1.170186
  28700/1000000: episode: 287, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 56.845, mean reward: 0.568 [0.503, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.098], loss: 0.001589, mae: 0.043621, mean_q: 1.166028
  28800/1000000: episode: 288, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 57.091, mean reward: 0.571 [0.501, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.051, 10.155], loss: 0.001630, mae: 0.044208, mean_q: 1.165635
  28900/1000000: episode: 289, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 60.637, mean reward: 0.606 [0.501, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.229], loss: 0.001601, mae: 0.043402, mean_q: 1.163501
  29000/1000000: episode: 290, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 61.302, mean reward: 0.613 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.382, 10.249], loss: 0.001607, mae: 0.043755, mean_q: 1.161607
  29100/1000000: episode: 291, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.007, mean reward: 0.580 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.786, 10.098], loss: 0.001609, mae: 0.043981, mean_q: 1.168442
  29200/1000000: episode: 292, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 58.333, mean reward: 0.583 [0.500, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.730, 10.098], loss: 0.001617, mae: 0.043690, mean_q: 1.164930
  29300/1000000: episode: 293, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 61.641, mean reward: 0.616 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.829, 10.239], loss: 0.001725, mae: 0.045844, mean_q: 1.164979
  29400/1000000: episode: 294, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 57.217, mean reward: 0.572 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.945, 10.098], loss: 0.001554, mae: 0.043630, mean_q: 1.163456
  29500/1000000: episode: 295, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 58.795, mean reward: 0.588 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.098], loss: 0.001508, mae: 0.042999, mean_q: 1.167501
  29600/1000000: episode: 296, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 57.630, mean reward: 0.576 [0.510, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.954, 10.345], loss: 0.001707, mae: 0.043717, mean_q: 1.166775
  29700/1000000: episode: 297, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 60.601, mean reward: 0.606 [0.500, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.375, 10.314], loss: 0.001525, mae: 0.042731, mean_q: 1.165784
  29800/1000000: episode: 298, duration: 0.702s, episode steps: 100, steps per second: 143, episode reward: 56.760, mean reward: 0.568 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.250], loss: 0.001580, mae: 0.042953, mean_q: 1.161579
  29900/1000000: episode: 299, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.924, mean reward: 0.579 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.173], loss: 0.001685, mae: 0.044656, mean_q: 1.167528
  30000/1000000: episode: 300, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.455, mean reward: 0.585 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.544, 10.135], loss: 0.001549, mae: 0.043146, mean_q: 1.164374
  30100/1000000: episode: 301, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 60.791, mean reward: 0.608 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.740, 10.151], loss: 0.001568, mae: 0.042912, mean_q: 1.166903
  30200/1000000: episode: 302, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 57.090, mean reward: 0.571 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.184], loss: 0.001552, mae: 0.043072, mean_q: 1.162440
  30300/1000000: episode: 303, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.944, mean reward: 0.589 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.925, 10.175], loss: 0.001522, mae: 0.043406, mean_q: 1.166821
  30400/1000000: episode: 304, duration: 0.755s, episode steps: 100, steps per second: 133, episode reward: 59.801, mean reward: 0.598 [0.512, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.298], loss: 0.001618, mae: 0.043496, mean_q: 1.164476
  30500/1000000: episode: 305, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.791, mean reward: 0.598 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.836, 10.224], loss: 0.001595, mae: 0.043956, mean_q: 1.167347
  30600/1000000: episode: 306, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.591, mean reward: 0.586 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.955, 10.232], loss: 0.001649, mae: 0.044205, mean_q: 1.166446
  30700/1000000: episode: 307, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 59.134, mean reward: 0.591 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.606, 10.098], loss: 0.001610, mae: 0.044202, mean_q: 1.165565
  30800/1000000: episode: 308, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 60.173, mean reward: 0.602 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.093, 10.307], loss: 0.001470, mae: 0.041785, mean_q: 1.168242
  30900/1000000: episode: 309, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 57.727, mean reward: 0.577 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.190], loss: 0.001574, mae: 0.043329, mean_q: 1.168838
  31000/1000000: episode: 310, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.610, mean reward: 0.586 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.586, 10.109], loss: 0.001685, mae: 0.044571, mean_q: 1.168337
  31100/1000000: episode: 311, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 60.994, mean reward: 0.610 [0.506, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.054, 10.098], loss: 0.001503, mae: 0.042723, mean_q: 1.170009
  31200/1000000: episode: 312, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 58.109, mean reward: 0.581 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.225, 10.098], loss: 0.001571, mae: 0.043354, mean_q: 1.167956
  31300/1000000: episode: 313, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.337, mean reward: 0.583 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.744, 10.129], loss: 0.001680, mae: 0.043888, mean_q: 1.167938
  31400/1000000: episode: 314, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 57.796, mean reward: 0.578 [0.503, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.878, 10.180], loss: 0.001534, mae: 0.043786, mean_q: 1.170993
  31500/1000000: episode: 315, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 58.861, mean reward: 0.589 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.208, 10.098], loss: 0.001493, mae: 0.042526, mean_q: 1.168038
  31600/1000000: episode: 316, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 61.375, mean reward: 0.614 [0.510, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.398, 10.252], loss: 0.001498, mae: 0.043376, mean_q: 1.167202
  31700/1000000: episode: 317, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.360, mean reward: 0.574 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.958, 10.216], loss: 0.001528, mae: 0.042848, mean_q: 1.166054
  31800/1000000: episode: 318, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 56.822, mean reward: 0.568 [0.504, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.196], loss: 0.001426, mae: 0.041953, mean_q: 1.165921
  31900/1000000: episode: 319, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 58.029, mean reward: 0.580 [0.502, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.098], loss: 0.001611, mae: 0.044189, mean_q: 1.165735
  32000/1000000: episode: 320, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 57.812, mean reward: 0.578 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.547, 10.118], loss: 0.001624, mae: 0.044264, mean_q: 1.168287
  32100/1000000: episode: 321, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 61.337, mean reward: 0.613 [0.506, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.863, 10.214], loss: 0.001499, mae: 0.042266, mean_q: 1.167603
  32200/1000000: episode: 322, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 61.150, mean reward: 0.611 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.080, 10.364], loss: 0.001439, mae: 0.041804, mean_q: 1.167817
  32300/1000000: episode: 323, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 59.218, mean reward: 0.592 [0.503, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.624, 10.250], loss: 0.001608, mae: 0.043621, mean_q: 1.167168
  32400/1000000: episode: 324, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.553, mean reward: 0.596 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.014, 10.098], loss: 0.001676, mae: 0.044380, mean_q: 1.167948
  32500/1000000: episode: 325, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 61.764, mean reward: 0.618 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.967, 10.098], loss: 0.001538, mae: 0.042696, mean_q: 1.168489
  32600/1000000: episode: 326, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 64.603, mean reward: 0.646 [0.538, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.203, 10.098], loss: 0.001573, mae: 0.043439, mean_q: 1.170787
  32700/1000000: episode: 327, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 55.991, mean reward: 0.560 [0.501, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.701, 10.098], loss: 0.001562, mae: 0.043364, mean_q: 1.173664
  32800/1000000: episode: 328, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 59.918, mean reward: 0.599 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.387, 10.098], loss: 0.001702, mae: 0.043845, mean_q: 1.168692
  32900/1000000: episode: 329, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 58.164, mean reward: 0.582 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.160], loss: 0.001626, mae: 0.043315, mean_q: 1.170340
  33000/1000000: episode: 330, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 59.915, mean reward: 0.599 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.919, 10.132], loss: 0.001533, mae: 0.042671, mean_q: 1.169242
  33100/1000000: episode: 331, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 60.727, mean reward: 0.607 [0.514, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.345, 10.437], loss: 0.001558, mae: 0.042815, mean_q: 1.167396
  33200/1000000: episode: 332, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.030, mean reward: 0.580 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.193, 10.098], loss: 0.001614, mae: 0.044211, mean_q: 1.168557
  33300/1000000: episode: 333, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.367, mean reward: 0.584 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.965, 10.368], loss: 0.001507, mae: 0.042331, mean_q: 1.169546
  33400/1000000: episode: 334, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 57.859, mean reward: 0.579 [0.509, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.655, 10.098], loss: 0.001496, mae: 0.041967, mean_q: 1.164708
  33500/1000000: episode: 335, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 57.947, mean reward: 0.579 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.251, 10.098], loss: 0.001636, mae: 0.043630, mean_q: 1.171398
  33600/1000000: episode: 336, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 59.277, mean reward: 0.593 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.098], loss: 0.001453, mae: 0.041959, mean_q: 1.167849
  33700/1000000: episode: 337, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 57.322, mean reward: 0.573 [0.506, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.961, 10.157], loss: 0.001573, mae: 0.042720, mean_q: 1.166454
  33800/1000000: episode: 338, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.657, mean reward: 0.587 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.069, 10.140], loss: 0.001445, mae: 0.041504, mean_q: 1.166215
  33900/1000000: episode: 339, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 57.491, mean reward: 0.575 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.918, 10.108], loss: 0.001420, mae: 0.041013, mean_q: 1.165169
  34000/1000000: episode: 340, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 57.050, mean reward: 0.571 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.515, 10.139], loss: 0.001502, mae: 0.042061, mean_q: 1.164467
  34100/1000000: episode: 341, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 59.022, mean reward: 0.590 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.746, 10.163], loss: 0.001535, mae: 0.042140, mean_q: 1.164409
  34200/1000000: episode: 342, duration: 0.687s, episode steps: 100, steps per second: 145, episode reward: 58.763, mean reward: 0.588 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.410, 10.098], loss: 0.001579, mae: 0.043289, mean_q: 1.165446
  34300/1000000: episode: 343, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 59.287, mean reward: 0.593 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.213, 10.098], loss: 0.001457, mae: 0.041741, mean_q: 1.165883
  34400/1000000: episode: 344, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.769, mean reward: 0.588 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.102], loss: 0.001550, mae: 0.043285, mean_q: 1.165730
  34500/1000000: episode: 345, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.679, mean reward: 0.587 [0.507, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.949, 10.208], loss: 0.001560, mae: 0.043037, mean_q: 1.168075
  34600/1000000: episode: 346, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 57.438, mean reward: 0.574 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.340, 10.098], loss: 0.001560, mae: 0.043160, mean_q: 1.166289
  34700/1000000: episode: 347, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.479, mean reward: 0.585 [0.499, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.214], loss: 0.001608, mae: 0.043984, mean_q: 1.166673
  34800/1000000: episode: 348, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 60.022, mean reward: 0.600 [0.499, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.339, 10.098], loss: 0.001505, mae: 0.042233, mean_q: 1.167897
  34900/1000000: episode: 349, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.073, mean reward: 0.591 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.136], loss: 0.001440, mae: 0.041681, mean_q: 1.167415
  35000/1000000: episode: 350, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 57.840, mean reward: 0.578 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.822, 10.144], loss: 0.001620, mae: 0.043558, mean_q: 1.166981
  35100/1000000: episode: 351, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 59.550, mean reward: 0.595 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.443, 10.161], loss: 0.001456, mae: 0.041568, mean_q: 1.166929
  35200/1000000: episode: 352, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 58.411, mean reward: 0.584 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.302, 10.312], loss: 0.001652, mae: 0.043475, mean_q: 1.167783
  35300/1000000: episode: 353, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 57.739, mean reward: 0.577 [0.510, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.800, 10.301], loss: 0.001518, mae: 0.042418, mean_q: 1.166207
  35400/1000000: episode: 354, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 58.282, mean reward: 0.583 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.258, 10.098], loss: 0.001564, mae: 0.042625, mean_q: 1.164944
  35500/1000000: episode: 355, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.262, mean reward: 0.583 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.098], loss: 0.001555, mae: 0.043306, mean_q: 1.167054
  35600/1000000: episode: 356, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 60.362, mean reward: 0.604 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.500, 10.419], loss: 0.001658, mae: 0.044329, mean_q: 1.164689
  35700/1000000: episode: 357, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 57.925, mean reward: 0.579 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.949, 10.098], loss: 0.001566, mae: 0.042828, mean_q: 1.168359
  35800/1000000: episode: 358, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.234, mean reward: 0.582 [0.503, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.658, 10.150], loss: 0.001489, mae: 0.041909, mean_q: 1.166795
  35900/1000000: episode: 359, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.241, mean reward: 0.582 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.409, 10.260], loss: 0.001537, mae: 0.043130, mean_q: 1.161949
  36000/1000000: episode: 360, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 57.554, mean reward: 0.576 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.987, 10.286], loss: 0.001455, mae: 0.042175, mean_q: 1.164047
  36100/1000000: episode: 361, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 62.617, mean reward: 0.626 [0.538, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.953, 10.255], loss: 0.001620, mae: 0.043501, mean_q: 1.163412
  36200/1000000: episode: 362, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 57.712, mean reward: 0.577 [0.501, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.813, 10.098], loss: 0.001573, mae: 0.043510, mean_q: 1.165010
  36300/1000000: episode: 363, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.157, mean reward: 0.592 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.413, 10.272], loss: 0.001502, mae: 0.041849, mean_q: 1.161013
  36400/1000000: episode: 364, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 61.403, mean reward: 0.614 [0.509, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.369], loss: 0.001460, mae: 0.041720, mean_q: 1.165586
  36500/1000000: episode: 365, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 59.425, mean reward: 0.594 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.098], loss: 0.001634, mae: 0.043664, mean_q: 1.168802
  36600/1000000: episode: 366, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 57.744, mean reward: 0.577 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.133, 10.115], loss: 0.001570, mae: 0.042730, mean_q: 1.165388
  36700/1000000: episode: 367, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 62.188, mean reward: 0.622 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.701, 10.403], loss: 0.001483, mae: 0.041843, mean_q: 1.166073
  36800/1000000: episode: 368, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 60.469, mean reward: 0.605 [0.516, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.352, 10.098], loss: 0.001562, mae: 0.042720, mean_q: 1.168404
  36900/1000000: episode: 369, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 60.600, mean reward: 0.606 [0.516, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.062, 10.250], loss: 0.001504, mae: 0.042078, mean_q: 1.167037
  37000/1000000: episode: 370, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.776, mean reward: 0.578 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.734, 10.126], loss: 0.001906, mae: 0.045938, mean_q: 1.168241
  37100/1000000: episode: 371, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.387, mean reward: 0.574 [0.501, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.618, 10.098], loss: 0.001470, mae: 0.042079, mean_q: 1.165887
  37200/1000000: episode: 372, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 61.663, mean reward: 0.617 [0.505, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.499, 10.372], loss: 0.001523, mae: 0.042915, mean_q: 1.165759
  37300/1000000: episode: 373, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 57.354, mean reward: 0.574 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.219, 10.211], loss: 0.001489, mae: 0.042466, mean_q: 1.166758
  37400/1000000: episode: 374, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 60.339, mean reward: 0.603 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.061, 10.317], loss: 0.001475, mae: 0.042691, mean_q: 1.166949
  37500/1000000: episode: 375, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 61.496, mean reward: 0.615 [0.519, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.113, 10.452], loss: 0.001490, mae: 0.041850, mean_q: 1.169281
  37600/1000000: episode: 376, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.161, mean reward: 0.582 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.242, 10.251], loss: 0.001482, mae: 0.042326, mean_q: 1.165735
  37700/1000000: episode: 377, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.301, mean reward: 0.573 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.206, 10.141], loss: 0.001696, mae: 0.044273, mean_q: 1.165643
  37800/1000000: episode: 378, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 64.657, mean reward: 0.647 [0.510, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.356, 10.129], loss: 0.001414, mae: 0.041005, mean_q: 1.164462
  37900/1000000: episode: 379, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 61.529, mean reward: 0.615 [0.501, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.834, 10.098], loss: 0.001487, mae: 0.041763, mean_q: 1.170926
  38000/1000000: episode: 380, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.090, mean reward: 0.581 [0.506, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.513, 10.156], loss: 0.001454, mae: 0.041837, mean_q: 1.165915
  38100/1000000: episode: 381, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.673, mean reward: 0.587 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.670, 10.248], loss: 0.001463, mae: 0.041868, mean_q: 1.169791
  38200/1000000: episode: 382, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 58.876, mean reward: 0.589 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.483, 10.207], loss: 0.001436, mae: 0.041304, mean_q: 1.168644
  38300/1000000: episode: 383, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 59.696, mean reward: 0.597 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.349, 10.098], loss: 0.001449, mae: 0.041723, mean_q: 1.166119
  38400/1000000: episode: 384, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 56.955, mean reward: 0.570 [0.510, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.831, 10.098], loss: 0.001480, mae: 0.042312, mean_q: 1.169726
  38500/1000000: episode: 385, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 60.786, mean reward: 0.608 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.303, 10.098], loss: 0.001490, mae: 0.041858, mean_q: 1.169279
  38600/1000000: episode: 386, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.651, mean reward: 0.577 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.434, 10.237], loss: 0.001504, mae: 0.042278, mean_q: 1.169132
  38700/1000000: episode: 387, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 58.255, mean reward: 0.583 [0.517, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.890, 10.098], loss: 0.001596, mae: 0.043553, mean_q: 1.169012
  38800/1000000: episode: 388, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 56.807, mean reward: 0.568 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.098], loss: 0.001468, mae: 0.041951, mean_q: 1.166822
  38900/1000000: episode: 389, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 59.495, mean reward: 0.595 [0.514, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.142, 10.134], loss: 0.001440, mae: 0.041731, mean_q: 1.168357
  39000/1000000: episode: 390, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 59.699, mean reward: 0.597 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.245, 10.297], loss: 0.001501, mae: 0.042316, mean_q: 1.167891
  39100/1000000: episode: 391, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.138, mean reward: 0.591 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.783, 10.284], loss: 0.001468, mae: 0.042550, mean_q: 1.172604
  39200/1000000: episode: 392, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 60.031, mean reward: 0.600 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.541, 10.098], loss: 0.001451, mae: 0.042190, mean_q: 1.170077
  39300/1000000: episode: 393, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 62.257, mean reward: 0.623 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.342, 10.193], loss: 0.001568, mae: 0.043140, mean_q: 1.167982
  39400/1000000: episode: 394, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 62.815, mean reward: 0.628 [0.500, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.302, 10.395], loss: 0.001485, mae: 0.042080, mean_q: 1.174959
  39500/1000000: episode: 395, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 58.982, mean reward: 0.590 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.265], loss: 0.001449, mae: 0.041878, mean_q: 1.169928
  39600/1000000: episode: 396, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.645, mean reward: 0.586 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.730, 10.144], loss: 0.001600, mae: 0.043661, mean_q: 1.172084
  39700/1000000: episode: 397, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 59.930, mean reward: 0.599 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.341, 10.098], loss: 0.001421, mae: 0.041019, mean_q: 1.171603
  39800/1000000: episode: 398, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.789, mean reward: 0.578 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.299, 10.221], loss: 0.001475, mae: 0.042638, mean_q: 1.171686
  39900/1000000: episode: 399, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 56.807, mean reward: 0.568 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.492, 10.142], loss: 0.001498, mae: 0.043045, mean_q: 1.171626
  40000/1000000: episode: 400, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 60.394, mean reward: 0.604 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.303, 10.098], loss: 0.001564, mae: 0.043580, mean_q: 1.172633
  40100/1000000: episode: 401, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.847, mean reward: 0.588 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.365, 10.298], loss: 0.001471, mae: 0.042223, mean_q: 1.172518
  40200/1000000: episode: 402, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 57.752, mean reward: 0.578 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.348, 10.098], loss: 0.001480, mae: 0.042169, mean_q: 1.174280
  40300/1000000: episode: 403, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.633, mean reward: 0.576 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.555, 10.098], loss: 0.001510, mae: 0.042901, mean_q: 1.172974
  40400/1000000: episode: 404, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 57.891, mean reward: 0.579 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.511, 10.142], loss: 0.001478, mae: 0.042712, mean_q: 1.172277
  40500/1000000: episode: 405, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 57.956, mean reward: 0.580 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.626, 10.098], loss: 0.001457, mae: 0.042483, mean_q: 1.173673
  40600/1000000: episode: 406, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 60.485, mean reward: 0.605 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.986, 10.098], loss: 0.001532, mae: 0.043608, mean_q: 1.175997
  40700/1000000: episode: 407, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.386, mean reward: 0.604 [0.499, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.049, 10.354], loss: 0.001419, mae: 0.042166, mean_q: 1.174274
  40800/1000000: episode: 408, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 57.493, mean reward: 0.575 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.679, 10.107], loss: 0.001510, mae: 0.042994, mean_q: 1.170078
  40900/1000000: episode: 409, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 58.830, mean reward: 0.588 [0.511, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.186, 10.360], loss: 0.001466, mae: 0.042192, mean_q: 1.171704
  41000/1000000: episode: 410, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.434, mean reward: 0.584 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.750, 10.098], loss: 0.001496, mae: 0.042780, mean_q: 1.175803
  41100/1000000: episode: 411, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 59.190, mean reward: 0.592 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.236, 10.098], loss: 0.001447, mae: 0.042088, mean_q: 1.170594
  41200/1000000: episode: 412, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 57.231, mean reward: 0.572 [0.510, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.649, 10.098], loss: 0.001452, mae: 0.041977, mean_q: 1.174116
  41300/1000000: episode: 413, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.558, mean reward: 0.576 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.504, 10.098], loss: 0.001426, mae: 0.041693, mean_q: 1.172408
  41400/1000000: episode: 414, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 60.055, mean reward: 0.601 [0.497, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.192, 10.098], loss: 0.001433, mae: 0.041686, mean_q: 1.172252
  41500/1000000: episode: 415, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 57.767, mean reward: 0.578 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.173], loss: 0.001464, mae: 0.042034, mean_q: 1.172636
  41600/1000000: episode: 416, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.432, mean reward: 0.604 [0.510, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.516, 10.098], loss: 0.001447, mae: 0.041775, mean_q: 1.171528
  41700/1000000: episode: 417, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 58.018, mean reward: 0.580 [0.503, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.363, 10.105], loss: 0.001393, mae: 0.041354, mean_q: 1.167027
  41800/1000000: episode: 418, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 58.067, mean reward: 0.581 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.125], loss: 0.001433, mae: 0.041554, mean_q: 1.169476
  41900/1000000: episode: 419, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 57.536, mean reward: 0.575 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.322, 10.138], loss: 0.001505, mae: 0.042810, mean_q: 1.172721
  42000/1000000: episode: 420, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 55.737, mean reward: 0.557 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.605, 10.098], loss: 0.001503, mae: 0.042378, mean_q: 1.168449
  42100/1000000: episode: 421, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 58.702, mean reward: 0.587 [0.514, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.746, 10.163], loss: 0.001426, mae: 0.041716, mean_q: 1.170591
  42200/1000000: episode: 422, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.306, mean reward: 0.593 [0.497, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.306], loss: 0.001394, mae: 0.041594, mean_q: 1.167318
  42300/1000000: episode: 423, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.503, mean reward: 0.595 [0.516, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.629, 10.151], loss: 0.001549, mae: 0.043206, mean_q: 1.165229
  42400/1000000: episode: 424, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 60.367, mean reward: 0.604 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.505, 10.098], loss: 0.001499, mae: 0.042910, mean_q: 1.167950
  42500/1000000: episode: 425, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 58.157, mean reward: 0.582 [0.520, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.263, 10.098], loss: 0.001352, mae: 0.040757, mean_q: 1.168338
  42600/1000000: episode: 426, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 59.975, mean reward: 0.600 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.266, 10.136], loss: 0.001424, mae: 0.041512, mean_q: 1.167325
  42700/1000000: episode: 427, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.511, mean reward: 0.585 [0.497, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.918, 10.098], loss: 0.001299, mae: 0.039874, mean_q: 1.164372
  42800/1000000: episode: 428, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.041, mean reward: 0.590 [0.502, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.465], loss: 0.001352, mae: 0.040446, mean_q: 1.165485
  42900/1000000: episode: 429, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 58.173, mean reward: 0.582 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.396, 10.122], loss: 0.001445, mae: 0.042084, mean_q: 1.163138
  43000/1000000: episode: 430, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 58.925, mean reward: 0.589 [0.510, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.874, 10.141], loss: 0.001386, mae: 0.041400, mean_q: 1.163302
  43100/1000000: episode: 431, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 58.751, mean reward: 0.588 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.392, 10.362], loss: 0.001482, mae: 0.042852, mean_q: 1.165338
  43200/1000000: episode: 432, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 63.880, mean reward: 0.639 [0.518, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.076, 10.664], loss: 0.001463, mae: 0.042430, mean_q: 1.166478
  43300/1000000: episode: 433, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.789, mean reward: 0.578 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.098], loss: 0.001474, mae: 0.041625, mean_q: 1.162650
  43400/1000000: episode: 434, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 62.307, mean reward: 0.623 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.633, 10.098], loss: 0.001383, mae: 0.040419, mean_q: 1.162172
  43500/1000000: episode: 435, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 58.389, mean reward: 0.584 [0.497, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.490, 10.098], loss: 0.001458, mae: 0.042173, mean_q: 1.166722
  43600/1000000: episode: 436, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 57.852, mean reward: 0.579 [0.508, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.847, 10.118], loss: 0.001485, mae: 0.042837, mean_q: 1.164266
  43695/1000000: episode: 437, duration: 0.755s, episode steps: 95, steps per second: 126, episode reward: 60.581, mean reward: 0.638 [0.502, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.379 [-1.020, 10.020], loss: 0.001520, mae: 0.042822, mean_q: 1.166151
  43795/1000000: episode: 438, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.598, mean reward: 0.586 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.174, 10.158], loss: 0.001511, mae: 0.042524, mean_q: 1.165712
  43895/1000000: episode: 439, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 60.053, mean reward: 0.601 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.776, 10.098], loss: 0.001609, mae: 0.043390, mean_q: 1.169830
  43995/1000000: episode: 440, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.787, mean reward: 0.598 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.120], loss: 0.001619, mae: 0.043651, mean_q: 1.165942
  44095/1000000: episode: 441, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 58.050, mean reward: 0.580 [0.509, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.236, 10.324], loss: 0.001577, mae: 0.043303, mean_q: 1.169367
  44195/1000000: episode: 442, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 59.304, mean reward: 0.593 [0.501, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.976, 10.098], loss: 0.001440, mae: 0.041489, mean_q: 1.167098
  44295/1000000: episode: 443, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 56.798, mean reward: 0.568 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.328, 10.129], loss: 0.001505, mae: 0.041966, mean_q: 1.164256
  44395/1000000: episode: 444, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 56.570, mean reward: 0.566 [0.504, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.113, 10.098], loss: 0.001487, mae: 0.042248, mean_q: 1.166766
  44495/1000000: episode: 445, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.450, mean reward: 0.574 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.678, 10.098], loss: 0.001442, mae: 0.041906, mean_q: 1.163973
  44595/1000000: episode: 446, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.398, mean reward: 0.584 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.150], loss: 0.001556, mae: 0.042807, mean_q: 1.159739
  44695/1000000: episode: 447, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.043, mean reward: 0.570 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.098], loss: 0.001462, mae: 0.041816, mean_q: 1.166844
  44795/1000000: episode: 448, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.030, mean reward: 0.570 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.243], loss: 0.001400, mae: 0.040783, mean_q: 1.158574
  44895/1000000: episode: 449, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.396, mean reward: 0.574 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.081, 10.212], loss: 0.001590, mae: 0.043346, mean_q: 1.161022
  44995/1000000: episode: 450, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.305, mean reward: 0.583 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.130, 10.098], loss: 0.001403, mae: 0.041448, mean_q: 1.164120
  45095/1000000: episode: 451, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 57.985, mean reward: 0.580 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.925, 10.106], loss: 0.001412, mae: 0.041579, mean_q: 1.162410
  45195/1000000: episode: 452, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 57.989, mean reward: 0.580 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.935, 10.098], loss: 0.001492, mae: 0.042192, mean_q: 1.161358
  45295/1000000: episode: 453, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 58.974, mean reward: 0.590 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.218, 10.177], loss: 0.001557, mae: 0.042605, mean_q: 1.163353
  45395/1000000: episode: 454, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 60.001, mean reward: 0.600 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.098], loss: 0.001473, mae: 0.042019, mean_q: 1.159949
  45495/1000000: episode: 455, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.155, mean reward: 0.592 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.707, 10.211], loss: 0.001524, mae: 0.042754, mean_q: 1.161719
  45595/1000000: episode: 456, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 58.988, mean reward: 0.590 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.390, 10.141], loss: 0.001434, mae: 0.041715, mean_q: 1.164343
  45695/1000000: episode: 457, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 58.815, mean reward: 0.588 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.784, 10.098], loss: 0.001441, mae: 0.041031, mean_q: 1.161804
  45795/1000000: episode: 458, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 57.681, mean reward: 0.577 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.213], loss: 0.001406, mae: 0.041481, mean_q: 1.161317
  45895/1000000: episode: 459, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 59.832, mean reward: 0.598 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.986, 10.218], loss: 0.001454, mae: 0.041072, mean_q: 1.162279
  45995/1000000: episode: 460, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 58.314, mean reward: 0.583 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.266, 10.098], loss: 0.001660, mae: 0.043265, mean_q: 1.162560
  46095/1000000: episode: 461, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.372, mean reward: 0.594 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.039, 10.272], loss: 0.001490, mae: 0.040933, mean_q: 1.158596
  46195/1000000: episode: 462, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 58.586, mean reward: 0.586 [0.503, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.712, 10.098], loss: 0.001353, mae: 0.040317, mean_q: 1.162500
  46295/1000000: episode: 463, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 60.595, mean reward: 0.606 [0.514, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.104, 10.098], loss: 0.001426, mae: 0.040543, mean_q: 1.162885
  46395/1000000: episode: 464, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 58.306, mean reward: 0.583 [0.498, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.001, 10.196], loss: 0.001529, mae: 0.042543, mean_q: 1.163698
  46495/1000000: episode: 465, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.397, mean reward: 0.574 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.914, 10.245], loss: 0.001501, mae: 0.042089, mean_q: 1.160351
  46595/1000000: episode: 466, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.733, mean reward: 0.577 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.126, 10.137], loss: 0.001414, mae: 0.040889, mean_q: 1.161921
  46695/1000000: episode: 467, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 58.134, mean reward: 0.581 [0.513, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.156], loss: 0.001602, mae: 0.042792, mean_q: 1.160780
  46795/1000000: episode: 468, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 61.856, mean reward: 0.619 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.459, 10.098], loss: 0.001661, mae: 0.044263, mean_q: 1.162270
  46895/1000000: episode: 469, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 59.026, mean reward: 0.590 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.278, 10.278], loss: 0.001381, mae: 0.040603, mean_q: 1.163749
  46995/1000000: episode: 470, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 64.458, mean reward: 0.645 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.698, 10.098], loss: 0.001491, mae: 0.041916, mean_q: 1.164806
  47095/1000000: episode: 471, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: 59.322, mean reward: 0.593 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.229], loss: 0.001387, mae: 0.040839, mean_q: 1.169431
  47195/1000000: episode: 472, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 57.441, mean reward: 0.574 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.496, 10.098], loss: 0.001491, mae: 0.042024, mean_q: 1.165076
  47295/1000000: episode: 473, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 60.245, mean reward: 0.602 [0.513, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.169], loss: 0.001390, mae: 0.041181, mean_q: 1.167840
  47395/1000000: episode: 474, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.063, mean reward: 0.581 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.265, 10.098], loss: 0.001322, mae: 0.039681, mean_q: 1.167808
  47495/1000000: episode: 475, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 57.784, mean reward: 0.578 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.686, 10.098], loss: 0.001369, mae: 0.040233, mean_q: 1.164102
  47595/1000000: episode: 476, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 56.886, mean reward: 0.569 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.248, 10.098], loss: 0.001516, mae: 0.042562, mean_q: 1.165134
  47695/1000000: episode: 477, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 59.059, mean reward: 0.591 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.944, 10.098], loss: 0.001355, mae: 0.040782, mean_q: 1.166300
  47795/1000000: episode: 478, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 61.192, mean reward: 0.612 [0.518, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.270, 10.098], loss: 0.001485, mae: 0.041702, mean_q: 1.166100
  47895/1000000: episode: 479, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 58.927, mean reward: 0.589 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.148, 10.098], loss: 0.001592, mae: 0.042486, mean_q: 1.168516
  47995/1000000: episode: 480, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 57.423, mean reward: 0.574 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.165, 10.098], loss: 0.001445, mae: 0.041098, mean_q: 1.166056
  48095/1000000: episode: 481, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 62.021, mean reward: 0.620 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.596, 10.212], loss: 0.001341, mae: 0.039329, mean_q: 1.165074
  48195/1000000: episode: 482, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 60.913, mean reward: 0.609 [0.514, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.786, 10.272], loss: 0.001407, mae: 0.040599, mean_q: 1.167019
  48295/1000000: episode: 483, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.788, mean reward: 0.578 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.584, 10.196], loss: 0.001462, mae: 0.042377, mean_q: 1.167109
  48395/1000000: episode: 484, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 59.449, mean reward: 0.594 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.292, 10.210], loss: 0.001416, mae: 0.040722, mean_q: 1.164170
  48495/1000000: episode: 485, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 58.685, mean reward: 0.587 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.520, 10.412], loss: 0.001424, mae: 0.041590, mean_q: 1.165581
  48595/1000000: episode: 486, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 59.012, mean reward: 0.590 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.773, 10.163], loss: 0.001378, mae: 0.039965, mean_q: 1.165118
  48695/1000000: episode: 487, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.398, mean reward: 0.584 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.550, 10.147], loss: 0.001402, mae: 0.040646, mean_q: 1.161829
  48795/1000000: episode: 488, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 60.119, mean reward: 0.601 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.430, 10.107], loss: 0.001398, mae: 0.040709, mean_q: 1.163232
  48895/1000000: episode: 489, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 57.933, mean reward: 0.579 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.505, 10.170], loss: 0.001310, mae: 0.040075, mean_q: 1.162579
  48995/1000000: episode: 490, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.770, mean reward: 0.598 [0.498, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.142, 10.098], loss: 0.001456, mae: 0.042441, mean_q: 1.163107
  49095/1000000: episode: 491, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 59.629, mean reward: 0.596 [0.511, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.600, 10.331], loss: 0.001364, mae: 0.040141, mean_q: 1.164125
  49195/1000000: episode: 492, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.062, mean reward: 0.571 [0.503, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.442, 10.098], loss: 0.001352, mae: 0.040360, mean_q: 1.162776
  49295/1000000: episode: 493, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.275, mean reward: 0.583 [0.503, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.192, 10.098], loss: 0.001481, mae: 0.042187, mean_q: 1.163839
  49395/1000000: episode: 494, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 63.150, mean reward: 0.632 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.901, 10.098], loss: 0.001453, mae: 0.042016, mean_q: 1.165197
  49495/1000000: episode: 495, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 61.754, mean reward: 0.618 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.339, 10.098], loss: 0.001348, mae: 0.040856, mean_q: 1.165142
  49595/1000000: episode: 496, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.179, mean reward: 0.582 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.336, 10.128], loss: 0.001489, mae: 0.043058, mean_q: 1.165851
  49695/1000000: episode: 497, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 58.901, mean reward: 0.589 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.493, 10.098], loss: 0.001496, mae: 0.042003, mean_q: 1.165206
  49795/1000000: episode: 498, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 58.996, mean reward: 0.590 [0.512, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.308, 10.261], loss: 0.001433, mae: 0.041536, mean_q: 1.169068
  49895/1000000: episode: 499, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.360, mean reward: 0.574 [0.505, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.800, 10.113], loss: 0.001571, mae: 0.042930, mean_q: 1.169206
  49995/1000000: episode: 500, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 62.257, mean reward: 0.623 [0.509, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.106], loss: 0.001431, mae: 0.041503, mean_q: 1.167818
  50095/1000000: episode: 501, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 57.886, mean reward: 0.579 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.583, 10.098], loss: 0.001467, mae: 0.041922, mean_q: 1.168479
  50195/1000000: episode: 502, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 64.263, mean reward: 0.643 [0.500, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.676, 10.098], loss: 0.001555, mae: 0.042772, mean_q: 1.170260
  50295/1000000: episode: 503, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 60.352, mean reward: 0.604 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.750, 10.098], loss: 0.001682, mae: 0.044946, mean_q: 1.173579
  50395/1000000: episode: 504, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 58.130, mean reward: 0.581 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.434, 10.306], loss: 0.001404, mae: 0.041020, mean_q: 1.174147
  50495/1000000: episode: 505, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 59.315, mean reward: 0.593 [0.510, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.876, 10.231], loss: 0.001442, mae: 0.041426, mean_q: 1.174571
  50595/1000000: episode: 506, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 58.879, mean reward: 0.589 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.110, 10.098], loss: 0.001510, mae: 0.042291, mean_q: 1.176487
  50695/1000000: episode: 507, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.636, mean reward: 0.586 [0.517, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.209, 10.098], loss: 0.001623, mae: 0.043708, mean_q: 1.174173
  50795/1000000: episode: 508, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 57.022, mean reward: 0.570 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.368, 10.141], loss: 0.001442, mae: 0.041414, mean_q: 1.170809
  50895/1000000: episode: 509, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 57.116, mean reward: 0.571 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.202, 10.178], loss: 0.001565, mae: 0.043594, mean_q: 1.171816
  50995/1000000: episode: 510, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 62.488, mean reward: 0.625 [0.516, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.965, 10.249], loss: 0.001449, mae: 0.042108, mean_q: 1.173617
  51095/1000000: episode: 511, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: 59.195, mean reward: 0.592 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.940, 10.098], loss: 0.001568, mae: 0.042769, mean_q: 1.171518
  51195/1000000: episode: 512, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.245, mean reward: 0.582 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.098], loss: 0.001542, mae: 0.043142, mean_q: 1.171997
  51295/1000000: episode: 513, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 60.995, mean reward: 0.610 [0.517, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.591, 10.457], loss: 0.001505, mae: 0.042506, mean_q: 1.171955
  51395/1000000: episode: 514, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 59.187, mean reward: 0.592 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.933, 10.098], loss: 0.001492, mae: 0.042460, mean_q: 1.174971
  51495/1000000: episode: 515, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.653, mean reward: 0.587 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.078, 10.129], loss: 0.001412, mae: 0.041351, mean_q: 1.173102
  51595/1000000: episode: 516, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 60.304, mean reward: 0.603 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.539, 10.098], loss: 0.001539, mae: 0.043378, mean_q: 1.174572
  51695/1000000: episode: 517, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 57.238, mean reward: 0.572 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.383, 10.168], loss: 0.001615, mae: 0.044011, mean_q: 1.173090
  51795/1000000: episode: 518, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 59.072, mean reward: 0.591 [0.509, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.353, 10.196], loss: 0.001430, mae: 0.041421, mean_q: 1.171619
  51895/1000000: episode: 519, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 59.319, mean reward: 0.593 [0.522, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.326, 10.129], loss: 0.001423, mae: 0.041964, mean_q: 1.173084
  51995/1000000: episode: 520, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 61.431, mean reward: 0.614 [0.507, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.742, 10.098], loss: 0.001600, mae: 0.044108, mean_q: 1.169215
  52095/1000000: episode: 521, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.417, mean reward: 0.574 [0.507, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.879, 10.178], loss: 0.001451, mae: 0.042629, mean_q: 1.170792
  52195/1000000: episode: 522, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.606, mean reward: 0.596 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.660, 10.138], loss: 0.001390, mae: 0.040609, mean_q: 1.166865
  52295/1000000: episode: 523, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 57.994, mean reward: 0.580 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.565, 10.319], loss: 0.001484, mae: 0.042516, mean_q: 1.168250
  52395/1000000: episode: 524, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 59.588, mean reward: 0.596 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.795, 10.112], loss: 0.001469, mae: 0.042110, mean_q: 1.172444
  52495/1000000: episode: 525, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 60.142, mean reward: 0.601 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.296, 10.098], loss: 0.001361, mae: 0.040812, mean_q: 1.173371
  52595/1000000: episode: 526, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 57.328, mean reward: 0.573 [0.502, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.610, 10.123], loss: 0.001475, mae: 0.042001, mean_q: 1.173287
  52695/1000000: episode: 527, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 62.908, mean reward: 0.629 [0.512, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.301, 10.098], loss: 0.001434, mae: 0.041925, mean_q: 1.172772
  52795/1000000: episode: 528, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.136, mean reward: 0.591 [0.506, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.946, 10.392], loss: 0.001414, mae: 0.041400, mean_q: 1.172264
  52895/1000000: episode: 529, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 58.725, mean reward: 0.587 [0.500, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.098], loss: 0.001438, mae: 0.041321, mean_q: 1.174390
  52995/1000000: episode: 530, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 57.296, mean reward: 0.573 [0.505, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.689, 10.130], loss: 0.001418, mae: 0.041054, mean_q: 1.176087
  53095/1000000: episode: 531, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 58.880, mean reward: 0.589 [0.508, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.333, 10.217], loss: 0.001450, mae: 0.041925, mean_q: 1.171403
  53195/1000000: episode: 532, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.701, mean reward: 0.587 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.128], loss: 0.001365, mae: 0.040314, mean_q: 1.168214
  53295/1000000: episode: 533, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 59.649, mean reward: 0.596 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.105, 10.119], loss: 0.001400, mae: 0.041105, mean_q: 1.173836
  53395/1000000: episode: 534, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 58.434, mean reward: 0.584 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.613, 10.098], loss: 0.001431, mae: 0.041628, mean_q: 1.171446
  53495/1000000: episode: 535, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.202, mean reward: 0.582 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.313, 10.098], loss: 0.001357, mae: 0.040694, mean_q: 1.171713
  53595/1000000: episode: 536, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 60.865, mean reward: 0.609 [0.526, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.369, 10.098], loss: 0.001484, mae: 0.042176, mean_q: 1.171083
  53695/1000000: episode: 537, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 56.874, mean reward: 0.569 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.829, 10.222], loss: 0.001493, mae: 0.042256, mean_q: 1.172391
  53795/1000000: episode: 538, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 56.007, mean reward: 0.560 [0.505, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.900, 10.119], loss: 0.001458, mae: 0.041809, mean_q: 1.172985
  53895/1000000: episode: 539, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 58.628, mean reward: 0.586 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.320, 10.333], loss: 0.001341, mae: 0.040763, mean_q: 1.169531
  53995/1000000: episode: 540, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 61.085, mean reward: 0.611 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.478], loss: 0.001441, mae: 0.041806, mean_q: 1.169342
  54095/1000000: episode: 541, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 59.362, mean reward: 0.594 [0.502, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.950, 10.283], loss: 0.001379, mae: 0.040461, mean_q: 1.171655
  54195/1000000: episode: 542, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 59.524, mean reward: 0.595 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.033, 10.098], loss: 0.001267, mae: 0.039277, mean_q: 1.172223
  54295/1000000: episode: 543, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 59.645, mean reward: 0.596 [0.513, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.562, 10.098], loss: 0.001441, mae: 0.042115, mean_q: 1.172245
  54395/1000000: episode: 544, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 57.603, mean reward: 0.576 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.035, 10.118], loss: 0.001343, mae: 0.040832, mean_q: 1.170978
  54495/1000000: episode: 545, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.895, mean reward: 0.589 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.098], loss: 0.001336, mae: 0.040305, mean_q: 1.169282
  54595/1000000: episode: 546, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.825, mean reward: 0.588 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.311, 10.098], loss: 0.001355, mae: 0.040829, mean_q: 1.169666
  54695/1000000: episode: 547, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 58.430, mean reward: 0.584 [0.502, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.736, 10.200], loss: 0.001482, mae: 0.041856, mean_q: 1.169695
  54795/1000000: episode: 548, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.809, mean reward: 0.578 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.025, 10.098], loss: 0.001272, mae: 0.038892, mean_q: 1.165929
  54895/1000000: episode: 549, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 58.224, mean reward: 0.582 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.425, 10.098], loss: 0.001322, mae: 0.040195, mean_q: 1.171287
  54995/1000000: episode: 550, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 57.733, mean reward: 0.577 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.803, 10.098], loss: 0.001369, mae: 0.040659, mean_q: 1.167979
  55095/1000000: episode: 551, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 59.017, mean reward: 0.590 [0.512, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.580, 10.098], loss: 0.001415, mae: 0.041406, mean_q: 1.170716
  55195/1000000: episode: 552, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 60.277, mean reward: 0.603 [0.512, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.051, 10.403], loss: 0.001355, mae: 0.041189, mean_q: 1.166104
  55295/1000000: episode: 553, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.304, mean reward: 0.593 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.754, 10.203], loss: 0.001376, mae: 0.040838, mean_q: 1.164404
  55395/1000000: episode: 554, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 61.100, mean reward: 0.611 [0.515, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.053, 10.171], loss: 0.001361, mae: 0.040871, mean_q: 1.169396
  55495/1000000: episode: 555, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.014, mean reward: 0.590 [0.506, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.653, 10.098], loss: 0.001292, mae: 0.039588, mean_q: 1.168278
  55595/1000000: episode: 556, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 59.444, mean reward: 0.594 [0.521, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.339, 10.185], loss: 0.001338, mae: 0.040347, mean_q: 1.167376
  55695/1000000: episode: 557, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 58.195, mean reward: 0.582 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.908, 10.098], loss: 0.001285, mae: 0.038925, mean_q: 1.168475
  55795/1000000: episode: 558, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 57.963, mean reward: 0.580 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.636, 10.098], loss: 0.001300, mae: 0.039471, mean_q: 1.168583
  55895/1000000: episode: 559, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 61.735, mean reward: 0.617 [0.514, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.663, 10.098], loss: 0.001634, mae: 0.042645, mean_q: 1.165612
  55995/1000000: episode: 560, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 57.892, mean reward: 0.579 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.446, 10.330], loss: 0.001332, mae: 0.040360, mean_q: 1.167711
  56095/1000000: episode: 561, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 61.526, mean reward: 0.615 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.098], loss: 0.001373, mae: 0.040875, mean_q: 1.164575
  56195/1000000: episode: 562, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 57.837, mean reward: 0.578 [0.510, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.503, 10.231], loss: 0.001351, mae: 0.040444, mean_q: 1.169356
  56295/1000000: episode: 563, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 59.120, mean reward: 0.591 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.460, 10.356], loss: 0.001274, mae: 0.039575, mean_q: 1.166947
  56395/1000000: episode: 564, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 67.745, mean reward: 0.677 [0.511, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.448], loss: 0.001311, mae: 0.039879, mean_q: 1.167621
  56495/1000000: episode: 565, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 60.251, mean reward: 0.603 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.492, 10.098], loss: 0.001300, mae: 0.039569, mean_q: 1.170360
  56595/1000000: episode: 566, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.859, mean reward: 0.599 [0.497, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.552, 10.098], loss: 0.001313, mae: 0.039802, mean_q: 1.169160
  56695/1000000: episode: 567, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 57.246, mean reward: 0.572 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.524, 10.236], loss: 0.001253, mae: 0.038930, mean_q: 1.173786
  56795/1000000: episode: 568, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.517, mean reward: 0.585 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.293, 10.098], loss: 0.001294, mae: 0.039493, mean_q: 1.170195
  56895/1000000: episode: 569, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 59.550, mean reward: 0.596 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.633, 10.098], loss: 0.001266, mae: 0.038876, mean_q: 1.169803
  56995/1000000: episode: 570, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.761, mean reward: 0.578 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.493, 10.134], loss: 0.001310, mae: 0.039854, mean_q: 1.168797
  57095/1000000: episode: 571, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 60.304, mean reward: 0.603 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.192, 10.098], loss: 0.001245, mae: 0.038851, mean_q: 1.168957
  57195/1000000: episode: 572, duration: 0.702s, episode steps: 100, steps per second: 143, episode reward: 60.213, mean reward: 0.602 [0.510, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.143, 10.295], loss: 0.001329, mae: 0.039854, mean_q: 1.166994
  57295/1000000: episode: 573, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 57.110, mean reward: 0.571 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.285, 10.173], loss: 0.001410, mae: 0.040855, mean_q: 1.170369
  57395/1000000: episode: 574, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 58.496, mean reward: 0.585 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.084, 10.098], loss: 0.001524, mae: 0.042181, mean_q: 1.167800
  57495/1000000: episode: 575, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 57.062, mean reward: 0.571 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.098], loss: 0.001372, mae: 0.040488, mean_q: 1.170938
  57595/1000000: episode: 576, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.432, mean reward: 0.594 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.247, 10.098], loss: 0.001399, mae: 0.041183, mean_q: 1.167810
  57695/1000000: episode: 577, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.437, mean reward: 0.594 [0.505, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.098], loss: 0.001495, mae: 0.042227, mean_q: 1.167126
  57795/1000000: episode: 578, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 56.982, mean reward: 0.570 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.011, 10.098], loss: 0.001371, mae: 0.040662, mean_q: 1.167380
  57895/1000000: episode: 579, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 57.121, mean reward: 0.571 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.098], loss: 0.001320, mae: 0.039777, mean_q: 1.167997
  57995/1000000: episode: 580, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 59.127, mean reward: 0.591 [0.502, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.598, 10.214], loss: 0.001320, mae: 0.040105, mean_q: 1.170171
  58095/1000000: episode: 581, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 57.687, mean reward: 0.577 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.810, 10.116], loss: 0.001497, mae: 0.041689, mean_q: 1.169872
  58195/1000000: episode: 582, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 60.063, mean reward: 0.601 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.418, 10.098], loss: 0.001319, mae: 0.040225, mean_q: 1.165741
  58295/1000000: episode: 583, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 60.656, mean reward: 0.607 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.139, 10.098], loss: 0.001400, mae: 0.040996, mean_q: 1.166518
  58395/1000000: episode: 584, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 59.018, mean reward: 0.590 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.556, 10.098], loss: 0.001313, mae: 0.039678, mean_q: 1.168167
  58495/1000000: episode: 585, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 63.704, mean reward: 0.637 [0.505, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.664, 10.098], loss: 0.001420, mae: 0.040959, mean_q: 1.170814
  58595/1000000: episode: 586, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 57.613, mean reward: 0.576 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.226], loss: 0.001477, mae: 0.041763, mean_q: 1.165539
  58695/1000000: episode: 587, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 58.532, mean reward: 0.585 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.282, 10.175], loss: 0.001308, mae: 0.039302, mean_q: 1.166118
  58795/1000000: episode: 588, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 59.911, mean reward: 0.599 [0.498, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.910, 10.263], loss: 0.001402, mae: 0.040550, mean_q: 1.170625
  58895/1000000: episode: 589, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.148, mean reward: 0.581 [0.503, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.319, 10.285], loss: 0.001292, mae: 0.039371, mean_q: 1.170760
  58995/1000000: episode: 590, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 57.787, mean reward: 0.578 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.210], loss: 0.001472, mae: 0.041876, mean_q: 1.173292
  59095/1000000: episode: 591, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 56.317, mean reward: 0.563 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.471, 10.098], loss: 0.001291, mae: 0.039593, mean_q: 1.169398
  59195/1000000: episode: 592, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.614, mean reward: 0.586 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.150, 10.340], loss: 0.001368, mae: 0.040391, mean_q: 1.168019
  59295/1000000: episode: 593, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 59.248, mean reward: 0.592 [0.509, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.098], loss: 0.001353, mae: 0.040285, mean_q: 1.166892
  59395/1000000: episode: 594, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 58.627, mean reward: 0.586 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.910, 10.202], loss: 0.001338, mae: 0.039956, mean_q: 1.168213
  59495/1000000: episode: 595, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 57.044, mean reward: 0.570 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.454, 10.177], loss: 0.001459, mae: 0.041560, mean_q: 1.169202
  59595/1000000: episode: 596, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 58.064, mean reward: 0.581 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.510, 10.345], loss: 0.001353, mae: 0.040463, mean_q: 1.168183
  59695/1000000: episode: 597, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.643, mean reward: 0.586 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.391, 10.098], loss: 0.001323, mae: 0.039810, mean_q: 1.166545
  59795/1000000: episode: 598, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 58.731, mean reward: 0.587 [0.512, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.694, 10.311], loss: 0.001370, mae: 0.039916, mean_q: 1.169047
  59895/1000000: episode: 599, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 58.384, mean reward: 0.584 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.633, 10.098], loss: 0.001361, mae: 0.041120, mean_q: 1.167880
  59995/1000000: episode: 600, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 61.405, mean reward: 0.614 [0.508, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.796, 10.098], loss: 0.001306, mae: 0.039503, mean_q: 1.170173
  60095/1000000: episode: 601, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.665, mean reward: 0.577 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.364, 10.275], loss: 0.001309, mae: 0.039616, mean_q: 1.172398
  60195/1000000: episode: 602, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 58.672, mean reward: 0.587 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.613, 10.154], loss: 0.001356, mae: 0.040685, mean_q: 1.171970
  60295/1000000: episode: 603, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 57.388, mean reward: 0.574 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.411, 10.103], loss: 0.001376, mae: 0.040183, mean_q: 1.167447
  60395/1000000: episode: 604, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.733, mean reward: 0.577 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.067, 10.205], loss: 0.001377, mae: 0.040722, mean_q: 1.168236
  60495/1000000: episode: 605, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 58.317, mean reward: 0.583 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.225, 10.187], loss: 0.001374, mae: 0.040960, mean_q: 1.166815
  60595/1000000: episode: 606, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 59.137, mean reward: 0.591 [0.520, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.241], loss: 0.001306, mae: 0.040009, mean_q: 1.168484
  60695/1000000: episode: 607, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 60.047, mean reward: 0.600 [0.500, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.563, 10.173], loss: 0.001338, mae: 0.039985, mean_q: 1.167066
  60795/1000000: episode: 608, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 56.970, mean reward: 0.570 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.929, 10.137], loss: 0.001453, mae: 0.041487, mean_q: 1.169847
  60895/1000000: episode: 609, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 61.522, mean reward: 0.615 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.424], loss: 0.001340, mae: 0.040386, mean_q: 1.166442
  60995/1000000: episode: 610, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.014, mean reward: 0.590 [0.516, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.129, 10.098], loss: 0.001285, mae: 0.039707, mean_q: 1.167993
  61095/1000000: episode: 611, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 58.712, mean reward: 0.587 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.233], loss: 0.001377, mae: 0.040671, mean_q: 1.167089
  61195/1000000: episode: 612, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 60.631, mean reward: 0.606 [0.500, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.471, 10.098], loss: 0.001292, mae: 0.039716, mean_q: 1.164805
  61295/1000000: episode: 613, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 58.340, mean reward: 0.583 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.513, 10.098], loss: 0.001321, mae: 0.040078, mean_q: 1.165317
  61395/1000000: episode: 614, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 58.173, mean reward: 0.582 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.451, 10.210], loss: 0.001357, mae: 0.040194, mean_q: 1.160964
  61495/1000000: episode: 615, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 56.061, mean reward: 0.561 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.916, 10.162], loss: 0.001362, mae: 0.040648, mean_q: 1.161507
  61595/1000000: episode: 616, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 58.820, mean reward: 0.588 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.591, 10.098], loss: 0.001302, mae: 0.039751, mean_q: 1.160744
  61695/1000000: episode: 617, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.657, mean reward: 0.577 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.514, 10.112], loss: 0.001384, mae: 0.041054, mean_q: 1.161578
  61795/1000000: episode: 618, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.335, mean reward: 0.573 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.292, 10.098], loss: 0.001361, mae: 0.040212, mean_q: 1.160501
  61895/1000000: episode: 619, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 61.998, mean reward: 0.620 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.918, 10.098], loss: 0.001406, mae: 0.040946, mean_q: 1.162070
  61995/1000000: episode: 620, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 60.599, mean reward: 0.606 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.454, 10.098], loss: 0.001317, mae: 0.039549, mean_q: 1.160965
  62095/1000000: episode: 621, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.142, mean reward: 0.571 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.008, 10.098], loss: 0.001391, mae: 0.040366, mean_q: 1.159316
  62195/1000000: episode: 622, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 62.122, mean reward: 0.621 [0.507, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.462], loss: 0.001385, mae: 0.040933, mean_q: 1.158193
  62295/1000000: episode: 623, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.893, mean reward: 0.589 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.931, 10.165], loss: 0.001412, mae: 0.041138, mean_q: 1.160362
  62395/1000000: episode: 624, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.643, mean reward: 0.586 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.147, 10.098], loss: 0.001351, mae: 0.040381, mean_q: 1.158444
  62495/1000000: episode: 625, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 56.705, mean reward: 0.567 [0.497, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.149, 10.098], loss: 0.001393, mae: 0.040886, mean_q: 1.166858
  62595/1000000: episode: 626, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 59.467, mean reward: 0.595 [0.502, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.191, 10.243], loss: 0.001418, mae: 0.040853, mean_q: 1.160334
  62695/1000000: episode: 627, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.682, mean reward: 0.597 [0.501, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.253, 10.277], loss: 0.001358, mae: 0.040438, mean_q: 1.159157
  62795/1000000: episode: 628, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.700, mean reward: 0.587 [0.501, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.490, 10.143], loss: 0.001417, mae: 0.041086, mean_q: 1.163704
  62895/1000000: episode: 629, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.181, mean reward: 0.582 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.544, 10.110], loss: 0.001491, mae: 0.042279, mean_q: 1.162021
  62995/1000000: episode: 630, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.469, mean reward: 0.585 [0.512, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.315, 10.213], loss: 0.001428, mae: 0.041553, mean_q: 1.162133
  63095/1000000: episode: 631, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 57.437, mean reward: 0.574 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.506, 10.142], loss: 0.001353, mae: 0.040844, mean_q: 1.163296
  63195/1000000: episode: 632, duration: 0.673s, episode steps: 100, steps per second: 148, episode reward: 60.414, mean reward: 0.604 [0.505, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.630, 10.098], loss: 0.001389, mae: 0.041265, mean_q: 1.161810
  63295/1000000: episode: 633, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 60.034, mean reward: 0.600 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.528, 10.170], loss: 0.001374, mae: 0.040372, mean_q: 1.161780
  63395/1000000: episode: 634, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 57.007, mean reward: 0.570 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.062, 10.236], loss: 0.001401, mae: 0.041368, mean_q: 1.158441
  63495/1000000: episode: 635, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 59.449, mean reward: 0.594 [0.507, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.297, 10.108], loss: 0.001397, mae: 0.040973, mean_q: 1.158713
  63595/1000000: episode: 636, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.887, mean reward: 0.579 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.736, 10.098], loss: 0.001465, mae: 0.041519, mean_q: 1.160654
  63695/1000000: episode: 637, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 60.834, mean reward: 0.608 [0.517, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.796, 10.190], loss: 0.001419, mae: 0.041213, mean_q: 1.162284
  63795/1000000: episode: 638, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 59.801, mean reward: 0.598 [0.511, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.326, 10.098], loss: 0.001437, mae: 0.041529, mean_q: 1.162840
  63895/1000000: episode: 639, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.439, mean reward: 0.584 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.878, 10.208], loss: 0.001361, mae: 0.040136, mean_q: 1.163743
  63995/1000000: episode: 640, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.432, mean reward: 0.574 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.733, 10.098], loss: 0.001499, mae: 0.042885, mean_q: 1.163776
  64095/1000000: episode: 641, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.922, mean reward: 0.579 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.798, 10.178], loss: 0.001402, mae: 0.040742, mean_q: 1.165055
  64195/1000000: episode: 642, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 59.794, mean reward: 0.598 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.897, 10.098], loss: 0.001401, mae: 0.040416, mean_q: 1.159865
  64295/1000000: episode: 643, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 56.893, mean reward: 0.569 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.905, 10.191], loss: 0.001459, mae: 0.041971, mean_q: 1.161793
  64395/1000000: episode: 644, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 60.628, mean reward: 0.606 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.235, 10.490], loss: 0.001560, mae: 0.043601, mean_q: 1.160274
  64495/1000000: episode: 645, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.584, mean reward: 0.586 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.048, 10.321], loss: 0.001531, mae: 0.043102, mean_q: 1.159791
  64595/1000000: episode: 646, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 59.213, mean reward: 0.592 [0.511, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.959, 10.098], loss: 0.001465, mae: 0.042202, mean_q: 1.162735
  64695/1000000: episode: 647, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.469, mean reward: 0.575 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.627, 10.098], loss: 0.001450, mae: 0.042147, mean_q: 1.166525
  64795/1000000: episode: 648, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.495, mean reward: 0.595 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.338, 10.129], loss: 0.001540, mae: 0.043347, mean_q: 1.163383
  64895/1000000: episode: 649, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.122, mean reward: 0.581 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.646, 10.098], loss: 0.001530, mae: 0.042048, mean_q: 1.160699
  64995/1000000: episode: 650, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 58.923, mean reward: 0.589 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.098], loss: 0.001410, mae: 0.041079, mean_q: 1.159815
  65095/1000000: episode: 651, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 56.712, mean reward: 0.567 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.223, 10.098], loss: 0.001438, mae: 0.041766, mean_q: 1.161405
  65195/1000000: episode: 652, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.090, mean reward: 0.591 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.311, 10.330], loss: 0.001477, mae: 0.042190, mean_q: 1.159523
  65295/1000000: episode: 653, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 57.845, mean reward: 0.578 [0.507, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.197], loss: 0.001477, mae: 0.042531, mean_q: 1.164231
  65395/1000000: episode: 654, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 59.804, mean reward: 0.598 [0.507, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.427, 10.244], loss: 0.001413, mae: 0.041136, mean_q: 1.161020
  65495/1000000: episode: 655, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.840, mean reward: 0.578 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.745, 10.197], loss: 0.001511, mae: 0.042269, mean_q: 1.165039
  65595/1000000: episode: 656, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 57.658, mean reward: 0.577 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.933, 10.380], loss: 0.001387, mae: 0.040911, mean_q: 1.159883
  65695/1000000: episode: 657, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.522, mean reward: 0.585 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.021, 10.215], loss: 0.001355, mae: 0.040571, mean_q: 1.159796
  65795/1000000: episode: 658, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 59.977, mean reward: 0.600 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.636, 10.225], loss: 0.001336, mae: 0.039905, mean_q: 1.160519
  65895/1000000: episode: 659, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.221, mean reward: 0.572 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.428, 10.210], loss: 0.001466, mae: 0.041526, mean_q: 1.162148
  65995/1000000: episode: 660, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 63.468, mean reward: 0.635 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.523, 10.098], loss: 0.001459, mae: 0.041610, mean_q: 1.164905
  66095/1000000: episode: 661, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 60.143, mean reward: 0.601 [0.520, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.288, 10.164], loss: 0.001372, mae: 0.040123, mean_q: 1.161608
  66195/1000000: episode: 662, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.695, mean reward: 0.577 [0.504, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.677, 10.302], loss: 0.001497, mae: 0.042349, mean_q: 1.162764
  66295/1000000: episode: 663, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 59.316, mean reward: 0.593 [0.510, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.050, 10.295], loss: 0.001427, mae: 0.041074, mean_q: 1.159259
  66395/1000000: episode: 664, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.744, mean reward: 0.587 [0.510, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.443, 10.098], loss: 0.001349, mae: 0.040429, mean_q: 1.159862
  66495/1000000: episode: 665, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.901, mean reward: 0.589 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.364, 10.098], loss: 0.001505, mae: 0.041885, mean_q: 1.164167
  66595/1000000: episode: 666, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 63.771, mean reward: 0.638 [0.536, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.775, 10.245], loss: 0.001351, mae: 0.040347, mean_q: 1.164117
  66695/1000000: episode: 667, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.196, mean reward: 0.592 [0.499, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.003, 10.098], loss: 0.001384, mae: 0.040378, mean_q: 1.166549
  66795/1000000: episode: 668, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 57.787, mean reward: 0.578 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.982, 10.101], loss: 0.001477, mae: 0.041482, mean_q: 1.162065
  66895/1000000: episode: 669, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 57.085, mean reward: 0.571 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.833, 10.098], loss: 0.001328, mae: 0.039853, mean_q: 1.162875
  66995/1000000: episode: 670, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 59.852, mean reward: 0.599 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.505, 10.287], loss: 0.001483, mae: 0.041840, mean_q: 1.163687
  67095/1000000: episode: 671, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 59.202, mean reward: 0.592 [0.503, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.347, 10.171], loss: 0.001463, mae: 0.040952, mean_q: 1.164508
  67195/1000000: episode: 672, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 62.175, mean reward: 0.622 [0.520, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.320, 10.298], loss: 0.001468, mae: 0.041005, mean_q: 1.162230
  67295/1000000: episode: 673, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 59.365, mean reward: 0.594 [0.511, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.014, 10.098], loss: 0.001461, mae: 0.041273, mean_q: 1.165720
  67395/1000000: episode: 674, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.847, mean reward: 0.598 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.372, 10.160], loss: 0.001451, mae: 0.040955, mean_q: 1.165907
  67495/1000000: episode: 675, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 58.562, mean reward: 0.586 [0.514, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.098], loss: 0.001427, mae: 0.041322, mean_q: 1.165721
  67595/1000000: episode: 676, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 59.606, mean reward: 0.596 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.977, 10.098], loss: 0.001459, mae: 0.041540, mean_q: 1.165986
  67695/1000000: episode: 677, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.205, mean reward: 0.572 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.562, 10.114], loss: 0.001488, mae: 0.041817, mean_q: 1.165596
  67795/1000000: episode: 678, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: 58.807, mean reward: 0.588 [0.509, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.704, 10.098], loss: 0.001614, mae: 0.043797, mean_q: 1.165549
  67895/1000000: episode: 679, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 58.994, mean reward: 0.590 [0.498, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.805, 10.244], loss: 0.001524, mae: 0.042440, mean_q: 1.163551
  67995/1000000: episode: 680, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 56.513, mean reward: 0.565 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.388, 10.098], loss: 0.001447, mae: 0.041727, mean_q: 1.164780
  68095/1000000: episode: 681, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 59.115, mean reward: 0.591 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.545, 10.107], loss: 0.001506, mae: 0.042188, mean_q: 1.165586
  68195/1000000: episode: 682, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.212, mean reward: 0.582 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.517, 10.322], loss: 0.001330, mae: 0.039757, mean_q: 1.161568
  68295/1000000: episode: 683, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 59.434, mean reward: 0.594 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.827, 10.243], loss: 0.001435, mae: 0.041433, mean_q: 1.162548
  68395/1000000: episode: 684, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 59.021, mean reward: 0.590 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.481, 10.098], loss: 0.001499, mae: 0.041897, mean_q: 1.163765
  68495/1000000: episode: 685, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 57.942, mean reward: 0.579 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.819, 10.227], loss: 0.001488, mae: 0.041792, mean_q: 1.164850
  68595/1000000: episode: 686, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 60.377, mean reward: 0.604 [0.522, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.248], loss: 0.001423, mae: 0.041217, mean_q: 1.165953
  68695/1000000: episode: 687, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 61.059, mean reward: 0.611 [0.501, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.607, 10.486], loss: 0.001384, mae: 0.040435, mean_q: 1.166685
  68795/1000000: episode: 688, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.907, mean reward: 0.579 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.159, 10.149], loss: 0.001383, mae: 0.040331, mean_q: 1.167550
  68895/1000000: episode: 689, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 56.699, mean reward: 0.567 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.388, 10.205], loss: 0.001358, mae: 0.040296, mean_q: 1.167006
  68995/1000000: episode: 690, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 59.377, mean reward: 0.594 [0.502, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.520, 10.214], loss: 0.001405, mae: 0.040771, mean_q: 1.163698
  69095/1000000: episode: 691, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 59.026, mean reward: 0.590 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.181], loss: 0.001336, mae: 0.039844, mean_q: 1.165568
  69195/1000000: episode: 692, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 59.594, mean reward: 0.596 [0.498, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.598, 10.098], loss: 0.001396, mae: 0.040885, mean_q: 1.165986
  69295/1000000: episode: 693, duration: 0.727s, episode steps: 100, steps per second: 137, episode reward: 59.248, mean reward: 0.592 [0.503, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.839, 10.098], loss: 0.001346, mae: 0.040005, mean_q: 1.168869
  69395/1000000: episode: 694, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 62.371, mean reward: 0.624 [0.515, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.476, 10.098], loss: 0.001305, mae: 0.039781, mean_q: 1.166049
  69495/1000000: episode: 695, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.230, mean reward: 0.582 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.824, 10.287], loss: 0.001486, mae: 0.042356, mean_q: 1.168674
  69595/1000000: episode: 696, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 58.415, mean reward: 0.584 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.028, 10.129], loss: 0.001354, mae: 0.039898, mean_q: 1.166017
  69695/1000000: episode: 697, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 57.652, mean reward: 0.577 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.608, 10.104], loss: 0.001476, mae: 0.041878, mean_q: 1.168326
  69795/1000000: episode: 698, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 56.303, mean reward: 0.563 [0.506, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.098], loss: 0.001322, mae: 0.039851, mean_q: 1.166989
  69895/1000000: episode: 699, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 61.516, mean reward: 0.615 [0.504, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.999, 10.098], loss: 0.001354, mae: 0.040054, mean_q: 1.166081
  69995/1000000: episode: 700, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.949, mean reward: 0.589 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.369, 10.098], loss: 0.001374, mae: 0.040669, mean_q: 1.166962
  70095/1000000: episode: 701, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.285, mean reward: 0.593 [0.518, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.098], loss: 0.001345, mae: 0.039841, mean_q: 1.164430
  70195/1000000: episode: 702, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.577, mean reward: 0.576 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.157, 10.234], loss: 0.001414, mae: 0.041061, mean_q: 1.164797
  70295/1000000: episode: 703, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 59.562, mean reward: 0.596 [0.522, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.744, 10.098], loss: 0.001464, mae: 0.042236, mean_q: 1.167457
  70395/1000000: episode: 704, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 63.181, mean reward: 0.632 [0.511, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.634, 10.098], loss: 0.001407, mae: 0.041014, mean_q: 1.168934
  70495/1000000: episode: 705, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 61.191, mean reward: 0.612 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.098], loss: 0.001408, mae: 0.041154, mean_q: 1.169101
  70595/1000000: episode: 706, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 59.514, mean reward: 0.595 [0.517, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.579, 10.257], loss: 0.001591, mae: 0.043616, mean_q: 1.171036
  70695/1000000: episode: 707, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 57.822, mean reward: 0.578 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.244], loss: 0.001450, mae: 0.041940, mean_q: 1.170933
  70795/1000000: episode: 708, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.726, mean reward: 0.587 [0.503, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.249, 10.167], loss: 0.001530, mae: 0.042756, mean_q: 1.169841
  70895/1000000: episode: 709, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 60.119, mean reward: 0.601 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.821, 10.098], loss: 0.001460, mae: 0.041762, mean_q: 1.172724
  70995/1000000: episode: 710, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 61.090, mean reward: 0.611 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.098], loss: 0.001479, mae: 0.041829, mean_q: 1.169763
  71095/1000000: episode: 711, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 57.465, mean reward: 0.575 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.764, 10.098], loss: 0.001435, mae: 0.041240, mean_q: 1.169179
  71195/1000000: episode: 712, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 61.367, mean reward: 0.614 [0.520, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.416, 10.098], loss: 0.001462, mae: 0.041711, mean_q: 1.170949
  71295/1000000: episode: 713, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 58.004, mean reward: 0.580 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.062, 10.098], loss: 0.001469, mae: 0.041524, mean_q: 1.171544
  71395/1000000: episode: 714, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.571, mean reward: 0.606 [0.507, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.116], loss: 0.001506, mae: 0.042124, mean_q: 1.172851
  71495/1000000: episode: 715, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 60.744, mean reward: 0.607 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.485, 10.209], loss: 0.001540, mae: 0.042572, mean_q: 1.172299
  71595/1000000: episode: 716, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 61.494, mean reward: 0.615 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.616, 10.098], loss: 0.001487, mae: 0.042350, mean_q: 1.170653
  71695/1000000: episode: 717, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 59.140, mean reward: 0.591 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.014, 10.098], loss: 0.001377, mae: 0.041143, mean_q: 1.170143
  71795/1000000: episode: 718, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.857, mean reward: 0.579 [0.506, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.621, 10.098], loss: 0.001466, mae: 0.041525, mean_q: 1.169702
  71895/1000000: episode: 719, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 59.265, mean reward: 0.593 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.147], loss: 0.001431, mae: 0.041500, mean_q: 1.171762
  71995/1000000: episode: 720, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 58.607, mean reward: 0.586 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.268, 10.415], loss: 0.001381, mae: 0.040656, mean_q: 1.170793
  72095/1000000: episode: 721, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 61.652, mean reward: 0.617 [0.531, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.098], loss: 0.001419, mae: 0.041297, mean_q: 1.172330
  72195/1000000: episode: 722, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.553, mean reward: 0.586 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.091, 10.098], loss: 0.001406, mae: 0.041514, mean_q: 1.171180
  72295/1000000: episode: 723, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 58.636, mean reward: 0.586 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.044, 10.098], loss: 0.001489, mae: 0.042655, mean_q: 1.170857
  72395/1000000: episode: 724, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 61.631, mean reward: 0.616 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.687, 10.098], loss: 0.001396, mae: 0.040726, mean_q: 1.171638
  72495/1000000: episode: 725, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 57.653, mean reward: 0.577 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.398, 10.098], loss: 0.001443, mae: 0.041305, mean_q: 1.171098
  72595/1000000: episode: 726, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.474, mean reward: 0.585 [0.520, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.934, 10.194], loss: 0.001413, mae: 0.041267, mean_q: 1.172996
  72695/1000000: episode: 727, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 58.779, mean reward: 0.588 [0.510, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.272, 10.098], loss: 0.001322, mae: 0.040259, mean_q: 1.170100
  72795/1000000: episode: 728, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.369, mean reward: 0.584 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.420, 10.252], loss: 0.001429, mae: 0.041766, mean_q: 1.170251
  72895/1000000: episode: 729, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 58.690, mean reward: 0.587 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.406, 10.118], loss: 0.001327, mae: 0.039932, mean_q: 1.171302
  72995/1000000: episode: 730, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 60.096, mean reward: 0.601 [0.519, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.456, 10.140], loss: 0.001389, mae: 0.041441, mean_q: 1.174208
  73095/1000000: episode: 731, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 62.887, mean reward: 0.629 [0.512, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.538, 10.098], loss: 0.001440, mae: 0.041633, mean_q: 1.173211
  73195/1000000: episode: 732, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.814, mean reward: 0.588 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.049, 10.267], loss: 0.001505, mae: 0.042243, mean_q: 1.173157
  73295/1000000: episode: 733, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.763, mean reward: 0.588 [0.503, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 0.001442, mae: 0.041022, mean_q: 1.174319
  73395/1000000: episode: 734, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.465, mean reward: 0.585 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.041, 10.098], loss: 0.001543, mae: 0.043236, mean_q: 1.174760
  73495/1000000: episode: 735, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.988, mean reward: 0.590 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.508, 10.197], loss: 0.001490, mae: 0.041901, mean_q: 1.177156
  73595/1000000: episode: 736, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 58.793, mean reward: 0.588 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.141, 10.098], loss: 0.001524, mae: 0.042855, mean_q: 1.175924
  73695/1000000: episode: 737, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.742, mean reward: 0.597 [0.526, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.276], loss: 0.001331, mae: 0.039960, mean_q: 1.172466
  73795/1000000: episode: 738, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 57.311, mean reward: 0.573 [0.504, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.739, 10.443], loss: 0.001509, mae: 0.042712, mean_q: 1.173690
  73895/1000000: episode: 739, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.022, mean reward: 0.590 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.075, 10.310], loss: 0.001453, mae: 0.041642, mean_q: 1.172472
  73995/1000000: episode: 740, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.164, mean reward: 0.592 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.846, 10.098], loss: 0.001441, mae: 0.041387, mean_q: 1.178249
  74095/1000000: episode: 741, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.594, mean reward: 0.596 [0.507, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.439, 10.122], loss: 0.001388, mae: 0.041020, mean_q: 1.171768
  74195/1000000: episode: 742, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 57.757, mean reward: 0.578 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.345], loss: 0.001434, mae: 0.041291, mean_q: 1.169000
  74295/1000000: episode: 743, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.377, mean reward: 0.594 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.639, 10.098], loss: 0.001409, mae: 0.040964, mean_q: 1.175437
  74395/1000000: episode: 744, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 58.039, mean reward: 0.580 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.714, 10.153], loss: 0.001463, mae: 0.041935, mean_q: 1.173352
  74495/1000000: episode: 745, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 58.540, mean reward: 0.585 [0.512, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.520, 10.170], loss: 0.001368, mae: 0.040190, mean_q: 1.169770
  74595/1000000: episode: 746, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 58.051, mean reward: 0.581 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.945, 10.203], loss: 0.001431, mae: 0.041453, mean_q: 1.171557
  74695/1000000: episode: 747, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 60.224, mean reward: 0.602 [0.519, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.076, 10.098], loss: 0.001437, mae: 0.042024, mean_q: 1.173922
  74795/1000000: episode: 748, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 59.087, mean reward: 0.591 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.343], loss: 0.001468, mae: 0.041959, mean_q: 1.173698
  74895/1000000: episode: 749, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.548, mean reward: 0.575 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.596, 10.098], loss: 0.001396, mae: 0.041032, mean_q: 1.174308
  74995/1000000: episode: 750, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 58.997, mean reward: 0.590 [0.499, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.760, 10.283], loss: 0.001504, mae: 0.042500, mean_q: 1.174269
  75095/1000000: episode: 751, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.832, mean reward: 0.588 [0.505, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.709, 10.203], loss: 0.001458, mae: 0.041751, mean_q: 1.175189
  75195/1000000: episode: 752, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.011, mean reward: 0.590 [0.508, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.027, 10.098], loss: 0.001411, mae: 0.041268, mean_q: 1.171991
  75295/1000000: episode: 753, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.737, mean reward: 0.577 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.354, 10.201], loss: 0.001470, mae: 0.041790, mean_q: 1.172604
  75395/1000000: episode: 754, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 60.629, mean reward: 0.606 [0.512, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.520, 10.128], loss: 0.001383, mae: 0.040582, mean_q: 1.171389
  75495/1000000: episode: 755, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 58.455, mean reward: 0.585 [0.500, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.590, 10.280], loss: 0.001380, mae: 0.041030, mean_q: 1.172984
  75595/1000000: episode: 756, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 57.921, mean reward: 0.579 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.236], loss: 0.001333, mae: 0.040656, mean_q: 1.167804
  75695/1000000: episode: 757, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 57.963, mean reward: 0.580 [0.509, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.436, 10.129], loss: 0.001450, mae: 0.041590, mean_q: 1.169774
  75795/1000000: episode: 758, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 58.351, mean reward: 0.584 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.451, 10.307], loss: 0.001457, mae: 0.041385, mean_q: 1.168202
  75895/1000000: episode: 759, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 58.798, mean reward: 0.588 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.622, 10.229], loss: 0.001548, mae: 0.043118, mean_q: 1.171289
  75995/1000000: episode: 760, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 58.740, mean reward: 0.587 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.135], loss: 0.001490, mae: 0.042236, mean_q: 1.168358
  76095/1000000: episode: 761, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.334, mean reward: 0.573 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.098], loss: 0.001423, mae: 0.041287, mean_q: 1.169786
  76195/1000000: episode: 762, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 58.002, mean reward: 0.580 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.601, 10.390], loss: 0.001321, mae: 0.040234, mean_q: 1.167161
  76295/1000000: episode: 763, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 60.172, mean reward: 0.602 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.734, 10.359], loss: 0.001299, mae: 0.039534, mean_q: 1.166263
  76395/1000000: episode: 764, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 58.370, mean reward: 0.584 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.098], loss: 0.001381, mae: 0.040251, mean_q: 1.166105
  76495/1000000: episode: 765, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 58.156, mean reward: 0.582 [0.508, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.190, 10.098], loss: 0.001382, mae: 0.040718, mean_q: 1.168967
  76595/1000000: episode: 766, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 58.299, mean reward: 0.583 [0.516, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.191, 10.194], loss: 0.001279, mae: 0.039163, mean_q: 1.164755
  76695/1000000: episode: 767, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 58.828, mean reward: 0.588 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.127], loss: 0.001400, mae: 0.040828, mean_q: 1.163134
  76795/1000000: episode: 768, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.263, mean reward: 0.573 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.826, 10.098], loss: 0.001353, mae: 0.040124, mean_q: 1.165251
  76895/1000000: episode: 769, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.648, mean reward: 0.576 [0.500, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.113, 10.098], loss: 0.001393, mae: 0.040348, mean_q: 1.160713
  76995/1000000: episode: 770, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 58.645, mean reward: 0.586 [0.512, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.526, 10.098], loss: 0.001329, mae: 0.039845, mean_q: 1.162514
  77095/1000000: episode: 771, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 57.699, mean reward: 0.577 [0.497, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.904, 10.101], loss: 0.001356, mae: 0.039836, mean_q: 1.158427
  77195/1000000: episode: 772, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 59.339, mean reward: 0.593 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.778, 10.098], loss: 0.001388, mae: 0.040542, mean_q: 1.162147
  77295/1000000: episode: 773, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 61.128, mean reward: 0.611 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.627, 10.098], loss: 0.001469, mae: 0.041163, mean_q: 1.160511
  77395/1000000: episode: 774, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 57.267, mean reward: 0.573 [0.499, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.541, 10.338], loss: 0.001454, mae: 0.041095, mean_q: 1.162145
  77495/1000000: episode: 775, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 58.469, mean reward: 0.585 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.564, 10.141], loss: 0.001467, mae: 0.041536, mean_q: 1.161633
  77595/1000000: episode: 776, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 58.782, mean reward: 0.588 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.458, 10.190], loss: 0.001403, mae: 0.041009, mean_q: 1.159558
  77695/1000000: episode: 777, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 59.158, mean reward: 0.592 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.152, 10.174], loss: 0.001505, mae: 0.042697, mean_q: 1.160867
  77795/1000000: episode: 778, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 56.797, mean reward: 0.568 [0.514, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.302], loss: 0.001378, mae: 0.040983, mean_q: 1.160410
  77895/1000000: episode: 779, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.029, mean reward: 0.600 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.805, 10.166], loss: 0.001371, mae: 0.039758, mean_q: 1.162400
  77995/1000000: episode: 780, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 59.745, mean reward: 0.597 [0.514, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.756, 10.428], loss: 0.001467, mae: 0.040887, mean_q: 1.159796
  78095/1000000: episode: 781, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 64.133, mean reward: 0.641 [0.512, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.122, 10.098], loss: 0.001444, mae: 0.040902, mean_q: 1.160343
  78195/1000000: episode: 782, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 60.558, mean reward: 0.606 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.990, 10.098], loss: 0.001339, mae: 0.040651, mean_q: 1.163324
  78295/1000000: episode: 783, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.463, mean reward: 0.585 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.316, 10.098], loss: 0.001467, mae: 0.040982, mean_q: 1.160868
  78395/1000000: episode: 784, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.591, mean reward: 0.576 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.374, 10.098], loss: 0.001460, mae: 0.041437, mean_q: 1.163122
  78495/1000000: episode: 785, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 57.032, mean reward: 0.570 [0.499, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.796, 10.154], loss: 0.001482, mae: 0.041440, mean_q: 1.160523
  78595/1000000: episode: 786, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 57.982, mean reward: 0.580 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.477, 10.098], loss: 0.001362, mae: 0.040370, mean_q: 1.159944
  78695/1000000: episode: 787, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.919, mean reward: 0.579 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.991, 10.218], loss: 0.001495, mae: 0.041566, mean_q: 1.161134
  78795/1000000: episode: 788, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 56.597, mean reward: 0.566 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.511, 10.098], loss: 0.001454, mae: 0.041551, mean_q: 1.159912
  78895/1000000: episode: 789, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 58.354, mean reward: 0.584 [0.506, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.098], loss: 0.001614, mae: 0.043538, mean_q: 1.158092
  78995/1000000: episode: 790, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 59.441, mean reward: 0.594 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.294], loss: 0.001502, mae: 0.042103, mean_q: 1.159224
  79095/1000000: episode: 791, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 56.703, mean reward: 0.567 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.285], loss: 0.001427, mae: 0.040961, mean_q: 1.159497
  79195/1000000: episode: 792, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 60.818, mean reward: 0.608 [0.520, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 0.001501, mae: 0.041777, mean_q: 1.158644
  79295/1000000: episode: 793, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 57.701, mean reward: 0.577 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.849, 10.098], loss: 0.001529, mae: 0.042076, mean_q: 1.160904
  79395/1000000: episode: 794, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 61.542, mean reward: 0.615 [0.518, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.031, 10.508], loss: 0.001535, mae: 0.042477, mean_q: 1.159058
  79495/1000000: episode: 795, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.532, mean reward: 0.585 [0.505, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.993, 10.170], loss: 0.001511, mae: 0.041507, mean_q: 1.160021
  79595/1000000: episode: 796, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 57.362, mean reward: 0.574 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.634, 10.315], loss: 0.001552, mae: 0.042566, mean_q: 1.160762
  79695/1000000: episode: 797, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 58.050, mean reward: 0.580 [0.514, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.768, 10.207], loss: 0.001565, mae: 0.042527, mean_q: 1.158125
  79795/1000000: episode: 798, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 60.034, mean reward: 0.600 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.085, 10.168], loss: 0.001463, mae: 0.041562, mean_q: 1.159681
  79895/1000000: episode: 799, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.661, mean reward: 0.597 [0.519, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.856, 10.275], loss: 0.001535, mae: 0.042358, mean_q: 1.161316
  79995/1000000: episode: 800, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 57.188, mean reward: 0.572 [0.498, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.320, 10.135], loss: 0.001484, mae: 0.041801, mean_q: 1.161373
  80095/1000000: episode: 801, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 58.426, mean reward: 0.584 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.349, 10.117], loss: 0.001414, mae: 0.040737, mean_q: 1.159017
  80195/1000000: episode: 802, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.093, mean reward: 0.591 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.001447, mae: 0.041319, mean_q: 1.160304
  80295/1000000: episode: 803, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 59.216, mean reward: 0.592 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.221, 10.109], loss: 0.001535, mae: 0.042310, mean_q: 1.163280
  80395/1000000: episode: 804, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 58.125, mean reward: 0.581 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.862, 10.242], loss: 0.001545, mae: 0.042725, mean_q: 1.158520
  80495/1000000: episode: 805, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 57.990, mean reward: 0.580 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.514, 10.098], loss: 0.001388, mae: 0.040588, mean_q: 1.158664
  80595/1000000: episode: 806, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.044, mean reward: 0.570 [0.511, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.558, 10.098], loss: 0.001498, mae: 0.041825, mean_q: 1.160524
  80695/1000000: episode: 807, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 58.409, mean reward: 0.584 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.567, 10.200], loss: 0.001522, mae: 0.042090, mean_q: 1.159712
  80795/1000000: episode: 808, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.999, mean reward: 0.600 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.154, 10.293], loss: 0.001689, mae: 0.044288, mean_q: 1.160103
  80895/1000000: episode: 809, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 59.603, mean reward: 0.596 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.117, 10.098], loss: 0.001631, mae: 0.043944, mean_q: 1.159318
  80995/1000000: episode: 810, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 62.198, mean reward: 0.622 [0.503, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.359, 10.361], loss: 0.001577, mae: 0.043078, mean_q: 1.161223
  81095/1000000: episode: 811, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 58.518, mean reward: 0.585 [0.503, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.699, 10.098], loss: 0.001474, mae: 0.041061, mean_q: 1.163487
  81195/1000000: episode: 812, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 59.341, mean reward: 0.593 [0.515, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.777, 10.163], loss: 0.001526, mae: 0.042450, mean_q: 1.166276
  81295/1000000: episode: 813, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 59.677, mean reward: 0.597 [0.512, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.260, 10.098], loss: 0.001601, mae: 0.042887, mean_q: 1.160894
  81395/1000000: episode: 814, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.294, mean reward: 0.573 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.228], loss: 0.001430, mae: 0.041283, mean_q: 1.161624
  81495/1000000: episode: 815, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 57.472, mean reward: 0.575 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.245, 10.180], loss: 0.001551, mae: 0.042950, mean_q: 1.164107
  81595/1000000: episode: 816, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 59.888, mean reward: 0.599 [0.518, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.584, 10.098], loss: 0.001506, mae: 0.042130, mean_q: 1.162003
  81695/1000000: episode: 817, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.631, mean reward: 0.586 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.937, 10.268], loss: 0.001588, mae: 0.042939, mean_q: 1.163185
  81795/1000000: episode: 818, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 57.712, mean reward: 0.577 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.859, 10.098], loss: 0.001539, mae: 0.041941, mean_q: 1.161421
  81895/1000000: episode: 819, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 56.756, mean reward: 0.568 [0.502, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.100], loss: 0.001469, mae: 0.041427, mean_q: 1.160277
  81995/1000000: episode: 820, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 56.789, mean reward: 0.568 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.246, 10.195], loss: 0.001527, mae: 0.042032, mean_q: 1.162143
  82095/1000000: episode: 821, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 58.448, mean reward: 0.584 [0.515, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.703, 10.165], loss: 0.001626, mae: 0.043695, mean_q: 1.163110
  82195/1000000: episode: 822, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 58.317, mean reward: 0.583 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.495, 10.133], loss: 0.001501, mae: 0.041603, mean_q: 1.164520
  82295/1000000: episode: 823, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 56.937, mean reward: 0.569 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.173], loss: 0.001467, mae: 0.041684, mean_q: 1.159618
  82395/1000000: episode: 824, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.582, mean reward: 0.576 [0.507, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.188, 10.201], loss: 0.001499, mae: 0.042035, mean_q: 1.160818
  82495/1000000: episode: 825, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 58.900, mean reward: 0.589 [0.507, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.534, 10.156], loss: 0.001583, mae: 0.043131, mean_q: 1.159600
  82595/1000000: episode: 826, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.507, mean reward: 0.585 [0.508, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.355, 10.098], loss: 0.001517, mae: 0.042314, mean_q: 1.160161
  82695/1000000: episode: 827, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 57.418, mean reward: 0.574 [0.498, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.958, 10.098], loss: 0.001503, mae: 0.042259, mean_q: 1.161385
  82795/1000000: episode: 828, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 58.965, mean reward: 0.590 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.025, 10.241], loss: 0.001506, mae: 0.042616, mean_q: 1.162723
  82895/1000000: episode: 829, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 59.587, mean reward: 0.596 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.215, 10.098], loss: 0.001460, mae: 0.042038, mean_q: 1.162020
  82995/1000000: episode: 830, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 60.338, mean reward: 0.603 [0.508, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.098], loss: 0.001421, mae: 0.041200, mean_q: 1.159964
  83095/1000000: episode: 831, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.722, mean reward: 0.597 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.298, 10.388], loss: 0.001468, mae: 0.042161, mean_q: 1.155883
  83195/1000000: episode: 832, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 57.882, mean reward: 0.579 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.344, 10.098], loss: 0.001427, mae: 0.041275, mean_q: 1.154273
  83295/1000000: episode: 833, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 65.207, mean reward: 0.652 [0.515, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.902, 10.371], loss: 0.001486, mae: 0.042102, mean_q: 1.156980
  83395/1000000: episode: 834, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.129, mean reward: 0.591 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.584, 10.098], loss: 0.001563, mae: 0.043493, mean_q: 1.160303
  83495/1000000: episode: 835, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.687, mean reward: 0.597 [0.505, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.289], loss: 0.001490, mae: 0.041791, mean_q: 1.161863
  83595/1000000: episode: 836, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 58.862, mean reward: 0.589 [0.510, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.416, 10.164], loss: 0.001561, mae: 0.043052, mean_q: 1.162836
  83695/1000000: episode: 837, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.449, mean reward: 0.584 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.367, 10.351], loss: 0.001427, mae: 0.041432, mean_q: 1.157865
  83795/1000000: episode: 838, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 57.843, mean reward: 0.578 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.183, 10.174], loss: 0.001547, mae: 0.042332, mean_q: 1.160786
  83895/1000000: episode: 839, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 59.475, mean reward: 0.595 [0.512, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.828, 10.258], loss: 0.001336, mae: 0.040042, mean_q: 1.158582
  83995/1000000: episode: 840, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.192, mean reward: 0.592 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.605, 10.185], loss: 0.001381, mae: 0.041291, mean_q: 1.162263
  84095/1000000: episode: 841, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.821, mean reward: 0.588 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.867, 10.142], loss: 0.001458, mae: 0.041996, mean_q: 1.163056
  84195/1000000: episode: 842, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 59.121, mean reward: 0.591 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.493, 10.098], loss: 0.001465, mae: 0.041988, mean_q: 1.162089
  84295/1000000: episode: 843, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 57.097, mean reward: 0.571 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.369, 10.242], loss: 0.001415, mae: 0.041445, mean_q: 1.166875
  84395/1000000: episode: 844, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 57.141, mean reward: 0.571 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.200, 10.237], loss: 0.001376, mae: 0.040493, mean_q: 1.163600
  84495/1000000: episode: 845, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 57.905, mean reward: 0.579 [0.501, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.042, 10.240], loss: 0.001339, mae: 0.040082, mean_q: 1.161376
  84595/1000000: episode: 846, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.276, mean reward: 0.583 [0.510, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.416, 10.098], loss: 0.001414, mae: 0.041016, mean_q: 1.163549
  84695/1000000: episode: 847, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 57.485, mean reward: 0.575 [0.515, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.226], loss: 0.001491, mae: 0.042219, mean_q: 1.159536
  84795/1000000: episode: 848, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 58.822, mean reward: 0.588 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.098], loss: 0.001364, mae: 0.040401, mean_q: 1.161639
  84895/1000000: episode: 849, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 57.389, mean reward: 0.574 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.734, 10.098], loss: 0.001299, mae: 0.039499, mean_q: 1.160000
  84995/1000000: episode: 850, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 57.820, mean reward: 0.578 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.753, 10.098], loss: 0.001306, mae: 0.039657, mean_q: 1.156473
  85095/1000000: episode: 851, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 56.704, mean reward: 0.567 [0.497, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.258, 10.157], loss: 0.001407, mae: 0.040745, mean_q: 1.161582
  85195/1000000: episode: 852, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.708, mean reward: 0.577 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.429, 10.187], loss: 0.001315, mae: 0.039665, mean_q: 1.156266
  85295/1000000: episode: 853, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.062, mean reward: 0.581 [0.513, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.274, 10.114], loss: 0.001376, mae: 0.040460, mean_q: 1.158308
  85395/1000000: episode: 854, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.507, mean reward: 0.585 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.147, 10.340], loss: 0.001212, mae: 0.038308, mean_q: 1.155816
  85495/1000000: episode: 855, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 58.121, mean reward: 0.581 [0.513, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.919, 10.098], loss: 0.001272, mae: 0.038410, mean_q: 1.158140
  85595/1000000: episode: 856, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 59.285, mean reward: 0.593 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.873, 10.098], loss: 0.001424, mae: 0.040698, mean_q: 1.162528
  85695/1000000: episode: 857, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.248, mean reward: 0.582 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.643, 10.098], loss: 0.001342, mae: 0.040193, mean_q: 1.159140
  85795/1000000: episode: 858, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 64.922, mean reward: 0.649 [0.507, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.908, 10.602], loss: 0.001325, mae: 0.039009, mean_q: 1.160830
  85895/1000000: episode: 859, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.511, mean reward: 0.585 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.129, 10.098], loss: 0.001323, mae: 0.039495, mean_q: 1.161710
  85995/1000000: episode: 860, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 66.351, mean reward: 0.664 [0.517, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.442, 10.511], loss: 0.001384, mae: 0.041051, mean_q: 1.163043
  86095/1000000: episode: 861, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 56.507, mean reward: 0.565 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.478, 10.098], loss: 0.001331, mae: 0.039650, mean_q: 1.162777
  86195/1000000: episode: 862, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.161, mean reward: 0.592 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.351, 10.245], loss: 0.001309, mae: 0.039494, mean_q: 1.160877
  86295/1000000: episode: 863, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 59.049, mean reward: 0.590 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.098], loss: 0.001394, mae: 0.040439, mean_q: 1.160607
  86395/1000000: episode: 864, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 61.122, mean reward: 0.611 [0.505, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.059, 10.098], loss: 0.001254, mae: 0.039063, mean_q: 1.160858
  86495/1000000: episode: 865, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.838, mean reward: 0.578 [0.507, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.256, 10.117], loss: 0.001387, mae: 0.040422, mean_q: 1.163236
  86595/1000000: episode: 866, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 57.960, mean reward: 0.580 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.098], loss: 0.001381, mae: 0.040304, mean_q: 1.162638
  86695/1000000: episode: 867, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 57.443, mean reward: 0.574 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.154], loss: 0.001366, mae: 0.040541, mean_q: 1.161300
  86795/1000000: episode: 868, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 62.747, mean reward: 0.627 [0.501, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.418, 10.098], loss: 0.001405, mae: 0.040806, mean_q: 1.163926
  86895/1000000: episode: 869, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.216, mean reward: 0.572 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.278, 10.253], loss: 0.001216, mae: 0.037701, mean_q: 1.161605
  86995/1000000: episode: 870, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.501, mean reward: 0.605 [0.524, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.194, 10.098], loss: 0.001298, mae: 0.039013, mean_q: 1.164730
  87095/1000000: episode: 871, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 59.894, mean reward: 0.599 [0.503, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.434, 10.148], loss: 0.001294, mae: 0.039842, mean_q: 1.166207
  87195/1000000: episode: 872, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 61.061, mean reward: 0.611 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.492, 10.098], loss: 0.001309, mae: 0.039137, mean_q: 1.164374
  87295/1000000: episode: 873, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 60.513, mean reward: 0.605 [0.505, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.070, 10.239], loss: 0.001365, mae: 0.040399, mean_q: 1.166095
  87395/1000000: episode: 874, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 56.895, mean reward: 0.569 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.879, 10.098], loss: 0.001431, mae: 0.041473, mean_q: 1.170250
  87495/1000000: episode: 875, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 58.909, mean reward: 0.589 [0.499, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.563, 10.098], loss: 0.001293, mae: 0.038998, mean_q: 1.167767
  87595/1000000: episode: 876, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 58.665, mean reward: 0.587 [0.514, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.568, 10.098], loss: 0.001382, mae: 0.041094, mean_q: 1.166263
  87695/1000000: episode: 877, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.269, mean reward: 0.573 [0.506, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.644, 10.160], loss: 0.001470, mae: 0.041175, mean_q: 1.168652
  87795/1000000: episode: 878, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 58.024, mean reward: 0.580 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.924, 10.320], loss: 0.001368, mae: 0.040690, mean_q: 1.171000
  87895/1000000: episode: 879, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.580, mean reward: 0.596 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.534, 10.163], loss: 0.001399, mae: 0.041517, mean_q: 1.168915
  87995/1000000: episode: 880, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 57.743, mean reward: 0.577 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.188], loss: 0.001284, mae: 0.039376, mean_q: 1.168547
  88095/1000000: episode: 881, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 59.444, mean reward: 0.594 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.316], loss: 0.001480, mae: 0.041770, mean_q: 1.162763
  88195/1000000: episode: 882, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.256, mean reward: 0.593 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.845, 10.203], loss: 0.001460, mae: 0.042025, mean_q: 1.164034
  88295/1000000: episode: 883, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.235, mean reward: 0.582 [0.504, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.648, 10.277], loss: 0.001460, mae: 0.041411, mean_q: 1.164790
  88395/1000000: episode: 884, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 56.962, mean reward: 0.570 [0.503, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.522, 10.145], loss: 0.001371, mae: 0.040795, mean_q: 1.164751
  88495/1000000: episode: 885, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 59.348, mean reward: 0.593 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.098], loss: 0.001508, mae: 0.042720, mean_q: 1.162274
  88595/1000000: episode: 886, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 60.762, mean reward: 0.608 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.785, 10.183], loss: 0.001306, mae: 0.039950, mean_q: 1.165303
  88695/1000000: episode: 887, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.120, mean reward: 0.581 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.245, 10.098], loss: 0.001396, mae: 0.041052, mean_q: 1.166080
  88795/1000000: episode: 888, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: 59.908, mean reward: 0.599 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.910, 10.124], loss: 0.001483, mae: 0.041781, mean_q: 1.165827
  88895/1000000: episode: 889, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.810, mean reward: 0.578 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.506, 10.110], loss: 0.001441, mae: 0.041152, mean_q: 1.162811
  88995/1000000: episode: 890, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 59.344, mean reward: 0.593 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.665, 10.196], loss: 0.001438, mae: 0.041749, mean_q: 1.164769
  89095/1000000: episode: 891, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.009, mean reward: 0.590 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.357, 10.269], loss: 0.001381, mae: 0.040642, mean_q: 1.164395
  89195/1000000: episode: 892, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 61.152, mean reward: 0.612 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.464, 10.098], loss: 0.001253, mae: 0.039210, mean_q: 1.163467
  89295/1000000: episode: 893, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 57.071, mean reward: 0.571 [0.500, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.563, 10.098], loss: 0.001358, mae: 0.040392, mean_q: 1.164138
  89395/1000000: episode: 894, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 57.213, mean reward: 0.572 [0.500, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.071, 10.098], loss: 0.001426, mae: 0.041373, mean_q: 1.168321
  89495/1000000: episode: 895, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 57.294, mean reward: 0.573 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.891, 10.231], loss: 0.001354, mae: 0.039492, mean_q: 1.164814
  89595/1000000: episode: 896, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 58.581, mean reward: 0.586 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.456, 10.098], loss: 0.001454, mae: 0.041694, mean_q: 1.166006
  89695/1000000: episode: 897, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 60.526, mean reward: 0.605 [0.521, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.955, 10.098], loss: 0.001453, mae: 0.041354, mean_q: 1.167291
  89795/1000000: episode: 898, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 56.705, mean reward: 0.567 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.018, 10.098], loss: 0.001335, mae: 0.040110, mean_q: 1.166518
  89895/1000000: episode: 899, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 58.321, mean reward: 0.583 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.591, 10.098], loss: 0.001421, mae: 0.040573, mean_q: 1.168899
  89995/1000000: episode: 900, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.114, mean reward: 0.571 [0.505, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.620, 10.098], loss: 0.001392, mae: 0.040441, mean_q: 1.165155
  90095/1000000: episode: 901, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 60.560, mean reward: 0.606 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.077, 10.124], loss: 0.001373, mae: 0.040311, mean_q: 1.164156
  90195/1000000: episode: 902, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 63.525, mean reward: 0.635 [0.512, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.216, 10.098], loss: 0.001516, mae: 0.041976, mean_q: 1.167968
  90295/1000000: episode: 903, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 62.822, mean reward: 0.628 [0.512, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.935, 10.098], loss: 0.001331, mae: 0.039953, mean_q: 1.170747
  90395/1000000: episode: 904, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 59.239, mean reward: 0.592 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.550, 10.246], loss: 0.001481, mae: 0.042207, mean_q: 1.174086
  90495/1000000: episode: 905, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 59.685, mean reward: 0.597 [0.506, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.050, 10.098], loss: 0.001388, mae: 0.040890, mean_q: 1.168759
  90595/1000000: episode: 906, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 61.517, mean reward: 0.615 [0.513, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.207, 10.538], loss: 0.001375, mae: 0.040704, mean_q: 1.173810
  90695/1000000: episode: 907, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 61.458, mean reward: 0.615 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.332, 10.307], loss: 0.001438, mae: 0.041320, mean_q: 1.175186
  90795/1000000: episode: 908, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.939, mean reward: 0.579 [0.509, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.696, 10.231], loss: 0.001415, mae: 0.041002, mean_q: 1.172261
  90895/1000000: episode: 909, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 58.089, mean reward: 0.581 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.749, 10.154], loss: 0.001482, mae: 0.042235, mean_q: 1.171412
  90995/1000000: episode: 910, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.132, mean reward: 0.581 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.096, 10.098], loss: 0.001318, mae: 0.039883, mean_q: 1.171309
  91095/1000000: episode: 911, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 59.613, mean reward: 0.596 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.794, 10.098], loss: 0.001554, mae: 0.043383, mean_q: 1.169741
  91195/1000000: episode: 912, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.242, mean reward: 0.592 [0.519, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.773, 10.098], loss: 0.001407, mae: 0.041554, mean_q: 1.170820
  91295/1000000: episode: 913, duration: 0.712s, episode steps: 100, steps per second: 141, episode reward: 62.238, mean reward: 0.622 [0.506, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.848, 10.168], loss: 0.001444, mae: 0.041346, mean_q: 1.171726
  91395/1000000: episode: 914, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 58.795, mean reward: 0.588 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.355, 10.098], loss: 0.001296, mae: 0.039619, mean_q: 1.170762
  91495/1000000: episode: 915, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 58.337, mean reward: 0.583 [0.512, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.951, 10.228], loss: 0.001416, mae: 0.041301, mean_q: 1.169379
  91595/1000000: episode: 916, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.505, mean reward: 0.585 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.098], loss: 0.001459, mae: 0.041963, mean_q: 1.171030
  91695/1000000: episode: 917, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.814, mean reward: 0.578 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.424, 10.298], loss: 0.001403, mae: 0.041151, mean_q: 1.171348
  91795/1000000: episode: 918, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 59.820, mean reward: 0.598 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.156], loss: 0.001440, mae: 0.041067, mean_q: 1.168945
  91895/1000000: episode: 919, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 59.107, mean reward: 0.591 [0.511, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.689, 10.124], loss: 0.001436, mae: 0.041714, mean_q: 1.169634
  91995/1000000: episode: 920, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 58.904, mean reward: 0.589 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.172], loss: 0.001549, mae: 0.043501, mean_q: 1.170212
  92095/1000000: episode: 921, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 66.297, mean reward: 0.663 [0.516, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.897, 10.098], loss: 0.001409, mae: 0.041253, mean_q: 1.168470
  92195/1000000: episode: 922, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 62.178, mean reward: 0.622 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.051, 10.396], loss: 0.001371, mae: 0.040277, mean_q: 1.166433
  92295/1000000: episode: 923, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 60.036, mean reward: 0.600 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.941, 10.200], loss: 0.001443, mae: 0.041900, mean_q: 1.170294
  92395/1000000: episode: 924, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.818, mean reward: 0.588 [0.514, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.073, 10.098], loss: 0.001374, mae: 0.040820, mean_q: 1.171420
  92495/1000000: episode: 925, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 59.871, mean reward: 0.599 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.441, 10.146], loss: 0.001413, mae: 0.041113, mean_q: 1.173834
  92595/1000000: episode: 926, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 61.097, mean reward: 0.611 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.648, 10.098], loss: 0.001337, mae: 0.040283, mean_q: 1.173067
  92695/1000000: episode: 927, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.781, mean reward: 0.588 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.110, 10.275], loss: 0.001471, mae: 0.041825, mean_q: 1.177745
  92795/1000000: episode: 928, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.242, mean reward: 0.582 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.017, 10.098], loss: 0.001473, mae: 0.042191, mean_q: 1.173445
  92895/1000000: episode: 929, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 59.113, mean reward: 0.591 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.359, 10.145], loss: 0.001461, mae: 0.042025, mean_q: 1.172811
  92995/1000000: episode: 930, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 57.560, mean reward: 0.576 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.929, 10.178], loss: 0.001430, mae: 0.041449, mean_q: 1.177255
  93095/1000000: episode: 931, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 61.432, mean reward: 0.614 [0.525, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.921, 10.296], loss: 0.001411, mae: 0.041392, mean_q: 1.173827
  93195/1000000: episode: 932, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 58.201, mean reward: 0.582 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.183, 10.217], loss: 0.001355, mae: 0.040677, mean_q: 1.176297
  93295/1000000: episode: 933, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 59.199, mean reward: 0.592 [0.512, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.582, 10.098], loss: 0.001481, mae: 0.041871, mean_q: 1.176954
  93395/1000000: episode: 934, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 58.240, mean reward: 0.582 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.563, 10.205], loss: 0.001489, mae: 0.042770, mean_q: 1.175112
  93495/1000000: episode: 935, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 58.543, mean reward: 0.585 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.329, 10.098], loss: 0.001430, mae: 0.041480, mean_q: 1.174615
  93595/1000000: episode: 936, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 60.054, mean reward: 0.601 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.271], loss: 0.001416, mae: 0.041409, mean_q: 1.172674
  93695/1000000: episode: 937, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 57.401, mean reward: 0.574 [0.515, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.121, 10.098], loss: 0.001398, mae: 0.040517, mean_q: 1.172182
  93795/1000000: episode: 938, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.451, mean reward: 0.605 [0.515, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.104], loss: 0.001519, mae: 0.042912, mean_q: 1.178970
  93895/1000000: episode: 939, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 59.120, mean reward: 0.591 [0.498, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.891, 10.135], loss: 0.001521, mae: 0.043204, mean_q: 1.176111
  93995/1000000: episode: 940, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 61.060, mean reward: 0.611 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.515, 10.098], loss: 0.001497, mae: 0.042145, mean_q: 1.174906
  94095/1000000: episode: 941, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.451, mean reward: 0.585 [0.506, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.564, 10.322], loss: 0.001425, mae: 0.040907, mean_q: 1.173131
  94195/1000000: episode: 942, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 58.071, mean reward: 0.581 [0.502, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.098], loss: 0.001399, mae: 0.041109, mean_q: 1.173708
  94295/1000000: episode: 943, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.641, mean reward: 0.576 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.926, 10.169], loss: 0.001417, mae: 0.041525, mean_q: 1.173816
  94395/1000000: episode: 944, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 58.051, mean reward: 0.581 [0.506, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.150, 10.291], loss: 0.001477, mae: 0.041929, mean_q: 1.174166
  94495/1000000: episode: 945, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 60.203, mean reward: 0.602 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.191], loss: 0.001564, mae: 0.043312, mean_q: 1.177849
  94595/1000000: episode: 946, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 58.539, mean reward: 0.585 [0.509, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.977, 10.165], loss: 0.001408, mae: 0.041335, mean_q: 1.171815
  94695/1000000: episode: 947, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 59.348, mean reward: 0.593 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.189, 10.098], loss: 0.001422, mae: 0.041324, mean_q: 1.176122
  94795/1000000: episode: 948, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.923, mean reward: 0.599 [0.514, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.407, 10.199], loss: 0.001524, mae: 0.043386, mean_q: 1.176739
  94895/1000000: episode: 949, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 62.619, mean reward: 0.626 [0.504, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.161, 10.382], loss: 0.001459, mae: 0.041913, mean_q: 1.178352
  94995/1000000: episode: 950, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 58.407, mean reward: 0.584 [0.503, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.098], loss: 0.001499, mae: 0.042345, mean_q: 1.180163
  95095/1000000: episode: 951, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.897, mean reward: 0.589 [0.503, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.280], loss: 0.001402, mae: 0.040781, mean_q: 1.176548
  95195/1000000: episode: 952, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 56.662, mean reward: 0.567 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.586, 10.132], loss: 0.001440, mae: 0.041479, mean_q: 1.178874
  95295/1000000: episode: 953, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 61.430, mean reward: 0.614 [0.517, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.098], loss: 0.001566, mae: 0.042682, mean_q: 1.171059
  95395/1000000: episode: 954, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 60.076, mean reward: 0.601 [0.514, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.141, 10.369], loss: 0.001390, mae: 0.041024, mean_q: 1.170161
  95495/1000000: episode: 955, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.117, mean reward: 0.581 [0.498, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.290, 10.098], loss: 0.001576, mae: 0.043366, mean_q: 1.176079
  95595/1000000: episode: 956, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 57.985, mean reward: 0.580 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.558, 10.415], loss: 0.001520, mae: 0.042586, mean_q: 1.174973
  95695/1000000: episode: 957, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.950, mean reward: 0.590 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.879, 10.127], loss: 0.001575, mae: 0.043240, mean_q: 1.172451
  95795/1000000: episode: 958, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 62.006, mean reward: 0.620 [0.516, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.854, 10.098], loss: 0.001509, mae: 0.042537, mean_q: 1.178204
  95895/1000000: episode: 959, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 61.248, mean reward: 0.612 [0.514, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.873, 10.167], loss: 0.001522, mae: 0.043030, mean_q: 1.177006
  95995/1000000: episode: 960, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 58.478, mean reward: 0.585 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.736, 10.125], loss: 0.001553, mae: 0.043208, mean_q: 1.181094
  96095/1000000: episode: 961, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 59.773, mean reward: 0.598 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.384, 10.098], loss: 0.001500, mae: 0.042455, mean_q: 1.178140
  96195/1000000: episode: 962, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 58.114, mean reward: 0.581 [0.511, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.098], loss: 0.001614, mae: 0.044347, mean_q: 1.176281
  96295/1000000: episode: 963, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 61.463, mean reward: 0.615 [0.522, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.144, 10.098], loss: 0.001723, mae: 0.045230, mean_q: 1.176097
  96395/1000000: episode: 964, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 57.225, mean reward: 0.572 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.836, 10.098], loss: 0.001656, mae: 0.043973, mean_q: 1.176064
  96495/1000000: episode: 965, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 57.171, mean reward: 0.572 [0.498, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.216, 10.098], loss: 0.001400, mae: 0.040702, mean_q: 1.170923
  96595/1000000: episode: 966, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 63.189, mean reward: 0.632 [0.512, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.098], loss: 0.001565, mae: 0.042916, mean_q: 1.177832
  96695/1000000: episode: 967, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 60.481, mean reward: 0.605 [0.508, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.225, 10.282], loss: 0.001551, mae: 0.042795, mean_q: 1.180772
  96795/1000000: episode: 968, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 58.574, mean reward: 0.586 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.824, 10.177], loss: 0.001658, mae: 0.043733, mean_q: 1.178355
  96895/1000000: episode: 969, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 59.726, mean reward: 0.597 [0.508, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.398, 10.221], loss: 0.001425, mae: 0.041469, mean_q: 1.173707
  96995/1000000: episode: 970, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 56.976, mean reward: 0.570 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.525, 10.098], loss: 0.001439, mae: 0.041568, mean_q: 1.176832
  97095/1000000: episode: 971, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 58.496, mean reward: 0.585 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.131, 10.179], loss: 0.001586, mae: 0.043735, mean_q: 1.173777
  97195/1000000: episode: 972, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 62.058, mean reward: 0.621 [0.499, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.441, 10.526], loss: 0.001537, mae: 0.043278, mean_q: 1.171725
  97295/1000000: episode: 973, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.790, mean reward: 0.588 [0.523, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.458, 10.156], loss: 0.001611, mae: 0.043532, mean_q: 1.172179
  97395/1000000: episode: 974, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 57.366, mean reward: 0.574 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.824, 10.220], loss: 0.001439, mae: 0.041564, mean_q: 1.170680
  97495/1000000: episode: 975, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.201, mean reward: 0.592 [0.515, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.262, 10.098], loss: 0.001571, mae: 0.042994, mean_q: 1.170879
  97595/1000000: episode: 976, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 57.329, mean reward: 0.573 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.580, 10.098], loss: 0.001485, mae: 0.042132, mean_q: 1.169826
  97695/1000000: episode: 977, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 59.814, mean reward: 0.598 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.790, 10.330], loss: 0.001628, mae: 0.043850, mean_q: 1.172111
  97795/1000000: episode: 978, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 59.677, mean reward: 0.597 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.177, 10.098], loss: 0.001604, mae: 0.042806, mean_q: 1.168892
  97895/1000000: episode: 979, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.757, mean reward: 0.578 [0.501, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.108], loss: 0.001456, mae: 0.041581, mean_q: 1.170626
  97995/1000000: episode: 980, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 57.838, mean reward: 0.578 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.786, 10.120], loss: 0.001506, mae: 0.042400, mean_q: 1.173600
  98095/1000000: episode: 981, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 58.817, mean reward: 0.588 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.306], loss: 0.001488, mae: 0.042032, mean_q: 1.170782
  98195/1000000: episode: 982, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 62.113, mean reward: 0.621 [0.517, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.620, 10.231], loss: 0.001516, mae: 0.042425, mean_q: 1.168658
  98295/1000000: episode: 983, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 57.831, mean reward: 0.578 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.756, 10.102], loss: 0.001447, mae: 0.041634, mean_q: 1.171526
  98395/1000000: episode: 984, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 57.143, mean reward: 0.571 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.098], loss: 0.001470, mae: 0.042027, mean_q: 1.169193
  98495/1000000: episode: 985, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 60.018, mean reward: 0.600 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.881, 10.281], loss: 0.001482, mae: 0.041620, mean_q: 1.168324
  98595/1000000: episode: 986, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.311, mean reward: 0.583 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.113], loss: 0.001419, mae: 0.041054, mean_q: 1.169328
  98695/1000000: episode: 987, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 57.780, mean reward: 0.578 [0.501, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.945, 10.098], loss: 0.001592, mae: 0.043578, mean_q: 1.172304
  98795/1000000: episode: 988, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.106, mean reward: 0.581 [0.510, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.202, 10.098], loss: 0.001581, mae: 0.043523, mean_q: 1.167779
  98895/1000000: episode: 989, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.529, mean reward: 0.595 [0.513, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.033, 10.181], loss: 0.001384, mae: 0.040780, mean_q: 1.166882
  98995/1000000: episode: 990, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 58.560, mean reward: 0.586 [0.499, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.936, 10.098], loss: 0.001586, mae: 0.042696, mean_q: 1.168678
  99095/1000000: episode: 991, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 56.147, mean reward: 0.561 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.098], loss: 0.001480, mae: 0.042219, mean_q: 1.166448
  99195/1000000: episode: 992, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.337, mean reward: 0.583 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.343, 10.098], loss: 0.001529, mae: 0.042164, mean_q: 1.167608
  99295/1000000: episode: 993, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 58.702, mean reward: 0.587 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.674, 10.098], loss: 0.001458, mae: 0.041853, mean_q: 1.170411
  99395/1000000: episode: 994, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.721, mean reward: 0.597 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.356, 10.098], loss: 0.001437, mae: 0.041751, mean_q: 1.169347
  99495/1000000: episode: 995, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.428, mean reward: 0.604 [0.515, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.019, 10.098], loss: 0.001391, mae: 0.040454, mean_q: 1.168895
  99595/1000000: episode: 996, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 58.337, mean reward: 0.583 [0.515, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.918, 10.098], loss: 0.001465, mae: 0.041874, mean_q: 1.168928
  99695/1000000: episode: 997, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 62.904, mean reward: 0.629 [0.511, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.514, 10.417], loss: 0.001393, mae: 0.040474, mean_q: 1.169190
  99795/1000000: episode: 998, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 58.660, mean reward: 0.587 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.338, 10.150], loss: 0.001359, mae: 0.040248, mean_q: 1.169075
  99895/1000000: episode: 999, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.972, mean reward: 0.580 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.366, 10.098], loss: 0.001570, mae: 0.043004, mean_q: 1.171971
  99995/1000000: episode: 1000, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 60.267, mean reward: 0.603 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.700, 10.101], loss: 0.001532, mae: 0.042753, mean_q: 1.169582
 100095/1000000: episode: 1001, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 60.321, mean reward: 0.603 [0.500, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.563, 10.098], loss: 0.001560, mae: 0.043300, mean_q: 1.166848
 100195/1000000: episode: 1002, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.690, mean reward: 0.577 [0.505, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.342, 10.256], loss: 0.001640, mae: 0.043829, mean_q: 1.169112
 100295/1000000: episode: 1003, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 59.327, mean reward: 0.593 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.215], loss: 0.001466, mae: 0.041772, mean_q: 1.168830
 100395/1000000: episode: 1004, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 56.953, mean reward: 0.570 [0.500, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.249, 10.260], loss: 0.001486, mae: 0.042007, mean_q: 1.166230
 100495/1000000: episode: 1005, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 60.613, mean reward: 0.606 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.370], loss: 0.001779, mae: 0.045103, mean_q: 1.168812
 100595/1000000: episode: 1006, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 57.134, mean reward: 0.571 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.407, 10.098], loss: 0.001473, mae: 0.041802, mean_q: 1.167640
 100695/1000000: episode: 1007, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.929, mean reward: 0.589 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.694, 10.271], loss: 0.001561, mae: 0.043339, mean_q: 1.169228
 100795/1000000: episode: 1008, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.808, mean reward: 0.598 [0.516, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.645, 10.265], loss: 0.001562, mae: 0.043537, mean_q: 1.167699
 100895/1000000: episode: 1009, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 59.318, mean reward: 0.593 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.968, 10.243], loss: 0.001560, mae: 0.043144, mean_q: 1.166152
 100995/1000000: episode: 1010, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 58.398, mean reward: 0.584 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.239, 10.187], loss: 0.001461, mae: 0.042154, mean_q: 1.166439
 101095/1000000: episode: 1011, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.667, mean reward: 0.577 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.942, 10.227], loss: 0.001539, mae: 0.043086, mean_q: 1.164957
 101195/1000000: episode: 1012, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 59.653, mean reward: 0.597 [0.513, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.600, 10.098], loss: 0.001493, mae: 0.042623, mean_q: 1.166192
 101295/1000000: episode: 1013, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.695, mean reward: 0.587 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.849, 10.202], loss: 0.001481, mae: 0.042422, mean_q: 1.163269
 101395/1000000: episode: 1014, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.588, mean reward: 0.576 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.451, 10.124], loss: 0.001466, mae: 0.042482, mean_q: 1.164546
 101495/1000000: episode: 1015, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.123, mean reward: 0.591 [0.510, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.175], loss: 0.001419, mae: 0.041878, mean_q: 1.165314
 101595/1000000: episode: 1016, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 60.161, mean reward: 0.602 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.143, 10.326], loss: 0.001395, mae: 0.041290, mean_q: 1.163727
 101695/1000000: episode: 1017, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 57.469, mean reward: 0.575 [0.500, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.501, 10.098], loss: 0.001516, mae: 0.042366, mean_q: 1.161278
 101795/1000000: episode: 1018, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 61.077, mean reward: 0.611 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.223, 10.098], loss: 0.001499, mae: 0.042676, mean_q: 1.164784
 101895/1000000: episode: 1019, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.552, mean reward: 0.586 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.740, 10.098], loss: 0.001492, mae: 0.041836, mean_q: 1.163232
 101995/1000000: episode: 1020, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 57.356, mean reward: 0.574 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.820, 10.194], loss: 0.001435, mae: 0.041350, mean_q: 1.162925
 102095/1000000: episode: 1021, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.827, mean reward: 0.588 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.098], loss: 0.001406, mae: 0.040494, mean_q: 1.163217
 102195/1000000: episode: 1022, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 58.864, mean reward: 0.589 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.072, 10.098], loss: 0.001454, mae: 0.042179, mean_q: 1.162930
 102295/1000000: episode: 1023, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.475, mean reward: 0.585 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.711, 10.337], loss: 0.001501, mae: 0.042210, mean_q: 1.165417
 102395/1000000: episode: 1024, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 60.475, mean reward: 0.605 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.217, 10.325], loss: 0.001435, mae: 0.041697, mean_q: 1.164859
 102495/1000000: episode: 1025, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 58.162, mean reward: 0.582 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.655, 10.098], loss: 0.001390, mae: 0.040939, mean_q: 1.162509
 102595/1000000: episode: 1026, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 56.446, mean reward: 0.564 [0.506, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.606, 10.098], loss: 0.001529, mae: 0.042726, mean_q: 1.164405
 102695/1000000: episode: 1027, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 56.596, mean reward: 0.566 [0.503, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.098], loss: 0.001525, mae: 0.042604, mean_q: 1.163198
 102795/1000000: episode: 1028, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 59.735, mean reward: 0.597 [0.500, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.858, 10.098], loss: 0.001530, mae: 0.042786, mean_q: 1.161661
 102895/1000000: episode: 1029, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 58.807, mean reward: 0.588 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.066, 10.098], loss: 0.001474, mae: 0.042284, mean_q: 1.160465
 102995/1000000: episode: 1030, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.303, mean reward: 0.573 [0.501, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.843, 10.100], loss: 0.001504, mae: 0.042659, mean_q: 1.161361
 103095/1000000: episode: 1031, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 63.730, mean reward: 0.637 [0.501, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.655, 10.098], loss: 0.001520, mae: 0.042887, mean_q: 1.164104
 103195/1000000: episode: 1032, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 58.051, mean reward: 0.581 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.730, 10.184], loss: 0.001409, mae: 0.040390, mean_q: 1.161948
 103295/1000000: episode: 1033, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 60.658, mean reward: 0.607 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.098], loss: 0.001385, mae: 0.040810, mean_q: 1.160167
 103395/1000000: episode: 1034, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.947, mean reward: 0.589 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.609, 10.224], loss: 0.001431, mae: 0.040866, mean_q: 1.162766
 103495/1000000: episode: 1035, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 58.937, mean reward: 0.589 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.336, 10.401], loss: 0.001495, mae: 0.042012, mean_q: 1.165915
 103595/1000000: episode: 1036, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 60.590, mean reward: 0.606 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.801, 10.098], loss: 0.001661, mae: 0.044321, mean_q: 1.165215
 103695/1000000: episode: 1037, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.486, mean reward: 0.585 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.785, 10.431], loss: 0.001764, mae: 0.045338, mean_q: 1.164837
 103795/1000000: episode: 1038, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 59.694, mean reward: 0.597 [0.513, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.927, 10.098], loss: 0.001491, mae: 0.042098, mean_q: 1.166661
 103895/1000000: episode: 1039, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 58.352, mean reward: 0.584 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.098], loss: 0.001551, mae: 0.043112, mean_q: 1.170811
 103995/1000000: episode: 1040, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 62.759, mean reward: 0.628 [0.522, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.966, 10.176], loss: 0.001457, mae: 0.041813, mean_q: 1.166046
 104095/1000000: episode: 1041, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.085, mean reward: 0.581 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.098], loss: 0.001447, mae: 0.041847, mean_q: 1.167453
 104195/1000000: episode: 1042, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 56.289, mean reward: 0.563 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.098], loss: 0.001492, mae: 0.042505, mean_q: 1.165605
 104295/1000000: episode: 1043, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 59.083, mean reward: 0.591 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.912, 10.098], loss: 0.001530, mae: 0.042202, mean_q: 1.165788
 104395/1000000: episode: 1044, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.720, mean reward: 0.597 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.260, 10.343], loss: 0.001467, mae: 0.042153, mean_q: 1.169078
 104495/1000000: episode: 1045, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 59.013, mean reward: 0.590 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.923, 10.098], loss: 0.001545, mae: 0.042802, mean_q: 1.168451
 104595/1000000: episode: 1046, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.652, mean reward: 0.587 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.291, 10.268], loss: 0.001433, mae: 0.042285, mean_q: 1.165717
 104695/1000000: episode: 1047, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 60.887, mean reward: 0.609 [0.499, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.301, 10.202], loss: 0.001452, mae: 0.042398, mean_q: 1.164567
 104795/1000000: episode: 1048, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.224, mean reward: 0.582 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.322], loss: 0.001511, mae: 0.042728, mean_q: 1.162626
 104895/1000000: episode: 1049, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 60.760, mean reward: 0.608 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.025, 10.117], loss: 0.001442, mae: 0.041544, mean_q: 1.167462
 104995/1000000: episode: 1050, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 60.389, mean reward: 0.604 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.223, 10.421], loss: 0.001569, mae: 0.043017, mean_q: 1.168562
 105095/1000000: episode: 1051, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 58.291, mean reward: 0.583 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.931, 10.098], loss: 0.001448, mae: 0.041881, mean_q: 1.167711
 105195/1000000: episode: 1052, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.355, mean reward: 0.584 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.824, 10.119], loss: 0.001409, mae: 0.041415, mean_q: 1.170511
 105295/1000000: episode: 1053, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.169, mean reward: 0.582 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.487, 10.098], loss: 0.001522, mae: 0.042497, mean_q: 1.168012
 105395/1000000: episode: 1054, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 61.527, mean reward: 0.615 [0.507, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.192, 10.116], loss: 0.001513, mae: 0.042447, mean_q: 1.166329
 105495/1000000: episode: 1055, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.496, mean reward: 0.575 [0.517, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.098], loss: 0.001634, mae: 0.044231, mean_q: 1.171318
 105595/1000000: episode: 1056, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.265, mean reward: 0.583 [0.509, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.566, 10.098], loss: 0.001399, mae: 0.040944, mean_q: 1.168796
 105695/1000000: episode: 1057, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 59.171, mean reward: 0.592 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.920, 10.364], loss: 0.001467, mae: 0.042052, mean_q: 1.167932
 105795/1000000: episode: 1058, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 61.644, mean reward: 0.616 [0.523, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.179, 10.254], loss: 0.001446, mae: 0.041374, mean_q: 1.167071
 105895/1000000: episode: 1059, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 58.542, mean reward: 0.585 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.158, 10.205], loss: 0.001387, mae: 0.040497, mean_q: 1.169689
 105995/1000000: episode: 1060, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 59.642, mean reward: 0.596 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.218, 10.098], loss: 0.001464, mae: 0.041681, mean_q: 1.168629
 106095/1000000: episode: 1061, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.011, mean reward: 0.580 [0.499, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.950, 10.147], loss: 0.001576, mae: 0.043240, mean_q: 1.167476
 106195/1000000: episode: 1062, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.485, mean reward: 0.595 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.108, 10.098], loss: 0.001418, mae: 0.041771, mean_q: 1.169536
 106295/1000000: episode: 1063, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.642, mean reward: 0.576 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.098], loss: 0.001320, mae: 0.039768, mean_q: 1.165986
 106395/1000000: episode: 1064, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 60.003, mean reward: 0.600 [0.512, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.585, 10.523], loss: 0.001529, mae: 0.042473, mean_q: 1.171061
 106495/1000000: episode: 1065, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 62.201, mean reward: 0.622 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.630, 10.184], loss: 0.001444, mae: 0.041804, mean_q: 1.168791
 106595/1000000: episode: 1066, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 61.194, mean reward: 0.612 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.661, 10.098], loss: 0.001585, mae: 0.043419, mean_q: 1.170094
 106695/1000000: episode: 1067, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.229, mean reward: 0.582 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.098], loss: 0.001473, mae: 0.041685, mean_q: 1.171749
 106795/1000000: episode: 1068, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 59.121, mean reward: 0.591 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.094, 10.098], loss: 0.001371, mae: 0.040326, mean_q: 1.171606
 106895/1000000: episode: 1069, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 60.633, mean reward: 0.606 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.430, 10.098], loss: 0.001439, mae: 0.041581, mean_q: 1.170236
 106995/1000000: episode: 1070, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 63.043, mean reward: 0.630 [0.520, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.004, 10.098], loss: 0.001612, mae: 0.043323, mean_q: 1.171516
 107095/1000000: episode: 1071, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.956, mean reward: 0.580 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.965, 10.208], loss: 0.001544, mae: 0.042971, mean_q: 1.173101
 107195/1000000: episode: 1072, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 61.730, mean reward: 0.617 [0.502, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.114, 10.351], loss: 0.001536, mae: 0.042745, mean_q: 1.174823
 107295/1000000: episode: 1073, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 58.625, mean reward: 0.586 [0.511, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.857, 10.109], loss: 0.001426, mae: 0.041521, mean_q: 1.173282
 107395/1000000: episode: 1074, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.470, mean reward: 0.585 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.159, 10.259], loss: 0.001467, mae: 0.041894, mean_q: 1.174092
 107495/1000000: episode: 1075, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.385, mean reward: 0.594 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.098], loss: 0.001400, mae: 0.040950, mean_q: 1.172766
 107595/1000000: episode: 1076, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 61.560, mean reward: 0.616 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.711, 10.264], loss: 0.001395, mae: 0.040603, mean_q: 1.175622
 107695/1000000: episode: 1077, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 59.006, mean reward: 0.590 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.469, 10.355], loss: 0.001432, mae: 0.041390, mean_q: 1.175280
 107795/1000000: episode: 1078, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 64.182, mean reward: 0.642 [0.511, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.660, 10.296], loss: 0.001600, mae: 0.043392, mean_q: 1.176616
 107895/1000000: episode: 1079, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 57.980, mean reward: 0.580 [0.504, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.684, 10.147], loss: 0.001599, mae: 0.043979, mean_q: 1.179994
 107995/1000000: episode: 1080, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.666, mean reward: 0.587 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.938, 10.157], loss: 0.001505, mae: 0.042112, mean_q: 1.177137
 108095/1000000: episode: 1081, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 60.276, mean reward: 0.603 [0.511, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.401, 10.298], loss: 0.001520, mae: 0.042397, mean_q: 1.178692
 108195/1000000: episode: 1082, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.120, mean reward: 0.581 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.047, 10.166], loss: 0.001422, mae: 0.040833, mean_q: 1.176959
 108295/1000000: episode: 1083, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.937, mean reward: 0.589 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.582, 10.154], loss: 0.001450, mae: 0.041935, mean_q: 1.179684
 108395/1000000: episode: 1084, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 60.239, mean reward: 0.602 [0.523, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.216, 10.169], loss: 0.001493, mae: 0.041855, mean_q: 1.174841
 108495/1000000: episode: 1085, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 57.255, mean reward: 0.573 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.352, 10.301], loss: 0.001549, mae: 0.042625, mean_q: 1.177744
 108595/1000000: episode: 1086, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.826, mean reward: 0.578 [0.509, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.566, 10.098], loss: 0.001386, mae: 0.040656, mean_q: 1.175787
 108695/1000000: episode: 1087, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 62.384, mean reward: 0.624 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.298, 10.260], loss: 0.001512, mae: 0.041744, mean_q: 1.179114
 108795/1000000: episode: 1088, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 58.977, mean reward: 0.590 [0.509, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.100], loss: 0.001590, mae: 0.042339, mean_q: 1.179221
 108895/1000000: episode: 1089, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 57.536, mean reward: 0.575 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.804, 10.098], loss: 0.001451, mae: 0.041270, mean_q: 1.180806
 108995/1000000: episode: 1090, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 60.248, mean reward: 0.602 [0.516, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.413, 10.098], loss: 0.001439, mae: 0.041198, mean_q: 1.172094
 109095/1000000: episode: 1091, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 57.369, mean reward: 0.574 [0.504, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.999, 10.179], loss: 0.001475, mae: 0.041757, mean_q: 1.175992
 109195/1000000: episode: 1092, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 58.920, mean reward: 0.589 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.084, 10.177], loss: 0.001576, mae: 0.043531, mean_q: 1.176094
 109295/1000000: episode: 1093, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 59.360, mean reward: 0.594 [0.518, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.876, 10.297], loss: 0.001470, mae: 0.041508, mean_q: 1.175029
 109395/1000000: episode: 1094, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 60.702, mean reward: 0.607 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.194], loss: 0.001481, mae: 0.042216, mean_q: 1.175934
 109495/1000000: episode: 1095, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: 64.249, mean reward: 0.642 [0.515, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.599], loss: 0.001451, mae: 0.041699, mean_q: 1.178869
 109595/1000000: episode: 1096, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.618, mean reward: 0.596 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.558, 10.152], loss: 0.001540, mae: 0.042323, mean_q: 1.180883
 109695/1000000: episode: 1097, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: 58.599, mean reward: 0.586 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.379], loss: 0.001564, mae: 0.042893, mean_q: 1.178776
 109795/1000000: episode: 1098, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 58.832, mean reward: 0.588 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.958, 10.207], loss: 0.001485, mae: 0.042124, mean_q: 1.180387
 109895/1000000: episode: 1099, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 60.528, mean reward: 0.605 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.306, 10.395], loss: 0.001535, mae: 0.042130, mean_q: 1.178088
 109995/1000000: episode: 1100, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 56.473, mean reward: 0.565 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.091, 10.120], loss: 0.001498, mae: 0.042248, mean_q: 1.177559
 110095/1000000: episode: 1101, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 58.207, mean reward: 0.582 [0.499, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.998, 10.098], loss: 0.001443, mae: 0.041412, mean_q: 1.176460
 110195/1000000: episode: 1102, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 60.834, mean reward: 0.608 [0.519, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.194, 10.098], loss: 0.001586, mae: 0.043241, mean_q: 1.180964
 110295/1000000: episode: 1103, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 58.471, mean reward: 0.585 [0.503, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.476, 10.304], loss: 0.001516, mae: 0.042682, mean_q: 1.178985
 110395/1000000: episode: 1104, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 56.973, mean reward: 0.570 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.465, 10.137], loss: 0.001501, mae: 0.042061, mean_q: 1.181294
 110495/1000000: episode: 1105, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 57.588, mean reward: 0.576 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.093, 10.166], loss: 0.001504, mae: 0.042085, mean_q: 1.177714
 110595/1000000: episode: 1106, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.139, mean reward: 0.581 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.634, 10.098], loss: 0.001578, mae: 0.043140, mean_q: 1.178841
 110695/1000000: episode: 1107, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 57.212, mean reward: 0.572 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.229, 10.262], loss: 0.001558, mae: 0.042880, mean_q: 1.174438
 110795/1000000: episode: 1108, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 57.904, mean reward: 0.579 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.279, 10.188], loss: 0.001476, mae: 0.041910, mean_q: 1.170783
 110895/1000000: episode: 1109, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 59.149, mean reward: 0.591 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.889, 10.098], loss: 0.001395, mae: 0.040630, mean_q: 1.170533
 110995/1000000: episode: 1110, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 58.894, mean reward: 0.589 [0.498, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.970, 10.146], loss: 0.001526, mae: 0.042735, mean_q: 1.174573
 111095/1000000: episode: 1111, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 60.893, mean reward: 0.609 [0.506, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.265, 10.098], loss: 0.001398, mae: 0.041008, mean_q: 1.174286
 111195/1000000: episode: 1112, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 58.256, mean reward: 0.583 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.650, 10.299], loss: 0.001479, mae: 0.041890, mean_q: 1.173251
 111295/1000000: episode: 1113, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 57.156, mean reward: 0.572 [0.500, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.186], loss: 0.001624, mae: 0.043063, mean_q: 1.175075
 111395/1000000: episode: 1114, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.772, mean reward: 0.588 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.253], loss: 0.001966, mae: 0.046324, mean_q: 1.176917
 111495/1000000: episode: 1115, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 59.802, mean reward: 0.598 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.006, 10.164], loss: 0.001507, mae: 0.042068, mean_q: 1.174598
 111595/1000000: episode: 1116, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.365, mean reward: 0.584 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.571, 10.209], loss: 0.001420, mae: 0.040643, mean_q: 1.170871
 111695/1000000: episode: 1117, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 60.979, mean reward: 0.610 [0.517, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.257], loss: 0.001432, mae: 0.040803, mean_q: 1.173144
 111795/1000000: episode: 1118, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 60.576, mean reward: 0.606 [0.514, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.284, 10.098], loss: 0.001488, mae: 0.041576, mean_q: 1.171254
 111895/1000000: episode: 1119, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.273, mean reward: 0.573 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.208, 10.098], loss: 0.001420, mae: 0.040969, mean_q: 1.173512
 111995/1000000: episode: 1120, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.898, mean reward: 0.579 [0.502, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.840, 10.214], loss: 0.001452, mae: 0.041239, mean_q: 1.171076
 112095/1000000: episode: 1121, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 58.324, mean reward: 0.583 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.881, 10.098], loss: 0.001416, mae: 0.041516, mean_q: 1.171931
 112195/1000000: episode: 1122, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 56.208, mean reward: 0.562 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.438, 10.170], loss: 0.001506, mae: 0.042765, mean_q: 1.170570
 112295/1000000: episode: 1123, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 60.235, mean reward: 0.602 [0.508, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.699, 10.207], loss: 0.001434, mae: 0.041342, mean_q: 1.171062
 112395/1000000: episode: 1124, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 58.852, mean reward: 0.589 [0.498, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.838, 10.098], loss: 0.001422, mae: 0.041235, mean_q: 1.170047
 112495/1000000: episode: 1125, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 56.491, mean reward: 0.565 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.193, 10.098], loss: 0.001417, mae: 0.040643, mean_q: 1.167127
 112595/1000000: episode: 1126, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 59.468, mean reward: 0.595 [0.512, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.185, 10.157], loss: 0.001362, mae: 0.040525, mean_q: 1.163522
 112695/1000000: episode: 1127, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 59.455, mean reward: 0.595 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.411, 10.098], loss: 0.001482, mae: 0.041557, mean_q: 1.168108
 112795/1000000: episode: 1128, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.502, mean reward: 0.575 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.134], loss: 0.001512, mae: 0.041313, mean_q: 1.165437
 112895/1000000: episode: 1129, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 60.213, mean reward: 0.602 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.438, 10.167], loss: 0.001469, mae: 0.041876, mean_q: 1.165701
 112995/1000000: episode: 1130, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.527, mean reward: 0.575 [0.501, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.098], loss: 0.001498, mae: 0.042185, mean_q: 1.162269
 113095/1000000: episode: 1131, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 61.320, mean reward: 0.613 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.344, 10.098], loss: 0.001476, mae: 0.041462, mean_q: 1.166751
 113195/1000000: episode: 1132, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 60.890, mean reward: 0.609 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.928, 10.098], loss: 0.001574, mae: 0.043197, mean_q: 1.169209
 113295/1000000: episode: 1133, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 57.984, mean reward: 0.580 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.284, 10.098], loss: 0.001533, mae: 0.042446, mean_q: 1.165926
 113395/1000000: episode: 1134, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 56.721, mean reward: 0.567 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.824, 10.098], loss: 0.001450, mae: 0.041817, mean_q: 1.166130
 113495/1000000: episode: 1135, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 59.112, mean reward: 0.591 [0.506, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.271, 10.098], loss: 0.001436, mae: 0.041618, mean_q: 1.165379
 113595/1000000: episode: 1136, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 56.918, mean reward: 0.569 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.609, 10.098], loss: 0.001513, mae: 0.042644, mean_q: 1.164287
 113695/1000000: episode: 1137, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.413, mean reward: 0.594 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.045, 10.120], loss: 0.001422, mae: 0.041396, mean_q: 1.167583
 113795/1000000: episode: 1138, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.756, mean reward: 0.588 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.866, 10.232], loss: 0.001496, mae: 0.042678, mean_q: 1.161378
 113895/1000000: episode: 1139, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 58.470, mean reward: 0.585 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.802, 10.098], loss: 0.001431, mae: 0.041107, mean_q: 1.160942
 113995/1000000: episode: 1140, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 57.170, mean reward: 0.572 [0.506, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.008, 10.181], loss: 0.001442, mae: 0.041542, mean_q: 1.163109
 114095/1000000: episode: 1141, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 61.897, mean reward: 0.619 [0.503, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.568, 10.098], loss: 0.001499, mae: 0.042076, mean_q: 1.160557
 114195/1000000: episode: 1142, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 58.600, mean reward: 0.586 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.002, 10.221], loss: 0.001704, mae: 0.044450, mean_q: 1.163562
 114295/1000000: episode: 1143, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 58.803, mean reward: 0.588 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.738, 10.098], loss: 0.001472, mae: 0.041423, mean_q: 1.163316
 114395/1000000: episode: 1144, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 59.256, mean reward: 0.593 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.880, 10.098], loss: 0.001452, mae: 0.041879, mean_q: 1.163513
 114495/1000000: episode: 1145, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 60.547, mean reward: 0.605 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.796, 10.098], loss: 0.001508, mae: 0.042478, mean_q: 1.161845
 114595/1000000: episode: 1146, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 56.955, mean reward: 0.570 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.140], loss: 0.001468, mae: 0.041315, mean_q: 1.160521
 114695/1000000: episode: 1147, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 59.271, mean reward: 0.593 [0.497, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.378], loss: 0.001460, mae: 0.041911, mean_q: 1.156036
 114795/1000000: episode: 1148, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 57.136, mean reward: 0.571 [0.500, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.459, 10.098], loss: 0.001483, mae: 0.041671, mean_q: 1.158000
 114895/1000000: episode: 1149, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.413, mean reward: 0.594 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.109, 10.136], loss: 0.001571, mae: 0.043155, mean_q: 1.160889
 114995/1000000: episode: 1150, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.277, mean reward: 0.573 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.786, 10.098], loss: 0.001546, mae: 0.042558, mean_q: 1.162267
 115095/1000000: episode: 1151, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 60.451, mean reward: 0.605 [0.513, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.988, 10.420], loss: 0.001517, mae: 0.042615, mean_q: 1.156901
 115195/1000000: episode: 1152, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.165, mean reward: 0.582 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.584, 10.207], loss: 0.001561, mae: 0.042651, mean_q: 1.159983
 115295/1000000: episode: 1153, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.013, mean reward: 0.570 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.997, 10.098], loss: 0.001566, mae: 0.042720, mean_q: 1.159095
 115395/1000000: episode: 1154, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 57.966, mean reward: 0.580 [0.502, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.206, 10.098], loss: 0.001450, mae: 0.041416, mean_q: 1.158691
 115495/1000000: episode: 1155, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 60.652, mean reward: 0.607 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.247, 10.132], loss: 0.001588, mae: 0.042707, mean_q: 1.161536
 115595/1000000: episode: 1156, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.484, mean reward: 0.595 [0.515, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.967, 10.098], loss: 0.001468, mae: 0.041922, mean_q: 1.157562
 115695/1000000: episode: 1157, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 60.869, mean reward: 0.609 [0.501, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.112, 10.386], loss: 0.001436, mae: 0.041475, mean_q: 1.161940
 115795/1000000: episode: 1158, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 57.933, mean reward: 0.579 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.200, 10.161], loss: 0.001457, mae: 0.041749, mean_q: 1.162125
 115895/1000000: episode: 1159, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.651, mean reward: 0.587 [0.515, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.232], loss: 0.001556, mae: 0.043010, mean_q: 1.160927
 115995/1000000: episode: 1160, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 58.962, mean reward: 0.590 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.116, 10.098], loss: 0.001398, mae: 0.040913, mean_q: 1.163726
 116095/1000000: episode: 1161, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 59.465, mean reward: 0.595 [0.504, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.108, 10.098], loss: 0.001367, mae: 0.040646, mean_q: 1.161561
 116195/1000000: episode: 1162, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 57.915, mean reward: 0.579 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.098], loss: 0.001421, mae: 0.041309, mean_q: 1.162191
 116295/1000000: episode: 1163, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 59.459, mean reward: 0.595 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.098], loss: 0.001521, mae: 0.042972, mean_q: 1.161781
 116395/1000000: episode: 1164, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.442, mean reward: 0.584 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.940, 10.098], loss: 0.001472, mae: 0.041133, mean_q: 1.158437
 116495/1000000: episode: 1165, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 59.275, mean reward: 0.593 [0.510, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.046, 10.098], loss: 0.001437, mae: 0.041340, mean_q: 1.161136
 116595/1000000: episode: 1166, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 61.102, mean reward: 0.611 [0.501, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.456, 10.098], loss: 0.001484, mae: 0.041691, mean_q: 1.161348
 116695/1000000: episode: 1167, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.404, mean reward: 0.594 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.487, 10.183], loss: 0.001689, mae: 0.044768, mean_q: 1.164075
 116795/1000000: episode: 1168, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 58.486, mean reward: 0.585 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.595, 10.098], loss: 0.001464, mae: 0.041554, mean_q: 1.159054
 116895/1000000: episode: 1169, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.233, mean reward: 0.582 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.244], loss: 0.001506, mae: 0.042148, mean_q: 1.165393
 116995/1000000: episode: 1170, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.446, mean reward: 0.574 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.843, 10.175], loss: 0.001449, mae: 0.041719, mean_q: 1.165079
 117095/1000000: episode: 1171, duration: 0.755s, episode steps: 100, steps per second: 133, episode reward: 60.812, mean reward: 0.608 [0.526, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.513, 10.244], loss: 0.001480, mae: 0.041847, mean_q: 1.160833
 117195/1000000: episode: 1172, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 58.679, mean reward: 0.587 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.949, 10.233], loss: 0.001519, mae: 0.041920, mean_q: 1.168321
 117295/1000000: episode: 1173, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 59.827, mean reward: 0.598 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.766, 10.107], loss: 0.001585, mae: 0.043039, mean_q: 1.164454
 117395/1000000: episode: 1174, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 57.141, mean reward: 0.571 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.182, 10.287], loss: 0.001603, mae: 0.043219, mean_q: 1.162885
 117495/1000000: episode: 1175, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 56.602, mean reward: 0.566 [0.507, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.758, 10.098], loss: 0.001481, mae: 0.042329, mean_q: 1.164544
 117595/1000000: episode: 1176, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.465, mean reward: 0.595 [0.505, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.883, 10.098], loss: 0.001373, mae: 0.040395, mean_q: 1.163368
 117695/1000000: episode: 1177, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 58.407, mean reward: 0.584 [0.510, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.334, 10.098], loss: 0.001545, mae: 0.042771, mean_q: 1.161762
 117795/1000000: episode: 1178, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 58.740, mean reward: 0.587 [0.513, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.655, 10.098], loss: 0.001546, mae: 0.042707, mean_q: 1.164026
 117895/1000000: episode: 1179, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 58.328, mean reward: 0.583 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.174], loss: 0.001533, mae: 0.042072, mean_q: 1.160335
 117995/1000000: episode: 1180, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 56.405, mean reward: 0.564 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.098], loss: 0.001481, mae: 0.042232, mean_q: 1.162287
 118095/1000000: episode: 1181, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 60.894, mean reward: 0.609 [0.513, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.817, 10.394], loss: 0.001494, mae: 0.041790, mean_q: 1.158740
 118195/1000000: episode: 1182, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 61.323, mean reward: 0.613 [0.518, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.240], loss: 0.001489, mae: 0.041906, mean_q: 1.158769
 118295/1000000: episode: 1183, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 59.416, mean reward: 0.594 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.602, 10.098], loss: 0.001497, mae: 0.042078, mean_q: 1.162637
 118395/1000000: episode: 1184, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 57.521, mean reward: 0.575 [0.514, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.184], loss: 0.001702, mae: 0.045155, mean_q: 1.164823
 118495/1000000: episode: 1185, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.368, mean reward: 0.584 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.630, 10.344], loss: 0.001511, mae: 0.042198, mean_q: 1.167312
 118595/1000000: episode: 1186, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.217, mean reward: 0.592 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.842, 10.351], loss: 0.001493, mae: 0.042089, mean_q: 1.161854
 118695/1000000: episode: 1187, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.067, mean reward: 0.581 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.824, 10.283], loss: 0.001592, mae: 0.043301, mean_q: 1.165977
 118795/1000000: episode: 1188, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 60.673, mean reward: 0.607 [0.502, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.602, 10.362], loss: 0.001475, mae: 0.042225, mean_q: 1.165384
 118895/1000000: episode: 1189, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 58.801, mean reward: 0.588 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.410, 10.281], loss: 0.001523, mae: 0.042135, mean_q: 1.163341
 118995/1000000: episode: 1190, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: 60.090, mean reward: 0.601 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.974, 10.218], loss: 0.001652, mae: 0.043557, mean_q: 1.162948
 119095/1000000: episode: 1191, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.790, mean reward: 0.578 [0.505, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.104], loss: 0.001453, mae: 0.041349, mean_q: 1.165687
 119195/1000000: episode: 1192, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 61.307, mean reward: 0.613 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.939, 10.098], loss: 0.001394, mae: 0.040727, mean_q: 1.164689
 119295/1000000: episode: 1193, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 64.070, mean reward: 0.641 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.409, 10.098], loss: 0.001554, mae: 0.043027, mean_q: 1.169616
 119395/1000000: episode: 1194, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 56.412, mean reward: 0.564 [0.505, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.640, 10.098], loss: 0.001442, mae: 0.041551, mean_q: 1.164584
 119495/1000000: episode: 1195, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.390, mean reward: 0.584 [0.505, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.334, 10.098], loss: 0.001487, mae: 0.042551, mean_q: 1.164639
 119595/1000000: episode: 1196, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 57.488, mean reward: 0.575 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.195], loss: 0.001390, mae: 0.040610, mean_q: 1.164507
 119695/1000000: episode: 1197, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.405, mean reward: 0.594 [0.504, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.035, 10.098], loss: 0.001448, mae: 0.041207, mean_q: 1.164860
 119795/1000000: episode: 1198, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.591, mean reward: 0.586 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.684, 10.099], loss: 0.001445, mae: 0.041134, mean_q: 1.164037
 119895/1000000: episode: 1199, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 60.036, mean reward: 0.600 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.531, 10.098], loss: 0.001501, mae: 0.041696, mean_q: 1.169729
 119995/1000000: episode: 1200, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 58.954, mean reward: 0.590 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.367, 10.133], loss: 0.001461, mae: 0.041405, mean_q: 1.166038
 120095/1000000: episode: 1201, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 59.541, mean reward: 0.595 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.747, 10.151], loss: 0.001487, mae: 0.041615, mean_q: 1.167464
 120195/1000000: episode: 1202, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 58.707, mean reward: 0.587 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.449, 10.098], loss: 0.001473, mae: 0.041619, mean_q: 1.165071
 120295/1000000: episode: 1203, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 60.312, mean reward: 0.603 [0.512, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.811, 10.352], loss: 0.001587, mae: 0.043153, mean_q: 1.166489
 120395/1000000: episode: 1204, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.184, mean reward: 0.592 [0.517, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.173, 10.292], loss: 0.001511, mae: 0.041963, mean_q: 1.168294
 120495/1000000: episode: 1205, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.854, mean reward: 0.579 [0.505, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.146], loss: 0.001488, mae: 0.041821, mean_q: 1.166699
 120595/1000000: episode: 1206, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.699, mean reward: 0.577 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.689, 10.098], loss: 0.001604, mae: 0.043595, mean_q: 1.166665
 120695/1000000: episode: 1207, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 58.864, mean reward: 0.589 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.160, 10.098], loss: 0.001534, mae: 0.042222, mean_q: 1.166680
 120795/1000000: episode: 1208, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 60.876, mean reward: 0.609 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.529, 10.192], loss: 0.001520, mae: 0.042279, mean_q: 1.165241
 120895/1000000: episode: 1209, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 60.939, mean reward: 0.609 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.920, 10.324], loss: 0.001577, mae: 0.042994, mean_q: 1.169030
 120995/1000000: episode: 1210, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.798, mean reward: 0.588 [0.497, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.619, 10.098], loss: 0.001753, mae: 0.043874, mean_q: 1.169978
 121095/1000000: episode: 1211, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 61.691, mean reward: 0.617 [0.501, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.465, 10.098], loss: 0.001442, mae: 0.041146, mean_q: 1.168836
 121195/1000000: episode: 1212, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.724, mean reward: 0.587 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.098], loss: 0.001620, mae: 0.043557, mean_q: 1.168759
 121295/1000000: episode: 1213, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 60.486, mean reward: 0.605 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.449, 10.276], loss: 0.001521, mae: 0.042247, mean_q: 1.169098
 121395/1000000: episode: 1214, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.748, mean reward: 0.597 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.098], loss: 0.001477, mae: 0.042058, mean_q: 1.167773
 121495/1000000: episode: 1215, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 57.582, mean reward: 0.576 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.917, 10.366], loss: 0.001472, mae: 0.041730, mean_q: 1.172109
 121595/1000000: episode: 1216, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 56.889, mean reward: 0.569 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.777, 10.098], loss: 0.001525, mae: 0.042997, mean_q: 1.171969
 121695/1000000: episode: 1217, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 60.493, mean reward: 0.605 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.109, 10.098], loss: 0.001424, mae: 0.041250, mean_q: 1.165862
 121795/1000000: episode: 1218, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 57.479, mean reward: 0.575 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.201], loss: 0.001441, mae: 0.041472, mean_q: 1.169634
 121895/1000000: episode: 1219, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 61.086, mean reward: 0.611 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.891, 10.347], loss: 0.001489, mae: 0.041659, mean_q: 1.171698
 121995/1000000: episode: 1220, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.696, mean reward: 0.597 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.774, 10.098], loss: 0.001436, mae: 0.041508, mean_q: 1.170052
 122095/1000000: episode: 1221, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 57.620, mean reward: 0.576 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.234, 10.187], loss: 0.001544, mae: 0.042847, mean_q: 1.168129
 122195/1000000: episode: 1222, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 60.063, mean reward: 0.601 [0.516, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.350, 10.132], loss: 0.001417, mae: 0.041182, mean_q: 1.170174
 122295/1000000: episode: 1223, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 58.726, mean reward: 0.587 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.286, 10.218], loss: 0.001469, mae: 0.041647, mean_q: 1.172367
 122395/1000000: episode: 1224, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 61.042, mean reward: 0.610 [0.503, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.754, 10.098], loss: 0.001346, mae: 0.040471, mean_q: 1.171166
 122495/1000000: episode: 1225, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 60.964, mean reward: 0.610 [0.516, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.728, 10.098], loss: 0.001435, mae: 0.041654, mean_q: 1.167862
 122595/1000000: episode: 1226, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 61.377, mean reward: 0.614 [0.507, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.098], loss: 0.001440, mae: 0.041506, mean_q: 1.169545
 122695/1000000: episode: 1227, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 57.057, mean reward: 0.571 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.881, 10.129], loss: 0.001477, mae: 0.042078, mean_q: 1.172914
 122795/1000000: episode: 1228, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 61.016, mean reward: 0.610 [0.499, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.462, 10.291], loss: 0.001543, mae: 0.042830, mean_q: 1.175623
 122895/1000000: episode: 1229, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 57.795, mean reward: 0.578 [0.509, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.109, 10.098], loss: 0.001421, mae: 0.040958, mean_q: 1.174651
 122995/1000000: episode: 1230, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.699, mean reward: 0.577 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.113, 10.121], loss: 0.001441, mae: 0.041358, mean_q: 1.172780
 123095/1000000: episode: 1231, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.821, mean reward: 0.588 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.326, 10.207], loss: 0.001391, mae: 0.040971, mean_q: 1.174492
 123195/1000000: episode: 1232, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 57.335, mean reward: 0.573 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.290, 10.256], loss: 0.001375, mae: 0.040193, mean_q: 1.170525
 123295/1000000: episode: 1233, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.486, mean reward: 0.585 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.130, 10.178], loss: 0.001409, mae: 0.040958, mean_q: 1.171642
 123395/1000000: episode: 1234, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 57.507, mean reward: 0.575 [0.500, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.773, 10.213], loss: 0.001328, mae: 0.039911, mean_q: 1.169365
 123495/1000000: episode: 1235, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.246, mean reward: 0.572 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.221], loss: 0.001385, mae: 0.040390, mean_q: 1.174343
 123595/1000000: episode: 1236, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 60.600, mean reward: 0.606 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.214, 10.098], loss: 0.001417, mae: 0.040904, mean_q: 1.169658
 123695/1000000: episode: 1237, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 59.079, mean reward: 0.591 [0.509, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.066, 10.131], loss: 0.001426, mae: 0.040907, mean_q: 1.173695
 123795/1000000: episode: 1238, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.740, mean reward: 0.597 [0.513, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.298, 10.112], loss: 0.001503, mae: 0.042120, mean_q: 1.170486
 123895/1000000: episode: 1239, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 58.928, mean reward: 0.589 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.373], loss: 0.001417, mae: 0.041322, mean_q: 1.170925
 123995/1000000: episode: 1240, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.658, mean reward: 0.597 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.513, 10.098], loss: 0.001400, mae: 0.040951, mean_q: 1.171076
 124095/1000000: episode: 1241, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 57.809, mean reward: 0.578 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.472, 10.098], loss: 0.001440, mae: 0.040872, mean_q: 1.167538
 124195/1000000: episode: 1242, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: 61.178, mean reward: 0.612 [0.505, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.934, 10.098], loss: 0.001375, mae: 0.040532, mean_q: 1.170586
 124295/1000000: episode: 1243, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 58.143, mean reward: 0.581 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.371, 10.310], loss: 0.001324, mae: 0.040055, mean_q: 1.170183
 124395/1000000: episode: 1244, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 56.737, mean reward: 0.567 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.906, 10.191], loss: 0.001434, mae: 0.041447, mean_q: 1.170084
 124495/1000000: episode: 1245, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 59.695, mean reward: 0.597 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.367, 10.438], loss: 0.001494, mae: 0.042164, mean_q: 1.167430
 124595/1000000: episode: 1246, duration: 1.763s, episode steps: 100, steps per second: 57, episode reward: 57.269, mean reward: 0.573 [0.507, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.571, 10.207], loss: 0.001388, mae: 0.040626, mean_q: 1.168171
 124695/1000000: episode: 1247, duration: 1.620s, episode steps: 100, steps per second: 62, episode reward: 59.599, mean reward: 0.596 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.098], loss: 0.001361, mae: 0.040170, mean_q: 1.167221
 124795/1000000: episode: 1248, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 62.623, mean reward: 0.626 [0.498, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.907, 10.098], loss: 0.001437, mae: 0.041629, mean_q: 1.170871
 124895/1000000: episode: 1249, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 56.049, mean reward: 0.560 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.273, 10.252], loss: 0.001441, mae: 0.041580, mean_q: 1.169598
 124995/1000000: episode: 1250, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 57.849, mean reward: 0.578 [0.508, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.585, 10.125], loss: 0.001433, mae: 0.041391, mean_q: 1.169265
 125095/1000000: episode: 1251, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.498, mean reward: 0.575 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.959, 10.116], loss: 0.001358, mae: 0.040487, mean_q: 1.172133
 125195/1000000: episode: 1252, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.911, mean reward: 0.579 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.665, 10.336], loss: 0.001428, mae: 0.041854, mean_q: 1.170337
 125295/1000000: episode: 1253, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 58.138, mean reward: 0.581 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.799, 10.098], loss: 0.001519, mae: 0.042341, mean_q: 1.166871
 125395/1000000: episode: 1254, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 62.778, mean reward: 0.628 [0.518, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.341, 10.098], loss: 0.001332, mae: 0.039504, mean_q: 1.165804
 125495/1000000: episode: 1255, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.124, mean reward: 0.571 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.648, 10.098], loss: 0.001388, mae: 0.040339, mean_q: 1.168222
 125595/1000000: episode: 1256, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.186, mean reward: 0.592 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.725, 10.098], loss: 0.001395, mae: 0.040928, mean_q: 1.165993
 125695/1000000: episode: 1257, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 56.766, mean reward: 0.568 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.192, 10.284], loss: 0.001335, mae: 0.039577, mean_q: 1.167718
 125795/1000000: episode: 1258, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 60.344, mean reward: 0.603 [0.512, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.354, 10.098], loss: 0.001332, mae: 0.039911, mean_q: 1.166760
 125895/1000000: episode: 1259, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 56.980, mean reward: 0.570 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.155, 10.098], loss: 0.001373, mae: 0.040664, mean_q: 1.164134
 125995/1000000: episode: 1260, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.478, mean reward: 0.595 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.385, 10.098], loss: 0.001410, mae: 0.040951, mean_q: 1.165843
 126095/1000000: episode: 1261, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 57.971, mean reward: 0.580 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.070, 10.289], loss: 0.001321, mae: 0.039208, mean_q: 1.162946
 126195/1000000: episode: 1262, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 58.753, mean reward: 0.588 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.078, 10.226], loss: 0.001250, mae: 0.038452, mean_q: 1.163649
 126295/1000000: episode: 1263, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 57.939, mean reward: 0.579 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.057, 10.129], loss: 0.001359, mae: 0.039561, mean_q: 1.162616
 126395/1000000: episode: 1264, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 60.975, mean reward: 0.610 [0.512, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.056, 10.292], loss: 0.001427, mae: 0.041042, mean_q: 1.165252
 126495/1000000: episode: 1265, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 57.155, mean reward: 0.572 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.823, 10.114], loss: 0.001319, mae: 0.039185, mean_q: 1.164516
 126595/1000000: episode: 1266, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 57.781, mean reward: 0.578 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.764, 10.123], loss: 0.001395, mae: 0.040897, mean_q: 1.163295
 126695/1000000: episode: 1267, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 60.695, mean reward: 0.607 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.268, 10.098], loss: 0.001446, mae: 0.040846, mean_q: 1.163046
 126795/1000000: episode: 1268, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 57.793, mean reward: 0.578 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.243, 10.226], loss: 0.001389, mae: 0.040642, mean_q: 1.165703
 126895/1000000: episode: 1269, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.303, mean reward: 0.573 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.329, 10.184], loss: 0.001384, mae: 0.040324, mean_q: 1.167590
 126995/1000000: episode: 1270, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 59.512, mean reward: 0.595 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.028, 10.268], loss: 0.001391, mae: 0.040616, mean_q: 1.163897
 127095/1000000: episode: 1271, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 57.722, mean reward: 0.577 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.314, 10.298], loss: 0.001361, mae: 0.040372, mean_q: 1.165018
 127195/1000000: episode: 1272, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 61.794, mean reward: 0.618 [0.518, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.358, 10.356], loss: 0.001339, mae: 0.040377, mean_q: 1.163811
 127295/1000000: episode: 1273, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 57.470, mean reward: 0.575 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.210, 10.151], loss: 0.001404, mae: 0.040650, mean_q: 1.165913
 127395/1000000: episode: 1274, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.350, mean reward: 0.573 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.160], loss: 0.001259, mae: 0.039061, mean_q: 1.162984
 127495/1000000: episode: 1275, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.388, mean reward: 0.574 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.673, 10.275], loss: 0.001380, mae: 0.040812, mean_q: 1.162017
 127595/1000000: episode: 1276, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.149, mean reward: 0.591 [0.500, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.098], loss: 0.001410, mae: 0.040991, mean_q: 1.158171
 127695/1000000: episode: 1277, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 60.034, mean reward: 0.600 [0.515, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.320, 10.252], loss: 0.001383, mae: 0.040459, mean_q: 1.157776
 127795/1000000: episode: 1278, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 63.779, mean reward: 0.638 [0.518, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.617, 10.098], loss: 0.001407, mae: 0.041238, mean_q: 1.161383
 127895/1000000: episode: 1279, duration: 1.401s, episode steps: 100, steps per second: 71, episode reward: 58.192, mean reward: 0.582 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.468, 10.098], loss: 0.001380, mae: 0.040641, mean_q: 1.164405
 127995/1000000: episode: 1280, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 59.961, mean reward: 0.600 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.003, 10.098], loss: 0.001294, mae: 0.039192, mean_q: 1.160144
 128095/1000000: episode: 1281, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 57.974, mean reward: 0.580 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.239, 10.098], loss: 0.001323, mae: 0.039542, mean_q: 1.160187
 128195/1000000: episode: 1282, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.359, mean reward: 0.584 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.159, 10.116], loss: 0.001397, mae: 0.040920, mean_q: 1.161772
 128295/1000000: episode: 1283, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 62.278, mean reward: 0.623 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.234, 10.241], loss: 0.001309, mae: 0.039321, mean_q: 1.163150
 128395/1000000: episode: 1284, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 59.323, mean reward: 0.593 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.682, 10.104], loss: 0.001518, mae: 0.042576, mean_q: 1.168966
 128495/1000000: episode: 1285, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 61.148, mean reward: 0.611 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.063, 10.098], loss: 0.001487, mae: 0.042151, mean_q: 1.167391
 128595/1000000: episode: 1286, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 59.722, mean reward: 0.597 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.435, 10.249], loss: 0.001495, mae: 0.041994, mean_q: 1.166642
 128695/1000000: episode: 1287, duration: 1.453s, episode steps: 100, steps per second: 69, episode reward: 56.429, mean reward: 0.564 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.131, 10.177], loss: 0.001440, mae: 0.041687, mean_q: 1.163789
 128795/1000000: episode: 1288, duration: 1.716s, episode steps: 100, steps per second: 58, episode reward: 58.791, mean reward: 0.588 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.728, 10.146], loss: 0.001390, mae: 0.041231, mean_q: 1.165614
 128895/1000000: episode: 1289, duration: 1.774s, episode steps: 100, steps per second: 56, episode reward: 64.301, mean reward: 0.643 [0.507, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.076, 10.098], loss: 0.001481, mae: 0.042018, mean_q: 1.162483
 128995/1000000: episode: 1290, duration: 1.874s, episode steps: 100, steps per second: 53, episode reward: 58.733, mean reward: 0.587 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.271], loss: 0.001491, mae: 0.041932, mean_q: 1.169808
 129095/1000000: episode: 1291, duration: 1.605s, episode steps: 100, steps per second: 62, episode reward: 61.741, mean reward: 0.617 [0.514, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.098], loss: 0.001480, mae: 0.042298, mean_q: 1.164440
 129195/1000000: episode: 1292, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 58.657, mean reward: 0.587 [0.511, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.615, 10.181], loss: 0.001452, mae: 0.041403, mean_q: 1.167138
 129295/1000000: episode: 1293, duration: 2.187s, episode steps: 100, steps per second: 46, episode reward: 58.588, mean reward: 0.586 [0.498, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.411, 10.098], loss: 0.001460, mae: 0.041907, mean_q: 1.166103
 129395/1000000: episode: 1294, duration: 1.526s, episode steps: 100, steps per second: 66, episode reward: 58.847, mean reward: 0.588 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.714, 10.098], loss: 0.001415, mae: 0.041255, mean_q: 1.164519
 129495/1000000: episode: 1295, duration: 1.490s, episode steps: 100, steps per second: 67, episode reward: 58.670, mean reward: 0.587 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.634, 10.415], loss: 0.001527, mae: 0.042780, mean_q: 1.167579
 129595/1000000: episode: 1296, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 59.878, mean reward: 0.599 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.332], loss: 0.001426, mae: 0.041308, mean_q: 1.169170
 129695/1000000: episode: 1297, duration: 1.242s, episode steps: 100, steps per second: 80, episode reward: 57.650, mean reward: 0.576 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.329, 10.098], loss: 0.001489, mae: 0.042358, mean_q: 1.168334
 129795/1000000: episode: 1298, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 58.656, mean reward: 0.587 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.057, 10.098], loss: 0.001434, mae: 0.041209, mean_q: 1.168643
 129895/1000000: episode: 1299, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.611, mean reward: 0.586 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.169, 10.140], loss: 0.001421, mae: 0.041253, mean_q: 1.168778
 129995/1000000: episode: 1300, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.488, mean reward: 0.595 [0.514, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.648, 10.318], loss: 0.001422, mae: 0.041704, mean_q: 1.165650
 130095/1000000: episode: 1301, duration: 1.262s, episode steps: 100, steps per second: 79, episode reward: 59.046, mean reward: 0.590 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.175, 10.098], loss: 0.001502, mae: 0.043189, mean_q: 1.169789
 130195/1000000: episode: 1302, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 56.786, mean reward: 0.568 [0.502, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.234, 10.284], loss: 0.001442, mae: 0.041263, mean_q: 1.168121
 130295/1000000: episode: 1303, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 62.023, mean reward: 0.620 [0.507, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.385, 10.098], loss: 0.001384, mae: 0.040982, mean_q: 1.166608
 130395/1000000: episode: 1304, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 58.559, mean reward: 0.586 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.098], loss: 0.001510, mae: 0.041632, mean_q: 1.168142
 130495/1000000: episode: 1305, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.947, mean reward: 0.579 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.912, 10.408], loss: 0.001558, mae: 0.042835, mean_q: 1.169451
 130595/1000000: episode: 1306, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.455, mean reward: 0.595 [0.498, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.010, 10.098], loss: 0.001508, mae: 0.042060, mean_q: 1.166383
 130695/1000000: episode: 1307, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 60.299, mean reward: 0.603 [0.511, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.497, 10.409], loss: 0.001554, mae: 0.042404, mean_q: 1.165822
 130795/1000000: episode: 1308, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 59.150, mean reward: 0.591 [0.503, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.733, 10.368], loss: 0.001486, mae: 0.041853, mean_q: 1.167264
 130895/1000000: episode: 1309, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.884, mean reward: 0.579 [0.497, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.098], loss: 0.001467, mae: 0.042064, mean_q: 1.166063
 130995/1000000: episode: 1310, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 56.430, mean reward: 0.564 [0.506, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.585, 10.098], loss: 0.001496, mae: 0.042560, mean_q: 1.168110
 131095/1000000: episode: 1311, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 65.031, mean reward: 0.650 [0.503, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.800, 10.522], loss: 0.001406, mae: 0.040467, mean_q: 1.169402
 131195/1000000: episode: 1312, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.064, mean reward: 0.581 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.332, 10.098], loss: 0.001492, mae: 0.042389, mean_q: 1.170500
 131295/1000000: episode: 1313, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 57.590, mean reward: 0.576 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.987, 10.173], loss: 0.001578, mae: 0.043505, mean_q: 1.176684
 131395/1000000: episode: 1314, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 56.804, mean reward: 0.568 [0.497, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.098], loss: 0.001527, mae: 0.042510, mean_q: 1.168171
 131495/1000000: episode: 1315, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 61.051, mean reward: 0.611 [0.519, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.849, 10.098], loss: 0.001524, mae: 0.043134, mean_q: 1.174147
 131595/1000000: episode: 1316, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 56.476, mean reward: 0.565 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.721, 10.098], loss: 0.001500, mae: 0.041922, mean_q: 1.174080
 131695/1000000: episode: 1317, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.361, mean reward: 0.594 [0.498, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.103], loss: 0.001644, mae: 0.044112, mean_q: 1.169342
 131795/1000000: episode: 1318, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 60.601, mean reward: 0.606 [0.499, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.454, 10.173], loss: 0.001486, mae: 0.041924, mean_q: 1.170208
 131895/1000000: episode: 1319, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.125, mean reward: 0.581 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.308, 10.122], loss: 0.001484, mae: 0.041946, mean_q: 1.171853
 131995/1000000: episode: 1320, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 56.833, mean reward: 0.568 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.551, 10.106], loss: 0.001413, mae: 0.040692, mean_q: 1.173109
 132095/1000000: episode: 1321, duration: 0.755s, episode steps: 100, steps per second: 133, episode reward: 60.656, mean reward: 0.607 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.595, 10.221], loss: 0.001387, mae: 0.040611, mean_q: 1.169275
 132195/1000000: episode: 1322, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 58.626, mean reward: 0.586 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.098], loss: 0.001411, mae: 0.040795, mean_q: 1.168097
 132295/1000000: episode: 1323, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.705, mean reward: 0.587 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.098], loss: 0.001504, mae: 0.042315, mean_q: 1.168653
 132395/1000000: episode: 1324, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 58.388, mean reward: 0.584 [0.508, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.679, 10.098], loss: 0.001570, mae: 0.042843, mean_q: 1.171646
 132495/1000000: episode: 1325, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 58.754, mean reward: 0.588 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.399, 10.182], loss: 0.001640, mae: 0.043802, mean_q: 1.172239
 132595/1000000: episode: 1326, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 61.543, mean reward: 0.615 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.386, 10.098], loss: 0.001491, mae: 0.042351, mean_q: 1.172225
 132695/1000000: episode: 1327, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.494, mean reward: 0.585 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.202], loss: 0.001595, mae: 0.043172, mean_q: 1.170305
 132795/1000000: episode: 1328, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.348, mean reward: 0.593 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.226, 10.098], loss: 0.001422, mae: 0.041454, mean_q: 1.167365
 132895/1000000: episode: 1329, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 58.717, mean reward: 0.587 [0.506, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.169, 10.098], loss: 0.001508, mae: 0.042360, mean_q: 1.170346
 132995/1000000: episode: 1330, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 61.303, mean reward: 0.613 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.374, 10.098], loss: 0.001433, mae: 0.041811, mean_q: 1.173042
 133095/1000000: episode: 1331, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.239, mean reward: 0.582 [0.515, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.339, 10.098], loss: 0.001569, mae: 0.043101, mean_q: 1.169945
 133195/1000000: episode: 1332, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 59.441, mean reward: 0.594 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.173, 10.106], loss: 0.001457, mae: 0.041499, mean_q: 1.170987
 133295/1000000: episode: 1333, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 58.789, mean reward: 0.588 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.224, 10.098], loss: 0.001529, mae: 0.042569, mean_q: 1.170797
 133395/1000000: episode: 1334, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: 60.496, mean reward: 0.605 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.819, 10.250], loss: 0.001469, mae: 0.041763, mean_q: 1.167246
 133495/1000000: episode: 1335, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 57.793, mean reward: 0.578 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.444, 10.112], loss: 0.001469, mae: 0.041780, mean_q: 1.172377
 133595/1000000: episode: 1336, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 58.661, mean reward: 0.587 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.611, 10.315], loss: 0.001532, mae: 0.042606, mean_q: 1.172190
 133695/1000000: episode: 1337, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.729, mean reward: 0.587 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.282, 10.333], loss: 0.001713, mae: 0.044118, mean_q: 1.170293
 133795/1000000: episode: 1338, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.824, mean reward: 0.578 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.618, 10.132], loss: 0.001475, mae: 0.041827, mean_q: 1.170663
 133895/1000000: episode: 1339, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 58.111, mean reward: 0.581 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.455, 10.098], loss: 0.001532, mae: 0.042477, mean_q: 1.167703
 133995/1000000: episode: 1340, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 59.743, mean reward: 0.597 [0.515, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.055, 10.098], loss: 0.001434, mae: 0.040799, mean_q: 1.162124
 134095/1000000: episode: 1341, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.019, mean reward: 0.570 [0.504, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.998, 10.098], loss: 0.001479, mae: 0.041126, mean_q: 1.167858
 134195/1000000: episode: 1342, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.417, mean reward: 0.584 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.196], loss: 0.001530, mae: 0.042354, mean_q: 1.166144
 134295/1000000: episode: 1343, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 57.729, mean reward: 0.577 [0.510, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.745, 10.250], loss: 0.001479, mae: 0.041505, mean_q: 1.166107
 134395/1000000: episode: 1344, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 57.828, mean reward: 0.578 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.482, 10.098], loss: 0.001552, mae: 0.042674, mean_q: 1.166313
 134495/1000000: episode: 1345, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 57.304, mean reward: 0.573 [0.498, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.495, 10.188], loss: 0.001505, mae: 0.041842, mean_q: 1.167673
 134595/1000000: episode: 1346, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 61.106, mean reward: 0.611 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.786, 10.164], loss: 0.001478, mae: 0.040836, mean_q: 1.165156
 134695/1000000: episode: 1347, duration: 0.710s, episode steps: 100, steps per second: 141, episode reward: 59.149, mean reward: 0.591 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.829, 10.274], loss: 0.001523, mae: 0.042206, mean_q: 1.164260
 134795/1000000: episode: 1348, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 62.417, mean reward: 0.624 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.456], loss: 0.001582, mae: 0.042981, mean_q: 1.167783
 134895/1000000: episode: 1349, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 59.441, mean reward: 0.594 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.284, 10.281], loss: 0.001568, mae: 0.043010, mean_q: 1.171901
 134995/1000000: episode: 1350, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.985, mean reward: 0.600 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.166, 10.098], loss: 0.001472, mae: 0.041615, mean_q: 1.166291
 135095/1000000: episode: 1351, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 56.719, mean reward: 0.567 [0.498, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.234, 10.098], loss: 0.001519, mae: 0.042354, mean_q: 1.169414
 135195/1000000: episode: 1352, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.972, mean reward: 0.580 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.659, 10.098], loss: 0.001629, mae: 0.043521, mean_q: 1.167646
 135295/1000000: episode: 1353, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.678, mean reward: 0.577 [0.500, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.118, 10.098], loss: 0.001519, mae: 0.041804, mean_q: 1.167718
 135395/1000000: episode: 1354, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 61.057, mean reward: 0.611 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.082, 10.098], loss: 0.001408, mae: 0.040902, mean_q: 1.165448
 135495/1000000: episode: 1355, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 61.266, mean reward: 0.613 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.098], loss: 0.001450, mae: 0.041196, mean_q: 1.168209
 135595/1000000: episode: 1356, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 59.305, mean reward: 0.593 [0.504, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.700, 10.098], loss: 0.001666, mae: 0.044553, mean_q: 1.166130
 135695/1000000: episode: 1357, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.325, mean reward: 0.603 [0.515, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.380, 10.098], loss: 0.001519, mae: 0.042363, mean_q: 1.166281
 135795/1000000: episode: 1358, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.552, mean reward: 0.576 [0.510, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.835, 10.098], loss: 0.001385, mae: 0.040807, mean_q: 1.166040
 135895/1000000: episode: 1359, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 58.670, mean reward: 0.587 [0.499, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.098], loss: 0.001407, mae: 0.041070, mean_q: 1.166664
 135995/1000000: episode: 1360, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.632, mean reward: 0.586 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.842, 10.098], loss: 0.001397, mae: 0.040642, mean_q: 1.165210
 136095/1000000: episode: 1361, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 60.973, mean reward: 0.610 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.554, 10.148], loss: 0.001517, mae: 0.042519, mean_q: 1.164359
 136195/1000000: episode: 1362, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 58.400, mean reward: 0.584 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.991, 10.098], loss: 0.001442, mae: 0.041527, mean_q: 1.166054
 136295/1000000: episode: 1363, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.488, mean reward: 0.585 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.736, 10.165], loss: 0.001537, mae: 0.042517, mean_q: 1.168609
 136395/1000000: episode: 1364, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 60.084, mean reward: 0.601 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.797, 10.166], loss: 0.001525, mae: 0.042489, mean_q: 1.170736
 136495/1000000: episode: 1365, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.822, mean reward: 0.598 [0.509, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.922, 10.098], loss: 0.001578, mae: 0.042789, mean_q: 1.168552
 136595/1000000: episode: 1366, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 57.260, mean reward: 0.573 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.183], loss: 0.001514, mae: 0.042715, mean_q: 1.171112
 136695/1000000: episode: 1367, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.175, mean reward: 0.592 [0.499, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.658, 10.194], loss: 0.001501, mae: 0.041835, mean_q: 1.169078
 136795/1000000: episode: 1368, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 61.491, mean reward: 0.615 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.171, 10.098], loss: 0.001452, mae: 0.041613, mean_q: 1.166321
 136895/1000000: episode: 1369, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 60.684, mean reward: 0.607 [0.513, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.683, 10.098], loss: 0.001595, mae: 0.043547, mean_q: 1.172040
 136995/1000000: episode: 1370, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.442, mean reward: 0.574 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.448, 10.098], loss: 0.001515, mae: 0.042316, mean_q: 1.172750
 137095/1000000: episode: 1371, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.987, mean reward: 0.580 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.098, 10.098], loss: 0.001441, mae: 0.041406, mean_q: 1.169721
 137195/1000000: episode: 1372, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.009, mean reward: 0.580 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.613, 10.127], loss: 0.001472, mae: 0.042009, mean_q: 1.168140
 137295/1000000: episode: 1373, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.345, mean reward: 0.583 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.176, 10.098], loss: 0.001446, mae: 0.041671, mean_q: 1.164636
 137395/1000000: episode: 1374, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 61.531, mean reward: 0.615 [0.510, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.898, 10.098], loss: 0.001614, mae: 0.044040, mean_q: 1.168320
 137495/1000000: episode: 1375, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.759, mean reward: 0.578 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.014, 10.098], loss: 0.001504, mae: 0.042330, mean_q: 1.169849
 137595/1000000: episode: 1376, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 60.714, mean reward: 0.607 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.484, 10.442], loss: 0.001603, mae: 0.043767, mean_q: 1.171947
 137695/1000000: episode: 1377, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 57.089, mean reward: 0.571 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.098], loss: 0.001522, mae: 0.042510, mean_q: 1.172388
 137795/1000000: episode: 1378, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 64.852, mean reward: 0.649 [0.505, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.331, 10.302], loss: 0.001599, mae: 0.043708, mean_q: 1.175040
 137895/1000000: episode: 1379, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.433, mean reward: 0.584 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.134], loss: 0.001487, mae: 0.041814, mean_q: 1.169187
 137995/1000000: episode: 1380, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 56.928, mean reward: 0.569 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.093, 10.220], loss: 0.001505, mae: 0.042147, mean_q: 1.169574
 138095/1000000: episode: 1381, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 58.018, mean reward: 0.580 [0.509, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.650, 10.098], loss: 0.001503, mae: 0.042168, mean_q: 1.166977
 138195/1000000: episode: 1382, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 57.737, mean reward: 0.577 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.480, 10.101], loss: 0.001442, mae: 0.041624, mean_q: 1.164871
 138295/1000000: episode: 1383, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 57.083, mean reward: 0.571 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.895, 10.098], loss: 0.001362, mae: 0.040371, mean_q: 1.169418
 138395/1000000: episode: 1384, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.373, mean reward: 0.594 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.354, 10.422], loss: 0.001426, mae: 0.041150, mean_q: 1.168856
 138495/1000000: episode: 1385, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 56.606, mean reward: 0.566 [0.502, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.234, 10.098], loss: 0.001461, mae: 0.041636, mean_q: 1.164788
 138595/1000000: episode: 1386, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.440, mean reward: 0.584 [0.509, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.190], loss: 0.001417, mae: 0.041331, mean_q: 1.167185
 138695/1000000: episode: 1387, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 59.689, mean reward: 0.597 [0.512, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.224, 10.158], loss: 0.001433, mae: 0.040898, mean_q: 1.167708
 138795/1000000: episode: 1388, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 59.669, mean reward: 0.597 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.718, 10.098], loss: 0.001554, mae: 0.043116, mean_q: 1.166878
 138895/1000000: episode: 1389, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.154, mean reward: 0.592 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.412, 10.098], loss: 0.001505, mae: 0.042484, mean_q: 1.169133
 138995/1000000: episode: 1390, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.685, mean reward: 0.597 [0.519, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.495, 10.098], loss: 0.001424, mae: 0.040706, mean_q: 1.163547
 139095/1000000: episode: 1391, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.433, mean reward: 0.584 [0.514, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.457, 10.098], loss: 0.001489, mae: 0.042179, mean_q: 1.169632
 139195/1000000: episode: 1392, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.026, mean reward: 0.570 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.062, 10.168], loss: 0.001492, mae: 0.042374, mean_q: 1.170824
 139295/1000000: episode: 1393, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 58.541, mean reward: 0.585 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.263], loss: 0.001380, mae: 0.040529, mean_q: 1.171324
 139395/1000000: episode: 1394, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.716, mean reward: 0.597 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.627, 10.098], loss: 0.001368, mae: 0.040074, mean_q: 1.169329
 139495/1000000: episode: 1395, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.920, mean reward: 0.599 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.609, 10.098], loss: 0.001506, mae: 0.042160, mean_q: 1.167113
 139595/1000000: episode: 1396, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 59.192, mean reward: 0.592 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.117, 10.098], loss: 0.001369, mae: 0.040675, mean_q: 1.170728
 139695/1000000: episode: 1397, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 59.137, mean reward: 0.591 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.838, 10.343], loss: 0.001404, mae: 0.040486, mean_q: 1.170314
 139795/1000000: episode: 1398, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 59.938, mean reward: 0.599 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.880, 10.141], loss: 0.001395, mae: 0.040778, mean_q: 1.167309
 139895/1000000: episode: 1399, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.137, mean reward: 0.591 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.098], loss: 0.001380, mae: 0.040682, mean_q: 1.167407
 139995/1000000: episode: 1400, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.446, mean reward: 0.594 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.716, 10.486], loss: 0.001506, mae: 0.042867, mean_q: 1.167505
 140095/1000000: episode: 1401, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 63.191, mean reward: 0.632 [0.515, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.471, 10.256], loss: 0.001423, mae: 0.041063, mean_q: 1.168355
 140195/1000000: episode: 1402, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 63.087, mean reward: 0.631 [0.513, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.576, 10.317], loss: 0.001455, mae: 0.041209, mean_q: 1.168815
 140295/1000000: episode: 1403, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 56.542, mean reward: 0.565 [0.498, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.697, 10.098], loss: 0.001395, mae: 0.041154, mean_q: 1.169628
 140395/1000000: episode: 1404, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 58.154, mean reward: 0.582 [0.517, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.137, 10.098], loss: 0.001414, mae: 0.040895, mean_q: 1.170728
 140495/1000000: episode: 1405, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 59.705, mean reward: 0.597 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.571, 10.098], loss: 0.001384, mae: 0.040388, mean_q: 1.171867
 140595/1000000: episode: 1406, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.748, mean reward: 0.577 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.993, 10.165], loss: 0.001385, mae: 0.040200, mean_q: 1.172793
 140695/1000000: episode: 1407, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 58.514, mean reward: 0.585 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.335, 10.098], loss: 0.001395, mae: 0.040680, mean_q: 1.171042
 140795/1000000: episode: 1408, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 58.300, mean reward: 0.583 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.201], loss: 0.001314, mae: 0.039936, mean_q: 1.168417
 140895/1000000: episode: 1409, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 62.276, mean reward: 0.623 [0.520, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.411, 10.466], loss: 0.001308, mae: 0.039642, mean_q: 1.168156
 140995/1000000: episode: 1410, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 57.979, mean reward: 0.580 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.534, 10.151], loss: 0.001406, mae: 0.041177, mean_q: 1.171267
 141095/1000000: episode: 1411, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.107, mean reward: 0.581 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.098], loss: 0.001353, mae: 0.040042, mean_q: 1.166915
 141195/1000000: episode: 1412, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 59.298, mean reward: 0.593 [0.508, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.269, 10.298], loss: 0.001373, mae: 0.040456, mean_q: 1.165797
 141295/1000000: episode: 1413, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.393, mean reward: 0.574 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.506, 10.229], loss: 0.001371, mae: 0.040263, mean_q: 1.171529
 141395/1000000: episode: 1414, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 57.771, mean reward: 0.578 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.505, 10.098], loss: 0.001368, mae: 0.040468, mean_q: 1.169793
 141495/1000000: episode: 1415, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 58.484, mean reward: 0.585 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.098], loss: 0.001346, mae: 0.040087, mean_q: 1.169752
 141595/1000000: episode: 1416, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.856, mean reward: 0.579 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: 0.001345, mae: 0.040169, mean_q: 1.167174
 141695/1000000: episode: 1417, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.120, mean reward: 0.591 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.519, 10.193], loss: 0.001285, mae: 0.039232, mean_q: 1.169782
 141795/1000000: episode: 1418, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 56.283, mean reward: 0.563 [0.505, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.220, 10.164], loss: 0.001262, mae: 0.038911, mean_q: 1.167710
 141895/1000000: episode: 1419, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 66.259, mean reward: 0.663 [0.505, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.042, 10.098], loss: 0.001328, mae: 0.039866, mean_q: 1.167989
 141995/1000000: episode: 1420, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 56.861, mean reward: 0.569 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.907, 10.224], loss: 0.001326, mae: 0.040361, mean_q: 1.167697
 142095/1000000: episode: 1421, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 59.521, mean reward: 0.595 [0.513, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.434, 10.177], loss: 0.001337, mae: 0.039875, mean_q: 1.167817
 142195/1000000: episode: 1422, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 61.603, mean reward: 0.616 [0.515, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.302, 10.126], loss: 0.001387, mae: 0.041282, mean_q: 1.169514
 142295/1000000: episode: 1423, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 58.426, mean reward: 0.584 [0.498, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.905, 10.276], loss: 0.001290, mae: 0.039716, mean_q: 1.170480
 142395/1000000: episode: 1424, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 60.288, mean reward: 0.603 [0.500, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.182, 10.137], loss: 0.001383, mae: 0.040797, mean_q: 1.170991
 142495/1000000: episode: 1425, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 57.629, mean reward: 0.576 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.336, 10.098], loss: 0.001338, mae: 0.040031, mean_q: 1.169042
 142595/1000000: episode: 1426, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.764, mean reward: 0.588 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.241, 10.098], loss: 0.001410, mae: 0.041021, mean_q: 1.170121
 142695/1000000: episode: 1427, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 60.609, mean reward: 0.606 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.419], loss: 0.001288, mae: 0.039256, mean_q: 1.165512
 142795/1000000: episode: 1428, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 59.102, mean reward: 0.591 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.837, 10.114], loss: 0.001359, mae: 0.040762, mean_q: 1.167934
 142895/1000000: episode: 1429, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 58.428, mean reward: 0.584 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.148, 10.098], loss: 0.001374, mae: 0.040820, mean_q: 1.168130
 142995/1000000: episode: 1430, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.152, mean reward: 0.582 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.765, 10.398], loss: 0.001435, mae: 0.040949, mean_q: 1.163733
 143095/1000000: episode: 1431, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 61.585, mean reward: 0.616 [0.513, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.568, 10.374], loss: 0.001358, mae: 0.040304, mean_q: 1.167317
 143195/1000000: episode: 1432, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.901, mean reward: 0.579 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.000, 10.211], loss: 0.001443, mae: 0.041646, mean_q: 1.170452
 143295/1000000: episode: 1433, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 57.426, mean reward: 0.574 [0.500, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.547, 10.098], loss: 0.001379, mae: 0.040622, mean_q: 1.171966
 143395/1000000: episode: 1434, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 57.605, mean reward: 0.576 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.388, 10.113], loss: 0.001316, mae: 0.039516, mean_q: 1.168975
 143495/1000000: episode: 1435, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.566, mean reward: 0.606 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.619, 10.106], loss: 0.001299, mae: 0.039626, mean_q: 1.162532
 143595/1000000: episode: 1436, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 59.321, mean reward: 0.593 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.013, 10.198], loss: 0.001387, mae: 0.040880, mean_q: 1.173021
 143695/1000000: episode: 1437, duration: 1.300s, episode steps: 100, steps per second: 77, episode reward: 57.607, mean reward: 0.576 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.068, 10.108], loss: 0.001410, mae: 0.041252, mean_q: 1.168019
 143795/1000000: episode: 1438, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.217, mean reward: 0.582 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.903, 10.148], loss: 0.001338, mae: 0.040322, mean_q: 1.168620
 143895/1000000: episode: 1439, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 57.082, mean reward: 0.571 [0.499, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.213, 10.098], loss: 0.001354, mae: 0.040422, mean_q: 1.167048
 143995/1000000: episode: 1440, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 57.180, mean reward: 0.572 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.889, 10.098], loss: 0.001328, mae: 0.040265, mean_q: 1.166371
 144095/1000000: episode: 1441, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 57.667, mean reward: 0.577 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.162, 10.098], loss: 0.001450, mae: 0.042006, mean_q: 1.167046
 144195/1000000: episode: 1442, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 61.477, mean reward: 0.615 [0.505, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.620, 10.098], loss: 0.001459, mae: 0.041525, mean_q: 1.167258
 144295/1000000: episode: 1443, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 58.481, mean reward: 0.585 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.794, 10.262], loss: 0.001415, mae: 0.040819, mean_q: 1.166123
 144395/1000000: episode: 1444, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 61.056, mean reward: 0.611 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.598, 10.098], loss: 0.001375, mae: 0.040520, mean_q: 1.172105
 144495/1000000: episode: 1445, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 61.618, mean reward: 0.616 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.586, 10.247], loss: 0.001382, mae: 0.040951, mean_q: 1.166298
 144595/1000000: episode: 1446, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: 60.857, mean reward: 0.609 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.533, 10.105], loss: 0.001420, mae: 0.040954, mean_q: 1.174570
 144695/1000000: episode: 1447, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 57.913, mean reward: 0.579 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.956, 10.128], loss: 0.001559, mae: 0.042946, mean_q: 1.170526
 144795/1000000: episode: 1448, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 59.290, mean reward: 0.593 [0.510, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.502, 10.279], loss: 0.001516, mae: 0.042691, mean_q: 1.169393
 144895/1000000: episode: 1449, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.035, mean reward: 0.580 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.904, 10.098], loss: 0.001487, mae: 0.041397, mean_q: 1.170300
 144995/1000000: episode: 1450, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.296, mean reward: 0.583 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.856, 10.098], loss: 0.001553, mae: 0.043168, mean_q: 1.170377
 145095/1000000: episode: 1451, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.704, mean reward: 0.597 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.324], loss: 0.001461, mae: 0.041170, mean_q: 1.162022
 145195/1000000: episode: 1452, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.599, mean reward: 0.586 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.330, 10.124], loss: 0.001441, mae: 0.041442, mean_q: 1.164285
 145295/1000000: episode: 1453, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 60.623, mean reward: 0.606 [0.515, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.678, 10.137], loss: 0.001652, mae: 0.044229, mean_q: 1.169046
 145395/1000000: episode: 1454, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 59.129, mean reward: 0.591 [0.502, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.978, 10.098], loss: 0.001581, mae: 0.042782, mean_q: 1.167962
 145495/1000000: episode: 1455, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.632, mean reward: 0.596 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.137, 10.155], loss: 0.001399, mae: 0.041284, mean_q: 1.170997
 145595/1000000: episode: 1456, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 58.504, mean reward: 0.585 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.411, 10.098], loss: 0.001513, mae: 0.041611, mean_q: 1.167201
 145695/1000000: episode: 1457, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.009, mean reward: 0.590 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.871, 10.104], loss: 0.001583, mae: 0.042887, mean_q: 1.165607
 145795/1000000: episode: 1458, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.543, mean reward: 0.605 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.712, 10.192], loss: 0.001510, mae: 0.042325, mean_q: 1.168410
 145895/1000000: episode: 1459, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.575, mean reward: 0.596 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.876, 10.433], loss: 0.001615, mae: 0.042980, mean_q: 1.169093
 145995/1000000: episode: 1460, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.190, mean reward: 0.592 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.990, 10.366], loss: 0.001488, mae: 0.041645, mean_q: 1.167405
 146095/1000000: episode: 1461, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 59.514, mean reward: 0.595 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.379, 10.317], loss: 0.001619, mae: 0.043492, mean_q: 1.166896
 146195/1000000: episode: 1462, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 58.109, mean reward: 0.581 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.688, 10.098], loss: 0.001577, mae: 0.043127, mean_q: 1.166733
 146295/1000000: episode: 1463, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 56.940, mean reward: 0.569 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.267, 10.123], loss: 0.001543, mae: 0.042591, mean_q: 1.166686
 146395/1000000: episode: 1464, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 56.724, mean reward: 0.567 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.260, 10.098], loss: 0.001484, mae: 0.041887, mean_q: 1.167961
 146495/1000000: episode: 1465, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.423, mean reward: 0.594 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.867, 10.098], loss: 0.001606, mae: 0.043048, mean_q: 1.167493
 146595/1000000: episode: 1466, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 61.100, mean reward: 0.611 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.059, 10.181], loss: 0.001546, mae: 0.042657, mean_q: 1.168871
 146695/1000000: episode: 1467, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.083, mean reward: 0.581 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.125], loss: 0.001546, mae: 0.042532, mean_q: 1.167987
 146795/1000000: episode: 1468, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 62.340, mean reward: 0.623 [0.514, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.644, 10.098], loss: 0.001581, mae: 0.043140, mean_q: 1.172607
 146895/1000000: episode: 1469, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 61.157, mean reward: 0.612 [0.511, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.315], loss: 0.001605, mae: 0.043860, mean_q: 1.170109
 146995/1000000: episode: 1470, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 59.085, mean reward: 0.591 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.967, 10.124], loss: 0.001488, mae: 0.042671, mean_q: 1.170558
 147095/1000000: episode: 1471, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 58.302, mean reward: 0.583 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.854, 10.170], loss: 0.001548, mae: 0.042985, mean_q: 1.169713
 147195/1000000: episode: 1472, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.806, mean reward: 0.588 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.576, 10.098], loss: 0.001440, mae: 0.041194, mean_q: 1.170372
 147295/1000000: episode: 1473, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 56.348, mean reward: 0.563 [0.497, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.827, 10.134], loss: 0.001572, mae: 0.043252, mean_q: 1.169489
 147395/1000000: episode: 1474, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 57.776, mean reward: 0.578 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.152, 10.288], loss: 0.001492, mae: 0.042550, mean_q: 1.168155
 147495/1000000: episode: 1475, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 56.966, mean reward: 0.570 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.175], loss: 0.001449, mae: 0.041277, mean_q: 1.163358
 147595/1000000: episode: 1476, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 58.205, mean reward: 0.582 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.134], loss: 0.001484, mae: 0.042025, mean_q: 1.164919
 147695/1000000: episode: 1477, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.228, mean reward: 0.592 [0.518, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.993, 10.181], loss: 0.001548, mae: 0.042908, mean_q: 1.168984
 147795/1000000: episode: 1478, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 56.317, mean reward: 0.563 [0.501, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.098], loss: 0.001497, mae: 0.041968, mean_q: 1.166609
 147895/1000000: episode: 1479, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.789, mean reward: 0.578 [0.505, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.163, 10.211], loss: 0.001524, mae: 0.042736, mean_q: 1.165044
 147995/1000000: episode: 1480, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 62.088, mean reward: 0.621 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.445, 10.306], loss: 0.001510, mae: 0.042306, mean_q: 1.165323
 148095/1000000: episode: 1481, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 57.347, mean reward: 0.573 [0.509, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.248], loss: 0.001480, mae: 0.042114, mean_q: 1.160451
 148195/1000000: episode: 1482, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.069, mean reward: 0.591 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.852, 10.247], loss: 0.001438, mae: 0.041348, mean_q: 1.163959
 148295/1000000: episode: 1483, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 61.360, mean reward: 0.614 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.743, 10.098], loss: 0.001537, mae: 0.042818, mean_q: 1.166282
 148395/1000000: episode: 1484, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 59.123, mean reward: 0.591 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.954, 10.224], loss: 0.001534, mae: 0.042807, mean_q: 1.168235
 148495/1000000: episode: 1485, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 62.619, mean reward: 0.626 [0.503, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.139, 10.423], loss: 0.001340, mae: 0.039747, mean_q: 1.162781
 148595/1000000: episode: 1486, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 58.722, mean reward: 0.587 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.923, 10.098], loss: 0.001494, mae: 0.041956, mean_q: 1.167114
 148695/1000000: episode: 1487, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 60.455, mean reward: 0.605 [0.508, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.123, 10.098], loss: 0.001493, mae: 0.042300, mean_q: 1.169088
 148795/1000000: episode: 1488, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 57.824, mean reward: 0.578 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.203], loss: 0.001449, mae: 0.041873, mean_q: 1.168862
 148895/1000000: episode: 1489, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 57.195, mean reward: 0.572 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.098], loss: 0.001427, mae: 0.041116, mean_q: 1.162104
 148995/1000000: episode: 1490, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 57.853, mean reward: 0.579 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.833, 10.098], loss: 0.001504, mae: 0.042285, mean_q: 1.170165
 149095/1000000: episode: 1491, duration: 0.755s, episode steps: 100, steps per second: 133, episode reward: 58.469, mean reward: 0.585 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.878, 10.098], loss: 0.001531, mae: 0.043036, mean_q: 1.166637
 149195/1000000: episode: 1492, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.943, mean reward: 0.579 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.098], loss: 0.001459, mae: 0.041700, mean_q: 1.168681
 149295/1000000: episode: 1493, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.831, mean reward: 0.578 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.843, 10.111], loss: 0.001609, mae: 0.044131, mean_q: 1.165690
 149395/1000000: episode: 1494, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 57.996, mean reward: 0.580 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.249, 10.098], loss: 0.001562, mae: 0.043244, mean_q: 1.168298
 149495/1000000: episode: 1495, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.954, mean reward: 0.590 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.616, 10.366], loss: 0.001554, mae: 0.043227, mean_q: 1.164443
 149595/1000000: episode: 1496, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 57.622, mean reward: 0.576 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.157, 10.098], loss: 0.001533, mae: 0.042674, mean_q: 1.165210
 149695/1000000: episode: 1497, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 57.684, mean reward: 0.577 [0.508, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.254, 10.190], loss: 0.001463, mae: 0.041706, mean_q: 1.165776
 149795/1000000: episode: 1498, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.566, mean reward: 0.586 [0.500, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.814, 10.098], loss: 0.001482, mae: 0.041813, mean_q: 1.160679
 149895/1000000: episode: 1499, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 59.836, mean reward: 0.598 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.285, 10.126], loss: 0.001521, mae: 0.042513, mean_q: 1.164232
 149995/1000000: episode: 1500, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 58.526, mean reward: 0.585 [0.511, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.542, 10.098], loss: 0.001623, mae: 0.044180, mean_q: 1.164127
 150095/1000000: episode: 1501, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 60.362, mean reward: 0.604 [0.525, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.924, 10.109], loss: 0.001372, mae: 0.040717, mean_q: 1.162307
 150195/1000000: episode: 1502, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: 59.365, mean reward: 0.594 [0.507, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.640, 10.108], loss: 0.001491, mae: 0.042468, mean_q: 1.164736
 150295/1000000: episode: 1503, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 59.429, mean reward: 0.594 [0.499, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.890, 10.292], loss: 0.001455, mae: 0.041709, mean_q: 1.164768
 150395/1000000: episode: 1504, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.903, mean reward: 0.609 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.998, 10.098], loss: 0.001445, mae: 0.041749, mean_q: 1.158451
 150495/1000000: episode: 1505, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 56.990, mean reward: 0.570 [0.499, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.098], loss: 0.001514, mae: 0.042207, mean_q: 1.164453
 150595/1000000: episode: 1506, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.933, mean reward: 0.609 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.232, 10.175], loss: 0.001417, mae: 0.040988, mean_q: 1.160898
 150695/1000000: episode: 1507, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 59.904, mean reward: 0.599 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.180, 10.098], loss: 0.001485, mae: 0.041700, mean_q: 1.163176
 150795/1000000: episode: 1508, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.053, mean reward: 0.581 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.890, 10.098], loss: 0.001429, mae: 0.041608, mean_q: 1.163804
 150895/1000000: episode: 1509, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 56.855, mean reward: 0.569 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.268, 10.147], loss: 0.001454, mae: 0.041659, mean_q: 1.165336
 150995/1000000: episode: 1510, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.594, mean reward: 0.596 [0.520, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.871, 10.233], loss: 0.001549, mae: 0.043083, mean_q: 1.166965
 151095/1000000: episode: 1511, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.280, mean reward: 0.583 [0.511, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.072, 10.098], loss: 0.001359, mae: 0.040032, mean_q: 1.164460
 151195/1000000: episode: 1512, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 60.058, mean reward: 0.601 [0.515, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.188, 10.098], loss: 0.001484, mae: 0.042286, mean_q: 1.166456
 151295/1000000: episode: 1513, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 57.979, mean reward: 0.580 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.878, 10.277], loss: 0.001463, mae: 0.041460, mean_q: 1.166546
 151395/1000000: episode: 1514, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 58.276, mean reward: 0.583 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.245, 10.098], loss: 0.001399, mae: 0.040811, mean_q: 1.163203
 151495/1000000: episode: 1515, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 58.864, mean reward: 0.589 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.430, 10.392], loss: 0.001379, mae: 0.040636, mean_q: 1.160202
 151595/1000000: episode: 1516, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 60.178, mean reward: 0.602 [0.518, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.598, 10.275], loss: 0.001452, mae: 0.040882, mean_q: 1.162667
 151695/1000000: episode: 1517, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 60.790, mean reward: 0.608 [0.514, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.983, 10.098], loss: 0.001406, mae: 0.040449, mean_q: 1.165223
 151795/1000000: episode: 1518, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 57.627, mean reward: 0.576 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.516, 10.124], loss: 0.001329, mae: 0.040091, mean_q: 1.164519
 151895/1000000: episode: 1519, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 59.434, mean reward: 0.594 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.680, 10.098], loss: 0.001396, mae: 0.040343, mean_q: 1.163414
 151995/1000000: episode: 1520, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 59.851, mean reward: 0.599 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.707, 10.098], loss: 0.001400, mae: 0.040445, mean_q: 1.161791
 152095/1000000: episode: 1521, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.228, mean reward: 0.582 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.089, 10.098], loss: 0.001430, mae: 0.041054, mean_q: 1.162716
 152195/1000000: episode: 1522, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 59.270, mean reward: 0.593 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.279, 10.098], loss: 0.001450, mae: 0.041454, mean_q: 1.162306
 152295/1000000: episode: 1523, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 58.367, mean reward: 0.584 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.690, 10.206], loss: 0.001575, mae: 0.043328, mean_q: 1.166146
 152395/1000000: episode: 1524, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 59.804, mean reward: 0.598 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.528, 10.397], loss: 0.001360, mae: 0.040304, mean_q: 1.163680
 152495/1000000: episode: 1525, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 58.672, mean reward: 0.587 [0.500, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.440], loss: 0.001507, mae: 0.042158, mean_q: 1.164327
 152595/1000000: episode: 1526, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 60.454, mean reward: 0.605 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.231, 10.311], loss: 0.001420, mae: 0.040911, mean_q: 1.169661
 152695/1000000: episode: 1527, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.017, mean reward: 0.580 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.847, 10.198], loss: 0.001475, mae: 0.041354, mean_q: 1.161347
 152795/1000000: episode: 1528, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 57.866, mean reward: 0.579 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.014, 10.098], loss: 0.001443, mae: 0.041185, mean_q: 1.167478
 152895/1000000: episode: 1529, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 62.505, mean reward: 0.625 [0.524, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.338, 10.098], loss: 0.001440, mae: 0.041221, mean_q: 1.168458
 152995/1000000: episode: 1530, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 59.701, mean reward: 0.597 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.408], loss: 0.001493, mae: 0.041555, mean_q: 1.169908
 153095/1000000: episode: 1531, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 58.282, mean reward: 0.583 [0.512, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.199, 10.176], loss: 0.001496, mae: 0.041958, mean_q: 1.164106
 153195/1000000: episode: 1532, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 56.716, mean reward: 0.567 [0.498, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.667, 10.098], loss: 0.001540, mae: 0.042675, mean_q: 1.163702
 153295/1000000: episode: 1533, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 60.749, mean reward: 0.607 [0.515, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.054, 10.106], loss: 0.001464, mae: 0.041077, mean_q: 1.166288
 153395/1000000: episode: 1534, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.219, mean reward: 0.582 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.161], loss: 0.001437, mae: 0.040869, mean_q: 1.163449
 153495/1000000: episode: 1535, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 58.431, mean reward: 0.584 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.098], loss: 0.001397, mae: 0.040257, mean_q: 1.165802
 153595/1000000: episode: 1536, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 62.482, mean reward: 0.625 [0.502, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.281], loss: 0.001510, mae: 0.041960, mean_q: 1.165141
 153695/1000000: episode: 1537, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 58.034, mean reward: 0.580 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.937, 10.335], loss: 0.001480, mae: 0.041690, mean_q: 1.162866
 153795/1000000: episode: 1538, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.281, mean reward: 0.593 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.600, 10.098], loss: 0.001410, mae: 0.040739, mean_q: 1.165356
 153895/1000000: episode: 1539, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 60.079, mean reward: 0.601 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.767, 10.098], loss: 0.001584, mae: 0.042185, mean_q: 1.166298
 153995/1000000: episode: 1540, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 61.181, mean reward: 0.612 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.249, 10.195], loss: 0.001392, mae: 0.040512, mean_q: 1.167198
 154095/1000000: episode: 1541, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.323, mean reward: 0.593 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.698, 10.302], loss: 0.001472, mae: 0.041243, mean_q: 1.167883
 154195/1000000: episode: 1542, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 59.538, mean reward: 0.595 [0.505, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.133], loss: 0.001441, mae: 0.041133, mean_q: 1.168368
 154295/1000000: episode: 1543, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 62.788, mean reward: 0.628 [0.513, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.380, 10.098], loss: 0.001398, mae: 0.040641, mean_q: 1.167064
 154395/1000000: episode: 1544, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 57.253, mean reward: 0.573 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.452, 10.285], loss: 0.001483, mae: 0.041784, mean_q: 1.172356
 154495/1000000: episode: 1545, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 58.888, mean reward: 0.589 [0.508, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.916, 10.211], loss: 0.001389, mae: 0.039964, mean_q: 1.168704
 154595/1000000: episode: 1546, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.844, mean reward: 0.578 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.098], loss: 0.001618, mae: 0.043203, mean_q: 1.169808
 154695/1000000: episode: 1547, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.991, mean reward: 0.590 [0.514, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.105, 10.098], loss: 0.001438, mae: 0.041040, mean_q: 1.170882
 154795/1000000: episode: 1548, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 57.970, mean reward: 0.580 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.709, 10.098], loss: 0.001597, mae: 0.043213, mean_q: 1.172693
 154895/1000000: episode: 1549, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.796, mean reward: 0.588 [0.519, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.480, 10.404], loss: 0.001520, mae: 0.041771, mean_q: 1.174453
 154995/1000000: episode: 1550, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.201, mean reward: 0.592 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.098], loss: 0.001421, mae: 0.041286, mean_q: 1.172892
 155095/1000000: episode: 1551, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 58.795, mean reward: 0.588 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.361, 10.098], loss: 0.001439, mae: 0.040420, mean_q: 1.167070
 155195/1000000: episode: 1552, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 58.155, mean reward: 0.582 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.893, 10.224], loss: 0.001473, mae: 0.041775, mean_q: 1.167921
 155295/1000000: episode: 1553, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 58.196, mean reward: 0.582 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.397, 10.133], loss: 0.001521, mae: 0.041824, mean_q: 1.170742
 155395/1000000: episode: 1554, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 58.099, mean reward: 0.581 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.782, 10.130], loss: 0.001420, mae: 0.041299, mean_q: 1.171518
 155495/1000000: episode: 1555, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 60.357, mean reward: 0.604 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.450, 10.105], loss: 0.001462, mae: 0.041155, mean_q: 1.170379
 155595/1000000: episode: 1556, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.387, mean reward: 0.574 [0.515, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.627, 10.120], loss: 0.001622, mae: 0.043477, mean_q: 1.170832
 155695/1000000: episode: 1557, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 56.637, mean reward: 0.566 [0.508, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.669, 10.098], loss: 0.001389, mae: 0.040482, mean_q: 1.168650
 155795/1000000: episode: 1558, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.532, mean reward: 0.575 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.401, 10.215], loss: 0.001452, mae: 0.041311, mean_q: 1.169702
 155895/1000000: episode: 1559, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.802, mean reward: 0.578 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.596, 10.206], loss: 0.001429, mae: 0.040562, mean_q: 1.167038
 155995/1000000: episode: 1560, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: 57.719, mean reward: 0.577 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.678, 10.158], loss: 0.001442, mae: 0.041226, mean_q: 1.164186
 156095/1000000: episode: 1561, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.576, mean reward: 0.576 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.712, 10.300], loss: 0.001463, mae: 0.041598, mean_q: 1.166384
 156195/1000000: episode: 1562, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.064, mean reward: 0.571 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.444, 10.182], loss: 0.001427, mae: 0.040412, mean_q: 1.168602
 156295/1000000: episode: 1563, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 58.814, mean reward: 0.588 [0.511, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.042, 10.217], loss: 0.001623, mae: 0.043369, mean_q: 1.168489
 156395/1000000: episode: 1564, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 58.554, mean reward: 0.586 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.099], loss: 0.001507, mae: 0.042020, mean_q: 1.169016
 156495/1000000: episode: 1565, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 57.490, mean reward: 0.575 [0.512, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.380, 10.098], loss: 0.001420, mae: 0.040974, mean_q: 1.164400
 156595/1000000: episode: 1566, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.497, mean reward: 0.575 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.139, 10.098], loss: 0.001483, mae: 0.042215, mean_q: 1.165983
 156695/1000000: episode: 1567, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 57.345, mean reward: 0.573 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.289, 10.120], loss: 0.001498, mae: 0.041970, mean_q: 1.164263
 156795/1000000: episode: 1568, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.066, mean reward: 0.571 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.176, 10.228], loss: 0.001500, mae: 0.041835, mean_q: 1.161389
 156895/1000000: episode: 1569, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.008, mean reward: 0.570 [0.498, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.054, 10.098], loss: 0.001486, mae: 0.041768, mean_q: 1.161074
 156995/1000000: episode: 1570, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 58.595, mean reward: 0.586 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.098], loss: 0.001459, mae: 0.041281, mean_q: 1.160317
 157095/1000000: episode: 1571, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 58.299, mean reward: 0.583 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.583, 10.178], loss: 0.001452, mae: 0.041539, mean_q: 1.159923
 157195/1000000: episode: 1572, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.280, mean reward: 0.593 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.438, 10.349], loss: 0.001473, mae: 0.041897, mean_q: 1.163316
 157295/1000000: episode: 1573, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 61.194, mean reward: 0.612 [0.516, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.743, 10.098], loss: 0.001438, mae: 0.041055, mean_q: 1.162045
 157395/1000000: episode: 1574, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 58.025, mean reward: 0.580 [0.501, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.728, 10.098], loss: 0.001387, mae: 0.040313, mean_q: 1.161117
 157495/1000000: episode: 1575, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 56.610, mean reward: 0.566 [0.514, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.459, 10.261], loss: 0.001361, mae: 0.039802, mean_q: 1.160975
 157595/1000000: episode: 1576, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 61.900, mean reward: 0.619 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.297, 10.098], loss: 0.001408, mae: 0.040754, mean_q: 1.157274
 157695/1000000: episode: 1577, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.793, mean reward: 0.598 [0.510, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.137], loss: 0.001433, mae: 0.041182, mean_q: 1.160472
 157795/1000000: episode: 1578, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 60.380, mean reward: 0.604 [0.514, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.299], loss: 0.001434, mae: 0.041035, mean_q: 1.160058
 157895/1000000: episode: 1579, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 57.527, mean reward: 0.575 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.752, 10.098], loss: 0.001442, mae: 0.041379, mean_q: 1.159401
 157995/1000000: episode: 1580, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 57.405, mean reward: 0.574 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.621, 10.120], loss: 0.001471, mae: 0.041040, mean_q: 1.158702
 158095/1000000: episode: 1581, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.912, mean reward: 0.579 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.098], loss: 0.001526, mae: 0.042051, mean_q: 1.158230
 158195/1000000: episode: 1582, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 56.537, mean reward: 0.565 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.305, 10.098], loss: 0.001501, mae: 0.042397, mean_q: 1.159243
 158295/1000000: episode: 1583, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 56.930, mean reward: 0.569 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.123], loss: 0.001525, mae: 0.042083, mean_q: 1.158848
 158395/1000000: episode: 1584, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.824, mean reward: 0.588 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.995, 10.262], loss: 0.001401, mae: 0.040336, mean_q: 1.156367
 158495/1000000: episode: 1585, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 61.317, mean reward: 0.613 [0.506, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.393, 10.098], loss: 0.001407, mae: 0.040590, mean_q: 1.163659
 158595/1000000: episode: 1586, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 59.533, mean reward: 0.595 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.067, 10.218], loss: 0.001471, mae: 0.041513, mean_q: 1.156825
 158695/1000000: episode: 1587, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 59.527, mean reward: 0.595 [0.518, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.942, 10.233], loss: 0.001432, mae: 0.041020, mean_q: 1.158864
 158795/1000000: episode: 1588, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 63.495, mean reward: 0.635 [0.535, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.604, 10.269], loss: 0.001447, mae: 0.041164, mean_q: 1.159195
 158895/1000000: episode: 1589, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 60.406, mean reward: 0.604 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.261, 10.098], loss: 0.001480, mae: 0.042071, mean_q: 1.161920
 158995/1000000: episode: 1590, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 59.486, mean reward: 0.595 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.975, 10.098], loss: 0.001468, mae: 0.041755, mean_q: 1.160612
 159095/1000000: episode: 1591, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 60.588, mean reward: 0.606 [0.510, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.342, 10.098], loss: 0.001456, mae: 0.041345, mean_q: 1.157543
 159195/1000000: episode: 1592, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.654, mean reward: 0.587 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.542, 10.273], loss: 0.001462, mae: 0.041616, mean_q: 1.160077
 159295/1000000: episode: 1593, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.843, mean reward: 0.578 [0.507, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.391, 10.098], loss: 0.001517, mae: 0.042121, mean_q: 1.157737
 159395/1000000: episode: 1594, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 58.801, mean reward: 0.588 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.276, 10.098], loss: 0.001551, mae: 0.042367, mean_q: 1.159769
 159495/1000000: episode: 1595, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.897, mean reward: 0.579 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.569, 10.259], loss: 0.001388, mae: 0.040856, mean_q: 1.157347
 159595/1000000: episode: 1596, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.523, mean reward: 0.585 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.824, 10.231], loss: 0.001456, mae: 0.041664, mean_q: 1.155764
 159695/1000000: episode: 1597, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: 57.679, mean reward: 0.577 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.707, 10.273], loss: 0.001498, mae: 0.041948, mean_q: 1.158403
 159795/1000000: episode: 1598, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.077, mean reward: 0.591 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.516, 10.098], loss: 0.001559, mae: 0.043272, mean_q: 1.158833
 159895/1000000: episode: 1599, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 61.634, mean reward: 0.616 [0.518, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.104, 10.098], loss: 0.001517, mae: 0.042173, mean_q: 1.160934
 159995/1000000: episode: 1600, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.756, mean reward: 0.588 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.316, 10.277], loss: 0.001416, mae: 0.041440, mean_q: 1.160079
 160095/1000000: episode: 1601, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 58.134, mean reward: 0.581 [0.512, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.084, 10.098], loss: 0.001431, mae: 0.041538, mean_q: 1.158838
 160195/1000000: episode: 1602, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 60.243, mean reward: 0.602 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.481, 10.098], loss: 0.001479, mae: 0.041970, mean_q: 1.158691
 160295/1000000: episode: 1603, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 61.994, mean reward: 0.620 [0.527, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.451, 10.158], loss: 0.001408, mae: 0.040790, mean_q: 1.160647
 160395/1000000: episode: 1604, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 59.762, mean reward: 0.598 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.305], loss: 0.001574, mae: 0.042638, mean_q: 1.159984
 160495/1000000: episode: 1605, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 57.930, mean reward: 0.579 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.182], loss: 0.001373, mae: 0.040386, mean_q: 1.158104
 160595/1000000: episode: 1606, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 60.037, mean reward: 0.600 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.545, 10.149], loss: 0.001365, mae: 0.040752, mean_q: 1.160589
 160695/1000000: episode: 1607, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 57.596, mean reward: 0.576 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.436, 10.102], loss: 0.001513, mae: 0.041769, mean_q: 1.161684
 160795/1000000: episode: 1608, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.999, mean reward: 0.580 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.098], loss: 0.001435, mae: 0.041445, mean_q: 1.163360
 160895/1000000: episode: 1609, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.839, mean reward: 0.598 [0.507, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.136, 10.098], loss: 0.001398, mae: 0.040984, mean_q: 1.163288
 160995/1000000: episode: 1610, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.942, mean reward: 0.589 [0.514, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.496, 10.389], loss: 0.001495, mae: 0.042140, mean_q: 1.167835
 161095/1000000: episode: 1611, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 58.438, mean reward: 0.584 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.706, 10.151], loss: 0.001482, mae: 0.041982, mean_q: 1.165136
 161195/1000000: episode: 1612, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 58.546, mean reward: 0.585 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.931, 10.368], loss: 0.001467, mae: 0.041410, mean_q: 1.165856
 161295/1000000: episode: 1613, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.734, mean reward: 0.597 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.088, 10.280], loss: 0.001469, mae: 0.042245, mean_q: 1.164822
 161395/1000000: episode: 1614, duration: 0.816s, episode steps: 100, steps per second: 122, episode reward: 56.968, mean reward: 0.570 [0.502, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.831, 10.098], loss: 0.001372, mae: 0.040537, mean_q: 1.167260
 161495/1000000: episode: 1615, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: 57.747, mean reward: 0.577 [0.500, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.465, 10.292], loss: 0.001469, mae: 0.041902, mean_q: 1.162708
 161595/1000000: episode: 1616, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 56.842, mean reward: 0.568 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.667, 10.137], loss: 0.001503, mae: 0.042088, mean_q: 1.164865
 161695/1000000: episode: 1617, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 60.536, mean reward: 0.605 [0.507, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.147, 10.233], loss: 0.001444, mae: 0.041842, mean_q: 1.166520
 161795/1000000: episode: 1618, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 62.109, mean reward: 0.621 [0.507, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.738, 10.098], loss: 0.001485, mae: 0.041556, mean_q: 1.165791
 161895/1000000: episode: 1619, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 58.919, mean reward: 0.589 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.098], loss: 0.001428, mae: 0.041040, mean_q: 1.168448
 161995/1000000: episode: 1620, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 58.264, mean reward: 0.583 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.098], loss: 0.001467, mae: 0.041818, mean_q: 1.168931
 162095/1000000: episode: 1621, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 63.849, mean reward: 0.638 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.047, 10.098], loss: 0.001437, mae: 0.041258, mean_q: 1.169078
 162195/1000000: episode: 1622, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 59.436, mean reward: 0.594 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.415, 10.098], loss: 0.001478, mae: 0.041511, mean_q: 1.172706
 162295/1000000: episode: 1623, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.958, mean reward: 0.590 [0.523, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.952, 10.182], loss: 0.001466, mae: 0.041396, mean_q: 1.169774
 162395/1000000: episode: 1624, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 58.755, mean reward: 0.588 [0.498, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.227], loss: 0.001499, mae: 0.041622, mean_q: 1.169619
 162495/1000000: episode: 1625, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.616, mean reward: 0.596 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.140, 10.098], loss: 0.001529, mae: 0.043110, mean_q: 1.172095
 162595/1000000: episode: 1626, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 57.056, mean reward: 0.571 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.202], loss: 0.001400, mae: 0.040957, mean_q: 1.169800
 162695/1000000: episode: 1627, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: 60.601, mean reward: 0.606 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.856, 10.233], loss: 0.001407, mae: 0.040610, mean_q: 1.166307
 162795/1000000: episode: 1628, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 61.040, mean reward: 0.610 [0.502, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.098], loss: 0.001428, mae: 0.041428, mean_q: 1.172795
 162895/1000000: episode: 1629, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 58.354, mean reward: 0.584 [0.514, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.396, 10.169], loss: 0.001552, mae: 0.041933, mean_q: 1.167483
 162995/1000000: episode: 1630, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.981, mean reward: 0.580 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.172, 10.397], loss: 0.001396, mae: 0.040639, mean_q: 1.167315
 163095/1000000: episode: 1631, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 60.331, mean reward: 0.603 [0.519, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.425], loss: 0.001525, mae: 0.042105, mean_q: 1.172107
 163195/1000000: episode: 1632, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 56.982, mean reward: 0.570 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.432, 10.160], loss: 0.001391, mae: 0.040969, mean_q: 1.169904
 163295/1000000: episode: 1633, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 63.150, mean reward: 0.631 [0.538, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.702, 10.098], loss: 0.001486, mae: 0.042050, mean_q: 1.175727
 163395/1000000: episode: 1634, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 59.455, mean reward: 0.595 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.004, 10.098], loss: 0.001452, mae: 0.041708, mean_q: 1.173123
 163495/1000000: episode: 1635, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 60.021, mean reward: 0.600 [0.503, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.624, 10.252], loss: 0.001443, mae: 0.041314, mean_q: 1.175765
 163595/1000000: episode: 1636, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.761, mean reward: 0.588 [0.509, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.358, 10.236], loss: 0.001385, mae: 0.041223, mean_q: 1.175226
 163695/1000000: episode: 1637, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 60.158, mean reward: 0.602 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.098], loss: 0.001357, mae: 0.040517, mean_q: 1.171911
 163795/1000000: episode: 1638, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 58.357, mean reward: 0.584 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.272], loss: 0.001486, mae: 0.041622, mean_q: 1.170651
 163895/1000000: episode: 1639, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 56.124, mean reward: 0.561 [0.498, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.131], loss: 0.001445, mae: 0.041524, mean_q: 1.173536
 163995/1000000: episode: 1640, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 57.235, mean reward: 0.572 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.236, 10.098], loss: 0.001499, mae: 0.042229, mean_q: 1.172974
 164095/1000000: episode: 1641, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 61.696, mean reward: 0.617 [0.499, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.024, 10.409], loss: 0.001429, mae: 0.041069, mean_q: 1.167742
 164195/1000000: episode: 1642, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 57.788, mean reward: 0.578 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.254, 10.098], loss: 0.001404, mae: 0.041256, mean_q: 1.169117
 164295/1000000: episode: 1643, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 57.072, mean reward: 0.571 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.123], loss: 0.001459, mae: 0.041349, mean_q: 1.169354
 164395/1000000: episode: 1644, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 58.206, mean reward: 0.582 [0.501, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.254, 10.127], loss: 0.001305, mae: 0.039237, mean_q: 1.168546
 164495/1000000: episode: 1645, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.627, mean reward: 0.586 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.202], loss: 0.001364, mae: 0.041206, mean_q: 1.166911
 164595/1000000: episode: 1646, duration: 0.738s, episode steps: 100, steps per second: 135, episode reward: 59.528, mean reward: 0.595 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.112], loss: 0.001293, mae: 0.039670, mean_q: 1.166294
 164695/1000000: episode: 1647, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.923, mean reward: 0.589 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.235, 10.098], loss: 0.001307, mae: 0.039903, mean_q: 1.167080
 164795/1000000: episode: 1648, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.284, mean reward: 0.583 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.626, 10.218], loss: 0.001460, mae: 0.041579, mean_q: 1.170894
 164895/1000000: episode: 1649, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 59.029, mean reward: 0.590 [0.501, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.098], loss: 0.001295, mae: 0.039808, mean_q: 1.168005
 164995/1000000: episode: 1650, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 59.611, mean reward: 0.596 [0.513, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.240], loss: 0.001396, mae: 0.040841, mean_q: 1.169718
 165095/1000000: episode: 1651, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 60.407, mean reward: 0.604 [0.513, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.098], loss: 0.001419, mae: 0.041838, mean_q: 1.172340
 165195/1000000: episode: 1652, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 59.896, mean reward: 0.599 [0.510, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.502, 10.098], loss: 0.001383, mae: 0.041135, mean_q: 1.166746
 165295/1000000: episode: 1653, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 60.902, mean reward: 0.609 [0.506, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.028, 10.098], loss: 0.001247, mae: 0.038963, mean_q: 1.167763
 165395/1000000: episode: 1654, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 60.670, mean reward: 0.607 [0.503, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.465, 10.098], loss: 0.001321, mae: 0.040294, mean_q: 1.167287
 165495/1000000: episode: 1655, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.846, mean reward: 0.578 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.663, 10.098], loss: 0.001463, mae: 0.042199, mean_q: 1.169487
 165595/1000000: episode: 1656, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 58.017, mean reward: 0.580 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.384, 10.200], loss: 0.001452, mae: 0.041647, mean_q: 1.172060
 165695/1000000: episode: 1657, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 60.338, mean reward: 0.603 [0.503, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.098], loss: 0.001404, mae: 0.041390, mean_q: 1.167215
 165795/1000000: episode: 1658, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 59.611, mean reward: 0.596 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.385, 10.098], loss: 0.001334, mae: 0.039880, mean_q: 1.167905
 165895/1000000: episode: 1659, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 62.824, mean reward: 0.628 [0.516, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.222], loss: 0.001443, mae: 0.041266, mean_q: 1.168199
 165995/1000000: episode: 1660, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 59.745, mean reward: 0.597 [0.499, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.684, 10.098], loss: 0.001499, mae: 0.041945, mean_q: 1.172450
 166095/1000000: episode: 1661, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 57.023, mean reward: 0.570 [0.502, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.316, 10.111], loss: 0.001445, mae: 0.041468, mean_q: 1.173293
 166195/1000000: episode: 1662, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 60.166, mean reward: 0.602 [0.512, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.804, 10.187], loss: 0.001476, mae: 0.041467, mean_q: 1.171570
 166295/1000000: episode: 1663, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.264, mean reward: 0.583 [0.513, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.391, 10.306], loss: 0.001497, mae: 0.041861, mean_q: 1.171559
 166395/1000000: episode: 1664, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.348, mean reward: 0.573 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.045, 10.233], loss: 0.001371, mae: 0.040259, mean_q: 1.169284
 166495/1000000: episode: 1665, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 64.563, mean reward: 0.646 [0.513, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.556, 10.098], loss: 0.001519, mae: 0.042363, mean_q: 1.172684
 166595/1000000: episode: 1666, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 59.840, mean reward: 0.598 [0.510, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.240, 10.227], loss: 0.001513, mae: 0.042264, mean_q: 1.179827
 166695/1000000: episode: 1667, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 57.646, mean reward: 0.576 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.225], loss: 0.001528, mae: 0.042878, mean_q: 1.176815
 166795/1000000: episode: 1668, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 58.680, mean reward: 0.587 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.937, 10.098], loss: 0.001644, mae: 0.043535, mean_q: 1.176726
 166895/1000000: episode: 1669, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 59.024, mean reward: 0.590 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.713, 10.348], loss: 0.001455, mae: 0.041355, mean_q: 1.171370
 166995/1000000: episode: 1670, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 58.819, mean reward: 0.588 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.627, 10.337], loss: 0.001367, mae: 0.040817, mean_q: 1.172228
 167095/1000000: episode: 1671, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 57.684, mean reward: 0.577 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.098], loss: 0.001458, mae: 0.041325, mean_q: 1.173057
 167195/1000000: episode: 1672, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.150, mean reward: 0.581 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.553, 10.224], loss: 0.001537, mae: 0.042100, mean_q: 1.170094
 167295/1000000: episode: 1673, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.360, mean reward: 0.574 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.284, 10.098], loss: 0.001425, mae: 0.041058, mean_q: 1.170829
 167395/1000000: episode: 1674, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 61.356, mean reward: 0.614 [0.515, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.678, 10.224], loss: 0.001527, mae: 0.042576, mean_q: 1.169459
 167495/1000000: episode: 1675, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.652, mean reward: 0.577 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.979, 10.130], loss: 0.001510, mae: 0.042147, mean_q: 1.174006
 167595/1000000: episode: 1676, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 58.253, mean reward: 0.583 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.892, 10.098], loss: 0.001466, mae: 0.041978, mean_q: 1.173015
 167695/1000000: episode: 1677, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 57.807, mean reward: 0.578 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.165, 10.098], loss: 0.001485, mae: 0.041805, mean_q: 1.169687
 167795/1000000: episode: 1678, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 57.473, mean reward: 0.575 [0.505, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.217, 10.286], loss: 0.001471, mae: 0.042173, mean_q: 1.166432
 167895/1000000: episode: 1679, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 59.894, mean reward: 0.599 [0.507, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.538, 10.098], loss: 0.001427, mae: 0.041265, mean_q: 1.170448
 167995/1000000: episode: 1680, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.080, mean reward: 0.581 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.021, 10.098], loss: 0.001539, mae: 0.042761, mean_q: 1.169905
 168095/1000000: episode: 1681, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 60.152, mean reward: 0.602 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.397, 10.098], loss: 0.001412, mae: 0.040757, mean_q: 1.170040
 168195/1000000: episode: 1682, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 60.573, mean reward: 0.606 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.761, 10.125], loss: 0.001461, mae: 0.041745, mean_q: 1.169773
 168295/1000000: episode: 1683, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 57.595, mean reward: 0.576 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.713, 10.243], loss: 0.001603, mae: 0.043929, mean_q: 1.171688
 168395/1000000: episode: 1684, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 58.790, mean reward: 0.588 [0.512, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.266, 10.174], loss: 0.001559, mae: 0.042696, mean_q: 1.169183
 168495/1000000: episode: 1685, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 58.104, mean reward: 0.581 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.200, 10.187], loss: 0.001422, mae: 0.040946, mean_q: 1.168058
 168595/1000000: episode: 1686, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 56.920, mean reward: 0.569 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.019, 10.098], loss: 0.001519, mae: 0.042209, mean_q: 1.168397
 168695/1000000: episode: 1687, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.036, mean reward: 0.580 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.872, 10.153], loss: 0.001619, mae: 0.043881, mean_q: 1.166914
 168795/1000000: episode: 1688, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 58.767, mean reward: 0.588 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.495, 10.204], loss: 0.001594, mae: 0.042992, mean_q: 1.167594
 168895/1000000: episode: 1689, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 60.346, mean reward: 0.603 [0.515, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.097, 10.344], loss: 0.001438, mae: 0.041147, mean_q: 1.165297
 168995/1000000: episode: 1690, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 58.779, mean reward: 0.588 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.307, 10.098], loss: 0.001565, mae: 0.042623, mean_q: 1.167293
 169095/1000000: episode: 1691, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 61.036, mean reward: 0.610 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.844, 10.098], loss: 0.001537, mae: 0.042554, mean_q: 1.165298
 169195/1000000: episode: 1692, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 58.965, mean reward: 0.590 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.098], loss: 0.001488, mae: 0.041715, mean_q: 1.167022
 169295/1000000: episode: 1693, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.335, mean reward: 0.593 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.750, 10.098], loss: 0.001394, mae: 0.040568, mean_q: 1.166256
 169395/1000000: episode: 1694, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 57.078, mean reward: 0.571 [0.502, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.666, 10.304], loss: 0.001613, mae: 0.043639, mean_q: 1.169673
 169495/1000000: episode: 1695, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 58.171, mean reward: 0.582 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.222, 10.240], loss: 0.001476, mae: 0.041786, mean_q: 1.170440
 169595/1000000: episode: 1696, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 57.867, mean reward: 0.579 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.374, 10.101], loss: 0.001471, mae: 0.041006, mean_q: 1.166887
 169695/1000000: episode: 1697, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 57.560, mean reward: 0.576 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.728, 10.174], loss: 0.001609, mae: 0.043029, mean_q: 1.167375
 169795/1000000: episode: 1698, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 60.511, mean reward: 0.605 [0.497, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.112], loss: 0.001519, mae: 0.042059, mean_q: 1.163477
 169895/1000000: episode: 1699, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.388, mean reward: 0.584 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.098], loss: 0.001526, mae: 0.041590, mean_q: 1.166484
 169995/1000000: episode: 1700, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 63.632, mean reward: 0.636 [0.522, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.330], loss: 0.001562, mae: 0.042243, mean_q: 1.167729
 170095/1000000: episode: 1701, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 59.600, mean reward: 0.596 [0.518, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.840, 10.204], loss: 0.001614, mae: 0.043328, mean_q: 1.167619
 170195/1000000: episode: 1702, duration: 0.858s, episode steps: 100, steps per second: 116, episode reward: 58.200, mean reward: 0.582 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.964, 10.098], loss: 0.001478, mae: 0.041807, mean_q: 1.167749
 170295/1000000: episode: 1703, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 58.311, mean reward: 0.583 [0.513, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.138], loss: 0.001436, mae: 0.041434, mean_q: 1.169544
 170395/1000000: episode: 1704, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 60.645, mean reward: 0.606 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.191, 10.098], loss: 0.001484, mae: 0.041851, mean_q: 1.165981
 170495/1000000: episode: 1705, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.194, mean reward: 0.582 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.369, 10.098], loss: 0.001599, mae: 0.042926, mean_q: 1.166683
 170595/1000000: episode: 1706, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 59.101, mean reward: 0.591 [0.512, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.207, 10.245], loss: 0.001464, mae: 0.041582, mean_q: 1.167035
 170695/1000000: episode: 1707, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 59.438, mean reward: 0.594 [0.511, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.355, 10.098], loss: 0.001472, mae: 0.041619, mean_q: 1.169315
 170795/1000000: episode: 1708, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 57.705, mean reward: 0.577 [0.503, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.665, 10.129], loss: 0.001357, mae: 0.039890, mean_q: 1.168486
 170895/1000000: episode: 1709, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.578, mean reward: 0.576 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.603, 10.254], loss: 0.001513, mae: 0.041392, mean_q: 1.166115
 170995/1000000: episode: 1710, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 57.482, mean reward: 0.575 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.359, 10.098], loss: 0.001389, mae: 0.040503, mean_q: 1.162340
 171095/1000000: episode: 1711, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 58.724, mean reward: 0.587 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.860, 10.199], loss: 0.001494, mae: 0.042241, mean_q: 1.165349
 171195/1000000: episode: 1712, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 58.486, mean reward: 0.585 [0.498, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.838, 10.365], loss: 0.001490, mae: 0.041216, mean_q: 1.165228
 171295/1000000: episode: 1713, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 61.360, mean reward: 0.614 [0.504, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.200, 10.262], loss: 0.001414, mae: 0.041304, mean_q: 1.163844
 171395/1000000: episode: 1714, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 58.650, mean reward: 0.587 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.507, 10.188], loss: 0.001362, mae: 0.040311, mean_q: 1.165140
 171495/1000000: episode: 1715, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 57.755, mean reward: 0.578 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.430, 10.098], loss: 0.001540, mae: 0.042545, mean_q: 1.167776
 171595/1000000: episode: 1716, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 60.253, mean reward: 0.603 [0.509, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.098], loss: 0.001525, mae: 0.042436, mean_q: 1.162706
 171695/1000000: episode: 1717, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 58.720, mean reward: 0.587 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.309, 10.185], loss: 0.001422, mae: 0.041508, mean_q: 1.163004
 171795/1000000: episode: 1718, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 58.322, mean reward: 0.583 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.188], loss: 0.001438, mae: 0.041735, mean_q: 1.164932
 171895/1000000: episode: 1719, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 62.685, mean reward: 0.627 [0.515, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.862, 10.098], loss: 0.001359, mae: 0.039799, mean_q: 1.166231
 171995/1000000: episode: 1720, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 61.873, mean reward: 0.619 [0.499, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.229, 10.439], loss: 0.001406, mae: 0.040505, mean_q: 1.164941
 172095/1000000: episode: 1721, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 60.302, mean reward: 0.603 [0.524, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.176, 10.098], loss: 0.001455, mae: 0.041527, mean_q: 1.166355
 172195/1000000: episode: 1722, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 61.567, mean reward: 0.616 [0.502, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.291, 10.098], loss: 0.001426, mae: 0.041136, mean_q: 1.167659
 172295/1000000: episode: 1723, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 57.664, mean reward: 0.577 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.632, 10.098], loss: 0.001523, mae: 0.042026, mean_q: 1.168279
 172395/1000000: episode: 1724, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 59.211, mean reward: 0.592 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.279, 10.274], loss: 0.001528, mae: 0.042484, mean_q: 1.169844
 172495/1000000: episode: 1725, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 58.437, mean reward: 0.584 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.052, 10.098], loss: 0.001529, mae: 0.042619, mean_q: 1.170023
 172595/1000000: episode: 1726, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 59.729, mean reward: 0.597 [0.511, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.416, 10.111], loss: 0.001403, mae: 0.040675, mean_q: 1.168027
 172695/1000000: episode: 1727, duration: 1.359s, episode steps: 100, steps per second: 74, episode reward: 58.467, mean reward: 0.585 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.345, 10.130], loss: 0.001551, mae: 0.042507, mean_q: 1.167233
 172795/1000000: episode: 1728, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 58.181, mean reward: 0.582 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.132, 10.122], loss: 0.001434, mae: 0.042156, mean_q: 1.169884
 172895/1000000: episode: 1729, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.890, mean reward: 0.589 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.304], loss: 0.001389, mae: 0.040184, mean_q: 1.167771
 172995/1000000: episode: 1730, duration: 1.376s, episode steps: 100, steps per second: 73, episode reward: 58.800, mean reward: 0.588 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.437, 10.268], loss: 0.001352, mae: 0.040097, mean_q: 1.168845
 173095/1000000: episode: 1731, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 58.456, mean reward: 0.585 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.088, 10.098], loss: 0.001404, mae: 0.040540, mean_q: 1.168702
 173195/1000000: episode: 1732, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 60.325, mean reward: 0.603 [0.510, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.574, 10.332], loss: 0.001408, mae: 0.040723, mean_q: 1.165898
 173295/1000000: episode: 1733, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 60.200, mean reward: 0.602 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.370, 10.129], loss: 0.001460, mae: 0.041735, mean_q: 1.169344
 173395/1000000: episode: 1734, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.120, mean reward: 0.581 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.880, 10.300], loss: 0.001429, mae: 0.041290, mean_q: 1.169868
 173495/1000000: episode: 1735, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 61.031, mean reward: 0.610 [0.517, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.364], loss: 0.001419, mae: 0.041039, mean_q: 1.168437
 173595/1000000: episode: 1736, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.342, mean reward: 0.593 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.322, 10.166], loss: 0.001506, mae: 0.042987, mean_q: 1.173732
 173695/1000000: episode: 1737, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.276, mean reward: 0.573 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.782, 10.180], loss: 0.001459, mae: 0.041463, mean_q: 1.171980
 173795/1000000: episode: 1738, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.685, mean reward: 0.587 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.007, 10.255], loss: 0.001491, mae: 0.041573, mean_q: 1.174114
 173895/1000000: episode: 1739, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.290, mean reward: 0.603 [0.512, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.445, 10.324], loss: 0.001550, mae: 0.042836, mean_q: 1.171985
 173995/1000000: episode: 1740, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 57.568, mean reward: 0.576 [0.511, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.762, 10.098], loss: 0.001460, mae: 0.041771, mean_q: 1.176400
 174095/1000000: episode: 1741, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: 58.577, mean reward: 0.586 [0.518, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.603, 10.098], loss: 0.001354, mae: 0.040020, mean_q: 1.170324
 174195/1000000: episode: 1742, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.090, mean reward: 0.581 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.942, 10.098], loss: 0.001326, mae: 0.040021, mean_q: 1.169838
 174295/1000000: episode: 1743, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.607, mean reward: 0.576 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.638, 10.098], loss: 0.001411, mae: 0.040625, mean_q: 1.170548
 174395/1000000: episode: 1744, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 60.704, mean reward: 0.607 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.427, 10.318], loss: 0.001544, mae: 0.042731, mean_q: 1.169294
 174495/1000000: episode: 1745, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.291, mean reward: 0.573 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.098], loss: 0.001428, mae: 0.041336, mean_q: 1.170918
 174595/1000000: episode: 1746, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.865, mean reward: 0.589 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.564, 10.098], loss: 0.001471, mae: 0.041711, mean_q: 1.169193
 174695/1000000: episode: 1747, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 56.569, mean reward: 0.566 [0.509, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.578, 10.098], loss: 0.001375, mae: 0.040170, mean_q: 1.164730
 174795/1000000: episode: 1748, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 57.818, mean reward: 0.578 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.868, 10.098], loss: 0.001400, mae: 0.040540, mean_q: 1.169973
 174895/1000000: episode: 1749, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 56.892, mean reward: 0.569 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.901, 10.259], loss: 0.001292, mae: 0.039494, mean_q: 1.166013
 174995/1000000: episode: 1750, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.489, mean reward: 0.575 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.236, 10.098], loss: 0.001359, mae: 0.040380, mean_q: 1.166196
 175095/1000000: episode: 1751, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 63.118, mean reward: 0.631 [0.520, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.173, 10.280], loss: 0.001401, mae: 0.041110, mean_q: 1.166059
 175195/1000000: episode: 1752, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 59.856, mean reward: 0.599 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.062, 10.138], loss: 0.001423, mae: 0.041156, mean_q: 1.163814
 175295/1000000: episode: 1753, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 59.001, mean reward: 0.590 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.467, 10.098], loss: 0.001465, mae: 0.042356, mean_q: 1.169801
 175395/1000000: episode: 1754, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 58.050, mean reward: 0.580 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.098], loss: 0.001559, mae: 0.042597, mean_q: 1.165588
 175495/1000000: episode: 1755, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.048, mean reward: 0.580 [0.506, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.715, 10.224], loss: 0.001315, mae: 0.039012, mean_q: 1.163544
 175595/1000000: episode: 1756, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 61.839, mean reward: 0.618 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.115, 10.441], loss: 0.001378, mae: 0.040715, mean_q: 1.164623
 175695/1000000: episode: 1757, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 65.920, mean reward: 0.659 [0.506, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.452, 10.098], loss: 0.001397, mae: 0.040962, mean_q: 1.167872
 175795/1000000: episode: 1758, duration: 1.450s, episode steps: 100, steps per second: 69, episode reward: 60.079, mean reward: 0.601 [0.510, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.586, 10.098], loss: 0.001424, mae: 0.040865, mean_q: 1.172967
 175895/1000000: episode: 1759, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.371, mean reward: 0.594 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.916, 10.098], loss: 0.001451, mae: 0.041988, mean_q: 1.170793
 175995/1000000: episode: 1760, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 56.892, mean reward: 0.569 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.236, 10.119], loss: 0.001454, mae: 0.041707, mean_q: 1.171861
 176095/1000000: episode: 1761, duration: 1.310s, episode steps: 100, steps per second: 76, episode reward: 58.119, mean reward: 0.581 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.257, 10.098], loss: 0.001431, mae: 0.041623, mean_q: 1.172081
 176195/1000000: episode: 1762, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.986, mean reward: 0.590 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.819, 10.098], loss: 0.001470, mae: 0.042053, mean_q: 1.170743
 176295/1000000: episode: 1763, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.440, mean reward: 0.604 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.269, 10.336], loss: 0.001422, mae: 0.041502, mean_q: 1.172159
 176395/1000000: episode: 1764, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 57.416, mean reward: 0.574 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.098], loss: 0.001429, mae: 0.041316, mean_q: 1.172955
 176495/1000000: episode: 1765, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.354, mean reward: 0.584 [0.508, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.720, 10.137], loss: 0.001405, mae: 0.041059, mean_q: 1.173372
 176595/1000000: episode: 1766, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.986, mean reward: 0.590 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.382, 10.338], loss: 0.001397, mae: 0.040943, mean_q: 1.171990
 176695/1000000: episode: 1767, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 58.904, mean reward: 0.589 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.181, 10.098], loss: 0.001413, mae: 0.040754, mean_q: 1.168513
 176795/1000000: episode: 1768, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 61.305, mean reward: 0.613 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.853, 10.098], loss: 0.001403, mae: 0.040551, mean_q: 1.169448
 176895/1000000: episode: 1769, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 61.490, mean reward: 0.615 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.550, 10.098], loss: 0.001323, mae: 0.039946, mean_q: 1.171640
 176995/1000000: episode: 1770, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 57.686, mean reward: 0.577 [0.499, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.156], loss: 0.001416, mae: 0.041085, mean_q: 1.169485
 177095/1000000: episode: 1771, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 65.739, mean reward: 0.657 [0.512, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.884, 10.098], loss: 0.001467, mae: 0.041603, mean_q: 1.167670
 177195/1000000: episode: 1772, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 56.764, mean reward: 0.568 [0.513, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.757, 10.098], loss: 0.001388, mae: 0.040705, mean_q: 1.171750
 177295/1000000: episode: 1773, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.493, mean reward: 0.595 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.649, 10.098], loss: 0.001386, mae: 0.040322, mean_q: 1.167845
 177395/1000000: episode: 1774, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.709, mean reward: 0.597 [0.519, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.904, 10.098], loss: 0.001457, mae: 0.041874, mean_q: 1.170003
 177495/1000000: episode: 1775, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 62.611, mean reward: 0.626 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.838, 10.098], loss: 0.001468, mae: 0.041607, mean_q: 1.167107
 177595/1000000: episode: 1776, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.581, mean reward: 0.586 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.619, 10.098], loss: 0.001474, mae: 0.041857, mean_q: 1.169244
 177695/1000000: episode: 1777, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.507, mean reward: 0.595 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.996, 10.098], loss: 0.001468, mae: 0.041988, mean_q: 1.169496
 177795/1000000: episode: 1778, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 59.881, mean reward: 0.599 [0.518, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.521, 10.193], loss: 0.001493, mae: 0.042630, mean_q: 1.169491
 177895/1000000: episode: 1779, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: 59.519, mean reward: 0.595 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.093, 10.133], loss: 0.001526, mae: 0.042809, mean_q: 1.172805
 177995/1000000: episode: 1780, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.569, mean reward: 0.576 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.098], loss: 0.001519, mae: 0.043088, mean_q: 1.174252
 178095/1000000: episode: 1781, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 60.802, mean reward: 0.608 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.364, 10.098], loss: 0.001391, mae: 0.041134, mean_q: 1.172166
 178195/1000000: episode: 1782, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: 57.401, mean reward: 0.574 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.898, 10.274], loss: 0.001430, mae: 0.041783, mean_q: 1.174167
 178295/1000000: episode: 1783, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: 56.514, mean reward: 0.565 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.578, 10.098], loss: 0.001419, mae: 0.041107, mean_q: 1.174989
 178395/1000000: episode: 1784, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: 58.293, mean reward: 0.583 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.098], loss: 0.001424, mae: 0.041337, mean_q: 1.171932
 178495/1000000: episode: 1785, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 60.439, mean reward: 0.604 [0.513, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.283, 10.195], loss: 0.001465, mae: 0.042233, mean_q: 1.173192
 178595/1000000: episode: 1786, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.223, mean reward: 0.582 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.098], loss: 0.001502, mae: 0.042389, mean_q: 1.172238
 178695/1000000: episode: 1787, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 57.055, mean reward: 0.571 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.199, 10.291], loss: 0.001539, mae: 0.043337, mean_q: 1.171814
 178795/1000000: episode: 1788, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 57.351, mean reward: 0.574 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.156], loss: 0.001420, mae: 0.041541, mean_q: 1.170877
 178895/1000000: episode: 1789, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 58.655, mean reward: 0.587 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.189, 10.098], loss: 0.001539, mae: 0.043185, mean_q: 1.170677
 178995/1000000: episode: 1790, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 59.798, mean reward: 0.598 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.894, 10.208], loss: 0.001346, mae: 0.040737, mean_q: 1.170097
 179095/1000000: episode: 1791, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 60.137, mean reward: 0.601 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.070, 10.098], loss: 0.001425, mae: 0.041563, mean_q: 1.170680
 179195/1000000: episode: 1792, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.041, mean reward: 0.590 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.313, 10.287], loss: 0.001393, mae: 0.041152, mean_q: 1.169209
 179295/1000000: episode: 1793, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: 57.296, mean reward: 0.573 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.789, 10.098], loss: 0.001456, mae: 0.041827, mean_q: 1.173140
 179395/1000000: episode: 1794, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 59.557, mean reward: 0.596 [0.508, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.713, 10.148], loss: 0.001466, mae: 0.041812, mean_q: 1.171532
 179495/1000000: episode: 1795, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 60.436, mean reward: 0.604 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.054, 10.346], loss: 0.001463, mae: 0.042154, mean_q: 1.173756
 179595/1000000: episode: 1796, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 61.233, mean reward: 0.612 [0.512, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.345, 10.098], loss: 0.001601, mae: 0.044253, mean_q: 1.171503
 179695/1000000: episode: 1797, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 58.419, mean reward: 0.584 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.963, 10.098], loss: 0.001491, mae: 0.042187, mean_q: 1.173513
 179795/1000000: episode: 1798, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 59.669, mean reward: 0.597 [0.503, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.986, 10.098], loss: 0.001589, mae: 0.042555, mean_q: 1.176158
 179895/1000000: episode: 1799, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 58.196, mean reward: 0.582 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.517, 10.176], loss: 0.001720, mae: 0.044827, mean_q: 1.174310
 179995/1000000: episode: 1800, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 60.849, mean reward: 0.608 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.889, 10.308], loss: 0.001708, mae: 0.044476, mean_q: 1.176273
 180095/1000000: episode: 1801, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 58.472, mean reward: 0.585 [0.506, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.608, 10.172], loss: 0.001680, mae: 0.044221, mean_q: 1.173645
 180195/1000000: episode: 1802, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.136, mean reward: 0.581 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.427, 10.111], loss: 0.001610, mae: 0.043556, mean_q: 1.174564
 180295/1000000: episode: 1803, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 61.468, mean reward: 0.615 [0.512, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.462, 10.098], loss: 0.001577, mae: 0.042413, mean_q: 1.174946
 180395/1000000: episode: 1804, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.915, mean reward: 0.579 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.680, 10.309], loss: 0.001452, mae: 0.041165, mean_q: 1.171950
 180495/1000000: episode: 1805, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.688, mean reward: 0.577 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.308, 10.098], loss: 0.001437, mae: 0.041245, mean_q: 1.175050
 180595/1000000: episode: 1806, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 58.204, mean reward: 0.582 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.930, 10.104], loss: 0.001420, mae: 0.041110, mean_q: 1.170569
 180695/1000000: episode: 1807, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 60.934, mean reward: 0.609 [0.522, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.941, 10.098], loss: 0.001557, mae: 0.042779, mean_q: 1.170108
 180795/1000000: episode: 1808, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 58.206, mean reward: 0.582 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.979, 10.106], loss: 0.001584, mae: 0.042468, mean_q: 1.171403
 180895/1000000: episode: 1809, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.963, mean reward: 0.600 [0.509, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.721, 10.234], loss: 0.001575, mae: 0.042567, mean_q: 1.168139
 180995/1000000: episode: 1810, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 65.239, mean reward: 0.652 [0.510, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.599, 10.098], loss: 0.001505, mae: 0.041192, mean_q: 1.170445
 181095/1000000: episode: 1811, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.729, mean reward: 0.587 [0.510, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.775, 10.098], loss: 0.001501, mae: 0.041841, mean_q: 1.169139
 181195/1000000: episode: 1812, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 60.737, mean reward: 0.607 [0.516, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.255, 10.098], loss: 0.001658, mae: 0.043187, mean_q: 1.171507
 181295/1000000: episode: 1813, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 61.307, mean reward: 0.613 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.289], loss: 0.001474, mae: 0.041769, mean_q: 1.174115
 181395/1000000: episode: 1814, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 57.722, mean reward: 0.577 [0.500, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.167, 10.098], loss: 0.001558, mae: 0.043089, mean_q: 1.173479
 181495/1000000: episode: 1815, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 58.792, mean reward: 0.588 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.042, 10.098], loss: 0.001594, mae: 0.042840, mean_q: 1.176521
 181595/1000000: episode: 1816, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.355, mean reward: 0.584 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.768, 10.364], loss: 0.001595, mae: 0.043302, mean_q: 1.174445
 181695/1000000: episode: 1817, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.302, mean reward: 0.573 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.945, 10.098], loss: 0.001667, mae: 0.043948, mean_q: 1.173073
 181795/1000000: episode: 1818, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.395, mean reward: 0.574 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.205, 10.206], loss: 0.001551, mae: 0.042772, mean_q: 1.170620
 181895/1000000: episode: 1819, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 58.248, mean reward: 0.582 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.743, 10.098], loss: 0.001564, mae: 0.043247, mean_q: 1.169641
 181995/1000000: episode: 1820, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 59.811, mean reward: 0.598 [0.512, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.998, 10.098], loss: 0.001743, mae: 0.045345, mean_q: 1.170262
 182095/1000000: episode: 1821, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 57.411, mean reward: 0.574 [0.504, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.076, 10.170], loss: 0.001544, mae: 0.042986, mean_q: 1.171701
 182195/1000000: episode: 1822, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 59.563, mean reward: 0.596 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.196, 10.098], loss: 0.001581, mae: 0.043317, mean_q: 1.168582
 182295/1000000: episode: 1823, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.815, mean reward: 0.598 [0.512, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.796, 10.407], loss: 0.001660, mae: 0.043953, mean_q: 1.171865
 182395/1000000: episode: 1824, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 64.280, mean reward: 0.643 [0.515, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.762, 10.098], loss: 0.001554, mae: 0.042773, mean_q: 1.170515
 182495/1000000: episode: 1825, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 57.292, mean reward: 0.573 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.158], loss: 0.001533, mae: 0.042668, mean_q: 1.167568
 182595/1000000: episode: 1826, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.738, mean reward: 0.597 [0.509, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.387, 10.368], loss: 0.001554, mae: 0.042742, mean_q: 1.168277
 182695/1000000: episode: 1827, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 60.732, mean reward: 0.607 [0.529, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.067, 10.098], loss: 0.001527, mae: 0.042425, mean_q: 1.169790
 182795/1000000: episode: 1828, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 62.052, mean reward: 0.621 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.664, 10.154], loss: 0.001514, mae: 0.042294, mean_q: 1.171951
 182895/1000000: episode: 1829, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.574, mean reward: 0.586 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.653, 10.228], loss: 0.001594, mae: 0.043635, mean_q: 1.175455
 182995/1000000: episode: 1830, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.534, mean reward: 0.585 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.509, 10.098], loss: 0.001467, mae: 0.041076, mean_q: 1.173210
 183095/1000000: episode: 1831, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 59.100, mean reward: 0.591 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.427, 10.098], loss: 0.001452, mae: 0.041204, mean_q: 1.172578
 183195/1000000: episode: 1832, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 57.175, mean reward: 0.572 [0.499, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.285, 10.098], loss: 0.001668, mae: 0.044010, mean_q: 1.177062
 183295/1000000: episode: 1833, duration: 1.635s, episode steps: 100, steps per second: 61, episode reward: 57.045, mean reward: 0.570 [0.505, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.537, 10.098], loss: 0.001526, mae: 0.042249, mean_q: 1.172378
 183395/1000000: episode: 1834, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.774, mean reward: 0.578 [0.499, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.174], loss: 0.001458, mae: 0.041426, mean_q: 1.172845
 183495/1000000: episode: 1835, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 56.345, mean reward: 0.563 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.382, 10.098], loss: 0.001516, mae: 0.041943, mean_q: 1.171607
 183595/1000000: episode: 1836, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.660, mean reward: 0.597 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.567, 10.098], loss: 0.001625, mae: 0.042914, mean_q: 1.169085
 183695/1000000: episode: 1837, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 58.961, mean reward: 0.590 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.403, 10.233], loss: 0.001598, mae: 0.042655, mean_q: 1.172100
 183795/1000000: episode: 1838, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 60.563, mean reward: 0.606 [0.509, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.781, 10.276], loss: 0.001564, mae: 0.042261, mean_q: 1.167220
 183895/1000000: episode: 1839, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 58.773, mean reward: 0.588 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.873, 10.294], loss: 0.001569, mae: 0.042459, mean_q: 1.171630
 183995/1000000: episode: 1840, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 58.145, mean reward: 0.581 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.098], loss: 0.001538, mae: 0.042253, mean_q: 1.171159
 184095/1000000: episode: 1841, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 59.371, mean reward: 0.594 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.731, 10.098], loss: 0.001595, mae: 0.043113, mean_q: 1.171487
 184195/1000000: episode: 1842, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.173, mean reward: 0.582 [0.498, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.295, 10.219], loss: 0.001555, mae: 0.042536, mean_q: 1.173663
 184295/1000000: episode: 1843, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 56.828, mean reward: 0.568 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.508, 10.158], loss: 0.001460, mae: 0.041115, mean_q: 1.168958
 184395/1000000: episode: 1844, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.089, mean reward: 0.581 [0.500, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.749, 10.319], loss: 0.001438, mae: 0.040520, mean_q: 1.169788
 184495/1000000: episode: 1845, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 59.084, mean reward: 0.591 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.147], loss: 0.001539, mae: 0.042049, mean_q: 1.171947
 184595/1000000: episode: 1846, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 60.677, mean reward: 0.607 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.909, 10.269], loss: 0.001511, mae: 0.042075, mean_q: 1.168373
 184695/1000000: episode: 1847, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.785, mean reward: 0.578 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.481, 10.098], loss: 0.001474, mae: 0.041011, mean_q: 1.169236
 184795/1000000: episode: 1848, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 59.797, mean reward: 0.598 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.921, 10.098], loss: 0.001518, mae: 0.042087, mean_q: 1.172004
 184895/1000000: episode: 1849, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.750, mean reward: 0.577 [0.497, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.122], loss: 0.001457, mae: 0.041637, mean_q: 1.170882
 184995/1000000: episode: 1850, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.429, mean reward: 0.584 [0.511, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.162], loss: 0.001542, mae: 0.041675, mean_q: 1.170047
 185095/1000000: episode: 1851, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 59.027, mean reward: 0.590 [0.509, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.034, 10.098], loss: 0.001605, mae: 0.043379, mean_q: 1.166499
 185195/1000000: episode: 1852, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 60.167, mean reward: 0.602 [0.498, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.600, 10.391], loss: 0.001497, mae: 0.041819, mean_q: 1.167623
 185295/1000000: episode: 1853, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 57.634, mean reward: 0.576 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.593, 10.131], loss: 0.001429, mae: 0.041132, mean_q: 1.167173
 185395/1000000: episode: 1854, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.237, mean reward: 0.572 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.586, 10.127], loss: 0.001515, mae: 0.041584, mean_q: 1.167121
 185495/1000000: episode: 1855, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.939, mean reward: 0.579 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.150, 10.211], loss: 0.001459, mae: 0.041300, mean_q: 1.166029
 185595/1000000: episode: 1856, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 56.775, mean reward: 0.568 [0.509, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.375, 10.209], loss: 0.001497, mae: 0.041524, mean_q: 1.167161
 185695/1000000: episode: 1857, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.003, mean reward: 0.580 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.245, 10.202], loss: 0.001564, mae: 0.042535, mean_q: 1.166705
 185795/1000000: episode: 1858, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 57.985, mean reward: 0.580 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.336, 10.223], loss: 0.001670, mae: 0.042655, mean_q: 1.164008
 185895/1000000: episode: 1859, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 60.814, mean reward: 0.608 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.098], loss: 0.001422, mae: 0.041281, mean_q: 1.163049
 185995/1000000: episode: 1860, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 58.663, mean reward: 0.587 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.553, 10.098], loss: 0.001527, mae: 0.042229, mean_q: 1.162521
 186095/1000000: episode: 1861, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 61.135, mean reward: 0.611 [0.504, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.798, 10.098], loss: 0.001517, mae: 0.042228, mean_q: 1.162922
 186195/1000000: episode: 1862, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 56.471, mean reward: 0.565 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.163], loss: 0.001489, mae: 0.041509, mean_q: 1.166145
 186295/1000000: episode: 1863, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.020, mean reward: 0.590 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.199, 10.279], loss: 0.001364, mae: 0.039885, mean_q: 1.160312
 186395/1000000: episode: 1864, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 59.493, mean reward: 0.595 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.944, 10.103], loss: 0.001432, mae: 0.040736, mean_q: 1.162928
 186495/1000000: episode: 1865, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.627, mean reward: 0.606 [0.501, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.363], loss: 0.001354, mae: 0.040052, mean_q: 1.162176
 186595/1000000: episode: 1866, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.770, mean reward: 0.598 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.501, 10.185], loss: 0.001467, mae: 0.041429, mean_q: 1.161861
 186695/1000000: episode: 1867, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.435, mean reward: 0.584 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.562, 10.098], loss: 0.001432, mae: 0.040612, mean_q: 1.161327
 186795/1000000: episode: 1868, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 59.014, mean reward: 0.590 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.750, 10.239], loss: 0.001469, mae: 0.041457, mean_q: 1.166167
 186895/1000000: episode: 1869, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 59.024, mean reward: 0.590 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.292, 10.169], loss: 0.001386, mae: 0.040468, mean_q: 1.166615
 186995/1000000: episode: 1870, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 57.242, mean reward: 0.572 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.336, 10.461], loss: 0.001405, mae: 0.040452, mean_q: 1.164839
 187095/1000000: episode: 1871, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.723, mean reward: 0.587 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.363, 10.098], loss: 0.001393, mae: 0.040058, mean_q: 1.162826
 187195/1000000: episode: 1872, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.647, mean reward: 0.586 [0.505, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.790, 10.098], loss: 0.001445, mae: 0.041409, mean_q: 1.165653
 187295/1000000: episode: 1873, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.664, mean reward: 0.587 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.139], loss: 0.001459, mae: 0.040836, mean_q: 1.162988
 187395/1000000: episode: 1874, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 56.904, mean reward: 0.569 [0.497, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.450, 10.177], loss: 0.001427, mae: 0.040982, mean_q: 1.161821
 187495/1000000: episode: 1875, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.657, mean reward: 0.587 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.098], loss: 0.001493, mae: 0.041504, mean_q: 1.158992
 187595/1000000: episode: 1876, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 56.444, mean reward: 0.564 [0.508, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.884, 10.146], loss: 0.001435, mae: 0.040507, mean_q: 1.160608
 187695/1000000: episode: 1877, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 58.830, mean reward: 0.588 [0.504, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.547, 10.098], loss: 0.001498, mae: 0.042063, mean_q: 1.160705
 187795/1000000: episode: 1878, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 63.803, mean reward: 0.638 [0.521, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.169, 10.521], loss: 0.001488, mae: 0.041203, mean_q: 1.156458
 187895/1000000: episode: 1879, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 59.883, mean reward: 0.599 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.317, 10.098], loss: 0.001411, mae: 0.040969, mean_q: 1.159600
 187995/1000000: episode: 1880, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 59.906, mean reward: 0.599 [0.509, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.098], loss: 0.001552, mae: 0.042032, mean_q: 1.159741
 188095/1000000: episode: 1881, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 59.096, mean reward: 0.591 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.519, 10.098], loss: 0.001476, mae: 0.041703, mean_q: 1.160420
 188195/1000000: episode: 1882, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.347, mean reward: 0.583 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.996, 10.137], loss: 0.001542, mae: 0.041927, mean_q: 1.157039
 188295/1000000: episode: 1883, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 59.125, mean reward: 0.591 [0.498, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.281, 10.329], loss: 0.001438, mae: 0.041091, mean_q: 1.158485
 188395/1000000: episode: 1884, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 58.667, mean reward: 0.587 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.436, 10.098], loss: 0.001383, mae: 0.039973, mean_q: 1.159838
 188495/1000000: episode: 1885, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 59.453, mean reward: 0.595 [0.508, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.740, 10.098], loss: 0.001506, mae: 0.041874, mean_q: 1.163520
 188595/1000000: episode: 1886, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 57.306, mean reward: 0.573 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.071, 10.279], loss: 0.001445, mae: 0.041199, mean_q: 1.159384
 188695/1000000: episode: 1887, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 57.383, mean reward: 0.574 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.098], loss: 0.001387, mae: 0.040464, mean_q: 1.164173
 188795/1000000: episode: 1888, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 57.175, mean reward: 0.572 [0.509, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.131, 10.098], loss: 0.001389, mae: 0.040115, mean_q: 1.161014
 188895/1000000: episode: 1889, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 59.063, mean reward: 0.591 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.311, 10.098], loss: 0.001488, mae: 0.041617, mean_q: 1.161995
 188995/1000000: episode: 1890, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 59.098, mean reward: 0.591 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.086, 10.098], loss: 0.001476, mae: 0.041387, mean_q: 1.161775
 189095/1000000: episode: 1891, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 56.342, mean reward: 0.563 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.563, 10.178], loss: 0.001488, mae: 0.042473, mean_q: 1.161956
 189195/1000000: episode: 1892, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 56.780, mean reward: 0.568 [0.499, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.869, 10.204], loss: 0.001430, mae: 0.041194, mean_q: 1.160979
 189295/1000000: episode: 1893, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 58.178, mean reward: 0.582 [0.507, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.518, 10.098], loss: 0.001397, mae: 0.040385, mean_q: 1.161989
 189395/1000000: episode: 1894, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.423, mean reward: 0.574 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.551, 10.121], loss: 0.001481, mae: 0.041123, mean_q: 1.163648
 189495/1000000: episode: 1895, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 68.576, mean reward: 0.686 [0.520, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.559, 10.410], loss: 0.001374, mae: 0.040329, mean_q: 1.160734
 189595/1000000: episode: 1896, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.220, mean reward: 0.572 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.317, 10.098], loss: 0.001400, mae: 0.041043, mean_q: 1.164048
 189695/1000000: episode: 1897, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.873, mean reward: 0.579 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.815, 10.098], loss: 0.001498, mae: 0.041827, mean_q: 1.162569
 189795/1000000: episode: 1898, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 62.561, mean reward: 0.626 [0.511, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.356, 10.098], loss: 0.001491, mae: 0.041702, mean_q: 1.164328
 189895/1000000: episode: 1899, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 61.917, mean reward: 0.619 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.842, 10.098], loss: 0.001348, mae: 0.039706, mean_q: 1.161328
 189995/1000000: episode: 1900, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 62.272, mean reward: 0.623 [0.500, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.098], loss: 0.001438, mae: 0.041010, mean_q: 1.166068
 190095/1000000: episode: 1901, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 57.780, mean reward: 0.578 [0.503, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.098], loss: 0.001450, mae: 0.041127, mean_q: 1.169612
 190195/1000000: episode: 1902, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.050, mean reward: 0.580 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.945, 10.099], loss: 0.001408, mae: 0.040109, mean_q: 1.167001
 190295/1000000: episode: 1903, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.550, mean reward: 0.605 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.626, 10.225], loss: 0.001437, mae: 0.040669, mean_q: 1.161405
 190395/1000000: episode: 1904, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.949, mean reward: 0.589 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.773, 10.162], loss: 0.001488, mae: 0.041222, mean_q: 1.166615
 190495/1000000: episode: 1905, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 57.125, mean reward: 0.571 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.098], loss: 0.001654, mae: 0.043596, mean_q: 1.164387
 190595/1000000: episode: 1906, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 63.213, mean reward: 0.632 [0.500, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.642, 10.098], loss: 0.001467, mae: 0.041608, mean_q: 1.163838
 190695/1000000: episode: 1907, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 58.137, mean reward: 0.581 [0.500, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.013, 10.098], loss: 0.001558, mae: 0.042721, mean_q: 1.172240
 190795/1000000: episode: 1908, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 64.911, mean reward: 0.649 [0.516, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.319], loss: 0.001508, mae: 0.042220, mean_q: 1.169170
 190895/1000000: episode: 1909, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 59.306, mean reward: 0.593 [0.512, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.763, 10.285], loss: 0.001556, mae: 0.042604, mean_q: 1.169191
 190995/1000000: episode: 1910, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 62.911, mean reward: 0.629 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.606, 10.098], loss: 0.001417, mae: 0.040748, mean_q: 1.172529
 191095/1000000: episode: 1911, duration: 1.326s, episode steps: 100, steps per second: 75, episode reward: 60.090, mean reward: 0.601 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.450, 10.116], loss: 0.001439, mae: 0.041683, mean_q: 1.172371
 191195/1000000: episode: 1912, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.148, mean reward: 0.581 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.697, 10.395], loss: 0.001446, mae: 0.040847, mean_q: 1.172620
 191295/1000000: episode: 1913, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 60.127, mean reward: 0.601 [0.505, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.098], loss: 0.001534, mae: 0.042605, mean_q: 1.173978
 191395/1000000: episode: 1914, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 57.544, mean reward: 0.575 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.190, 10.098], loss: 0.001472, mae: 0.041663, mean_q: 1.175326
 191495/1000000: episode: 1915, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 62.091, mean reward: 0.621 [0.508, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.104], loss: 0.001446, mae: 0.041857, mean_q: 1.176295
 191595/1000000: episode: 1916, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 58.238, mean reward: 0.582 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.138, 10.297], loss: 0.001432, mae: 0.041147, mean_q: 1.172388
 191695/1000000: episode: 1917, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.123, mean reward: 0.581 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.218, 10.352], loss: 0.001509, mae: 0.042052, mean_q: 1.171868
 191795/1000000: episode: 1918, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 60.661, mean reward: 0.607 [0.511, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.273], loss: 0.001475, mae: 0.041525, mean_q: 1.172156
 191895/1000000: episode: 1919, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 59.822, mean reward: 0.598 [0.497, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.098], loss: 0.001504, mae: 0.042247, mean_q: 1.172251
 191995/1000000: episode: 1920, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 59.645, mean reward: 0.596 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.218, 10.249], loss: 0.001541, mae: 0.042893, mean_q: 1.177795
 192095/1000000: episode: 1921, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 57.997, mean reward: 0.580 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.254], loss: 0.001481, mae: 0.042401, mean_q: 1.172768
 192195/1000000: episode: 1922, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 58.544, mean reward: 0.585 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.122], loss: 0.001574, mae: 0.043101, mean_q: 1.173297
 192295/1000000: episode: 1923, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 60.002, mean reward: 0.600 [0.499, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.732, 10.098], loss: 0.001584, mae: 0.043704, mean_q: 1.173879
 192395/1000000: episode: 1924, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 59.693, mean reward: 0.597 [0.499, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.817, 10.371], loss: 0.001456, mae: 0.041418, mean_q: 1.173810
 192495/1000000: episode: 1925, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 56.799, mean reward: 0.568 [0.506, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.857, 10.103], loss: 0.001538, mae: 0.042614, mean_q: 1.177472
 192595/1000000: episode: 1926, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 63.879, mean reward: 0.639 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.481, 10.421], loss: 0.001636, mae: 0.044590, mean_q: 1.177972
 192695/1000000: episode: 1927, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.942, mean reward: 0.589 [0.516, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.894, 10.264], loss: 0.001607, mae: 0.043738, mean_q: 1.180571
 192795/1000000: episode: 1928, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 57.341, mean reward: 0.573 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.254, 10.280], loss: 0.001564, mae: 0.042855, mean_q: 1.174692
 192895/1000000: episode: 1929, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 57.652, mean reward: 0.577 [0.511, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.661, 10.098], loss: 0.001479, mae: 0.041951, mean_q: 1.176459
 192995/1000000: episode: 1930, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 59.843, mean reward: 0.598 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.807, 10.371], loss: 0.001394, mae: 0.040492, mean_q: 1.171949
 193095/1000000: episode: 1931, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 59.533, mean reward: 0.595 [0.514, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.714, 10.269], loss: 0.001520, mae: 0.042022, mean_q: 1.173883
 193195/1000000: episode: 1932, duration: 1.274s, episode steps: 100, steps per second: 78, episode reward: 57.826, mean reward: 0.578 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.946, 10.267], loss: 0.001450, mae: 0.041577, mean_q: 1.174643
 193295/1000000: episode: 1933, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 65.078, mean reward: 0.651 [0.512, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.098], loss: 0.001520, mae: 0.042695, mean_q: 1.175180
 193395/1000000: episode: 1934, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.947, mean reward: 0.589 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.442, 10.115], loss: 0.001464, mae: 0.042417, mean_q: 1.178745
 193495/1000000: episode: 1935, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 60.218, mean reward: 0.602 [0.525, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.098], loss: 0.001427, mae: 0.041218, mean_q: 1.172401
 193595/1000000: episode: 1936, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 63.975, mean reward: 0.640 [0.503, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.989, 10.309], loss: 0.001413, mae: 0.041462, mean_q: 1.180286
 193695/1000000: episode: 1937, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 59.556, mean reward: 0.596 [0.507, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.625, 10.098], loss: 0.001448, mae: 0.041825, mean_q: 1.180509
 193795/1000000: episode: 1938, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 60.680, mean reward: 0.607 [0.520, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.573, 10.332], loss: 0.001383, mae: 0.040769, mean_q: 1.180864
 193895/1000000: episode: 1939, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 56.183, mean reward: 0.562 [0.503, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.186], loss: 0.001455, mae: 0.041584, mean_q: 1.178938
 193995/1000000: episode: 1940, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.457, mean reward: 0.575 [0.512, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.205, 10.098], loss: 0.001352, mae: 0.041108, mean_q: 1.180168
 194095/1000000: episode: 1941, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 57.701, mean reward: 0.577 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.761, 10.283], loss: 0.001411, mae: 0.041678, mean_q: 1.178362
 194195/1000000: episode: 1942, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 60.041, mean reward: 0.600 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.693, 10.320], loss: 0.001455, mae: 0.042021, mean_q: 1.182407
 194295/1000000: episode: 1943, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 60.738, mean reward: 0.607 [0.525, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.083, 10.348], loss: 0.001589, mae: 0.043610, mean_q: 1.180146
 194395/1000000: episode: 1944, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 56.468, mean reward: 0.565 [0.507, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.316, 10.270], loss: 0.001528, mae: 0.042340, mean_q: 1.182810
 194495/1000000: episode: 1945, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 59.025, mean reward: 0.590 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.433, 10.171], loss: 0.001623, mae: 0.044014, mean_q: 1.184321
 194595/1000000: episode: 1946, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.451, mean reward: 0.585 [0.500, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.652, 10.221], loss: 0.001476, mae: 0.042364, mean_q: 1.179088
 194695/1000000: episode: 1947, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.135, mean reward: 0.591 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.378, 10.186], loss: 0.001474, mae: 0.041601, mean_q: 1.179424
 194795/1000000: episode: 1948, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.474, mean reward: 0.585 [0.516, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.887, 10.185], loss: 0.001485, mae: 0.042237, mean_q: 1.177838
 194895/1000000: episode: 1949, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.107, mean reward: 0.571 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.548, 10.219], loss: 0.001538, mae: 0.042935, mean_q: 1.176654
 194995/1000000: episode: 1950, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.460, mean reward: 0.585 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.193, 10.098], loss: 0.001474, mae: 0.042334, mean_q: 1.179085
 195095/1000000: episode: 1951, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 57.666, mean reward: 0.577 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.270, 10.242], loss: 0.001488, mae: 0.041843, mean_q: 1.176653
 195195/1000000: episode: 1952, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 57.984, mean reward: 0.580 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.483, 10.098], loss: 0.001468, mae: 0.042463, mean_q: 1.175869
 195295/1000000: episode: 1953, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.226, mean reward: 0.582 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.482, 10.098], loss: 0.001480, mae: 0.042022, mean_q: 1.174778
 195395/1000000: episode: 1954, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 60.706, mean reward: 0.607 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.032, 10.098], loss: 0.001434, mae: 0.041855, mean_q: 1.172072
 195495/1000000: episode: 1955, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 62.197, mean reward: 0.622 [0.525, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.098], loss: 0.001408, mae: 0.041573, mean_q: 1.175919
 195595/1000000: episode: 1956, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 57.296, mean reward: 0.573 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.190, 10.248], loss: 0.001435, mae: 0.041185, mean_q: 1.174497
 195695/1000000: episode: 1957, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.829, mean reward: 0.588 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.704, 10.142], loss: 0.001448, mae: 0.041997, mean_q: 1.176686
 195795/1000000: episode: 1958, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 56.118, mean reward: 0.561 [0.504, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.397, 10.098], loss: 0.001449, mae: 0.041682, mean_q: 1.174685
 195895/1000000: episode: 1959, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.699, mean reward: 0.577 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.099, 10.098], loss: 0.001515, mae: 0.042923, mean_q: 1.174616
 195995/1000000: episode: 1960, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 59.701, mean reward: 0.597 [0.508, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.544, 10.105], loss: 0.001479, mae: 0.042081, mean_q: 1.167311
 196095/1000000: episode: 1961, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.082, mean reward: 0.581 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.310, 10.297], loss: 0.001361, mae: 0.040860, mean_q: 1.168858
 196195/1000000: episode: 1962, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 57.313, mean reward: 0.573 [0.505, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.746, 10.236], loss: 0.001455, mae: 0.041971, mean_q: 1.171054
 196295/1000000: episode: 1963, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 58.427, mean reward: 0.584 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.042, 10.098], loss: 0.001388, mae: 0.040991, mean_q: 1.169850
 196395/1000000: episode: 1964, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.595, mean reward: 0.576 [0.508, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.198, 10.098], loss: 0.001407, mae: 0.041067, mean_q: 1.167493
 196495/1000000: episode: 1965, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.866, mean reward: 0.589 [0.497, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.195, 10.124], loss: 0.001390, mae: 0.040762, mean_q: 1.169989
 196595/1000000: episode: 1966, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.397, mean reward: 0.594 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.119, 10.203], loss: 0.001478, mae: 0.041935, mean_q: 1.167043
 196695/1000000: episode: 1967, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.834, mean reward: 0.598 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.831, 10.098], loss: 0.001444, mae: 0.041445, mean_q: 1.164320
 196795/1000000: episode: 1968, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.817, mean reward: 0.598 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.690, 10.330], loss: 0.001418, mae: 0.041141, mean_q: 1.166114
 196895/1000000: episode: 1969, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 60.577, mean reward: 0.606 [0.506, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.699, 10.098], loss: 0.001358, mae: 0.040439, mean_q: 1.163706
 196995/1000000: episode: 1970, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 56.840, mean reward: 0.568 [0.501, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.475, 10.098], loss: 0.001304, mae: 0.039597, mean_q: 1.168691
 197095/1000000: episode: 1971, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 56.541, mean reward: 0.565 [0.508, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.514, 10.098], loss: 0.001364, mae: 0.040340, mean_q: 1.163032
 197195/1000000: episode: 1972, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.150, mean reward: 0.572 [0.502, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.959, 10.112], loss: 0.001344, mae: 0.039794, mean_q: 1.165208
 197295/1000000: episode: 1973, duration: 1.273s, episode steps: 100, steps per second: 79, episode reward: 59.034, mean reward: 0.590 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.400, 10.140], loss: 0.001353, mae: 0.040154, mean_q: 1.166614
 197395/1000000: episode: 1974, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 57.926, mean reward: 0.579 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.692, 10.100], loss: 0.001312, mae: 0.039886, mean_q: 1.163581
 197495/1000000: episode: 1975, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 55.813, mean reward: 0.558 [0.503, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.607, 10.150], loss: 0.001322, mae: 0.039924, mean_q: 1.164826
 197595/1000000: episode: 1976, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 59.629, mean reward: 0.596 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.185, 10.156], loss: 0.001304, mae: 0.039551, mean_q: 1.161061
 197695/1000000: episode: 1977, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 56.791, mean reward: 0.568 [0.503, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.768, 10.098], loss: 0.001306, mae: 0.039716, mean_q: 1.163838
 197795/1000000: episode: 1978, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 58.070, mean reward: 0.581 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.603, 10.110], loss: 0.001282, mae: 0.039451, mean_q: 1.163021
 197895/1000000: episode: 1979, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.986, mean reward: 0.590 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.026, 10.249], loss: 0.001328, mae: 0.040118, mean_q: 1.163351
 197995/1000000: episode: 1980, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 58.340, mean reward: 0.583 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.734, 10.192], loss: 0.001344, mae: 0.040435, mean_q: 1.162041
 198095/1000000: episode: 1981, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.232, mean reward: 0.582 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.657, 10.098], loss: 0.001410, mae: 0.041024, mean_q: 1.160809
 198195/1000000: episode: 1982, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 56.424, mean reward: 0.564 [0.504, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.017, 10.133], loss: 0.001313, mae: 0.039606, mean_q: 1.162931
 198295/1000000: episode: 1983, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 56.837, mean reward: 0.568 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.098], loss: 0.001307, mae: 0.039767, mean_q: 1.161175
 198395/1000000: episode: 1984, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 59.554, mean reward: 0.596 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.836, 10.124], loss: 0.001327, mae: 0.040086, mean_q: 1.156982
 198495/1000000: episode: 1985, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 61.177, mean reward: 0.612 [0.516, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.882, 10.098], loss: 0.001379, mae: 0.040398, mean_q: 1.157544
 198595/1000000: episode: 1986, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.007, mean reward: 0.590 [0.517, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.123], loss: 0.001330, mae: 0.040426, mean_q: 1.154617
 198695/1000000: episode: 1987, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.971, mean reward: 0.590 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.196, 10.346], loss: 0.001231, mae: 0.038573, mean_q: 1.156357
 198795/1000000: episode: 1988, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.131, mean reward: 0.591 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.782, 10.147], loss: 0.001282, mae: 0.038947, mean_q: 1.157029
 198895/1000000: episode: 1989, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.359, mean reward: 0.594 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.060, 10.232], loss: 0.001372, mae: 0.040390, mean_q: 1.153054
 198995/1000000: episode: 1990, duration: 1.047s, episode steps: 100, steps per second: 95, episode reward: 59.324, mean reward: 0.593 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.720, 10.098], loss: 0.001279, mae: 0.039483, mean_q: 1.154606
 199095/1000000: episode: 1991, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.008, mean reward: 0.600 [0.508, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.772, 10.380], loss: 0.001313, mae: 0.039457, mean_q: 1.156867
 199195/1000000: episode: 1992, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 57.401, mean reward: 0.574 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.696, 10.098], loss: 0.001306, mae: 0.039948, mean_q: 1.157388
 199295/1000000: episode: 1993, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 57.264, mean reward: 0.573 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.980, 10.098], loss: 0.001322, mae: 0.039761, mean_q: 1.153280
 199395/1000000: episode: 1994, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 59.939, mean reward: 0.599 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.420, 10.103], loss: 0.001350, mae: 0.039837, mean_q: 1.156696
 199495/1000000: episode: 1995, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 60.175, mean reward: 0.602 [0.513, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.320, 10.244], loss: 0.001408, mae: 0.040923, mean_q: 1.156988
 199595/1000000: episode: 1996, duration: 0.755s, episode steps: 100, steps per second: 133, episode reward: 63.759, mean reward: 0.638 [0.502, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.522, 10.110], loss: 0.001349, mae: 0.039705, mean_q: 1.159513
 199695/1000000: episode: 1997, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 61.390, mean reward: 0.614 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.305, 10.364], loss: 0.001343, mae: 0.039867, mean_q: 1.159113
 199795/1000000: episode: 1998, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 55.841, mean reward: 0.558 [0.511, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.098], loss: 0.001413, mae: 0.040959, mean_q: 1.160429
 199895/1000000: episode: 1999, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.531, mean reward: 0.585 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.443, 10.098], loss: 0.001404, mae: 0.040660, mean_q: 1.162041
 199995/1000000: episode: 2000, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.967, mean reward: 0.580 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.683, 10.173], loss: 0.001403, mae: 0.040956, mean_q: 1.162729
 200095/1000000: episode: 2001, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.566, mean reward: 0.576 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.409, 10.204], loss: 0.001321, mae: 0.039638, mean_q: 1.159659
 200195/1000000: episode: 2002, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.066, mean reward: 0.581 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.377, 10.098], loss: 0.001305, mae: 0.039874, mean_q: 1.160999
 200295/1000000: episode: 2003, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 56.861, mean reward: 0.569 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.946, 10.098], loss: 0.001316, mae: 0.039629, mean_q: 1.160564
 200395/1000000: episode: 2004, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 59.600, mean reward: 0.596 [0.512, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.975, 10.098], loss: 0.001363, mae: 0.040634, mean_q: 1.157418
 200495/1000000: episode: 2005, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 61.594, mean reward: 0.616 [0.512, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.307], loss: 0.001317, mae: 0.039811, mean_q: 1.158031
 200595/1000000: episode: 2006, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.065, mean reward: 0.591 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.098], loss: 0.001370, mae: 0.040249, mean_q: 1.156877
 200695/1000000: episode: 2007, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 63.094, mean reward: 0.631 [0.510, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.886, 10.098], loss: 0.001347, mae: 0.040080, mean_q: 1.159381
 200795/1000000: episode: 2008, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 63.384, mean reward: 0.634 [0.514, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.689, 10.098], loss: 0.001395, mae: 0.041206, mean_q: 1.160972
 200895/1000000: episode: 2009, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 57.923, mean reward: 0.579 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.218, 10.098], loss: 0.001313, mae: 0.039864, mean_q: 1.164962
 200995/1000000: episode: 2010, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.555, mean reward: 0.576 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.567, 10.154], loss: 0.001345, mae: 0.040602, mean_q: 1.159847
 201095/1000000: episode: 2011, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.987, mean reward: 0.600 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.981, 10.284], loss: 0.001370, mae: 0.040686, mean_q: 1.164374
 201195/1000000: episode: 2012, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.392, mean reward: 0.584 [0.509, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 0.001384, mae: 0.040728, mean_q: 1.163183
 201295/1000000: episode: 2013, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 56.677, mean reward: 0.567 [0.500, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.640, 10.231], loss: 0.001426, mae: 0.042048, mean_q: 1.163068
 201395/1000000: episode: 2014, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 61.647, mean reward: 0.616 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.181, 10.098], loss: 0.001492, mae: 0.042722, mean_q: 1.162125
 201495/1000000: episode: 2015, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.048, mean reward: 0.580 [0.499, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.098], loss: 0.001580, mae: 0.043271, mean_q: 1.165868
 201595/1000000: episode: 2016, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 56.681, mean reward: 0.567 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.686, 10.098], loss: 0.001408, mae: 0.041841, mean_q: 1.165210
 201695/1000000: episode: 2017, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.503, mean reward: 0.585 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.098], loss: 0.001216, mae: 0.038472, mean_q: 1.165155
 201795/1000000: episode: 2018, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.818, mean reward: 0.588 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.577, 10.098], loss: 0.001337, mae: 0.040498, mean_q: 1.163531
 201895/1000000: episode: 2019, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 56.511, mean reward: 0.565 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.974, 10.156], loss: 0.001396, mae: 0.041314, mean_q: 1.164474
 201995/1000000: episode: 2020, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 58.452, mean reward: 0.585 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.098], loss: 0.001368, mae: 0.040357, mean_q: 1.161464
 202095/1000000: episode: 2021, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 57.937, mean reward: 0.579 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.427, 10.098], loss: 0.001365, mae: 0.040908, mean_q: 1.161698
 202195/1000000: episode: 2022, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 61.248, mean reward: 0.612 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.754, 10.357], loss: 0.001330, mae: 0.040184, mean_q: 1.162055
 202295/1000000: episode: 2023, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 57.954, mean reward: 0.580 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.518, 10.098], loss: 0.001439, mae: 0.041499, mean_q: 1.163118
 202395/1000000: episode: 2024, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 58.502, mean reward: 0.585 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.603, 10.170], loss: 0.001413, mae: 0.041581, mean_q: 1.163947
 202495/1000000: episode: 2025, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 59.684, mean reward: 0.597 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.031, 10.231], loss: 0.001473, mae: 0.041818, mean_q: 1.164768
 202595/1000000: episode: 2026, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 66.437, mean reward: 0.664 [0.519, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.586, 10.221], loss: 0.001417, mae: 0.040973, mean_q: 1.163554
 202695/1000000: episode: 2027, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.723, mean reward: 0.577 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.964, 10.321], loss: 0.001421, mae: 0.041442, mean_q: 1.169829
 202795/1000000: episode: 2028, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.852, mean reward: 0.589 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.246, 10.276], loss: 0.001477, mae: 0.042157, mean_q: 1.168028
 202895/1000000: episode: 2029, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 61.129, mean reward: 0.611 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.103, 10.098], loss: 0.001442, mae: 0.041414, mean_q: 1.167614
 202995/1000000: episode: 2030, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 60.609, mean reward: 0.606 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.323, 10.445], loss: 0.001404, mae: 0.040970, mean_q: 1.170110
 203095/1000000: episode: 2031, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.538, mean reward: 0.575 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.847, 10.098], loss: 0.001437, mae: 0.041969, mean_q: 1.172659
 203195/1000000: episode: 2032, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 65.128, mean reward: 0.651 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.367, 10.238], loss: 0.001346, mae: 0.040396, mean_q: 1.173348
 203295/1000000: episode: 2033, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.713, mean reward: 0.597 [0.498, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.553, 10.175], loss: 0.001441, mae: 0.041591, mean_q: 1.175319
 203395/1000000: episode: 2034, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.506, mean reward: 0.575 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.637, 10.098], loss: 0.001485, mae: 0.042270, mean_q: 1.174864
 203495/1000000: episode: 2035, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.144, mean reward: 0.581 [0.507, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.874, 10.098], loss: 0.001444, mae: 0.041669, mean_q: 1.171950
 203595/1000000: episode: 2036, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 57.161, mean reward: 0.572 [0.500, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.291], loss: 0.001317, mae: 0.039607, mean_q: 1.172277
 203695/1000000: episode: 2037, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.880, mean reward: 0.599 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.994, 10.098], loss: 0.001529, mae: 0.043029, mean_q: 1.173754
 203795/1000000: episode: 2038, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 58.698, mean reward: 0.587 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.279], loss: 0.001503, mae: 0.042745, mean_q: 1.172534
 203895/1000000: episode: 2039, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: 57.427, mean reward: 0.574 [0.511, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.599, 10.192], loss: 0.001522, mae: 0.042162, mean_q: 1.172733
 203995/1000000: episode: 2040, duration: 1.499s, episode steps: 100, steps per second: 67, episode reward: 57.239, mean reward: 0.572 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.480, 10.263], loss: 0.001423, mae: 0.041041, mean_q: 1.173630
 204095/1000000: episode: 2041, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.546, mean reward: 0.585 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.440, 10.106], loss: 0.001430, mae: 0.041133, mean_q: 1.172846
 204195/1000000: episode: 2042, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.275, mean reward: 0.593 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.663, 10.113], loss: 0.001434, mae: 0.041465, mean_q: 1.168103
 204295/1000000: episode: 2043, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 58.117, mean reward: 0.581 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.122, 10.098], loss: 0.001404, mae: 0.041101, mean_q: 1.171266
 204395/1000000: episode: 2044, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 59.757, mean reward: 0.598 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.466, 10.410], loss: 0.001408, mae: 0.040756, mean_q: 1.168683
 204495/1000000: episode: 2045, duration: 1.416s, episode steps: 100, steps per second: 71, episode reward: 61.833, mean reward: 0.618 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.889, 10.098], loss: 0.001393, mae: 0.041554, mean_q: 1.174915
 204595/1000000: episode: 2046, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 57.174, mean reward: 0.572 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.400, 10.098], loss: 0.001507, mae: 0.042500, mean_q: 1.168765
 204695/1000000: episode: 2047, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 57.888, mean reward: 0.579 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 0.001415, mae: 0.041060, mean_q: 1.164476
 204795/1000000: episode: 2048, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 57.352, mean reward: 0.574 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.425, 10.287], loss: 0.001428, mae: 0.041923, mean_q: 1.169557
 204895/1000000: episode: 2049, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 58.633, mean reward: 0.586 [0.511, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.609, 10.134], loss: 0.001434, mae: 0.041729, mean_q: 1.170171
 204995/1000000: episode: 2050, duration: 1.679s, episode steps: 100, steps per second: 60, episode reward: 58.630, mean reward: 0.586 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.009, 10.098], loss: 0.001508, mae: 0.042484, mean_q: 1.169870
 205095/1000000: episode: 2051, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 59.600, mean reward: 0.596 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.726, 10.098], loss: 0.001336, mae: 0.040278, mean_q: 1.166285
 205195/1000000: episode: 2052, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 57.855, mean reward: 0.579 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.892, 10.098], loss: 0.001371, mae: 0.040998, mean_q: 1.170249
 205295/1000000: episode: 2053, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 57.874, mean reward: 0.579 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.295, 10.271], loss: 0.001464, mae: 0.041798, mean_q: 1.165534
 205395/1000000: episode: 2054, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 59.217, mean reward: 0.592 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.228, 10.098], loss: 0.001503, mae: 0.041827, mean_q: 1.168643
 205495/1000000: episode: 2055, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.151, mean reward: 0.582 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.068, 10.199], loss: 0.001446, mae: 0.041158, mean_q: 1.170449
 205595/1000000: episode: 2056, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 59.523, mean reward: 0.595 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.333, 10.098], loss: 0.001399, mae: 0.040999, mean_q: 1.168075
 205695/1000000: episode: 2057, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 61.087, mean reward: 0.611 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.130, 10.098], loss: 0.001381, mae: 0.040493, mean_q: 1.167464
 205795/1000000: episode: 2058, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 57.850, mean reward: 0.578 [0.508, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.183, 10.200], loss: 0.001358, mae: 0.040274, mean_q: 1.165987
 205895/1000000: episode: 2059, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 66.275, mean reward: 0.663 [0.514, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.085, 10.098], loss: 0.001455, mae: 0.041988, mean_q: 1.167367
 205995/1000000: episode: 2060, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 59.409, mean reward: 0.594 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.222], loss: 0.001357, mae: 0.040420, mean_q: 1.170588
 206095/1000000: episode: 2061, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.356, mean reward: 0.594 [0.511, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.616, 10.098], loss: 0.001340, mae: 0.040047, mean_q: 1.169512
 206195/1000000: episode: 2062, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 57.709, mean reward: 0.577 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.241], loss: 0.001366, mae: 0.040559, mean_q: 1.167253
 206295/1000000: episode: 2063, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 59.111, mean reward: 0.591 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.424], loss: 0.001379, mae: 0.040862, mean_q: 1.172891
 206395/1000000: episode: 2064, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.778, mean reward: 0.598 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.643, 10.098], loss: 0.001397, mae: 0.040854, mean_q: 1.170412
 206495/1000000: episode: 2065, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.531, mean reward: 0.585 [0.502, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.667, 10.098], loss: 0.001409, mae: 0.040957, mean_q: 1.167554
 206595/1000000: episode: 2066, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 58.294, mean reward: 0.583 [0.511, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.728, 10.098], loss: 0.001477, mae: 0.041703, mean_q: 1.168070
 206695/1000000: episode: 2067, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 58.090, mean reward: 0.581 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.575, 10.214], loss: 0.001420, mae: 0.041570, mean_q: 1.169733
 206795/1000000: episode: 2068, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 59.391, mean reward: 0.594 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.622, 10.170], loss: 0.001470, mae: 0.041597, mean_q: 1.171713
 206895/1000000: episode: 2069, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 60.347, mean reward: 0.603 [0.508, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.560, 10.098], loss: 0.001452, mae: 0.041837, mean_q: 1.171131
 206995/1000000: episode: 2070, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 61.241, mean reward: 0.612 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.745, 10.098], loss: 0.001417, mae: 0.040997, mean_q: 1.173748
 207095/1000000: episode: 2071, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.108, mean reward: 0.581 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.894, 10.196], loss: 0.001542, mae: 0.043536, mean_q: 1.175176
 207195/1000000: episode: 2072, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 60.242, mean reward: 0.602 [0.503, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.504, 10.317], loss: 0.001368, mae: 0.040833, mean_q: 1.168876
 207295/1000000: episode: 2073, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 59.276, mean reward: 0.593 [0.515, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.143], loss: 0.001431, mae: 0.041423, mean_q: 1.176208
 207395/1000000: episode: 2074, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 61.019, mean reward: 0.610 [0.518, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.059, 10.305], loss: 0.001384, mae: 0.040911, mean_q: 1.173618
 207495/1000000: episode: 2075, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.589, mean reward: 0.586 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.201, 10.098], loss: 0.001439, mae: 0.041814, mean_q: 1.176670
 207595/1000000: episode: 2076, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 59.272, mean reward: 0.593 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.189], loss: 0.001431, mae: 0.041116, mean_q: 1.173914
 207695/1000000: episode: 2077, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 57.226, mean reward: 0.572 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.702, 10.098], loss: 0.001461, mae: 0.041668, mean_q: 1.172763
 207795/1000000: episode: 2078, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 57.557, mean reward: 0.576 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.916, 10.260], loss: 0.001529, mae: 0.042746, mean_q: 1.171620
 207895/1000000: episode: 2079, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 58.641, mean reward: 0.586 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.934, 10.191], loss: 0.001573, mae: 0.042799, mean_q: 1.169809
 207995/1000000: episode: 2080, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.561, mean reward: 0.596 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.423, 10.098], loss: 0.001456, mae: 0.041601, mean_q: 1.167461
 208095/1000000: episode: 2081, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.562, mean reward: 0.596 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.641, 10.361], loss: 0.001415, mae: 0.041231, mean_q: 1.167931
 208195/1000000: episode: 2082, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: 60.056, mean reward: 0.601 [0.515, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.198, 10.098], loss: 0.001357, mae: 0.040282, mean_q: 1.170405
 208295/1000000: episode: 2083, duration: 1.830s, episode steps: 100, steps per second: 55, episode reward: 57.059, mean reward: 0.571 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.755, 10.172], loss: 0.001421, mae: 0.041415, mean_q: 1.168791
 208395/1000000: episode: 2084, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 57.102, mean reward: 0.571 [0.499, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.098], loss: 0.001287, mae: 0.039321, mean_q: 1.165984
 208495/1000000: episode: 2085, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 58.004, mean reward: 0.580 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.982, 10.098], loss: 0.001341, mae: 0.040481, mean_q: 1.161566
 208595/1000000: episode: 2086, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 58.694, mean reward: 0.587 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.098], loss: 0.001421, mae: 0.041614, mean_q: 1.164909
 208695/1000000: episode: 2087, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 60.209, mean reward: 0.602 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.947, 10.098], loss: 0.001364, mae: 0.040452, mean_q: 1.164585
 208795/1000000: episode: 2088, duration: 1.506s, episode steps: 100, steps per second: 66, episode reward: 59.283, mean reward: 0.593 [0.503, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.263, 10.098], loss: 0.001461, mae: 0.042108, mean_q: 1.169339
 208895/1000000: episode: 2089, duration: 1.566s, episode steps: 100, steps per second: 64, episode reward: 57.367, mean reward: 0.574 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.192, 10.098], loss: 0.001373, mae: 0.040892, mean_q: 1.166517
 208995/1000000: episode: 2090, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.181, mean reward: 0.582 [0.504, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.391, 10.098], loss: 0.001382, mae: 0.041057, mean_q: 1.170711
 209095/1000000: episode: 2091, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.685, mean reward: 0.587 [0.509, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.252, 10.239], loss: 0.001473, mae: 0.042090, mean_q: 1.167564
 209195/1000000: episode: 2092, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 58.414, mean reward: 0.584 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.974, 10.209], loss: 0.001422, mae: 0.041526, mean_q: 1.165554
 209295/1000000: episode: 2093, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 63.469, mean reward: 0.635 [0.497, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.454, 10.098], loss: 0.001316, mae: 0.039798, mean_q: 1.163851
 209395/1000000: episode: 2094, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 60.684, mean reward: 0.607 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.907, 10.469], loss: 0.001403, mae: 0.041385, mean_q: 1.167143
 209495/1000000: episode: 2095, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.874, mean reward: 0.599 [0.498, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.698, 10.098], loss: 0.001431, mae: 0.041217, mean_q: 1.167212
 209595/1000000: episode: 2096, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.162, mean reward: 0.592 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.450, 10.098], loss: 0.001331, mae: 0.039816, mean_q: 1.170409
 209695/1000000: episode: 2097, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 56.473, mean reward: 0.565 [0.509, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.333, 10.098], loss: 0.001326, mae: 0.039985, mean_q: 1.166870
 209795/1000000: episode: 2098, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.373, mean reward: 0.584 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.035, 10.231], loss: 0.001302, mae: 0.039786, mean_q: 1.168183
 209895/1000000: episode: 2099, duration: 1.407s, episode steps: 100, steps per second: 71, episode reward: 59.114, mean reward: 0.591 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.562, 10.129], loss: 0.001407, mae: 0.041520, mean_q: 1.169270
 209995/1000000: episode: 2100, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 58.953, mean reward: 0.590 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.184], loss: 0.001354, mae: 0.040626, mean_q: 1.171373
 210095/1000000: episode: 2101, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 60.020, mean reward: 0.600 [0.507, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.318, 10.098], loss: 0.001377, mae: 0.041344, mean_q: 1.167495
 210195/1000000: episode: 2102, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.085, mean reward: 0.571 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.382, 10.098], loss: 0.001393, mae: 0.040807, mean_q: 1.171053
 210295/1000000: episode: 2103, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 60.417, mean reward: 0.604 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.393, 10.317], loss: 0.001353, mae: 0.040607, mean_q: 1.170082
 210395/1000000: episode: 2104, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.612, mean reward: 0.576 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.098], loss: 0.001392, mae: 0.040644, mean_q: 1.170464
 210495/1000000: episode: 2105, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 58.613, mean reward: 0.586 [0.509, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.563, 10.333], loss: 0.001336, mae: 0.039803, mean_q: 1.169869
 210595/1000000: episode: 2106, duration: 0.873s, episode steps: 100, steps per second: 114, episode reward: 59.149, mean reward: 0.591 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.551, 10.098], loss: 0.001410, mae: 0.041354, mean_q: 1.168323
 210695/1000000: episode: 2107, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 60.597, mean reward: 0.606 [0.510, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.680, 10.243], loss: 0.001345, mae: 0.040273, mean_q: 1.165679
 210795/1000000: episode: 2108, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 59.923, mean reward: 0.599 [0.517, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.608, 10.178], loss: 0.001367, mae: 0.040644, mean_q: 1.166890
 210895/1000000: episode: 2109, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.833, mean reward: 0.598 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.726, 10.248], loss: 0.001406, mae: 0.040975, mean_q: 1.163571
 210995/1000000: episode: 2110, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.046, mean reward: 0.600 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.863, 10.243], loss: 0.001505, mae: 0.041808, mean_q: 1.165333
 211095/1000000: episode: 2111, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 58.476, mean reward: 0.585 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.303], loss: 0.001421, mae: 0.041129, mean_q: 1.166316
 211195/1000000: episode: 2112, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 59.095, mean reward: 0.591 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.696, 10.145], loss: 0.001361, mae: 0.040059, mean_q: 1.168634
 211295/1000000: episode: 2113, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 58.304, mean reward: 0.583 [0.504, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.792, 10.190], loss: 0.001346, mae: 0.039994, mean_q: 1.167022
 211395/1000000: episode: 2114, duration: 1.328s, episode steps: 100, steps per second: 75, episode reward: 62.712, mean reward: 0.627 [0.514, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.192, 10.098], loss: 0.001504, mae: 0.042486, mean_q: 1.169296
 211495/1000000: episode: 2115, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 63.230, mean reward: 0.632 [0.515, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.355, 10.324], loss: 0.001487, mae: 0.041995, mean_q: 1.168762
 211595/1000000: episode: 2116, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.197, mean reward: 0.592 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.813, 10.135], loss: 0.001495, mae: 0.042615, mean_q: 1.171643
 211695/1000000: episode: 2117, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 57.463, mean reward: 0.575 [0.502, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.098], loss: 0.001420, mae: 0.041250, mean_q: 1.171262
 211795/1000000: episode: 2118, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.153, mean reward: 0.592 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.344, 10.098], loss: 0.001411, mae: 0.041490, mean_q: 1.177533
 211895/1000000: episode: 2119, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.077, mean reward: 0.591 [0.507, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.279, 10.098], loss: 0.001551, mae: 0.042946, mean_q: 1.170926
 211995/1000000: episode: 2120, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 58.395, mean reward: 0.584 [0.515, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.935, 10.098], loss: 0.001431, mae: 0.040989, mean_q: 1.172540
 212095/1000000: episode: 2121, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 58.967, mean reward: 0.590 [0.519, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.584, 10.300], loss: 0.001365, mae: 0.040278, mean_q: 1.175070
 212195/1000000: episode: 2122, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 61.663, mean reward: 0.617 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.098], loss: 0.001322, mae: 0.039487, mean_q: 1.168639
 212295/1000000: episode: 2123, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 58.570, mean reward: 0.586 [0.516, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.319, 10.098], loss: 0.001526, mae: 0.042457, mean_q: 1.171050
 212395/1000000: episode: 2124, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 59.606, mean reward: 0.596 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.973, 10.106], loss: 0.001416, mae: 0.040643, mean_q: 1.165898
 212495/1000000: episode: 2125, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 58.263, mean reward: 0.583 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.418, 10.098], loss: 0.001367, mae: 0.039670, mean_q: 1.167061
 212595/1000000: episode: 2126, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 60.753, mean reward: 0.608 [0.518, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.447, 10.098], loss: 0.001376, mae: 0.040680, mean_q: 1.170705
 212695/1000000: episode: 2127, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 55.905, mean reward: 0.559 [0.507, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.507, 10.125], loss: 0.001480, mae: 0.042247, mean_q: 1.169803
 212795/1000000: episode: 2128, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.124, mean reward: 0.581 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.792, 10.098], loss: 0.001415, mae: 0.041598, mean_q: 1.172266
 212895/1000000: episode: 2129, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 59.026, mean reward: 0.590 [0.514, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.263], loss: 0.001395, mae: 0.040997, mean_q: 1.168683
 212995/1000000: episode: 2130, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.614, mean reward: 0.576 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.237, 10.098], loss: 0.001425, mae: 0.041151, mean_q: 1.171406
 213095/1000000: episode: 2131, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 58.603, mean reward: 0.586 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.175], loss: 0.001442, mae: 0.041280, mean_q: 1.169719
 213195/1000000: episode: 2132, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.243, mean reward: 0.582 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.447, 10.098], loss: 0.001385, mae: 0.040100, mean_q: 1.168406
 213295/1000000: episode: 2133, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 58.152, mean reward: 0.582 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.701, 10.219], loss: 0.001408, mae: 0.040689, mean_q: 1.167656
 213395/1000000: episode: 2134, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 58.609, mean reward: 0.586 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.484, 10.122], loss: 0.001403, mae: 0.041020, mean_q: 1.169673
 213495/1000000: episode: 2135, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 60.274, mean reward: 0.603 [0.509, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.210, 10.102], loss: 0.001348, mae: 0.040090, mean_q: 1.171681
 213595/1000000: episode: 2136, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.659, mean reward: 0.597 [0.512, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.639, 10.098], loss: 0.001387, mae: 0.040102, mean_q: 1.170952
 213695/1000000: episode: 2137, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 59.408, mean reward: 0.594 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.964, 10.098], loss: 0.001378, mae: 0.040610, mean_q: 1.171167
 213795/1000000: episode: 2138, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 58.836, mean reward: 0.588 [0.506, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.931, 10.139], loss: 0.001428, mae: 0.041247, mean_q: 1.169169
 213895/1000000: episode: 2139, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 57.772, mean reward: 0.578 [0.500, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.199, 10.098], loss: 0.001424, mae: 0.041577, mean_q: 1.170196
 213995/1000000: episode: 2140, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.745, mean reward: 0.597 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.885, 10.297], loss: 0.001468, mae: 0.041587, mean_q: 1.166496
 214095/1000000: episode: 2141, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 58.772, mean reward: 0.588 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.162], loss: 0.001448, mae: 0.041741, mean_q: 1.173010
 214195/1000000: episode: 2142, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 60.093, mean reward: 0.601 [0.520, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.711, 10.098], loss: 0.001370, mae: 0.040739, mean_q: 1.168025
 214295/1000000: episode: 2143, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 59.464, mean reward: 0.595 [0.508, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.388, 10.152], loss: 0.001568, mae: 0.042992, mean_q: 1.173566
 214395/1000000: episode: 2144, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 60.674, mean reward: 0.607 [0.510, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.230], loss: 0.001524, mae: 0.042340, mean_q: 1.166058
 214495/1000000: episode: 2145, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 61.436, mean reward: 0.614 [0.508, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.687, 10.098], loss: 0.001505, mae: 0.042603, mean_q: 1.168467
 214595/1000000: episode: 2146, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 58.411, mean reward: 0.584 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.126, 10.208], loss: 0.001605, mae: 0.043458, mean_q: 1.172680
 214695/1000000: episode: 2147, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 60.144, mean reward: 0.601 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.451, 10.141], loss: 0.001513, mae: 0.042866, mean_q: 1.174372
 214795/1000000: episode: 2148, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.478, mean reward: 0.575 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.559, 10.098], loss: 0.001462, mae: 0.042119, mean_q: 1.172279
 214895/1000000: episode: 2149, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.437, mean reward: 0.584 [0.499, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.098], loss: 0.001456, mae: 0.041948, mean_q: 1.170743
 214995/1000000: episode: 2150, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 58.441, mean reward: 0.584 [0.501, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.098], loss: 0.001469, mae: 0.042302, mean_q: 1.171729
 215095/1000000: episode: 2151, duration: 0.866s, episode steps: 100, steps per second: 116, episode reward: 58.410, mean reward: 0.584 [0.503, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.135], loss: 0.001472, mae: 0.041903, mean_q: 1.170066
 215195/1000000: episode: 2152, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.824, mean reward: 0.588 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.799, 10.098], loss: 0.001468, mae: 0.041824, mean_q: 1.173469
 215295/1000000: episode: 2153, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 58.983, mean reward: 0.590 [0.513, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.192, 10.098], loss: 0.001561, mae: 0.043496, mean_q: 1.173606
 215395/1000000: episode: 2154, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 59.530, mean reward: 0.595 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.082, 10.098], loss: 0.001525, mae: 0.042262, mean_q: 1.172139
 215495/1000000: episode: 2155, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.804, mean reward: 0.578 [0.510, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.287, 10.239], loss: 0.001657, mae: 0.044999, mean_q: 1.171305
 215595/1000000: episode: 2156, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.233, mean reward: 0.582 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.245, 10.178], loss: 0.001468, mae: 0.042277, mean_q: 1.173011
 215695/1000000: episode: 2157, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 60.481, mean reward: 0.605 [0.522, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.511, 10.098], loss: 0.001507, mae: 0.042486, mean_q: 1.172849
 215795/1000000: episode: 2158, duration: 1.330s, episode steps: 100, steps per second: 75, episode reward: 57.318, mean reward: 0.573 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.619, 10.098], loss: 0.001657, mae: 0.044183, mean_q: 1.166801
 215895/1000000: episode: 2159, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.783, mean reward: 0.578 [0.512, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.868, 10.098], loss: 0.001490, mae: 0.042388, mean_q: 1.168177
 215995/1000000: episode: 2160, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.632, mean reward: 0.586 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.266, 10.098], loss: 0.001395, mae: 0.040565, mean_q: 1.165151
 216095/1000000: episode: 2161, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.617, mean reward: 0.576 [0.508, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.121], loss: 0.001437, mae: 0.041522, mean_q: 1.168392
 216195/1000000: episode: 2162, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 58.695, mean reward: 0.587 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.295, 10.098], loss: 0.001436, mae: 0.041397, mean_q: 1.168018
 216295/1000000: episode: 2163, duration: 1.170s, episode steps: 100, steps per second: 86, episode reward: 60.020, mean reward: 0.600 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.001458, mae: 0.041659, mean_q: 1.165766
 216395/1000000: episode: 2164, duration: 1.401s, episode steps: 100, steps per second: 71, episode reward: 58.782, mean reward: 0.588 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.702, 10.103], loss: 0.001533, mae: 0.043142, mean_q: 1.167240
 216495/1000000: episode: 2165, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 59.672, mean reward: 0.597 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.682, 10.098], loss: 0.001538, mae: 0.042809, mean_q: 1.165503
 216595/1000000: episode: 2166, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 57.647, mean reward: 0.576 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.351, 10.249], loss: 0.001468, mae: 0.042015, mean_q: 1.166213
 216695/1000000: episode: 2167, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 63.760, mean reward: 0.638 [0.503, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.373], loss: 0.001545, mae: 0.043064, mean_q: 1.165048
 216795/1000000: episode: 2168, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 57.788, mean reward: 0.578 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.403, 10.098], loss: 0.001518, mae: 0.042938, mean_q: 1.167186
 216895/1000000: episode: 2169, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 58.638, mean reward: 0.586 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.909, 10.098], loss: 0.001453, mae: 0.041557, mean_q: 1.168089
 216995/1000000: episode: 2170, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.730, mean reward: 0.587 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.185, 10.098], loss: 0.001495, mae: 0.042373, mean_q: 1.169264
 217095/1000000: episode: 2171, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.939, mean reward: 0.579 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.172, 10.202], loss: 0.001569, mae: 0.043081, mean_q: 1.167565
 217195/1000000: episode: 2172, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.404, mean reward: 0.584 [0.515, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.532, 10.098], loss: 0.001463, mae: 0.041916, mean_q: 1.165486
 217295/1000000: episode: 2173, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.974, mean reward: 0.590 [0.499, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.455, 10.098], loss: 0.001525, mae: 0.043079, mean_q: 1.167200
 217395/1000000: episode: 2174, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 60.157, mean reward: 0.602 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.798, 10.098], loss: 0.001501, mae: 0.042598, mean_q: 1.164307
 217495/1000000: episode: 2175, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 61.919, mean reward: 0.619 [0.526, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.240], loss: 0.001572, mae: 0.043549, mean_q: 1.163001
 217595/1000000: episode: 2176, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 56.396, mean reward: 0.564 [0.500, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.902, 10.146], loss: 0.001602, mae: 0.043785, mean_q: 1.165944
 217695/1000000: episode: 2177, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 58.498, mean reward: 0.585 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.323, 10.135], loss: 0.001557, mae: 0.043533, mean_q: 1.164147
 217795/1000000: episode: 2178, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.130, mean reward: 0.581 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.182, 10.197], loss: 0.001572, mae: 0.043533, mean_q: 1.163595
 217895/1000000: episode: 2179, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 58.845, mean reward: 0.588 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.098], loss: 0.001541, mae: 0.043105, mean_q: 1.167957
 217995/1000000: episode: 2180, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 57.870, mean reward: 0.579 [0.499, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.540, 10.279], loss: 0.001517, mae: 0.043081, mean_q: 1.167545
 218095/1000000: episode: 2181, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 61.199, mean reward: 0.612 [0.525, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.457, 10.098], loss: 0.001568, mae: 0.043401, mean_q: 1.166757
 218195/1000000: episode: 2182, duration: 1.414s, episode steps: 100, steps per second: 71, episode reward: 60.807, mean reward: 0.608 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.927, 10.244], loss: 0.001577, mae: 0.043494, mean_q: 1.165116
 218295/1000000: episode: 2183, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.358, mean reward: 0.574 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.854, 10.141], loss: 0.001561, mae: 0.043013, mean_q: 1.169609
 218395/1000000: episode: 2184, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 60.687, mean reward: 0.607 [0.513, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.864, 10.446], loss: 0.001486, mae: 0.042563, mean_q: 1.167351
 218495/1000000: episode: 2185, duration: 1.465s, episode steps: 100, steps per second: 68, episode reward: 58.212, mean reward: 0.582 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.098], loss: 0.001550, mae: 0.043448, mean_q: 1.168201
 218595/1000000: episode: 2186, duration: 1.268s, episode steps: 100, steps per second: 79, episode reward: 57.103, mean reward: 0.571 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.553, 10.146], loss: 0.001524, mae: 0.042450, mean_q: 1.168478
 218695/1000000: episode: 2187, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 60.303, mean reward: 0.603 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.509, 10.098], loss: 0.001472, mae: 0.041721, mean_q: 1.167240
 218795/1000000: episode: 2188, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 58.497, mean reward: 0.585 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.380, 10.101], loss: 0.001493, mae: 0.042222, mean_q: 1.168113
 218895/1000000: episode: 2189, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 57.426, mean reward: 0.574 [0.504, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.080, 10.174], loss: 0.001559, mae: 0.042920, mean_q: 1.169547
 218995/1000000: episode: 2190, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 57.305, mean reward: 0.573 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.238, 10.098], loss: 0.001478, mae: 0.042045, mean_q: 1.171185
 219095/1000000: episode: 2191, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 60.043, mean reward: 0.600 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.647, 10.253], loss: 0.001467, mae: 0.041499, mean_q: 1.165926
 219195/1000000: episode: 2192, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.524, mean reward: 0.585 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.491, 10.188], loss: 0.001524, mae: 0.042958, mean_q: 1.165511
 219295/1000000: episode: 2193, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 58.442, mean reward: 0.584 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.149, 10.142], loss: 0.001451, mae: 0.041706, mean_q: 1.164442
 219395/1000000: episode: 2194, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 62.720, mean reward: 0.627 [0.504, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.498, 10.227], loss: 0.001464, mae: 0.042121, mean_q: 1.160221
 219495/1000000: episode: 2195, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 57.550, mean reward: 0.576 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.594, 10.098], loss: 0.001519, mae: 0.042323, mean_q: 1.165641
 219595/1000000: episode: 2196, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 56.062, mean reward: 0.561 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.159, 10.098], loss: 0.001491, mae: 0.042411, mean_q: 1.164994
 219695/1000000: episode: 2197, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 58.743, mean reward: 0.587 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.666, 10.098], loss: 0.001553, mae: 0.042912, mean_q: 1.164505
 219795/1000000: episode: 2198, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.805, mean reward: 0.588 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.765, 10.242], loss: 0.001456, mae: 0.041252, mean_q: 1.166443
 219895/1000000: episode: 2199, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 59.729, mean reward: 0.597 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.367], loss: 0.001502, mae: 0.042251, mean_q: 1.164524
 219995/1000000: episode: 2200, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.123, mean reward: 0.591 [0.518, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.595, 10.117], loss: 0.001646, mae: 0.044273, mean_q: 1.164386
 220095/1000000: episode: 2201, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 61.120, mean reward: 0.611 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.358, 10.098], loss: 0.001530, mae: 0.042807, mean_q: 1.165379
 220195/1000000: episode: 2202, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.595, mean reward: 0.586 [0.498, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.522, 10.174], loss: 0.001474, mae: 0.042113, mean_q: 1.163858
 220295/1000000: episode: 2203, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 57.505, mean reward: 0.575 [0.503, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.395], loss: 0.001500, mae: 0.041424, mean_q: 1.163467
 220395/1000000: episode: 2204, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 57.435, mean reward: 0.574 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.020, 10.164], loss: 0.001525, mae: 0.042500, mean_q: 1.163919
 220495/1000000: episode: 2205, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 65.142, mean reward: 0.651 [0.525, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.098], loss: 0.001515, mae: 0.042184, mean_q: 1.164064
 220595/1000000: episode: 2206, duration: 1.270s, episode steps: 100, steps per second: 79, episode reward: 58.358, mean reward: 0.584 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.474, 10.098], loss: 0.001531, mae: 0.042780, mean_q: 1.162391
 220695/1000000: episode: 2207, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 57.688, mean reward: 0.577 [0.513, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.100, 10.218], loss: 0.001546, mae: 0.043166, mean_q: 1.163707
 220795/1000000: episode: 2208, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 58.641, mean reward: 0.586 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.098], loss: 0.001589, mae: 0.042971, mean_q: 1.164779
 220895/1000000: episode: 2209, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 63.925, mean reward: 0.639 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.098], loss: 0.001576, mae: 0.042915, mean_q: 1.165055
 220995/1000000: episode: 2210, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 57.283, mean reward: 0.573 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.435, 10.115], loss: 0.001445, mae: 0.042126, mean_q: 1.167184
 221095/1000000: episode: 2211, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 57.456, mean reward: 0.575 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.700, 10.098], loss: 0.001537, mae: 0.042802, mean_q: 1.169039
 221195/1000000: episode: 2212, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 60.643, mean reward: 0.606 [0.512, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.863, 10.098], loss: 0.001457, mae: 0.041852, mean_q: 1.165033
 221295/1000000: episode: 2213, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 61.690, mean reward: 0.617 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.715, 10.274], loss: 0.001521, mae: 0.042249, mean_q: 1.168692
 221395/1000000: episode: 2214, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 57.492, mean reward: 0.575 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.350, 10.119], loss: 0.001423, mae: 0.041847, mean_q: 1.168397
 221495/1000000: episode: 2215, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.621, mean reward: 0.576 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.387, 10.242], loss: 0.001441, mae: 0.042138, mean_q: 1.166873
 221595/1000000: episode: 2216, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.444, mean reward: 0.574 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.561, 10.131], loss: 0.001486, mae: 0.042214, mean_q: 1.169544
 221695/1000000: episode: 2217, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 60.538, mean reward: 0.605 [0.501, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.327, 10.098], loss: 0.001482, mae: 0.041899, mean_q: 1.167649
 221795/1000000: episode: 2218, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 56.767, mean reward: 0.568 [0.504, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.445, 10.098], loss: 0.001504, mae: 0.042253, mean_q: 1.167146
 221895/1000000: episode: 2219, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.953, mean reward: 0.580 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.912, 10.202], loss: 0.001473, mae: 0.042260, mean_q: 1.163802
 221995/1000000: episode: 2220, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.527, mean reward: 0.595 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.221], loss: 0.001428, mae: 0.040996, mean_q: 1.168196
 222095/1000000: episode: 2221, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 58.687, mean reward: 0.587 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.214], loss: 0.001461, mae: 0.041941, mean_q: 1.166015
 222195/1000000: episode: 2222, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 57.523, mean reward: 0.575 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.811, 10.098], loss: 0.001440, mae: 0.041313, mean_q: 1.168461
 222295/1000000: episode: 2223, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: 58.897, mean reward: 0.589 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.354, 10.282], loss: 0.001481, mae: 0.041908, mean_q: 1.166987
 222395/1000000: episode: 2224, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 57.097, mean reward: 0.571 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.994, 10.098], loss: 0.001592, mae: 0.042930, mean_q: 1.166100
 222495/1000000: episode: 2225, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.875, mean reward: 0.589 [0.502, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.058, 10.098], loss: 0.001530, mae: 0.042283, mean_q: 1.164455
 222595/1000000: episode: 2226, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.668, mean reward: 0.587 [0.511, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.667, 10.098], loss: 0.001483, mae: 0.042565, mean_q: 1.165053
 222695/1000000: episode: 2227, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 59.274, mean reward: 0.593 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.198], loss: 0.001497, mae: 0.042472, mean_q: 1.168169
 222795/1000000: episode: 2228, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 64.135, mean reward: 0.641 [0.502, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.575, 10.399], loss: 0.001513, mae: 0.042839, mean_q: 1.165580
 222895/1000000: episode: 2229, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.588, mean reward: 0.596 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.800, 10.291], loss: 0.001423, mae: 0.041484, mean_q: 1.165954
 222995/1000000: episode: 2230, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 56.709, mean reward: 0.567 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.513, 10.140], loss: 0.001333, mae: 0.040068, mean_q: 1.169536
 223095/1000000: episode: 2231, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 57.743, mean reward: 0.577 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.299, 10.183], loss: 0.001501, mae: 0.042143, mean_q: 1.165924
 223195/1000000: episode: 2232, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.425, mean reward: 0.584 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.412, 10.098], loss: 0.001357, mae: 0.040267, mean_q: 1.162208
 223295/1000000: episode: 2233, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.225, mean reward: 0.592 [0.511, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.789, 10.098], loss: 0.001364, mae: 0.040549, mean_q: 1.164071
 223395/1000000: episode: 2234, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 57.643, mean reward: 0.576 [0.513, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.215, 10.256], loss: 0.001388, mae: 0.040551, mean_q: 1.164364
 223495/1000000: episode: 2235, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 58.679, mean reward: 0.587 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.185, 10.106], loss: 0.001424, mae: 0.041179, mean_q: 1.165664
 223595/1000000: episode: 2236, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 61.601, mean reward: 0.616 [0.507, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.861, 10.098], loss: 0.001401, mae: 0.040846, mean_q: 1.162237
 223695/1000000: episode: 2237, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.542, mean reward: 0.575 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.611, 10.126], loss: 0.001390, mae: 0.041160, mean_q: 1.165840
 223795/1000000: episode: 2238, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 57.081, mean reward: 0.571 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.956, 10.098], loss: 0.001441, mae: 0.041257, mean_q: 1.164887
 223895/1000000: episode: 2239, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.944, mean reward: 0.579 [0.513, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.741, 10.098], loss: 0.001463, mae: 0.041409, mean_q: 1.165480
 223995/1000000: episode: 2240, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.962, mean reward: 0.580 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.104], loss: 0.001412, mae: 0.040969, mean_q: 1.171594
 224095/1000000: episode: 2241, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.103, mean reward: 0.571 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.134, 10.098], loss: 0.001401, mae: 0.040354, mean_q: 1.165671
 224195/1000000: episode: 2242, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 56.947, mean reward: 0.569 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.339, 10.126], loss: 0.001363, mae: 0.040213, mean_q: 1.163736
 224295/1000000: episode: 2243, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: 58.162, mean reward: 0.582 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.977, 10.098], loss: 0.001354, mae: 0.040138, mean_q: 1.164844
 224395/1000000: episode: 2244, duration: 1.257s, episode steps: 100, steps per second: 80, episode reward: 57.626, mean reward: 0.576 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.274, 10.359], loss: 0.001444, mae: 0.040976, mean_q: 1.161488
 224495/1000000: episode: 2245, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 62.336, mean reward: 0.623 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.813, 10.385], loss: 0.001405, mae: 0.040805, mean_q: 1.164029
 224595/1000000: episode: 2246, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 61.375, mean reward: 0.614 [0.498, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.697, 10.098], loss: 0.001399, mae: 0.040812, mean_q: 1.164390
 224695/1000000: episode: 2247, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.165, mean reward: 0.592 [0.507, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.873, 10.098], loss: 0.001438, mae: 0.041189, mean_q: 1.165804
 224795/1000000: episode: 2248, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 57.863, mean reward: 0.579 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.295, 10.098], loss: 0.001249, mae: 0.038259, mean_q: 1.165862
 224895/1000000: episode: 2249, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.386, mean reward: 0.574 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.293, 10.098], loss: 0.001407, mae: 0.040096, mean_q: 1.162268
 224995/1000000: episode: 2250, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 59.497, mean reward: 0.595 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.102, 10.098], loss: 0.001435, mae: 0.041131, mean_q: 1.165352
 225095/1000000: episode: 2251, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 58.969, mean reward: 0.590 [0.508, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.098], loss: 0.001312, mae: 0.039215, mean_q: 1.162926
 225195/1000000: episode: 2252, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 58.200, mean reward: 0.582 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.136, 10.204], loss: 0.001321, mae: 0.039416, mean_q: 1.163434
 225295/1000000: episode: 2253, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 57.764, mean reward: 0.578 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.465, 10.098], loss: 0.001359, mae: 0.040331, mean_q: 1.162972
 225395/1000000: episode: 2254, duration: 0.778s, episode steps: 100, steps per second: 128, episode reward: 61.274, mean reward: 0.613 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.032, 10.098], loss: 0.001415, mae: 0.040795, mean_q: 1.160246
 225495/1000000: episode: 2255, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: 60.420, mean reward: 0.604 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.980, 10.462], loss: 0.001374, mae: 0.040678, mean_q: 1.161784
 225595/1000000: episode: 2256, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.388, mean reward: 0.584 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.139], loss: 0.001367, mae: 0.040156, mean_q: 1.164533
 225695/1000000: episode: 2257, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 57.533, mean reward: 0.575 [0.513, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.449, 10.219], loss: 0.001451, mae: 0.041531, mean_q: 1.163725
 225795/1000000: episode: 2258, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 59.319, mean reward: 0.593 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.057, 10.214], loss: 0.001392, mae: 0.040405, mean_q: 1.166667
 225895/1000000: episode: 2259, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 60.435, mean reward: 0.604 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.469, 10.098], loss: 0.001389, mae: 0.040085, mean_q: 1.162909
 225995/1000000: episode: 2260, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 56.445, mean reward: 0.564 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.294, 10.098], loss: 0.001410, mae: 0.040754, mean_q: 1.160154
 226095/1000000: episode: 2261, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.313, mean reward: 0.583 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.098], loss: 0.001437, mae: 0.041129, mean_q: 1.162967
 226195/1000000: episode: 2262, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.553, mean reward: 0.586 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.718, 10.098], loss: 0.001432, mae: 0.040865, mean_q: 1.164526
 226295/1000000: episode: 2263, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 57.132, mean reward: 0.571 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.527, 10.262], loss: 0.001339, mae: 0.039608, mean_q: 1.158305
 226395/1000000: episode: 2264, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 58.679, mean reward: 0.587 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.603, 10.301], loss: 0.001502, mae: 0.042082, mean_q: 1.159139
 226495/1000000: episode: 2265, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 61.914, mean reward: 0.619 [0.508, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.098], loss: 0.001385, mae: 0.039991, mean_q: 1.161028
 226595/1000000: episode: 2266, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 59.781, mean reward: 0.598 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.215, 10.169], loss: 0.001356, mae: 0.039846, mean_q: 1.160843
 226695/1000000: episode: 2267, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 59.393, mean reward: 0.594 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.917, 10.098], loss: 0.001308, mae: 0.038924, mean_q: 1.158027
 226795/1000000: episode: 2268, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.310, mean reward: 0.593 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.501, 10.098], loss: 0.001373, mae: 0.040180, mean_q: 1.163738
 226895/1000000: episode: 2269, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 62.069, mean reward: 0.621 [0.516, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.614, 10.456], loss: 0.001460, mae: 0.041073, mean_q: 1.165451
 226995/1000000: episode: 2270, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 57.458, mean reward: 0.575 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.606, 10.283], loss: 0.001463, mae: 0.041202, mean_q: 1.163532
 227095/1000000: episode: 2271, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 56.651, mean reward: 0.567 [0.500, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.971, 10.098], loss: 0.001417, mae: 0.040823, mean_q: 1.160300
 227195/1000000: episode: 2272, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.749, mean reward: 0.587 [0.515, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.088, 10.205], loss: 0.001472, mae: 0.041260, mean_q: 1.161270
 227295/1000000: episode: 2273, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 58.578, mean reward: 0.586 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.098], loss: 0.001375, mae: 0.040035, mean_q: 1.166767
 227395/1000000: episode: 2274, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.958, mean reward: 0.600 [0.519, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.277, 10.378], loss: 0.001404, mae: 0.041269, mean_q: 1.165995
 227495/1000000: episode: 2275, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 60.322, mean reward: 0.603 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.222, 10.377], loss: 0.001403, mae: 0.040409, mean_q: 1.166230
 227595/1000000: episode: 2276, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.568, mean reward: 0.596 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.994, 10.131], loss: 0.001323, mae: 0.039622, mean_q: 1.164233
 227695/1000000: episode: 2277, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 57.992, mean reward: 0.580 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.295, 10.110], loss: 0.001317, mae: 0.040145, mean_q: 1.167120
 227795/1000000: episode: 2278, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 59.716, mean reward: 0.597 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.689, 10.281], loss: 0.001450, mae: 0.041627, mean_q: 1.163832
 227895/1000000: episode: 2279, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.820, mean reward: 0.588 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.249, 10.293], loss: 0.001334, mae: 0.039709, mean_q: 1.162662
 227995/1000000: episode: 2280, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 60.627, mean reward: 0.606 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.701, 10.161], loss: 0.001483, mae: 0.041741, mean_q: 1.165755
 228095/1000000: episode: 2281, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 57.066, mean reward: 0.571 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.614, 10.113], loss: 0.001423, mae: 0.041200, mean_q: 1.165824
 228195/1000000: episode: 2282, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 58.963, mean reward: 0.590 [0.500, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.098], loss: 0.001491, mae: 0.042471, mean_q: 1.164077
 228295/1000000: episode: 2283, duration: 1.461s, episode steps: 100, steps per second: 68, episode reward: 58.909, mean reward: 0.589 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.171, 10.131], loss: 0.001460, mae: 0.041677, mean_q: 1.160191
 228395/1000000: episode: 2284, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 59.177, mean reward: 0.592 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.218, 10.098], loss: 0.001377, mae: 0.040505, mean_q: 1.163384
 228495/1000000: episode: 2285, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.646, mean reward: 0.596 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.609, 10.098], loss: 0.001494, mae: 0.042259, mean_q: 1.169681
 228595/1000000: episode: 2286, duration: 1.327s, episode steps: 100, steps per second: 75, episode reward: 57.888, mean reward: 0.579 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.098], loss: 0.001383, mae: 0.040657, mean_q: 1.165498
 228695/1000000: episode: 2287, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: 57.652, mean reward: 0.577 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.732, 10.098], loss: 0.001494, mae: 0.041400, mean_q: 1.162081
 228795/1000000: episode: 2288, duration: 1.273s, episode steps: 100, steps per second: 79, episode reward: 59.399, mean reward: 0.594 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.098], loss: 0.001441, mae: 0.041692, mean_q: 1.163273
 228895/1000000: episode: 2289, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 58.378, mean reward: 0.584 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.283, 10.219], loss: 0.001418, mae: 0.041346, mean_q: 1.166823
 228995/1000000: episode: 2290, duration: 1.488s, episode steps: 100, steps per second: 67, episode reward: 57.115, mean reward: 0.571 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.050, 10.098], loss: 0.001418, mae: 0.041026, mean_q: 1.166718
 229095/1000000: episode: 2291, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 58.050, mean reward: 0.580 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.042, 10.098], loss: 0.001434, mae: 0.041242, mean_q: 1.166671
 229195/1000000: episode: 2292, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 58.667, mean reward: 0.587 [0.512, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.812, 10.134], loss: 0.001380, mae: 0.040542, mean_q: 1.166442
 229295/1000000: episode: 2293, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.887, mean reward: 0.599 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.919, 10.098], loss: 0.001476, mae: 0.042303, mean_q: 1.167516
 229395/1000000: episode: 2294, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 57.062, mean reward: 0.571 [0.505, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.124], loss: 0.001454, mae: 0.041444, mean_q: 1.166729
 229495/1000000: episode: 2295, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.971, mean reward: 0.590 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.695, 10.098], loss: 0.001378, mae: 0.040219, mean_q: 1.164313
 229595/1000000: episode: 2296, duration: 1.322s, episode steps: 100, steps per second: 76, episode reward: 59.060, mean reward: 0.591 [0.514, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.196, 10.098], loss: 0.001545, mae: 0.042784, mean_q: 1.166927
 229695/1000000: episode: 2297, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 62.793, mean reward: 0.628 [0.513, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.052, 10.098], loss: 0.001406, mae: 0.041245, mean_q: 1.165142
 229795/1000000: episode: 2298, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 57.805, mean reward: 0.578 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.450, 10.098], loss: 0.001437, mae: 0.041234, mean_q: 1.163926
 229895/1000000: episode: 2299, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.199, mean reward: 0.592 [0.509, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.683, 10.212], loss: 0.001495, mae: 0.042329, mean_q: 1.165663
 229995/1000000: episode: 2300, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 60.719, mean reward: 0.607 [0.501, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.326, 10.140], loss: 0.001492, mae: 0.041916, mean_q: 1.168132
 230095/1000000: episode: 2301, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 59.211, mean reward: 0.592 [0.512, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.098], loss: 0.001509, mae: 0.042057, mean_q: 1.164777
 230195/1000000: episode: 2302, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 58.605, mean reward: 0.586 [0.514, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.647, 10.098], loss: 0.001542, mae: 0.042827, mean_q: 1.167018
 230295/1000000: episode: 2303, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 62.465, mean reward: 0.625 [0.528, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.098], loss: 0.001513, mae: 0.042546, mean_q: 1.167330
 230395/1000000: episode: 2304, duration: 1.345s, episode steps: 100, steps per second: 74, episode reward: 67.044, mean reward: 0.670 [0.518, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.085, 10.340], loss: 0.001556, mae: 0.042601, mean_q: 1.165062
 230495/1000000: episode: 2305, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: 60.063, mean reward: 0.601 [0.516, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.010, 10.098], loss: 0.001551, mae: 0.042843, mean_q: 1.169978
 230595/1000000: episode: 2306, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 58.732, mean reward: 0.587 [0.503, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.023, 10.232], loss: 0.001424, mae: 0.041146, mean_q: 1.171054
 230695/1000000: episode: 2307, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 60.776, mean reward: 0.608 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.283, 10.167], loss: 0.001445, mae: 0.041228, mean_q: 1.171842
 230795/1000000: episode: 2308, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 61.714, mean reward: 0.617 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.358, 10.098], loss: 0.001433, mae: 0.040686, mean_q: 1.167428
 230895/1000000: episode: 2309, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 61.227, mean reward: 0.612 [0.501, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.879, 10.098], loss: 0.001517, mae: 0.042245, mean_q: 1.171015
 230995/1000000: episode: 2310, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 59.594, mean reward: 0.596 [0.510, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.825, 10.211], loss: 0.001499, mae: 0.042374, mean_q: 1.171766
 231095/1000000: episode: 2311, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.706, mean reward: 0.607 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.148, 10.310], loss: 0.001569, mae: 0.043273, mean_q: 1.171372
 231195/1000000: episode: 2312, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.409, mean reward: 0.584 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.296, 10.100], loss: 0.001478, mae: 0.041869, mean_q: 1.172054
 231295/1000000: episode: 2313, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 56.938, mean reward: 0.569 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.098], loss: 0.001538, mae: 0.042735, mean_q: 1.176071
 231395/1000000: episode: 2314, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 57.646, mean reward: 0.576 [0.505, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.201], loss: 0.001505, mae: 0.042086, mean_q: 1.174455
 231495/1000000: episode: 2315, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 61.084, mean reward: 0.611 [0.522, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.801, 10.098], loss: 0.001456, mae: 0.041518, mean_q: 1.173806
 231595/1000000: episode: 2316, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.271, mean reward: 0.573 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.095, 10.098], loss: 0.001431, mae: 0.041670, mean_q: 1.170094
 231695/1000000: episode: 2317, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 57.205, mean reward: 0.572 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.108, 10.099], loss: 0.001455, mae: 0.041514, mean_q: 1.171870
 231795/1000000: episode: 2318, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.370, mean reward: 0.574 [0.507, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.303, 10.098], loss: 0.001424, mae: 0.040730, mean_q: 1.170015
 231895/1000000: episode: 2319, duration: 1.378s, episode steps: 100, steps per second: 73, episode reward: 56.601, mean reward: 0.566 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.135], loss: 0.001497, mae: 0.042218, mean_q: 1.172164
 231995/1000000: episode: 2320, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 57.482, mean reward: 0.575 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.975, 10.108], loss: 0.001442, mae: 0.041351, mean_q: 1.168370
 232095/1000000: episode: 2321, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 59.544, mean reward: 0.595 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.637, 10.098], loss: 0.001480, mae: 0.041936, mean_q: 1.168381
 232195/1000000: episode: 2322, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 61.524, mean reward: 0.615 [0.498, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.119, 10.106], loss: 0.001448, mae: 0.041582, mean_q: 1.173143
 232295/1000000: episode: 2323, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 58.192, mean reward: 0.582 [0.509, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.468, 10.098], loss: 0.001486, mae: 0.042339, mean_q: 1.174139
 232395/1000000: episode: 2324, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 56.632, mean reward: 0.566 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.067, 10.130], loss: 0.001416, mae: 0.041171, mean_q: 1.171699
 232495/1000000: episode: 2325, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 59.293, mean reward: 0.593 [0.500, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.547, 10.098], loss: 0.001408, mae: 0.041290, mean_q: 1.169778
 232595/1000000: episode: 2326, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.056, mean reward: 0.601 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.629, 10.098], loss: 0.001443, mae: 0.041397, mean_q: 1.170350
 232695/1000000: episode: 2327, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 60.725, mean reward: 0.607 [0.508, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.121, 10.098], loss: 0.001448, mae: 0.041731, mean_q: 1.173205
 232795/1000000: episode: 2328, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.304, mean reward: 0.583 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.138], loss: 0.001406, mae: 0.040812, mean_q: 1.171806
 232895/1000000: episode: 2329, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 59.100, mean reward: 0.591 [0.502, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.623, 10.224], loss: 0.001422, mae: 0.040570, mean_q: 1.166256
 232995/1000000: episode: 2330, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 57.703, mean reward: 0.577 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.750, 10.290], loss: 0.001540, mae: 0.042698, mean_q: 1.173702
 233095/1000000: episode: 2331, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.785, mean reward: 0.578 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.925, 10.255], loss: 0.001522, mae: 0.042877, mean_q: 1.170358
 233195/1000000: episode: 2332, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 58.772, mean reward: 0.588 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.087, 10.164], loss: 0.001564, mae: 0.043123, mean_q: 1.170904
 233295/1000000: episode: 2333, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.359, mean reward: 0.594 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.244, 10.106], loss: 0.001572, mae: 0.043094, mean_q: 1.170190
 233395/1000000: episode: 2334, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 59.755, mean reward: 0.598 [0.515, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.392, 10.288], loss: 0.001566, mae: 0.043043, mean_q: 1.172641
 233495/1000000: episode: 2335, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 60.921, mean reward: 0.609 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.278, 10.098], loss: 0.001544, mae: 0.043060, mean_q: 1.169621
 233595/1000000: episode: 2336, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 58.554, mean reward: 0.586 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.098], loss: 0.001554, mae: 0.042797, mean_q: 1.173127
 233695/1000000: episode: 2337, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 62.946, mean reward: 0.629 [0.516, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.046, 10.413], loss: 0.001487, mae: 0.041669, mean_q: 1.173085
 233795/1000000: episode: 2338, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 59.676, mean reward: 0.597 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.772, 10.167], loss: 0.001374, mae: 0.040015, mean_q: 1.175371
 233895/1000000: episode: 2339, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 59.706, mean reward: 0.597 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.528, 10.098], loss: 0.001476, mae: 0.042044, mean_q: 1.172445
 233995/1000000: episode: 2340, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 60.621, mean reward: 0.606 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.109, 10.098], loss: 0.001497, mae: 0.042522, mean_q: 1.175785
 234095/1000000: episode: 2341, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.588, mean reward: 0.586 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.309, 10.299], loss: 0.001452, mae: 0.041725, mean_q: 1.174247
 234195/1000000: episode: 2342, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 56.770, mean reward: 0.568 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.787, 10.098], loss: 0.001548, mae: 0.042829, mean_q: 1.177645
 234295/1000000: episode: 2343, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.612, mean reward: 0.586 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.368, 10.202], loss: 0.001515, mae: 0.042196, mean_q: 1.180556
 234395/1000000: episode: 2344, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 57.126, mean reward: 0.571 [0.501, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.763, 10.098], loss: 0.001651, mae: 0.043816, mean_q: 1.176066
 234495/1000000: episode: 2345, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.806, mean reward: 0.578 [0.501, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.431, 10.124], loss: 0.001403, mae: 0.041038, mean_q: 1.176177
 234595/1000000: episode: 2346, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 60.048, mean reward: 0.600 [0.503, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.327, 10.316], loss: 0.001569, mae: 0.043547, mean_q: 1.173611
 234695/1000000: episode: 2347, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.342, mean reward: 0.573 [0.509, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.252, 10.150], loss: 0.001483, mae: 0.042278, mean_q: 1.172544
 234795/1000000: episode: 2348, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.731, mean reward: 0.587 [0.506, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.693, 10.098], loss: 0.001472, mae: 0.042333, mean_q: 1.172864
 234895/1000000: episode: 2349, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 59.138, mean reward: 0.591 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.376, 10.098], loss: 0.001475, mae: 0.042595, mean_q: 1.171182
 234995/1000000: episode: 2350, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 57.556, mean reward: 0.576 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.640, 10.318], loss: 0.001511, mae: 0.042666, mean_q: 1.169334
 235095/1000000: episode: 2351, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.786, mean reward: 0.588 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.688, 10.098], loss: 0.001445, mae: 0.041853, mean_q: 1.169810
 235195/1000000: episode: 2352, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 56.148, mean reward: 0.561 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.674, 10.098], loss: 0.001371, mae: 0.041154, mean_q: 1.168977
 235295/1000000: episode: 2353, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 57.832, mean reward: 0.578 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.187, 10.203], loss: 0.001502, mae: 0.042655, mean_q: 1.169342
 235395/1000000: episode: 2354, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.731, mean reward: 0.597 [0.514, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.171], loss: 0.001316, mae: 0.039899, mean_q: 1.164358
 235495/1000000: episode: 2355, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 58.609, mean reward: 0.586 [0.509, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.695, 10.110], loss: 0.001479, mae: 0.042162, mean_q: 1.166110
 235595/1000000: episode: 2356, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 58.906, mean reward: 0.589 [0.517, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.366, 10.241], loss: 0.001459, mae: 0.042005, mean_q: 1.166663
 235695/1000000: episode: 2357, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 59.931, mean reward: 0.599 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.582, 10.098], loss: 0.001368, mae: 0.040462, mean_q: 1.163108
 235795/1000000: episode: 2358, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 59.595, mean reward: 0.596 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.791, 10.170], loss: 0.001497, mae: 0.042576, mean_q: 1.164073
 235895/1000000: episode: 2359, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.919, mean reward: 0.579 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.794, 10.156], loss: 0.001468, mae: 0.041674, mean_q: 1.164313
 235995/1000000: episode: 2360, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.446, mean reward: 0.584 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.151], loss: 0.001398, mae: 0.040779, mean_q: 1.161629
 236095/1000000: episode: 2361, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.132, mean reward: 0.581 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.304, 10.122], loss: 0.001491, mae: 0.041529, mean_q: 1.160357
 236195/1000000: episode: 2362, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 63.265, mean reward: 0.633 [0.515, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.001489, mae: 0.042179, mean_q: 1.162885
 236295/1000000: episode: 2363, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.360, mean reward: 0.584 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.913, 10.134], loss: 0.001449, mae: 0.041817, mean_q: 1.163925
 236395/1000000: episode: 2364, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.117, mean reward: 0.591 [0.504, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.041, 10.112], loss: 0.001489, mae: 0.041866, mean_q: 1.163772
 236495/1000000: episode: 2365, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 57.459, mean reward: 0.575 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.294, 10.098], loss: 0.001412, mae: 0.040603, mean_q: 1.161439
 236595/1000000: episode: 2366, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 59.240, mean reward: 0.592 [0.499, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.921, 10.098], loss: 0.001504, mae: 0.042750, mean_q: 1.164410
 236695/1000000: episode: 2367, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.975, mean reward: 0.590 [0.516, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.397, 10.171], loss: 0.001571, mae: 0.043255, mean_q: 1.164142
 236795/1000000: episode: 2368, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.290, mean reward: 0.593 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.578, 10.177], loss: 0.001440, mae: 0.041489, mean_q: 1.165571
 236895/1000000: episode: 2369, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.868, mean reward: 0.579 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.948, 10.221], loss: 0.001548, mae: 0.042902, mean_q: 1.162566
 236995/1000000: episode: 2370, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 59.323, mean reward: 0.593 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.542, 10.125], loss: 0.001572, mae: 0.043336, mean_q: 1.167114
 237095/1000000: episode: 2371, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.763, mean reward: 0.588 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.767, 10.231], loss: 0.001544, mae: 0.043177, mean_q: 1.168792
 237195/1000000: episode: 2372, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 59.868, mean reward: 0.599 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.177, 10.098], loss: 0.001590, mae: 0.043886, mean_q: 1.165601
 237295/1000000: episode: 2373, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 67.017, mean reward: 0.670 [0.515, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.023, 10.387], loss: 0.001524, mae: 0.042845, mean_q: 1.167189
 237395/1000000: episode: 2374, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 61.487, mean reward: 0.615 [0.515, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.901, 10.554], loss: 0.001613, mae: 0.044302, mean_q: 1.170524
 237495/1000000: episode: 2375, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 59.329, mean reward: 0.593 [0.505, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.129, 10.134], loss: 0.001606, mae: 0.043740, mean_q: 1.171096
 237595/1000000: episode: 2376, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.800, mean reward: 0.588 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.622, 10.098], loss: 0.001502, mae: 0.042299, mean_q: 1.167781
 237695/1000000: episode: 2377, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 57.635, mean reward: 0.576 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.576, 10.286], loss: 0.001467, mae: 0.042396, mean_q: 1.168151
 237795/1000000: episode: 2378, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 57.927, mean reward: 0.579 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.860, 10.098], loss: 0.001520, mae: 0.042444, mean_q: 1.164408
 237895/1000000: episode: 2379, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 60.965, mean reward: 0.610 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.568, 10.273], loss: 0.001411, mae: 0.041808, mean_q: 1.171894
 237995/1000000: episode: 2380, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.489, mean reward: 0.595 [0.514, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.776, 10.281], loss: 0.001589, mae: 0.043324, mean_q: 1.170388
 238095/1000000: episode: 2381, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.989, mean reward: 0.580 [0.505, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.356, 10.098], loss: 0.001553, mae: 0.043241, mean_q: 1.171655
 238195/1000000: episode: 2382, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.843, mean reward: 0.588 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.587, 10.098], loss: 0.001470, mae: 0.041934, mean_q: 1.171376
 238295/1000000: episode: 2383, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 56.884, mean reward: 0.569 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.035, 10.145], loss: 0.001588, mae: 0.043592, mean_q: 1.173755
 238395/1000000: episode: 2384, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.328, mean reward: 0.583 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.098], loss: 0.001536, mae: 0.042805, mean_q: 1.170912
 238495/1000000: episode: 2385, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.104, mean reward: 0.571 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.630, 10.098], loss: 0.001581, mae: 0.043844, mean_q: 1.168261
 238595/1000000: episode: 2386, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 61.397, mean reward: 0.614 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.395, 10.098], loss: 0.001331, mae: 0.039992, mean_q: 1.166303
 238695/1000000: episode: 2387, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 57.397, mean reward: 0.574 [0.503, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.808, 10.221], loss: 0.001610, mae: 0.043010, mean_q: 1.172397
 238795/1000000: episode: 2388, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 57.303, mean reward: 0.573 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.516, 10.346], loss: 0.001476, mae: 0.041735, mean_q: 1.166023
 238895/1000000: episode: 2389, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 62.591, mean reward: 0.626 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.406], loss: 0.001432, mae: 0.040979, mean_q: 1.164299
 238995/1000000: episode: 2390, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 59.583, mean reward: 0.596 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.098], loss: 0.001406, mae: 0.040948, mean_q: 1.163923
 239095/1000000: episode: 2391, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 60.366, mean reward: 0.604 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.300, 10.293], loss: 0.001450, mae: 0.041714, mean_q: 1.169352
 239195/1000000: episode: 2392, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.907, mean reward: 0.599 [0.503, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.990, 10.098], loss: 0.001338, mae: 0.040189, mean_q: 1.162585
 239295/1000000: episode: 2393, duration: 0.767s, episode steps: 100, steps per second: 130, episode reward: 57.074, mean reward: 0.571 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.770, 10.257], loss: 0.001478, mae: 0.041784, mean_q: 1.164511
 239395/1000000: episode: 2394, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 59.873, mean reward: 0.599 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.098], loss: 0.001353, mae: 0.040200, mean_q: 1.161982
 239495/1000000: episode: 2395, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 57.677, mean reward: 0.577 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.268, 10.149], loss: 0.001381, mae: 0.041305, mean_q: 1.170785
 239595/1000000: episode: 2396, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 57.801, mean reward: 0.578 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.823, 10.305], loss: 0.001364, mae: 0.040300, mean_q: 1.164237
 239695/1000000: episode: 2397, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 60.437, mean reward: 0.604 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.769, 10.098], loss: 0.001380, mae: 0.040738, mean_q: 1.166556
 239795/1000000: episode: 2398, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 61.695, mean reward: 0.617 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.098], loss: 0.001465, mae: 0.041819, mean_q: 1.168540
 239895/1000000: episode: 2399, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 61.661, mean reward: 0.617 [0.502, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.103], loss: 0.001428, mae: 0.041820, mean_q: 1.171526
 239995/1000000: episode: 2400, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 59.435, mean reward: 0.594 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.379], loss: 0.001525, mae: 0.042775, mean_q: 1.171971
 240095/1000000: episode: 2401, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 61.678, mean reward: 0.617 [0.511, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.852, 10.098], loss: 0.001328, mae: 0.040202, mean_q: 1.171498
 240195/1000000: episode: 2402, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 63.464, mean reward: 0.635 [0.503, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.098], loss: 0.001516, mae: 0.042632, mean_q: 1.174787
 240295/1000000: episode: 2403, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.711, mean reward: 0.597 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.096, 10.140], loss: 0.001429, mae: 0.041311, mean_q: 1.173587
 240395/1000000: episode: 2404, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.764, mean reward: 0.588 [0.503, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.100, 10.317], loss: 0.001318, mae: 0.039870, mean_q: 1.180079
 240495/1000000: episode: 2405, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 59.974, mean reward: 0.600 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.444, 10.314], loss: 0.001429, mae: 0.041846, mean_q: 1.176223
 240595/1000000: episode: 2406, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 58.524, mean reward: 0.585 [0.515, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.915, 10.098], loss: 0.001550, mae: 0.042630, mean_q: 1.173278
 240695/1000000: episode: 2407, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 57.988, mean reward: 0.580 [0.501, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.578, 10.174], loss: 0.001473, mae: 0.041912, mean_q: 1.176268
 240795/1000000: episode: 2408, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 62.110, mean reward: 0.621 [0.503, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.354, 10.251], loss: 0.001509, mae: 0.042566, mean_q: 1.178527
 240895/1000000: episode: 2409, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.730, mean reward: 0.597 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.574, 10.165], loss: 0.001526, mae: 0.043009, mean_q: 1.176546
 240995/1000000: episode: 2410, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.171, mean reward: 0.582 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.784, 10.110], loss: 0.001582, mae: 0.044063, mean_q: 1.180049
 241095/1000000: episode: 2411, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.750, mean reward: 0.588 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.666, 10.098], loss: 0.001516, mae: 0.042468, mean_q: 1.179093
 241195/1000000: episode: 2412, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.666, mean reward: 0.597 [0.512, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.428, 10.098], loss: 0.001593, mae: 0.043620, mean_q: 1.179885
 241295/1000000: episode: 2413, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 60.799, mean reward: 0.608 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.814, 10.098], loss: 0.001508, mae: 0.042616, mean_q: 1.180491
 241395/1000000: episode: 2414, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.803, mean reward: 0.588 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.373], loss: 0.001480, mae: 0.042290, mean_q: 1.180099
 241495/1000000: episode: 2415, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 62.350, mean reward: 0.624 [0.507, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.782, 10.098], loss: 0.001605, mae: 0.043509, mean_q: 1.177794
 241595/1000000: episode: 2416, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 60.641, mean reward: 0.606 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.960, 10.493], loss: 0.001662, mae: 0.043623, mean_q: 1.175052
 241695/1000000: episode: 2417, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 61.409, mean reward: 0.614 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.850, 10.098], loss: 0.001410, mae: 0.041291, mean_q: 1.179918
 241795/1000000: episode: 2418, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 59.643, mean reward: 0.596 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.195, 10.098], loss: 0.001472, mae: 0.041838, mean_q: 1.185058
 241895/1000000: episode: 2419, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.228, mean reward: 0.592 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.146, 10.098], loss: 0.001467, mae: 0.042083, mean_q: 1.181506
 241995/1000000: episode: 2420, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 60.055, mean reward: 0.601 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.346, 10.273], loss: 0.001431, mae: 0.041565, mean_q: 1.184428
 242095/1000000: episode: 2421, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 60.803, mean reward: 0.608 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.365, 10.098], loss: 0.001516, mae: 0.042185, mean_q: 1.181496
 242195/1000000: episode: 2422, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 57.703, mean reward: 0.577 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.578, 10.207], loss: 0.001465, mae: 0.042272, mean_q: 1.180146
 242295/1000000: episode: 2423, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 57.688, mean reward: 0.577 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.488, 10.098], loss: 0.001418, mae: 0.041013, mean_q: 1.179531
 242395/1000000: episode: 2424, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 59.307, mean reward: 0.593 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.708, 10.311], loss: 0.001424, mae: 0.041216, mean_q: 1.180163
 242495/1000000: episode: 2425, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.312, mean reward: 0.593 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.324, 10.098], loss: 0.001332, mae: 0.039609, mean_q: 1.180224
 242595/1000000: episode: 2426, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 59.953, mean reward: 0.600 [0.516, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: 0.001416, mae: 0.040801, mean_q: 1.179450
 242695/1000000: episode: 2427, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.812, mean reward: 0.578 [0.500, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.962, 10.280], loss: 0.001468, mae: 0.041991, mean_q: 1.178559
 242795/1000000: episode: 2428, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.076, mean reward: 0.581 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.854, 10.123], loss: 0.001418, mae: 0.041207, mean_q: 1.177222
 242895/1000000: episode: 2429, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.166, mean reward: 0.592 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.418, 10.208], loss: 0.001504, mae: 0.042784, mean_q: 1.182155
 242995/1000000: episode: 2430, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 60.607, mean reward: 0.606 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.585, 10.107], loss: 0.001531, mae: 0.042255, mean_q: 1.176749
 243095/1000000: episode: 2431, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 59.971, mean reward: 0.600 [0.504, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.101, 10.108], loss: 0.001440, mae: 0.041798, mean_q: 1.178529
 243195/1000000: episode: 2432, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 60.705, mean reward: 0.607 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.627, 10.098], loss: 0.001423, mae: 0.040808, mean_q: 1.178125
 243295/1000000: episode: 2433, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 59.358, mean reward: 0.594 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.039, 10.219], loss: 0.001479, mae: 0.042489, mean_q: 1.181722
 243395/1000000: episode: 2434, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 60.036, mean reward: 0.600 [0.512, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.784, 10.269], loss: 0.001512, mae: 0.041757, mean_q: 1.179543
 243495/1000000: episode: 2435, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 60.122, mean reward: 0.601 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.580, 10.200], loss: 0.001431, mae: 0.041299, mean_q: 1.180129
 243595/1000000: episode: 2436, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 58.750, mean reward: 0.588 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.168], loss: 0.001514, mae: 0.042610, mean_q: 1.183965
 243695/1000000: episode: 2437, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.808, mean reward: 0.578 [0.509, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.767, 10.098], loss: 0.001482, mae: 0.042428, mean_q: 1.181453
 243795/1000000: episode: 2438, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 56.393, mean reward: 0.564 [0.506, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.070, 10.171], loss: 0.001573, mae: 0.043127, mean_q: 1.183059
 243895/1000000: episode: 2439, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.513, mean reward: 0.595 [0.515, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.131], loss: 0.001542, mae: 0.043271, mean_q: 1.179172
 243995/1000000: episode: 2440, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 59.365, mean reward: 0.594 [0.511, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.992, 10.098], loss: 0.001660, mae: 0.044166, mean_q: 1.183079
 244095/1000000: episode: 2441, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 60.282, mean reward: 0.603 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.580, 10.098], loss: 0.001450, mae: 0.041244, mean_q: 1.178814
 244195/1000000: episode: 2442, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 58.128, mean reward: 0.581 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.956, 10.227], loss: 0.001593, mae: 0.043370, mean_q: 1.182142
 244295/1000000: episode: 2443, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 59.271, mean reward: 0.593 [0.513, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.098], loss: 0.001530, mae: 0.043132, mean_q: 1.177157
 244395/1000000: episode: 2444, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 59.016, mean reward: 0.590 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.482, 10.181], loss: 0.001484, mae: 0.041878, mean_q: 1.176185
 244495/1000000: episode: 2445, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.707, mean reward: 0.587 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.207], loss: 0.001501, mae: 0.042114, mean_q: 1.183531
 244595/1000000: episode: 2446, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.299, mean reward: 0.573 [0.504, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.510, 10.216], loss: 0.001440, mae: 0.041010, mean_q: 1.179547
 244695/1000000: episode: 2447, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.394, mean reward: 0.574 [0.501, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.860, 10.134], loss: 0.001429, mae: 0.040861, mean_q: 1.174969
 244795/1000000: episode: 2448, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 58.769, mean reward: 0.588 [0.502, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.030, 10.098], loss: 0.001461, mae: 0.041506, mean_q: 1.176572
 244895/1000000: episode: 2449, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.645, mean reward: 0.586 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.945, 10.246], loss: 0.001428, mae: 0.041179, mean_q: 1.178023
 244995/1000000: episode: 2450, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 60.222, mean reward: 0.602 [0.513, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.788, 10.098], loss: 0.001566, mae: 0.043104, mean_q: 1.178586
 245095/1000000: episode: 2451, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 58.153, mean reward: 0.582 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.360, 10.150], loss: 0.001456, mae: 0.041557, mean_q: 1.176911
 245195/1000000: episode: 2452, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 57.924, mean reward: 0.579 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.566, 10.104], loss: 0.001550, mae: 0.042741, mean_q: 1.170165
 245295/1000000: episode: 2453, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.595, mean reward: 0.576 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.508, 10.098], loss: 0.001529, mae: 0.042260, mean_q: 1.169530
 245395/1000000: episode: 2454, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 62.763, mean reward: 0.628 [0.518, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.635, 10.406], loss: 0.001476, mae: 0.041894, mean_q: 1.174810
 245495/1000000: episode: 2455, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.431, mean reward: 0.574 [0.502, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.189, 10.098], loss: 0.001495, mae: 0.041984, mean_q: 1.173622
 245595/1000000: episode: 2456, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 62.171, mean reward: 0.622 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.640, 10.198], loss: 0.001486, mae: 0.041540, mean_q: 1.175660
 245695/1000000: episode: 2457, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.955, mean reward: 0.600 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.476, 10.098], loss: 0.001451, mae: 0.041452, mean_q: 1.174281
 245795/1000000: episode: 2458, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.503, mean reward: 0.575 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.224], loss: 0.001475, mae: 0.041426, mean_q: 1.168009
 245895/1000000: episode: 2459, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 57.780, mean reward: 0.578 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.741, 10.251], loss: 0.001489, mae: 0.041472, mean_q: 1.176362
 245995/1000000: episode: 2460, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.111, mean reward: 0.571 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.222], loss: 0.001449, mae: 0.041214, mean_q: 1.171666
 246095/1000000: episode: 2461, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.449, mean reward: 0.584 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.242, 10.098], loss: 0.001481, mae: 0.042015, mean_q: 1.170515
 246195/1000000: episode: 2462, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.462, mean reward: 0.575 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.088, 10.168], loss: 0.001474, mae: 0.041841, mean_q: 1.171171
 246295/1000000: episode: 2463, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 57.293, mean reward: 0.573 [0.498, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.520, 10.281], loss: 0.001475, mae: 0.041242, mean_q: 1.167493
 246395/1000000: episode: 2464, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 60.031, mean reward: 0.600 [0.505, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.098], loss: 0.001521, mae: 0.042130, mean_q: 1.170593
 246495/1000000: episode: 2465, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 60.016, mean reward: 0.600 [0.504, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.104, 10.098], loss: 0.001481, mae: 0.041693, mean_q: 1.168635
 246595/1000000: episode: 2466, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 60.112, mean reward: 0.601 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.318, 10.098], loss: 0.001678, mae: 0.043375, mean_q: 1.166938
 246695/1000000: episode: 2467, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 58.397, mean reward: 0.584 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.058, 10.189], loss: 0.001471, mae: 0.041497, mean_q: 1.166214
 246795/1000000: episode: 2468, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 58.174, mean reward: 0.582 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.047, 10.227], loss: 0.001524, mae: 0.042014, mean_q: 1.164138
 246895/1000000: episode: 2469, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 59.545, mean reward: 0.595 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.727, 10.098], loss: 0.001543, mae: 0.042694, mean_q: 1.167067
 246995/1000000: episode: 2470, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 63.830, mean reward: 0.638 [0.521, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.070, 10.264], loss: 0.001618, mae: 0.043085, mean_q: 1.167855
 247095/1000000: episode: 2471, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 61.394, mean reward: 0.614 [0.529, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.654, 10.098], loss: 0.001723, mae: 0.044686, mean_q: 1.171218
 247195/1000000: episode: 2472, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.185, mean reward: 0.572 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.859, 10.105], loss: 0.001695, mae: 0.044800, mean_q: 1.169243
 247295/1000000: episode: 2473, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.160, mean reward: 0.572 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.098], loss: 0.001571, mae: 0.042833, mean_q: 1.171729
 247395/1000000: episode: 2474, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.320, mean reward: 0.583 [0.503, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.098], loss: 0.001604, mae: 0.043119, mean_q: 1.167848
 247495/1000000: episode: 2475, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.333, mean reward: 0.573 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.747, 10.098], loss: 0.001570, mae: 0.043231, mean_q: 1.166658
 247595/1000000: episode: 2476, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 56.552, mean reward: 0.566 [0.502, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.617, 10.238], loss: 0.001468, mae: 0.041352, mean_q: 1.164766
 247695/1000000: episode: 2477, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 62.045, mean reward: 0.620 [0.519, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.972, 10.299], loss: 0.001608, mae: 0.043463, mean_q: 1.168663
 247795/1000000: episode: 2478, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 58.658, mean reward: 0.587 [0.511, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.118], loss: 0.001618, mae: 0.043844, mean_q: 1.170893
 247895/1000000: episode: 2479, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.786, mean reward: 0.578 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.441, 10.098], loss: 0.001562, mae: 0.043004, mean_q: 1.167283
 247995/1000000: episode: 2480, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.908, mean reward: 0.589 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.137], loss: 0.001508, mae: 0.042228, mean_q: 1.162110
 248095/1000000: episode: 2481, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.612, mean reward: 0.576 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.987, 10.098], loss: 0.001454, mae: 0.041676, mean_q: 1.162137
 248195/1000000: episode: 2482, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 62.437, mean reward: 0.624 [0.515, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.060, 10.195], loss: 0.001465, mae: 0.041889, mean_q: 1.166154
 248295/1000000: episode: 2483, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 59.997, mean reward: 0.600 [0.507, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.252], loss: 0.001566, mae: 0.042854, mean_q: 1.165689
 248395/1000000: episode: 2484, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 62.471, mean reward: 0.625 [0.519, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.511, 10.320], loss: 0.001530, mae: 0.041646, mean_q: 1.165079
 248495/1000000: episode: 2485, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 58.303, mean reward: 0.583 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.222, 10.144], loss: 0.001533, mae: 0.042656, mean_q: 1.166453
 248595/1000000: episode: 2486, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.809, mean reward: 0.588 [0.517, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.239], loss: 0.001474, mae: 0.041917, mean_q: 1.167823
 248695/1000000: episode: 2487, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.962, mean reward: 0.580 [0.505, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.111, 10.110], loss: 0.001482, mae: 0.042201, mean_q: 1.162683
 248795/1000000: episode: 2488, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 60.742, mean reward: 0.607 [0.522, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.048, 10.371], loss: 0.001546, mae: 0.042638, mean_q: 1.169451
 248895/1000000: episode: 2489, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 60.659, mean reward: 0.607 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.829, 10.098], loss: 0.001493, mae: 0.042032, mean_q: 1.163574
 248995/1000000: episode: 2490, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.879, mean reward: 0.579 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.598, 10.125], loss: 0.001492, mae: 0.041324, mean_q: 1.164798
 249095/1000000: episode: 2491, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 57.195, mean reward: 0.572 [0.498, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.098], loss: 0.001622, mae: 0.043358, mean_q: 1.168962
 249195/1000000: episode: 2492, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.212, mean reward: 0.582 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.129], loss: 0.001479, mae: 0.042261, mean_q: 1.167637
 249295/1000000: episode: 2493, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.474, mean reward: 0.575 [0.499, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.346, 10.098], loss: 0.001592, mae: 0.043062, mean_q: 1.170269
 249395/1000000: episode: 2494, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.417, mean reward: 0.584 [0.498, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.265, 10.147], loss: 0.001599, mae: 0.043271, mean_q: 1.162844
 249495/1000000: episode: 2495, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.186, mean reward: 0.592 [0.508, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.524, 10.098], loss: 0.001505, mae: 0.041619, mean_q: 1.166747
 249595/1000000: episode: 2496, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 60.294, mean reward: 0.603 [0.507, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.155, 10.272], loss: 0.001545, mae: 0.042988, mean_q: 1.170293
 249695/1000000: episode: 2497, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 59.951, mean reward: 0.600 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.853, 10.241], loss: 0.001547, mae: 0.042374, mean_q: 1.169683
 249795/1000000: episode: 2498, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: 58.998, mean reward: 0.590 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.743, 10.098], loss: 0.001587, mae: 0.043083, mean_q: 1.170104
 249895/1000000: episode: 2499, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.042, mean reward: 0.590 [0.502, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.738, 10.192], loss: 0.001515, mae: 0.041974, mean_q: 1.167341
 249995/1000000: episode: 2500, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.102, mean reward: 0.581 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.098], loss: 0.001693, mae: 0.044315, mean_q: 1.167544
 250095/1000000: episode: 2501, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.247, mean reward: 0.582 [0.521, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.227, 10.181], loss: 0.001545, mae: 0.042768, mean_q: 1.166807
 250195/1000000: episode: 2502, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.981, mean reward: 0.600 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.934, 10.098], loss: 0.001628, mae: 0.044463, mean_q: 1.165736
 250295/1000000: episode: 2503, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 58.951, mean reward: 0.590 [0.500, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.629, 10.098], loss: 0.001603, mae: 0.043901, mean_q: 1.170626
 250395/1000000: episode: 2504, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 60.591, mean reward: 0.606 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.274, 10.158], loss: 0.001575, mae: 0.042585, mean_q: 1.166557
 250495/1000000: episode: 2505, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.897, mean reward: 0.599 [0.518, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.894, 10.399], loss: 0.001471, mae: 0.041666, mean_q: 1.164652
 250595/1000000: episode: 2506, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 58.054, mean reward: 0.581 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.176, 10.133], loss: 0.001522, mae: 0.042588, mean_q: 1.169164
 250695/1000000: episode: 2507, duration: 1.578s, episode steps: 100, steps per second: 63, episode reward: 58.683, mean reward: 0.587 [0.498, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.315, 10.107], loss: 0.001541, mae: 0.042926, mean_q: 1.168365
 250795/1000000: episode: 2508, duration: 1.277s, episode steps: 100, steps per second: 78, episode reward: 57.410, mean reward: 0.574 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.533, 10.108], loss: 0.001590, mae: 0.043811, mean_q: 1.169838
 250895/1000000: episode: 2509, duration: 1.477s, episode steps: 100, steps per second: 68, episode reward: 57.418, mean reward: 0.574 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.564, 10.098], loss: 0.001596, mae: 0.042940, mean_q: 1.167800
 250995/1000000: episode: 2510, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.153, mean reward: 0.582 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.200, 10.098], loss: 0.001491, mae: 0.042034, mean_q: 1.169966
 251095/1000000: episode: 2511, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 61.042, mean reward: 0.610 [0.513, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.268, 10.167], loss: 0.001660, mae: 0.044316, mean_q: 1.167359
 251195/1000000: episode: 2512, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 58.171, mean reward: 0.582 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.115], loss: 0.001591, mae: 0.042844, mean_q: 1.168204
 251295/1000000: episode: 2513, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 58.509, mean reward: 0.585 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.572, 10.098], loss: 0.001541, mae: 0.042546, mean_q: 1.168050
 251395/1000000: episode: 2514, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 60.349, mean reward: 0.603 [0.497, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.905, 10.098], loss: 0.001539, mae: 0.043155, mean_q: 1.171814
 251495/1000000: episode: 2515, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 58.421, mean reward: 0.584 [0.499, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.728, 10.122], loss: 0.001590, mae: 0.043490, mean_q: 1.169972
 251595/1000000: episode: 2516, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 59.133, mean reward: 0.591 [0.505, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.113, 10.098], loss: 0.001590, mae: 0.042843, mean_q: 1.168882
 251695/1000000: episode: 2517, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 60.457, mean reward: 0.605 [0.510, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.152], loss: 0.001598, mae: 0.043124, mean_q: 1.167910
 251795/1000000: episode: 2518, duration: 0.762s, episode steps: 100, steps per second: 131, episode reward: 62.168, mean reward: 0.622 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.281, 10.098], loss: 0.001604, mae: 0.043309, mean_q: 1.168478
 251895/1000000: episode: 2519, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 58.439, mean reward: 0.584 [0.507, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.192], loss: 0.001555, mae: 0.042558, mean_q: 1.165789
 251995/1000000: episode: 2520, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 56.938, mean reward: 0.569 [0.501, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.098], loss: 0.001644, mae: 0.043560, mean_q: 1.167153
 252095/1000000: episode: 2521, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 57.560, mean reward: 0.576 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.331, 10.149], loss: 0.001670, mae: 0.044018, mean_q: 1.166228
 252195/1000000: episode: 2522, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 62.279, mean reward: 0.623 [0.514, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.682, 10.426], loss: 0.001546, mae: 0.042660, mean_q: 1.167101
 252295/1000000: episode: 2523, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 60.101, mean reward: 0.601 [0.507, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.440, 10.176], loss: 0.001614, mae: 0.043464, mean_q: 1.166779
 252395/1000000: episode: 2524, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 57.673, mean reward: 0.577 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.136, 10.098], loss: 0.001588, mae: 0.043344, mean_q: 1.169121
 252495/1000000: episode: 2525, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.160, mean reward: 0.582 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.421, 10.227], loss: 0.001678, mae: 0.043944, mean_q: 1.172913
 252595/1000000: episode: 2526, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 57.550, mean reward: 0.576 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.415, 10.126], loss: 0.001548, mae: 0.042870, mean_q: 1.166540
 252695/1000000: episode: 2527, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.101, mean reward: 0.581 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.245], loss: 0.001467, mae: 0.041582, mean_q: 1.166238
 252795/1000000: episode: 2528, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 58.335, mean reward: 0.583 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.098], loss: 0.001563, mae: 0.043352, mean_q: 1.166016
 252895/1000000: episode: 2529, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 61.014, mean reward: 0.610 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.092, 10.098], loss: 0.001508, mae: 0.042349, mean_q: 1.165963
 252995/1000000: episode: 2530, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 57.553, mean reward: 0.576 [0.503, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.497, 10.098], loss: 0.001697, mae: 0.044238, mean_q: 1.168526
 253095/1000000: episode: 2531, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 57.441, mean reward: 0.574 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.098], loss: 0.001491, mae: 0.042154, mean_q: 1.169078
 253195/1000000: episode: 2532, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 57.929, mean reward: 0.579 [0.511, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.305, 10.230], loss: 0.001601, mae: 0.043355, mean_q: 1.166198
 253295/1000000: episode: 2533, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 56.893, mean reward: 0.569 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.769, 10.116], loss: 0.001506, mae: 0.042217, mean_q: 1.164052
 253395/1000000: episode: 2534, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 59.064, mean reward: 0.591 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.914, 10.190], loss: 0.001524, mae: 0.042087, mean_q: 1.162135
 253495/1000000: episode: 2535, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 61.818, mean reward: 0.618 [0.513, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.219, 10.098], loss: 0.001610, mae: 0.043103, mean_q: 1.169410
 253595/1000000: episode: 2536, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 61.084, mean reward: 0.611 [0.531, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.204, 10.388], loss: 0.001537, mae: 0.042757, mean_q: 1.168100
 253695/1000000: episode: 2537, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 61.873, mean reward: 0.619 [0.499, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.532, 10.327], loss: 0.001637, mae: 0.042373, mean_q: 1.165844
 253795/1000000: episode: 2538, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.349, mean reward: 0.593 [0.512, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.098], loss: 0.001748, mae: 0.044800, mean_q: 1.167782
 253895/1000000: episode: 2539, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 57.949, mean reward: 0.579 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.098], loss: 0.001558, mae: 0.043047, mean_q: 1.168205
 253995/1000000: episode: 2540, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 58.015, mean reward: 0.580 [0.498, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.921, 10.098], loss: 0.001444, mae: 0.041436, mean_q: 1.166020
 254095/1000000: episode: 2541, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 57.946, mean reward: 0.579 [0.515, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.245, 10.098], loss: 0.001572, mae: 0.043283, mean_q: 1.166326
 254195/1000000: episode: 2542, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.609, mean reward: 0.586 [0.515, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.105, 10.287], loss: 0.001542, mae: 0.042584, mean_q: 1.166482
 254295/1000000: episode: 2543, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 60.613, mean reward: 0.606 [0.519, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.634, 10.098], loss: 0.001501, mae: 0.042174, mean_q: 1.167515
 254395/1000000: episode: 2544, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.206, mean reward: 0.582 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.886, 10.098], loss: 0.001512, mae: 0.042329, mean_q: 1.168684
 254495/1000000: episode: 2545, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.568, mean reward: 0.586 [0.502, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.098], loss: 0.001701, mae: 0.044860, mean_q: 1.166841
 254595/1000000: episode: 2546, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.661, mean reward: 0.597 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.098], loss: 0.001874, mae: 0.046000, mean_q: 1.167001
 254695/1000000: episode: 2547, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.228, mean reward: 0.582 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.098], loss: 0.001756, mae: 0.045204, mean_q: 1.163589
 254795/1000000: episode: 2548, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.487, mean reward: 0.585 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.881, 10.098], loss: 0.001649, mae: 0.043658, mean_q: 1.166139
 254895/1000000: episode: 2549, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.622, mean reward: 0.576 [0.512, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.142], loss: 0.001581, mae: 0.043196, mean_q: 1.169792
 254995/1000000: episode: 2550, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 59.431, mean reward: 0.594 [0.503, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.098], loss: 0.001609, mae: 0.043692, mean_q: 1.168506
 255095/1000000: episode: 2551, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.823, mean reward: 0.598 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.320, 10.131], loss: 0.001564, mae: 0.042961, mean_q: 1.167283
 255195/1000000: episode: 2552, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 60.231, mean reward: 0.602 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.052, 10.187], loss: 0.001596, mae: 0.043168, mean_q: 1.166366
 255295/1000000: episode: 2553, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.254, mean reward: 0.593 [0.506, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.032, 10.213], loss: 0.001560, mae: 0.042429, mean_q: 1.169624
 255395/1000000: episode: 2554, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 60.168, mean reward: 0.602 [0.513, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.635, 10.275], loss: 0.001646, mae: 0.043653, mean_q: 1.168788
 255495/1000000: episode: 2555, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 56.963, mean reward: 0.570 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.076, 10.148], loss: 0.001527, mae: 0.042356, mean_q: 1.168676
 255595/1000000: episode: 2556, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 57.997, mean reward: 0.580 [0.503, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.110, 10.098], loss: 0.001601, mae: 0.043444, mean_q: 1.165162
 255695/1000000: episode: 2557, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.626, mean reward: 0.596 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.272, 10.098], loss: 0.001677, mae: 0.043731, mean_q: 1.166936
 255795/1000000: episode: 2558, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.188, mean reward: 0.582 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.993, 10.098], loss: 0.001575, mae: 0.042730, mean_q: 1.167976
 255895/1000000: episode: 2559, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.065, mean reward: 0.581 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.265, 10.151], loss: 0.001577, mae: 0.042761, mean_q: 1.168801
 255995/1000000: episode: 2560, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 59.173, mean reward: 0.592 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.237, 10.098], loss: 0.001780, mae: 0.045116, mean_q: 1.168631
 256095/1000000: episode: 2561, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 58.043, mean reward: 0.580 [0.504, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.483, 10.376], loss: 0.001603, mae: 0.043413, mean_q: 1.168885
 256195/1000000: episode: 2562, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.956, mean reward: 0.600 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.901, 10.098], loss: 0.001609, mae: 0.043063, mean_q: 1.167257
 256295/1000000: episode: 2563, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 61.836, mean reward: 0.618 [0.504, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.956, 10.098], loss: 0.001530, mae: 0.042087, mean_q: 1.164673
 256395/1000000: episode: 2564, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 61.749, mean reward: 0.617 [0.513, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.098], loss: 0.001482, mae: 0.040775, mean_q: 1.167861
 256495/1000000: episode: 2565, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 57.425, mean reward: 0.574 [0.498, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.544, 10.212], loss: 0.001514, mae: 0.042062, mean_q: 1.171641
 256595/1000000: episode: 2566, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.461, mean reward: 0.585 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.601, 10.098], loss: 0.001448, mae: 0.041085, mean_q: 1.165643
 256695/1000000: episode: 2567, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.318, mean reward: 0.593 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.985, 10.441], loss: 0.001498, mae: 0.042001, mean_q: 1.166862
 256795/1000000: episode: 2568, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 59.611, mean reward: 0.596 [0.507, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.899, 10.147], loss: 0.001548, mae: 0.042102, mean_q: 1.167786
 256895/1000000: episode: 2569, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 56.128, mean reward: 0.561 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.521, 10.127], loss: 0.001565, mae: 0.042474, mean_q: 1.166709
 256995/1000000: episode: 2570, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.077, mean reward: 0.581 [0.511, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.098], loss: 0.001605, mae: 0.043280, mean_q: 1.169752
 257095/1000000: episode: 2571, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.474, mean reward: 0.575 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.882, 10.262], loss: 0.001526, mae: 0.042397, mean_q: 1.167001
 257195/1000000: episode: 2572, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 61.886, mean reward: 0.619 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.523, 10.098], loss: 0.001492, mae: 0.041170, mean_q: 1.164770
 257295/1000000: episode: 2573, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 60.002, mean reward: 0.600 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.333, 10.322], loss: 0.001515, mae: 0.041952, mean_q: 1.166075
 257395/1000000: episode: 2574, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 59.120, mean reward: 0.591 [0.503, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.788, 10.355], loss: 0.001554, mae: 0.042626, mean_q: 1.161725
 257495/1000000: episode: 2575, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 58.762, mean reward: 0.588 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.564, 10.136], loss: 0.001483, mae: 0.041012, mean_q: 1.165935
 257595/1000000: episode: 2576, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.085, mean reward: 0.591 [0.500, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.293, 10.098], loss: 0.001653, mae: 0.043656, mean_q: 1.167740
 257695/1000000: episode: 2577, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 57.335, mean reward: 0.573 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.815, 10.229], loss: 0.001569, mae: 0.042006, mean_q: 1.167646
 257795/1000000: episode: 2578, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.120, mean reward: 0.581 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.257, 10.291], loss: 0.001611, mae: 0.043040, mean_q: 1.167843
 257895/1000000: episode: 2579, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.123, mean reward: 0.581 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.288, 10.239], loss: 0.001581, mae: 0.042942, mean_q: 1.165800
 257995/1000000: episode: 2580, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 58.418, mean reward: 0.584 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.189, 10.207], loss: 0.001541, mae: 0.041931, mean_q: 1.167859
 258095/1000000: episode: 2581, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 57.412, mean reward: 0.574 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.335, 10.098], loss: 0.001547, mae: 0.042565, mean_q: 1.167725
 258195/1000000: episode: 2582, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 57.172, mean reward: 0.572 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.178, 10.098], loss: 0.001404, mae: 0.040076, mean_q: 1.162361
 258295/1000000: episode: 2583, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.358, mean reward: 0.584 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.471, 10.182], loss: 0.001573, mae: 0.043064, mean_q: 1.169205
 258395/1000000: episode: 2584, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 59.326, mean reward: 0.593 [0.509, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.098], loss: 0.001725, mae: 0.044535, mean_q: 1.170221
 258495/1000000: episode: 2585, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 66.138, mean reward: 0.661 [0.502, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.449, 10.098], loss: 0.001558, mae: 0.042120, mean_q: 1.166004
 258595/1000000: episode: 2586, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 58.183, mean reward: 0.582 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.809, 10.284], loss: 0.001541, mae: 0.042104, mean_q: 1.166923
 258695/1000000: episode: 2587, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.231, mean reward: 0.592 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.520, 10.322], loss: 0.001469, mae: 0.041568, mean_q: 1.166052
 258795/1000000: episode: 2588, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.459, mean reward: 0.585 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.751, 10.098], loss: 0.001492, mae: 0.041249, mean_q: 1.164878
 258895/1000000: episode: 2589, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 60.860, mean reward: 0.609 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.376, 10.098], loss: 0.001459, mae: 0.040924, mean_q: 1.165905
 258995/1000000: episode: 2590, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.822, mean reward: 0.598 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.437, 10.128], loss: 0.001483, mae: 0.041607, mean_q: 1.168363
 259095/1000000: episode: 2591, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 56.733, mean reward: 0.567 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.470, 10.235], loss: 0.001489, mae: 0.041299, mean_q: 1.166795
 259195/1000000: episode: 2592, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 56.736, mean reward: 0.567 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.044, 10.193], loss: 0.001550, mae: 0.042636, mean_q: 1.169395
 259295/1000000: episode: 2593, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.541, mean reward: 0.585 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.634, 10.163], loss: 0.001575, mae: 0.042672, mean_q: 1.168673
 259395/1000000: episode: 2594, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 59.389, mean reward: 0.594 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.106, 10.233], loss: 0.001584, mae: 0.042551, mean_q: 1.163289
 259495/1000000: episode: 2595, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 60.631, mean reward: 0.606 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.838, 10.098], loss: 0.001514, mae: 0.042227, mean_q: 1.164104
 259595/1000000: episode: 2596, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 61.475, mean reward: 0.615 [0.508, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.530, 10.098], loss: 0.001373, mae: 0.040373, mean_q: 1.162827
 259695/1000000: episode: 2597, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 57.427, mean reward: 0.574 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.787, 10.098], loss: 0.001397, mae: 0.040532, mean_q: 1.165218
 259795/1000000: episode: 2598, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.946, mean reward: 0.599 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.892, 10.098], loss: 0.001472, mae: 0.041675, mean_q: 1.168302
 259895/1000000: episode: 2599, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 60.058, mean reward: 0.601 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.708, 10.098], loss: 0.001398, mae: 0.040912, mean_q: 1.167426
 259995/1000000: episode: 2600, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 63.259, mean reward: 0.633 [0.505, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.858, 10.098], loss: 0.001494, mae: 0.042352, mean_q: 1.168104
 260095/1000000: episode: 2601, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 58.050, mean reward: 0.581 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.685, 10.098], loss: 0.001506, mae: 0.042682, mean_q: 1.171659
 260195/1000000: episode: 2602, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 59.872, mean reward: 0.599 [0.500, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.069, 10.098], loss: 0.001504, mae: 0.041952, mean_q: 1.168474
 260295/1000000: episode: 2603, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 59.214, mean reward: 0.592 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.225, 10.335], loss: 0.001592, mae: 0.042899, mean_q: 1.167988
 260395/1000000: episode: 2604, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.682, mean reward: 0.587 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.333], loss: 0.001431, mae: 0.041447, mean_q: 1.170694
 260495/1000000: episode: 2605, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 59.855, mean reward: 0.599 [0.509, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.529, 10.098], loss: 0.001451, mae: 0.041495, mean_q: 1.168677
 260595/1000000: episode: 2606, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 58.733, mean reward: 0.587 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.274, 10.222], loss: 0.001442, mae: 0.041307, mean_q: 1.166635
 260695/1000000: episode: 2607, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 58.476, mean reward: 0.585 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.098], loss: 0.001493, mae: 0.042011, mean_q: 1.169046
 260795/1000000: episode: 2608, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.635, mean reward: 0.576 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.814, 10.098], loss: 0.001367, mae: 0.040435, mean_q: 1.166371
 260895/1000000: episode: 2609, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 60.770, mean reward: 0.608 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.639, 10.166], loss: 0.001606, mae: 0.043054, mean_q: 1.172054
 260995/1000000: episode: 2610, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 56.905, mean reward: 0.569 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.068, 10.240], loss: 0.001503, mae: 0.041732, mean_q: 1.170206
 261095/1000000: episode: 2611, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 60.045, mean reward: 0.600 [0.510, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.895, 10.098], loss: 0.001563, mae: 0.042458, mean_q: 1.174936
 261195/1000000: episode: 2612, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.458, mean reward: 0.585 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.076, 10.181], loss: 0.001515, mae: 0.041929, mean_q: 1.171253
 261295/1000000: episode: 2613, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 59.120, mean reward: 0.591 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.785, 10.321], loss: 0.001485, mae: 0.041905, mean_q: 1.171955
 261395/1000000: episode: 2614, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 58.050, mean reward: 0.580 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.183, 10.187], loss: 0.001531, mae: 0.042541, mean_q: 1.169163
 261495/1000000: episode: 2615, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.094, mean reward: 0.591 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.365, 10.098], loss: 0.001607, mae: 0.043601, mean_q: 1.169842
 261595/1000000: episode: 2616, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.089, mean reward: 0.581 [0.499, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.190], loss: 0.001484, mae: 0.041764, mean_q: 1.172431
 261695/1000000: episode: 2617, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.748, mean reward: 0.577 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.724, 10.141], loss: 0.001580, mae: 0.042921, mean_q: 1.167385
 261795/1000000: episode: 2618, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 58.110, mean reward: 0.581 [0.508, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.453, 10.415], loss: 0.001444, mae: 0.041358, mean_q: 1.162816
 261895/1000000: episode: 2619, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.941, mean reward: 0.589 [0.516, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.403, 10.125], loss: 0.001370, mae: 0.040273, mean_q: 1.164793
 261995/1000000: episode: 2620, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 58.332, mean reward: 0.583 [0.514, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.971, 10.098], loss: 0.001472, mae: 0.041643, mean_q: 1.165526
 262095/1000000: episode: 2621, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.922, mean reward: 0.589 [0.507, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.518, 10.122], loss: 0.001549, mae: 0.043023, mean_q: 1.168527
 262195/1000000: episode: 2622, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 61.296, mean reward: 0.613 [0.527, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.775, 10.098], loss: 0.001518, mae: 0.042037, mean_q: 1.167456
 262295/1000000: episode: 2623, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 58.079, mean reward: 0.581 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.159], loss: 0.001454, mae: 0.040986, mean_q: 1.168893
 262395/1000000: episode: 2624, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.574, mean reward: 0.576 [0.500, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.138, 10.271], loss: 0.001495, mae: 0.041966, mean_q: 1.165795
 262495/1000000: episode: 2625, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 62.640, mean reward: 0.626 [0.517, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.580, 10.098], loss: 0.001420, mae: 0.041215, mean_q: 1.169668
 262595/1000000: episode: 2626, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 58.021, mean reward: 0.580 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.098], loss: 0.001520, mae: 0.041967, mean_q: 1.167275
 262695/1000000: episode: 2627, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.200, mean reward: 0.582 [0.505, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.015, 10.098], loss: 0.001479, mae: 0.041496, mean_q: 1.167207
 262795/1000000: episode: 2628, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 58.891, mean reward: 0.589 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.002, 10.271], loss: 0.001523, mae: 0.042276, mean_q: 1.170628
 262895/1000000: episode: 2629, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 59.276, mean reward: 0.593 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.098], loss: 0.001481, mae: 0.042058, mean_q: 1.167248
 262995/1000000: episode: 2630, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 57.820, mean reward: 0.578 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.013, 10.098], loss: 0.001573, mae: 0.043234, mean_q: 1.168251
 263095/1000000: episode: 2631, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 57.684, mean reward: 0.577 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.098], loss: 0.001382, mae: 0.040305, mean_q: 1.167706
 263195/1000000: episode: 2632, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 57.377, mean reward: 0.574 [0.497, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.561, 10.114], loss: 0.001512, mae: 0.042202, mean_q: 1.170122
 263295/1000000: episode: 2633, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 61.133, mean reward: 0.611 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.093, 10.098], loss: 0.001511, mae: 0.042068, mean_q: 1.171644
 263395/1000000: episode: 2634, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.729, mean reward: 0.597 [0.506, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.697, 10.238], loss: 0.001475, mae: 0.041917, mean_q: 1.173345
 263495/1000000: episode: 2635, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.246, mean reward: 0.582 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.436], loss: 0.001522, mae: 0.042789, mean_q: 1.167873
 263595/1000000: episode: 2636, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.509, mean reward: 0.605 [0.506, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.891, 10.427], loss: 0.001600, mae: 0.043390, mean_q: 1.167851
 263695/1000000: episode: 2637, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 62.368, mean reward: 0.624 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.949, 10.098], loss: 0.001496, mae: 0.042193, mean_q: 1.168391
 263795/1000000: episode: 2638, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.765, mean reward: 0.598 [0.504, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.965, 10.269], loss: 0.001583, mae: 0.042903, mean_q: 1.168397
 263895/1000000: episode: 2639, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 59.699, mean reward: 0.597 [0.500, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.443, 10.385], loss: 0.001520, mae: 0.042021, mean_q: 1.171116
 263995/1000000: episode: 2640, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: 57.526, mean reward: 0.575 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.574, 10.098], loss: 0.001523, mae: 0.042043, mean_q: 1.168717
 264095/1000000: episode: 2641, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 59.486, mean reward: 0.595 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.624, 10.267], loss: 0.001544, mae: 0.042492, mean_q: 1.169314
 264195/1000000: episode: 2642, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.370, mean reward: 0.584 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.632, 10.178], loss: 0.001515, mae: 0.042172, mean_q: 1.172359
 264295/1000000: episode: 2643, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 57.377, mean reward: 0.574 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.408, 10.287], loss: 0.001554, mae: 0.042933, mean_q: 1.173421
 264395/1000000: episode: 2644, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 60.029, mean reward: 0.600 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.212], loss: 0.001575, mae: 0.043057, mean_q: 1.173896
 264495/1000000: episode: 2645, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 59.321, mean reward: 0.593 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.098], loss: 0.001600, mae: 0.043201, mean_q: 1.172482
 264595/1000000: episode: 2646, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.911, mean reward: 0.589 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.151, 10.098], loss: 0.001516, mae: 0.042219, mean_q: 1.171619
 264695/1000000: episode: 2647, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.674, mean reward: 0.597 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.092, 10.357], loss: 0.001475, mae: 0.041384, mean_q: 1.166079
 264795/1000000: episode: 2648, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: 58.715, mean reward: 0.587 [0.510, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.371, 10.137], loss: 0.001394, mae: 0.040390, mean_q: 1.168313
 264895/1000000: episode: 2649, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.882, mean reward: 0.599 [0.511, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.208, 10.098], loss: 0.001514, mae: 0.041938, mean_q: 1.170047
 264995/1000000: episode: 2650, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 59.065, mean reward: 0.591 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.448, 10.098], loss: 0.001491, mae: 0.041556, mean_q: 1.164405
 265095/1000000: episode: 2651, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 64.601, mean reward: 0.646 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.872, 10.098], loss: 0.001414, mae: 0.040693, mean_q: 1.172269
 265195/1000000: episode: 2652, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 59.283, mean reward: 0.593 [0.507, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.864, 10.361], loss: 0.001348, mae: 0.039659, mean_q: 1.165808
 265295/1000000: episode: 2653, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 59.447, mean reward: 0.594 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.310, 10.141], loss: 0.001477, mae: 0.041626, mean_q: 1.171454
 265395/1000000: episode: 2654, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 60.736, mean reward: 0.607 [0.501, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.908, 10.124], loss: 0.001546, mae: 0.043017, mean_q: 1.167671
 265495/1000000: episode: 2655, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.936, mean reward: 0.609 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.332, 10.098], loss: 0.001470, mae: 0.041365, mean_q: 1.167699
 265595/1000000: episode: 2656, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 57.929, mean reward: 0.579 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.525, 10.108], loss: 0.001468, mae: 0.042028, mean_q: 1.170401
 265695/1000000: episode: 2657, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.983, mean reward: 0.580 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.417, 10.196], loss: 0.001540, mae: 0.042326, mean_q: 1.174162
 265795/1000000: episode: 2658, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 62.270, mean reward: 0.623 [0.515, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.098], loss: 0.001451, mae: 0.041900, mean_q: 1.172126
 265895/1000000: episode: 2659, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 61.378, mean reward: 0.614 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.338, 10.098], loss: 0.001424, mae: 0.041072, mean_q: 1.171549
 265995/1000000: episode: 2660, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 59.125, mean reward: 0.591 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.412], loss: 0.001534, mae: 0.042236, mean_q: 1.173002
 266095/1000000: episode: 2661, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 56.788, mean reward: 0.568 [0.503, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.957, 10.098], loss: 0.001370, mae: 0.040518, mean_q: 1.171341
 266195/1000000: episode: 2662, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 57.343, mean reward: 0.573 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.511, 10.098], loss: 0.001513, mae: 0.042468, mean_q: 1.172117
 266295/1000000: episode: 2663, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 59.206, mean reward: 0.592 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.153, 10.111], loss: 0.001454, mae: 0.041515, mean_q: 1.174641
 266395/1000000: episode: 2664, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.750, mean reward: 0.588 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.417, 10.233], loss: 0.001525, mae: 0.042474, mean_q: 1.172282
 266495/1000000: episode: 2665, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.916, mean reward: 0.599 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.252, 10.098], loss: 0.001490, mae: 0.041843, mean_q: 1.173259
 266595/1000000: episode: 2666, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 57.414, mean reward: 0.574 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.995, 10.098], loss: 0.001390, mae: 0.040000, mean_q: 1.171046
 266695/1000000: episode: 2667, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 58.190, mean reward: 0.582 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.177, 10.172], loss: 0.001370, mae: 0.040129, mean_q: 1.171814
 266795/1000000: episode: 2668, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 57.730, mean reward: 0.577 [0.511, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.112], loss: 0.001496, mae: 0.042102, mean_q: 1.174525
 266895/1000000: episode: 2669, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 58.750, mean reward: 0.587 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.507, 10.161], loss: 0.001460, mae: 0.041799, mean_q: 1.174757
 266995/1000000: episode: 2670, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 57.108, mean reward: 0.571 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.809, 10.098], loss: 0.001410, mae: 0.040408, mean_q: 1.169329
 267095/1000000: episode: 2671, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 58.548, mean reward: 0.585 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.130, 10.319], loss: 0.001417, mae: 0.040856, mean_q: 1.170504
 267195/1000000: episode: 2672, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 56.290, mean reward: 0.563 [0.501, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.861, 10.266], loss: 0.001336, mae: 0.039931, mean_q: 1.168923
 267295/1000000: episode: 2673, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 63.123, mean reward: 0.631 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.857, 10.405], loss: 0.001429, mae: 0.041157, mean_q: 1.170538
 267395/1000000: episode: 2674, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 60.012, mean reward: 0.600 [0.525, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.695, 10.098], loss: 0.001491, mae: 0.041933, mean_q: 1.175642
 267495/1000000: episode: 2675, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 56.913, mean reward: 0.569 [0.500, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.170], loss: 0.001411, mae: 0.040501, mean_q: 1.171945
 267595/1000000: episode: 2676, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.890, mean reward: 0.589 [0.508, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.600, 10.098], loss: 0.001415, mae: 0.041069, mean_q: 1.170088
 267695/1000000: episode: 2677, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 62.696, mean reward: 0.627 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.502, 10.098], loss: 0.001435, mae: 0.041372, mean_q: 1.169539
 267795/1000000: episode: 2678, duration: 0.922s, episode steps: 100, steps per second: 109, episode reward: 58.696, mean reward: 0.587 [0.502, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.426, 10.098], loss: 0.001388, mae: 0.040357, mean_q: 1.173061
 267895/1000000: episode: 2679, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: 58.097, mean reward: 0.581 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.439, 10.207], loss: 0.001474, mae: 0.041414, mean_q: 1.172954
 267995/1000000: episode: 2680, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 60.021, mean reward: 0.600 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.487, 10.348], loss: 0.001420, mae: 0.040558, mean_q: 1.173364
 268095/1000000: episode: 2681, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 58.643, mean reward: 0.586 [0.498, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.954, 10.098], loss: 0.001430, mae: 0.040914, mean_q: 1.171178
 268195/1000000: episode: 2682, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.237, mean reward: 0.582 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.987, 10.098], loss: 0.001401, mae: 0.040596, mean_q: 1.172383
 268295/1000000: episode: 2683, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 57.833, mean reward: 0.578 [0.510, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.543, 10.098], loss: 0.001470, mae: 0.041639, mean_q: 1.170213
 268395/1000000: episode: 2684, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 59.206, mean reward: 0.592 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.747, 10.244], loss: 0.001348, mae: 0.039346, mean_q: 1.170243
 268495/1000000: episode: 2685, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 59.309, mean reward: 0.593 [0.499, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.535, 10.098], loss: 0.001476, mae: 0.041146, mean_q: 1.173846
 268595/1000000: episode: 2686, duration: 0.766s, episode steps: 100, steps per second: 131, episode reward: 56.860, mean reward: 0.569 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.484, 10.098], loss: 0.001365, mae: 0.040195, mean_q: 1.169722
 268695/1000000: episode: 2687, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.116, mean reward: 0.591 [0.501, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.814, 10.098], loss: 0.001523, mae: 0.042111, mean_q: 1.170764
 268795/1000000: episode: 2688, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 59.801, mean reward: 0.598 [0.508, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.141, 10.104], loss: 0.001470, mae: 0.041762, mean_q: 1.166499
 268895/1000000: episode: 2689, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 57.654, mean reward: 0.577 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.173], loss: 0.001418, mae: 0.041150, mean_q: 1.167360
 268995/1000000: episode: 2690, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.505, mean reward: 0.585 [0.504, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.589, 10.167], loss: 0.001418, mae: 0.040759, mean_q: 1.169339
 269095/1000000: episode: 2691, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.100, mean reward: 0.581 [0.513, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.205], loss: 0.001335, mae: 0.039896, mean_q: 1.168326
 269195/1000000: episode: 2692, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.208, mean reward: 0.582 [0.512, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.688, 10.326], loss: 0.001427, mae: 0.040941, mean_q: 1.165868
 269295/1000000: episode: 2693, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.851, mean reward: 0.599 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.042, 10.354], loss: 0.001466, mae: 0.041236, mean_q: 1.170249
 269395/1000000: episode: 2694, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 58.367, mean reward: 0.584 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.215, 10.280], loss: 0.001440, mae: 0.040794, mean_q: 1.168720
 269495/1000000: episode: 2695, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 58.879, mean reward: 0.589 [0.513, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.149, 10.212], loss: 0.001421, mae: 0.040662, mean_q: 1.168846
 269595/1000000: episode: 2696, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.076, mean reward: 0.591 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.814, 10.099], loss: 0.001382, mae: 0.040244, mean_q: 1.166740
 269695/1000000: episode: 2697, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 59.943, mean reward: 0.599 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.271], loss: 0.001402, mae: 0.040694, mean_q: 1.167801
 269795/1000000: episode: 2698, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 59.090, mean reward: 0.591 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.908, 10.098], loss: 0.001455, mae: 0.041156, mean_q: 1.167778
 269895/1000000: episode: 2699, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 59.002, mean reward: 0.590 [0.504, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.574, 10.232], loss: 0.001501, mae: 0.041614, mean_q: 1.169434
 269995/1000000: episode: 2700, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.131, mean reward: 0.581 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.257, 10.166], loss: 0.001454, mae: 0.041463, mean_q: 1.168955
 270095/1000000: episode: 2701, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 58.568, mean reward: 0.586 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.986, 10.098], loss: 0.001512, mae: 0.041628, mean_q: 1.162374
 270195/1000000: episode: 2702, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 61.086, mean reward: 0.611 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.886, 10.133], loss: 0.001535, mae: 0.042259, mean_q: 1.162106
 270295/1000000: episode: 2703, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 60.228, mean reward: 0.602 [0.523, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.403], loss: 0.001491, mae: 0.041513, mean_q: 1.164127
 270395/1000000: episode: 2704, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 57.287, mean reward: 0.573 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.450, 10.098], loss: 0.001442, mae: 0.041001, mean_q: 1.167853
 270495/1000000: episode: 2705, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 59.662, mean reward: 0.597 [0.509, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.194, 10.098], loss: 0.001556, mae: 0.043381, mean_q: 1.165481
 270595/1000000: episode: 2706, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 58.082, mean reward: 0.581 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.360, 10.098], loss: 0.001511, mae: 0.041943, mean_q: 1.161787
 270695/1000000: episode: 2707, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 60.198, mean reward: 0.602 [0.501, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.914, 10.098], loss: 0.001421, mae: 0.040734, mean_q: 1.166090
 270795/1000000: episode: 2708, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 56.161, mean reward: 0.562 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.881, 10.127], loss: 0.001587, mae: 0.042780, mean_q: 1.166571
 270895/1000000: episode: 2709, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 58.949, mean reward: 0.589 [0.510, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.956, 10.214], loss: 0.001477, mae: 0.041635, mean_q: 1.161334
 270995/1000000: episode: 2710, duration: 1.480s, episode steps: 100, steps per second: 68, episode reward: 58.478, mean reward: 0.585 [0.513, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.468, 10.107], loss: 0.001469, mae: 0.041495, mean_q: 1.161875
 271095/1000000: episode: 2711, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 57.538, mean reward: 0.575 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.826, 10.119], loss: 0.001507, mae: 0.042364, mean_q: 1.164605
 271195/1000000: episode: 2712, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 56.364, mean reward: 0.564 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.156], loss: 0.001549, mae: 0.042323, mean_q: 1.163505
 271295/1000000: episode: 2713, duration: 1.530s, episode steps: 100, steps per second: 65, episode reward: 62.094, mean reward: 0.621 [0.513, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.222], loss: 0.001547, mae: 0.042197, mean_q: 1.164108
 271395/1000000: episode: 2714, duration: 1.806s, episode steps: 100, steps per second: 55, episode reward: 60.893, mean reward: 0.609 [0.509, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.609, 10.475], loss: 0.001456, mae: 0.041391, mean_q: 1.160880
 271495/1000000: episode: 2715, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 62.276, mean reward: 0.623 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.348, 10.098], loss: 0.001562, mae: 0.042894, mean_q: 1.166001
 271595/1000000: episode: 2716, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 59.065, mean reward: 0.591 [0.502, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.840, 10.098], loss: 0.001524, mae: 0.042386, mean_q: 1.166180
 271695/1000000: episode: 2717, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.651, mean reward: 0.597 [0.507, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.379, 10.383], loss: 0.001528, mae: 0.042265, mean_q: 1.163219
 271795/1000000: episode: 2718, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.691, mean reward: 0.577 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.098], loss: 0.001611, mae: 0.043174, mean_q: 1.169201
 271895/1000000: episode: 2719, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 60.408, mean reward: 0.604 [0.508, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.669, 10.130], loss: 0.001612, mae: 0.043252, mean_q: 1.166543
 271995/1000000: episode: 2720, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 57.971, mean reward: 0.580 [0.498, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.273], loss: 0.001557, mae: 0.042886, mean_q: 1.168388
 272095/1000000: episode: 2721, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 59.686, mean reward: 0.597 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.098], loss: 0.001514, mae: 0.042105, mean_q: 1.168693
 272195/1000000: episode: 2722, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 60.198, mean reward: 0.602 [0.519, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.879, 10.427], loss: 0.001556, mae: 0.042589, mean_q: 1.168655
 272295/1000000: episode: 2723, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 57.974, mean reward: 0.580 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.051, 10.155], loss: 0.001541, mae: 0.042206, mean_q: 1.165713
 272395/1000000: episode: 2724, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 60.030, mean reward: 0.600 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.363, 10.437], loss: 0.001521, mae: 0.041891, mean_q: 1.165954
 272495/1000000: episode: 2725, duration: 0.922s, episode steps: 100, steps per second: 109, episode reward: 60.489, mean reward: 0.605 [0.506, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.725, 10.098], loss: 0.001568, mae: 0.043195, mean_q: 1.167065
 272595/1000000: episode: 2726, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 61.262, mean reward: 0.613 [0.510, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.875, 10.098], loss: 0.001506, mae: 0.042252, mean_q: 1.169771
 272695/1000000: episode: 2727, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.955, mean reward: 0.590 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.148], loss: 0.001599, mae: 0.043684, mean_q: 1.168437
 272795/1000000: episode: 2728, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 58.921, mean reward: 0.589 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.186], loss: 0.001526, mae: 0.042382, mean_q: 1.169181
 272895/1000000: episode: 2729, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 57.306, mean reward: 0.573 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.031, 10.098], loss: 0.001587, mae: 0.043744, mean_q: 1.168842
 272995/1000000: episode: 2730, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.610, mean reward: 0.586 [0.499, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.324, 10.107], loss: 0.001637, mae: 0.044415, mean_q: 1.166823
 273095/1000000: episode: 2731, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.593, mean reward: 0.586 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.732, 10.098], loss: 0.001609, mae: 0.043508, mean_q: 1.165205
 273195/1000000: episode: 2732, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.549, mean reward: 0.575 [0.511, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.098], loss: 0.001528, mae: 0.042458, mean_q: 1.167431
 273295/1000000: episode: 2733, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 61.342, mean reward: 0.613 [0.502, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.314, 10.403], loss: 0.001609, mae: 0.043787, mean_q: 1.164818
 273395/1000000: episode: 2734, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.557, mean reward: 0.606 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.409, 10.373], loss: 0.001633, mae: 0.043579, mean_q: 1.168142
 273495/1000000: episode: 2735, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 60.523, mean reward: 0.605 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.521, 10.128], loss: 0.001580, mae: 0.043137, mean_q: 1.168207
 273595/1000000: episode: 2736, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 60.414, mean reward: 0.604 [0.514, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.891, 10.419], loss: 0.001479, mae: 0.041656, mean_q: 1.168878
 273695/1000000: episode: 2737, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 59.469, mean reward: 0.595 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.098], loss: 0.001622, mae: 0.043721, mean_q: 1.170122
 273795/1000000: episode: 2738, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 61.298, mean reward: 0.613 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.795, 10.098], loss: 0.001416, mae: 0.041255, mean_q: 1.171276
 273895/1000000: episode: 2739, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 56.667, mean reward: 0.567 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.524, 10.098], loss: 0.001497, mae: 0.041916, mean_q: 1.171933
 273995/1000000: episode: 2740, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 61.033, mean reward: 0.610 [0.511, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.915, 10.371], loss: 0.001508, mae: 0.042466, mean_q: 1.171073
 274095/1000000: episode: 2741, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 57.923, mean reward: 0.579 [0.501, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.178, 10.116], loss: 0.001659, mae: 0.044635, mean_q: 1.174684
 274195/1000000: episode: 2742, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 65.196, mean reward: 0.652 [0.501, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.090, 10.443], loss: 0.001536, mae: 0.042224, mean_q: 1.173848
 274295/1000000: episode: 2743, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 56.929, mean reward: 0.569 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.634, 10.231], loss: 0.001517, mae: 0.042235, mean_q: 1.176819
 274395/1000000: episode: 2744, duration: 0.763s, episode steps: 100, steps per second: 131, episode reward: 59.119, mean reward: 0.591 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.616, 10.098], loss: 0.001507, mae: 0.042576, mean_q: 1.174784
 274495/1000000: episode: 2745, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.429, mean reward: 0.574 [0.506, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.573, 10.098], loss: 0.001572, mae: 0.043324, mean_q: 1.172914
 274595/1000000: episode: 2746, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 60.143, mean reward: 0.601 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.542, 10.098], loss: 0.001492, mae: 0.041538, mean_q: 1.172407
 274695/1000000: episode: 2747, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 59.389, mean reward: 0.594 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.697, 10.222], loss: 0.001470, mae: 0.041853, mean_q: 1.172380
 274795/1000000: episode: 2748, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 65.896, mean reward: 0.659 [0.503, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.623, 10.178], loss: 0.001629, mae: 0.043767, mean_q: 1.171064
 274895/1000000: episode: 2749, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.235, mean reward: 0.572 [0.510, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.841, 10.205], loss: 0.001604, mae: 0.043758, mean_q: 1.177946
 274995/1000000: episode: 2750, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 60.048, mean reward: 0.600 [0.498, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.107], loss: 0.001533, mae: 0.042636, mean_q: 1.174869
 275095/1000000: episode: 2751, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.067, mean reward: 0.571 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.836, 10.106], loss: 0.001486, mae: 0.042210, mean_q: 1.176075
 275195/1000000: episode: 2752, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.506, mean reward: 0.595 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.293, 10.098], loss: 0.001485, mae: 0.041788, mean_q: 1.181617
 275295/1000000: episode: 2753, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 65.108, mean reward: 0.651 [0.505, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.191, 10.098], loss: 0.001594, mae: 0.043110, mean_q: 1.177108
 275395/1000000: episode: 2754, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 60.425, mean reward: 0.604 [0.507, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.321, 10.098], loss: 0.001567, mae: 0.041974, mean_q: 1.176074
 275495/1000000: episode: 2755, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.704, mean reward: 0.587 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.115], loss: 0.001653, mae: 0.044356, mean_q: 1.182972
 275595/1000000: episode: 2756, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 62.061, mean reward: 0.621 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.891, 10.356], loss: 0.001572, mae: 0.043261, mean_q: 1.179981
 275695/1000000: episode: 2757, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.312, mean reward: 0.573 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.584, 10.098], loss: 0.001658, mae: 0.044029, mean_q: 1.182033
 275795/1000000: episode: 2758, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 56.952, mean reward: 0.570 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.564, 10.268], loss: 0.001622, mae: 0.043409, mean_q: 1.181619
 275895/1000000: episode: 2759, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 58.019, mean reward: 0.580 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.337, 10.098], loss: 0.001652, mae: 0.043880, mean_q: 1.180353
 275995/1000000: episode: 2760, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 61.683, mean reward: 0.617 [0.498, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.843, 10.338], loss: 0.001432, mae: 0.041230, mean_q: 1.177828
 276095/1000000: episode: 2761, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 57.348, mean reward: 0.573 [0.509, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.852, 10.140], loss: 0.001579, mae: 0.042526, mean_q: 1.179264
 276195/1000000: episode: 2762, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 58.889, mean reward: 0.589 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.544, 10.126], loss: 0.001455, mae: 0.041142, mean_q: 1.178406
 276295/1000000: episode: 2763, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 56.668, mean reward: 0.567 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.670, 10.174], loss: 0.001521, mae: 0.042480, mean_q: 1.180846
 276395/1000000: episode: 2764, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.602, mean reward: 0.576 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.477, 10.136], loss: 0.001464, mae: 0.041254, mean_q: 1.182677
 276495/1000000: episode: 2765, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 56.994, mean reward: 0.570 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.154, 10.098], loss: 0.001557, mae: 0.042954, mean_q: 1.176923
 276595/1000000: episode: 2766, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 57.817, mean reward: 0.578 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.865, 10.113], loss: 0.001514, mae: 0.042233, mean_q: 1.175889
 276695/1000000: episode: 2767, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.799, mean reward: 0.578 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.358, 10.098], loss: 0.001448, mae: 0.041323, mean_q: 1.173385
 276795/1000000: episode: 2768, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 57.633, mean reward: 0.576 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.956, 10.205], loss: 0.001394, mae: 0.040017, mean_q: 1.177070
 276895/1000000: episode: 2769, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 58.810, mean reward: 0.588 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.779, 10.098], loss: 0.001439, mae: 0.041389, mean_q: 1.175053
 276995/1000000: episode: 2770, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 57.582, mean reward: 0.576 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.228, 10.098], loss: 0.001416, mae: 0.040844, mean_q: 1.173896
 277095/1000000: episode: 2771, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.034, mean reward: 0.590 [0.504, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.337, 10.098], loss: 0.001417, mae: 0.040626, mean_q: 1.170925
 277195/1000000: episode: 2772, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.285, mean reward: 0.593 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.703, 10.098], loss: 0.001411, mae: 0.040918, mean_q: 1.174389
 277295/1000000: episode: 2773, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 57.712, mean reward: 0.577 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.408, 10.119], loss: 0.001444, mae: 0.040956, mean_q: 1.172275
 277395/1000000: episode: 2774, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 58.708, mean reward: 0.587 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.754, 10.111], loss: 0.001385, mae: 0.040316, mean_q: 1.169784
 277495/1000000: episode: 2775, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.846, mean reward: 0.598 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.596, 10.339], loss: 0.001373, mae: 0.040814, mean_q: 1.171427
 277595/1000000: episode: 2776, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.058, mean reward: 0.581 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.520, 10.098], loss: 0.001403, mae: 0.040692, mean_q: 1.169534
 277695/1000000: episode: 2777, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 56.538, mean reward: 0.565 [0.498, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.422, 10.098], loss: 0.001421, mae: 0.040774, mean_q: 1.172143
 277795/1000000: episode: 2778, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 60.771, mean reward: 0.608 [0.507, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.142, 10.396], loss: 0.001439, mae: 0.041482, mean_q: 1.170758
 277895/1000000: episode: 2779, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.797, mean reward: 0.578 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.001410, mae: 0.040613, mean_q: 1.170017
 277995/1000000: episode: 2780, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 59.671, mean reward: 0.597 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.447, 10.289], loss: 0.001412, mae: 0.041008, mean_q: 1.164162
 278086/1000000: episode: 2781, duration: 0.764s, episode steps: 91, steps per second: 119, episode reward: 61.146, mean reward: 0.672 [0.512, 1.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.346 [-0.843, 9.864], loss: 0.001519, mae: 0.042265, mean_q: 1.172935
 278186/1000000: episode: 2782, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 62.390, mean reward: 0.624 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.686, 10.438], loss: 0.001356, mae: 0.039791, mean_q: 1.174008
 278286/1000000: episode: 2783, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 59.862, mean reward: 0.599 [0.511, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.093, 10.098], loss: 0.001510, mae: 0.040787, mean_q: 1.170704
 278386/1000000: episode: 2784, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.592, mean reward: 0.586 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.838, 10.275], loss: 0.001370, mae: 0.040820, mean_q: 1.172630
 278486/1000000: episode: 2785, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.472, mean reward: 0.585 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.796, 10.287], loss: 0.001423, mae: 0.040571, mean_q: 1.176034
 278586/1000000: episode: 2786, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.794, mean reward: 0.608 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.458, 10.138], loss: 0.001450, mae: 0.041240, mean_q: 1.172047
 278686/1000000: episode: 2787, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 57.272, mean reward: 0.573 [0.510, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.263, 10.098], loss: 0.001369, mae: 0.040692, mean_q: 1.176873
 278786/1000000: episode: 2788, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.171, mean reward: 0.572 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.539, 10.098], loss: 0.001489, mae: 0.041714, mean_q: 1.172182
 278886/1000000: episode: 2789, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.163, mean reward: 0.572 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.001388, mae: 0.040480, mean_q: 1.173542
 278986/1000000: episode: 2790, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 57.623, mean reward: 0.576 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.447, 10.174], loss: 0.001456, mae: 0.041058, mean_q: 1.171742
 279086/1000000: episode: 2791, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.899, mean reward: 0.589 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.773, 10.098], loss: 0.001454, mae: 0.041683, mean_q: 1.172692
 279186/1000000: episode: 2792, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.458, mean reward: 0.595 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.143, 10.119], loss: 0.001399, mae: 0.040979, mean_q: 1.168547
 279286/1000000: episode: 2793, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 62.230, mean reward: 0.622 [0.514, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.098], loss: 0.001304, mae: 0.039678, mean_q: 1.171286
 279386/1000000: episode: 2794, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.162, mean reward: 0.582 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.487, 10.216], loss: 0.001339, mae: 0.039975, mean_q: 1.169180
 279486/1000000: episode: 2795, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 59.787, mean reward: 0.598 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.423, 10.098], loss: 0.001385, mae: 0.040703, mean_q: 1.171203
 279586/1000000: episode: 2796, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 63.929, mean reward: 0.639 [0.510, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.509, 10.229], loss: 0.001323, mae: 0.039989, mean_q: 1.170361
 279686/1000000: episode: 2797, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 57.364, mean reward: 0.574 [0.497, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.693, 10.187], loss: 0.001537, mae: 0.042032, mean_q: 1.172672
 279786/1000000: episode: 2798, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.581, mean reward: 0.576 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.224, 10.138], loss: 0.001342, mae: 0.040132, mean_q: 1.165212
 279886/1000000: episode: 2799, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.612, mean reward: 0.586 [0.508, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.284, 10.210], loss: 0.001403, mae: 0.040807, mean_q: 1.165382
 279986/1000000: episode: 2800, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.849, mean reward: 0.578 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.843, 10.098], loss: 0.001492, mae: 0.040684, mean_q: 1.170118
 280086/1000000: episode: 2801, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 59.646, mean reward: 0.596 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.558, 10.148], loss: 0.001497, mae: 0.041491, mean_q: 1.167797
 280186/1000000: episode: 2802, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 60.705, mean reward: 0.607 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.628, 10.098], loss: 0.001503, mae: 0.041766, mean_q: 1.170258
 280286/1000000: episode: 2803, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 59.127, mean reward: 0.591 [0.507, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.832, 10.098], loss: 0.001442, mae: 0.040788, mean_q: 1.167294
 280386/1000000: episode: 2804, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.585, mean reward: 0.596 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.817, 10.133], loss: 0.001426, mae: 0.041684, mean_q: 1.166090
 280486/1000000: episode: 2805, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.764, mean reward: 0.608 [0.512, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.581, 10.291], loss: 0.001575, mae: 0.041254, mean_q: 1.166714
 280586/1000000: episode: 2806, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 59.066, mean reward: 0.591 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.064, 10.170], loss: 0.001571, mae: 0.042699, mean_q: 1.167645
 280686/1000000: episode: 2807, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.374, mean reward: 0.574 [0.503, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.112], loss: 0.001520, mae: 0.041519, mean_q: 1.160089
 280786/1000000: episode: 2808, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 60.460, mean reward: 0.605 [0.513, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.768, 10.098], loss: 0.001512, mae: 0.041539, mean_q: 1.165363
 280886/1000000: episode: 2809, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 58.409, mean reward: 0.584 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.491, 10.157], loss: 0.001388, mae: 0.040667, mean_q: 1.167017
 280986/1000000: episode: 2810, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 59.466, mean reward: 0.595 [0.508, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.398], loss: 0.001539, mae: 0.042058, mean_q: 1.168726
 281086/1000000: episode: 2811, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.687, mean reward: 0.577 [0.500, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.394, 10.098], loss: 0.001452, mae: 0.041570, mean_q: 1.170132
 281186/1000000: episode: 2812, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.613, mean reward: 0.576 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.117], loss: 0.001464, mae: 0.041625, mean_q: 1.166380
 281286/1000000: episode: 2813, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.809, mean reward: 0.588 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.761, 10.158], loss: 0.001366, mae: 0.040792, mean_q: 1.165187
 281386/1000000: episode: 2814, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 59.397, mean reward: 0.594 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.130], loss: 0.001416, mae: 0.041416, mean_q: 1.169250
 281486/1000000: episode: 2815, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.084, mean reward: 0.591 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.330, 10.248], loss: 0.001604, mae: 0.042785, mean_q: 1.167486
 281586/1000000: episode: 2816, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 61.314, mean reward: 0.613 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.600, 10.272], loss: 0.001466, mae: 0.041586, mean_q: 1.170938
 281686/1000000: episode: 2817, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 57.609, mean reward: 0.576 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.016, 10.282], loss: 0.001563, mae: 0.041929, mean_q: 1.168711
 281786/1000000: episode: 2818, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 57.401, mean reward: 0.574 [0.504, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.046, 10.291], loss: 0.001594, mae: 0.042879, mean_q: 1.171683
 281886/1000000: episode: 2819, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 60.068, mean reward: 0.601 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.439], loss: 0.001490, mae: 0.041778, mean_q: 1.172863
 281986/1000000: episode: 2820, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 57.552, mean reward: 0.576 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.604, 10.098], loss: 0.001458, mae: 0.041156, mean_q: 1.170557
 282086/1000000: episode: 2821, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.787, mean reward: 0.598 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.553, 10.098], loss: 0.001388, mae: 0.040626, mean_q: 1.171743
 282186/1000000: episode: 2822, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.678, mean reward: 0.587 [0.509, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.347, 10.098], loss: 0.001529, mae: 0.042480, mean_q: 1.172622
 282286/1000000: episode: 2823, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 57.253, mean reward: 0.573 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.098], loss: 0.001447, mae: 0.040809, mean_q: 1.172305
 282386/1000000: episode: 2824, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 58.246, mean reward: 0.582 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.440, 10.098], loss: 0.001389, mae: 0.040995, mean_q: 1.169340
 282486/1000000: episode: 2825, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 58.262, mean reward: 0.583 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.542, 10.275], loss: 0.001569, mae: 0.041313, mean_q: 1.166913
 282586/1000000: episode: 2826, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.220, mean reward: 0.582 [0.510, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.475, 10.205], loss: 0.001480, mae: 0.040960, mean_q: 1.169670
 282686/1000000: episode: 2827, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.336, mean reward: 0.583 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.873, 10.106], loss: 0.001385, mae: 0.040686, mean_q: 1.171528
 282786/1000000: episode: 2828, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.539, mean reward: 0.595 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.170], loss: 0.001559, mae: 0.041936, mean_q: 1.168961
 282886/1000000: episode: 2829, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.828, mean reward: 0.588 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.835, 10.436], loss: 0.001442, mae: 0.040574, mean_q: 1.165131
 282986/1000000: episode: 2830, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.562, mean reward: 0.596 [0.513, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.577, 10.427], loss: 0.001539, mae: 0.041550, mean_q: 1.170054
 283086/1000000: episode: 2831, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 63.263, mean reward: 0.633 [0.498, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.959, 10.489], loss: 0.001522, mae: 0.042241, mean_q: 1.167180
 283186/1000000: episode: 2832, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 59.170, mean reward: 0.592 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.237, 10.240], loss: 0.001491, mae: 0.041181, mean_q: 1.164357
 283286/1000000: episode: 2833, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 59.844, mean reward: 0.598 [0.498, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.764, 10.098], loss: 0.001587, mae: 0.043110, mean_q: 1.169756
 283386/1000000: episode: 2834, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.277, mean reward: 0.583 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.413, 10.152], loss: 0.001442, mae: 0.041211, mean_q: 1.170335
 283486/1000000: episode: 2835, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 63.091, mean reward: 0.631 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.708, 10.098], loss: 0.001467, mae: 0.040894, mean_q: 1.166241
 283586/1000000: episode: 2836, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 60.151, mean reward: 0.602 [0.512, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.360, 10.130], loss: 0.001458, mae: 0.041154, mean_q: 1.171232
 283686/1000000: episode: 2837, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.532, mean reward: 0.595 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.250], loss: 0.001516, mae: 0.042114, mean_q: 1.172855
 283786/1000000: episode: 2838, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 60.564, mean reward: 0.606 [0.514, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.098], loss: 0.001489, mae: 0.040893, mean_q: 1.168250
 283886/1000000: episode: 2839, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 57.772, mean reward: 0.578 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.618, 10.162], loss: 0.001496, mae: 0.041824, mean_q: 1.170467
 283986/1000000: episode: 2840, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 60.324, mean reward: 0.603 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.981, 10.198], loss: 0.001355, mae: 0.039679, mean_q: 1.169726
 284086/1000000: episode: 2841, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 56.929, mean reward: 0.569 [0.503, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.675, 10.098], loss: 0.001507, mae: 0.041710, mean_q: 1.174135
 284186/1000000: episode: 2842, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 62.810, mean reward: 0.628 [0.519, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.517, 10.098], loss: 0.001391, mae: 0.040585, mean_q: 1.171845
 284286/1000000: episode: 2843, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.212, mean reward: 0.592 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.490, 10.187], loss: 0.001487, mae: 0.041823, mean_q: 1.170787
 284386/1000000: episode: 2844, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.801, mean reward: 0.578 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.881, 10.138], loss: 0.001440, mae: 0.041142, mean_q: 1.172052
 284486/1000000: episode: 2845, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 58.737, mean reward: 0.587 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.648, 10.184], loss: 0.001476, mae: 0.041957, mean_q: 1.172235
 284586/1000000: episode: 2846, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.289, mean reward: 0.593 [0.508, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.840, 10.244], loss: 0.001659, mae: 0.042690, mean_q: 1.167094
 284686/1000000: episode: 2847, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 56.869, mean reward: 0.569 [0.505, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.971, 10.098], loss: 0.001452, mae: 0.040724, mean_q: 1.166804
 284786/1000000: episode: 2848, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 62.990, mean reward: 0.630 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.272, 10.266], loss: 0.001549, mae: 0.042351, mean_q: 1.172269
 284886/1000000: episode: 2849, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 59.204, mean reward: 0.592 [0.515, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.894, 10.098], loss: 0.001491, mae: 0.042093, mean_q: 1.174119
 284986/1000000: episode: 2850, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 59.171, mean reward: 0.592 [0.513, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.738, 10.098], loss: 0.001467, mae: 0.041320, mean_q: 1.174392
 285086/1000000: episode: 2851, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.897, mean reward: 0.579 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.744, 10.170], loss: 0.001443, mae: 0.041326, mean_q: 1.173456
 285186/1000000: episode: 2852, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.067, mean reward: 0.591 [0.505, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.474, 10.098], loss: 0.001440, mae: 0.040933, mean_q: 1.170109
 285286/1000000: episode: 2853, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 60.157, mean reward: 0.602 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.384, 10.098], loss: 0.001440, mae: 0.040841, mean_q: 1.172758
 285386/1000000: episode: 2854, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 57.496, mean reward: 0.575 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.668, 10.098], loss: 0.001496, mae: 0.041914, mean_q: 1.172245
 285486/1000000: episode: 2855, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.412, mean reward: 0.594 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.107, 10.098], loss: 0.001539, mae: 0.043068, mean_q: 1.171353
 285586/1000000: episode: 2856, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 59.159, mean reward: 0.592 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.076, 10.098], loss: 0.001517, mae: 0.041985, mean_q: 1.170180
 285686/1000000: episode: 2857, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.029, mean reward: 0.570 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.358, 10.098], loss: 0.001482, mae: 0.041595, mean_q: 1.173199
 285786/1000000: episode: 2858, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 59.892, mean reward: 0.599 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.973, 10.098], loss: 0.001513, mae: 0.041858, mean_q: 1.169909
 285886/1000000: episode: 2859, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 57.855, mean reward: 0.579 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.001399, mae: 0.041323, mean_q: 1.170449
 285986/1000000: episode: 2860, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 56.914, mean reward: 0.569 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.376, 10.169], loss: 0.001513, mae: 0.041658, mean_q: 1.171511
 286086/1000000: episode: 2861, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.361, mean reward: 0.574 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.014, 10.190], loss: 0.001421, mae: 0.040767, mean_q: 1.172497
 286186/1000000: episode: 2862, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.765, mean reward: 0.588 [0.506, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.238, 10.166], loss: 0.001371, mae: 0.040193, mean_q: 1.164705
 286286/1000000: episode: 2863, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 62.037, mean reward: 0.620 [0.513, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.116], loss: 0.001430, mae: 0.040834, mean_q: 1.167918
 286386/1000000: episode: 2864, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 60.380, mean reward: 0.604 [0.507, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.206], loss: 0.001438, mae: 0.041441, mean_q: 1.168524
 286486/1000000: episode: 2865, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.335, mean reward: 0.583 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.462, 10.098], loss: 0.001482, mae: 0.041804, mean_q: 1.171039
 286586/1000000: episode: 2866, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 57.705, mean reward: 0.577 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.543, 10.174], loss: 0.001498, mae: 0.041535, mean_q: 1.171289
 286686/1000000: episode: 2867, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 62.329, mean reward: 0.623 [0.501, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.885, 10.098], loss: 0.001461, mae: 0.041083, mean_q: 1.170156
 286786/1000000: episode: 2868, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 61.645, mean reward: 0.616 [0.517, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.679, 10.098], loss: 0.001425, mae: 0.040697, mean_q: 1.172273
 286886/1000000: episode: 2869, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 59.598, mean reward: 0.596 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.154, 10.098], loss: 0.001613, mae: 0.043148, mean_q: 1.170368
 286986/1000000: episode: 2870, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.916, mean reward: 0.579 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.427, 10.225], loss: 0.001583, mae: 0.042506, mean_q: 1.168546
 287086/1000000: episode: 2871, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.255, mean reward: 0.583 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.158], loss: 0.001525, mae: 0.041863, mean_q: 1.173248
 287186/1000000: episode: 2872, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.673, mean reward: 0.587 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.848, 10.289], loss: 0.001452, mae: 0.040801, mean_q: 1.171653
 287286/1000000: episode: 2873, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 56.362, mean reward: 0.564 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.607, 10.098], loss: 0.001441, mae: 0.041008, mean_q: 1.169143
 287386/1000000: episode: 2874, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.567, mean reward: 0.596 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.696, 10.232], loss: 0.001370, mae: 0.039937, mean_q: 1.172891
 287486/1000000: episode: 2875, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 60.220, mean reward: 0.602 [0.515, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.476, 10.098], loss: 0.001542, mae: 0.041953, mean_q: 1.170015
 287586/1000000: episode: 2876, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.360, mean reward: 0.594 [0.517, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.232, 10.239], loss: 0.001546, mae: 0.042240, mean_q: 1.173946
 287686/1000000: episode: 2877, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.361, mean reward: 0.594 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.922, 10.156], loss: 0.001449, mae: 0.040637, mean_q: 1.174387
 287786/1000000: episode: 2878, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 61.345, mean reward: 0.613 [0.514, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.972, 10.098], loss: 0.001509, mae: 0.041681, mean_q: 1.170770
 287886/1000000: episode: 2879, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.893, mean reward: 0.579 [0.508, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.805, 10.100], loss: 0.001520, mae: 0.041892, mean_q: 1.177955
 287986/1000000: episode: 2880, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 56.553, mean reward: 0.566 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.098], loss: 0.001446, mae: 0.041051, mean_q: 1.169692
 288086/1000000: episode: 2881, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 60.542, mean reward: 0.605 [0.497, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.098], loss: 0.001411, mae: 0.040762, mean_q: 1.174410
 288186/1000000: episode: 2882, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 59.966, mean reward: 0.600 [0.498, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.334, 10.098], loss: 0.001575, mae: 0.042497, mean_q: 1.171349
 288286/1000000: episode: 2883, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 64.357, mean reward: 0.644 [0.511, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.704, 10.098], loss: 0.001451, mae: 0.040789, mean_q: 1.170765
 288386/1000000: episode: 2884, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.364, mean reward: 0.584 [0.510, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.316, 10.098], loss: 0.001408, mae: 0.040405, mean_q: 1.173723
 288486/1000000: episode: 2885, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.564, mean reward: 0.576 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.153], loss: 0.001439, mae: 0.040340, mean_q: 1.168469
 288586/1000000: episode: 2886, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.039, mean reward: 0.590 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.122], loss: 0.001474, mae: 0.041155, mean_q: 1.173632
 288686/1000000: episode: 2887, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.918, mean reward: 0.599 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.802, 10.426], loss: 0.001366, mae: 0.040189, mean_q: 1.174187
 288786/1000000: episode: 2888, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.467, mean reward: 0.575 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.014, 10.160], loss: 0.001466, mae: 0.041778, mean_q: 1.169154
 288886/1000000: episode: 2889, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 59.756, mean reward: 0.598 [0.514, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.098], loss: 0.001447, mae: 0.040967, mean_q: 1.169947
 288986/1000000: episode: 2890, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 61.998, mean reward: 0.620 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.646, 10.098], loss: 0.001374, mae: 0.039899, mean_q: 1.172583
 289086/1000000: episode: 2891, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.082, mean reward: 0.581 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.407, 10.098], loss: 0.001390, mae: 0.040127, mean_q: 1.171347
 289186/1000000: episode: 2892, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 61.141, mean reward: 0.611 [0.513, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.776, 10.167], loss: 0.001476, mae: 0.041606, mean_q: 1.175279
 289286/1000000: episode: 2893, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 58.587, mean reward: 0.586 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.780, 10.098], loss: 0.001475, mae: 0.040884, mean_q: 1.176287
 289386/1000000: episode: 2894, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 62.311, mean reward: 0.623 [0.530, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.928, 10.098], loss: 0.001431, mae: 0.040755, mean_q: 1.174560
 289486/1000000: episode: 2895, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 58.929, mean reward: 0.589 [0.506, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.505, 10.368], loss: 0.001461, mae: 0.040753, mean_q: 1.172675
 289586/1000000: episode: 2896, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 59.735, mean reward: 0.597 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.426, 10.278], loss: 0.001426, mae: 0.040512, mean_q: 1.172306
 289686/1000000: episode: 2897, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.923, mean reward: 0.579 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.271, 10.206], loss: 0.001379, mae: 0.039924, mean_q: 1.169620
 289786/1000000: episode: 2898, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.013, mean reward: 0.570 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.573, 10.310], loss: 0.001481, mae: 0.041157, mean_q: 1.172704
 289886/1000000: episode: 2899, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.653, mean reward: 0.587 [0.516, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.727, 10.246], loss: 0.001418, mae: 0.040089, mean_q: 1.171406
 289986/1000000: episode: 2900, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.986, mean reward: 0.580 [0.516, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.098], loss: 0.001447, mae: 0.041163, mean_q: 1.171322
 290086/1000000: episode: 2901, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.616, mean reward: 0.596 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.518, 10.098], loss: 0.001397, mae: 0.040192, mean_q: 1.172516
 290186/1000000: episode: 2902, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.831, mean reward: 0.578 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.931, 10.209], loss: 0.001393, mae: 0.040292, mean_q: 1.169901
 290286/1000000: episode: 2903, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 60.126, mean reward: 0.601 [0.511, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.309, 10.098], loss: 0.001359, mae: 0.039342, mean_q: 1.172275
 290386/1000000: episode: 2904, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 57.222, mean reward: 0.572 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.896, 10.146], loss: 0.001383, mae: 0.039798, mean_q: 1.168303
 290486/1000000: episode: 2905, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 59.041, mean reward: 0.590 [0.520, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.285, 10.129], loss: 0.001248, mae: 0.038185, mean_q: 1.170766
 290586/1000000: episode: 2906, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.005, mean reward: 0.580 [0.503, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.638, 10.248], loss: 0.001466, mae: 0.041164, mean_q: 1.170397
 290686/1000000: episode: 2907, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.067, mean reward: 0.591 [0.514, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.964, 10.216], loss: 0.001246, mae: 0.038489, mean_q: 1.167157
 290786/1000000: episode: 2908, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: 58.825, mean reward: 0.588 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.471, 10.261], loss: 0.001345, mae: 0.039591, mean_q: 1.167936
 290886/1000000: episode: 2909, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 56.972, mean reward: 0.570 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.334, 10.338], loss: 0.001464, mae: 0.040851, mean_q: 1.166777
 290986/1000000: episode: 2910, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.980, mean reward: 0.590 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.646, 10.098], loss: 0.001462, mae: 0.040765, mean_q: 1.170799
 291086/1000000: episode: 2911, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.396, mean reward: 0.594 [0.510, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.016, 10.098], loss: 0.001541, mae: 0.041722, mean_q: 1.173303
 291186/1000000: episode: 2912, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.743, mean reward: 0.597 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.745, 10.098], loss: 0.001420, mae: 0.040433, mean_q: 1.169542
 291286/1000000: episode: 2913, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.436, mean reward: 0.574 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.087, 10.368], loss: 0.001631, mae: 0.043120, mean_q: 1.172508
 291386/1000000: episode: 2914, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 58.360, mean reward: 0.584 [0.509, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.961, 10.274], loss: 0.001322, mae: 0.039049, mean_q: 1.170576
 291486/1000000: episode: 2915, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.085, mean reward: 0.571 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.548, 10.200], loss: 0.001516, mae: 0.041754, mean_q: 1.170316
 291586/1000000: episode: 2916, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 57.218, mean reward: 0.572 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.312, 10.141], loss: 0.001619, mae: 0.042821, mean_q: 1.170050
 291686/1000000: episode: 2917, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 59.707, mean reward: 0.597 [0.510, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.644, 10.247], loss: 0.001405, mae: 0.040602, mean_q: 1.164981
 291786/1000000: episode: 2918, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.118, mean reward: 0.601 [0.510, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.048, 10.098], loss: 0.001334, mae: 0.039567, mean_q: 1.166154
 291886/1000000: episode: 2919, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 58.457, mean reward: 0.585 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.358, 10.326], loss: 0.001325, mae: 0.039143, mean_q: 1.167489
 291986/1000000: episode: 2920, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 57.790, mean reward: 0.578 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.688, 10.098], loss: 0.001382, mae: 0.039956, mean_q: 1.170229
 292086/1000000: episode: 2921, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 60.329, mean reward: 0.603 [0.529, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.395, 10.098], loss: 0.001369, mae: 0.040080, mean_q: 1.170465
 292186/1000000: episode: 2922, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 57.200, mean reward: 0.572 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.583, 10.098], loss: 0.001346, mae: 0.039508, mean_q: 1.166094
 292286/1000000: episode: 2923, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 58.177, mean reward: 0.582 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.001366, mae: 0.039981, mean_q: 1.170675
 292386/1000000: episode: 2924, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 56.812, mean reward: 0.568 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.553, 10.260], loss: 0.001375, mae: 0.039980, mean_q: 1.169666
 292486/1000000: episode: 2925, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 59.588, mean reward: 0.596 [0.516, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.816, 10.271], loss: 0.001401, mae: 0.039722, mean_q: 1.165665
 292586/1000000: episode: 2926, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 61.108, mean reward: 0.611 [0.498, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.833, 10.116], loss: 0.001301, mae: 0.039060, mean_q: 1.166197
 292686/1000000: episode: 2927, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 61.590, mean reward: 0.616 [0.502, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.802, 10.098], loss: 0.001364, mae: 0.039503, mean_q: 1.167098
 292786/1000000: episode: 2928, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 57.912, mean reward: 0.579 [0.509, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.938, 10.116], loss: 0.001421, mae: 0.040724, mean_q: 1.170183
 292886/1000000: episode: 2929, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 60.529, mean reward: 0.605 [0.518, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.940, 10.098], loss: 0.001402, mae: 0.040374, mean_q: 1.165250
 292986/1000000: episode: 2930, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 57.564, mean reward: 0.576 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.494, 10.098], loss: 0.001507, mae: 0.041324, mean_q: 1.169534
 293086/1000000: episode: 2931, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 59.411, mean reward: 0.594 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.548, 10.390], loss: 0.001424, mae: 0.040546, mean_q: 1.168522
 293186/1000000: episode: 2932, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 61.134, mean reward: 0.611 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.525, 10.285], loss: 0.001379, mae: 0.040094, mean_q: 1.168111
 293286/1000000: episode: 2933, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 60.059, mean reward: 0.601 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.110], loss: 0.001377, mae: 0.039684, mean_q: 1.164495
 293386/1000000: episode: 2934, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 60.383, mean reward: 0.604 [0.514, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.228, 10.098], loss: 0.001470, mae: 0.041014, mean_q: 1.165665
 293486/1000000: episode: 2935, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 57.788, mean reward: 0.578 [0.509, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.592, 10.337], loss: 0.001402, mae: 0.039751, mean_q: 1.164296
 293586/1000000: episode: 2936, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.768, mean reward: 0.588 [0.510, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.952, 10.570], loss: 0.001371, mae: 0.039740, mean_q: 1.167688
 293686/1000000: episode: 2937, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 58.657, mean reward: 0.587 [0.510, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.185, 10.225], loss: 0.001279, mae: 0.038827, mean_q: 1.165360
 293786/1000000: episode: 2938, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.705, mean reward: 0.577 [0.511, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.264], loss: 0.001355, mae: 0.039167, mean_q: 1.164196
 293886/1000000: episode: 2939, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 59.222, mean reward: 0.592 [0.502, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.770, 10.219], loss: 0.001366, mae: 0.039338, mean_q: 1.164545
 293986/1000000: episode: 2940, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 60.249, mean reward: 0.602 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.098], loss: 0.001429, mae: 0.040783, mean_q: 1.164897
 294086/1000000: episode: 2941, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 60.241, mean reward: 0.602 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.841, 10.231], loss: 0.001426, mae: 0.040361, mean_q: 1.168381
 294186/1000000: episode: 2942, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 58.577, mean reward: 0.586 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.272, 10.098], loss: 0.001372, mae: 0.040004, mean_q: 1.165360
 294286/1000000: episode: 2943, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 61.465, mean reward: 0.615 [0.499, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.098], loss: 0.001415, mae: 0.040370, mean_q: 1.168460
 294386/1000000: episode: 2944, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.167, mean reward: 0.592 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.190], loss: 0.001479, mae: 0.040906, mean_q: 1.166534
 294486/1000000: episode: 2945, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 60.970, mean reward: 0.610 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.557, 10.301], loss: 0.001493, mae: 0.041638, mean_q: 1.165804
 294586/1000000: episode: 2946, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.094, mean reward: 0.581 [0.498, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.914, 10.299], loss: 0.001492, mae: 0.041663, mean_q: 1.168001
 294686/1000000: episode: 2947, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 56.636, mean reward: 0.566 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.548, 10.098], loss: 0.001468, mae: 0.040892, mean_q: 1.167658
 294786/1000000: episode: 2948, duration: 0.785s, episode steps: 100, steps per second: 127, episode reward: 57.271, mean reward: 0.573 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.290, 10.098], loss: 0.001490, mae: 0.041135, mean_q: 1.164581
 294886/1000000: episode: 2949, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 60.411, mean reward: 0.604 [0.515, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.155], loss: 0.001439, mae: 0.040554, mean_q: 1.168126
 294986/1000000: episode: 2950, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 59.049, mean reward: 0.590 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.859, 10.161], loss: 0.001398, mae: 0.040286, mean_q: 1.164223
 295086/1000000: episode: 2951, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 64.171, mean reward: 0.642 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.596, 10.098], loss: 0.001485, mae: 0.041124, mean_q: 1.167958
 295186/1000000: episode: 2952, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 60.220, mean reward: 0.602 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.315], loss: 0.001463, mae: 0.041539, mean_q: 1.167905
 295286/1000000: episode: 2953, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 67.091, mean reward: 0.671 [0.514, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.905, 10.098], loss: 0.001440, mae: 0.041138, mean_q: 1.169078
 295386/1000000: episode: 2954, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 64.399, mean reward: 0.644 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.726, 10.098], loss: 0.001404, mae: 0.040693, mean_q: 1.172056
 295486/1000000: episode: 2955, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.207, mean reward: 0.592 [0.511, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.377, 10.237], loss: 0.001352, mae: 0.039840, mean_q: 1.169356
 295586/1000000: episode: 2956, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 57.036, mean reward: 0.570 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.092, 10.171], loss: 0.001351, mae: 0.039701, mean_q: 1.170554
 295686/1000000: episode: 2957, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.270, mean reward: 0.573 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.141], loss: 0.001411, mae: 0.040560, mean_q: 1.169773
 295786/1000000: episode: 2958, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.913, mean reward: 0.589 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.863, 10.238], loss: 0.001564, mae: 0.042312, mean_q: 1.172612
 295886/1000000: episode: 2959, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.068, mean reward: 0.591 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.746, 10.098], loss: 0.001433, mae: 0.041034, mean_q: 1.174311
 295986/1000000: episode: 2960, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.546, mean reward: 0.595 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.392, 10.098], loss: 0.001443, mae: 0.041134, mean_q: 1.178097
 296086/1000000: episode: 2961, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.854, mean reward: 0.579 [0.512, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.493, 10.373], loss: 0.001494, mae: 0.041939, mean_q: 1.173679
 296186/1000000: episode: 2962, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.851, mean reward: 0.589 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.643, 10.118], loss: 0.001403, mae: 0.040861, mean_q: 1.175767
 296286/1000000: episode: 2963, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 59.450, mean reward: 0.595 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.751, 10.098], loss: 0.001387, mae: 0.040552, mean_q: 1.176192
 296386/1000000: episode: 2964, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.699, mean reward: 0.587 [0.518, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.271, 10.098], loss: 0.001385, mae: 0.040396, mean_q: 1.173266
 296486/1000000: episode: 2965, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 59.880, mean reward: 0.599 [0.507, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.106, 10.098], loss: 0.001438, mae: 0.041457, mean_q: 1.177883
 296586/1000000: episode: 2966, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.500, mean reward: 0.585 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.050, 10.269], loss: 0.001431, mae: 0.041332, mean_q: 1.176830
 296686/1000000: episode: 2967, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 58.179, mean reward: 0.582 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.274, 10.255], loss: 0.001410, mae: 0.041065, mean_q: 1.173986
 296786/1000000: episode: 2968, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 58.220, mean reward: 0.582 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.206, 10.221], loss: 0.001392, mae: 0.040902, mean_q: 1.177763
 296886/1000000: episode: 2969, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.812, mean reward: 0.578 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.880, 10.098], loss: 0.001445, mae: 0.041561, mean_q: 1.169807
 296986/1000000: episode: 2970, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 59.565, mean reward: 0.596 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.281, 10.098], loss: 0.001392, mae: 0.040527, mean_q: 1.172912
 297086/1000000: episode: 2971, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 60.386, mean reward: 0.604 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.098], loss: 0.001424, mae: 0.040980, mean_q: 1.176811
 297186/1000000: episode: 2972, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 63.077, mean reward: 0.631 [0.507, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.529, 10.098], loss: 0.001478, mae: 0.041996, mean_q: 1.177379
 297286/1000000: episode: 2973, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.659, mean reward: 0.587 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.201, 10.432], loss: 0.001374, mae: 0.040502, mean_q: 1.176464
 297386/1000000: episode: 2974, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.344, mean reward: 0.593 [0.507, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.748, 10.182], loss: 0.001404, mae: 0.040474, mean_q: 1.179659
 297486/1000000: episode: 2975, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.813, mean reward: 0.578 [0.509, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.376, 10.098], loss: 0.001459, mae: 0.041812, mean_q: 1.175689
 297586/1000000: episode: 2976, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 56.587, mean reward: 0.566 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.307, 10.098], loss: 0.001437, mae: 0.041509, mean_q: 1.176453
 297686/1000000: episode: 2977, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 64.130, mean reward: 0.641 [0.504, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.893, 10.353], loss: 0.001402, mae: 0.041091, mean_q: 1.175270
 297786/1000000: episode: 2978, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 56.469, mean reward: 0.565 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.517, 10.098], loss: 0.001435, mae: 0.041261, mean_q: 1.179061
 297886/1000000: episode: 2979, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.093, mean reward: 0.581 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.117, 10.198], loss: 0.001414, mae: 0.040864, mean_q: 1.174579
 297986/1000000: episode: 2980, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.404, mean reward: 0.574 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.156, 10.182], loss: 0.001259, mae: 0.039086, mean_q: 1.173573
 298086/1000000: episode: 2981, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.234, mean reward: 0.592 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.275], loss: 0.001392, mae: 0.041082, mean_q: 1.175823
 298186/1000000: episode: 2982, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.531, mean reward: 0.585 [0.520, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.232, 10.232], loss: 0.001497, mae: 0.042246, mean_q: 1.175267
 298286/1000000: episode: 2983, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.934, mean reward: 0.589 [0.505, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.648, 10.114], loss: 0.001420, mae: 0.041291, mean_q: 1.173694
 298386/1000000: episode: 2984, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.535, mean reward: 0.575 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.297], loss: 0.001403, mae: 0.040945, mean_q: 1.170526
 298486/1000000: episode: 2985, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 56.435, mean reward: 0.564 [0.499, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.098], loss: 0.001457, mae: 0.041303, mean_q: 1.177393
 298586/1000000: episode: 2986, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.952, mean reward: 0.580 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.225, 10.098], loss: 0.001368, mae: 0.040941, mean_q: 1.171889
 298686/1000000: episode: 2987, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 61.913, mean reward: 0.619 [0.509, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.230, 10.386], loss: 0.001432, mae: 0.041160, mean_q: 1.171315
 298786/1000000: episode: 2988, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 59.933, mean reward: 0.599 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.367, 10.227], loss: 0.001414, mae: 0.041393, mean_q: 1.176200
 298886/1000000: episode: 2989, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.414, mean reward: 0.584 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.979, 10.191], loss: 0.001418, mae: 0.041173, mean_q: 1.174951
 298986/1000000: episode: 2990, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 56.928, mean reward: 0.569 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.876, 10.098], loss: 0.001412, mae: 0.040535, mean_q: 1.173699
 299086/1000000: episode: 2991, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 60.506, mean reward: 0.605 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.887, 10.098], loss: 0.001588, mae: 0.042783, mean_q: 1.173102
 299186/1000000: episode: 2992, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.960, mean reward: 0.590 [0.500, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.491, 10.174], loss: 0.001400, mae: 0.040632, mean_q: 1.174918
 299286/1000000: episode: 2993, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 58.395, mean reward: 0.584 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.628, 10.098], loss: 0.001347, mae: 0.039776, mean_q: 1.170565
 299386/1000000: episode: 2994, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 58.098, mean reward: 0.581 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.919, 10.098], loss: 0.001413, mae: 0.041357, mean_q: 1.172957
 299486/1000000: episode: 2995, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.258, mean reward: 0.593 [0.510, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.857, 10.542], loss: 0.001475, mae: 0.041734, mean_q: 1.174901
 299586/1000000: episode: 2996, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 59.311, mean reward: 0.593 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.826, 10.121], loss: 0.001402, mae: 0.041181, mean_q: 1.169775
 299686/1000000: episode: 2997, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.948, mean reward: 0.589 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.364, 10.098], loss: 0.001518, mae: 0.042726, mean_q: 1.173038
 299786/1000000: episode: 2998, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 60.817, mean reward: 0.608 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.351, 10.098], loss: 0.001369, mae: 0.040632, mean_q: 1.173453
 299886/1000000: episode: 2999, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 58.353, mean reward: 0.584 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.501, 10.301], loss: 0.001338, mae: 0.039826, mean_q: 1.176832
 299986/1000000: episode: 3000, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.605, mean reward: 0.596 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.713, 10.098], loss: 0.001338, mae: 0.039811, mean_q: 1.172894
 300086/1000000: episode: 3001, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 57.140, mean reward: 0.571 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.303, 10.098], loss: 0.001505, mae: 0.041481, mean_q: 1.169889
 300186/1000000: episode: 3002, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.055, mean reward: 0.581 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.522, 10.098], loss: 0.001479, mae: 0.041948, mean_q: 1.170040
 300286/1000000: episode: 3003, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 58.151, mean reward: 0.582 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.810, 10.163], loss: 0.001441, mae: 0.040511, mean_q: 1.164583
 300386/1000000: episode: 3004, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 60.655, mean reward: 0.607 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.642, 10.098], loss: 0.001428, mae: 0.041419, mean_q: 1.165916
 300486/1000000: episode: 3005, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 60.127, mean reward: 0.601 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.906, 10.098], loss: 0.001528, mae: 0.041433, mean_q: 1.166645
 300586/1000000: episode: 3006, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.457, mean reward: 0.575 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.703, 10.098], loss: 0.001381, mae: 0.040973, mean_q: 1.164489
 300686/1000000: episode: 3007, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 61.191, mean reward: 0.612 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.726, 10.181], loss: 0.001410, mae: 0.040795, mean_q: 1.167027
 300786/1000000: episode: 3008, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.818, mean reward: 0.588 [0.516, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.542, 10.098], loss: 0.001486, mae: 0.041573, mean_q: 1.165557
 300886/1000000: episode: 3009, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.221, mean reward: 0.582 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.670, 10.159], loss: 0.001477, mae: 0.041495, mean_q: 1.168444
 300986/1000000: episode: 3010, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.207, mean reward: 0.592 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.784, 10.200], loss: 0.001389, mae: 0.040312, mean_q: 1.162782
 301086/1000000: episode: 3011, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 57.929, mean reward: 0.579 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.652, 10.272], loss: 0.001402, mae: 0.040520, mean_q: 1.166571
 301186/1000000: episode: 3012, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.637, mean reward: 0.586 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.621, 10.098], loss: 0.001515, mae: 0.041851, mean_q: 1.162794
 301286/1000000: episode: 3013, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 58.838, mean reward: 0.588 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.709, 10.303], loss: 0.001393, mae: 0.040977, mean_q: 1.165933
 301386/1000000: episode: 3014, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 63.738, mean reward: 0.637 [0.512, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.338, 10.229], loss: 0.001403, mae: 0.040121, mean_q: 1.168083
 301486/1000000: episode: 3015, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.619, mean reward: 0.576 [0.519, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.098], loss: 0.001463, mae: 0.041444, mean_q: 1.168782
 301586/1000000: episode: 3016, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.648, mean reward: 0.596 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.742, 10.140], loss: 0.001361, mae: 0.039498, mean_q: 1.168256
 301686/1000000: episode: 3017, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 61.732, mean reward: 0.617 [0.510, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.467, 10.282], loss: 0.001421, mae: 0.040777, mean_q: 1.170131
 301786/1000000: episode: 3018, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 56.851, mean reward: 0.569 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.939, 10.098], loss: 0.001334, mae: 0.040000, mean_q: 1.167644
 301886/1000000: episode: 3019, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 60.880, mean reward: 0.609 [0.501, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.059, 10.403], loss: 0.001335, mae: 0.039698, mean_q: 1.169354
 301986/1000000: episode: 3020, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.207, mean reward: 0.592 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.645, 10.098], loss: 0.001337, mae: 0.039986, mean_q: 1.169199
 302086/1000000: episode: 3021, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.205, mean reward: 0.572 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.563, 10.098], loss: 0.001438, mae: 0.040905, mean_q: 1.169484
 302186/1000000: episode: 3022, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 60.099, mean reward: 0.601 [0.516, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.644, 10.121], loss: 0.001551, mae: 0.042292, mean_q: 1.166421
 302286/1000000: episode: 3023, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.518, mean reward: 0.585 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.230, 10.172], loss: 0.001482, mae: 0.041980, mean_q: 1.166787
 302386/1000000: episode: 3024, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 56.933, mean reward: 0.569 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.126], loss: 0.001489, mae: 0.041866, mean_q: 1.162831
 302486/1000000: episode: 3025, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 60.155, mean reward: 0.602 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.063, 10.098], loss: 0.001394, mae: 0.040703, mean_q: 1.167149
 302586/1000000: episode: 3026, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 58.853, mean reward: 0.589 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.154], loss: 0.001526, mae: 0.041945, mean_q: 1.167325
 302686/1000000: episode: 3027, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: 59.540, mean reward: 0.595 [0.497, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.015, 10.103], loss: 0.001473, mae: 0.041600, mean_q: 1.170029
 302786/1000000: episode: 3028, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.007, mean reward: 0.600 [0.510, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.247, 10.155], loss: 0.001443, mae: 0.040877, mean_q: 1.167728
 302886/1000000: episode: 3029, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 58.567, mean reward: 0.586 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.832, 10.113], loss: 0.001438, mae: 0.040681, mean_q: 1.168978
 302986/1000000: episode: 3030, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 59.553, mean reward: 0.596 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.395, 10.098], loss: 0.001522, mae: 0.042248, mean_q: 1.170101
 303086/1000000: episode: 3031, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 60.813, mean reward: 0.608 [0.512, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.273, 10.136], loss: 0.001482, mae: 0.041535, mean_q: 1.166085
 303186/1000000: episode: 3032, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 57.232, mean reward: 0.572 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.520, 10.141], loss: 0.001469, mae: 0.041497, mean_q: 1.167150
 303286/1000000: episode: 3033, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 58.608, mean reward: 0.586 [0.516, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.591, 10.131], loss: 0.001354, mae: 0.039951, mean_q: 1.168869
 303386/1000000: episode: 3034, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.735, mean reward: 0.597 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.912, 10.098], loss: 0.001428, mae: 0.040801, mean_q: 1.168249
 303486/1000000: episode: 3035, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.503, mean reward: 0.575 [0.504, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.468, 10.226], loss: 0.001426, mae: 0.040774, mean_q: 1.168146
 303586/1000000: episode: 3036, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 61.873, mean reward: 0.619 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.698, 10.548], loss: 0.001470, mae: 0.041273, mean_q: 1.168583
 303686/1000000: episode: 3037, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 57.874, mean reward: 0.579 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.665, 10.102], loss: 0.001446, mae: 0.041198, mean_q: 1.170372
 303786/1000000: episode: 3038, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.521, mean reward: 0.575 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.685, 10.098], loss: 0.001404, mae: 0.040073, mean_q: 1.170749
 303886/1000000: episode: 3039, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 58.782, mean reward: 0.588 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.881, 10.217], loss: 0.001444, mae: 0.040638, mean_q: 1.170447
 303986/1000000: episode: 3040, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 61.098, mean reward: 0.611 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.508, 10.288], loss: 0.001465, mae: 0.041012, mean_q: 1.166991
 304086/1000000: episode: 3041, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 59.041, mean reward: 0.590 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.308, 10.098], loss: 0.001454, mae: 0.041031, mean_q: 1.170914
 304186/1000000: episode: 3042, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 60.430, mean reward: 0.604 [0.512, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.696, 10.255], loss: 0.001409, mae: 0.040217, mean_q: 1.170012
 304286/1000000: episode: 3043, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 57.738, mean reward: 0.577 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.891, 10.306], loss: 0.001497, mae: 0.041380, mean_q: 1.169145
 304386/1000000: episode: 3044, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.243, mean reward: 0.572 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.338, 10.098], loss: 0.001369, mae: 0.039948, mean_q: 1.173693
 304486/1000000: episode: 3045, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.814, mean reward: 0.598 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.478, 10.217], loss: 0.001412, mae: 0.040709, mean_q: 1.171161
 304586/1000000: episode: 3046, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.094, mean reward: 0.581 [0.507, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.456, 10.098], loss: 0.001377, mae: 0.039457, mean_q: 1.167009
 304686/1000000: episode: 3047, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.547, mean reward: 0.585 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.150, 10.292], loss: 0.001392, mae: 0.040430, mean_q: 1.168549
 304786/1000000: episode: 3048, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.270, mean reward: 0.583 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.284, 10.324], loss: 0.001417, mae: 0.040774, mean_q: 1.168595
 304886/1000000: episode: 3049, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.901, mean reward: 0.589 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.462, 10.098], loss: 0.001358, mae: 0.040224, mean_q: 1.168244
 304986/1000000: episode: 3050, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 57.551, mean reward: 0.576 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.200, 10.098], loss: 0.001436, mae: 0.040913, mean_q: 1.171488
 305086/1000000: episode: 3051, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: 57.260, mean reward: 0.573 [0.503, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.473, 10.098], loss: 0.001437, mae: 0.041119, mean_q: 1.168285
 305186/1000000: episode: 3052, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 56.467, mean reward: 0.565 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.436, 10.231], loss: 0.001539, mae: 0.041649, mean_q: 1.170626
 305286/1000000: episode: 3053, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 65.079, mean reward: 0.651 [0.522, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.888, 10.375], loss: 0.001352, mae: 0.039778, mean_q: 1.169728
 305386/1000000: episode: 3054, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 57.145, mean reward: 0.571 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.598, 10.124], loss: 0.001421, mae: 0.040725, mean_q: 1.172023
 305486/1000000: episode: 3055, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.745, mean reward: 0.577 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.623, 10.108], loss: 0.001375, mae: 0.039694, mean_q: 1.165670
 305586/1000000: episode: 3056, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.282, mean reward: 0.583 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.157], loss: 0.001416, mae: 0.040540, mean_q: 1.166216
 305686/1000000: episode: 3057, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.989, mean reward: 0.580 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.184, 10.098], loss: 0.001520, mae: 0.042103, mean_q: 1.167115
 305786/1000000: episode: 3058, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 59.204, mean reward: 0.592 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.042, 10.322], loss: 0.001347, mae: 0.039558, mean_q: 1.167307
 305886/1000000: episode: 3059, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 60.167, mean reward: 0.602 [0.498, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.921, 10.403], loss: 0.001363, mae: 0.039417, mean_q: 1.165773
 305986/1000000: episode: 3060, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 59.083, mean reward: 0.591 [0.513, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.371, 10.098], loss: 0.001404, mae: 0.039968, mean_q: 1.168466
 306086/1000000: episode: 3061, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 61.469, mean reward: 0.615 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.936, 10.098], loss: 0.001408, mae: 0.040672, mean_q: 1.168682
 306186/1000000: episode: 3062, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 59.040, mean reward: 0.590 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.999, 10.236], loss: 0.001381, mae: 0.040338, mean_q: 1.170986
 306286/1000000: episode: 3063, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 56.599, mean reward: 0.566 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.760, 10.098], loss: 0.001378, mae: 0.040040, mean_q: 1.168099
 306386/1000000: episode: 3064, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 59.386, mean reward: 0.594 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.098], loss: 0.001359, mae: 0.039637, mean_q: 1.166809
 306486/1000000: episode: 3065, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 62.302, mean reward: 0.623 [0.511, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.098], loss: 0.001568, mae: 0.042451, mean_q: 1.164989
 306586/1000000: episode: 3066, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 60.698, mean reward: 0.607 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.911, 10.251], loss: 0.001510, mae: 0.041817, mean_q: 1.168836
 306686/1000000: episode: 3067, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 58.601, mean reward: 0.586 [0.502, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.232, 10.253], loss: 0.001475, mae: 0.041075, mean_q: 1.168266
 306786/1000000: episode: 3068, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.467, mean reward: 0.585 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.300, 10.098], loss: 0.001456, mae: 0.041099, mean_q: 1.169037
 306886/1000000: episode: 3069, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 56.880, mean reward: 0.569 [0.499, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.046, 10.098], loss: 0.001465, mae: 0.041103, mean_q: 1.165925
 306986/1000000: episode: 3070, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.783, mean reward: 0.588 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.855, 10.098], loss: 0.001483, mae: 0.041494, mean_q: 1.167079
 307086/1000000: episode: 3071, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 58.797, mean reward: 0.588 [0.516, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.233], loss: 0.001533, mae: 0.042058, mean_q: 1.167923
 307186/1000000: episode: 3072, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 57.868, mean reward: 0.579 [0.507, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.003, 10.224], loss: 0.001484, mae: 0.041651, mean_q: 1.166761
 307286/1000000: episode: 3073, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.223, mean reward: 0.582 [0.504, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.117], loss: 0.001429, mae: 0.040301, mean_q: 1.163292
 307386/1000000: episode: 3074, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 57.847, mean reward: 0.578 [0.506, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.284, 10.175], loss: 0.001501, mae: 0.041784, mean_q: 1.165914
 307486/1000000: episode: 3075, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 63.073, mean reward: 0.631 [0.519, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.865, 10.098], loss: 0.001725, mae: 0.043285, mean_q: 1.169243
 307586/1000000: episode: 3076, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.752, mean reward: 0.598 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.372, 10.098], loss: 0.001782, mae: 0.043674, mean_q: 1.170871
 307686/1000000: episode: 3077, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.748, mean reward: 0.597 [0.500, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.035, 10.098], loss: 0.001715, mae: 0.043448, mean_q: 1.170300
 307786/1000000: episode: 3078, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 58.238, mean reward: 0.582 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.300, 10.405], loss: 0.001781, mae: 0.044295, mean_q: 1.168494
 307886/1000000: episode: 3079, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 61.755, mean reward: 0.618 [0.521, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.058, 10.375], loss: 0.001673, mae: 0.042870, mean_q: 1.168120
 307986/1000000: episode: 3080, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.671, mean reward: 0.577 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.411, 10.119], loss: 0.001740, mae: 0.044384, mean_q: 1.173439
 308086/1000000: episode: 3081, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.115, mean reward: 0.581 [0.502, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.108, 10.191], loss: 0.001655, mae: 0.042822, mean_q: 1.168726
 308186/1000000: episode: 3082, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.681, mean reward: 0.577 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.298], loss: 0.001687, mae: 0.043154, mean_q: 1.173361
 308286/1000000: episode: 3083, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 58.897, mean reward: 0.589 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.799, 10.194], loss: 0.001658, mae: 0.042991, mean_q: 1.167663
 308386/1000000: episode: 3084, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.040, mean reward: 0.580 [0.497, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.988, 10.098], loss: 0.001761, mae: 0.044176, mean_q: 1.167718
 308486/1000000: episode: 3085, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: 57.759, mean reward: 0.578 [0.509, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.511, 10.301], loss: 0.001807, mae: 0.044722, mean_q: 1.165872
 308586/1000000: episode: 3086, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.190, mean reward: 0.582 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.573, 10.098], loss: 0.001624, mae: 0.043026, mean_q: 1.164981
 308686/1000000: episode: 3087, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 63.216, mean reward: 0.632 [0.500, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.048, 10.098], loss: 0.001675, mae: 0.043782, mean_q: 1.166419
 308786/1000000: episode: 3088, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.547, mean reward: 0.595 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.878, 10.149], loss: 0.001675, mae: 0.043579, mean_q: 1.167452
 308886/1000000: episode: 3089, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 59.669, mean reward: 0.597 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.990, 10.214], loss: 0.001654, mae: 0.043574, mean_q: 1.170060
 308986/1000000: episode: 3090, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 57.083, mean reward: 0.571 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.696, 10.098], loss: 0.001691, mae: 0.044079, mean_q: 1.168282
 309086/1000000: episode: 3091, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.575, mean reward: 0.576 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.098], loss: 0.001648, mae: 0.043582, mean_q: 1.168179
 309186/1000000: episode: 3092, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 59.066, mean reward: 0.591 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.098], loss: 0.001520, mae: 0.042215, mean_q: 1.165373
 309286/1000000: episode: 3093, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 59.320, mean reward: 0.593 [0.511, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.341, 10.098], loss: 0.001630, mae: 0.043241, mean_q: 1.165736
 309386/1000000: episode: 3094, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.744, mean reward: 0.597 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.760, 10.443], loss: 0.001534, mae: 0.042011, mean_q: 1.166108
 309486/1000000: episode: 3095, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.708, mean reward: 0.577 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.098], loss: 0.001701, mae: 0.044097, mean_q: 1.165609
 309586/1000000: episode: 3096, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 56.698, mean reward: 0.567 [0.501, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.393, 10.098], loss: 0.001683, mae: 0.044279, mean_q: 1.166435
 309686/1000000: episode: 3097, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.622, mean reward: 0.586 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.175, 10.161], loss: 0.001780, mae: 0.045382, mean_q: 1.168459
 309786/1000000: episode: 3098, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.131, mean reward: 0.581 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.591, 10.098], loss: 0.001629, mae: 0.043816, mean_q: 1.164967
 309886/1000000: episode: 3099, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 59.322, mean reward: 0.593 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.500, 10.098], loss: 0.001574, mae: 0.042478, mean_q: 1.164368
 309986/1000000: episode: 3100, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 60.381, mean reward: 0.604 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.453, 10.098], loss: 0.001688, mae: 0.044538, mean_q: 1.165611
 310086/1000000: episode: 3101, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 56.851, mean reward: 0.569 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.705, 10.098], loss: 0.001749, mae: 0.045191, mean_q: 1.165465
 310186/1000000: episode: 3102, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 59.111, mean reward: 0.591 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.213, 10.209], loss: 0.001665, mae: 0.044279, mean_q: 1.165797
 310286/1000000: episode: 3103, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.881, mean reward: 0.589 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.475, 10.164], loss: 0.001770, mae: 0.045594, mean_q: 1.168176
 310386/1000000: episode: 3104, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 57.984, mean reward: 0.580 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.728, 10.098], loss: 0.001667, mae: 0.043805, mean_q: 1.167949
 310486/1000000: episode: 3105, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 58.738, mean reward: 0.587 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.586, 10.098], loss: 0.001702, mae: 0.045032, mean_q: 1.168352
 310586/1000000: episode: 3106, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 59.089, mean reward: 0.591 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.098], loss: 0.001745, mae: 0.045298, mean_q: 1.169985
 310686/1000000: episode: 3107, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 60.685, mean reward: 0.607 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.465, 10.098], loss: 0.001607, mae: 0.043857, mean_q: 1.162781
 310786/1000000: episode: 3108, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.241, mean reward: 0.572 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.511, 10.277], loss: 0.001611, mae: 0.043526, mean_q: 1.165160
 310886/1000000: episode: 3109, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.658, mean reward: 0.597 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.206, 10.098], loss: 0.001693, mae: 0.044689, mean_q: 1.165685
 310986/1000000: episode: 3110, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 57.867, mean reward: 0.579 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.261, 10.199], loss: 0.001830, mae: 0.045678, mean_q: 1.165199
 311086/1000000: episode: 3111, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 58.699, mean reward: 0.587 [0.509, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.231, 10.112], loss: 0.001634, mae: 0.043774, mean_q: 1.164300
 311186/1000000: episode: 3112, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 59.282, mean reward: 0.593 [0.509, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.360, 10.126], loss: 0.001590, mae: 0.043345, mean_q: 1.163847
 311286/1000000: episode: 3113, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.301, mean reward: 0.583 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.298, 10.110], loss: 0.001630, mae: 0.043828, mean_q: 1.167854
 311386/1000000: episode: 3114, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 60.966, mean reward: 0.610 [0.522, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.321, 10.098], loss: 0.001582, mae: 0.043285, mean_q: 1.162549
 311486/1000000: episode: 3115, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.217, mean reward: 0.592 [0.502, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.729, 10.098], loss: 0.001574, mae: 0.043335, mean_q: 1.164610
 311586/1000000: episode: 3116, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.332, mean reward: 0.583 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.598, 10.280], loss: 0.001426, mae: 0.041765, mean_q: 1.165430
 311686/1000000: episode: 3117, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 59.159, mean reward: 0.592 [0.513, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.671, 10.161], loss: 0.001557, mae: 0.042576, mean_q: 1.165128
 311786/1000000: episode: 3118, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.950, mean reward: 0.590 [0.512, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.390, 10.098], loss: 0.001618, mae: 0.043275, mean_q: 1.165764
 311886/1000000: episode: 3119, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 58.007, mean reward: 0.580 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.587, 10.164], loss: 0.001612, mae: 0.043683, mean_q: 1.166619
 311986/1000000: episode: 3120, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 58.858, mean reward: 0.589 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.784, 10.098], loss: 0.001576, mae: 0.043132, mean_q: 1.165045
 312086/1000000: episode: 3121, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 59.531, mean reward: 0.595 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.986, 10.302], loss: 0.001612, mae: 0.042862, mean_q: 1.165164
 312186/1000000: episode: 3122, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 58.661, mean reward: 0.587 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.593, 10.266], loss: 0.001598, mae: 0.043116, mean_q: 1.164699
 312286/1000000: episode: 3123, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 59.750, mean reward: 0.598 [0.515, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.614, 10.098], loss: 0.001619, mae: 0.043879, mean_q: 1.166274
 312386/1000000: episode: 3124, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 58.521, mean reward: 0.585 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.633, 10.124], loss: 0.001618, mae: 0.043836, mean_q: 1.167488
 312486/1000000: episode: 3125, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.090, mean reward: 0.571 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.208], loss: 0.001644, mae: 0.043531, mean_q: 1.165516
 312586/1000000: episode: 3126, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 60.595, mean reward: 0.606 [0.519, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.501, 10.108], loss: 0.001521, mae: 0.042277, mean_q: 1.159589
 312686/1000000: episode: 3127, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.760, mean reward: 0.578 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.875, 10.216], loss: 0.001707, mae: 0.043815, mean_q: 1.166326
 312786/1000000: episode: 3128, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 59.695, mean reward: 0.597 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.479, 10.098], loss: 0.001504, mae: 0.042017, mean_q: 1.164526
 312886/1000000: episode: 3129, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 60.015, mean reward: 0.600 [0.511, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.191, 10.217], loss: 0.001595, mae: 0.043410, mean_q: 1.165730
 312986/1000000: episode: 3130, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 57.902, mean reward: 0.579 [0.499, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.170], loss: 0.001750, mae: 0.044784, mean_q: 1.164389
 313086/1000000: episode: 3131, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.352, mean reward: 0.594 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.003, 10.098], loss: 0.001588, mae: 0.043420, mean_q: 1.165820
 313186/1000000: episode: 3132, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.289, mean reward: 0.573 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.308], loss: 0.001637, mae: 0.044182, mean_q: 1.164043
 313286/1000000: episode: 3133, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 64.439, mean reward: 0.644 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.330, 10.351], loss: 0.001580, mae: 0.042832, mean_q: 1.164271
 313386/1000000: episode: 3134, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 57.436, mean reward: 0.574 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.340, 10.124], loss: 0.001531, mae: 0.042962, mean_q: 1.163906
 313486/1000000: episode: 3135, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.035, mean reward: 0.580 [0.497, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.574, 10.244], loss: 0.001615, mae: 0.043771, mean_q: 1.167616
 313586/1000000: episode: 3136, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.277, mean reward: 0.583 [0.511, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.163, 10.098], loss: 0.001604, mae: 0.043089, mean_q: 1.168354
 313686/1000000: episode: 3137, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.685, mean reward: 0.577 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.540, 10.220], loss: 0.001557, mae: 0.042785, mean_q: 1.159318
 313786/1000000: episode: 3138, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 58.990, mean reward: 0.590 [0.499, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.078, 10.098], loss: 0.001551, mae: 0.043490, mean_q: 1.164871
 313886/1000000: episode: 3139, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.986, mean reward: 0.590 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.149, 10.144], loss: 0.001731, mae: 0.044841, mean_q: 1.165511
 313986/1000000: episode: 3140, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 59.623, mean reward: 0.596 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.471, 10.098], loss: 0.001592, mae: 0.043148, mean_q: 1.159918
 314086/1000000: episode: 3141, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 58.546, mean reward: 0.585 [0.514, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.453, 10.098], loss: 0.001522, mae: 0.042015, mean_q: 1.163923
 314186/1000000: episode: 3142, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 59.070, mean reward: 0.591 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.590, 10.098], loss: 0.001693, mae: 0.044762, mean_q: 1.165804
 314286/1000000: episode: 3143, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 58.583, mean reward: 0.586 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.305, 10.098], loss: 0.001689, mae: 0.044491, mean_q: 1.164848
 314386/1000000: episode: 3144, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 58.437, mean reward: 0.584 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.098], loss: 0.001567, mae: 0.043150, mean_q: 1.164481
 314486/1000000: episode: 3145, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 61.491, mean reward: 0.615 [0.509, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.865, 10.250], loss: 0.001625, mae: 0.043698, mean_q: 1.163095
 314586/1000000: episode: 3146, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.049, mean reward: 0.570 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.987, 10.207], loss: 0.001709, mae: 0.045043, mean_q: 1.167196
 314686/1000000: episode: 3147, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 60.654, mean reward: 0.607 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.748, 10.143], loss: 0.001657, mae: 0.044367, mean_q: 1.168190
 314786/1000000: episode: 3148, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 59.334, mean reward: 0.593 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.204], loss: 0.001666, mae: 0.043819, mean_q: 1.169191
 314886/1000000: episode: 3149, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.784, mean reward: 0.588 [0.515, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.304, 10.098], loss: 0.001532, mae: 0.042432, mean_q: 1.167409
 314986/1000000: episode: 3150, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.988, mean reward: 0.580 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.233, 10.098], loss: 0.001532, mae: 0.042516, mean_q: 1.167232
 315086/1000000: episode: 3151, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 57.963, mean reward: 0.580 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.098], loss: 0.001576, mae: 0.042901, mean_q: 1.166400
 315186/1000000: episode: 3152, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 56.569, mean reward: 0.566 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.119, 10.149], loss: 0.001544, mae: 0.042442, mean_q: 1.166222
 315286/1000000: episode: 3153, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.393, mean reward: 0.574 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.085, 10.379], loss: 0.001584, mae: 0.043033, mean_q: 1.167087
 315386/1000000: episode: 3154, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 61.580, mean reward: 0.616 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.365, 10.330], loss: 0.001613, mae: 0.042900, mean_q: 1.163484
 315486/1000000: episode: 3155, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.105, mean reward: 0.591 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.167, 10.259], loss: 0.001483, mae: 0.041889, mean_q: 1.164932
 315586/1000000: episode: 3156, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 58.856, mean reward: 0.589 [0.507, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.864, 10.354], loss: 0.001442, mae: 0.041595, mean_q: 1.163320
 315686/1000000: episode: 3157, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 57.023, mean reward: 0.570 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.755, 10.343], loss: 0.001546, mae: 0.042627, mean_q: 1.164163
 315786/1000000: episode: 3158, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 57.292, mean reward: 0.573 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.200], loss: 0.001477, mae: 0.041256, mean_q: 1.162659
 315886/1000000: episode: 3159, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.033, mean reward: 0.580 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.098], loss: 0.001514, mae: 0.042004, mean_q: 1.164951
 315986/1000000: episode: 3160, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 58.629, mean reward: 0.586 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.005, 10.098], loss: 0.001559, mae: 0.042336, mean_q: 1.164925
 316086/1000000: episode: 3161, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.906, mean reward: 0.589 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.531, 10.098], loss: 0.001485, mae: 0.041989, mean_q: 1.164882
 316186/1000000: episode: 3162, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 57.608, mean reward: 0.576 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.890, 10.184], loss: 0.001499, mae: 0.042302, mean_q: 1.163774
 316286/1000000: episode: 3163, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 60.888, mean reward: 0.609 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.679, 10.203], loss: 0.001430, mae: 0.041360, mean_q: 1.165943
 316386/1000000: episode: 3164, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 56.019, mean reward: 0.560 [0.502, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.440, 10.112], loss: 0.001491, mae: 0.042134, mean_q: 1.164827
 316486/1000000: episode: 3165, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 62.194, mean reward: 0.622 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.098], loss: 0.001492, mae: 0.042069, mean_q: 1.161678
 316586/1000000: episode: 3166, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 60.822, mean reward: 0.608 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.522, 10.337], loss: 0.001490, mae: 0.041741, mean_q: 1.164973
 316686/1000000: episode: 3167, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.087, mean reward: 0.591 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.098], loss: 0.001585, mae: 0.043178, mean_q: 1.165140
 316786/1000000: episode: 3168, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 64.342, mean reward: 0.643 [0.509, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.484], loss: 0.001447, mae: 0.041235, mean_q: 1.167096
 316886/1000000: episode: 3169, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.483, mean reward: 0.585 [0.504, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.135], loss: 0.001482, mae: 0.041806, mean_q: 1.167926
 316986/1000000: episode: 3170, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 58.648, mean reward: 0.586 [0.509, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.478, 10.183], loss: 0.001602, mae: 0.042845, mean_q: 1.170612
 317086/1000000: episode: 3171, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.941, mean reward: 0.579 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.182, 10.098], loss: 0.001458, mae: 0.040822, mean_q: 1.166237
 317186/1000000: episode: 3172, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.925, mean reward: 0.589 [0.508, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.306, 10.098], loss: 0.001526, mae: 0.042488, mean_q: 1.166793
 317286/1000000: episode: 3173, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.516, mean reward: 0.585 [0.503, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.001445, mae: 0.041130, mean_q: 1.168147
 317386/1000000: episode: 3174, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 59.086, mean reward: 0.591 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.243, 10.150], loss: 0.001532, mae: 0.041935, mean_q: 1.167240
 317486/1000000: episode: 3175, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 56.839, mean reward: 0.568 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.120, 10.184], loss: 0.001444, mae: 0.041157, mean_q: 1.169032
 317586/1000000: episode: 3176, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 58.736, mean reward: 0.587 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.445, 10.098], loss: 0.001480, mae: 0.041667, mean_q: 1.165539
 317686/1000000: episode: 3177, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 61.016, mean reward: 0.610 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.364], loss: 0.001496, mae: 0.041533, mean_q: 1.166316
 317786/1000000: episode: 3178, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 59.492, mean reward: 0.595 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.159, 10.272], loss: 0.001484, mae: 0.041270, mean_q: 1.167242
 317886/1000000: episode: 3179, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 61.735, mean reward: 0.617 [0.517, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.687, 10.172], loss: 0.001458, mae: 0.041391, mean_q: 1.168341
 317986/1000000: episode: 3180, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 57.795, mean reward: 0.578 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.807, 10.157], loss: 0.001359, mae: 0.039944, mean_q: 1.166108
 318086/1000000: episode: 3181, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 56.887, mean reward: 0.569 [0.498, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.934, 10.200], loss: 0.001341, mae: 0.040273, mean_q: 1.167734
 318186/1000000: episode: 3182, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 58.796, mean reward: 0.588 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.280, 10.277], loss: 0.001388, mae: 0.040779, mean_q: 1.166374
 318286/1000000: episode: 3183, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 60.339, mean reward: 0.603 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.910, 10.098], loss: 0.001471, mae: 0.041654, mean_q: 1.163248
 318386/1000000: episode: 3184, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 56.951, mean reward: 0.570 [0.506, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.098], loss: 0.001441, mae: 0.041082, mean_q: 1.168900
 318486/1000000: episode: 3185, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 57.614, mean reward: 0.576 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.934, 10.225], loss: 0.001396, mae: 0.040479, mean_q: 1.163379
 318586/1000000: episode: 3186, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 60.195, mean reward: 0.602 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.752, 10.269], loss: 0.001391, mae: 0.040023, mean_q: 1.166247
 318686/1000000: episode: 3187, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: 57.902, mean reward: 0.579 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.685, 10.098], loss: 0.001455, mae: 0.041174, mean_q: 1.165172
 318786/1000000: episode: 3188, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 56.358, mean reward: 0.564 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.245, 10.098], loss: 0.001356, mae: 0.039729, mean_q: 1.167499
 318886/1000000: episode: 3189, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 57.770, mean reward: 0.578 [0.520, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.639, 10.201], loss: 0.001387, mae: 0.040185, mean_q: 1.166366
 318986/1000000: episode: 3190, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 58.796, mean reward: 0.588 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.878, 10.234], loss: 0.001378, mae: 0.040450, mean_q: 1.165278
 319086/1000000: episode: 3191, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 57.032, mean reward: 0.570 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.004, 10.098], loss: 0.001348, mae: 0.039907, mean_q: 1.162252
 319186/1000000: episode: 3192, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 62.108, mean reward: 0.621 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.786, 10.098], loss: 0.001374, mae: 0.040291, mean_q: 1.164194
 319286/1000000: episode: 3193, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 58.253, mean reward: 0.583 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.006, 10.247], loss: 0.001357, mae: 0.039786, mean_q: 1.161845
 319386/1000000: episode: 3194, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.170, mean reward: 0.582 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.335, 10.098], loss: 0.001443, mae: 0.041327, mean_q: 1.163238
 319486/1000000: episode: 3195, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 59.573, mean reward: 0.596 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.101, 10.098], loss: 0.001372, mae: 0.040071, mean_q: 1.166539
 319586/1000000: episode: 3196, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 60.011, mean reward: 0.600 [0.527, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.438, 10.098], loss: 0.001398, mae: 0.040222, mean_q: 1.166734
 319686/1000000: episode: 3197, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 60.265, mean reward: 0.603 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.790, 10.257], loss: 0.001508, mae: 0.041768, mean_q: 1.167016
 319786/1000000: episode: 3198, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.006, mean reward: 0.590 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.837, 10.098], loss: 0.001262, mae: 0.038902, mean_q: 1.164364
 319886/1000000: episode: 3199, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 56.518, mean reward: 0.565 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.202, 10.098], loss: 0.001371, mae: 0.040347, mean_q: 1.166589
 319986/1000000: episode: 3200, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.813, mean reward: 0.588 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.911, 10.098], loss: 0.001371, mae: 0.040609, mean_q: 1.164593
 320086/1000000: episode: 3201, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.505, mean reward: 0.575 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.586, 10.098], loss: 0.001424, mae: 0.040923, mean_q: 1.162589
 320186/1000000: episode: 3202, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.521, mean reward: 0.585 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.918, 10.279], loss: 0.001431, mae: 0.041116, mean_q: 1.164210
 320286/1000000: episode: 3203, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.696, mean reward: 0.587 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.149], loss: 0.001440, mae: 0.041150, mean_q: 1.164517
 320386/1000000: episode: 3204, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 56.801, mean reward: 0.568 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.947, 10.098], loss: 0.001288, mae: 0.039472, mean_q: 1.163611
 320486/1000000: episode: 3205, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.942, mean reward: 0.589 [0.499, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.917, 10.141], loss: 0.001312, mae: 0.039238, mean_q: 1.160805
 320586/1000000: episode: 3206, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 59.083, mean reward: 0.591 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.379, 10.355], loss: 0.001296, mae: 0.038830, mean_q: 1.162477
 320686/1000000: episode: 3207, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 61.249, mean reward: 0.612 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.176, 10.098], loss: 0.001341, mae: 0.039700, mean_q: 1.161538
 320786/1000000: episode: 3208, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.447, mean reward: 0.574 [0.504, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.272, 10.142], loss: 0.001368, mae: 0.040189, mean_q: 1.165993
 320886/1000000: episode: 3209, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 57.652, mean reward: 0.577 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.148, 10.192], loss: 0.001446, mae: 0.041644, mean_q: 1.166082
 320986/1000000: episode: 3210, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 58.462, mean reward: 0.585 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.514, 10.098], loss: 0.001430, mae: 0.040707, mean_q: 1.166126
 321086/1000000: episode: 3211, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.218, mean reward: 0.592 [0.497, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.105, 10.195], loss: 0.001414, mae: 0.040766, mean_q: 1.164430
 321186/1000000: episode: 3212, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.052, mean reward: 0.581 [0.503, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.729, 10.112], loss: 0.001395, mae: 0.040628, mean_q: 1.167008
 321286/1000000: episode: 3213, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 61.824, mean reward: 0.618 [0.514, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.481, 10.384], loss: 0.001414, mae: 0.041294, mean_q: 1.162526
 321386/1000000: episode: 3214, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 60.097, mean reward: 0.601 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.904, 10.346], loss: 0.001438, mae: 0.040847, mean_q: 1.169105
 321486/1000000: episode: 3215, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.846, mean reward: 0.588 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.087, 10.098], loss: 0.001413, mae: 0.040568, mean_q: 1.167295
 321586/1000000: episode: 3216, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.552, mean reward: 0.586 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.184], loss: 0.001371, mae: 0.040766, mean_q: 1.166263
 321686/1000000: episode: 3217, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.227, mean reward: 0.572 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.678, 10.114], loss: 0.001347, mae: 0.039853, mean_q: 1.162475
 321786/1000000: episode: 3218, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.977, mean reward: 0.580 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.589, 10.152], loss: 0.001427, mae: 0.040800, mean_q: 1.159159
 321886/1000000: episode: 3219, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 60.511, mean reward: 0.605 [0.505, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.001510, mae: 0.042512, mean_q: 1.163744
 321986/1000000: episode: 3220, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 57.416, mean reward: 0.574 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.098], loss: 0.001398, mae: 0.040322, mean_q: 1.161275
 322086/1000000: episode: 3221, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 57.962, mean reward: 0.580 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.487, 10.215], loss: 0.001318, mae: 0.039249, mean_q: 1.161210
 322186/1000000: episode: 3222, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 60.337, mean reward: 0.603 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.720, 10.098], loss: 0.001410, mae: 0.040386, mean_q: 1.161746
 322286/1000000: episode: 3223, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 61.486, mean reward: 0.615 [0.518, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.495, 10.487], loss: 0.001440, mae: 0.041119, mean_q: 1.160517
 322386/1000000: episode: 3224, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.806, mean reward: 0.588 [0.499, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.438, 10.098], loss: 0.001418, mae: 0.041483, mean_q: 1.162603
 322486/1000000: episode: 3225, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 59.039, mean reward: 0.590 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.040, 10.337], loss: 0.001540, mae: 0.042585, mean_q: 1.164946
 322586/1000000: episode: 3226, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 58.928, mean reward: 0.589 [0.500, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.572, 10.098], loss: 0.001476, mae: 0.041538, mean_q: 1.165404
 322686/1000000: episode: 3227, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 56.893, mean reward: 0.569 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.974, 10.185], loss: 0.001422, mae: 0.041079, mean_q: 1.161974
 322786/1000000: episode: 3228, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.019, mean reward: 0.590 [0.516, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.577, 10.098], loss: 0.001548, mae: 0.042269, mean_q: 1.162573
 322886/1000000: episode: 3229, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.373, mean reward: 0.584 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.098], loss: 0.001308, mae: 0.039376, mean_q: 1.158770
 322986/1000000: episode: 3230, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 60.211, mean reward: 0.602 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.044, 10.373], loss: 0.001362, mae: 0.039789, mean_q: 1.158072
 323086/1000000: episode: 3231, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 59.707, mean reward: 0.597 [0.513, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.877, 10.098], loss: 0.001365, mae: 0.040244, mean_q: 1.160109
 323186/1000000: episode: 3232, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.747, mean reward: 0.597 [0.513, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.334, 10.098], loss: 0.001484, mae: 0.041878, mean_q: 1.163323
 323286/1000000: episode: 3233, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.478, mean reward: 0.595 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.237, 10.098], loss: 0.001418, mae: 0.041026, mean_q: 1.164669
 323386/1000000: episode: 3234, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 60.358, mean reward: 0.604 [0.510, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.204], loss: 0.001398, mae: 0.040865, mean_q: 1.161630
 323486/1000000: episode: 3235, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 59.614, mean reward: 0.596 [0.501, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.333, 10.098], loss: 0.001380, mae: 0.039975, mean_q: 1.163976
 323586/1000000: episode: 3236, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 65.653, mean reward: 0.657 [0.499, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.317, 10.098], loss: 0.001290, mae: 0.039548, mean_q: 1.165441
 323686/1000000: episode: 3237, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: 57.363, mean reward: 0.574 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.288, 10.170], loss: 0.001446, mae: 0.041028, mean_q: 1.165260
 323786/1000000: episode: 3238, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 56.975, mean reward: 0.570 [0.498, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.297], loss: 0.001416, mae: 0.040936, mean_q: 1.169194
 323886/1000000: episode: 3239, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 57.693, mean reward: 0.577 [0.511, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.612, 10.098], loss: 0.001359, mae: 0.040098, mean_q: 1.166436
 323986/1000000: episode: 3240, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: 57.915, mean reward: 0.579 [0.501, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.247, 10.164], loss: 0.001431, mae: 0.041133, mean_q: 1.169044
 324086/1000000: episode: 3241, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.793, mean reward: 0.588 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.711, 10.098], loss: 0.001474, mae: 0.041661, mean_q: 1.167430
 324186/1000000: episode: 3242, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.571, mean reward: 0.576 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.721, 10.098], loss: 0.001368, mae: 0.040397, mean_q: 1.165898
 324286/1000000: episode: 3243, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.492, mean reward: 0.585 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.564, 10.098], loss: 0.001519, mae: 0.042400, mean_q: 1.165141
 324386/1000000: episode: 3244, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.013, mean reward: 0.580 [0.520, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.433, 10.140], loss: 0.001407, mae: 0.040856, mean_q: 1.163875
 324486/1000000: episode: 3245, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.589, mean reward: 0.586 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.742, 10.098], loss: 0.001354, mae: 0.040337, mean_q: 1.162787
 324586/1000000: episode: 3246, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 60.189, mean reward: 0.602 [0.509, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.568, 10.178], loss: 0.001367, mae: 0.040420, mean_q: 1.164075
 324686/1000000: episode: 3247, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.671, mean reward: 0.577 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.656, 10.098], loss: 0.001333, mae: 0.040184, mean_q: 1.165741
 324786/1000000: episode: 3248, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.784, mean reward: 0.588 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.943, 10.282], loss: 0.001336, mae: 0.040109, mean_q: 1.161916
 324886/1000000: episode: 3249, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.900, mean reward: 0.579 [0.510, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.373, 10.098], loss: 0.001501, mae: 0.042095, mean_q: 1.165221
 324986/1000000: episode: 3250, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 60.767, mean reward: 0.608 [0.517, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.208, 10.427], loss: 0.001405, mae: 0.041130, mean_q: 1.163739
 325086/1000000: episode: 3251, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 60.768, mean reward: 0.608 [0.510, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.661, 10.098], loss: 0.001437, mae: 0.041299, mean_q: 1.165701
 325186/1000000: episode: 3252, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.847, mean reward: 0.578 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.216], loss: 0.001356, mae: 0.040011, mean_q: 1.165465
 325286/1000000: episode: 3253, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 59.199, mean reward: 0.592 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.098], loss: 0.001475, mae: 0.041801, mean_q: 1.166176
 325386/1000000: episode: 3254, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 56.692, mean reward: 0.567 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.841, 10.098], loss: 0.001350, mae: 0.039961, mean_q: 1.164836
 325486/1000000: episode: 3255, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 64.283, mean reward: 0.643 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.098], loss: 0.001404, mae: 0.040805, mean_q: 1.167432
 325586/1000000: episode: 3256, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 57.899, mean reward: 0.579 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.471, 10.176], loss: 0.001449, mae: 0.041057, mean_q: 1.168641
 325686/1000000: episode: 3257, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.839, mean reward: 0.578 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.001461, mae: 0.042233, mean_q: 1.168273
 325786/1000000: episode: 3258, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 57.580, mean reward: 0.576 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.526, 10.178], loss: 0.001435, mae: 0.041215, mean_q: 1.164782
 325886/1000000: episode: 3259, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 60.660, mean reward: 0.607 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.259, 10.098], loss: 0.001445, mae: 0.041516, mean_q: 1.167510
 325986/1000000: episode: 3260, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.285, mean reward: 0.583 [0.502, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.651, 10.226], loss: 0.001572, mae: 0.043309, mean_q: 1.167874
 326086/1000000: episode: 3261, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 59.644, mean reward: 0.596 [0.505, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.201, 10.319], loss: 0.001453, mae: 0.041503, mean_q: 1.168680
 326186/1000000: episode: 3262, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 59.060, mean reward: 0.591 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.743, 10.098], loss: 0.001459, mae: 0.041682, mean_q: 1.170526
 326286/1000000: episode: 3263, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 60.832, mean reward: 0.608 [0.506, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.036, 10.098], loss: 0.001434, mae: 0.041307, mean_q: 1.167094
 326386/1000000: episode: 3264, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 60.742, mean reward: 0.607 [0.512, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.592, 10.098], loss: 0.001388, mae: 0.040709, mean_q: 1.171684
 326486/1000000: episode: 3265, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 59.950, mean reward: 0.600 [0.518, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.132, 10.395], loss: 0.001456, mae: 0.041021, mean_q: 1.168057
 326586/1000000: episode: 3266, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 60.362, mean reward: 0.604 [0.513, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.916, 10.098], loss: 0.001474, mae: 0.041642, mean_q: 1.169345
 326686/1000000: episode: 3267, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 60.806, mean reward: 0.608 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.402, 10.376], loss: 0.001465, mae: 0.041047, mean_q: 1.169216
 326786/1000000: episode: 3268, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.315, mean reward: 0.583 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.642, 10.170], loss: 0.001309, mae: 0.038497, mean_q: 1.171866
 326886/1000000: episode: 3269, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 57.937, mean reward: 0.579 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.604, 10.098], loss: 0.001469, mae: 0.041188, mean_q: 1.173295
 326986/1000000: episode: 3270, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 58.594, mean reward: 0.586 [0.510, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.629, 10.134], loss: 0.001312, mae: 0.039310, mean_q: 1.171460
 327086/1000000: episode: 3271, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 57.510, mean reward: 0.575 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.001, 10.174], loss: 0.001453, mae: 0.040740, mean_q: 1.170108
 327186/1000000: episode: 3272, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.362, mean reward: 0.594 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.712, 10.303], loss: 0.001382, mae: 0.040442, mean_q: 1.170129
 327286/1000000: episode: 3273, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.663, mean reward: 0.587 [0.516, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.594, 10.098], loss: 0.001410, mae: 0.041006, mean_q: 1.169753
 327386/1000000: episode: 3274, duration: 0.922s, episode steps: 100, steps per second: 109, episode reward: 60.148, mean reward: 0.601 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.913, 10.098], loss: 0.001316, mae: 0.039749, mean_q: 1.170102
 327486/1000000: episode: 3275, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 56.969, mean reward: 0.570 [0.505, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.797, 10.168], loss: 0.001363, mae: 0.040141, mean_q: 1.167594
 327586/1000000: episode: 3276, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.855, mean reward: 0.579 [0.497, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.252], loss: 0.001425, mae: 0.040811, mean_q: 1.171337
 327686/1000000: episode: 3277, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.537, mean reward: 0.575 [0.500, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.497, 10.104], loss: 0.001451, mae: 0.040926, mean_q: 1.166698
 327786/1000000: episode: 3278, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 60.418, mean reward: 0.604 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.375, 10.129], loss: 0.001303, mae: 0.039364, mean_q: 1.164318
 327886/1000000: episode: 3279, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.624, mean reward: 0.596 [0.508, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.374, 10.319], loss: 0.001455, mae: 0.040898, mean_q: 1.165680
 327986/1000000: episode: 3280, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.379, mean reward: 0.584 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.128, 10.098], loss: 0.001402, mae: 0.041028, mean_q: 1.167369
 328086/1000000: episode: 3281, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 60.131, mean reward: 0.601 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.460, 10.098], loss: 0.001513, mae: 0.041986, mean_q: 1.170600
 328186/1000000: episode: 3282, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 60.112, mean reward: 0.601 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.485, 10.098], loss: 0.001504, mae: 0.042434, mean_q: 1.169490
 328286/1000000: episode: 3283, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.941, mean reward: 0.579 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.195], loss: 0.001457, mae: 0.041398, mean_q: 1.170403
 328386/1000000: episode: 3284, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 59.684, mean reward: 0.597 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.857, 10.098], loss: 0.001574, mae: 0.042360, mean_q: 1.169944
 328486/1000000: episode: 3285, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.386, mean reward: 0.584 [0.501, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.060, 10.098], loss: 0.001454, mae: 0.041404, mean_q: 1.171105
 328586/1000000: episode: 3286, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.234, mean reward: 0.582 [0.500, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.394, 10.259], loss: 0.001624, mae: 0.043210, mean_q: 1.170542
 328686/1000000: episode: 3287, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.960, mean reward: 0.590 [0.508, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.262], loss: 0.001403, mae: 0.040618, mean_q: 1.164503
 328786/1000000: episode: 3288, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.295, mean reward: 0.573 [0.499, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.114, 10.362], loss: 0.001408, mae: 0.040590, mean_q: 1.167035
 328886/1000000: episode: 3289, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.457, mean reward: 0.595 [0.502, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.600, 10.098], loss: 0.001517, mae: 0.041473, mean_q: 1.165581
 328986/1000000: episode: 3290, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 60.067, mean reward: 0.601 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.895, 10.098], loss: 0.001369, mae: 0.039976, mean_q: 1.165290
 329086/1000000: episode: 3291, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.460, mean reward: 0.585 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.286], loss: 0.001575, mae: 0.042766, mean_q: 1.168794
 329186/1000000: episode: 3292, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 56.865, mean reward: 0.569 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.847, 10.098], loss: 0.001538, mae: 0.041967, mean_q: 1.165992
 329286/1000000: episode: 3293, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.412, mean reward: 0.604 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.735, 10.215], loss: 0.001510, mae: 0.041887, mean_q: 1.171235
 329386/1000000: episode: 3294, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 62.506, mean reward: 0.625 [0.498, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.321], loss: 0.001463, mae: 0.040922, mean_q: 1.168568
 329486/1000000: episode: 3295, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.186, mean reward: 0.592 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.142, 10.188], loss: 0.001358, mae: 0.040632, mean_q: 1.171778
 329586/1000000: episode: 3296, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.192, mean reward: 0.572 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.301, 10.376], loss: 0.001646, mae: 0.042982, mean_q: 1.167868
 329686/1000000: episode: 3297, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 61.146, mean reward: 0.611 [0.510, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.186, 10.098], loss: 0.001506, mae: 0.041500, mean_q: 1.170146
 329786/1000000: episode: 3298, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.083, mean reward: 0.581 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.098], loss: 0.001388, mae: 0.039864, mean_q: 1.169698
 329886/1000000: episode: 3299, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.592, mean reward: 0.596 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.357, 10.098], loss: 0.001463, mae: 0.041179, mean_q: 1.173377
 329986/1000000: episode: 3300, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.611, mean reward: 0.576 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.977, 10.107], loss: 0.001416, mae: 0.040760, mean_q: 1.169816
 330086/1000000: episode: 3301, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 60.000, mean reward: 0.600 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.369, 10.365], loss: 0.001517, mae: 0.041495, mean_q: 1.166991
 330186/1000000: episode: 3302, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 58.075, mean reward: 0.581 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.337, 10.192], loss: 0.001484, mae: 0.041514, mean_q: 1.168392
 330286/1000000: episode: 3303, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.965, mean reward: 0.590 [0.498, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.923, 10.106], loss: 0.001538, mae: 0.042670, mean_q: 1.171141
 330386/1000000: episode: 3304, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.508, mean reward: 0.585 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.941, 10.098], loss: 0.001415, mae: 0.040376, mean_q: 1.170483
 330486/1000000: episode: 3305, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 59.531, mean reward: 0.595 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.566, 10.172], loss: 0.001507, mae: 0.042258, mean_q: 1.166365
 330586/1000000: episode: 3306, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 58.861, mean reward: 0.589 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.484, 10.350], loss: 0.001414, mae: 0.041179, mean_q: 1.167055
 330686/1000000: episode: 3307, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 57.186, mean reward: 0.572 [0.506, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.923, 10.125], loss: 0.001544, mae: 0.041943, mean_q: 1.167913
 330786/1000000: episode: 3308, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 58.241, mean reward: 0.582 [0.501, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.226, 10.244], loss: 0.001534, mae: 0.042062, mean_q: 1.167766
 330886/1000000: episode: 3309, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 60.530, mean reward: 0.605 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.812, 10.289], loss: 0.001491, mae: 0.041973, mean_q: 1.169036
 330986/1000000: episode: 3310, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 60.289, mean reward: 0.603 [0.512, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.756, 10.098], loss: 0.001599, mae: 0.042726, mean_q: 1.168746
 331086/1000000: episode: 3311, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.395, mean reward: 0.594 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.098], loss: 0.001523, mae: 0.042362, mean_q: 1.172631
 331186/1000000: episode: 3312, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.324, mean reward: 0.593 [0.511, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.124, 10.098], loss: 0.001521, mae: 0.041815, mean_q: 1.171302
 331286/1000000: episode: 3313, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 60.180, mean reward: 0.602 [0.510, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.242], loss: 0.001541, mae: 0.041910, mean_q: 1.168616
 331386/1000000: episode: 3314, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.855, mean reward: 0.589 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.475, 10.098], loss: 0.001454, mae: 0.040899, mean_q: 1.167881
 331486/1000000: episode: 3315, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.544, mean reward: 0.605 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.084, 10.347], loss: 0.001406, mae: 0.040288, mean_q: 1.170249
 331586/1000000: episode: 3316, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 59.359, mean reward: 0.594 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.098], loss: 0.001410, mae: 0.040659, mean_q: 1.168680
 331686/1000000: episode: 3317, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 58.329, mean reward: 0.583 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.295, 10.098], loss: 0.001459, mae: 0.040998, mean_q: 1.167489
 331786/1000000: episode: 3318, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 59.833, mean reward: 0.598 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.361, 10.098], loss: 0.001383, mae: 0.040002, mean_q: 1.167294
 331886/1000000: episode: 3319, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.103, mean reward: 0.581 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.416, 10.203], loss: 0.001433, mae: 0.040755, mean_q: 1.165910
 331986/1000000: episode: 3320, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 57.864, mean reward: 0.579 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.385, 10.098], loss: 0.001463, mae: 0.041528, mean_q: 1.169037
 332086/1000000: episode: 3321, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.294, mean reward: 0.593 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.050, 10.098], loss: 0.001396, mae: 0.040473, mean_q: 1.167112
 332186/1000000: episode: 3322, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 58.220, mean reward: 0.582 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.286, 10.098], loss: 0.001514, mae: 0.042604, mean_q: 1.171178
 332286/1000000: episode: 3323, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.349, mean reward: 0.573 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.113, 10.343], loss: 0.001431, mae: 0.041343, mean_q: 1.167219
 332386/1000000: episode: 3324, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 56.921, mean reward: 0.569 [0.502, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.460, 10.108], loss: 0.001580, mae: 0.042238, mean_q: 1.168234
 332486/1000000: episode: 3325, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.681, mean reward: 0.587 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.567, 10.098], loss: 0.001474, mae: 0.041139, mean_q: 1.164774
 332586/1000000: episode: 3326, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 61.369, mean reward: 0.614 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.186, 10.271], loss: 0.001526, mae: 0.042096, mean_q: 1.166387
 332686/1000000: episode: 3327, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 58.073, mean reward: 0.581 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.269, 10.112], loss: 0.001544, mae: 0.042408, mean_q: 1.170238
 332786/1000000: episode: 3328, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 58.043, mean reward: 0.580 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.365, 10.098], loss: 0.001560, mae: 0.042684, mean_q: 1.170015
 332886/1000000: episode: 3329, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 57.306, mean reward: 0.573 [0.501, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.823, 10.245], loss: 0.001446, mae: 0.041203, mean_q: 1.166476
 332986/1000000: episode: 3330, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 60.252, mean reward: 0.603 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.588, 10.234], loss: 0.001417, mae: 0.041194, mean_q: 1.173156
 333086/1000000: episode: 3331, duration: 0.778s, episode steps: 100, steps per second: 128, episode reward: 59.380, mean reward: 0.594 [0.508, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.120, 10.098], loss: 0.001415, mae: 0.041054, mean_q: 1.169134
 333186/1000000: episode: 3332, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 61.772, mean reward: 0.618 [0.516, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.929, 10.098], loss: 0.001448, mae: 0.041120, mean_q: 1.164328
 333286/1000000: episode: 3333, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 56.371, mean reward: 0.564 [0.509, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.760, 10.098], loss: 0.001445, mae: 0.041194, mean_q: 1.166628
 333386/1000000: episode: 3334, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.568, mean reward: 0.586 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.812, 10.125], loss: 0.001513, mae: 0.041402, mean_q: 1.166224
 333486/1000000: episode: 3335, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.736, mean reward: 0.587 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.809, 10.253], loss: 0.001468, mae: 0.041344, mean_q: 1.165775
 333586/1000000: episode: 3336, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 57.327, mean reward: 0.573 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.590, 10.098], loss: 0.001515, mae: 0.042486, mean_q: 1.167370
 333686/1000000: episode: 3337, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 59.976, mean reward: 0.600 [0.511, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.098], loss: 0.001511, mae: 0.041938, mean_q: 1.167511
 333786/1000000: episode: 3338, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 58.848, mean reward: 0.588 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.311, 10.264], loss: 0.001428, mae: 0.040904, mean_q: 1.167734
 333886/1000000: episode: 3339, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 60.801, mean reward: 0.608 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.863, 10.098], loss: 0.001523, mae: 0.042487, mean_q: 1.169727
 333986/1000000: episode: 3340, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.503, mean reward: 0.585 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.367, 10.313], loss: 0.001465, mae: 0.041402, mean_q: 1.166859
 334086/1000000: episode: 3341, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.908, mean reward: 0.589 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.583, 10.098], loss: 0.001425, mae: 0.040438, mean_q: 1.167461
 334186/1000000: episode: 3342, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 62.570, mean reward: 0.626 [0.502, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.461, 10.263], loss: 0.001468, mae: 0.041078, mean_q: 1.167555
 334286/1000000: episode: 3343, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 60.281, mean reward: 0.603 [0.504, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.430, 10.503], loss: 0.001442, mae: 0.040776, mean_q: 1.167743
 334386/1000000: episode: 3344, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 60.145, mean reward: 0.601 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.058, 10.098], loss: 0.001503, mae: 0.041517, mean_q: 1.165795
 334486/1000000: episode: 3345, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 60.873, mean reward: 0.609 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.294, 10.098], loss: 0.001487, mae: 0.041188, mean_q: 1.170536
 334586/1000000: episode: 3346, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 59.798, mean reward: 0.598 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.242, 10.098], loss: 0.001488, mae: 0.041423, mean_q: 1.170934
 334686/1000000: episode: 3347, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 60.243, mean reward: 0.602 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.976, 10.098], loss: 0.001512, mae: 0.041969, mean_q: 1.166726
 334786/1000000: episode: 3348, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 58.727, mean reward: 0.587 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.656, 10.098], loss: 0.001424, mae: 0.040502, mean_q: 1.169848
 334886/1000000: episode: 3349, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 61.301, mean reward: 0.613 [0.509, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.914, 10.231], loss: 0.001437, mae: 0.040915, mean_q: 1.169132
 334986/1000000: episode: 3350, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 57.426, mean reward: 0.574 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.651, 10.129], loss: 0.001521, mae: 0.041907, mean_q: 1.174138
 335086/1000000: episode: 3351, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 60.691, mean reward: 0.607 [0.505, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.627, 10.408], loss: 0.001545, mae: 0.041873, mean_q: 1.171561
 335186/1000000: episode: 3352, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 56.863, mean reward: 0.569 [0.502, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.151, 10.191], loss: 0.001566, mae: 0.042079, mean_q: 1.167796
 335286/1000000: episode: 3353, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 58.705, mean reward: 0.587 [0.513, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.406, 10.339], loss: 0.001627, mae: 0.043170, mean_q: 1.168583
 335386/1000000: episode: 3354, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 57.686, mean reward: 0.577 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.507, 10.233], loss: 0.001457, mae: 0.040640, mean_q: 1.167944
 335486/1000000: episode: 3355, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.089, mean reward: 0.591 [0.514, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.098], loss: 0.001405, mae: 0.040284, mean_q: 1.169105
 335586/1000000: episode: 3356, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.119, mean reward: 0.581 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.466, 10.098], loss: 0.001717, mae: 0.043961, mean_q: 1.169250
 335686/1000000: episode: 3357, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 60.250, mean reward: 0.603 [0.512, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.147, 10.134], loss: 0.001464, mae: 0.041250, mean_q: 1.171691
 335786/1000000: episode: 3358, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 58.960, mean reward: 0.590 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.766, 10.121], loss: 0.001443, mae: 0.041121, mean_q: 1.170297
 335886/1000000: episode: 3359, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 61.734, mean reward: 0.617 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.903, 10.098], loss: 0.001540, mae: 0.042178, mean_q: 1.171648
 335986/1000000: episode: 3360, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.080, mean reward: 0.581 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.766, 10.284], loss: 0.001520, mae: 0.041932, mean_q: 1.171902
 336086/1000000: episode: 3361, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 59.039, mean reward: 0.590 [0.520, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.663, 10.098], loss: 0.001592, mae: 0.042303, mean_q: 1.171672
 336186/1000000: episode: 3362, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.570, mean reward: 0.576 [0.510, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.446, 10.098], loss: 0.001528, mae: 0.042285, mean_q: 1.173772
 336286/1000000: episode: 3363, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.409, mean reward: 0.574 [0.503, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.098], loss: 0.001541, mae: 0.041602, mean_q: 1.169172
 336386/1000000: episode: 3364, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 57.728, mean reward: 0.577 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.057, 10.098], loss: 0.001480, mae: 0.041092, mean_q: 1.167835
 336486/1000000: episode: 3365, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.139, mean reward: 0.571 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.381, 10.098], loss: 0.001428, mae: 0.040894, mean_q: 1.168853
 336586/1000000: episode: 3366, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 57.341, mean reward: 0.573 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.809, 10.098], loss: 0.001471, mae: 0.041672, mean_q: 1.166163
 336686/1000000: episode: 3367, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.329, mean reward: 0.583 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.757, 10.098], loss: 0.001465, mae: 0.040741, mean_q: 1.164247
 336786/1000000: episode: 3368, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 59.335, mean reward: 0.593 [0.508, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.244, 10.098], loss: 0.001500, mae: 0.041971, mean_q: 1.165689
 336886/1000000: episode: 3369, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 62.782, mean reward: 0.628 [0.516, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.531, 10.178], loss: 0.001416, mae: 0.040801, mean_q: 1.167241
 336986/1000000: episode: 3370, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.504, mean reward: 0.585 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.344, 10.098], loss: 0.001434, mae: 0.040907, mean_q: 1.171146
 337086/1000000: episode: 3371, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 58.732, mean reward: 0.587 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.340, 10.200], loss: 0.001428, mae: 0.041251, mean_q: 1.167380
 337186/1000000: episode: 3372, duration: 1.058s, episode steps: 100, steps per second: 94, episode reward: 59.255, mean reward: 0.593 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.098], loss: 0.001446, mae: 0.040261, mean_q: 1.169297
 337286/1000000: episode: 3373, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 59.933, mean reward: 0.599 [0.514, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.222, 10.098], loss: 0.001434, mae: 0.040856, mean_q: 1.167230
 337386/1000000: episode: 3374, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 58.852, mean reward: 0.589 [0.512, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.366, 10.237], loss: 0.001426, mae: 0.041860, mean_q: 1.169983
 337486/1000000: episode: 3375, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 63.356, mean reward: 0.634 [0.517, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.991, 10.098], loss: 0.001408, mae: 0.041524, mean_q: 1.174078
 337586/1000000: episode: 3376, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.747, mean reward: 0.587 [0.507, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.270], loss: 0.001400, mae: 0.040315, mean_q: 1.171288
 337686/1000000: episode: 3377, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.132, mean reward: 0.581 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.926, 10.172], loss: 0.001451, mae: 0.040688, mean_q: 1.172281
 337786/1000000: episode: 3378, duration: 0.922s, episode steps: 100, steps per second: 109, episode reward: 59.890, mean reward: 0.599 [0.514, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.620, 10.098], loss: 0.001444, mae: 0.040713, mean_q: 1.171181
 337886/1000000: episode: 3379, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.117, mean reward: 0.581 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.110], loss: 0.001297, mae: 0.039020, mean_q: 1.172166
 337986/1000000: episode: 3380, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 58.770, mean reward: 0.588 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.436, 10.348], loss: 0.001336, mae: 0.039904, mean_q: 1.173584
 338086/1000000: episode: 3381, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 57.499, mean reward: 0.575 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.930, 10.098], loss: 0.001340, mae: 0.039818, mean_q: 1.170455
 338186/1000000: episode: 3382, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 60.095, mean reward: 0.601 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.929, 10.098], loss: 0.001392, mae: 0.040690, mean_q: 1.170843
 338286/1000000: episode: 3383, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 57.878, mean reward: 0.579 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.199], loss: 0.001478, mae: 0.040904, mean_q: 1.169375
 338386/1000000: episode: 3384, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 57.180, mean reward: 0.572 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.381, 10.261], loss: 0.001496, mae: 0.041531, mean_q: 1.172313
 338486/1000000: episode: 3385, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 57.893, mean reward: 0.579 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.656, 10.098], loss: 0.001418, mae: 0.040956, mean_q: 1.164903
 338586/1000000: episode: 3386, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.929, mean reward: 0.579 [0.506, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.098], loss: 0.001426, mae: 0.040587, mean_q: 1.166812
 338686/1000000: episode: 3387, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 57.117, mean reward: 0.571 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.650, 10.098], loss: 0.001408, mae: 0.040517, mean_q: 1.171106
 338786/1000000: episode: 3388, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 57.065, mean reward: 0.571 [0.507, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.437, 10.098], loss: 0.001375, mae: 0.040164, mean_q: 1.169182
 338886/1000000: episode: 3389, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.670, mean reward: 0.597 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.570, 10.135], loss: 0.001350, mae: 0.039767, mean_q: 1.166955
 338986/1000000: episode: 3390, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.608, mean reward: 0.576 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.898, 10.098], loss: 0.001486, mae: 0.041607, mean_q: 1.167694
 339086/1000000: episode: 3391, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 59.821, mean reward: 0.598 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.808, 10.098], loss: 0.001377, mae: 0.040502, mean_q: 1.169314
 339186/1000000: episode: 3392, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.153, mean reward: 0.582 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.496, 10.098], loss: 0.001303, mae: 0.039049, mean_q: 1.166389
 339286/1000000: episode: 3393, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 58.250, mean reward: 0.583 [0.508, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.098], loss: 0.001373, mae: 0.040217, mean_q: 1.166368
 339386/1000000: episode: 3394, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 59.137, mean reward: 0.591 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.109], loss: 0.001492, mae: 0.041725, mean_q: 1.166420
 339486/1000000: episode: 3395, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.861, mean reward: 0.579 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.777, 10.219], loss: 0.001338, mae: 0.040384, mean_q: 1.164970
 339586/1000000: episode: 3396, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 60.075, mean reward: 0.601 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.888, 10.098], loss: 0.001357, mae: 0.040484, mean_q: 1.161969
 339686/1000000: episode: 3397, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 57.573, mean reward: 0.576 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.490, 10.200], loss: 0.001314, mae: 0.040226, mean_q: 1.161094
 339786/1000000: episode: 3398, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.793, mean reward: 0.578 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.184, 10.098], loss: 0.001375, mae: 0.040131, mean_q: 1.165339
 339886/1000000: episode: 3399, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 58.609, mean reward: 0.586 [0.512, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.993, 10.217], loss: 0.001377, mae: 0.040866, mean_q: 1.164697
 339986/1000000: episode: 3400, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 61.321, mean reward: 0.613 [0.502, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.236, 10.098], loss: 0.001281, mae: 0.039449, mean_q: 1.164082
 340086/1000000: episode: 3401, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 57.053, mean reward: 0.571 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.343, 10.349], loss: 0.001353, mae: 0.040107, mean_q: 1.162492
 340186/1000000: episode: 3402, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.472, mean reward: 0.595 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.665, 10.098], loss: 0.001277, mae: 0.039188, mean_q: 1.162836
 340286/1000000: episode: 3403, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.002, mean reward: 0.570 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.292, 10.098], loss: 0.001413, mae: 0.041447, mean_q: 1.164378
 340386/1000000: episode: 3404, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 59.252, mean reward: 0.593 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.883, 10.196], loss: 0.001313, mae: 0.039803, mean_q: 1.160275
 340486/1000000: episode: 3405, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.410, mean reward: 0.584 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.098], loss: 0.001282, mae: 0.039274, mean_q: 1.162646
 340586/1000000: episode: 3406, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 56.656, mean reward: 0.567 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.319, 10.098], loss: 0.001362, mae: 0.040239, mean_q: 1.163067
 340686/1000000: episode: 3407, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.827, mean reward: 0.588 [0.508, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.275, 10.244], loss: 0.001308, mae: 0.039330, mean_q: 1.159772
 340786/1000000: episode: 3408, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 61.643, mean reward: 0.616 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.939, 10.098], loss: 0.001323, mae: 0.039772, mean_q: 1.159754
 340886/1000000: episode: 3409, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.885, mean reward: 0.599 [0.512, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.150, 10.098], loss: 0.001417, mae: 0.041119, mean_q: 1.161909
 340986/1000000: episode: 3410, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 60.922, mean reward: 0.609 [0.510, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.098], loss: 0.001338, mae: 0.040445, mean_q: 1.161587
 341086/1000000: episode: 3411, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 62.350, mean reward: 0.624 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.001, 10.098], loss: 0.001336, mae: 0.039994, mean_q: 1.162166
 341186/1000000: episode: 3412, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 60.168, mean reward: 0.602 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.265, 10.098], loss: 0.001350, mae: 0.040527, mean_q: 1.164048
 341286/1000000: episode: 3413, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 59.343, mean reward: 0.593 [0.501, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.485, 10.169], loss: 0.001379, mae: 0.040425, mean_q: 1.163982
 341386/1000000: episode: 3414, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.885, mean reward: 0.599 [0.499, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.064, 10.300], loss: 0.001294, mae: 0.039644, mean_q: 1.162383
 341486/1000000: episode: 3415, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.738, mean reward: 0.607 [0.512, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.750, 10.234], loss: 0.001368, mae: 0.040083, mean_q: 1.164330
 341586/1000000: episode: 3416, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.577, mean reward: 0.596 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.043, 10.098], loss: 0.001405, mae: 0.041035, mean_q: 1.166114
 341686/1000000: episode: 3417, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 61.406, mean reward: 0.614 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.580, 10.098], loss: 0.001346, mae: 0.040514, mean_q: 1.166253
 341786/1000000: episode: 3418, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 59.367, mean reward: 0.594 [0.513, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.204, 10.212], loss: 0.001325, mae: 0.040284, mean_q: 1.165808
 341886/1000000: episode: 3419, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.713, mean reward: 0.577 [0.500, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.617, 10.300], loss: 0.001456, mae: 0.042020, mean_q: 1.167822
 341986/1000000: episode: 3420, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 59.110, mean reward: 0.591 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.358, 10.197], loss: 0.001348, mae: 0.039905, mean_q: 1.169739
 342086/1000000: episode: 3421, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 57.731, mean reward: 0.577 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.328, 10.098], loss: 0.001402, mae: 0.040491, mean_q: 1.170135
 342186/1000000: episode: 3422, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 60.302, mean reward: 0.603 [0.515, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.518, 10.098], loss: 0.001354, mae: 0.040694, mean_q: 1.168710
 342286/1000000: episode: 3423, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.213, mean reward: 0.592 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.693, 10.098], loss: 0.001398, mae: 0.040593, mean_q: 1.167791
 342386/1000000: episode: 3424, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 59.531, mean reward: 0.595 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.166, 10.098], loss: 0.001432, mae: 0.041313, mean_q: 1.168822
 342486/1000000: episode: 3425, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 59.668, mean reward: 0.597 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.564, 10.098], loss: 0.001449, mae: 0.041940, mean_q: 1.167917
 342586/1000000: episode: 3426, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 59.376, mean reward: 0.594 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.920, 10.098], loss: 0.001341, mae: 0.040298, mean_q: 1.166317
 342686/1000000: episode: 3427, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.591, mean reward: 0.586 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.633, 10.206], loss: 0.001368, mae: 0.039971, mean_q: 1.166296
 342786/1000000: episode: 3428, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 58.299, mean reward: 0.583 [0.498, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.396, 10.098], loss: 0.001425, mae: 0.041662, mean_q: 1.167646
 342886/1000000: episode: 3429, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 60.416, mean reward: 0.604 [0.512, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.892, 10.098], loss: 0.001423, mae: 0.040325, mean_q: 1.166215
 342986/1000000: episode: 3430, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 60.115, mean reward: 0.601 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.587, 10.262], loss: 0.001495, mae: 0.042269, mean_q: 1.168972
 343086/1000000: episode: 3431, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.358, mean reward: 0.594 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.327, 10.303], loss: 0.001437, mae: 0.041627, mean_q: 1.170025
 343186/1000000: episode: 3432, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.560, mean reward: 0.586 [0.513, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.378, 10.131], loss: 0.001372, mae: 0.040656, mean_q: 1.165501
 343286/1000000: episode: 3433, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 58.462, mean reward: 0.585 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.682, 10.263], loss: 0.001386, mae: 0.040874, mean_q: 1.167395
 343386/1000000: episode: 3434, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 57.743, mean reward: 0.577 [0.503, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.098], loss: 0.001467, mae: 0.041671, mean_q: 1.168515
 343486/1000000: episode: 3435, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 57.271, mean reward: 0.573 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.110], loss: 0.001492, mae: 0.041965, mean_q: 1.167717
 343586/1000000: episode: 3436, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.699, mean reward: 0.577 [0.517, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.001375, mae: 0.040359, mean_q: 1.166346
 343686/1000000: episode: 3437, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.344, mean reward: 0.593 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.546, 10.217], loss: 0.001496, mae: 0.042014, mean_q: 1.169377
 343786/1000000: episode: 3438, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 59.196, mean reward: 0.592 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.689, 10.098], loss: 0.001448, mae: 0.041559, mean_q: 1.167907
 343886/1000000: episode: 3439, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 60.475, mean reward: 0.605 [0.513, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.716, 10.195], loss: 0.001406, mae: 0.040924, mean_q: 1.169050
 343986/1000000: episode: 3440, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 60.531, mean reward: 0.605 [0.515, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.260], loss: 0.001418, mae: 0.041171, mean_q: 1.168198
 344086/1000000: episode: 3441, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.731, mean reward: 0.607 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.675, 10.367], loss: 0.001397, mae: 0.040713, mean_q: 1.173042
 344186/1000000: episode: 3442, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.274, mean reward: 0.593 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.485, 10.340], loss: 0.001443, mae: 0.041377, mean_q: 1.167900
 344286/1000000: episode: 3443, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 60.917, mean reward: 0.609 [0.516, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.559, 10.098], loss: 0.001409, mae: 0.040784, mean_q: 1.171211
 344386/1000000: episode: 3444, duration: 0.797s, episode steps: 100, steps per second: 125, episode reward: 59.305, mean reward: 0.593 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.209, 10.098], loss: 0.001566, mae: 0.042748, mean_q: 1.174666
 344486/1000000: episode: 3445, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 56.249, mean reward: 0.562 [0.498, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.225, 10.099], loss: 0.001538, mae: 0.042427, mean_q: 1.172900
 344586/1000000: episode: 3446, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 56.489, mean reward: 0.565 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.937, 10.307], loss: 0.001504, mae: 0.041670, mean_q: 1.172871
 344686/1000000: episode: 3447, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 56.689, mean reward: 0.567 [0.504, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.043, 10.145], loss: 0.001474, mae: 0.041501, mean_q: 1.174213
 344786/1000000: episode: 3448, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 58.579, mean reward: 0.586 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.217, 10.098], loss: 0.001462, mae: 0.041399, mean_q: 1.170224
 344886/1000000: episode: 3449, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 58.765, mean reward: 0.588 [0.498, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.912, 10.098], loss: 0.001536, mae: 0.043153, mean_q: 1.169947
 344986/1000000: episode: 3450, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.930, mean reward: 0.589 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.020, 10.098], loss: 0.001528, mae: 0.042389, mean_q: 1.170463
 345086/1000000: episode: 3451, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 58.698, mean reward: 0.587 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.474, 10.357], loss: 0.001497, mae: 0.041628, mean_q: 1.169618
 345186/1000000: episode: 3452, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 57.324, mean reward: 0.573 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.098], loss: 0.001425, mae: 0.040960, mean_q: 1.168982
 345286/1000000: episode: 3453, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 57.552, mean reward: 0.576 [0.508, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.505, 10.392], loss: 0.001558, mae: 0.042409, mean_q: 1.170503
 345386/1000000: episode: 3454, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 59.879, mean reward: 0.599 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.268, 10.108], loss: 0.001350, mae: 0.039833, mean_q: 1.168415
 345486/1000000: episode: 3455, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.590, mean reward: 0.596 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.758, 10.098], loss: 0.001460, mae: 0.041333, mean_q: 1.171380
 345586/1000000: episode: 3456, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.572, mean reward: 0.576 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.264, 10.098], loss: 0.001598, mae: 0.043225, mean_q: 1.170102
 345686/1000000: episode: 3457, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.474, mean reward: 0.575 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.098], loss: 0.001376, mae: 0.040494, mean_q: 1.170510
 345786/1000000: episode: 3458, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 58.966, mean reward: 0.590 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.748, 10.235], loss: 0.001374, mae: 0.040627, mean_q: 1.168666
 345886/1000000: episode: 3459, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.462, mean reward: 0.585 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.098], loss: 0.001429, mae: 0.041151, mean_q: 1.169700
 345986/1000000: episode: 3460, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 58.705, mean reward: 0.587 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.005, 10.143], loss: 0.001446, mae: 0.040913, mean_q: 1.169930
 346086/1000000: episode: 3461, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 58.309, mean reward: 0.583 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.163, 10.098], loss: 0.001379, mae: 0.040497, mean_q: 1.167392
 346186/1000000: episode: 3462, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.030, mean reward: 0.570 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.453, 10.098], loss: 0.001409, mae: 0.040467, mean_q: 1.165972
 346286/1000000: episode: 3463, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.099, mean reward: 0.581 [0.507, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.819, 10.098], loss: 0.001323, mae: 0.039227, mean_q: 1.161737
 346386/1000000: episode: 3464, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 59.929, mean reward: 0.599 [0.513, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.862, 10.098], loss: 0.001379, mae: 0.040085, mean_q: 1.162658
 346486/1000000: episode: 3465, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 58.897, mean reward: 0.589 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.335, 10.281], loss: 0.001493, mae: 0.041904, mean_q: 1.165153
 346586/1000000: episode: 3466, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.578, mean reward: 0.576 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.885, 10.251], loss: 0.001476, mae: 0.041782, mean_q: 1.164829
 346686/1000000: episode: 3467, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.585, mean reward: 0.576 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.758, 10.098], loss: 0.001439, mae: 0.040614, mean_q: 1.163885
 346786/1000000: episode: 3468, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.747, mean reward: 0.597 [0.500, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.661, 10.098], loss: 0.001476, mae: 0.042014, mean_q: 1.164798
 346886/1000000: episode: 3469, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 58.387, mean reward: 0.584 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.414], loss: 0.001420, mae: 0.041320, mean_q: 1.164051
 346986/1000000: episode: 3470, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.047, mean reward: 0.580 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.098], loss: 0.001394, mae: 0.040909, mean_q: 1.157650
 347086/1000000: episode: 3471, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.484, mean reward: 0.575 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.631, 10.256], loss: 0.001433, mae: 0.041083, mean_q: 1.159601
 347186/1000000: episode: 3472, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.779, mean reward: 0.588 [0.509, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.710, 10.098], loss: 0.001511, mae: 0.042796, mean_q: 1.161466
 347286/1000000: episode: 3473, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 60.348, mean reward: 0.603 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.469, 10.237], loss: 0.001384, mae: 0.040848, mean_q: 1.159220
 347386/1000000: episode: 3474, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 56.950, mean reward: 0.570 [0.507, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.892, 10.310], loss: 0.001472, mae: 0.041604, mean_q: 1.161736
 347486/1000000: episode: 3475, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 57.784, mean reward: 0.578 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.828, 10.125], loss: 0.001394, mae: 0.041333, mean_q: 1.160484
 347586/1000000: episode: 3476, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.745, mean reward: 0.587 [0.507, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.190], loss: 0.001482, mae: 0.041838, mean_q: 1.161458
 347686/1000000: episode: 3477, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.148, mean reward: 0.591 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.399, 10.219], loss: 0.001475, mae: 0.041534, mean_q: 1.161901
 347786/1000000: episode: 3478, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 60.437, mean reward: 0.604 [0.512, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.442, 10.371], loss: 0.001507, mae: 0.042800, mean_q: 1.157994
 347886/1000000: episode: 3479, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 60.633, mean reward: 0.606 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.202, 10.235], loss: 0.001463, mae: 0.041804, mean_q: 1.162438
 347986/1000000: episode: 3480, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.106, mean reward: 0.591 [0.519, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.135, 10.237], loss: 0.001546, mae: 0.042766, mean_q: 1.162568
 348086/1000000: episode: 3481, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 59.896, mean reward: 0.599 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.280, 10.098], loss: 0.001446, mae: 0.041793, mean_q: 1.159083
 348186/1000000: episode: 3482, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.952, mean reward: 0.590 [0.498, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.981, 10.229], loss: 0.001538, mae: 0.042579, mean_q: 1.161671
 348286/1000000: episode: 3483, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.108, mean reward: 0.581 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.169, 10.155], loss: 0.001378, mae: 0.040415, mean_q: 1.163291
 348386/1000000: episode: 3484, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 56.806, mean reward: 0.568 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.491, 10.098], loss: 0.001429, mae: 0.041106, mean_q: 1.161027
 348486/1000000: episode: 3485, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 61.805, mean reward: 0.618 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.475, 10.098], loss: 0.001351, mae: 0.040589, mean_q: 1.161502
 348586/1000000: episode: 3486, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.481, mean reward: 0.585 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.680, 10.399], loss: 0.001462, mae: 0.041238, mean_q: 1.165091
 348686/1000000: episode: 3487, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 58.360, mean reward: 0.584 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.056, 10.128], loss: 0.001470, mae: 0.041701, mean_q: 1.160118
 348786/1000000: episode: 3488, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.185, mean reward: 0.582 [0.513, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.439, 10.120], loss: 0.001501, mae: 0.041948, mean_q: 1.164485
 348886/1000000: episode: 3489, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.012, mean reward: 0.590 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.757, 10.234], loss: 0.001460, mae: 0.041565, mean_q: 1.162301
 348986/1000000: episode: 3490, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.244, mean reward: 0.582 [0.505, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.161, 10.173], loss: 0.001495, mae: 0.042137, mean_q: 1.159843
 349086/1000000: episode: 3491, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 57.940, mean reward: 0.579 [0.508, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.861, 10.098], loss: 0.001496, mae: 0.041611, mean_q: 1.158759
 349186/1000000: episode: 3492, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.380, mean reward: 0.594 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.098], loss: 0.001424, mae: 0.040747, mean_q: 1.155377
 349286/1000000: episode: 3493, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 56.804, mean reward: 0.568 [0.505, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.111, 10.155], loss: 0.001420, mae: 0.041058, mean_q: 1.158594
 349386/1000000: episode: 3494, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 58.257, mean reward: 0.583 [0.500, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.073, 10.180], loss: 0.001469, mae: 0.041468, mean_q: 1.157236
 349486/1000000: episode: 3495, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 59.325, mean reward: 0.593 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.291, 10.220], loss: 0.001415, mae: 0.040701, mean_q: 1.157398
 349586/1000000: episode: 3496, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 60.399, mean reward: 0.604 [0.516, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.291, 10.098], loss: 0.001426, mae: 0.041403, mean_q: 1.158056
 349686/1000000: episode: 3497, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.332, mean reward: 0.583 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.450, 10.158], loss: 0.001445, mae: 0.041311, mean_q: 1.159877
 349786/1000000: episode: 3498, duration: 1.170s, episode steps: 100, steps per second: 86, episode reward: 58.929, mean reward: 0.589 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.516, 10.259], loss: 0.001529, mae: 0.042480, mean_q: 1.158302
 349886/1000000: episode: 3499, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.211, mean reward: 0.592 [0.497, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.851, 10.098], loss: 0.001455, mae: 0.041773, mean_q: 1.160853
 349986/1000000: episode: 3500, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.927, mean reward: 0.599 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.098], loss: 0.001505, mae: 0.042161, mean_q: 1.160607
 350086/1000000: episode: 3501, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.262, mean reward: 0.583 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.528, 10.098], loss: 0.001496, mae: 0.042086, mean_q: 1.163275
 350186/1000000: episode: 3502, duration: 1.349s, episode steps: 100, steps per second: 74, episode reward: 58.106, mean reward: 0.581 [0.517, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.851, 10.294], loss: 0.001404, mae: 0.040995, mean_q: 1.159232
 350286/1000000: episode: 3503, duration: 1.728s, episode steps: 100, steps per second: 58, episode reward: 58.459, mean reward: 0.585 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.640, 10.098], loss: 0.001492, mae: 0.041873, mean_q: 1.161501
 350386/1000000: episode: 3504, duration: 1.602s, episode steps: 100, steps per second: 62, episode reward: 57.496, mean reward: 0.575 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.956, 10.194], loss: 0.001468, mae: 0.041170, mean_q: 1.158633
 350486/1000000: episode: 3505, duration: 1.402s, episode steps: 100, steps per second: 71, episode reward: 59.368, mean reward: 0.594 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.925, 10.106], loss: 0.001391, mae: 0.040323, mean_q: 1.160870
 350586/1000000: episode: 3506, duration: 1.704s, episode steps: 100, steps per second: 59, episode reward: 60.455, mean reward: 0.605 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.488, 10.098], loss: 0.001544, mae: 0.042728, mean_q: 1.162269
 350686/1000000: episode: 3507, duration: 1.694s, episode steps: 100, steps per second: 59, episode reward: 57.210, mean reward: 0.572 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.368, 10.308], loss: 0.001467, mae: 0.041769, mean_q: 1.160793
 350786/1000000: episode: 3508, duration: 1.542s, episode steps: 100, steps per second: 65, episode reward: 63.373, mean reward: 0.634 [0.524, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.212, 10.416], loss: 0.001419, mae: 0.040845, mean_q: 1.158846
 350886/1000000: episode: 3509, duration: 1.717s, episode steps: 100, steps per second: 58, episode reward: 57.207, mean reward: 0.572 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.443, 10.478], loss: 0.001471, mae: 0.041411, mean_q: 1.157772
 350986/1000000: episode: 3510, duration: 2.040s, episode steps: 100, steps per second: 49, episode reward: 59.761, mean reward: 0.598 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.055, 10.098], loss: 0.001461, mae: 0.041035, mean_q: 1.163536
 351086/1000000: episode: 3511, duration: 1.606s, episode steps: 100, steps per second: 62, episode reward: 56.963, mean reward: 0.570 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.671, 10.098], loss: 0.001464, mae: 0.041475, mean_q: 1.161278
 351186/1000000: episode: 3512, duration: 1.782s, episode steps: 100, steps per second: 56, episode reward: 59.563, mean reward: 0.596 [0.515, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.098], loss: 0.001597, mae: 0.043576, mean_q: 1.165346
 351286/1000000: episode: 3513, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 59.722, mean reward: 0.597 [0.514, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.909, 10.357], loss: 0.001465, mae: 0.041531, mean_q: 1.164091
 351386/1000000: episode: 3514, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 56.905, mean reward: 0.569 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.298, 10.271], loss: 0.001438, mae: 0.041358, mean_q: 1.165687
 351486/1000000: episode: 3515, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 63.089, mean reward: 0.631 [0.538, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.601, 10.190], loss: 0.001372, mae: 0.040579, mean_q: 1.165272
 351586/1000000: episode: 3516, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 60.208, mean reward: 0.602 [0.511, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.871, 10.098], loss: 0.001501, mae: 0.042218, mean_q: 1.164923
 351686/1000000: episode: 3517, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 64.264, mean reward: 0.643 [0.528, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.297, 10.098], loss: 0.001471, mae: 0.041230, mean_q: 1.166300
 351786/1000000: episode: 3518, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 58.038, mean reward: 0.580 [0.514, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.168], loss: 0.001493, mae: 0.042288, mean_q: 1.167372
 351886/1000000: episode: 3519, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 57.612, mean reward: 0.576 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.565, 10.098], loss: 0.001472, mae: 0.041455, mean_q: 1.171250
 351986/1000000: episode: 3520, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 58.568, mean reward: 0.586 [0.511, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.493, 10.288], loss: 0.001474, mae: 0.041291, mean_q: 1.169944
 352086/1000000: episode: 3521, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 59.018, mean reward: 0.590 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.870, 10.477], loss: 0.001470, mae: 0.041784, mean_q: 1.169349
 352186/1000000: episode: 3522, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.005, mean reward: 0.580 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.273, 10.098], loss: 0.001445, mae: 0.040910, mean_q: 1.171313
 352286/1000000: episode: 3523, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.399, mean reward: 0.584 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.675, 10.098], loss: 0.001473, mae: 0.041274, mean_q: 1.168623
 352386/1000000: episode: 3524, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 60.644, mean reward: 0.606 [0.503, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.226, 10.098], loss: 0.001410, mae: 0.040453, mean_q: 1.167787
 352486/1000000: episode: 3525, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.844, mean reward: 0.578 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.689, 10.141], loss: 0.001480, mae: 0.041118, mean_q: 1.167984
 352586/1000000: episode: 3526, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.857, mean reward: 0.589 [0.511, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.912, 10.098], loss: 0.001385, mae: 0.040442, mean_q: 1.170351
 352686/1000000: episode: 3527, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.835, mean reward: 0.588 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.802, 10.098], loss: 0.001421, mae: 0.040708, mean_q: 1.169061
 352786/1000000: episode: 3528, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 61.608, mean reward: 0.616 [0.532, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.060, 10.326], loss: 0.001443, mae: 0.040702, mean_q: 1.171444
 352886/1000000: episode: 3529, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 59.879, mean reward: 0.599 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.011, 10.177], loss: 0.001385, mae: 0.040219, mean_q: 1.169005
 352986/1000000: episode: 3530, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 62.681, mean reward: 0.627 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.563, 10.272], loss: 0.001396, mae: 0.040054, mean_q: 1.167554
 353086/1000000: episode: 3531, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.328, mean reward: 0.593 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.152], loss: 0.001452, mae: 0.040615, mean_q: 1.169443
 353186/1000000: episode: 3532, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.491, mean reward: 0.575 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.866, 10.212], loss: 0.001363, mae: 0.039957, mean_q: 1.164470
 353286/1000000: episode: 3533, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.904, mean reward: 0.589 [0.506, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.577, 10.098], loss: 0.001373, mae: 0.039677, mean_q: 1.167985
 353386/1000000: episode: 3534, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 59.573, mean reward: 0.596 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.325, 10.098], loss: 0.001439, mae: 0.040986, mean_q: 1.171143
 353486/1000000: episode: 3535, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 60.365, mean reward: 0.604 [0.516, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.916, 10.098], loss: 0.001529, mae: 0.042214, mean_q: 1.171082
 353586/1000000: episode: 3536, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 57.424, mean reward: 0.574 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.098, 10.313], loss: 0.001357, mae: 0.039748, mean_q: 1.167633
 353686/1000000: episode: 3537, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.371, mean reward: 0.594 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.098], loss: 0.001476, mae: 0.041366, mean_q: 1.171976
 353786/1000000: episode: 3538, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 62.870, mean reward: 0.629 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.215], loss: 0.001542, mae: 0.041978, mean_q: 1.170588
 353886/1000000: episode: 3539, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 58.590, mean reward: 0.586 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.519, 10.388], loss: 0.001395, mae: 0.040487, mean_q: 1.171085
 353986/1000000: episode: 3540, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 58.090, mean reward: 0.581 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.560, 10.166], loss: 0.001623, mae: 0.043150, mean_q: 1.172100
 354086/1000000: episode: 3541, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 56.462, mean reward: 0.565 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.600, 10.098], loss: 0.001399, mae: 0.040511, mean_q: 1.173217
 354186/1000000: episode: 3542, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 59.185, mean reward: 0.592 [0.503, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.619, 10.135], loss: 0.001435, mae: 0.040865, mean_q: 1.170251
 354286/1000000: episode: 3543, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 59.830, mean reward: 0.598 [0.503, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.772, 10.185], loss: 0.001384, mae: 0.039676, mean_q: 1.171686
 354386/1000000: episode: 3544, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 62.268, mean reward: 0.623 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.667, 10.124], loss: 0.001425, mae: 0.040831, mean_q: 1.173133
 354486/1000000: episode: 3545, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.830, mean reward: 0.588 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.330, 10.098], loss: 0.001521, mae: 0.042408, mean_q: 1.174552
 354586/1000000: episode: 3546, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 60.010, mean reward: 0.600 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.800, 10.098], loss: 0.001512, mae: 0.042206, mean_q: 1.175520
 354686/1000000: episode: 3547, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 60.317, mean reward: 0.603 [0.516, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.417, 10.238], loss: 0.001395, mae: 0.040423, mean_q: 1.171241
 354786/1000000: episode: 3548, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 58.284, mean reward: 0.583 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.128, 10.098], loss: 0.001411, mae: 0.041135, mean_q: 1.175725
 354886/1000000: episode: 3549, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.231, mean reward: 0.582 [0.511, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.998, 10.098], loss: 0.001389, mae: 0.040405, mean_q: 1.173287
 354986/1000000: episode: 3550, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.508, mean reward: 0.575 [0.506, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.098], loss: 0.001416, mae: 0.041183, mean_q: 1.177202
 355086/1000000: episode: 3551, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 60.547, mean reward: 0.605 [0.536, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.119, 10.098], loss: 0.001351, mae: 0.039850, mean_q: 1.171241
 355186/1000000: episode: 3552, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 58.406, mean reward: 0.584 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.049, 10.216], loss: 0.001418, mae: 0.041533, mean_q: 1.172532
 355286/1000000: episode: 3553, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.893, mean reward: 0.579 [0.507, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.090, 10.186], loss: 0.001477, mae: 0.042070, mean_q: 1.174032
 355386/1000000: episode: 3554, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.373, mean reward: 0.594 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.874, 10.216], loss: 0.001488, mae: 0.041627, mean_q: 1.173375
 355486/1000000: episode: 3555, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.885, mean reward: 0.579 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.599, 10.233], loss: 0.001441, mae: 0.041401, mean_q: 1.175995
 355586/1000000: episode: 3556, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 61.618, mean reward: 0.616 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.646, 10.306], loss: 0.001483, mae: 0.042234, mean_q: 1.173790
 355686/1000000: episode: 3557, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 59.238, mean reward: 0.592 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.431, 10.098], loss: 0.001565, mae: 0.042804, mean_q: 1.177627
 355786/1000000: episode: 3558, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.698, mean reward: 0.587 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.129, 10.252], loss: 0.001550, mae: 0.043420, mean_q: 1.171813
 355886/1000000: episode: 3559, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 58.631, mean reward: 0.586 [0.508, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.220], loss: 0.001454, mae: 0.042015, mean_q: 1.174438
 355986/1000000: episode: 3560, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.216, mean reward: 0.582 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.098], loss: 0.001482, mae: 0.042406, mean_q: 1.174956
 356086/1000000: episode: 3561, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.203, mean reward: 0.592 [0.506, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.111, 10.098], loss: 0.001514, mae: 0.042395, mean_q: 1.174151
 356186/1000000: episode: 3562, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.123, mean reward: 0.591 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.098], loss: 0.001706, mae: 0.044331, mean_q: 1.171800
 356286/1000000: episode: 3563, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.924, mean reward: 0.589 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.793, 10.098], loss: 0.001465, mae: 0.042113, mean_q: 1.171279
 356386/1000000: episode: 3564, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.458, mean reward: 0.585 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.012, 10.098], loss: 0.001609, mae: 0.043604, mean_q: 1.173043
 356486/1000000: episode: 3565, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 58.713, mean reward: 0.587 [0.512, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.609, 10.098], loss: 0.001566, mae: 0.043284, mean_q: 1.175243
 356586/1000000: episode: 3566, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.865, mean reward: 0.579 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.142, 10.189], loss: 0.001429, mae: 0.041193, mean_q: 1.169408
 356686/1000000: episode: 3567, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 59.937, mean reward: 0.599 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.368, 10.235], loss: 0.001472, mae: 0.042277, mean_q: 1.168762
 356786/1000000: episode: 3568, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 60.754, mean reward: 0.608 [0.514, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.833, 10.308], loss: 0.001546, mae: 0.042838, mean_q: 1.171605
 356886/1000000: episode: 3569, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.086, mean reward: 0.571 [0.510, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.569, 10.098], loss: 0.001630, mae: 0.044168, mean_q: 1.173541
 356986/1000000: episode: 3570, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.275, mean reward: 0.593 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.708, 10.098], loss: 0.001540, mae: 0.042382, mean_q: 1.173593
 357086/1000000: episode: 3571, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.882, mean reward: 0.589 [0.498, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.420, 10.110], loss: 0.001566, mae: 0.043105, mean_q: 1.168989
 357186/1000000: episode: 3572, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.398, mean reward: 0.584 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.001503, mae: 0.042560, mean_q: 1.171255
 357286/1000000: episode: 3573, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 61.139, mean reward: 0.611 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.659, 10.098], loss: 0.001461, mae: 0.042068, mean_q: 1.168585
 357386/1000000: episode: 3574, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 59.588, mean reward: 0.596 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.734, 10.244], loss: 0.001458, mae: 0.041999, mean_q: 1.170804
 357486/1000000: episode: 3575, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.270, mean reward: 0.583 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.775, 10.098], loss: 0.001619, mae: 0.043874, mean_q: 1.173811
 357586/1000000: episode: 3576, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 59.081, mean reward: 0.591 [0.503, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.510, 10.327], loss: 0.001498, mae: 0.042747, mean_q: 1.168147
 357686/1000000: episode: 3577, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 57.788, mean reward: 0.578 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.508, 10.098], loss: 0.001512, mae: 0.042884, mean_q: 1.169929
 357786/1000000: episode: 3578, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.120, mean reward: 0.581 [0.511, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.949, 10.098], loss: 0.001530, mae: 0.043226, mean_q: 1.174151
 357886/1000000: episode: 3579, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.557, mean reward: 0.576 [0.510, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.323, 10.098], loss: 0.001548, mae: 0.043424, mean_q: 1.170284
 357986/1000000: episode: 3580, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 61.303, mean reward: 0.613 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.431, 10.098], loss: 0.001510, mae: 0.042633, mean_q: 1.167087
 358086/1000000: episode: 3581, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 59.144, mean reward: 0.591 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.179, 10.274], loss: 0.001560, mae: 0.043353, mean_q: 1.169686
 358186/1000000: episode: 3582, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 57.977, mean reward: 0.580 [0.500, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.718, 10.260], loss: 0.001452, mae: 0.042377, mean_q: 1.171818
 358286/1000000: episode: 3583, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.599, mean reward: 0.576 [0.498, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.083, 10.098], loss: 0.001480, mae: 0.042426, mean_q: 1.172658
 358386/1000000: episode: 3584, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 57.307, mean reward: 0.573 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.514, 10.098], loss: 0.001541, mae: 0.042782, mean_q: 1.166327
 358486/1000000: episode: 3585, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 60.004, mean reward: 0.600 [0.517, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.369, 10.159], loss: 0.001482, mae: 0.042740, mean_q: 1.167681
 358586/1000000: episode: 3586, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 59.251, mean reward: 0.593 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.150, 10.257], loss: 0.001560, mae: 0.043622, mean_q: 1.169337
 358686/1000000: episode: 3587, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.325, mean reward: 0.603 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.853, 10.366], loss: 0.001548, mae: 0.043153, mean_q: 1.167580
 358786/1000000: episode: 3588, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 57.814, mean reward: 0.578 [0.500, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.121], loss: 0.001519, mae: 0.043192, mean_q: 1.167040
 358886/1000000: episode: 3589, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 61.360, mean reward: 0.614 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.936, 10.098], loss: 0.001527, mae: 0.043324, mean_q: 1.166736
 358986/1000000: episode: 3590, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 59.566, mean reward: 0.596 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.406, 10.098], loss: 0.001419, mae: 0.041723, mean_q: 1.165662
 359086/1000000: episode: 3591, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 60.695, mean reward: 0.607 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.457, 10.458], loss: 0.001422, mae: 0.041722, mean_q: 1.169947
 359186/1000000: episode: 3592, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.636, mean reward: 0.576 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.316, 10.098], loss: 0.001537, mae: 0.042858, mean_q: 1.168548
 359286/1000000: episode: 3593, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.090, mean reward: 0.571 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.098], loss: 0.001521, mae: 0.043101, mean_q: 1.168790
 359386/1000000: episode: 3594, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.657, mean reward: 0.587 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.345, 10.098], loss: 0.001443, mae: 0.042033, mean_q: 1.163254
 359486/1000000: episode: 3595, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 57.821, mean reward: 0.578 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.098], loss: 0.001451, mae: 0.041877, mean_q: 1.165763
 359586/1000000: episode: 3596, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 59.604, mean reward: 0.596 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.840, 10.098], loss: 0.001532, mae: 0.043031, mean_q: 1.169224
 359686/1000000: episode: 3597, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.333, mean reward: 0.583 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.829, 10.098], loss: 0.001522, mae: 0.042397, mean_q: 1.164618
 359786/1000000: episode: 3598, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 58.325, mean reward: 0.583 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.520, 10.098], loss: 0.001553, mae: 0.042936, mean_q: 1.165037
 359886/1000000: episode: 3599, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 58.387, mean reward: 0.584 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.686, 10.196], loss: 0.001511, mae: 0.042806, mean_q: 1.162334
 359986/1000000: episode: 3600, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 60.101, mean reward: 0.601 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.215, 10.098], loss: 0.001389, mae: 0.040960, mean_q: 1.164798
 360086/1000000: episode: 3601, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.831, mean reward: 0.598 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.627, 10.098], loss: 0.001422, mae: 0.041509, mean_q: 1.167309
 360186/1000000: episode: 3602, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 58.626, mean reward: 0.586 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.887, 10.179], loss: 0.001444, mae: 0.041575, mean_q: 1.165570
 360286/1000000: episode: 3603, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.183, mean reward: 0.572 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.180], loss: 0.001626, mae: 0.043632, mean_q: 1.167373
 360386/1000000: episode: 3604, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.556, mean reward: 0.576 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.595, 10.098], loss: 0.001475, mae: 0.042662, mean_q: 1.165728
 360486/1000000: episode: 3605, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 59.585, mean reward: 0.596 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.908, 10.357], loss: 0.001430, mae: 0.041221, mean_q: 1.164835
 360586/1000000: episode: 3606, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 62.438, mean reward: 0.624 [0.505, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.717, 10.098], loss: 0.001380, mae: 0.040688, mean_q: 1.164165
 360686/1000000: episode: 3607, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.987, mean reward: 0.580 [0.509, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.376, 10.197], loss: 0.001531, mae: 0.042332, mean_q: 1.165879
 360786/1000000: episode: 3608, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 59.190, mean reward: 0.592 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.703, 10.249], loss: 0.001537, mae: 0.042916, mean_q: 1.166727
 360886/1000000: episode: 3609, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 59.190, mean reward: 0.592 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.152, 10.297], loss: 0.001531, mae: 0.043023, mean_q: 1.167297
 360986/1000000: episode: 3610, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.683, mean reward: 0.587 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.452, 10.101], loss: 0.001451, mae: 0.041431, mean_q: 1.168352
 361086/1000000: episode: 3611, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 57.205, mean reward: 0.572 [0.508, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.671, 10.098], loss: 0.001458, mae: 0.041175, mean_q: 1.163516
 361186/1000000: episode: 3612, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 61.327, mean reward: 0.613 [0.538, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.892, 10.098], loss: 0.001476, mae: 0.041853, mean_q: 1.164707
 361286/1000000: episode: 3613, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 66.380, mean reward: 0.664 [0.503, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.289, 10.487], loss: 0.001420, mae: 0.041697, mean_q: 1.166950
 361386/1000000: episode: 3614, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 59.682, mean reward: 0.597 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.942, 10.098], loss: 0.001468, mae: 0.041705, mean_q: 1.169095
 361486/1000000: episode: 3615, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.198, mean reward: 0.582 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.652, 10.157], loss: 0.001416, mae: 0.041204, mean_q: 1.167753
 361586/1000000: episode: 3616, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 57.464, mean reward: 0.575 [0.503, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.165], loss: 0.001424, mae: 0.041666, mean_q: 1.169049
 361686/1000000: episode: 3617, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 61.099, mean reward: 0.611 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.780, 10.463], loss: 0.001494, mae: 0.042363, mean_q: 1.168806
 361786/1000000: episode: 3618, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 58.389, mean reward: 0.584 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.380, 10.124], loss: 0.001362, mae: 0.040203, mean_q: 1.166262
 361886/1000000: episode: 3619, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 64.185, mean reward: 0.642 [0.510, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.247], loss: 0.001391, mae: 0.040790, mean_q: 1.170368
 361986/1000000: episode: 3620, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.444, mean reward: 0.594 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.127], loss: 0.001468, mae: 0.041322, mean_q: 1.167572
 362086/1000000: episode: 3621, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 59.195, mean reward: 0.592 [0.505, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.049, 10.132], loss: 0.001403, mae: 0.041221, mean_q: 1.173072
 362186/1000000: episode: 3622, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 57.460, mean reward: 0.575 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.157], loss: 0.001444, mae: 0.041505, mean_q: 1.175499
 362286/1000000: episode: 3623, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.913, mean reward: 0.579 [0.506, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.185], loss: 0.001545, mae: 0.042426, mean_q: 1.171515
 362386/1000000: episode: 3624, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.378, mean reward: 0.584 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.752, 10.098], loss: 0.001519, mae: 0.042680, mean_q: 1.169175
 362486/1000000: episode: 3625, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 63.598, mean reward: 0.636 [0.509, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.258, 10.098], loss: 0.001437, mae: 0.041678, mean_q: 1.170398
 362586/1000000: episode: 3626, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.751, mean reward: 0.578 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.985, 10.098], loss: 0.001427, mae: 0.040855, mean_q: 1.170553
 362686/1000000: episode: 3627, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 57.411, mean reward: 0.574 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.098], loss: 0.001447, mae: 0.041615, mean_q: 1.171225
 362786/1000000: episode: 3628, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 62.142, mean reward: 0.621 [0.508, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.874, 10.189], loss: 0.001426, mae: 0.040951, mean_q: 1.172375
 362886/1000000: episode: 3629, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 58.390, mean reward: 0.584 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.673, 10.098], loss: 0.001477, mae: 0.041972, mean_q: 1.172612
 362986/1000000: episode: 3630, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 62.258, mean reward: 0.623 [0.515, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.457, 10.349], loss: 0.001607, mae: 0.042902, mean_q: 1.173701
 363086/1000000: episode: 3631, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.244, mean reward: 0.582 [0.500, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.097, 10.378], loss: 0.001358, mae: 0.040335, mean_q: 1.172889
 363186/1000000: episode: 3632, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.244, mean reward: 0.572 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.377, 10.116], loss: 0.001463, mae: 0.041862, mean_q: 1.170525
 363286/1000000: episode: 3633, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 58.247, mean reward: 0.582 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.555, 10.188], loss: 0.001383, mae: 0.040291, mean_q: 1.171759
 363386/1000000: episode: 3634, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.507, mean reward: 0.585 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.590, 10.098], loss: 0.001440, mae: 0.041167, mean_q: 1.174999
 363486/1000000: episode: 3635, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.086, mean reward: 0.581 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.119], loss: 0.001521, mae: 0.042580, mean_q: 1.175007
 363586/1000000: episode: 3636, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 59.856, mean reward: 0.599 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.659, 10.248], loss: 0.001516, mae: 0.042484, mean_q: 1.171809
 363686/1000000: episode: 3637, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.403, mean reward: 0.584 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.350], loss: 0.001418, mae: 0.040407, mean_q: 1.169166
 363786/1000000: episode: 3638, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.418, mean reward: 0.584 [0.511, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.004, 10.262], loss: 0.001448, mae: 0.040889, mean_q: 1.172130
 363886/1000000: episode: 3639, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 60.697, mean reward: 0.607 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.415, 10.212], loss: 0.001547, mae: 0.041963, mean_q: 1.173857
 363986/1000000: episode: 3640, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 60.448, mean reward: 0.604 [0.517, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.039, 10.098], loss: 0.001473, mae: 0.041972, mean_q: 1.170132
 364086/1000000: episode: 3641, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.635, mean reward: 0.586 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.063, 10.098], loss: 0.001422, mae: 0.040703, mean_q: 1.168556
 364186/1000000: episode: 3642, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.875, mean reward: 0.589 [0.515, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.349, 10.106], loss: 0.001435, mae: 0.041250, mean_q: 1.172581
 364286/1000000: episode: 3643, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: 57.412, mean reward: 0.574 [0.498, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.098], loss: 0.001502, mae: 0.041877, mean_q: 1.171118
 364386/1000000: episode: 3644, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.992, mean reward: 0.600 [0.511, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.301, 10.098], loss: 0.001442, mae: 0.041013, mean_q: 1.171167
 364486/1000000: episode: 3645, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 62.675, mean reward: 0.627 [0.531, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.916, 10.098], loss: 0.001527, mae: 0.041582, mean_q: 1.173963
 364586/1000000: episode: 3646, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 58.471, mean reward: 0.585 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.420, 10.098], loss: 0.001434, mae: 0.041179, mean_q: 1.175756
 364686/1000000: episode: 3647, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 58.342, mean reward: 0.583 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.285, 10.098], loss: 0.001451, mae: 0.041431, mean_q: 1.177141
 364786/1000000: episode: 3648, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 56.776, mean reward: 0.568 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.441, 10.267], loss: 0.001429, mae: 0.040829, mean_q: 1.174427
 364886/1000000: episode: 3649, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.101, mean reward: 0.581 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.119, 10.108], loss: 0.001481, mae: 0.041671, mean_q: 1.173220
 364986/1000000: episode: 3650, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 59.678, mean reward: 0.597 [0.514, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.145], loss: 0.001578, mae: 0.042420, mean_q: 1.173061
 365086/1000000: episode: 3651, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.481, mean reward: 0.585 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.326, 10.169], loss: 0.001448, mae: 0.041560, mean_q: 1.170394
 365186/1000000: episode: 3652, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 61.152, mean reward: 0.612 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.338], loss: 0.001439, mae: 0.041369, mean_q: 1.172119
 365286/1000000: episode: 3653, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 58.882, mean reward: 0.589 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.008, 10.311], loss: 0.001388, mae: 0.040044, mean_q: 1.173229
 365386/1000000: episode: 3654, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 58.257, mean reward: 0.583 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.472, 10.180], loss: 0.001412, mae: 0.040938, mean_q: 1.175644
 365486/1000000: episode: 3655, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 58.190, mean reward: 0.582 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.229], loss: 0.001455, mae: 0.041178, mean_q: 1.175178
 365586/1000000: episode: 3656, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.341, mean reward: 0.583 [0.499, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.627, 10.098], loss: 0.001307, mae: 0.039116, mean_q: 1.172051
 365686/1000000: episode: 3657, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 58.558, mean reward: 0.586 [0.523, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.094, 10.098], loss: 0.001474, mae: 0.041558, mean_q: 1.168567
 365786/1000000: episode: 3658, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.188, mean reward: 0.582 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.736, 10.098], loss: 0.001467, mae: 0.041094, mean_q: 1.172089
 365886/1000000: episode: 3659, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 60.585, mean reward: 0.606 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.108, 10.320], loss: 0.001480, mae: 0.041720, mean_q: 1.171326
 365986/1000000: episode: 3660, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 56.385, mean reward: 0.564 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.369, 10.140], loss: 0.001443, mae: 0.041016, mean_q: 1.172068
 366086/1000000: episode: 3661, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.306, mean reward: 0.583 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.706, 10.098], loss: 0.001582, mae: 0.042619, mean_q: 1.173781
 366186/1000000: episode: 3662, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.038, mean reward: 0.570 [0.497, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.129, 10.215], loss: 0.001587, mae: 0.042775, mean_q: 1.170645
 366286/1000000: episode: 3663, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.883, mean reward: 0.579 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.892, 10.114], loss: 0.001607, mae: 0.043004, mean_q: 1.166412
 366386/1000000: episode: 3664, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 59.345, mean reward: 0.593 [0.508, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.546, 10.098], loss: 0.001448, mae: 0.040396, mean_q: 1.168589
 366486/1000000: episode: 3665, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 55.960, mean reward: 0.560 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.868, 10.190], loss: 0.001515, mae: 0.041125, mean_q: 1.168337
 366586/1000000: episode: 3666, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.685, mean reward: 0.577 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.083, 10.176], loss: 0.001542, mae: 0.042273, mean_q: 1.169167
 366686/1000000: episode: 3667, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 59.211, mean reward: 0.592 [0.501, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.365, 10.231], loss: 0.001456, mae: 0.041101, mean_q: 1.168596
 366786/1000000: episode: 3668, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 60.553, mean reward: 0.606 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.218, 10.459], loss: 0.001369, mae: 0.040079, mean_q: 1.163986
 366886/1000000: episode: 3669, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 56.750, mean reward: 0.567 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.141], loss: 0.001457, mae: 0.041012, mean_q: 1.162285
 366986/1000000: episode: 3670, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 62.127, mean reward: 0.621 [0.519, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.521, 10.098], loss: 0.001558, mae: 0.042206, mean_q: 1.163692
 367086/1000000: episode: 3671, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.258, mean reward: 0.593 [0.514, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.660, 10.098], loss: 0.001435, mae: 0.041083, mean_q: 1.164882
 367186/1000000: episode: 3672, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 57.527, mean reward: 0.575 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.936, 10.098], loss: 0.001507, mae: 0.041260, mean_q: 1.162859
 367286/1000000: episode: 3673, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.755, mean reward: 0.588 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.800, 10.291], loss: 0.001433, mae: 0.041001, mean_q: 1.161522
 367386/1000000: episode: 3674, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.350, mean reward: 0.594 [0.515, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.690, 10.251], loss: 0.001429, mae: 0.040582, mean_q: 1.167710
 367486/1000000: episode: 3675, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 61.113, mean reward: 0.611 [0.530, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.223, 10.098], loss: 0.001560, mae: 0.042892, mean_q: 1.167765
 367586/1000000: episode: 3676, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 58.374, mean reward: 0.584 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.120], loss: 0.001499, mae: 0.041971, mean_q: 1.168445
 367686/1000000: episode: 3677, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 58.544, mean reward: 0.585 [0.516, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.815, 10.098], loss: 0.001588, mae: 0.043237, mean_q: 1.168296
 367786/1000000: episode: 3678, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.821, mean reward: 0.588 [0.515, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.580, 10.098], loss: 0.001448, mae: 0.041164, mean_q: 1.165650
 367886/1000000: episode: 3679, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 60.652, mean reward: 0.607 [0.515, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.839, 10.098], loss: 0.001382, mae: 0.039881, mean_q: 1.165116
 367986/1000000: episode: 3680, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 57.286, mean reward: 0.573 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.254, 10.171], loss: 0.001518, mae: 0.041739, mean_q: 1.167971
 368086/1000000: episode: 3681, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 56.594, mean reward: 0.566 [0.511, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.467, 10.098], loss: 0.001479, mae: 0.041356, mean_q: 1.162494
 368186/1000000: episode: 3682, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 57.434, mean reward: 0.574 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.735, 10.346], loss: 0.001361, mae: 0.040155, mean_q: 1.160359
 368286/1000000: episode: 3683, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.957, mean reward: 0.590 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.121], loss: 0.001316, mae: 0.039332, mean_q: 1.159869
 368386/1000000: episode: 3684, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 60.607, mean reward: 0.606 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.141], loss: 0.001481, mae: 0.041273, mean_q: 1.162461
 368486/1000000: episode: 3685, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 59.295, mean reward: 0.593 [0.503, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.338, 10.110], loss: 0.001399, mae: 0.040266, mean_q: 1.163317
 368586/1000000: episode: 3686, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 62.095, mean reward: 0.621 [0.511, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.409, 10.129], loss: 0.001415, mae: 0.040775, mean_q: 1.163255
 368686/1000000: episode: 3687, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 58.166, mean reward: 0.582 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.250], loss: 0.001376, mae: 0.040198, mean_q: 1.167194
 368786/1000000: episode: 3688, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 58.040, mean reward: 0.580 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.458, 10.098], loss: 0.001380, mae: 0.040168, mean_q: 1.159776
 368886/1000000: episode: 3689, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 57.317, mean reward: 0.573 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.837, 10.108], loss: 0.001431, mae: 0.041655, mean_q: 1.164576
 368986/1000000: episode: 3690, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.696, mean reward: 0.587 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.309, 10.120], loss: 0.001476, mae: 0.041335, mean_q: 1.164448
 369086/1000000: episode: 3691, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.953, mean reward: 0.590 [0.508, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.819, 10.098], loss: 0.001462, mae: 0.041594, mean_q: 1.165744
 369186/1000000: episode: 3692, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.110, mean reward: 0.571 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.130], loss: 0.001491, mae: 0.042251, mean_q: 1.163228
 369286/1000000: episode: 3693, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 60.622, mean reward: 0.606 [0.502, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.894, 10.411], loss: 0.001423, mae: 0.040937, mean_q: 1.164346
 369386/1000000: episode: 3694, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.478, mean reward: 0.585 [0.506, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.309, 10.098], loss: 0.001395, mae: 0.040637, mean_q: 1.163817
 369486/1000000: episode: 3695, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.248, mean reward: 0.582 [0.508, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.379, 10.286], loss: 0.001485, mae: 0.041612, mean_q: 1.160324
 369586/1000000: episode: 3696, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 64.927, mean reward: 0.649 [0.504, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.236, 10.098], loss: 0.001571, mae: 0.042884, mean_q: 1.159433
 369686/1000000: episode: 3697, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.367, mean reward: 0.584 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.472, 10.098], loss: 0.001437, mae: 0.041063, mean_q: 1.162858
 369786/1000000: episode: 3698, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 61.245, mean reward: 0.612 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.295], loss: 0.001330, mae: 0.040026, mean_q: 1.163960
 369886/1000000: episode: 3699, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 57.559, mean reward: 0.576 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.165], loss: 0.001325, mae: 0.039884, mean_q: 1.163780
 369986/1000000: episode: 3700, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.325, mean reward: 0.593 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.425, 10.399], loss: 0.001379, mae: 0.040174, mean_q: 1.163882
 370086/1000000: episode: 3701, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 56.786, mean reward: 0.568 [0.504, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.183, 10.250], loss: 0.001383, mae: 0.040803, mean_q: 1.163732
 370186/1000000: episode: 3702, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 59.266, mean reward: 0.593 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.098], loss: 0.001384, mae: 0.040130, mean_q: 1.162643
 370286/1000000: episode: 3703, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 60.209, mean reward: 0.602 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.671, 10.397], loss: 0.001406, mae: 0.041009, mean_q: 1.162063
 370386/1000000: episode: 3704, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 58.735, mean reward: 0.587 [0.513, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.729, 10.098], loss: 0.001384, mae: 0.040370, mean_q: 1.163491
 370486/1000000: episode: 3705, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 59.691, mean reward: 0.597 [0.504, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.308, 10.098], loss: 0.001340, mae: 0.039727, mean_q: 1.162430
 370586/1000000: episode: 3706, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 57.771, mean reward: 0.578 [0.502, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.470, 10.226], loss: 0.001411, mae: 0.040339, mean_q: 1.162608
 370686/1000000: episode: 3707, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 57.916, mean reward: 0.579 [0.510, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.911, 10.137], loss: 0.001444, mae: 0.041072, mean_q: 1.162808
 370786/1000000: episode: 3708, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 59.578, mean reward: 0.596 [0.513, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.098], loss: 0.001362, mae: 0.040198, mean_q: 1.161340
 370886/1000000: episode: 3709, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 64.237, mean reward: 0.642 [0.506, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.713, 10.098], loss: 0.001438, mae: 0.041079, mean_q: 1.164212
 370986/1000000: episode: 3710, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.708, mean reward: 0.597 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.303, 10.208], loss: 0.001322, mae: 0.039672, mean_q: 1.164088
 371086/1000000: episode: 3711, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.552, mean reward: 0.586 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.331, 10.138], loss: 0.001461, mae: 0.041328, mean_q: 1.167085
 371186/1000000: episode: 3712, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.270, mean reward: 0.593 [0.519, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.268, 10.098], loss: 0.001420, mae: 0.041392, mean_q: 1.165591
 371286/1000000: episode: 3713, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.855, mean reward: 0.599 [0.519, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.456, 10.098], loss: 0.001536, mae: 0.041974, mean_q: 1.168217
 371386/1000000: episode: 3714, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.665, mean reward: 0.587 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.152, 10.144], loss: 0.001331, mae: 0.040108, mean_q: 1.171795
 371486/1000000: episode: 3715, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.314, mean reward: 0.583 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.617, 10.098], loss: 0.001358, mae: 0.041026, mean_q: 1.171933
 371586/1000000: episode: 3716, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 60.940, mean reward: 0.609 [0.523, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.690, 10.098], loss: 0.001492, mae: 0.041766, mean_q: 1.171292
 371686/1000000: episode: 3717, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.602, mean reward: 0.576 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.098], loss: 0.001406, mae: 0.040491, mean_q: 1.167626
 371786/1000000: episode: 3718, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.263, mean reward: 0.593 [0.499, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.521, 10.098], loss: 0.001384, mae: 0.040461, mean_q: 1.169618
 371886/1000000: episode: 3719, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 57.895, mean reward: 0.579 [0.502, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.773, 10.117], loss: 0.001307, mae: 0.039537, mean_q: 1.171587
 371986/1000000: episode: 3720, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 58.366, mean reward: 0.584 [0.506, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.872, 10.098], loss: 0.001472, mae: 0.041653, mean_q: 1.172936
 372086/1000000: episode: 3721, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 57.853, mean reward: 0.579 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.330, 10.348], loss: 0.001481, mae: 0.042047, mean_q: 1.170696
 372186/1000000: episode: 3722, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 59.297, mean reward: 0.593 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.279, 10.168], loss: 0.001483, mae: 0.041315, mean_q: 1.170445
 372286/1000000: episode: 3723, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 59.523, mean reward: 0.595 [0.516, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.241, 10.267], loss: 0.001490, mae: 0.041518, mean_q: 1.169287
 372386/1000000: episode: 3724, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 57.785, mean reward: 0.578 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.829, 10.098], loss: 0.001449, mae: 0.041579, mean_q: 1.169443
 372486/1000000: episode: 3725, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 57.117, mean reward: 0.571 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.469, 10.148], loss: 0.001372, mae: 0.040114, mean_q: 1.166206
 372586/1000000: episode: 3726, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.229, mean reward: 0.572 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.233, 10.098], loss: 0.001369, mae: 0.039912, mean_q: 1.166376
 372686/1000000: episode: 3727, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.946, mean reward: 0.599 [0.516, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-2.636, 10.098], loss: 0.001341, mae: 0.039864, mean_q: 1.166060
 372786/1000000: episode: 3728, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 61.288, mean reward: 0.613 [0.514, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.243, 10.217], loss: 0.001530, mae: 0.042092, mean_q: 1.169080
 372886/1000000: episode: 3729, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: 59.161, mean reward: 0.592 [0.507, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.920, 10.204], loss: 0.001384, mae: 0.039745, mean_q: 1.164444
 372986/1000000: episode: 3730, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 61.006, mean reward: 0.610 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.323, 10.098], loss: 0.001446, mae: 0.041172, mean_q: 1.166105
 373086/1000000: episode: 3731, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 63.879, mean reward: 0.639 [0.512, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.453, 10.098], loss: 0.001467, mae: 0.041595, mean_q: 1.170794
 373186/1000000: episode: 3732, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 61.192, mean reward: 0.612 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.690, 10.098], loss: 0.001533, mae: 0.042001, mean_q: 1.171602
 373286/1000000: episode: 3733, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 57.459, mean reward: 0.575 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.209, 10.098], loss: 0.001402, mae: 0.040562, mean_q: 1.171362
 373386/1000000: episode: 3734, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 63.326, mean reward: 0.633 [0.512, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.288, 10.098], loss: 0.001478, mae: 0.041601, mean_q: 1.173351
 373486/1000000: episode: 3735, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 60.636, mean reward: 0.606 [0.513, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.067, 10.098], loss: 0.001417, mae: 0.040744, mean_q: 1.174474
 373586/1000000: episode: 3736, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 58.540, mean reward: 0.585 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.805, 10.098], loss: 0.001452, mae: 0.041651, mean_q: 1.174029
 373686/1000000: episode: 3737, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.811, mean reward: 0.588 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.900, 10.098], loss: 0.001379, mae: 0.040294, mean_q: 1.174929
 373786/1000000: episode: 3738, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 57.761, mean reward: 0.578 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.832, 10.112], loss: 0.001473, mae: 0.042222, mean_q: 1.175559
 373886/1000000: episode: 3739, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 58.196, mean reward: 0.582 [0.498, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.940, 10.098], loss: 0.001393, mae: 0.040949, mean_q: 1.176209
 373986/1000000: episode: 3740, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 59.040, mean reward: 0.590 [0.515, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.024, 10.293], loss: 0.001523, mae: 0.042625, mean_q: 1.173808
 374086/1000000: episode: 3741, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.744, mean reward: 0.577 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.634, 10.285], loss: 0.001418, mae: 0.041655, mean_q: 1.174604
 374186/1000000: episode: 3742, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 58.086, mean reward: 0.581 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.705, 10.216], loss: 0.001386, mae: 0.040280, mean_q: 1.173067
 374286/1000000: episode: 3743, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 61.499, mean reward: 0.615 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.847, 10.098], loss: 0.001443, mae: 0.041544, mean_q: 1.174419
 374386/1000000: episode: 3744, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.180, mean reward: 0.572 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.229, 10.098], loss: 0.001370, mae: 0.040337, mean_q: 1.172306
 374486/1000000: episode: 3745, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 56.897, mean reward: 0.569 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.246, 10.106], loss: 0.001415, mae: 0.041202, mean_q: 1.173483
 374586/1000000: episode: 3746, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.226, mean reward: 0.582 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.374, 10.146], loss: 0.001402, mae: 0.040800, mean_q: 1.171242
 374686/1000000: episode: 3747, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 58.698, mean reward: 0.587 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.503, 10.110], loss: 0.001522, mae: 0.042140, mean_q: 1.170199
 374786/1000000: episode: 3748, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 58.519, mean reward: 0.585 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.272, 10.098], loss: 0.001523, mae: 0.042112, mean_q: 1.171557
 374886/1000000: episode: 3749, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 58.945, mean reward: 0.589 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.974, 10.101], loss: 0.001464, mae: 0.041417, mean_q: 1.167369
 374986/1000000: episode: 3750, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 58.835, mean reward: 0.588 [0.509, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.114, 10.160], loss: 0.001508, mae: 0.042241, mean_q: 1.171231
 375086/1000000: episode: 3751, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 59.360, mean reward: 0.594 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.569, 10.231], loss: 0.001435, mae: 0.040644, mean_q: 1.171410
 375186/1000000: episode: 3752, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.218, mean reward: 0.582 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.891, 10.098], loss: 0.001575, mae: 0.042375, mean_q: 1.173001
 375286/1000000: episode: 3753, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 58.436, mean reward: 0.584 [0.511, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.370, 10.098], loss: 0.001457, mae: 0.041784, mean_q: 1.174044
 375386/1000000: episode: 3754, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 60.713, mean reward: 0.607 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.155], loss: 0.001476, mae: 0.041528, mean_q: 1.173476
 375486/1000000: episode: 3755, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 61.334, mean reward: 0.613 [0.503, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.287, 10.130], loss: 0.001473, mae: 0.041446, mean_q: 1.172483
 375586/1000000: episode: 3756, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 61.215, mean reward: 0.612 [0.507, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.082, 10.098], loss: 0.001504, mae: 0.042099, mean_q: 1.172847
 375686/1000000: episode: 3757, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 58.088, mean reward: 0.581 [0.498, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.871, 10.098], loss: 0.001577, mae: 0.042718, mean_q: 1.174538
 375786/1000000: episode: 3758, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 58.528, mean reward: 0.585 [0.507, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.394, 10.124], loss: 0.001527, mae: 0.041828, mean_q: 1.175158
 375886/1000000: episode: 3759, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.251, mean reward: 0.593 [0.504, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.905, 10.098], loss: 0.001528, mae: 0.042188, mean_q: 1.172389
 375986/1000000: episode: 3760, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 58.261, mean reward: 0.583 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.657, 10.129], loss: 0.001468, mae: 0.041520, mean_q: 1.171092
 376086/1000000: episode: 3761, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 59.059, mean reward: 0.591 [0.503, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.071, 10.098], loss: 0.001389, mae: 0.040360, mean_q: 1.169922
 376186/1000000: episode: 3762, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 63.757, mean reward: 0.638 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.666, 10.098], loss: 0.001522, mae: 0.041701, mean_q: 1.170977
 376286/1000000: episode: 3763, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.415, mean reward: 0.584 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.226, 10.190], loss: 0.001425, mae: 0.041085, mean_q: 1.171497
 376386/1000000: episode: 3764, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 57.798, mean reward: 0.578 [0.499, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.774, 10.224], loss: 0.001447, mae: 0.040805, mean_q: 1.172514
 376486/1000000: episode: 3765, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 59.778, mean reward: 0.598 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.288, 10.157], loss: 0.001401, mae: 0.039972, mean_q: 1.174616
 376586/1000000: episode: 3766, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 58.176, mean reward: 0.582 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.975, 10.107], loss: 0.001496, mae: 0.041639, mean_q: 1.170664
 376686/1000000: episode: 3767, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.920, mean reward: 0.579 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.098], loss: 0.001523, mae: 0.042113, mean_q: 1.171650
 376786/1000000: episode: 3768, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.861, mean reward: 0.589 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.903, 10.098], loss: 0.001264, mae: 0.038683, mean_q: 1.168432
 376886/1000000: episode: 3769, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.906, mean reward: 0.589 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.803, 10.291], loss: 0.001419, mae: 0.040699, mean_q: 1.172683
 376986/1000000: episode: 3770, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.268, mean reward: 0.583 [0.507, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.173, 10.258], loss: 0.001398, mae: 0.040723, mean_q: 1.169208
 377086/1000000: episode: 3771, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 58.827, mean reward: 0.588 [0.510, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.540, 10.098], loss: 0.001431, mae: 0.040873, mean_q: 1.170216
 377186/1000000: episode: 3772, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.330, mean reward: 0.573 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.616, 10.098], loss: 0.001331, mae: 0.039663, mean_q: 1.169064
 377286/1000000: episode: 3773, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 57.013, mean reward: 0.570 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.519, 10.149], loss: 0.001347, mae: 0.039765, mean_q: 1.171242
 377386/1000000: episode: 3774, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.508, mean reward: 0.585 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.553, 10.098], loss: 0.001414, mae: 0.040768, mean_q: 1.168775
 377486/1000000: episode: 3775, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 56.391, mean reward: 0.564 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.781, 10.098], loss: 0.001492, mae: 0.042174, mean_q: 1.170411
 377586/1000000: episode: 3776, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.616, mean reward: 0.586 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.170, 10.137], loss: 0.001441, mae: 0.040988, mean_q: 1.168841
 377686/1000000: episode: 3777, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.954, mean reward: 0.590 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.805, 10.213], loss: 0.001421, mae: 0.040609, mean_q: 1.168630
 377786/1000000: episode: 3778, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.036, mean reward: 0.590 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.403, 10.294], loss: 0.001329, mae: 0.040509, mean_q: 1.171022
 377886/1000000: episode: 3779, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 56.995, mean reward: 0.570 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.041, 10.098], loss: 0.001397, mae: 0.040300, mean_q: 1.168217
 377986/1000000: episode: 3780, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 61.957, mean reward: 0.620 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.959, 10.292], loss: 0.001438, mae: 0.041284, mean_q: 1.169842
 378086/1000000: episode: 3781, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.463, mean reward: 0.575 [0.497, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.202], loss: 0.001439, mae: 0.041031, mean_q: 1.167116
 378186/1000000: episode: 3782, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.734, mean reward: 0.597 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.766, 10.321], loss: 0.001421, mae: 0.040677, mean_q: 1.163831
 378286/1000000: episode: 3783, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 59.699, mean reward: 0.597 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.006, 10.136], loss: 0.001585, mae: 0.043343, mean_q: 1.167635
 378386/1000000: episode: 3784, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.974, mean reward: 0.590 [0.500, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.989, 10.307], loss: 0.001433, mae: 0.040562, mean_q: 1.166825
 378486/1000000: episode: 3785, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 60.443, mean reward: 0.604 [0.514, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.390, 10.098], loss: 0.001474, mae: 0.041404, mean_q: 1.164206
 378586/1000000: episode: 3786, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: 57.407, mean reward: 0.574 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.766, 10.254], loss: 0.001484, mae: 0.041749, mean_q: 1.161305
 378686/1000000: episode: 3787, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 58.396, mean reward: 0.584 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.996, 10.098], loss: 0.001458, mae: 0.041063, mean_q: 1.163471
 378786/1000000: episode: 3788, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: 58.985, mean reward: 0.590 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.629, 10.173], loss: 0.001420, mae: 0.040491, mean_q: 1.160644
 378886/1000000: episode: 3789, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.225, mean reward: 0.582 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.993, 10.168], loss: 0.001448, mae: 0.041215, mean_q: 1.166430
 378986/1000000: episode: 3790, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 59.653, mean reward: 0.597 [0.498, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.157], loss: 0.001387, mae: 0.040123, mean_q: 1.166195
 379086/1000000: episode: 3791, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.872, mean reward: 0.579 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.980, 10.188], loss: 0.001520, mae: 0.041678, mean_q: 1.169279
 379186/1000000: episode: 3792, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.467, mean reward: 0.575 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.556, 10.098], loss: 0.001363, mae: 0.040266, mean_q: 1.166199
 379286/1000000: episode: 3793, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 56.873, mean reward: 0.569 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.829, 10.098], loss: 0.001456, mae: 0.041024, mean_q: 1.166152
 379386/1000000: episode: 3794, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.286, mean reward: 0.583 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.223, 10.253], loss: 0.001529, mae: 0.041981, mean_q: 1.165847
 379486/1000000: episode: 3795, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 57.520, mean reward: 0.575 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.303, 10.134], loss: 0.001382, mae: 0.039493, mean_q: 1.162416
 379586/1000000: episode: 3796, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.919, mean reward: 0.589 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.664, 10.098], loss: 0.001431, mae: 0.041359, mean_q: 1.161099
 379686/1000000: episode: 3797, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 61.180, mean reward: 0.612 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.153, 10.098], loss: 0.001419, mae: 0.040865, mean_q: 1.162677
 379786/1000000: episode: 3798, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.254, mean reward: 0.583 [0.510, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.799, 10.098], loss: 0.001485, mae: 0.042201, mean_q: 1.164918
 379886/1000000: episode: 3799, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 58.202, mean reward: 0.582 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.963, 10.098], loss: 0.001493, mae: 0.041324, mean_q: 1.165094
 379986/1000000: episode: 3800, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 58.593, mean reward: 0.586 [0.498, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.259], loss: 0.001437, mae: 0.040877, mean_q: 1.164935
 380086/1000000: episode: 3801, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 61.151, mean reward: 0.612 [0.503, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.010, 10.173], loss: 0.001435, mae: 0.040897, mean_q: 1.162558
 380186/1000000: episode: 3802, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 61.386, mean reward: 0.614 [0.528, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.490, 10.128], loss: 0.001449, mae: 0.041406, mean_q: 1.163843
 380286/1000000: episode: 3803, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 60.637, mean reward: 0.606 [0.512, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.805, 10.098], loss: 0.001365, mae: 0.040333, mean_q: 1.165963
 380386/1000000: episode: 3804, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 60.385, mean reward: 0.604 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.780, 10.473], loss: 0.001348, mae: 0.039692, mean_q: 1.162992
 380486/1000000: episode: 3805, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 57.266, mean reward: 0.573 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.295, 10.171], loss: 0.001426, mae: 0.041015, mean_q: 1.163231
 380586/1000000: episode: 3806, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 60.327, mean reward: 0.603 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.607, 10.391], loss: 0.001471, mae: 0.041483, mean_q: 1.163776
 380686/1000000: episode: 3807, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.935, mean reward: 0.579 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.714, 10.098], loss: 0.001340, mae: 0.040184, mean_q: 1.166294
 380786/1000000: episode: 3808, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.503, mean reward: 0.585 [0.497, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.375, 10.118], loss: 0.001380, mae: 0.040476, mean_q: 1.162410
 380886/1000000: episode: 3809, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.091, mean reward: 0.591 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.504, 10.098], loss: 0.001411, mae: 0.040541, mean_q: 1.164123
 380986/1000000: episode: 3810, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.625, mean reward: 0.586 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.188], loss: 0.001423, mae: 0.040953, mean_q: 1.162779
 381086/1000000: episode: 3811, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 63.169, mean reward: 0.632 [0.514, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.072, 10.538], loss: 0.001394, mae: 0.040530, mean_q: 1.164539
 381186/1000000: episode: 3812, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.354, mean reward: 0.594 [0.498, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.945, 10.236], loss: 0.001355, mae: 0.040111, mean_q: 1.163246
 381286/1000000: episode: 3813, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 58.643, mean reward: 0.586 [0.505, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.923, 10.297], loss: 0.001525, mae: 0.041774, mean_q: 1.164171
 381386/1000000: episode: 3814, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 59.427, mean reward: 0.594 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.796, 10.198], loss: 0.001438, mae: 0.040808, mean_q: 1.162938
 381486/1000000: episode: 3815, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 58.367, mean reward: 0.584 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.247, 10.098], loss: 0.001470, mae: 0.041406, mean_q: 1.162740
 381586/1000000: episode: 3816, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.014, mean reward: 0.580 [0.501, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.098], loss: 0.001495, mae: 0.041806, mean_q: 1.162967
 381686/1000000: episode: 3817, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 58.999, mean reward: 0.590 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.098], loss: 0.001412, mae: 0.040890, mean_q: 1.166250
 381786/1000000: episode: 3818, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 59.571, mean reward: 0.596 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.867, 10.157], loss: 0.001474, mae: 0.041933, mean_q: 1.164585
 381886/1000000: episode: 3819, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 59.256, mean reward: 0.593 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.262], loss: 0.001470, mae: 0.041972, mean_q: 1.167248
 381986/1000000: episode: 3820, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 56.061, mean reward: 0.561 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.661, 10.098], loss: 0.001428, mae: 0.041326, mean_q: 1.167122
 382086/1000000: episode: 3821, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.864, mean reward: 0.599 [0.505, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.197, 10.098], loss: 0.001393, mae: 0.040236, mean_q: 1.165176
 382186/1000000: episode: 3822, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.605, mean reward: 0.576 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.221, 10.149], loss: 0.001422, mae: 0.041038, mean_q: 1.162738
 382286/1000000: episode: 3823, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.764, mean reward: 0.588 [0.519, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.494, 10.264], loss: 0.001411, mae: 0.040855, mean_q: 1.162978
 382386/1000000: episode: 3824, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 61.231, mean reward: 0.612 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.821, 10.239], loss: 0.001503, mae: 0.042224, mean_q: 1.166860
 382486/1000000: episode: 3825, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.395, mean reward: 0.584 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.088, 10.098], loss: 0.001434, mae: 0.041164, mean_q: 1.166978
 382586/1000000: episode: 3826, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 59.364, mean reward: 0.594 [0.511, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.724, 10.215], loss: 0.001342, mae: 0.039817, mean_q: 1.165398
 382686/1000000: episode: 3827, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 61.528, mean reward: 0.615 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.393, 10.132], loss: 0.001352, mae: 0.039870, mean_q: 1.165403
 382786/1000000: episode: 3828, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 59.379, mean reward: 0.594 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.578, 10.421], loss: 0.001507, mae: 0.041136, mean_q: 1.165636
 382886/1000000: episode: 3829, duration: 0.816s, episode steps: 100, steps per second: 123, episode reward: 60.598, mean reward: 0.606 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.131, 10.098], loss: 0.001446, mae: 0.041433, mean_q: 1.168721
 382986/1000000: episode: 3830, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 63.684, mean reward: 0.637 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.459, 10.545], loss: 0.001357, mae: 0.040215, mean_q: 1.169760
 383086/1000000: episode: 3831, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: 60.972, mean reward: 0.610 [0.508, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.522, 10.098], loss: 0.001469, mae: 0.041564, mean_q: 1.173679
 383186/1000000: episode: 3832, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 62.255, mean reward: 0.623 [0.512, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.737, 10.291], loss: 0.001454, mae: 0.041153, mean_q: 1.171773
 383286/1000000: episode: 3833, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.287, mean reward: 0.583 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.308, 10.098], loss: 0.001497, mae: 0.042190, mean_q: 1.173269
 383386/1000000: episode: 3834, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 57.281, mean reward: 0.573 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.441, 10.294], loss: 0.001407, mae: 0.040977, mean_q: 1.174456
 383486/1000000: episode: 3835, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.140, mean reward: 0.581 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.308], loss: 0.001405, mae: 0.040514, mean_q: 1.171305
 383586/1000000: episode: 3836, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: 57.788, mean reward: 0.578 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.254], loss: 0.001428, mae: 0.041231, mean_q: 1.174603
 383686/1000000: episode: 3837, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.458, mean reward: 0.585 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.641, 10.098], loss: 0.001509, mae: 0.042096, mean_q: 1.172740
 383786/1000000: episode: 3838, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 58.573, mean reward: 0.586 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.098], loss: 0.001567, mae: 0.042994, mean_q: 1.169896
 383886/1000000: episode: 3839, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.836, mean reward: 0.598 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.879, 10.098], loss: 0.001441, mae: 0.040802, mean_q: 1.168954
 383986/1000000: episode: 3840, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 63.560, mean reward: 0.636 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.098], loss: 0.001456, mae: 0.041307, mean_q: 1.172850
 384086/1000000: episode: 3841, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 57.607, mean reward: 0.576 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.103, 10.098], loss: 0.001385, mae: 0.040697, mean_q: 1.174861
 384186/1000000: episode: 3842, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 59.497, mean reward: 0.595 [0.516, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.348, 10.190], loss: 0.001406, mae: 0.041206, mean_q: 1.173594
 384286/1000000: episode: 3843, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.582, mean reward: 0.586 [0.499, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.970, 10.131], loss: 0.001423, mae: 0.041088, mean_q: 1.176004
 384386/1000000: episode: 3844, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 59.974, mean reward: 0.600 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.479, 10.101], loss: 0.001352, mae: 0.040536, mean_q: 1.176087
 384486/1000000: episode: 3845, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 59.753, mean reward: 0.598 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.604, 10.239], loss: 0.001393, mae: 0.041072, mean_q: 1.175909
 384586/1000000: episode: 3846, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 61.239, mean reward: 0.612 [0.509, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.077, 10.098], loss: 0.001340, mae: 0.040270, mean_q: 1.177497
 384686/1000000: episode: 3847, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 60.848, mean reward: 0.608 [0.514, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.997, 10.098], loss: 0.001314, mae: 0.040203, mean_q: 1.176147
 384786/1000000: episode: 3848, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 57.673, mean reward: 0.577 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.718, 10.128], loss: 0.001437, mae: 0.041219, mean_q: 1.176839
 384886/1000000: episode: 3849, duration: 0.797s, episode steps: 100, steps per second: 126, episode reward: 59.471, mean reward: 0.595 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.005, 10.287], loss: 0.001425, mae: 0.041091, mean_q: 1.176153
 384986/1000000: episode: 3850, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 62.355, mean reward: 0.624 [0.530, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.008, 10.234], loss: 0.001375, mae: 0.040970, mean_q: 1.180344
 385086/1000000: episode: 3851, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 60.523, mean reward: 0.605 [0.506, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.633, 10.098], loss: 0.001466, mae: 0.041417, mean_q: 1.176960
 385186/1000000: episode: 3852, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 60.112, mean reward: 0.601 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.098], loss: 0.001341, mae: 0.039783, mean_q: 1.180059
 385286/1000000: episode: 3853, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 56.670, mean reward: 0.567 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.819, 10.098], loss: 0.001384, mae: 0.040375, mean_q: 1.177134
 385386/1000000: episode: 3854, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 56.687, mean reward: 0.567 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.114], loss: 0.001334, mae: 0.040041, mean_q: 1.173322
 385486/1000000: episode: 3855, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.676, mean reward: 0.587 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.031, 10.098], loss: 0.001508, mae: 0.041632, mean_q: 1.176063
 385586/1000000: episode: 3856, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 58.793, mean reward: 0.588 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.045, 10.098], loss: 0.001686, mae: 0.043895, mean_q: 1.175414
 385686/1000000: episode: 3857, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 56.560, mean reward: 0.566 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.599, 10.182], loss: 0.001465, mae: 0.041549, mean_q: 1.175116
 385786/1000000: episode: 3858, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 57.942, mean reward: 0.579 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.103], loss: 0.001473, mae: 0.041185, mean_q: 1.174028
 385886/1000000: episode: 3859, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.822, mean reward: 0.588 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.370, 10.098], loss: 0.001504, mae: 0.042149, mean_q: 1.170125
 385986/1000000: episode: 3860, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 59.668, mean reward: 0.597 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.586, 10.445], loss: 0.001472, mae: 0.042048, mean_q: 1.172864
 386086/1000000: episode: 3861, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 60.869, mean reward: 0.609 [0.497, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.134], loss: 0.001456, mae: 0.041363, mean_q: 1.171423
 386186/1000000: episode: 3862, duration: 0.793s, episode steps: 100, steps per second: 126, episode reward: 60.064, mean reward: 0.601 [0.511, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.778, 10.160], loss: 0.001451, mae: 0.041503, mean_q: 1.171890
 386286/1000000: episode: 3863, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 60.338, mean reward: 0.603 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.086, 10.098], loss: 0.001477, mae: 0.041824, mean_q: 1.172874
 386386/1000000: episode: 3864, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 63.729, mean reward: 0.637 [0.508, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.994, 10.505], loss: 0.001529, mae: 0.042356, mean_q: 1.175115
 386486/1000000: episode: 3865, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 60.517, mean reward: 0.605 [0.523, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.600, 10.257], loss: 0.001480, mae: 0.042366, mean_q: 1.179433
 386586/1000000: episode: 3866, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.884, mean reward: 0.579 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.236, 10.378], loss: 0.001476, mae: 0.041427, mean_q: 1.174688
 386686/1000000: episode: 3867, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.074, mean reward: 0.581 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.098], loss: 0.001469, mae: 0.041786, mean_q: 1.178847
 386786/1000000: episode: 3868, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 59.051, mean reward: 0.591 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.881, 10.098], loss: 0.001422, mae: 0.041600, mean_q: 1.180656
 386886/1000000: episode: 3869, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 56.253, mean reward: 0.563 [0.501, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.439, 10.098], loss: 0.001518, mae: 0.042272, mean_q: 1.178413
 386986/1000000: episode: 3870, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.924, mean reward: 0.579 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.944, 10.243], loss: 0.001535, mae: 0.042389, mean_q: 1.179962
 387086/1000000: episode: 3871, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.481, mean reward: 0.585 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.098], loss: 0.001532, mae: 0.042636, mean_q: 1.180288
 387186/1000000: episode: 3872, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.201, mean reward: 0.592 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.609, 10.098], loss: 0.001462, mae: 0.041440, mean_q: 1.177242
 387286/1000000: episode: 3873, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 56.933, mean reward: 0.569 [0.498, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.508, 10.223], loss: 0.001522, mae: 0.042554, mean_q: 1.179457
 387386/1000000: episode: 3874, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 59.989, mean reward: 0.600 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.130, 10.198], loss: 0.001426, mae: 0.041123, mean_q: 1.179422
 387486/1000000: episode: 3875, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 58.717, mean reward: 0.587 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.471, 10.292], loss: 0.001481, mae: 0.042124, mean_q: 1.180152
 387586/1000000: episode: 3876, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 57.735, mean reward: 0.577 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.347, 10.299], loss: 0.001444, mae: 0.041729, mean_q: 1.176281
 387686/1000000: episode: 3877, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 61.847, mean reward: 0.618 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.739, 10.126], loss: 0.001485, mae: 0.042058, mean_q: 1.177110
 387786/1000000: episode: 3878, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 58.151, mean reward: 0.582 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.862, 10.136], loss: 0.001344, mae: 0.040376, mean_q: 1.173099
 387886/1000000: episode: 3879, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.095, mean reward: 0.571 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.397, 10.098], loss: 0.001425, mae: 0.040876, mean_q: 1.174842
 387986/1000000: episode: 3880, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.301, mean reward: 0.593 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.146, 10.414], loss: 0.001408, mae: 0.040854, mean_q: 1.176184
 388086/1000000: episode: 3881, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.088, mean reward: 0.571 [0.502, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.363, 10.109], loss: 0.001420, mae: 0.040683, mean_q: 1.173077
 388186/1000000: episode: 3882, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 62.243, mean reward: 0.622 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.283, 10.098], loss: 0.001441, mae: 0.041225, mean_q: 1.169202
 388286/1000000: episode: 3883, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.210, mean reward: 0.582 [0.502, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.930, 10.137], loss: 0.001400, mae: 0.040501, mean_q: 1.168378
 388386/1000000: episode: 3884, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.430, mean reward: 0.574 [0.510, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.410, 10.153], loss: 0.001406, mae: 0.040935, mean_q: 1.169566
 388486/1000000: episode: 3885, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 59.851, mean reward: 0.599 [0.500, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.342, 10.098], loss: 0.001437, mae: 0.041292, mean_q: 1.171864
 388586/1000000: episode: 3886, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 57.756, mean reward: 0.578 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.727, 10.098], loss: 0.001388, mae: 0.040461, mean_q: 1.168189
 388686/1000000: episode: 3887, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.997, mean reward: 0.600 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.375, 10.109], loss: 0.001407, mae: 0.041133, mean_q: 1.167801
 388786/1000000: episode: 3888, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 56.879, mean reward: 0.569 [0.497, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.621, 10.228], loss: 0.001452, mae: 0.041804, mean_q: 1.173620
 388886/1000000: episode: 3889, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.506, mean reward: 0.575 [0.504, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.579, 10.098], loss: 0.001379, mae: 0.040615, mean_q: 1.171194
 388986/1000000: episode: 3890, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 57.637, mean reward: 0.576 [0.499, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.098], loss: 0.001363, mae: 0.039970, mean_q: 1.167520
 389086/1000000: episode: 3891, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.610, mean reward: 0.586 [0.508, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.109, 10.098], loss: 0.001428, mae: 0.040831, mean_q: 1.169608
 389186/1000000: episode: 3892, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 57.876, mean reward: 0.579 [0.498, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.923, 10.098], loss: 0.001365, mae: 0.040595, mean_q: 1.166405
 389286/1000000: episode: 3893, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 57.187, mean reward: 0.572 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.163, 10.098], loss: 0.001470, mae: 0.040912, mean_q: 1.166284
 389386/1000000: episode: 3894, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.296, mean reward: 0.583 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.280, 10.098], loss: 0.001420, mae: 0.040866, mean_q: 1.167078
 389486/1000000: episode: 3895, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 58.030, mean reward: 0.580 [0.505, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.110, 10.150], loss: 0.001429, mae: 0.041287, mean_q: 1.164368
 389586/1000000: episode: 3896, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 64.487, mean reward: 0.645 [0.511, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.856, 10.098], loss: 0.001393, mae: 0.040491, mean_q: 1.166487
 389686/1000000: episode: 3897, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 59.207, mean reward: 0.592 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.759, 10.216], loss: 0.001298, mae: 0.039685, mean_q: 1.164465
 389786/1000000: episode: 3898, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.174, mean reward: 0.572 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.521, 10.098], loss: 0.001343, mae: 0.040089, mean_q: 1.165523
 389886/1000000: episode: 3899, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.653, mean reward: 0.577 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.633, 10.145], loss: 0.001356, mae: 0.039748, mean_q: 1.165863
 389986/1000000: episode: 3900, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 57.885, mean reward: 0.579 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.207, 10.098], loss: 0.001467, mae: 0.041714, mean_q: 1.165126
 390086/1000000: episode: 3901, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 58.862, mean reward: 0.589 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.110, 10.098], loss: 0.001461, mae: 0.041641, mean_q: 1.162907
 390186/1000000: episode: 3902, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 58.973, mean reward: 0.590 [0.510, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.460, 10.265], loss: 0.001351, mae: 0.039873, mean_q: 1.161935
 390286/1000000: episode: 3903, duration: 0.804s, episode steps: 100, steps per second: 124, episode reward: 57.351, mean reward: 0.574 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.778, 10.146], loss: 0.001356, mae: 0.039886, mean_q: 1.161579
 390386/1000000: episode: 3904, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 60.763, mean reward: 0.608 [0.522, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.203], loss: 0.001424, mae: 0.041163, mean_q: 1.164334
 390486/1000000: episode: 3905, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.427, mean reward: 0.584 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.813, 10.098], loss: 0.001478, mae: 0.041994, mean_q: 1.163683
 390586/1000000: episode: 3906, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 59.031, mean reward: 0.590 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.098], loss: 0.001344, mae: 0.039836, mean_q: 1.163472
 390686/1000000: episode: 3907, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 59.076, mean reward: 0.591 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.518, 10.273], loss: 0.001383, mae: 0.040611, mean_q: 1.162312
 390786/1000000: episode: 3908, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 58.741, mean reward: 0.587 [0.498, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.262, 10.098], loss: 0.001519, mae: 0.041803, mean_q: 1.162982
 390886/1000000: episode: 3909, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 60.657, mean reward: 0.607 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.582, 10.098], loss: 0.001368, mae: 0.040903, mean_q: 1.164094
 390986/1000000: episode: 3910, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 59.052, mean reward: 0.591 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.871, 10.306], loss: 0.001426, mae: 0.040982, mean_q: 1.165788
 391086/1000000: episode: 3911, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.822, mean reward: 0.588 [0.498, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.727, 10.276], loss: 0.001421, mae: 0.041534, mean_q: 1.164690
 391186/1000000: episode: 3912, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 59.425, mean reward: 0.594 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.005, 10.423], loss: 0.001373, mae: 0.040934, mean_q: 1.164544
 391286/1000000: episode: 3913, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 61.035, mean reward: 0.610 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.471, 10.098], loss: 0.001369, mae: 0.040743, mean_q: 1.164228
 391386/1000000: episode: 3914, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.566, mean reward: 0.596 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.660, 10.098], loss: 0.001388, mae: 0.040618, mean_q: 1.164526
 391486/1000000: episode: 3915, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.528, mean reward: 0.585 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.098], loss: 0.001319, mae: 0.039518, mean_q: 1.163903
 391586/1000000: episode: 3916, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.807, mean reward: 0.588 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.313, 10.243], loss: 0.001405, mae: 0.041123, mean_q: 1.164598
 391686/1000000: episode: 3917, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 59.582, mean reward: 0.596 [0.508, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.288, 10.325], loss: 0.001439, mae: 0.041321, mean_q: 1.165632
 391786/1000000: episode: 3918, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.745, mean reward: 0.597 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.684, 10.345], loss: 0.001391, mae: 0.040916, mean_q: 1.163579
 391886/1000000: episode: 3919, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.767, mean reward: 0.578 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.306, 10.254], loss: 0.001383, mae: 0.040489, mean_q: 1.160401
 391986/1000000: episode: 3920, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 59.660, mean reward: 0.597 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.098], loss: 0.001356, mae: 0.040325, mean_q: 1.162874
 392086/1000000: episode: 3921, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.038, mean reward: 0.580 [0.498, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.098], loss: 0.001383, mae: 0.040756, mean_q: 1.165804
 392186/1000000: episode: 3922, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 59.214, mean reward: 0.592 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.061, 10.184], loss: 0.001399, mae: 0.041156, mean_q: 1.166028
 392286/1000000: episode: 3923, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.182, mean reward: 0.582 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.717, 10.254], loss: 0.001415, mae: 0.040973, mean_q: 1.165192
 392386/1000000: episode: 3924, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 56.901, mean reward: 0.569 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.265, 10.257], loss: 0.001525, mae: 0.042380, mean_q: 1.162046
 392486/1000000: episode: 3925, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.999, mean reward: 0.580 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.746, 10.240], loss: 0.001334, mae: 0.039644, mean_q: 1.161102
 392586/1000000: episode: 3926, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 59.128, mean reward: 0.591 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.258], loss: 0.001456, mae: 0.041771, mean_q: 1.165550
 392686/1000000: episode: 3927, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 61.644, mean reward: 0.616 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.079, 10.459], loss: 0.001313, mae: 0.039204, mean_q: 1.162218
 392786/1000000: episode: 3928, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 58.615, mean reward: 0.586 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.654, 10.285], loss: 0.001423, mae: 0.041319, mean_q: 1.164212
 392886/1000000: episode: 3929, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.344, mean reward: 0.583 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.365, 10.191], loss: 0.001487, mae: 0.042528, mean_q: 1.162258
 392986/1000000: episode: 3930, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.858, mean reward: 0.589 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.675, 10.098], loss: 0.001449, mae: 0.041113, mean_q: 1.163628
 393086/1000000: episode: 3931, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 60.887, mean reward: 0.609 [0.511, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.916, 10.280], loss: 0.001434, mae: 0.041529, mean_q: 1.168507
 393186/1000000: episode: 3932, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 56.192, mean reward: 0.562 [0.510, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.174], loss: 0.001428, mae: 0.041485, mean_q: 1.163229
 393286/1000000: episode: 3933, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 57.460, mean reward: 0.575 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.650, 10.098], loss: 0.001405, mae: 0.041250, mean_q: 1.161949
 393386/1000000: episode: 3934, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 59.720, mean reward: 0.597 [0.509, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.358, 10.098], loss: 0.001496, mae: 0.042102, mean_q: 1.162137
 393486/1000000: episode: 3935, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.143, mean reward: 0.571 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.896, 10.098], loss: 0.001396, mae: 0.040855, mean_q: 1.167311
 393586/1000000: episode: 3936, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 59.363, mean reward: 0.594 [0.499, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.490, 10.399], loss: 0.001472, mae: 0.041824, mean_q: 1.162476
 393686/1000000: episode: 3937, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.362, mean reward: 0.584 [0.515, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.610, 10.179], loss: 0.001474, mae: 0.041636, mean_q: 1.161600
 393786/1000000: episode: 3938, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 60.093, mean reward: 0.601 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.559, 10.305], loss: 0.001453, mae: 0.041417, mean_q: 1.164822
 393886/1000000: episode: 3939, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 57.791, mean reward: 0.578 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.369, 10.159], loss: 0.001504, mae: 0.042319, mean_q: 1.164988
 393986/1000000: episode: 3940, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.109, mean reward: 0.571 [0.502, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.791, 10.098], loss: 0.001429, mae: 0.040950, mean_q: 1.162796
 394086/1000000: episode: 3941, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 58.871, mean reward: 0.589 [0.516, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.482, 10.146], loss: 0.001452, mae: 0.041288, mean_q: 1.164582
 394186/1000000: episode: 3942, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 58.865, mean reward: 0.589 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.552, 10.108], loss: 0.001512, mae: 0.042231, mean_q: 1.164615
 394286/1000000: episode: 3943, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 58.246, mean reward: 0.582 [0.501, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.001528, mae: 0.042070, mean_q: 1.167375
 394386/1000000: episode: 3944, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.987, mean reward: 0.580 [0.509, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.518, 10.098], loss: 0.001397, mae: 0.040905, mean_q: 1.164157
 394486/1000000: episode: 3945, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 59.269, mean reward: 0.593 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.171, 10.413], loss: 0.001379, mae: 0.040571, mean_q: 1.162892
 394586/1000000: episode: 3946, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.724, mean reward: 0.577 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.996, 10.117], loss: 0.001521, mae: 0.042076, mean_q: 1.163230
 394686/1000000: episode: 3947, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 55.845, mean reward: 0.558 [0.506, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.810, 10.162], loss: 0.001526, mae: 0.042110, mean_q: 1.159773
 394786/1000000: episode: 3948, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 56.545, mean reward: 0.565 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.208, 10.113], loss: 0.001531, mae: 0.042252, mean_q: 1.162620
 394886/1000000: episode: 3949, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.486, mean reward: 0.585 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.047, 10.156], loss: 0.001451, mae: 0.041736, mean_q: 1.162437
 394986/1000000: episode: 3950, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 57.750, mean reward: 0.577 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.657, 10.098], loss: 0.001532, mae: 0.042000, mean_q: 1.161914
 395086/1000000: episode: 3951, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.522, mean reward: 0.585 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.579, 10.333], loss: 0.001444, mae: 0.040973, mean_q: 1.157487
 395186/1000000: episode: 3952, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 59.811, mean reward: 0.598 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.513, 10.279], loss: 0.001445, mae: 0.041363, mean_q: 1.162460
 395286/1000000: episode: 3953, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.473, mean reward: 0.585 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.779, 10.114], loss: 0.001568, mae: 0.042851, mean_q: 1.164518
 395386/1000000: episode: 3954, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 58.960, mean reward: 0.590 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.297, 10.131], loss: 0.001446, mae: 0.041033, mean_q: 1.163293
 395486/1000000: episode: 3955, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 65.077, mean reward: 0.651 [0.509, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.774, 10.476], loss: 0.001377, mae: 0.040161, mean_q: 1.160600
 395586/1000000: episode: 3956, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 58.638, mean reward: 0.586 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.173, 10.098], loss: 0.001457, mae: 0.041612, mean_q: 1.164157
 395686/1000000: episode: 3957, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 63.318, mean reward: 0.633 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.883, 10.098], loss: 0.001483, mae: 0.041840, mean_q: 1.163768
 395786/1000000: episode: 3958, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.169, mean reward: 0.582 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.800, 10.098], loss: 0.001483, mae: 0.041747, mean_q: 1.165118
 395886/1000000: episode: 3959, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.830, mean reward: 0.588 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.274, 10.245], loss: 0.001456, mae: 0.041289, mean_q: 1.164689
 395986/1000000: episode: 3960, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 58.259, mean reward: 0.583 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.286, 10.098], loss: 0.001405, mae: 0.040954, mean_q: 1.163087
 396086/1000000: episode: 3961, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 61.320, mean reward: 0.613 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.451, 10.098], loss: 0.001464, mae: 0.041183, mean_q: 1.161688
 396186/1000000: episode: 3962, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 59.176, mean reward: 0.592 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.915, 10.098], loss: 0.001570, mae: 0.042906, mean_q: 1.167822
 396286/1000000: episode: 3963, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 63.538, mean reward: 0.635 [0.508, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.063, 10.098], loss: 0.001341, mae: 0.039856, mean_q: 1.164058
 396386/1000000: episode: 3964, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 56.899, mean reward: 0.569 [0.503, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.859, 10.098], loss: 0.001457, mae: 0.041012, mean_q: 1.165799
 396486/1000000: episode: 3965, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: 59.555, mean reward: 0.596 [0.501, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.332, 10.098], loss: 0.001431, mae: 0.041402, mean_q: 1.169569
 396586/1000000: episode: 3966, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.835, mean reward: 0.588 [0.499, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.411, 10.188], loss: 0.001446, mae: 0.041154, mean_q: 1.164739
 396686/1000000: episode: 3967, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 57.846, mean reward: 0.578 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.162], loss: 0.001477, mae: 0.042080, mean_q: 1.168532
 396786/1000000: episode: 3968, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.587, mean reward: 0.596 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.207, 10.131], loss: 0.001399, mae: 0.040506, mean_q: 1.163510
 396886/1000000: episode: 3969, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.640, mean reward: 0.576 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.001427, mae: 0.041154, mean_q: 1.164234
 396986/1000000: episode: 3970, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 60.351, mean reward: 0.604 [0.507, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.557, 10.098], loss: 0.001479, mae: 0.041954, mean_q: 1.165003
 397086/1000000: episode: 3971, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 61.497, mean reward: 0.615 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.098], loss: 0.001454, mae: 0.041483, mean_q: 1.166875
 397186/1000000: episode: 3972, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 58.704, mean reward: 0.587 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.575, 10.470], loss: 0.001390, mae: 0.040459, mean_q: 1.166067
 397286/1000000: episode: 3973, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.279, mean reward: 0.583 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.370, 10.241], loss: 0.001453, mae: 0.041556, mean_q: 1.163344
 397386/1000000: episode: 3974, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 61.095, mean reward: 0.611 [0.515, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.331, 10.098], loss: 0.001363, mae: 0.039866, mean_q: 1.169301
 397486/1000000: episode: 3975, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 57.566, mean reward: 0.576 [0.503, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.582, 10.098], loss: 0.001374, mae: 0.040385, mean_q: 1.161931
 397586/1000000: episode: 3976, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.430, mean reward: 0.584 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.257], loss: 0.001470, mae: 0.042055, mean_q: 1.164811
 397686/1000000: episode: 3977, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.291, mean reward: 0.583 [0.499, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.695, 10.362], loss: 0.001433, mae: 0.041320, mean_q: 1.164113
 397786/1000000: episode: 3978, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.734, mean reward: 0.577 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.155, 10.137], loss: 0.001476, mae: 0.041482, mean_q: 1.162831
 397886/1000000: episode: 3979, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.014, mean reward: 0.590 [0.498, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.111, 10.203], loss: 0.001364, mae: 0.040136, mean_q: 1.168281
 397986/1000000: episode: 3980, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 58.573, mean reward: 0.586 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.930, 10.400], loss: 0.001403, mae: 0.040188, mean_q: 1.163833
 398086/1000000: episode: 3981, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 60.390, mean reward: 0.604 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.990, 10.098], loss: 0.001464, mae: 0.041232, mean_q: 1.163793
 398186/1000000: episode: 3982, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 59.426, mean reward: 0.594 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.444, 10.098], loss: 0.001364, mae: 0.040391, mean_q: 1.167341
 398286/1000000: episode: 3983, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 59.163, mean reward: 0.592 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.236, 10.098], loss: 0.001352, mae: 0.040106, mean_q: 1.164210
 398386/1000000: episode: 3984, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 61.295, mean reward: 0.613 [0.512, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.249, 10.150], loss: 0.001361, mae: 0.039814, mean_q: 1.166956
 398486/1000000: episode: 3985, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 60.348, mean reward: 0.603 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.130, 10.098], loss: 0.001407, mae: 0.039996, mean_q: 1.169770
 398586/1000000: episode: 3986, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 60.696, mean reward: 0.607 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.280, 10.279], loss: 0.001352, mae: 0.040301, mean_q: 1.168186
 398686/1000000: episode: 3987, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 56.695, mean reward: 0.567 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.098], loss: 0.001374, mae: 0.040519, mean_q: 1.170876
 398786/1000000: episode: 3988, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 57.635, mean reward: 0.576 [0.513, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.960, 10.308], loss: 0.001424, mae: 0.040677, mean_q: 1.166650
 398886/1000000: episode: 3989, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 59.304, mean reward: 0.593 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.029, 10.214], loss: 0.001384, mae: 0.040744, mean_q: 1.166149
 398986/1000000: episode: 3990, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.536, mean reward: 0.585 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.969, 10.174], loss: 0.001396, mae: 0.040772, mean_q: 1.171903
 399086/1000000: episode: 3991, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.738, mean reward: 0.587 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.285, 10.116], loss: 0.001455, mae: 0.041732, mean_q: 1.167084
 399186/1000000: episode: 3992, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 58.767, mean reward: 0.588 [0.510, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.802, 10.098], loss: 0.001401, mae: 0.040986, mean_q: 1.170837
 399286/1000000: episode: 3993, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.464, mean reward: 0.595 [0.513, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.054, 10.098], loss: 0.001326, mae: 0.039945, mean_q: 1.168819
 399386/1000000: episode: 3994, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.855, mean reward: 0.589 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.604, 10.451], loss: 0.001393, mae: 0.040366, mean_q: 1.171730
 399486/1000000: episode: 3995, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.594, mean reward: 0.586 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.826, 10.098], loss: 0.001349, mae: 0.040310, mean_q: 1.171419
 399586/1000000: episode: 3996, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 58.010, mean reward: 0.580 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.938, 10.154], loss: 0.001389, mae: 0.040641, mean_q: 1.172595
 399686/1000000: episode: 3997, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 59.448, mean reward: 0.594 [0.505, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.689, 10.359], loss: 0.001382, mae: 0.040673, mean_q: 1.168646
 399786/1000000: episode: 3998, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 61.942, mean reward: 0.619 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.105, 10.358], loss: 0.001457, mae: 0.041407, mean_q: 1.176812
 399886/1000000: episode: 3999, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.393, mean reward: 0.584 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.014, 10.217], loss: 0.001454, mae: 0.041573, mean_q: 1.173116
 399986/1000000: episode: 4000, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 59.071, mean reward: 0.591 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.098], loss: 0.001337, mae: 0.039983, mean_q: 1.172893
 400086/1000000: episode: 4001, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 57.160, mean reward: 0.572 [0.497, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.482, 10.098], loss: 0.001377, mae: 0.040371, mean_q: 1.167666
 400186/1000000: episode: 4002, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 58.190, mean reward: 0.582 [0.516, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.791, 10.098], loss: 0.001412, mae: 0.040738, mean_q: 1.169053
 400286/1000000: episode: 4003, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 61.118, mean reward: 0.611 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.878, 10.098], loss: 0.001367, mae: 0.040615, mean_q: 1.173492
 400386/1000000: episode: 4004, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.465, mean reward: 0.575 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.381, 10.256], loss: 0.001401, mae: 0.041056, mean_q: 1.173273
 400486/1000000: episode: 4005, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 56.898, mean reward: 0.569 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.459, 10.098], loss: 0.001462, mae: 0.041485, mean_q: 1.171612
 400586/1000000: episode: 4006, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 57.093, mean reward: 0.571 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.115], loss: 0.001352, mae: 0.039949, mean_q: 1.169125
 400686/1000000: episode: 4007, duration: 0.816s, episode steps: 100, steps per second: 122, episode reward: 63.086, mean reward: 0.631 [0.510, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.718, 10.395], loss: 0.001337, mae: 0.039622, mean_q: 1.169011
 400786/1000000: episode: 4008, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.736, mean reward: 0.587 [0.497, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.020, 10.238], loss: 0.001352, mae: 0.040111, mean_q: 1.172778
 400886/1000000: episode: 4009, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 61.848, mean reward: 0.618 [0.516, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.729, 10.098], loss: 0.001398, mae: 0.040295, mean_q: 1.168069
 400986/1000000: episode: 4010, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 59.710, mean reward: 0.597 [0.510, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.025, 10.394], loss: 0.001412, mae: 0.040447, mean_q: 1.171313
 401086/1000000: episode: 4011, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 56.849, mean reward: 0.568 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.862, 10.098], loss: 0.001370, mae: 0.039980, mean_q: 1.169739
 401186/1000000: episode: 4012, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 60.480, mean reward: 0.605 [0.507, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.246], loss: 0.001391, mae: 0.040618, mean_q: 1.168588
 401286/1000000: episode: 4013, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 58.051, mean reward: 0.581 [0.499, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.098], loss: 0.001355, mae: 0.040098, mean_q: 1.168791
 401386/1000000: episode: 4014, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 62.888, mean reward: 0.629 [0.499, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.703, 10.360], loss: 0.001448, mae: 0.040997, mean_q: 1.168774
 401486/1000000: episode: 4015, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.782, mean reward: 0.588 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.630, 10.098], loss: 0.001426, mae: 0.041122, mean_q: 1.172699
 401586/1000000: episode: 4016, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.357, mean reward: 0.604 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.358, 10.098], loss: 0.001577, mae: 0.043040, mean_q: 1.171097
 401686/1000000: episode: 4017, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.236, mean reward: 0.582 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.121], loss: 0.001378, mae: 0.039972, mean_q: 1.173949
 401786/1000000: episode: 4018, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.871, mean reward: 0.579 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.477, 10.098], loss: 0.001387, mae: 0.040585, mean_q: 1.170595
 401886/1000000: episode: 4019, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.746, mean reward: 0.577 [0.510, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.176, 10.319], loss: 0.001449, mae: 0.041079, mean_q: 1.172168
 401986/1000000: episode: 4020, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.811, mean reward: 0.588 [0.517, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.740, 10.098], loss: 0.001435, mae: 0.040548, mean_q: 1.172297
 402086/1000000: episode: 4021, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 59.155, mean reward: 0.592 [0.514, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.250, 10.098], loss: 0.001332, mae: 0.039780, mean_q: 1.169709
 402186/1000000: episode: 4022, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 61.685, mean reward: 0.617 [0.503, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.629, 10.279], loss: 0.001448, mae: 0.040576, mean_q: 1.169722
 402286/1000000: episode: 4023, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 62.583, mean reward: 0.626 [0.517, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.543, 10.098], loss: 0.001383, mae: 0.040390, mean_q: 1.173867
 402386/1000000: episode: 4024, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 58.778, mean reward: 0.588 [0.497, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.172, 10.098], loss: 0.001481, mae: 0.041700, mean_q: 1.171664
 402486/1000000: episode: 4025, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 58.057, mean reward: 0.581 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.133], loss: 0.001528, mae: 0.042618, mean_q: 1.170274
 402586/1000000: episode: 4026, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 57.395, mean reward: 0.574 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.321, 10.098], loss: 0.001397, mae: 0.040218, mean_q: 1.170694
 402686/1000000: episode: 4027, duration: 0.866s, episode steps: 100, steps per second: 116, episode reward: 61.804, mean reward: 0.618 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.685, 10.423], loss: 0.001434, mae: 0.041607, mean_q: 1.167573
 402786/1000000: episode: 4028, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 58.311, mean reward: 0.583 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.609, 10.112], loss: 0.001498, mae: 0.042224, mean_q: 1.169964
 402886/1000000: episode: 4029, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 58.951, mean reward: 0.590 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.248, 10.144], loss: 0.001446, mae: 0.041491, mean_q: 1.170426
 402986/1000000: episode: 4030, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 57.490, mean reward: 0.575 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.118, 10.098], loss: 0.001437, mae: 0.041136, mean_q: 1.171066
 403086/1000000: episode: 4031, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 57.106, mean reward: 0.571 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.973, 10.119], loss: 0.001476, mae: 0.042144, mean_q: 1.173808
 403186/1000000: episode: 4032, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 58.856, mean reward: 0.589 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.539, 10.098], loss: 0.001396, mae: 0.040831, mean_q: 1.168775
 403286/1000000: episode: 4033, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.027, mean reward: 0.590 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.810, 10.433], loss: 0.001399, mae: 0.040164, mean_q: 1.170833
 403386/1000000: episode: 4034, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.036, mean reward: 0.580 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.660, 10.149], loss: 0.001482, mae: 0.042449, mean_q: 1.171343
 403486/1000000: episode: 4035, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 59.214, mean reward: 0.592 [0.500, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.854, 10.098], loss: 0.001567, mae: 0.042937, mean_q: 1.170530
 403586/1000000: episode: 4036, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 61.770, mean reward: 0.618 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.433, 10.098], loss: 0.001538, mae: 0.042461, mean_q: 1.171031
 403686/1000000: episode: 4037, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 56.943, mean reward: 0.569 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.192, 10.238], loss: 0.001505, mae: 0.041876, mean_q: 1.172191
 403786/1000000: episode: 4038, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.225, mean reward: 0.582 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.619, 10.222], loss: 0.001498, mae: 0.042271, mean_q: 1.170174
 403886/1000000: episode: 4039, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.338, mean reward: 0.603 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.731, 10.098], loss: 0.001462, mae: 0.041773, mean_q: 1.167865
 403986/1000000: episode: 4040, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.161, mean reward: 0.592 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.713, 10.135], loss: 0.001418, mae: 0.040883, mean_q: 1.171390
 404086/1000000: episode: 4041, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.422, mean reward: 0.574 [0.497, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.534, 10.098], loss: 0.001459, mae: 0.041766, mean_q: 1.171670
 404186/1000000: episode: 4042, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 58.973, mean reward: 0.590 [0.513, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.294, 10.198], loss: 0.001410, mae: 0.041136, mean_q: 1.171663
 404286/1000000: episode: 4043, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.593, mean reward: 0.586 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.602, 10.098], loss: 0.001341, mae: 0.039820, mean_q: 1.165828
 404386/1000000: episode: 4044, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 57.377, mean reward: 0.574 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.707, 10.098], loss: 0.001521, mae: 0.041909, mean_q: 1.166674
 404486/1000000: episode: 4045, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 57.903, mean reward: 0.579 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.306, 10.098], loss: 0.001469, mae: 0.041898, mean_q: 1.169119
 404586/1000000: episode: 4046, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 59.690, mean reward: 0.597 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.785, 10.098], loss: 0.001485, mae: 0.041968, mean_q: 1.168891
 404686/1000000: episode: 4047, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.527, mean reward: 0.585 [0.515, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.066, 10.098], loss: 0.001494, mae: 0.042402, mean_q: 1.170125
 404786/1000000: episode: 4048, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 63.293, mean reward: 0.633 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.098], loss: 0.001481, mae: 0.041982, mean_q: 1.168454
 404886/1000000: episode: 4049, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.912, mean reward: 0.589 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.635, 10.126], loss: 0.001711, mae: 0.044623, mean_q: 1.170501
 404986/1000000: episode: 4050, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 60.674, mean reward: 0.607 [0.508, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.249, 10.098], loss: 0.001548, mae: 0.042756, mean_q: 1.171995
 405086/1000000: episode: 4051, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 57.191, mean reward: 0.572 [0.499, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.505, 10.213], loss: 0.001436, mae: 0.041221, mean_q: 1.166258
 405186/1000000: episode: 4052, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 57.918, mean reward: 0.579 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.252], loss: 0.001493, mae: 0.042255, mean_q: 1.169562
 405286/1000000: episode: 4053, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 60.385, mean reward: 0.604 [0.519, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.833, 10.266], loss: 0.001500, mae: 0.041968, mean_q: 1.168956
 405386/1000000: episode: 4054, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 59.580, mean reward: 0.596 [0.508, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.328, 10.098], loss: 0.001598, mae: 0.043523, mean_q: 1.170333
 405486/1000000: episode: 4055, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.248, mean reward: 0.582 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.133], loss: 0.001595, mae: 0.043446, mean_q: 1.169563
 405586/1000000: episode: 4056, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 60.135, mean reward: 0.601 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.985, 10.098], loss: 0.001543, mae: 0.042872, mean_q: 1.173054
 405686/1000000: episode: 4057, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 59.886, mean reward: 0.599 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.252, 10.098], loss: 0.001489, mae: 0.042008, mean_q: 1.169003
 405786/1000000: episode: 4058, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 58.493, mean reward: 0.585 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.734, 10.098], loss: 0.001442, mae: 0.041093, mean_q: 1.169592
 405886/1000000: episode: 4059, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 57.758, mean reward: 0.578 [0.513, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.341, 10.206], loss: 0.001599, mae: 0.043156, mean_q: 1.167358
 405986/1000000: episode: 4060, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 56.853, mean reward: 0.569 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.163], loss: 0.001550, mae: 0.042933, mean_q: 1.168413
 406086/1000000: episode: 4061, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.873, mean reward: 0.589 [0.502, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.099, 10.116], loss: 0.001698, mae: 0.044761, mean_q: 1.170911
 406186/1000000: episode: 4062, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 61.233, mean reward: 0.612 [0.525, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.130, 10.098], loss: 0.001483, mae: 0.041764, mean_q: 1.166377
 406286/1000000: episode: 4063, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 57.963, mean reward: 0.580 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.958, 10.098], loss: 0.001577, mae: 0.042749, mean_q: 1.169448
 406386/1000000: episode: 4064, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 57.126, mean reward: 0.571 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.658, 10.237], loss: 0.001549, mae: 0.042873, mean_q: 1.166006
 406486/1000000: episode: 4065, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 60.121, mean reward: 0.601 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.227, 10.098], loss: 0.001538, mae: 0.042732, mean_q: 1.170124
 406586/1000000: episode: 4066, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 59.242, mean reward: 0.592 [0.507, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.680, 10.098], loss: 0.001501, mae: 0.042092, mean_q: 1.171009
 406686/1000000: episode: 4067, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 60.940, mean reward: 0.609 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.523, 10.159], loss: 0.001623, mae: 0.044082, mean_q: 1.169634
 406786/1000000: episode: 4068, duration: 1.474s, episode steps: 100, steps per second: 68, episode reward: 56.984, mean reward: 0.570 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.340, 10.099], loss: 0.001518, mae: 0.042269, mean_q: 1.170127
 406886/1000000: episode: 4069, duration: 1.475s, episode steps: 100, steps per second: 68, episode reward: 61.469, mean reward: 0.615 [0.504, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.389], loss: 0.001563, mae: 0.042626, mean_q: 1.169883
 406986/1000000: episode: 4070, duration: 1.447s, episode steps: 100, steps per second: 69, episode reward: 60.395, mean reward: 0.604 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.491, 10.191], loss: 0.001696, mae: 0.044556, mean_q: 1.169702
 407086/1000000: episode: 4071, duration: 1.679s, episode steps: 100, steps per second: 60, episode reward: 59.860, mean reward: 0.599 [0.499, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.106, 10.247], loss: 0.001469, mae: 0.041705, mean_q: 1.168994
 407186/1000000: episode: 4072, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 60.227, mean reward: 0.602 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.299, 10.098], loss: 0.001529, mae: 0.042759, mean_q: 1.168589
 407286/1000000: episode: 4073, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.778, mean reward: 0.588 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.837, 10.098], loss: 0.001564, mae: 0.043098, mean_q: 1.164436
 407386/1000000: episode: 4074, duration: 1.545s, episode steps: 100, steps per second: 65, episode reward: 60.305, mean reward: 0.603 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.499, 10.139], loss: 0.001534, mae: 0.042979, mean_q: 1.169208
 407486/1000000: episode: 4075, duration: 1.405s, episode steps: 100, steps per second: 71, episode reward: 58.858, mean reward: 0.589 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.148, 10.098], loss: 0.001620, mae: 0.043527, mean_q: 1.173233
 407586/1000000: episode: 4076, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.651, mean reward: 0.587 [0.499, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.781, 10.174], loss: 0.001568, mae: 0.043005, mean_q: 1.169017
 407686/1000000: episode: 4077, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.239, mean reward: 0.582 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.318, 10.138], loss: 0.001523, mae: 0.042311, mean_q: 1.164279
 407786/1000000: episode: 4078, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.456, mean reward: 0.585 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.210, 10.146], loss: 0.001495, mae: 0.041742, mean_q: 1.167138
 407886/1000000: episode: 4079, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 58.006, mean reward: 0.580 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.928, 10.132], loss: 0.001506, mae: 0.042463, mean_q: 1.167507
 407986/1000000: episode: 4080, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.076, mean reward: 0.571 [0.504, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.154], loss: 0.001461, mae: 0.041247, mean_q: 1.169602
 408086/1000000: episode: 4081, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 59.196, mean reward: 0.592 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.602, 10.098], loss: 0.001472, mae: 0.041849, mean_q: 1.166067
 408186/1000000: episode: 4082, duration: 1.367s, episode steps: 100, steps per second: 73, episode reward: 58.047, mean reward: 0.580 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.164, 10.158], loss: 0.001507, mae: 0.042580, mean_q: 1.168399
 408286/1000000: episode: 4083, duration: 1.542s, episode steps: 100, steps per second: 65, episode reward: 58.153, mean reward: 0.582 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.098], loss: 0.001555, mae: 0.042401, mean_q: 1.167086
 408386/1000000: episode: 4084, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 57.091, mean reward: 0.571 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.992, 10.251], loss: 0.001524, mae: 0.042239, mean_q: 1.169056
 408486/1000000: episode: 4085, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 58.072, mean reward: 0.581 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.516, 10.098], loss: 0.001533, mae: 0.042515, mean_q: 1.165035
 408586/1000000: episode: 4086, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.858, mean reward: 0.579 [0.500, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.098], loss: 0.001471, mae: 0.041844, mean_q: 1.163746
 408686/1000000: episode: 4087, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.305, mean reward: 0.593 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.719, 10.098], loss: 0.001435, mae: 0.041379, mean_q: 1.161117
 408786/1000000: episode: 4088, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 57.076, mean reward: 0.571 [0.500, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.492, 10.098], loss: 0.001510, mae: 0.042092, mean_q: 1.167866
 408886/1000000: episode: 4089, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 61.168, mean reward: 0.612 [0.513, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.791, 10.098], loss: 0.001620, mae: 0.043347, mean_q: 1.166439
 408986/1000000: episode: 4090, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.526, mean reward: 0.585 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.354, 10.283], loss: 0.001548, mae: 0.042748, mean_q: 1.170116
 409086/1000000: episode: 4091, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 58.897, mean reward: 0.589 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.686, 10.098], loss: 0.001482, mae: 0.042200, mean_q: 1.165826
 409186/1000000: episode: 4092, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.138, mean reward: 0.581 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.761, 10.098], loss: 0.001458, mae: 0.041504, mean_q: 1.165739
 409286/1000000: episode: 4093, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.047, mean reward: 0.590 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.890, 10.176], loss: 0.001567, mae: 0.043066, mean_q: 1.167300
 409386/1000000: episode: 4094, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.434, mean reward: 0.594 [0.523, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.496, 10.098], loss: 0.001575, mae: 0.043390, mean_q: 1.168920
 409486/1000000: episode: 4095, duration: 0.810s, episode steps: 100, steps per second: 124, episode reward: 58.788, mean reward: 0.588 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.285, 10.310], loss: 0.001493, mae: 0.041704, mean_q: 1.167699
 409586/1000000: episode: 4096, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 58.511, mean reward: 0.585 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.813, 10.098], loss: 0.001575, mae: 0.043089, mean_q: 1.164255
 409686/1000000: episode: 4097, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 57.437, mean reward: 0.574 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.109], loss: 0.001558, mae: 0.043007, mean_q: 1.165568
 409786/1000000: episode: 4098, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.406, mean reward: 0.584 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.242], loss: 0.001497, mae: 0.041838, mean_q: 1.163338
 409886/1000000: episode: 4099, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 58.153, mean reward: 0.582 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.156], loss: 0.001421, mae: 0.041156, mean_q: 1.163497
 409986/1000000: episode: 4100, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 59.251, mean reward: 0.593 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.902, 10.098], loss: 0.001461, mae: 0.041260, mean_q: 1.164436
 410086/1000000: episode: 4101, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 57.856, mean reward: 0.579 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.159, 10.115], loss: 0.001507, mae: 0.042077, mean_q: 1.163448
 410186/1000000: episode: 4102, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 61.053, mean reward: 0.611 [0.520, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.949, 10.098], loss: 0.001388, mae: 0.040624, mean_q: 1.164270
 410286/1000000: episode: 4103, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 59.278, mean reward: 0.593 [0.512, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.601, 10.098], loss: 0.001530, mae: 0.042388, mean_q: 1.162414
 410386/1000000: episode: 4104, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.451, mean reward: 0.595 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.202], loss: 0.001492, mae: 0.042161, mean_q: 1.165646
 410486/1000000: episode: 4105, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 63.678, mean reward: 0.637 [0.503, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.968, 10.098], loss: 0.001434, mae: 0.041495, mean_q: 1.166108
 410586/1000000: episode: 4106, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 62.664, mean reward: 0.627 [0.503, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.624, 10.441], loss: 0.001427, mae: 0.040840, mean_q: 1.165947
 410686/1000000: episode: 4107, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 61.452, mean reward: 0.615 [0.498, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.595, 10.098], loss: 0.001459, mae: 0.041642, mean_q: 1.166222
 410786/1000000: episode: 4108, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 58.363, mean reward: 0.584 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.098], loss: 0.001483, mae: 0.042231, mean_q: 1.166663
 410886/1000000: episode: 4109, duration: 1.344s, episode steps: 100, steps per second: 74, episode reward: 60.048, mean reward: 0.600 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.676, 10.098], loss: 0.001461, mae: 0.041233, mean_q: 1.167038
 410986/1000000: episode: 4110, duration: 1.379s, episode steps: 100, steps per second: 73, episode reward: 62.340, mean reward: 0.623 [0.503, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.531, 10.328], loss: 0.001428, mae: 0.041178, mean_q: 1.165899
 411086/1000000: episode: 4111, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 56.930, mean reward: 0.569 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.152], loss: 0.001485, mae: 0.042315, mean_q: 1.172543
 411186/1000000: episode: 4112, duration: 1.487s, episode steps: 100, steps per second: 67, episode reward: 59.676, mean reward: 0.597 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.970, 10.098], loss: 0.001456, mae: 0.042013, mean_q: 1.167717
 411286/1000000: episode: 4113, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 57.640, mean reward: 0.576 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.335], loss: 0.001502, mae: 0.042578, mean_q: 1.169546
 411386/1000000: episode: 4114, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 60.460, mean reward: 0.605 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.101, 10.098], loss: 0.001458, mae: 0.041770, mean_q: 1.168570
 411486/1000000: episode: 4115, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 58.063, mean reward: 0.581 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.995, 10.098], loss: 0.001433, mae: 0.041584, mean_q: 1.166588
 411586/1000000: episode: 4116, duration: 1.290s, episode steps: 100, steps per second: 77, episode reward: 58.755, mean reward: 0.588 [0.498, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.229, 10.188], loss: 0.001391, mae: 0.040588, mean_q: 1.168770
 411686/1000000: episode: 4117, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 61.048, mean reward: 0.610 [0.511, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.791, 10.098], loss: 0.001482, mae: 0.042434, mean_q: 1.172624
 411786/1000000: episode: 4118, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 60.030, mean reward: 0.600 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.080, 10.098], loss: 0.001438, mae: 0.041276, mean_q: 1.168194
 411886/1000000: episode: 4119, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 57.200, mean reward: 0.572 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.482, 10.157], loss: 0.001488, mae: 0.041792, mean_q: 1.169069
 411986/1000000: episode: 4120, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 58.469, mean reward: 0.585 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.002, 10.233], loss: 0.001477, mae: 0.041615, mean_q: 1.170196
 412086/1000000: episode: 4121, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 60.909, mean reward: 0.609 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.098], loss: 0.001436, mae: 0.040994, mean_q: 1.169795
 412186/1000000: episode: 4122, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 57.062, mean reward: 0.571 [0.500, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.039, 10.098], loss: 0.001551, mae: 0.042913, mean_q: 1.167460
 412286/1000000: episode: 4123, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.873, mean reward: 0.579 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.001392, mae: 0.040370, mean_q: 1.167616
 412386/1000000: episode: 4124, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 57.994, mean reward: 0.580 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.183, 10.269], loss: 0.001431, mae: 0.041248, mean_q: 1.164681
 412486/1000000: episode: 4125, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 57.190, mean reward: 0.572 [0.497, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.098], loss: 0.001372, mae: 0.040303, mean_q: 1.165150
 412586/1000000: episode: 4126, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.889, mean reward: 0.589 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.590, 10.098], loss: 0.001387, mae: 0.040655, mean_q: 1.164720
 412686/1000000: episode: 4127, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 57.189, mean reward: 0.572 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.459, 10.391], loss: 0.001448, mae: 0.041751, mean_q: 1.166239
 412786/1000000: episode: 4128, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 59.534, mean reward: 0.595 [0.516, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.008, 10.113], loss: 0.001408, mae: 0.040446, mean_q: 1.167294
 412886/1000000: episode: 4129, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 58.701, mean reward: 0.587 [0.514, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.171], loss: 0.001337, mae: 0.040252, mean_q: 1.165745
 412986/1000000: episode: 4130, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.451, mean reward: 0.595 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.426, 10.098], loss: 0.001429, mae: 0.040958, mean_q: 1.165976
 413086/1000000: episode: 4131, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.227, mean reward: 0.582 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.532, 10.098], loss: 0.001413, mae: 0.041077, mean_q: 1.167984
 413186/1000000: episode: 4132, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.111, mean reward: 0.601 [0.509, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.312], loss: 0.001402, mae: 0.040406, mean_q: 1.165826
 413286/1000000: episode: 4133, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 58.378, mean reward: 0.584 [0.502, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.825, 10.098], loss: 0.001375, mae: 0.040285, mean_q: 1.166802
 413386/1000000: episode: 4134, duration: 1.311s, episode steps: 100, steps per second: 76, episode reward: 57.318, mean reward: 0.573 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.643, 10.193], loss: 0.001454, mae: 0.041606, mean_q: 1.168160
 413486/1000000: episode: 4135, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 58.415, mean reward: 0.584 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.473, 10.170], loss: 0.001414, mae: 0.040519, mean_q: 1.168239
 413586/1000000: episode: 4136, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.074, mean reward: 0.591 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.168, 10.268], loss: 0.001400, mae: 0.040882, mean_q: 1.164928
 413686/1000000: episode: 4137, duration: 1.288s, episode steps: 100, steps per second: 78, episode reward: 59.563, mean reward: 0.596 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.617, 10.098], loss: 0.001337, mae: 0.039887, mean_q: 1.166737
 413786/1000000: episode: 4138, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 58.716, mean reward: 0.587 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.691, 10.299], loss: 0.001451, mae: 0.040911, mean_q: 1.168537
 413886/1000000: episode: 4139, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.272, mean reward: 0.593 [0.513, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.613, 10.098], loss: 0.001422, mae: 0.041036, mean_q: 1.171653
 413986/1000000: episode: 4140, duration: 1.354s, episode steps: 100, steps per second: 74, episode reward: 57.455, mean reward: 0.575 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.301, 10.243], loss: 0.001473, mae: 0.041220, mean_q: 1.171278
 414086/1000000: episode: 4141, duration: 1.316s, episode steps: 100, steps per second: 76, episode reward: 62.420, mean reward: 0.624 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.011, 10.180], loss: 0.001336, mae: 0.039380, mean_q: 1.163433
 414186/1000000: episode: 4142, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 58.371, mean reward: 0.584 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.601, 10.098], loss: 0.001382, mae: 0.039918, mean_q: 1.166737
 414286/1000000: episode: 4143, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 56.901, mean reward: 0.569 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.153, 10.098], loss: 0.001470, mae: 0.041309, mean_q: 1.167679
 414386/1000000: episode: 4144, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.917, mean reward: 0.589 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.143, 10.190], loss: 0.001402, mae: 0.040672, mean_q: 1.168923
 414486/1000000: episode: 4145, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 59.582, mean reward: 0.596 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.378, 10.098], loss: 0.001428, mae: 0.041001, mean_q: 1.168469
 414586/1000000: episode: 4146, duration: 1.282s, episode steps: 100, steps per second: 78, episode reward: 57.944, mean reward: 0.579 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.158], loss: 0.001467, mae: 0.041417, mean_q: 1.167851
 414686/1000000: episode: 4147, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 58.831, mean reward: 0.588 [0.516, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.308], loss: 0.001339, mae: 0.040063, mean_q: 1.166882
 414786/1000000: episode: 4148, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.378, mean reward: 0.584 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.197, 10.098], loss: 0.001320, mae: 0.039576, mean_q: 1.167556
 414886/1000000: episode: 4149, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 57.522, mean reward: 0.575 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.296, 10.141], loss: 0.001382, mae: 0.039938, mean_q: 1.167513
 414986/1000000: episode: 4150, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.584, mean reward: 0.586 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.915, 10.114], loss: 0.001410, mae: 0.040570, mean_q: 1.168549
 415086/1000000: episode: 4151, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 59.022, mean reward: 0.590 [0.509, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.481, 10.198], loss: 0.001450, mae: 0.041295, mean_q: 1.171923
 415186/1000000: episode: 4152, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 61.288, mean reward: 0.613 [0.514, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.999, 10.253], loss: 0.001327, mae: 0.039429, mean_q: 1.168723
 415286/1000000: episode: 4153, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.600, mean reward: 0.586 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.681, 10.098], loss: 0.001485, mae: 0.042073, mean_q: 1.170941
 415386/1000000: episode: 4154, duration: 0.966s, episode steps: 100, steps per second: 103, episode reward: 59.265, mean reward: 0.593 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.091, 10.098], loss: 0.001371, mae: 0.039919, mean_q: 1.170021
 415486/1000000: episode: 4155, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 57.773, mean reward: 0.578 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.456, 10.284], loss: 0.001444, mae: 0.040944, mean_q: 1.169335
 415586/1000000: episode: 4156, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 62.074, mean reward: 0.621 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.805, 10.320], loss: 0.001367, mae: 0.040492, mean_q: 1.167378
 415686/1000000: episode: 4157, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 59.856, mean reward: 0.599 [0.513, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.098], loss: 0.001398, mae: 0.040411, mean_q: 1.168147
 415786/1000000: episode: 4158, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 60.009, mean reward: 0.600 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.865, 10.534], loss: 0.001419, mae: 0.040901, mean_q: 1.167464
 415886/1000000: episode: 4159, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 58.906, mean reward: 0.589 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.630, 10.098], loss: 0.001443, mae: 0.041262, mean_q: 1.166747
 415986/1000000: episode: 4160, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 64.667, mean reward: 0.647 [0.500, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.897, 10.630], loss: 0.001437, mae: 0.040992, mean_q: 1.163886
 416086/1000000: episode: 4161, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 60.436, mean reward: 0.604 [0.502, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.135, 10.098], loss: 0.001434, mae: 0.040989, mean_q: 1.169470
 416186/1000000: episode: 4162, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.657, mean reward: 0.587 [0.499, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.175], loss: 0.001385, mae: 0.040643, mean_q: 1.167906
 416286/1000000: episode: 4163, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 64.854, mean reward: 0.649 [0.504, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.737, 10.596], loss: 0.001485, mae: 0.041937, mean_q: 1.172814
 416386/1000000: episode: 4164, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.613, mean reward: 0.576 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.318, 10.363], loss: 0.001444, mae: 0.041468, mean_q: 1.169380
 416486/1000000: episode: 4165, duration: 0.807s, episode steps: 100, steps per second: 124, episode reward: 59.519, mean reward: 0.595 [0.501, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.232, 10.339], loss: 0.001357, mae: 0.040202, mean_q: 1.169051
 416586/1000000: episode: 4166, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.910, mean reward: 0.589 [0.514, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.972, 10.098], loss: 0.001527, mae: 0.042437, mean_q: 1.170886
 416686/1000000: episode: 4167, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 58.395, mean reward: 0.584 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.477, 10.145], loss: 0.001498, mae: 0.041612, mean_q: 1.170553
 416786/1000000: episode: 4168, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 56.952, mean reward: 0.570 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.768, 10.162], loss: 0.001444, mae: 0.041052, mean_q: 1.168401
 416886/1000000: episode: 4169, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 56.957, mean reward: 0.570 [0.508, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.221], loss: 0.001333, mae: 0.039419, mean_q: 1.169141
 416986/1000000: episode: 4170, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 60.335, mean reward: 0.603 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.102, 10.098], loss: 0.001370, mae: 0.040694, mean_q: 1.169172
 417086/1000000: episode: 4171, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 58.219, mean reward: 0.582 [0.500, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.656, 10.098], loss: 0.001439, mae: 0.041402, mean_q: 1.169465
 417186/1000000: episode: 4172, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 57.719, mean reward: 0.577 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.428, 10.098], loss: 0.001447, mae: 0.041214, mean_q: 1.166800
 417286/1000000: episode: 4173, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 62.300, mean reward: 0.623 [0.517, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.351, 10.298], loss: 0.001441, mae: 0.040983, mean_q: 1.166698
 417386/1000000: episode: 4174, duration: 1.443s, episode steps: 100, steps per second: 69, episode reward: 60.414, mean reward: 0.604 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.475, 10.098], loss: 0.001498, mae: 0.041955, mean_q: 1.172269
 417486/1000000: episode: 4175, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 56.941, mean reward: 0.569 [0.506, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.595, 10.098], loss: 0.001351, mae: 0.040135, mean_q: 1.169932
 417586/1000000: episode: 4176, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 58.891, mean reward: 0.589 [0.513, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.644, 10.158], loss: 0.001306, mae: 0.039752, mean_q: 1.167769
 417686/1000000: episode: 4177, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 58.790, mean reward: 0.588 [0.513, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.261, 10.178], loss: 0.001442, mae: 0.040918, mean_q: 1.173678
 417786/1000000: episode: 4178, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 56.371, mean reward: 0.564 [0.501, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.863, 10.098], loss: 0.001479, mae: 0.041621, mean_q: 1.171364
 417886/1000000: episode: 4179, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 59.660, mean reward: 0.597 [0.516, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.321], loss: 0.001537, mae: 0.042298, mean_q: 1.172272
 417986/1000000: episode: 4180, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 57.560, mean reward: 0.576 [0.502, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.885, 10.209], loss: 0.001391, mae: 0.040589, mean_q: 1.168890
 418086/1000000: episode: 4181, duration: 1.130s, episode steps: 100, steps per second: 88, episode reward: 61.977, mean reward: 0.620 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.222, 10.098], loss: 0.001502, mae: 0.042364, mean_q: 1.172203
 418186/1000000: episode: 4182, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.970, mean reward: 0.590 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.812, 10.098], loss: 0.001521, mae: 0.041770, mean_q: 1.170164
 418286/1000000: episode: 4183, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.171, mean reward: 0.592 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.304, 10.441], loss: 0.001470, mae: 0.041431, mean_q: 1.168556
 418386/1000000: episode: 4184, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.953, mean reward: 0.600 [0.516, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.172, 10.112], loss: 0.001431, mae: 0.041240, mean_q: 1.174306
 418486/1000000: episode: 4185, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.726, mean reward: 0.587 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.465, 10.225], loss: 0.001393, mae: 0.040323, mean_q: 1.171312
 418586/1000000: episode: 4186, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 59.152, mean reward: 0.592 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.739, 10.098], loss: 0.001363, mae: 0.039921, mean_q: 1.171254
 418686/1000000: episode: 4187, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 60.265, mean reward: 0.603 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.193], loss: 0.001472, mae: 0.041742, mean_q: 1.172365
 418786/1000000: episode: 4188, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: 58.621, mean reward: 0.586 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.485, 10.162], loss: 0.001499, mae: 0.041912, mean_q: 1.169886
 418886/1000000: episode: 4189, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.049, mean reward: 0.570 [0.510, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.098], loss: 0.001482, mae: 0.041229, mean_q: 1.172598
 418986/1000000: episode: 4190, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.034, mean reward: 0.590 [0.516, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.956, 10.098], loss: 0.001532, mae: 0.042256, mean_q: 1.174512
 419086/1000000: episode: 4191, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.823, mean reward: 0.588 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.667, 10.300], loss: 0.001488, mae: 0.041469, mean_q: 1.171576
 419186/1000000: episode: 4192, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 56.952, mean reward: 0.570 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.716, 10.183], loss: 0.001344, mae: 0.039571, mean_q: 1.171574
 419286/1000000: episode: 4193, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.917, mean reward: 0.579 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.393, 10.324], loss: 0.001453, mae: 0.041330, mean_q: 1.171922
 419386/1000000: episode: 4194, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 61.226, mean reward: 0.612 [0.524, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.200, 10.377], loss: 0.001334, mae: 0.039271, mean_q: 1.167798
 419486/1000000: episode: 4195, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 56.672, mean reward: 0.567 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.404, 10.185], loss: 0.001416, mae: 0.040764, mean_q: 1.170515
 419586/1000000: episode: 4196, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 61.680, mean reward: 0.617 [0.519, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.355, 10.098], loss: 0.001430, mae: 0.040494, mean_q: 1.171878
 419686/1000000: episode: 4197, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 58.382, mean reward: 0.584 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.425, 10.098], loss: 0.001494, mae: 0.042280, mean_q: 1.172383
 419786/1000000: episode: 4198, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.374, mean reward: 0.584 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.314], loss: 0.001487, mae: 0.041169, mean_q: 1.168024
 419886/1000000: episode: 4199, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.005, mean reward: 0.570 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.098], loss: 0.001491, mae: 0.041857, mean_q: 1.170354
 419986/1000000: episode: 4200, duration: 1.152s, episode steps: 100, steps per second: 87, episode reward: 64.568, mean reward: 0.646 [0.498, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.334, 10.242], loss: 0.001394, mae: 0.040321, mean_q: 1.169919
 420086/1000000: episode: 4201, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 59.017, mean reward: 0.590 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.457, 10.098], loss: 0.001400, mae: 0.040744, mean_q: 1.172698
 420186/1000000: episode: 4202, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.624, mean reward: 0.586 [0.509, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.125, 10.313], loss: 0.001479, mae: 0.041417, mean_q: 1.175221
 420227/1000000: episode: 4203, duration: 0.671s, episode steps: 41, steps per second: 61, episode reward: 25.189, mean reward: 0.614 [0.516, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 0.892 [-0.435, 4.984], loss: 0.001618, mae: 0.043008, mean_q: 1.177548
 420327/1000000: episode: 4204, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 57.067, mean reward: 0.571 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.098], loss: 0.001539, mae: 0.041720, mean_q: 1.177003
 420427/1000000: episode: 4205, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.741, mean reward: 0.587 [0.497, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.562, 10.215], loss: 0.001487, mae: 0.042069, mean_q: 1.173168
 420527/1000000: episode: 4206, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 59.358, mean reward: 0.594 [0.500, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.262, 10.249], loss: 0.001458, mae: 0.041012, mean_q: 1.170501
 420627/1000000: episode: 4207, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 59.470, mean reward: 0.595 [0.512, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.773, 10.098], loss: 0.001484, mae: 0.041288, mean_q: 1.174397
 420727/1000000: episode: 4208, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: 57.326, mean reward: 0.573 [0.499, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.255, 10.181], loss: 0.001458, mae: 0.040962, mean_q: 1.171808
 420827/1000000: episode: 4209, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.755, mean reward: 0.588 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.098], loss: 0.001437, mae: 0.040637, mean_q: 1.169644
 420927/1000000: episode: 4210, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 56.838, mean reward: 0.568 [0.501, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.351, 10.136], loss: 0.001579, mae: 0.042592, mean_q: 1.168710
 421027/1000000: episode: 4211, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.728, mean reward: 0.597 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.098], loss: 0.001558, mae: 0.042556, mean_q: 1.165933
 421127/1000000: episode: 4212, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.678, mean reward: 0.587 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.916, 10.223], loss: 0.001415, mae: 0.040498, mean_q: 1.165640
 421227/1000000: episode: 4213, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 64.552, mean reward: 0.646 [0.505, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.558, 10.511], loss: 0.001500, mae: 0.041909, mean_q: 1.166768
 421327/1000000: episode: 4214, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 58.294, mean reward: 0.583 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.887, 10.098], loss: 0.001563, mae: 0.041920, mean_q: 1.162902
 421427/1000000: episode: 4215, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 59.573, mean reward: 0.596 [0.515, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.323, 10.143], loss: 0.001501, mae: 0.041453, mean_q: 1.164772
 421527/1000000: episode: 4216, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 59.742, mean reward: 0.597 [0.521, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.439, 10.098], loss: 0.001443, mae: 0.041205, mean_q: 1.166195
 421627/1000000: episode: 4217, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 60.944, mean reward: 0.609 [0.510, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.394], loss: 0.001443, mae: 0.041115, mean_q: 1.166648
 421727/1000000: episode: 4218, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 58.240, mean reward: 0.582 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.328, 10.098], loss: 0.001521, mae: 0.042223, mean_q: 1.170044
 421827/1000000: episode: 4219, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.932, mean reward: 0.579 [0.497, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.863, 10.212], loss: 0.001471, mae: 0.040745, mean_q: 1.165247
 421927/1000000: episode: 4220, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.283, mean reward: 0.583 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.983, 10.120], loss: 0.001552, mae: 0.041801, mean_q: 1.166990
 422027/1000000: episode: 4221, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 56.574, mean reward: 0.566 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.566, 10.190], loss: 0.001474, mae: 0.041613, mean_q: 1.167859
 422127/1000000: episode: 4222, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 59.865, mean reward: 0.599 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.113, 10.098], loss: 0.001626, mae: 0.042545, mean_q: 1.169984
 422227/1000000: episode: 4223, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 57.060, mean reward: 0.571 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.820, 10.183], loss: 0.001546, mae: 0.042065, mean_q: 1.164200
 422327/1000000: episode: 4224, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.483, mean reward: 0.575 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.608, 10.164], loss: 0.001462, mae: 0.041409, mean_q: 1.165401
 422427/1000000: episode: 4225, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 56.447, mean reward: 0.564 [0.499, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.099], loss: 0.001573, mae: 0.042451, mean_q: 1.166041
 422527/1000000: episode: 4226, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.595, mean reward: 0.586 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.214, 10.098], loss: 0.001467, mae: 0.040986, mean_q: 1.160564
 422627/1000000: episode: 4227, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 58.266, mean reward: 0.583 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.567, 10.169], loss: 0.001516, mae: 0.041829, mean_q: 1.163236
 422727/1000000: episode: 4228, duration: 1.281s, episode steps: 100, steps per second: 78, episode reward: 58.778, mean reward: 0.588 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.642, 10.108], loss: 0.001436, mae: 0.040871, mean_q: 1.166832
 422827/1000000: episode: 4229, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.291, mean reward: 0.583 [0.502, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.769, 10.098], loss: 0.001509, mae: 0.042029, mean_q: 1.166139
 422927/1000000: episode: 4230, duration: 1.047s, episode steps: 100, steps per second: 95, episode reward: 59.991, mean reward: 0.600 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.521, 10.098], loss: 0.001494, mae: 0.041749, mean_q: 1.163294
 423027/1000000: episode: 4231, duration: 1.274s, episode steps: 100, steps per second: 79, episode reward: 57.979, mean reward: 0.580 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.347, 10.098], loss: 0.001581, mae: 0.042593, mean_q: 1.167546
 423127/1000000: episode: 4232, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.407, mean reward: 0.584 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.924, 10.217], loss: 0.001575, mae: 0.042898, mean_q: 1.165450
 423227/1000000: episode: 4233, duration: 1.357s, episode steps: 100, steps per second: 74, episode reward: 59.909, mean reward: 0.599 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.215, 10.098], loss: 0.001595, mae: 0.042084, mean_q: 1.165775
 423327/1000000: episode: 4234, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 60.761, mean reward: 0.608 [0.512, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.471, 10.098], loss: 0.001481, mae: 0.040865, mean_q: 1.161894
 423427/1000000: episode: 4235, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 56.752, mean reward: 0.568 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.724, 10.098], loss: 0.001579, mae: 0.042663, mean_q: 1.166404
 423527/1000000: episode: 4236, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 58.541, mean reward: 0.585 [0.508, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.100, 10.221], loss: 0.001528, mae: 0.042381, mean_q: 1.164601
 423627/1000000: episode: 4237, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 58.554, mean reward: 0.586 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.397, 10.098], loss: 0.001524, mae: 0.041916, mean_q: 1.161909
 423727/1000000: episode: 4238, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 56.893, mean reward: 0.569 [0.502, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.353, 10.098], loss: 0.001573, mae: 0.042450, mean_q: 1.163704
 423827/1000000: episode: 4239, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 61.632, mean reward: 0.616 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.039, 10.098], loss: 0.001513, mae: 0.042303, mean_q: 1.167852
 423927/1000000: episode: 4240, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.876, mean reward: 0.589 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.166, 10.098], loss: 0.001498, mae: 0.042060, mean_q: 1.164132
 424027/1000000: episode: 4241, duration: 1.183s, episode steps: 100, steps per second: 84, episode reward: 61.884, mean reward: 0.619 [0.521, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.938, 10.098], loss: 0.001505, mae: 0.041831, mean_q: 1.166491
 424127/1000000: episode: 4242, duration: 1.368s, episode steps: 100, steps per second: 73, episode reward: 59.362, mean reward: 0.594 [0.505, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.846, 10.098], loss: 0.001647, mae: 0.043333, mean_q: 1.162292
 424227/1000000: episode: 4243, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 61.749, mean reward: 0.617 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.921, 10.344], loss: 0.001649, mae: 0.043028, mean_q: 1.166634
 424327/1000000: episode: 4244, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 59.594, mean reward: 0.596 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.098], loss: 0.001608, mae: 0.042520, mean_q: 1.163529
 424427/1000000: episode: 4245, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 57.504, mean reward: 0.575 [0.497, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.598, 10.098], loss: 0.001601, mae: 0.043056, mean_q: 1.169056
 424527/1000000: episode: 4246, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 60.295, mean reward: 0.603 [0.516, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.335, 10.098], loss: 0.001601, mae: 0.042778, mean_q: 1.169599
 424627/1000000: episode: 4247, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.824, mean reward: 0.578 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.067, 10.319], loss: 0.001600, mae: 0.042967, mean_q: 1.165564
 424727/1000000: episode: 4248, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 57.731, mean reward: 0.577 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.734, 10.117], loss: 0.001592, mae: 0.043347, mean_q: 1.167488
 424827/1000000: episode: 4249, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 57.219, mean reward: 0.572 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.505, 10.214], loss: 0.001586, mae: 0.042975, mean_q: 1.166939
 424927/1000000: episode: 4250, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.767, mean reward: 0.598 [0.508, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.510, 10.255], loss: 0.001490, mae: 0.041184, mean_q: 1.164466
 425027/1000000: episode: 4251, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 64.483, mean reward: 0.645 [0.501, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.445, 10.585], loss: 0.001480, mae: 0.041311, mean_q: 1.162705
 425127/1000000: episode: 4252, duration: 1.415s, episode steps: 100, steps per second: 71, episode reward: 59.012, mean reward: 0.590 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.972, 10.098], loss: 0.001573, mae: 0.042692, mean_q: 1.165377
 425227/1000000: episode: 4253, duration: 1.324s, episode steps: 100, steps per second: 76, episode reward: 60.138, mean reward: 0.601 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.896, 10.098], loss: 0.001524, mae: 0.041671, mean_q: 1.167692
 425327/1000000: episode: 4254, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 57.013, mean reward: 0.570 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.654, 10.098], loss: 0.001514, mae: 0.041853, mean_q: 1.169960
 425427/1000000: episode: 4255, duration: 1.321s, episode steps: 100, steps per second: 76, episode reward: 60.851, mean reward: 0.609 [0.509, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.111, 10.271], loss: 0.001550, mae: 0.041605, mean_q: 1.169781
 425527/1000000: episode: 4256, duration: 1.328s, episode steps: 100, steps per second: 75, episode reward: 57.841, mean reward: 0.578 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.024, 10.098], loss: 0.001597, mae: 0.042521, mean_q: 1.169822
 425627/1000000: episode: 4257, duration: 1.340s, episode steps: 100, steps per second: 75, episode reward: 58.684, mean reward: 0.587 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.752, 10.098], loss: 0.001547, mae: 0.041948, mean_q: 1.166287
 425727/1000000: episode: 4258, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: 59.270, mean reward: 0.593 [0.511, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.397, 10.098], loss: 0.001555, mae: 0.042063, mean_q: 1.166660
 425827/1000000: episode: 4259, duration: 1.335s, episode steps: 100, steps per second: 75, episode reward: 58.486, mean reward: 0.585 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.782, 10.150], loss: 0.001466, mae: 0.041620, mean_q: 1.167658
 425927/1000000: episode: 4260, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 56.875, mean reward: 0.569 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.640, 10.112], loss: 0.001535, mae: 0.042342, mean_q: 1.165932
 426027/1000000: episode: 4261, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 59.673, mean reward: 0.597 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.290, 10.132], loss: 0.001506, mae: 0.041756, mean_q: 1.166537
 426127/1000000: episode: 4262, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 55.880, mean reward: 0.559 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.189, 10.125], loss: 0.001578, mae: 0.042256, mean_q: 1.165467
 426227/1000000: episode: 4263, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 59.951, mean reward: 0.600 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.578, 10.171], loss: 0.001656, mae: 0.043782, mean_q: 1.166057
 426327/1000000: episode: 4264, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.666, mean reward: 0.587 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.085, 10.286], loss: 0.001507, mae: 0.041954, mean_q: 1.164385
 426427/1000000: episode: 4265, duration: 0.791s, episode steps: 100, steps per second: 127, episode reward: 58.208, mean reward: 0.582 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.098], loss: 0.001518, mae: 0.041640, mean_q: 1.165621
 426527/1000000: episode: 4266, duration: 0.820s, episode steps: 100, steps per second: 122, episode reward: 57.401, mean reward: 0.574 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.670, 10.118], loss: 0.001604, mae: 0.043120, mean_q: 1.162579
 426627/1000000: episode: 4267, duration: 0.823s, episode steps: 100, steps per second: 121, episode reward: 56.480, mean reward: 0.565 [0.506, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.245, 10.098], loss: 0.001493, mae: 0.042640, mean_q: 1.163119
 426727/1000000: episode: 4268, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.820, mean reward: 0.578 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.098], loss: 0.001522, mae: 0.041934, mean_q: 1.161417
 426827/1000000: episode: 4269, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 60.211, mean reward: 0.602 [0.518, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.668, 10.098], loss: 0.001542, mae: 0.042482, mean_q: 1.162996
 426927/1000000: episode: 4270, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.208, mean reward: 0.592 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.736, 10.457], loss: 0.001604, mae: 0.043018, mean_q: 1.162557
 427027/1000000: episode: 4271, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 58.344, mean reward: 0.583 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.853, 10.098], loss: 0.001573, mae: 0.041996, mean_q: 1.164904
 427127/1000000: episode: 4272, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 60.259, mean reward: 0.603 [0.512, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.880, 10.098], loss: 0.001515, mae: 0.041693, mean_q: 1.164998
 427227/1000000: episode: 4273, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 58.446, mean reward: 0.584 [0.510, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.017, 10.098], loss: 0.001530, mae: 0.041699, mean_q: 1.164686
 427327/1000000: episode: 4274, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 60.922, mean reward: 0.609 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.396, 10.435], loss: 0.001637, mae: 0.042967, mean_q: 1.166614
 427427/1000000: episode: 4275, duration: 1.340s, episode steps: 100, steps per second: 75, episode reward: 56.494, mean reward: 0.565 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.002, 10.113], loss: 0.001555, mae: 0.042082, mean_q: 1.167633
 427527/1000000: episode: 4276, duration: 1.342s, episode steps: 100, steps per second: 75, episode reward: 59.108, mean reward: 0.591 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.867, 10.098], loss: 0.001576, mae: 0.042143, mean_q: 1.161949
 427627/1000000: episode: 4277, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 59.367, mean reward: 0.594 [0.511, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.138, 10.259], loss: 0.001508, mae: 0.041381, mean_q: 1.162899
 427727/1000000: episode: 4278, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 59.107, mean reward: 0.591 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.767, 10.098], loss: 0.001543, mae: 0.042248, mean_q: 1.167462
 427827/1000000: episode: 4279, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.766, mean reward: 0.598 [0.508, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.586, 10.098], loss: 0.001530, mae: 0.041373, mean_q: 1.168026
 427927/1000000: episode: 4280, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 57.984, mean reward: 0.580 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.349, 10.148], loss: 0.001600, mae: 0.042197, mean_q: 1.168166
 428027/1000000: episode: 4281, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 62.692, mean reward: 0.627 [0.514, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.427, 10.279], loss: 0.001556, mae: 0.043006, mean_q: 1.167112
 428127/1000000: episode: 4282, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 58.795, mean reward: 0.588 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.022, 10.098], loss: 0.001541, mae: 0.042287, mean_q: 1.170314
 428227/1000000: episode: 4283, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 60.082, mean reward: 0.601 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.206], loss: 0.001646, mae: 0.043672, mean_q: 1.169605
 428327/1000000: episode: 4284, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 58.488, mean reward: 0.585 [0.502, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.397, 10.098], loss: 0.001589, mae: 0.043059, mean_q: 1.171176
 428427/1000000: episode: 4285, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 59.575, mean reward: 0.596 [0.514, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.264, 10.342], loss: 0.001560, mae: 0.042798, mean_q: 1.169708
 428527/1000000: episode: 4286, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 58.444, mean reward: 0.584 [0.515, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: 0.001526, mae: 0.042394, mean_q: 1.171045
 428627/1000000: episode: 4287, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 59.332, mean reward: 0.593 [0.518, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.330, 10.098], loss: 0.001627, mae: 0.042975, mean_q: 1.170570
 428727/1000000: episode: 4288, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 59.820, mean reward: 0.598 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.687, 10.098], loss: 0.001562, mae: 0.042612, mean_q: 1.166430
 428827/1000000: episode: 4289, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 57.723, mean reward: 0.577 [0.498, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.497, 10.098], loss: 0.001513, mae: 0.042088, mean_q: 1.167934
 428927/1000000: episode: 4290, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.660, mean reward: 0.587 [0.514, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.487, 10.108], loss: 0.001434, mae: 0.040783, mean_q: 1.166062
 429027/1000000: episode: 4291, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.321, mean reward: 0.583 [0.518, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.252], loss: 0.001434, mae: 0.040814, mean_q: 1.163653
 429127/1000000: episode: 4292, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.855, mean reward: 0.579 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.116, 10.375], loss: 0.001556, mae: 0.042613, mean_q: 1.167779
 429227/1000000: episode: 4293, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.808, mean reward: 0.598 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.628, 10.109], loss: 0.001573, mae: 0.042168, mean_q: 1.165323
 429327/1000000: episode: 4294, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 57.734, mean reward: 0.577 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.909, 10.098], loss: 0.001517, mae: 0.041342, mean_q: 1.163762
 429427/1000000: episode: 4295, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.244, mean reward: 0.572 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.380, 10.098], loss: 0.001549, mae: 0.042095, mean_q: 1.164048
 429527/1000000: episode: 4296, duration: 0.808s, episode steps: 100, steps per second: 124, episode reward: 57.896, mean reward: 0.579 [0.498, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.218], loss: 0.001496, mae: 0.041790, mean_q: 1.167589
 429627/1000000: episode: 4297, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 58.488, mean reward: 0.585 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.083, 10.098], loss: 0.001569, mae: 0.042727, mean_q: 1.162901
 429727/1000000: episode: 4298, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 60.275, mean reward: 0.603 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.273, 10.481], loss: 0.001437, mae: 0.040583, mean_q: 1.164710
 429827/1000000: episode: 4299, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 57.913, mean reward: 0.579 [0.501, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.403, 10.098], loss: 0.001450, mae: 0.041301, mean_q: 1.162195
 429927/1000000: episode: 4300, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 60.027, mean reward: 0.600 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.098], loss: 0.001430, mae: 0.040895, mean_q: 1.163150
 430027/1000000: episode: 4301, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 60.156, mean reward: 0.602 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.280, 10.239], loss: 0.001472, mae: 0.041849, mean_q: 1.159788
 430127/1000000: episode: 4302, duration: 0.810s, episode steps: 100, steps per second: 123, episode reward: 60.224, mean reward: 0.602 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.134, 10.186], loss: 0.001522, mae: 0.042117, mean_q: 1.163250
 430227/1000000: episode: 4303, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 57.792, mean reward: 0.578 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.252, 10.098], loss: 0.001473, mae: 0.041357, mean_q: 1.164149
 430327/1000000: episode: 4304, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 58.773, mean reward: 0.588 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.452, 10.098], loss: 0.001554, mae: 0.042763, mean_q: 1.165697
 430427/1000000: episode: 4305, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 57.998, mean reward: 0.580 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.180], loss: 0.001633, mae: 0.043891, mean_q: 1.163168
 430527/1000000: episode: 4306, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 58.726, mean reward: 0.587 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.867, 10.098], loss: 0.001503, mae: 0.042349, mean_q: 1.163727
 430627/1000000: episode: 4307, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 60.691, mean reward: 0.607 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.677, 10.098], loss: 0.001539, mae: 0.042661, mean_q: 1.161484
 430727/1000000: episode: 4308, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 60.608, mean reward: 0.606 [0.513, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.293, 10.149], loss: 0.001364, mae: 0.040234, mean_q: 1.164543
 430827/1000000: episode: 4309, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 58.734, mean reward: 0.587 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.098], loss: 0.001562, mae: 0.042694, mean_q: 1.166877
 430927/1000000: episode: 4310, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.039, mean reward: 0.570 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.707, 10.241], loss: 0.001591, mae: 0.043052, mean_q: 1.168204
 431027/1000000: episode: 4311, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 58.028, mean reward: 0.580 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.846, 10.218], loss: 0.001531, mae: 0.042027, mean_q: 1.168484
 431127/1000000: episode: 4312, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.582, mean reward: 0.576 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.172], loss: 0.001459, mae: 0.041504, mean_q: 1.166075
 431227/1000000: episode: 4313, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 58.623, mean reward: 0.586 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.172], loss: 0.001546, mae: 0.042399, mean_q: 1.164528
 431327/1000000: episode: 4314, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 58.253, mean reward: 0.583 [0.516, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.098, 10.098], loss: 0.001540, mae: 0.042571, mean_q: 1.165498
 431427/1000000: episode: 4315, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.799, mean reward: 0.588 [0.514, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.098], loss: 0.001457, mae: 0.041867, mean_q: 1.163591
 431527/1000000: episode: 4316, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 59.423, mean reward: 0.594 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.665, 10.098], loss: 0.001572, mae: 0.042596, mean_q: 1.167957
 431627/1000000: episode: 4317, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 61.803, mean reward: 0.618 [0.525, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.655, 10.445], loss: 0.001544, mae: 0.042914, mean_q: 1.164960
 431727/1000000: episode: 4318, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.799, mean reward: 0.588 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.739, 10.098], loss: 0.001571, mae: 0.042011, mean_q: 1.164975
 431827/1000000: episode: 4319, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 60.759, mean reward: 0.608 [0.520, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.432, 10.098], loss: 0.001452, mae: 0.041924, mean_q: 1.168590
 431927/1000000: episode: 4320, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.310, mean reward: 0.583 [0.497, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.245, 10.144], loss: 0.001632, mae: 0.043506, mean_q: 1.174435
 432027/1000000: episode: 4321, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.519, mean reward: 0.585 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.284, 10.216], loss: 0.001544, mae: 0.042565, mean_q: 1.172454
 432127/1000000: episode: 4322, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.468, mean reward: 0.595 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.332, 10.098], loss: 0.001468, mae: 0.041745, mean_q: 1.169846
 432227/1000000: episode: 4323, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 59.171, mean reward: 0.592 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.054, 10.098], loss: 0.001425, mae: 0.041086, mean_q: 1.165207
 432327/1000000: episode: 4324, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.947, mean reward: 0.589 [0.508, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.893, 10.109], loss: 0.001429, mae: 0.040968, mean_q: 1.164720
 432427/1000000: episode: 4325, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 59.916, mean reward: 0.599 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.010, 10.271], loss: 0.001447, mae: 0.041220, mean_q: 1.169530
 432527/1000000: episode: 4326, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 58.494, mean reward: 0.585 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.748, 10.138], loss: 0.001489, mae: 0.042088, mean_q: 1.169118
 432627/1000000: episode: 4327, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 60.327, mean reward: 0.603 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.625, 10.098], loss: 0.001466, mae: 0.041744, mean_q: 1.169497
 432727/1000000: episode: 4328, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 59.225, mean reward: 0.592 [0.510, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.502, 10.104], loss: 0.001397, mae: 0.040453, mean_q: 1.165743
 432827/1000000: episode: 4329, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 57.072, mean reward: 0.571 [0.498, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.582, 10.139], loss: 0.001492, mae: 0.042332, mean_q: 1.165159
 432927/1000000: episode: 4330, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.168, mean reward: 0.582 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.448, 10.098], loss: 0.001432, mae: 0.041224, mean_q: 1.162664
 433027/1000000: episode: 4331, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 60.252, mean reward: 0.603 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.543, 10.363], loss: 0.001458, mae: 0.041161, mean_q: 1.165031
 433127/1000000: episode: 4332, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 59.557, mean reward: 0.596 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.857, 10.098], loss: 0.001416, mae: 0.040904, mean_q: 1.164172
 433227/1000000: episode: 4333, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 60.035, mean reward: 0.600 [0.500, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.728, 10.186], loss: 0.001493, mae: 0.041849, mean_q: 1.165270
 433327/1000000: episode: 4334, duration: 1.321s, episode steps: 100, steps per second: 76, episode reward: 58.890, mean reward: 0.589 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.277, 10.098], loss: 0.001448, mae: 0.041250, mean_q: 1.165802
 433427/1000000: episode: 4335, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 59.595, mean reward: 0.596 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.378, 10.105], loss: 0.001403, mae: 0.041144, mean_q: 1.165521
 433527/1000000: episode: 4336, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 58.482, mean reward: 0.585 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.684, 10.098], loss: 0.001404, mae: 0.040733, mean_q: 1.169165
 433627/1000000: episode: 4337, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 60.004, mean reward: 0.600 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.543, 10.217], loss: 0.001472, mae: 0.041673, mean_q: 1.170382
 433727/1000000: episode: 4338, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 64.443, mean reward: 0.644 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.098], loss: 0.001429, mae: 0.041226, mean_q: 1.165917
 433827/1000000: episode: 4339, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 59.029, mean reward: 0.590 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.998, 10.098], loss: 0.001428, mae: 0.041546, mean_q: 1.170277
 433927/1000000: episode: 4340, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.383, mean reward: 0.574 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.723, 10.098], loss: 0.001446, mae: 0.041786, mean_q: 1.167900
 434027/1000000: episode: 4341, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 58.196, mean reward: 0.582 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.948, 10.098], loss: 0.001474, mae: 0.041992, mean_q: 1.166716
 434127/1000000: episode: 4342, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 61.079, mean reward: 0.611 [0.516, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.567, 10.445], loss: 0.001375, mae: 0.041166, mean_q: 1.168929
 434227/1000000: episode: 4343, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 61.296, mean reward: 0.613 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.782, 10.098], loss: 0.001352, mae: 0.040500, mean_q: 1.173434
 434327/1000000: episode: 4344, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.258, mean reward: 0.583 [0.500, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.037, 10.114], loss: 0.001478, mae: 0.042575, mean_q: 1.172023
 434427/1000000: episode: 4345, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.725, mean reward: 0.587 [0.497, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.949, 10.209], loss: 0.001466, mae: 0.041906, mean_q: 1.173038
 434527/1000000: episode: 4346, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 59.411, mean reward: 0.594 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.459, 10.161], loss: 0.001371, mae: 0.040412, mean_q: 1.172010
 434627/1000000: episode: 4347, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 59.273, mean reward: 0.593 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.380, 10.098], loss: 0.001485, mae: 0.042207, mean_q: 1.170483
 434727/1000000: episode: 4348, duration: 1.588s, episode steps: 100, steps per second: 63, episode reward: 57.385, mean reward: 0.574 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.994, 10.296], loss: 0.001558, mae: 0.043179, mean_q: 1.175248
 434827/1000000: episode: 4349, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 57.069, mean reward: 0.571 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.773, 10.206], loss: 0.001430, mae: 0.041129, mean_q: 1.174584
 434927/1000000: episode: 4350, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 60.405, mean reward: 0.604 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.438, 10.268], loss: 0.001505, mae: 0.042117, mean_q: 1.174370
 435027/1000000: episode: 4351, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.136, mean reward: 0.581 [0.509, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.795, 10.220], loss: 0.001408, mae: 0.040370, mean_q: 1.169082
 435127/1000000: episode: 4352, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.263, mean reward: 0.593 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.238], loss: 0.001509, mae: 0.042658, mean_q: 1.172710
 435227/1000000: episode: 4353, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 57.808, mean reward: 0.578 [0.501, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.411, 10.098], loss: 0.001429, mae: 0.041400, mean_q: 1.166746
 435327/1000000: episode: 4354, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 60.017, mean reward: 0.600 [0.501, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.098], loss: 0.001401, mae: 0.040493, mean_q: 1.171186
 435427/1000000: episode: 4355, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.159, mean reward: 0.582 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.664, 10.354], loss: 0.001454, mae: 0.041273, mean_q: 1.173334
 435527/1000000: episode: 4356, duration: 0.922s, episode steps: 100, steps per second: 109, episode reward: 61.328, mean reward: 0.613 [0.507, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.272], loss: 0.001388, mae: 0.040547, mean_q: 1.170458
 435627/1000000: episode: 4357, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 61.858, mean reward: 0.619 [0.516, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.364, 10.242], loss: 0.001446, mae: 0.040717, mean_q: 1.168959
 435727/1000000: episode: 4358, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.790, mean reward: 0.578 [0.502, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.876, 10.098], loss: 0.001576, mae: 0.043075, mean_q: 1.173210
 435827/1000000: episode: 4359, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 62.891, mean reward: 0.629 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.098], loss: 0.001550, mae: 0.043083, mean_q: 1.175587
 435927/1000000: episode: 4360, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 58.361, mean reward: 0.584 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.151], loss: 0.001514, mae: 0.042109, mean_q: 1.175307
 436027/1000000: episode: 4361, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 59.754, mean reward: 0.598 [0.514, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.746, 10.198], loss: 0.001465, mae: 0.041631, mean_q: 1.174107
 436127/1000000: episode: 4362, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 59.211, mean reward: 0.592 [0.515, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.922, 10.208], loss: 0.001470, mae: 0.042074, mean_q: 1.174529
 436227/1000000: episode: 4363, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 56.887, mean reward: 0.569 [0.498, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.603, 10.098], loss: 0.001483, mae: 0.041373, mean_q: 1.173104
 436327/1000000: episode: 4364, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.356, mean reward: 0.574 [0.500, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.673, 10.129], loss: 0.001556, mae: 0.042514, mean_q: 1.172329
 436427/1000000: episode: 4365, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 59.139, mean reward: 0.591 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.002, 10.098], loss: 0.001417, mae: 0.040283, mean_q: 1.171264
 436527/1000000: episode: 4366, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 58.809, mean reward: 0.588 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.707, 10.098], loss: 0.001504, mae: 0.041824, mean_q: 1.173110
 436627/1000000: episode: 4367, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 57.992, mean reward: 0.580 [0.515, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.039, 10.098], loss: 0.001488, mae: 0.041747, mean_q: 1.173799
 436727/1000000: episode: 4368, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.431, mean reward: 0.594 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.967, 10.263], loss: 0.001538, mae: 0.042271, mean_q: 1.170522
 436827/1000000: episode: 4369, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.839, mean reward: 0.588 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.597, 10.098], loss: 0.001427, mae: 0.041497, mean_q: 1.170741
 436927/1000000: episode: 4370, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 56.780, mean reward: 0.568 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.628, 10.098], loss: 0.001476, mae: 0.041261, mean_q: 1.169256
 437027/1000000: episode: 4371, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.262, mean reward: 0.573 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.612, 10.267], loss: 0.001450, mae: 0.041030, mean_q: 1.170441
 437127/1000000: episode: 4372, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 56.811, mean reward: 0.568 [0.509, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.255, 10.137], loss: 0.001536, mae: 0.042018, mean_q: 1.170015
 437227/1000000: episode: 4373, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 59.395, mean reward: 0.594 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.861, 10.359], loss: 0.001442, mae: 0.041403, mean_q: 1.168427
 437327/1000000: episode: 4374, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 59.226, mean reward: 0.592 [0.508, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.996, 10.098], loss: 0.001512, mae: 0.041802, mean_q: 1.170223
 437427/1000000: episode: 4375, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 59.221, mean reward: 0.592 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.325], loss: 0.001484, mae: 0.041630, mean_q: 1.167020
 437527/1000000: episode: 4376, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 58.879, mean reward: 0.589 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.769, 10.375], loss: 0.001544, mae: 0.042385, mean_q: 1.170839
 437627/1000000: episode: 4377, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 57.870, mean reward: 0.579 [0.511, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.933, 10.098], loss: 0.001501, mae: 0.041823, mean_q: 1.171606
 437727/1000000: episode: 4378, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.093, mean reward: 0.591 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.815, 10.188], loss: 0.001525, mae: 0.042466, mean_q: 1.171181
 437827/1000000: episode: 4379, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.872, mean reward: 0.589 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.162, 10.098], loss: 0.001610, mae: 0.042768, mean_q: 1.171993
 437927/1000000: episode: 4380, duration: 1.611s, episode steps: 100, steps per second: 62, episode reward: 59.495, mean reward: 0.595 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.034, 10.236], loss: 0.001462, mae: 0.041754, mean_q: 1.171318
 438027/1000000: episode: 4381, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 59.761, mean reward: 0.598 [0.510, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.217, 10.203], loss: 0.001607, mae: 0.042765, mean_q: 1.171680
 438127/1000000: episode: 4382, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 60.147, mean reward: 0.601 [0.514, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.098], loss: 0.001618, mae: 0.043459, mean_q: 1.175023
 438227/1000000: episode: 4383, duration: 1.585s, episode steps: 100, steps per second: 63, episode reward: 59.519, mean reward: 0.595 [0.510, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.145, 10.124], loss: 0.001472, mae: 0.041964, mean_q: 1.171637
 438327/1000000: episode: 4384, duration: 1.511s, episode steps: 100, steps per second: 66, episode reward: 57.713, mean reward: 0.577 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.388, 10.135], loss: 0.001472, mae: 0.041685, mean_q: 1.168820
 438427/1000000: episode: 4385, duration: 1.286s, episode steps: 100, steps per second: 78, episode reward: 60.081, mean reward: 0.601 [0.510, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.629, 10.159], loss: 0.001311, mae: 0.039316, mean_q: 1.165227
 438527/1000000: episode: 4386, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.515, mean reward: 0.585 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.742, 10.100], loss: 0.001479, mae: 0.041380, mean_q: 1.168872
 438627/1000000: episode: 4387, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 57.580, mean reward: 0.576 [0.501, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.441, 10.209], loss: 0.001383, mae: 0.040547, mean_q: 1.169441
 438727/1000000: episode: 4388, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 63.219, mean reward: 0.632 [0.526, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.938, 10.098], loss: 0.001556, mae: 0.042645, mean_q: 1.170352
 438827/1000000: episode: 4389, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.597, mean reward: 0.596 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.556, 10.098], loss: 0.001471, mae: 0.041181, mean_q: 1.170108
 438927/1000000: episode: 4390, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 60.143, mean reward: 0.601 [0.503, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.804, 10.187], loss: 0.001442, mae: 0.041406, mean_q: 1.169838
 439027/1000000: episode: 4391, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 60.359, mean reward: 0.604 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.823, 10.361], loss: 0.001528, mae: 0.042571, mean_q: 1.173459
 439127/1000000: episode: 4392, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.765, mean reward: 0.588 [0.507, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.396, 10.127], loss: 0.001560, mae: 0.042340, mean_q: 1.171137
 439227/1000000: episode: 4393, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 59.899, mean reward: 0.599 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.866, 10.098], loss: 0.001557, mae: 0.042233, mean_q: 1.169367
 439327/1000000: episode: 4394, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.746, mean reward: 0.597 [0.513, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.987, 10.294], loss: 0.001488, mae: 0.041766, mean_q: 1.170532
 439427/1000000: episode: 4395, duration: 1.370s, episode steps: 100, steps per second: 73, episode reward: 61.961, mean reward: 0.620 [0.524, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.276, 10.479], loss: 0.001505, mae: 0.041220, mean_q: 1.170089
 439527/1000000: episode: 4396, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 57.171, mean reward: 0.572 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.907, 10.236], loss: 0.001490, mae: 0.041494, mean_q: 1.169273
 439627/1000000: episode: 4397, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 55.709, mean reward: 0.557 [0.499, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.098], loss: 0.001502, mae: 0.041424, mean_q: 1.169907
 439727/1000000: episode: 4398, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.516, mean reward: 0.585 [0.508, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.566, 10.098], loss: 0.001463, mae: 0.041091, mean_q: 1.168014
 439827/1000000: episode: 4399, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 58.675, mean reward: 0.587 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.036, 10.098], loss: 0.001512, mae: 0.041871, mean_q: 1.170654
 439927/1000000: episode: 4400, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 60.126, mean reward: 0.601 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.907, 10.278], loss: 0.001479, mae: 0.041385, mean_q: 1.168660
 440027/1000000: episode: 4401, duration: 1.455s, episode steps: 100, steps per second: 69, episode reward: 58.372, mean reward: 0.584 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.169, 10.098], loss: 0.001406, mae: 0.040680, mean_q: 1.169904
 440127/1000000: episode: 4402, duration: 1.424s, episode steps: 100, steps per second: 70, episode reward: 60.093, mean reward: 0.601 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.495, 10.158], loss: 0.001469, mae: 0.041205, mean_q: 1.168755
 440227/1000000: episode: 4403, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 60.020, mean reward: 0.600 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.905, 10.106], loss: 0.001418, mae: 0.041530, mean_q: 1.168703
 440327/1000000: episode: 4404, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.127, mean reward: 0.581 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.387, 10.134], loss: 0.001473, mae: 0.041503, mean_q: 1.171534
 440427/1000000: episode: 4405, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 59.081, mean reward: 0.591 [0.513, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.098], loss: 0.001431, mae: 0.041653, mean_q: 1.171565
 440527/1000000: episode: 4406, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 61.944, mean reward: 0.619 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.436, 10.332], loss: 0.001311, mae: 0.040017, mean_q: 1.169922
 440627/1000000: episode: 4407, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 56.429, mean reward: 0.564 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.390, 10.160], loss: 0.001438, mae: 0.041630, mean_q: 1.169009
 440727/1000000: episode: 4408, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 58.015, mean reward: 0.580 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.430, 10.098], loss: 0.001503, mae: 0.042505, mean_q: 1.171490
 440827/1000000: episode: 4409, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 57.143, mean reward: 0.571 [0.513, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.472, 10.131], loss: 0.001408, mae: 0.041029, mean_q: 1.166015
 440927/1000000: episode: 4410, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 58.216, mean reward: 0.582 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.906, 10.130], loss: 0.001428, mae: 0.041167, mean_q: 1.163729
 441027/1000000: episode: 4411, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 62.193, mean reward: 0.622 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.353, 10.098], loss: 0.001457, mae: 0.041502, mean_q: 1.161685
 441127/1000000: episode: 4412, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 60.081, mean reward: 0.601 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.727, 10.379], loss: 0.001408, mae: 0.040983, mean_q: 1.164383
 441227/1000000: episode: 4413, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.001, mean reward: 0.600 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.663, 10.173], loss: 0.001409, mae: 0.040822, mean_q: 1.164259
 441327/1000000: episode: 4414, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 57.704, mean reward: 0.577 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.405, 10.108], loss: 0.001415, mae: 0.041406, mean_q: 1.165439
 441427/1000000: episode: 4415, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 59.525, mean reward: 0.595 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.285, 10.104], loss: 0.001394, mae: 0.040833, mean_q: 1.168582
 441527/1000000: episode: 4416, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 57.833, mean reward: 0.578 [0.497, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.495, 10.098], loss: 0.001421, mae: 0.041057, mean_q: 1.165151
 441627/1000000: episode: 4417, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: 59.062, mean reward: 0.591 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.098], loss: 0.001377, mae: 0.040913, mean_q: 1.170708
 441727/1000000: episode: 4418, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 60.522, mean reward: 0.605 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.031, 10.341], loss: 0.001437, mae: 0.041240, mean_q: 1.168271
 441827/1000000: episode: 4419, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 60.241, mean reward: 0.602 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.911, 10.378], loss: 0.001413, mae: 0.041267, mean_q: 1.168504
 441927/1000000: episode: 4420, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.840, mean reward: 0.578 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.394, 10.289], loss: 0.001423, mae: 0.041336, mean_q: 1.172988
 442027/1000000: episode: 4421, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 57.152, mean reward: 0.572 [0.498, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.118], loss: 0.001372, mae: 0.040525, mean_q: 1.172930
 442127/1000000: episode: 4422, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 60.899, mean reward: 0.609 [0.500, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.239, 10.274], loss: 0.001539, mae: 0.042747, mean_q: 1.167697
 442227/1000000: episode: 4423, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 60.191, mean reward: 0.602 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.611, 10.098], loss: 0.001410, mae: 0.041365, mean_q: 1.172553
 442327/1000000: episode: 4424, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.215, mean reward: 0.572 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.898, 10.171], loss: 0.001425, mae: 0.041377, mean_q: 1.168164
 442427/1000000: episode: 4425, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 59.898, mean reward: 0.599 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.002, 10.201], loss: 0.001414, mae: 0.041256, mean_q: 1.173339
 442527/1000000: episode: 4426, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 58.455, mean reward: 0.585 [0.513, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.804, 10.098], loss: 0.001449, mae: 0.041327, mean_q: 1.173138
 442627/1000000: episode: 4427, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 58.106, mean reward: 0.581 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.019, 10.321], loss: 0.001414, mae: 0.041374, mean_q: 1.170566
 442727/1000000: episode: 4428, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 57.837, mean reward: 0.578 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.873, 10.098], loss: 0.001407, mae: 0.040983, mean_q: 1.171582
 442827/1000000: episode: 4429, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 58.932, mean reward: 0.589 [0.515, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.300, 10.098], loss: 0.001405, mae: 0.041304, mean_q: 1.173272
 442927/1000000: episode: 4430, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 62.915, mean reward: 0.629 [0.509, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.018, 10.422], loss: 0.001322, mae: 0.040108, mean_q: 1.169421
 443027/1000000: episode: 4431, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 57.877, mean reward: 0.579 [0.508, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.153, 10.098], loss: 0.001389, mae: 0.040570, mean_q: 1.169098
 443127/1000000: episode: 4432, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.688, mean reward: 0.587 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.098], loss: 0.001343, mae: 0.040279, mean_q: 1.173737
 443227/1000000: episode: 4433, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 61.198, mean reward: 0.612 [0.518, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.605, 10.098], loss: 0.001440, mae: 0.041952, mean_q: 1.173262
 443327/1000000: episode: 4434, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 57.941, mean reward: 0.579 [0.511, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.560, 10.207], loss: 0.001561, mae: 0.043288, mean_q: 1.174333
 443427/1000000: episode: 4435, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 60.970, mean reward: 0.610 [0.515, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.139], loss: 0.001450, mae: 0.041870, mean_q: 1.170555
 443527/1000000: episode: 4436, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 58.583, mean reward: 0.586 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.375, 10.098], loss: 0.001435, mae: 0.041094, mean_q: 1.172672
 443627/1000000: episode: 4437, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 58.928, mean reward: 0.589 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.358, 10.098], loss: 0.001454, mae: 0.041356, mean_q: 1.172001
 443727/1000000: episode: 4438, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.651, mean reward: 0.597 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.287], loss: 0.001393, mae: 0.040491, mean_q: 1.172033
 443827/1000000: episode: 4439, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 57.631, mean reward: 0.576 [0.515, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.828, 10.153], loss: 0.001423, mae: 0.041522, mean_q: 1.167511
 443927/1000000: episode: 4440, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.025, mean reward: 0.570 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.354, 10.098], loss: 0.001415, mae: 0.040829, mean_q: 1.167435
 444027/1000000: episode: 4441, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.181, mean reward: 0.572 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.814, 10.098], loss: 0.001431, mae: 0.041562, mean_q: 1.168093
 444127/1000000: episode: 4442, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 59.554, mean reward: 0.596 [0.512, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.306, 10.335], loss: 0.001398, mae: 0.040684, mean_q: 1.166993
 444227/1000000: episode: 4443, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.007, mean reward: 0.570 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.482, 10.145], loss: 0.001337, mae: 0.040461, mean_q: 1.164755
 444327/1000000: episode: 4444, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 57.909, mean reward: 0.579 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.924, 10.098], loss: 0.001426, mae: 0.040937, mean_q: 1.165452
 444427/1000000: episode: 4445, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 59.983, mean reward: 0.600 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.223], loss: 0.001409, mae: 0.040933, mean_q: 1.165714
 444527/1000000: episode: 4446, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 60.132, mean reward: 0.601 [0.506, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.574, 10.512], loss: 0.001503, mae: 0.042092, mean_q: 1.167779
 444627/1000000: episode: 4447, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.365, mean reward: 0.584 [0.505, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.530, 10.127], loss: 0.001405, mae: 0.041099, mean_q: 1.166576
 444727/1000000: episode: 4448, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 58.800, mean reward: 0.588 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.098], loss: 0.001556, mae: 0.042612, mean_q: 1.166668
 444827/1000000: episode: 4449, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 59.462, mean reward: 0.595 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.161, 10.098], loss: 0.001419, mae: 0.041160, mean_q: 1.166328
 444927/1000000: episode: 4450, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 57.907, mean reward: 0.579 [0.508, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.025, 10.098], loss: 0.001432, mae: 0.040997, mean_q: 1.169147
 445027/1000000: episode: 4451, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 60.911, mean reward: 0.609 [0.515, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.655, 10.098], loss: 0.001397, mae: 0.040782, mean_q: 1.166062
 445127/1000000: episode: 4452, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 61.816, mean reward: 0.618 [0.529, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.494, 10.098], loss: 0.001481, mae: 0.041312, mean_q: 1.167804
 445227/1000000: episode: 4453, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 59.912, mean reward: 0.599 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.027, 10.184], loss: 0.001434, mae: 0.041560, mean_q: 1.166765
 445327/1000000: episode: 4454, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 62.108, mean reward: 0.621 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.363, 10.098], loss: 0.001438, mae: 0.041119, mean_q: 1.169589
 445427/1000000: episode: 4455, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 61.736, mean reward: 0.617 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.256, 10.098], loss: 0.001414, mae: 0.041172, mean_q: 1.167785
 445527/1000000: episode: 4456, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.354, mean reward: 0.594 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.380, 10.098], loss: 0.001362, mae: 0.040137, mean_q: 1.170545
 445627/1000000: episode: 4457, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.113, mean reward: 0.581 [0.515, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.392, 10.098], loss: 0.001370, mae: 0.040343, mean_q: 1.170781
 445727/1000000: episode: 4458, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 60.822, mean reward: 0.608 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.748, 10.299], loss: 0.001477, mae: 0.041044, mean_q: 1.173497
 445827/1000000: episode: 4459, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 57.705, mean reward: 0.577 [0.499, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.734, 10.098], loss: 0.001460, mae: 0.041758, mean_q: 1.172801
 445927/1000000: episode: 4460, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 61.413, mean reward: 0.614 [0.510, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.267, 10.381], loss: 0.001393, mae: 0.040542, mean_q: 1.171801
 446027/1000000: episode: 4461, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.058, mean reward: 0.581 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.695, 10.205], loss: 0.001489, mae: 0.042045, mean_q: 1.170490
 446127/1000000: episode: 4462, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 59.666, mean reward: 0.597 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.184, 10.288], loss: 0.001563, mae: 0.042685, mean_q: 1.174035
 446227/1000000: episode: 4463, duration: 1.512s, episode steps: 100, steps per second: 66, episode reward: 61.281, mean reward: 0.613 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.098], loss: 0.001539, mae: 0.043134, mean_q: 1.169228
 446327/1000000: episode: 4464, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 57.335, mean reward: 0.573 [0.513, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.575, 10.098], loss: 0.001497, mae: 0.041562, mean_q: 1.171996
 446427/1000000: episode: 4465, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 58.215, mean reward: 0.582 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.509, 10.228], loss: 0.001462, mae: 0.041927, mean_q: 1.173586
 446527/1000000: episode: 4466, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 61.548, mean reward: 0.615 [0.512, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.759, 10.098], loss: 0.001426, mae: 0.041324, mean_q: 1.174284
 446627/1000000: episode: 4467, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 57.920, mean reward: 0.579 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.789, 10.098], loss: 0.001428, mae: 0.041344, mean_q: 1.175129
 446727/1000000: episode: 4468, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.240, mean reward: 0.572 [0.518, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.290, 10.098], loss: 0.001489, mae: 0.041766, mean_q: 1.175324
 446827/1000000: episode: 4469, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 63.843, mean reward: 0.638 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.300], loss: 0.001388, mae: 0.040892, mean_q: 1.175089
 446927/1000000: episode: 4470, duration: 1.439s, episode steps: 100, steps per second: 69, episode reward: 59.565, mean reward: 0.596 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.339, 10.098], loss: 0.001394, mae: 0.040753, mean_q: 1.171739
 447027/1000000: episode: 4471, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 60.035, mean reward: 0.600 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.504, 10.098], loss: 0.001453, mae: 0.041082, mean_q: 1.171840
 447127/1000000: episode: 4472, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 58.588, mean reward: 0.586 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.785, 10.306], loss: 0.001445, mae: 0.040866, mean_q: 1.173176
 447227/1000000: episode: 4473, duration: 1.447s, episode steps: 100, steps per second: 69, episode reward: 62.909, mean reward: 0.629 [0.507, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.064, 10.098], loss: 0.001360, mae: 0.040618, mean_q: 1.173962
 447327/1000000: episode: 4474, duration: 1.412s, episode steps: 100, steps per second: 71, episode reward: 59.796, mean reward: 0.598 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.976, 10.098], loss: 0.001477, mae: 0.041821, mean_q: 1.174564
 447427/1000000: episode: 4475, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 58.317, mean reward: 0.583 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.803, 10.170], loss: 0.001420, mae: 0.040917, mean_q: 1.175006
 447527/1000000: episode: 4476, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 59.189, mean reward: 0.592 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.098], loss: 0.001456, mae: 0.041440, mean_q: 1.175854
 447627/1000000: episode: 4477, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 57.670, mean reward: 0.577 [0.507, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.521, 10.098], loss: 0.001364, mae: 0.040178, mean_q: 1.173397
 447727/1000000: episode: 4478, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 59.288, mean reward: 0.593 [0.519, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.918, 10.176], loss: 0.001490, mae: 0.041890, mean_q: 1.174067
 447827/1000000: episode: 4479, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 58.949, mean reward: 0.589 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.469, 10.098], loss: 0.001446, mae: 0.041565, mean_q: 1.176947
 447927/1000000: episode: 4480, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 60.482, mean reward: 0.605 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.179, 10.254], loss: 0.001466, mae: 0.042167, mean_q: 1.178308
 448027/1000000: episode: 4481, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 59.728, mean reward: 0.597 [0.511, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.444, 10.134], loss: 0.001552, mae: 0.042969, mean_q: 1.174551
 448127/1000000: episode: 4482, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 58.641, mean reward: 0.586 [0.514, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.609, 10.098], loss: 0.001373, mae: 0.040875, mean_q: 1.174178
 448227/1000000: episode: 4483, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 55.864, mean reward: 0.559 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.228], loss: 0.001391, mae: 0.040526, mean_q: 1.174069
 448327/1000000: episode: 4484, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 59.079, mean reward: 0.591 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.292, 10.098], loss: 0.001398, mae: 0.040340, mean_q: 1.174989
 448427/1000000: episode: 4485, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 60.506, mean reward: 0.605 [0.499, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.582, 10.136], loss: 0.001448, mae: 0.041504, mean_q: 1.176742
 448527/1000000: episode: 4486, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.232, mean reward: 0.592 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.161], loss: 0.001500, mae: 0.041916, mean_q: 1.174300
 448627/1000000: episode: 4487, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.157, mean reward: 0.592 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.753, 10.098], loss: 0.001440, mae: 0.041575, mean_q: 1.170750
 448727/1000000: episode: 4488, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 60.486, mean reward: 0.605 [0.524, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.371, 10.098], loss: 0.001370, mae: 0.040368, mean_q: 1.170823
 448827/1000000: episode: 4489, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 57.950, mean reward: 0.579 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.680, 10.098], loss: 0.001446, mae: 0.041387, mean_q: 1.176338
 448927/1000000: episode: 4490, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 57.440, mean reward: 0.574 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.271, 10.120], loss: 0.001409, mae: 0.040773, mean_q: 1.174121
 449027/1000000: episode: 4491, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 58.037, mean reward: 0.580 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.107], loss: 0.001449, mae: 0.041324, mean_q: 1.174762
 449127/1000000: episode: 4492, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 57.394, mean reward: 0.574 [0.505, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.557, 10.256], loss: 0.001531, mae: 0.042358, mean_q: 1.174729
 449227/1000000: episode: 4493, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.202, mean reward: 0.582 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.553, 10.133], loss: 0.001542, mae: 0.042676, mean_q: 1.174157
 449327/1000000: episode: 4494, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 60.646, mean reward: 0.606 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.531, 10.276], loss: 0.001502, mae: 0.042384, mean_q: 1.177492
 449427/1000000: episode: 4495, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 59.060, mean reward: 0.591 [0.513, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.492, 10.349], loss: 0.001527, mae: 0.042006, mean_q: 1.174823
 449527/1000000: episode: 4496, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 61.338, mean reward: 0.613 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.385, 10.098], loss: 0.001378, mae: 0.040026, mean_q: 1.176172
 449627/1000000: episode: 4497, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.586, mean reward: 0.586 [0.508, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.102, 10.246], loss: 0.001410, mae: 0.041215, mean_q: 1.177904
 449727/1000000: episode: 4498, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.773, mean reward: 0.578 [0.512, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.856, 10.183], loss: 0.001486, mae: 0.041970, mean_q: 1.178070
 449827/1000000: episode: 4499, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.817, mean reward: 0.588 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.291, 10.205], loss: 0.001439, mae: 0.041185, mean_q: 1.175159
 449927/1000000: episode: 4500, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.153, mean reward: 0.572 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.289, 10.098], loss: 0.001326, mae: 0.039755, mean_q: 1.176573
 450027/1000000: episode: 4501, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 60.260, mean reward: 0.603 [0.513, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.372, 10.442], loss: 0.001358, mae: 0.040158, mean_q: 1.176324
 450127/1000000: episode: 4502, duration: 1.352s, episode steps: 100, steps per second: 74, episode reward: 58.319, mean reward: 0.583 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.753, 10.126], loss: 0.001295, mae: 0.038944, mean_q: 1.173591
 450227/1000000: episode: 4503, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 63.464, mean reward: 0.635 [0.506, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.465, 10.098], loss: 0.001406, mae: 0.040290, mean_q: 1.173248
 450327/1000000: episode: 4504, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 59.191, mean reward: 0.592 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.570, 10.098], loss: 0.001398, mae: 0.040829, mean_q: 1.174543
 450427/1000000: episode: 4505, duration: 1.379s, episode steps: 100, steps per second: 72, episode reward: 58.759, mean reward: 0.588 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.187, 10.098], loss: 0.001360, mae: 0.040419, mean_q: 1.174211
 450527/1000000: episode: 4506, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 58.401, mean reward: 0.584 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.965, 10.098], loss: 0.001378, mae: 0.040495, mean_q: 1.175624
 450627/1000000: episode: 4507, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 57.842, mean reward: 0.578 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.266, 10.133], loss: 0.001439, mae: 0.041391, mean_q: 1.169509
 450727/1000000: episode: 4508, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 57.110, mean reward: 0.571 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.469, 10.098], loss: 0.001361, mae: 0.039544, mean_q: 1.168473
 450827/1000000: episode: 4509, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.814, mean reward: 0.578 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.030, 10.111], loss: 0.001374, mae: 0.040355, mean_q: 1.168675
 450927/1000000: episode: 4510, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 59.181, mean reward: 0.592 [0.507, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.935, 10.098], loss: 0.001324, mae: 0.039639, mean_q: 1.168402
 451027/1000000: episode: 4511, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 60.659, mean reward: 0.607 [0.506, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.267, 10.099], loss: 0.001364, mae: 0.040052, mean_q: 1.170066
 451127/1000000: episode: 4512, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.088, mean reward: 0.571 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.623, 10.098], loss: 0.001371, mae: 0.040232, mean_q: 1.166153
 451227/1000000: episode: 4513, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 58.727, mean reward: 0.587 [0.511, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.799, 10.124], loss: 0.001379, mae: 0.040084, mean_q: 1.168320
 451327/1000000: episode: 4514, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 56.976, mean reward: 0.570 [0.510, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.228], loss: 0.001256, mae: 0.038791, mean_q: 1.168126
 451427/1000000: episode: 4515, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.353, mean reward: 0.574 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.186], loss: 0.001387, mae: 0.040033, mean_q: 1.167773
 451527/1000000: episode: 4516, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 57.620, mean reward: 0.576 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.322, 10.207], loss: 0.001351, mae: 0.040072, mean_q: 1.167049
 451627/1000000: episode: 4517, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.496, mean reward: 0.585 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.957, 10.098], loss: 0.001382, mae: 0.040209, mean_q: 1.166602
 451727/1000000: episode: 4518, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.595, mean reward: 0.596 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.732, 10.131], loss: 0.001404, mae: 0.040539, mean_q: 1.166980
 451827/1000000: episode: 4519, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 57.587, mean reward: 0.576 [0.498, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.357, 10.153], loss: 0.001432, mae: 0.040744, mean_q: 1.165253
 451927/1000000: episode: 4520, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 60.195, mean reward: 0.602 [0.516, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.179, 10.218], loss: 0.001466, mae: 0.041011, mean_q: 1.167588
 452027/1000000: episode: 4521, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 57.699, mean reward: 0.577 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.875, 10.151], loss: 0.001427, mae: 0.041522, mean_q: 1.169996
 452127/1000000: episode: 4522, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 58.299, mean reward: 0.583 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.304, 10.098], loss: 0.001387, mae: 0.040044, mean_q: 1.164159
 452227/1000000: episode: 4523, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 58.030, mean reward: 0.580 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.675, 10.098], loss: 0.001325, mae: 0.039281, mean_q: 1.163014
 452327/1000000: episode: 4524, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 59.984, mean reward: 0.600 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.038, 10.350], loss: 0.001244, mae: 0.038749, mean_q: 1.164631
 452427/1000000: episode: 4525, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 63.899, mean reward: 0.639 [0.503, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.019, 10.098], loss: 0.001301, mae: 0.039211, mean_q: 1.161707
 452527/1000000: episode: 4526, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.346, mean reward: 0.583 [0.507, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.389, 10.098], loss: 0.001328, mae: 0.039609, mean_q: 1.162426
 452627/1000000: episode: 4527, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 60.206, mean reward: 0.602 [0.509, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.249, 10.227], loss: 0.001369, mae: 0.040223, mean_q: 1.164096
 452727/1000000: episode: 4528, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 59.261, mean reward: 0.593 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.606, 10.098], loss: 0.001349, mae: 0.039636, mean_q: 1.165157
 452827/1000000: episode: 4529, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 57.015, mean reward: 0.570 [0.498, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.216, 10.133], loss: 0.001327, mae: 0.039929, mean_q: 1.163400
 452927/1000000: episode: 4530, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.748, mean reward: 0.597 [0.509, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.429, 10.098], loss: 0.001360, mae: 0.040034, mean_q: 1.162042
 453027/1000000: episode: 4531, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 56.474, mean reward: 0.565 [0.500, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.098], loss: 0.001358, mae: 0.039826, mean_q: 1.164179
 453127/1000000: episode: 4532, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 59.077, mean reward: 0.591 [0.513, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.223, 10.098], loss: 0.001372, mae: 0.040207, mean_q: 1.165289
 453227/1000000: episode: 4533, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 60.327, mean reward: 0.603 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.057, 10.347], loss: 0.001425, mae: 0.040090, mean_q: 1.164399
 453327/1000000: episode: 4534, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 60.111, mean reward: 0.601 [0.518, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.551, 10.130], loss: 0.001291, mae: 0.039542, mean_q: 1.162866
 453427/1000000: episode: 4535, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.553, mean reward: 0.576 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.686, 10.098], loss: 0.001349, mae: 0.039940, mean_q: 1.161541
 453527/1000000: episode: 4536, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 58.340, mean reward: 0.583 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.999, 10.324], loss: 0.001491, mae: 0.041486, mean_q: 1.162564
 453627/1000000: episode: 4537, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 61.003, mean reward: 0.610 [0.513, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.687, 10.100], loss: 0.001401, mae: 0.040460, mean_q: 1.165157
 453727/1000000: episode: 4538, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 59.169, mean reward: 0.592 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.215, 10.256], loss: 0.001364, mae: 0.040319, mean_q: 1.161048
 453827/1000000: episode: 4539, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.323, mean reward: 0.583 [0.498, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.502, 10.366], loss: 0.001465, mae: 0.041332, mean_q: 1.164154
 453927/1000000: episode: 4540, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 57.559, mean reward: 0.576 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.149], loss: 0.001412, mae: 0.040274, mean_q: 1.162949
 454027/1000000: episode: 4541, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 60.015, mean reward: 0.600 [0.510, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.390, 10.298], loss: 0.001346, mae: 0.039231, mean_q: 1.166054
 454127/1000000: episode: 4542, duration: 1.636s, episode steps: 100, steps per second: 61, episode reward: 60.426, mean reward: 0.604 [0.525, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.731, 10.098], loss: 0.001458, mae: 0.040861, mean_q: 1.164560
 454227/1000000: episode: 4543, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 57.135, mean reward: 0.571 [0.510, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.155, 10.098], loss: 0.001373, mae: 0.040083, mean_q: 1.167166
 454327/1000000: episode: 4544, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.206, mean reward: 0.572 [0.498, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.105, 10.163], loss: 0.001421, mae: 0.040570, mean_q: 1.164999
 454427/1000000: episode: 4545, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 56.510, mean reward: 0.565 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.380, 10.165], loss: 0.001390, mae: 0.040414, mean_q: 1.162847
 454527/1000000: episode: 4546, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 58.212, mean reward: 0.582 [0.500, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.098], loss: 0.001356, mae: 0.039977, mean_q: 1.159951
 454627/1000000: episode: 4547, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 59.113, mean reward: 0.591 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.309, 10.098], loss: 0.001411, mae: 0.040714, mean_q: 1.166211
 454727/1000000: episode: 4548, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.615, mean reward: 0.596 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.098], loss: 0.001366, mae: 0.039473, mean_q: 1.162335
 454827/1000000: episode: 4549, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.668, mean reward: 0.587 [0.506, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.153, 10.202], loss: 0.001442, mae: 0.040861, mean_q: 1.163926
 454927/1000000: episode: 4550, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 57.903, mean reward: 0.579 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.972, 10.197], loss: 0.001382, mae: 0.040005, mean_q: 1.164778
 455027/1000000: episode: 4551, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.588, mean reward: 0.586 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.363, 10.098], loss: 0.001351, mae: 0.039881, mean_q: 1.165161
 455127/1000000: episode: 4552, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.413, mean reward: 0.574 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.098], loss: 0.001372, mae: 0.039543, mean_q: 1.162523
 455227/1000000: episode: 4553, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.866, mean reward: 0.579 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.968, 10.098], loss: 0.001461, mae: 0.041369, mean_q: 1.162196
 455327/1000000: episode: 4554, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 56.576, mean reward: 0.566 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.715, 10.135], loss: 0.001481, mae: 0.041395, mean_q: 1.161213
 455427/1000000: episode: 4555, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 59.086, mean reward: 0.591 [0.509, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.309, 10.098], loss: 0.001337, mae: 0.039490, mean_q: 1.161382
 455527/1000000: episode: 4556, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 57.396, mean reward: 0.574 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.736, 10.177], loss: 0.001316, mae: 0.038887, mean_q: 1.159188
 455627/1000000: episode: 4557, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 59.543, mean reward: 0.595 [0.509, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.520, 10.122], loss: 0.001324, mae: 0.039049, mean_q: 1.156841
 455727/1000000: episode: 4558, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.824, mean reward: 0.598 [0.514, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.341, 10.098], loss: 0.001356, mae: 0.039940, mean_q: 1.160145
 455827/1000000: episode: 4559, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 61.364, mean reward: 0.614 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.091, 10.250], loss: 0.001342, mae: 0.039621, mean_q: 1.162141
 455927/1000000: episode: 4560, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 56.667, mean reward: 0.567 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.272, 10.098], loss: 0.001433, mae: 0.040950, mean_q: 1.160900
 456027/1000000: episode: 4561, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 59.232, mean reward: 0.592 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.661, 10.098], loss: 0.001374, mae: 0.039917, mean_q: 1.159370
 456127/1000000: episode: 4562, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.193, mean reward: 0.572 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.996, 10.180], loss: 0.001371, mae: 0.040209, mean_q: 1.159509
 456227/1000000: episode: 4563, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 58.710, mean reward: 0.587 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.478, 10.177], loss: 0.001368, mae: 0.040492, mean_q: 1.159308
 456327/1000000: episode: 4564, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 61.459, mean reward: 0.615 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.705, 10.098], loss: 0.001462, mae: 0.041325, mean_q: 1.155936
 456427/1000000: episode: 4565, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.840, mean reward: 0.578 [0.514, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.861, 10.180], loss: 0.001484, mae: 0.041809, mean_q: 1.164086
 456527/1000000: episode: 4566, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.582, mean reward: 0.586 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.317, 10.392], loss: 0.001410, mae: 0.040928, mean_q: 1.162289
 456627/1000000: episode: 4567, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.212, mean reward: 0.592 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.216], loss: 0.001396, mae: 0.040354, mean_q: 1.163019
 456727/1000000: episode: 4568, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.378, mean reward: 0.584 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.577, 10.113], loss: 0.001455, mae: 0.040673, mean_q: 1.161110
 456827/1000000: episode: 4569, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 58.459, mean reward: 0.585 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.276], loss: 0.001392, mae: 0.040247, mean_q: 1.163269
 456927/1000000: episode: 4570, duration: 1.281s, episode steps: 100, steps per second: 78, episode reward: 59.514, mean reward: 0.595 [0.506, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.502, 10.489], loss: 0.001425, mae: 0.040405, mean_q: 1.165058
 457027/1000000: episode: 4571, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.974, mean reward: 0.590 [0.511, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.663, 10.359], loss: 0.001345, mae: 0.039734, mean_q: 1.159795
 457127/1000000: episode: 4572, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 59.220, mean reward: 0.592 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.896, 10.098], loss: 0.001462, mae: 0.041795, mean_q: 1.164257
 457227/1000000: episode: 4573, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.434, mean reward: 0.584 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.682, 10.098], loss: 0.001360, mae: 0.039707, mean_q: 1.162165
 457327/1000000: episode: 4574, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 58.450, mean reward: 0.584 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.754, 10.098], loss: 0.001386, mae: 0.040599, mean_q: 1.165228
 457427/1000000: episode: 4575, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 58.433, mean reward: 0.584 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.257], loss: 0.001430, mae: 0.041041, mean_q: 1.165293
 457527/1000000: episode: 4576, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 58.497, mean reward: 0.585 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.554, 10.399], loss: 0.001559, mae: 0.042711, mean_q: 1.160984
 457627/1000000: episode: 4577, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 56.773, mean reward: 0.568 [0.510, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.046, 10.165], loss: 0.001384, mae: 0.040608, mean_q: 1.160823
 457727/1000000: episode: 4578, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 57.157, mean reward: 0.572 [0.502, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.550, 10.098], loss: 0.001343, mae: 0.040089, mean_q: 1.155402
 457827/1000000: episode: 4579, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.780, mean reward: 0.578 [0.498, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.358, 10.321], loss: 0.001362, mae: 0.039820, mean_q: 1.160003
 457927/1000000: episode: 4580, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 59.145, mean reward: 0.591 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.032, 10.245], loss: 0.001436, mae: 0.041388, mean_q: 1.160885
 458027/1000000: episode: 4581, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.199, mean reward: 0.592 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.801, 10.147], loss: 0.001340, mae: 0.039928, mean_q: 1.162993
 458127/1000000: episode: 4582, duration: 0.803s, episode steps: 100, steps per second: 124, episode reward: 57.165, mean reward: 0.572 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.491, 10.159], loss: 0.001277, mae: 0.039149, mean_q: 1.158500
 458227/1000000: episode: 4583, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 61.496, mean reward: 0.615 [0.502, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.353, 10.098], loss: 0.001432, mae: 0.041611, mean_q: 1.160432
 458327/1000000: episode: 4584, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.528, mean reward: 0.585 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.364, 10.356], loss: 0.001477, mae: 0.040890, mean_q: 1.161768
 458427/1000000: episode: 4585, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.418, mean reward: 0.584 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.226, 10.289], loss: 0.001388, mae: 0.039865, mean_q: 1.160074
 458527/1000000: episode: 4586, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 57.226, mean reward: 0.572 [0.498, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.125, 10.098], loss: 0.001458, mae: 0.041647, mean_q: 1.158063
 458627/1000000: episode: 4587, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 57.002, mean reward: 0.570 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.180, 10.098], loss: 0.001404, mae: 0.040474, mean_q: 1.160231
 458727/1000000: episode: 4588, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.194, mean reward: 0.592 [0.508, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.245, 10.098], loss: 0.001379, mae: 0.039748, mean_q: 1.157896
 458827/1000000: episode: 4589, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 57.627, mean reward: 0.576 [0.500, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.095, 10.098], loss: 0.001314, mae: 0.039605, mean_q: 1.155627
 458927/1000000: episode: 4590, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.199, mean reward: 0.582 [0.502, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.605, 10.098], loss: 0.001299, mae: 0.039378, mean_q: 1.158933
 459027/1000000: episode: 4591, duration: 1.589s, episode steps: 100, steps per second: 63, episode reward: 58.930, mean reward: 0.589 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.787, 10.323], loss: 0.001426, mae: 0.041127, mean_q: 1.161496
 459127/1000000: episode: 4592, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.041, mean reward: 0.580 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.103], loss: 0.001255, mae: 0.038688, mean_q: 1.157783
 459227/1000000: episode: 4593, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 59.658, mean reward: 0.597 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.606, 10.098], loss: 0.001378, mae: 0.040120, mean_q: 1.153951
 459327/1000000: episode: 4594, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 61.965, mean reward: 0.620 [0.499, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.155, 10.535], loss: 0.001462, mae: 0.041420, mean_q: 1.156882
 459427/1000000: episode: 4595, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.397, mean reward: 0.574 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.911, 10.364], loss: 0.001438, mae: 0.040796, mean_q: 1.158246
 459527/1000000: episode: 4596, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 60.323, mean reward: 0.603 [0.511, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.731, 10.098], loss: 0.001359, mae: 0.040001, mean_q: 1.159004
 459627/1000000: episode: 4597, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 60.050, mean reward: 0.601 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.208, 10.487], loss: 0.001420, mae: 0.040313, mean_q: 1.160357
 459727/1000000: episode: 4598, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.487, mean reward: 0.585 [0.514, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.098], loss: 0.001433, mae: 0.040922, mean_q: 1.162489
 459827/1000000: episode: 4599, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 58.843, mean reward: 0.588 [0.509, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.821, 10.160], loss: 0.001440, mae: 0.041562, mean_q: 1.159927
 459927/1000000: episode: 4600, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 61.816, mean reward: 0.618 [0.507, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.015, 10.098], loss: 0.001503, mae: 0.041645, mean_q: 1.157803
 460027/1000000: episode: 4601, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 57.235, mean reward: 0.572 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.576, 10.311], loss: 0.001358, mae: 0.039925, mean_q: 1.161341
 460127/1000000: episode: 4602, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.982, mean reward: 0.580 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.321, 10.098], loss: 0.001379, mae: 0.040566, mean_q: 1.161712
 460227/1000000: episode: 4603, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.081, mean reward: 0.581 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.117], loss: 0.001398, mae: 0.040591, mean_q: 1.163321
 460327/1000000: episode: 4604, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 57.246, mean reward: 0.572 [0.508, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.879, 10.147], loss: 0.001401, mae: 0.040301, mean_q: 1.159163
 460427/1000000: episode: 4605, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 58.544, mean reward: 0.585 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.494, 10.137], loss: 0.001478, mae: 0.041633, mean_q: 1.162632
 460527/1000000: episode: 4606, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 58.602, mean reward: 0.586 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.431, 10.147], loss: 0.001402, mae: 0.040673, mean_q: 1.162794
 460627/1000000: episode: 4607, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 59.357, mean reward: 0.594 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.858, 10.296], loss: 0.001470, mae: 0.041255, mean_q: 1.162684
 460727/1000000: episode: 4608, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 59.180, mean reward: 0.592 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.750, 10.098], loss: 0.001330, mae: 0.039962, mean_q: 1.158311
 460827/1000000: episode: 4609, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 64.444, mean reward: 0.644 [0.513, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.098], loss: 0.001351, mae: 0.040110, mean_q: 1.158738
 460927/1000000: episode: 4610, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 63.316, mean reward: 0.633 [0.518, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.852, 10.196], loss: 0.001442, mae: 0.041374, mean_q: 1.165128
 461027/1000000: episode: 4611, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 59.152, mean reward: 0.592 [0.513, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.832, 10.098], loss: 0.001364, mae: 0.039750, mean_q: 1.165046
 461127/1000000: episode: 4612, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 58.068, mean reward: 0.581 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.477, 10.148], loss: 0.001319, mae: 0.039704, mean_q: 1.165211
 461227/1000000: episode: 4613, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.003, mean reward: 0.580 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.199], loss: 0.001404, mae: 0.040897, mean_q: 1.166107
 461327/1000000: episode: 4614, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 59.589, mean reward: 0.596 [0.518, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.951, 10.230], loss: 0.001388, mae: 0.041011, mean_q: 1.165392
 461427/1000000: episode: 4615, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 58.602, mean reward: 0.586 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.938, 10.130], loss: 0.001396, mae: 0.040539, mean_q: 1.166202
 461527/1000000: episode: 4616, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 57.801, mean reward: 0.578 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.420, 10.193], loss: 0.001445, mae: 0.040733, mean_q: 1.168027
 461627/1000000: episode: 4617, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 57.223, mean reward: 0.572 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.907, 10.098], loss: 0.001520, mae: 0.041828, mean_q: 1.165854
 461727/1000000: episode: 4618, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 59.508, mean reward: 0.595 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.298, 10.150], loss: 0.001351, mae: 0.040046, mean_q: 1.164453
 461827/1000000: episode: 4619, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 58.760, mean reward: 0.588 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.238, 10.098], loss: 0.001605, mae: 0.043600, mean_q: 1.166763
 461927/1000000: episode: 4620, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 56.427, mean reward: 0.564 [0.505, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.275], loss: 0.001471, mae: 0.041274, mean_q: 1.163413
 462027/1000000: episode: 4621, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 61.170, mean reward: 0.612 [0.512, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.154, 10.197], loss: 0.001512, mae: 0.042529, mean_q: 1.164190
 462127/1000000: episode: 4622, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.852, mean reward: 0.579 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.933, 10.320], loss: 0.001367, mae: 0.039536, mean_q: 1.164526
 462227/1000000: episode: 4623, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 59.321, mean reward: 0.593 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.251, 10.156], loss: 0.001478, mae: 0.040681, mean_q: 1.165055
 462327/1000000: episode: 4624, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 60.302, mean reward: 0.603 [0.514, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.292, 10.098], loss: 0.001414, mae: 0.040733, mean_q: 1.167509
 462427/1000000: episode: 4625, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 59.453, mean reward: 0.595 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.340, 10.379], loss: 0.001452, mae: 0.040887, mean_q: 1.163695
 462527/1000000: episode: 4626, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 60.063, mean reward: 0.601 [0.531, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.677, 10.143], loss: 0.001523, mae: 0.042452, mean_q: 1.167542
 462627/1000000: episode: 4627, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 59.470, mean reward: 0.595 [0.509, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.098], loss: 0.001469, mae: 0.040877, mean_q: 1.164830
 462727/1000000: episode: 4628, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.081, mean reward: 0.591 [0.514, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.345, 10.098], loss: 0.001529, mae: 0.042112, mean_q: 1.168082
 462827/1000000: episode: 4629, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 57.940, mean reward: 0.579 [0.513, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.771, 10.132], loss: 0.001520, mae: 0.041963, mean_q: 1.165842
 462927/1000000: episode: 4630, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 57.421, mean reward: 0.574 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.791, 10.098], loss: 0.001453, mae: 0.041120, mean_q: 1.166888
 463027/1000000: episode: 4631, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.869, mean reward: 0.589 [0.498, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.802, 10.098], loss: 0.001550, mae: 0.041936, mean_q: 1.165290
 463127/1000000: episode: 4632, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 56.755, mean reward: 0.568 [0.510, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.155, 10.098], loss: 0.001494, mae: 0.041186, mean_q: 1.168960
 463227/1000000: episode: 4633, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 59.476, mean reward: 0.595 [0.504, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.945, 10.098], loss: 0.001511, mae: 0.041579, mean_q: 1.167830
 463327/1000000: episode: 4634, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 59.267, mean reward: 0.593 [0.497, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.758, 10.098], loss: 0.001520, mae: 0.041843, mean_q: 1.168581
 463427/1000000: episode: 4635, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 57.688, mean reward: 0.577 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.344, 10.121], loss: 0.001460, mae: 0.041500, mean_q: 1.164490
 463527/1000000: episode: 4636, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 58.879, mean reward: 0.589 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.592, 10.098], loss: 0.001591, mae: 0.042749, mean_q: 1.167941
 463627/1000000: episode: 4637, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.203, mean reward: 0.572 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.174, 10.098], loss: 0.001525, mae: 0.042131, mean_q: 1.165804
 463727/1000000: episode: 4638, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 60.679, mean reward: 0.607 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.348, 10.209], loss: 0.001465, mae: 0.041257, mean_q: 1.166344
 463827/1000000: episode: 4639, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 57.576, mean reward: 0.576 [0.504, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.099], loss: 0.001522, mae: 0.041547, mean_q: 1.168577
 463927/1000000: episode: 4640, duration: 1.380s, episode steps: 100, steps per second: 72, episode reward: 60.485, mean reward: 0.605 [0.507, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.081, 10.098], loss: 0.001529, mae: 0.041952, mean_q: 1.168659
 464027/1000000: episode: 4641, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 58.442, mean reward: 0.584 [0.512, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.288, 10.336], loss: 0.001547, mae: 0.041949, mean_q: 1.168007
 464127/1000000: episode: 4642, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.807, mean reward: 0.588 [0.515, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.839, 10.098], loss: 0.001514, mae: 0.041493, mean_q: 1.170946
 464227/1000000: episode: 4643, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 58.077, mean reward: 0.581 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.247, 10.183], loss: 0.001488, mae: 0.041402, mean_q: 1.168423
 464327/1000000: episode: 4644, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 57.462, mean reward: 0.575 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.112, 10.125], loss: 0.001531, mae: 0.041453, mean_q: 1.164987
 464427/1000000: episode: 4645, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.465, mean reward: 0.595 [0.521, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.569, 10.346], loss: 0.001514, mae: 0.042280, mean_q: 1.163378
 464527/1000000: episode: 4646, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 57.858, mean reward: 0.579 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.962, 10.219], loss: 0.001437, mae: 0.041448, mean_q: 1.162606
 464627/1000000: episode: 4647, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 61.212, mean reward: 0.612 [0.502, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.951, 10.133], loss: 0.001500, mae: 0.041140, mean_q: 1.165208
 464727/1000000: episode: 4648, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 58.563, mean reward: 0.586 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.036, 10.414], loss: 0.001484, mae: 0.041836, mean_q: 1.166889
 464827/1000000: episode: 4649, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 59.147, mean reward: 0.591 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.623, 10.098], loss: 0.001507, mae: 0.041614, mean_q: 1.167980
 464927/1000000: episode: 4650, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.593, mean reward: 0.586 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.750, 10.098], loss: 0.001529, mae: 0.041985, mean_q: 1.166293
 465027/1000000: episode: 4651, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.408, mean reward: 0.594 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.719, 10.098], loss: 0.001524, mae: 0.042685, mean_q: 1.163681
 465127/1000000: episode: 4652, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 57.798, mean reward: 0.578 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.348, 10.128], loss: 0.001377, mae: 0.040339, mean_q: 1.162448
 465227/1000000: episode: 4653, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 61.547, mean reward: 0.615 [0.518, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.202, 10.505], loss: 0.001519, mae: 0.041974, mean_q: 1.163493
 465327/1000000: episode: 4654, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.231, mean reward: 0.582 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.678, 10.122], loss: 0.001436, mae: 0.041791, mean_q: 1.166883
 465427/1000000: episode: 4655, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 56.559, mean reward: 0.566 [0.499, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.098], loss: 0.001448, mae: 0.040904, mean_q: 1.165733
 465527/1000000: episode: 4656, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 60.779, mean reward: 0.608 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.272, 10.098], loss: 0.001487, mae: 0.041651, mean_q: 1.168456
 465627/1000000: episode: 4657, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 59.271, mean reward: 0.593 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.357, 10.355], loss: 0.001515, mae: 0.041757, mean_q: 1.165718
 465727/1000000: episode: 4658, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.570, mean reward: 0.586 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.138], loss: 0.001436, mae: 0.040998, mean_q: 1.165631
 465827/1000000: episode: 4659, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 61.346, mean reward: 0.613 [0.510, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.766, 10.098], loss: 0.001478, mae: 0.041676, mean_q: 1.163584
 465927/1000000: episode: 4660, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 56.769, mean reward: 0.568 [0.503, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.773, 10.189], loss: 0.001517, mae: 0.041683, mean_q: 1.163379
 466027/1000000: episode: 4661, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 57.408, mean reward: 0.574 [0.499, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.661, 10.142], loss: 0.001445, mae: 0.041150, mean_q: 1.162466
 466127/1000000: episode: 4662, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 57.938, mean reward: 0.579 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.098], loss: 0.001492, mae: 0.041596, mean_q: 1.163691
 466227/1000000: episode: 4663, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 59.986, mean reward: 0.600 [0.513, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.561, 10.098], loss: 0.001350, mae: 0.039327, mean_q: 1.163082
 466327/1000000: episode: 4664, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.368, mean reward: 0.584 [0.506, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.622, 10.098], loss: 0.001424, mae: 0.041155, mean_q: 1.161801
 466427/1000000: episode: 4665, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 58.508, mean reward: 0.585 [0.503, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.253, 10.098], loss: 0.001487, mae: 0.041763, mean_q: 1.158313
 466527/1000000: episode: 4666, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.560, mean reward: 0.576 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.792, 10.098], loss: 0.001394, mae: 0.040796, mean_q: 1.160197
 466627/1000000: episode: 4667, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 58.532, mean reward: 0.585 [0.500, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.403, 10.098], loss: 0.001541, mae: 0.042196, mean_q: 1.164395
 466727/1000000: episode: 4668, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 62.651, mean reward: 0.627 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.921, 10.098], loss: 0.001479, mae: 0.041489, mean_q: 1.163751
 466827/1000000: episode: 4669, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 56.791, mean reward: 0.568 [0.500, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.903, 10.112], loss: 0.001462, mae: 0.041394, mean_q: 1.166105
 466927/1000000: episode: 4670, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 58.943, mean reward: 0.589 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.262, 10.098], loss: 0.001460, mae: 0.042014, mean_q: 1.164964
 467027/1000000: episode: 4671, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 58.113, mean reward: 0.581 [0.506, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.650, 10.098], loss: 0.001420, mae: 0.041115, mean_q: 1.164813
 467127/1000000: episode: 4672, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.471, mean reward: 0.575 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.557, 10.223], loss: 0.001412, mae: 0.040691, mean_q: 1.163482
 467227/1000000: episode: 4673, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 60.874, mean reward: 0.609 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.898, 10.251], loss: 0.001288, mae: 0.039257, mean_q: 1.161794
 467327/1000000: episode: 4674, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.487, mean reward: 0.585 [0.498, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.402, 10.098], loss: 0.001394, mae: 0.040447, mean_q: 1.165133
 467427/1000000: episode: 4675, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 60.694, mean reward: 0.607 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.098], loss: 0.001411, mae: 0.041048, mean_q: 1.162012
 467527/1000000: episode: 4676, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 60.007, mean reward: 0.600 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.392], loss: 0.001375, mae: 0.040425, mean_q: 1.162828
 467627/1000000: episode: 4677, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 59.428, mean reward: 0.594 [0.509, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.108, 10.098], loss: 0.001355, mae: 0.040406, mean_q: 1.163532
 467727/1000000: episode: 4678, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 57.561, mean reward: 0.576 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.680, 10.163], loss: 0.001422, mae: 0.041376, mean_q: 1.161563
 467827/1000000: episode: 4679, duration: 1.486s, episode steps: 100, steps per second: 67, episode reward: 56.815, mean reward: 0.568 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.281, 10.247], loss: 0.001311, mae: 0.039394, mean_q: 1.164767
 467927/1000000: episode: 4680, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 58.801, mean reward: 0.588 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.098], loss: 0.001460, mae: 0.041609, mean_q: 1.165800
 468027/1000000: episode: 4681, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 59.356, mean reward: 0.594 [0.513, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.576, 10.098], loss: 0.001460, mae: 0.041152, mean_q: 1.163039
 468127/1000000: episode: 4682, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 56.563, mean reward: 0.566 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.440, 10.257], loss: 0.001427, mae: 0.041028, mean_q: 1.164293
 468227/1000000: episode: 4683, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 57.978, mean reward: 0.580 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.748, 10.125], loss: 0.001340, mae: 0.039783, mean_q: 1.162907
 468327/1000000: episode: 4684, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 58.631, mean reward: 0.586 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.209, 10.334], loss: 0.001459, mae: 0.041554, mean_q: 1.162312
 468427/1000000: episode: 4685, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.703, mean reward: 0.587 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.249, 10.098], loss: 0.001399, mae: 0.040529, mean_q: 1.163345
 468527/1000000: episode: 4686, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 60.216, mean reward: 0.602 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.100, 10.283], loss: 0.001416, mae: 0.041178, mean_q: 1.164297
 468627/1000000: episode: 4687, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.875, mean reward: 0.599 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.668, 10.230], loss: 0.001428, mae: 0.041068, mean_q: 1.164075
 468727/1000000: episode: 4688, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.253, mean reward: 0.573 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.723, 10.158], loss: 0.001402, mae: 0.040763, mean_q: 1.164240
 468827/1000000: episode: 4689, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 60.120, mean reward: 0.601 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.289, 10.098], loss: 0.001396, mae: 0.040952, mean_q: 1.165469
 468927/1000000: episode: 4690, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 58.433, mean reward: 0.584 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.601, 10.303], loss: 0.001419, mae: 0.041271, mean_q: 1.163492
 469027/1000000: episode: 4691, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 56.726, mean reward: 0.567 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.535, 10.159], loss: 0.001330, mae: 0.039842, mean_q: 1.159684
 469127/1000000: episode: 4692, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 62.746, mean reward: 0.627 [0.512, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.915, 10.098], loss: 0.001404, mae: 0.041224, mean_q: 1.166659
 469227/1000000: episode: 4693, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 60.946, mean reward: 0.609 [0.520, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.750, 10.227], loss: 0.001483, mae: 0.042467, mean_q: 1.166301
 469327/1000000: episode: 4694, duration: 1.454s, episode steps: 100, steps per second: 69, episode reward: 58.344, mean reward: 0.583 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.614, 10.098], loss: 0.001469, mae: 0.041228, mean_q: 1.165665
 469427/1000000: episode: 4695, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 60.652, mean reward: 0.607 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.966, 10.098], loss: 0.001404, mae: 0.040631, mean_q: 1.165899
 469527/1000000: episode: 4696, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 57.778, mean reward: 0.578 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.619, 10.098], loss: 0.001526, mae: 0.042657, mean_q: 1.166150
 469627/1000000: episode: 4697, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: 58.742, mean reward: 0.587 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.809, 10.098], loss: 0.001394, mae: 0.040214, mean_q: 1.165315
 469727/1000000: episode: 4698, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 61.673, mean reward: 0.617 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.289, 10.277], loss: 0.001390, mae: 0.040561, mean_q: 1.165510
 469827/1000000: episode: 4699, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 57.586, mean reward: 0.576 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.624, 10.098], loss: 0.001433, mae: 0.040873, mean_q: 1.167035
 469927/1000000: episode: 4700, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.694, mean reward: 0.597 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.288, 10.098], loss: 0.001384, mae: 0.040390, mean_q: 1.165549
 470027/1000000: episode: 4701, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 60.030, mean reward: 0.600 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.542, 10.098], loss: 0.001348, mae: 0.040221, mean_q: 1.167045
 470127/1000000: episode: 4702, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.672, mean reward: 0.587 [0.503, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.510, 10.304], loss: 0.001540, mae: 0.042622, mean_q: 1.166449
 470227/1000000: episode: 4703, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.322, mean reward: 0.583 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.503, 10.098], loss: 0.001480, mae: 0.041450, mean_q: 1.166123
 470327/1000000: episode: 4704, duration: 1.329s, episode steps: 100, steps per second: 75, episode reward: 59.079, mean reward: 0.591 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.440, 10.098], loss: 0.001434, mae: 0.040803, mean_q: 1.165499
 470427/1000000: episode: 4705, duration: 1.505s, episode steps: 100, steps per second: 66, episode reward: 58.678, mean reward: 0.587 [0.503, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.159, 10.120], loss: 0.001456, mae: 0.041644, mean_q: 1.166837
 470527/1000000: episode: 4706, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 57.447, mean reward: 0.574 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.536, 10.150], loss: 0.001381, mae: 0.040704, mean_q: 1.162974
 470627/1000000: episode: 4707, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 59.006, mean reward: 0.590 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.763, 10.390], loss: 0.001375, mae: 0.040521, mean_q: 1.163284
 470727/1000000: episode: 4708, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 59.384, mean reward: 0.594 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.908, 10.412], loss: 0.001451, mae: 0.041027, mean_q: 1.165822
 470827/1000000: episode: 4709, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 60.343, mean reward: 0.603 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.698, 10.098], loss: 0.001532, mae: 0.042728, mean_q: 1.171726
 470927/1000000: episode: 4710, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 60.307, mean reward: 0.603 [0.505, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.493, 10.447], loss: 0.001368, mae: 0.040200, mean_q: 1.168519
 471027/1000000: episode: 4711, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 58.826, mean reward: 0.588 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.009, 10.366], loss: 0.001440, mae: 0.041263, mean_q: 1.167509
 471127/1000000: episode: 4712, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 58.029, mean reward: 0.580 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.481, 10.098], loss: 0.001509, mae: 0.042194, mean_q: 1.165867
 471227/1000000: episode: 4713, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 59.352, mean reward: 0.594 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.330, 10.100], loss: 0.001489, mae: 0.042210, mean_q: 1.167644
 471327/1000000: episode: 4714, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 59.974, mean reward: 0.600 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.884, 10.098], loss: 0.001493, mae: 0.042106, mean_q: 1.167993
 471427/1000000: episode: 4715, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.702, mean reward: 0.577 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.278, 10.098], loss: 0.001382, mae: 0.040188, mean_q: 1.164839
 471527/1000000: episode: 4716, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.777, mean reward: 0.578 [0.506, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.233, 10.219], loss: 0.001534, mae: 0.042563, mean_q: 1.166943
 471627/1000000: episode: 4717, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 59.146, mean reward: 0.591 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.260, 10.152], loss: 0.001369, mae: 0.040211, mean_q: 1.165676
 471727/1000000: episode: 4718, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 60.247, mean reward: 0.602 [0.503, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.997, 10.316], loss: 0.001497, mae: 0.042075, mean_q: 1.162993
 471827/1000000: episode: 4719, duration: 1.373s, episode steps: 100, steps per second: 73, episode reward: 56.696, mean reward: 0.567 [0.502, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.947, 10.132], loss: 0.001435, mae: 0.041461, mean_q: 1.165816
 471927/1000000: episode: 4720, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 65.685, mean reward: 0.657 [0.502, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.666, 10.098], loss: 0.001429, mae: 0.041503, mean_q: 1.167844
 472027/1000000: episode: 4721, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 57.409, mean reward: 0.574 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.098], loss: 0.001393, mae: 0.040508, mean_q: 1.168029
 472127/1000000: episode: 4722, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 56.084, mean reward: 0.561 [0.504, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.685, 10.098], loss: 0.001485, mae: 0.041409, mean_q: 1.166286
 472227/1000000: episode: 4723, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.703, mean reward: 0.577 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.859, 10.098], loss: 0.001453, mae: 0.041600, mean_q: 1.165924
 472327/1000000: episode: 4724, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 57.930, mean reward: 0.579 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.713, 10.323], loss: 0.001398, mae: 0.040693, mean_q: 1.164301
 472427/1000000: episode: 4725, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 59.450, mean reward: 0.594 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.981, 10.098], loss: 0.001471, mae: 0.041362, mean_q: 1.165023
 472527/1000000: episode: 4726, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 59.696, mean reward: 0.597 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.098], loss: 0.001428, mae: 0.041228, mean_q: 1.167249
 472627/1000000: episode: 4727, duration: 0.837s, episode steps: 100, steps per second: 120, episode reward: 59.494, mean reward: 0.595 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.171, 10.098], loss: 0.001496, mae: 0.042347, mean_q: 1.165571
 472727/1000000: episode: 4728, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 58.079, mean reward: 0.581 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.074, 10.151], loss: 0.001480, mae: 0.041957, mean_q: 1.167779
 472827/1000000: episode: 4729, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.029, mean reward: 0.570 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.595, 10.291], loss: 0.001576, mae: 0.043045, mean_q: 1.167596
 472927/1000000: episode: 4730, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 57.847, mean reward: 0.578 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.793, 10.098], loss: 0.001503, mae: 0.041977, mean_q: 1.168392
 473027/1000000: episode: 4731, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 58.161, mean reward: 0.582 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.428, 10.098], loss: 0.001558, mae: 0.042561, mean_q: 1.166576
 473127/1000000: episode: 4732, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.732, mean reward: 0.597 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.689, 10.098], loss: 0.001497, mae: 0.041936, mean_q: 1.165257
 473227/1000000: episode: 4733, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 57.881, mean reward: 0.579 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.900, 10.387], loss: 0.001526, mae: 0.042348, mean_q: 1.165473
 473327/1000000: episode: 4734, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.332, mean reward: 0.583 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.216, 10.188], loss: 0.001445, mae: 0.040985, mean_q: 1.169445
 473427/1000000: episode: 4735, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 57.912, mean reward: 0.579 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.001456, mae: 0.041658, mean_q: 1.167281
 473527/1000000: episode: 4736, duration: 1.258s, episode steps: 100, steps per second: 79, episode reward: 55.865, mean reward: 0.559 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.820, 10.104], loss: 0.001444, mae: 0.041417, mean_q: 1.162108
 473627/1000000: episode: 4737, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 56.622, mean reward: 0.566 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.872, 10.139], loss: 0.001456, mae: 0.041578, mean_q: 1.162823
 473727/1000000: episode: 4738, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 57.906, mean reward: 0.579 [0.511, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.751, 10.098], loss: 0.001465, mae: 0.041149, mean_q: 1.166483
 473827/1000000: episode: 4739, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 61.506, mean reward: 0.615 [0.509, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.350, 10.098], loss: 0.001492, mae: 0.041468, mean_q: 1.166796
 473927/1000000: episode: 4740, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 61.127, mean reward: 0.611 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.384, 10.098], loss: 0.001477, mae: 0.041390, mean_q: 1.168582
 474027/1000000: episode: 4741, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.027, mean reward: 0.600 [0.505, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.196, 10.194], loss: 0.001507, mae: 0.041987, mean_q: 1.167649
 474127/1000000: episode: 4742, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 58.413, mean reward: 0.584 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.099], loss: 0.001523, mae: 0.041928, mean_q: 1.162727
 474227/1000000: episode: 4743, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.882, mean reward: 0.579 [0.504, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.098], loss: 0.001524, mae: 0.042291, mean_q: 1.162027
 474327/1000000: episode: 4744, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.803, mean reward: 0.588 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.999, 10.196], loss: 0.001536, mae: 0.042516, mean_q: 1.167694
 474427/1000000: episode: 4745, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 61.356, mean reward: 0.614 [0.510, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.764, 10.317], loss: 0.001451, mae: 0.041444, mean_q: 1.163051
 474527/1000000: episode: 4746, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 57.552, mean reward: 0.576 [0.514, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.422, 10.265], loss: 0.001619, mae: 0.043979, mean_q: 1.164512
 474627/1000000: episode: 4747, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 59.249, mean reward: 0.592 [0.512, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.806, 10.098], loss: 0.001350, mae: 0.040761, mean_q: 1.165507
 474727/1000000: episode: 4748, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 58.035, mean reward: 0.580 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.171, 10.098], loss: 0.001394, mae: 0.040742, mean_q: 1.163777
 474827/1000000: episode: 4749, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 57.931, mean reward: 0.579 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.621, 10.098], loss: 0.001402, mae: 0.040299, mean_q: 1.161511
 474927/1000000: episode: 4750, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 63.489, mean reward: 0.635 [0.509, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.981, 10.098], loss: 0.001509, mae: 0.042207, mean_q: 1.163901
 475027/1000000: episode: 4751, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 62.018, mean reward: 0.620 [0.502, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.476, 10.136], loss: 0.001541, mae: 0.042736, mean_q: 1.165699
 475127/1000000: episode: 4752, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 61.397, mean reward: 0.614 [0.498, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.310, 10.458], loss: 0.001476, mae: 0.041885, mean_q: 1.163968
 475227/1000000: episode: 4753, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.315, mean reward: 0.583 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.013, 10.098], loss: 0.001472, mae: 0.041778, mean_q: 1.165238
 475327/1000000: episode: 4754, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 63.413, mean reward: 0.634 [0.501, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.755, 10.604], loss: 0.001478, mae: 0.041859, mean_q: 1.164580
 475427/1000000: episode: 4755, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.661, mean reward: 0.587 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.240, 10.266], loss: 0.001592, mae: 0.043263, mean_q: 1.171145
 475527/1000000: episode: 4756, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 57.065, mean reward: 0.571 [0.501, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.645, 10.308], loss: 0.001514, mae: 0.042376, mean_q: 1.171192
 475627/1000000: episode: 4757, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 59.429, mean reward: 0.594 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.611, 10.249], loss: 0.001452, mae: 0.041514, mean_q: 1.172265
 475727/1000000: episode: 4758, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.842, mean reward: 0.598 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.486, 10.125], loss: 0.001394, mae: 0.040563, mean_q: 1.166523
 475827/1000000: episode: 4759, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 60.963, mean reward: 0.610 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.426], loss: 0.001489, mae: 0.041519, mean_q: 1.170818
 475927/1000000: episode: 4760, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 61.327, mean reward: 0.613 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.100, 10.297], loss: 0.001459, mae: 0.041052, mean_q: 1.163839
 476027/1000000: episode: 4761, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 59.695, mean reward: 0.597 [0.498, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.326, 10.098], loss: 0.001395, mae: 0.040228, mean_q: 1.166814
 476127/1000000: episode: 4762, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 59.765, mean reward: 0.598 [0.508, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.702, 10.098], loss: 0.001451, mae: 0.041923, mean_q: 1.169551
 476227/1000000: episode: 4763, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 60.026, mean reward: 0.600 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.313, 10.098], loss: 0.001431, mae: 0.041350, mean_q: 1.170285
 476327/1000000: episode: 4764, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.575, mean reward: 0.576 [0.507, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.041, 10.098], loss: 0.001528, mae: 0.041772, mean_q: 1.167976
 476427/1000000: episode: 4765, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 58.474, mean reward: 0.585 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.212], loss: 0.001446, mae: 0.040762, mean_q: 1.167586
 476527/1000000: episode: 4766, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 62.480, mean reward: 0.625 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.695, 10.270], loss: 0.001409, mae: 0.040833, mean_q: 1.171706
 476627/1000000: episode: 4767, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 59.894, mean reward: 0.599 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.312, 10.371], loss: 0.001438, mae: 0.040831, mean_q: 1.172149
 476727/1000000: episode: 4768, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 58.856, mean reward: 0.589 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.110, 10.098], loss: 0.001382, mae: 0.040479, mean_q: 1.172671
 476827/1000000: episode: 4769, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 57.339, mean reward: 0.573 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.001499, mae: 0.041202, mean_q: 1.171472
 476927/1000000: episode: 4770, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 60.419, mean reward: 0.604 [0.503, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.004, 10.148], loss: 0.001345, mae: 0.040010, mean_q: 1.169977
 477027/1000000: episode: 4771, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.999, mean reward: 0.590 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.098], loss: 0.001445, mae: 0.040958, mean_q: 1.170427
 477127/1000000: episode: 4772, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.386, mean reward: 0.594 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.098], loss: 0.001402, mae: 0.040753, mean_q: 1.167346
 477227/1000000: episode: 4773, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.594, mean reward: 0.586 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.670, 10.171], loss: 0.001405, mae: 0.041114, mean_q: 1.172516
 477327/1000000: episode: 4774, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 60.327, mean reward: 0.603 [0.516, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.441, 10.240], loss: 0.001463, mae: 0.041556, mean_q: 1.173615
 477427/1000000: episode: 4775, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 58.075, mean reward: 0.581 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.560, 10.098], loss: 0.001390, mae: 0.040605, mean_q: 1.173015
 477527/1000000: episode: 4776, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 59.990, mean reward: 0.600 [0.514, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.182, 10.098], loss: 0.001391, mae: 0.040768, mean_q: 1.175566
 477627/1000000: episode: 4777, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 59.039, mean reward: 0.590 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.153], loss: 0.001472, mae: 0.041506, mean_q: 1.172126
 477727/1000000: episode: 4778, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 57.568, mean reward: 0.576 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.401, 10.098], loss: 0.001408, mae: 0.040755, mean_q: 1.171982
 477827/1000000: episode: 4779, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 60.500, mean reward: 0.605 [0.516, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.365, 10.228], loss: 0.001472, mae: 0.041689, mean_q: 1.175048
 477927/1000000: episode: 4780, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 59.060, mean reward: 0.591 [0.516, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.801, 10.291], loss: 0.001379, mae: 0.040484, mean_q: 1.172291
 478027/1000000: episode: 4781, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 62.939, mean reward: 0.629 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.315, 10.411], loss: 0.001416, mae: 0.040918, mean_q: 1.177360
 478127/1000000: episode: 4782, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 58.773, mean reward: 0.588 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.029, 10.098], loss: 0.001429, mae: 0.041785, mean_q: 1.175466
 478227/1000000: episode: 4783, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.980, mean reward: 0.580 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.178, 10.098], loss: 0.001374, mae: 0.040111, mean_q: 1.175420
 478327/1000000: episode: 4784, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 60.310, mean reward: 0.603 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.771, 10.098], loss: 0.001359, mae: 0.040235, mean_q: 1.177024
 478427/1000000: episode: 4785, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.685, mean reward: 0.577 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.411, 10.235], loss: 0.001302, mae: 0.039419, mean_q: 1.177500
 478527/1000000: episode: 4786, duration: 1.395s, episode steps: 100, steps per second: 72, episode reward: 57.083, mean reward: 0.571 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.633, 10.098], loss: 0.001409, mae: 0.041008, mean_q: 1.179412
 478627/1000000: episode: 4787, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 58.224, mean reward: 0.582 [0.499, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.272, 10.229], loss: 0.001383, mae: 0.040542, mean_q: 1.176302
 478727/1000000: episode: 4788, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 57.673, mean reward: 0.577 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.659, 10.253], loss: 0.001315, mae: 0.039624, mean_q: 1.179703
 478827/1000000: episode: 4789, duration: 1.333s, episode steps: 100, steps per second: 75, episode reward: 58.395, mean reward: 0.584 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.221, 10.098], loss: 0.001530, mae: 0.042472, mean_q: 1.176165
 478927/1000000: episode: 4790, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.744, mean reward: 0.587 [0.512, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.196, 10.098], loss: 0.001408, mae: 0.041379, mean_q: 1.175030
 479027/1000000: episode: 4791, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 59.344, mean reward: 0.593 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.049, 10.178], loss: 0.001370, mae: 0.040239, mean_q: 1.173305
 479127/1000000: episode: 4792, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 58.295, mean reward: 0.583 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.432, 10.141], loss: 0.001393, mae: 0.041097, mean_q: 1.179334
 479227/1000000: episode: 4793, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 60.553, mean reward: 0.606 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.383], loss: 0.001452, mae: 0.041900, mean_q: 1.175112
 479327/1000000: episode: 4794, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 58.212, mean reward: 0.582 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.479, 10.098], loss: 0.001462, mae: 0.041892, mean_q: 1.174623
 479427/1000000: episode: 4795, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 57.937, mean reward: 0.579 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.695, 10.256], loss: 0.001424, mae: 0.041131, mean_q: 1.175097
 479527/1000000: episode: 4796, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 61.462, mean reward: 0.615 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.557, 10.157], loss: 0.001429, mae: 0.041271, mean_q: 1.175986
 479627/1000000: episode: 4797, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 58.065, mean reward: 0.581 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.579, 10.143], loss: 0.001405, mae: 0.040820, mean_q: 1.176903
 479727/1000000: episode: 4798, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.228, mean reward: 0.572 [0.504, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.838, 10.206], loss: 0.001447, mae: 0.041882, mean_q: 1.174153
 479827/1000000: episode: 4799, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.450, mean reward: 0.585 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.970, 10.232], loss: 0.001411, mae: 0.040431, mean_q: 1.174521
 479927/1000000: episode: 4800, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.953, mean reward: 0.580 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.001460, mae: 0.041712, mean_q: 1.174300
 480027/1000000: episode: 4801, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.644, mean reward: 0.586 [0.501, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.598, 10.098], loss: 0.001422, mae: 0.040907, mean_q: 1.170551
 480127/1000000: episode: 4802, duration: 1.660s, episode steps: 100, steps per second: 60, episode reward: 59.856, mean reward: 0.599 [0.514, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.963, 10.337], loss: 0.001470, mae: 0.041665, mean_q: 1.172933
 480227/1000000: episode: 4803, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.414, mean reward: 0.584 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.023, 10.098], loss: 0.001519, mae: 0.042233, mean_q: 1.170356
 480327/1000000: episode: 4804, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 55.876, mean reward: 0.559 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.947, 10.098], loss: 0.001441, mae: 0.041655, mean_q: 1.171698
 480427/1000000: episode: 4805, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 58.139, mean reward: 0.581 [0.506, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.098], loss: 0.001484, mae: 0.042109, mean_q: 1.167780
 480527/1000000: episode: 4806, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 56.404, mean reward: 0.564 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.611, 10.098], loss: 0.001430, mae: 0.040843, mean_q: 1.171018
 480627/1000000: episode: 4807, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 56.752, mean reward: 0.568 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.561, 10.098], loss: 0.001409, mae: 0.040458, mean_q: 1.167407
 480727/1000000: episode: 4808, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 61.617, mean reward: 0.616 [0.510, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.058, 10.098], loss: 0.001393, mae: 0.040816, mean_q: 1.165262
 480827/1000000: episode: 4809, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.708, mean reward: 0.587 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.240, 10.098], loss: 0.001458, mae: 0.041275, mean_q: 1.165396
 480927/1000000: episode: 4810, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 61.423, mean reward: 0.614 [0.521, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.038, 10.098], loss: 0.001428, mae: 0.040773, mean_q: 1.166580
 481027/1000000: episode: 4811, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 55.978, mean reward: 0.560 [0.500, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.065, 10.098], loss: 0.001395, mae: 0.040489, mean_q: 1.163990
 481127/1000000: episode: 4812, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 61.426, mean reward: 0.614 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.343], loss: 0.001417, mae: 0.041278, mean_q: 1.164343
 481227/1000000: episode: 4813, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 60.559, mean reward: 0.606 [0.519, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.079, 10.351], loss: 0.001405, mae: 0.041096, mean_q: 1.164203
 481327/1000000: episode: 4814, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 61.809, mean reward: 0.618 [0.506, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.783, 10.411], loss: 0.001424, mae: 0.041194, mean_q: 1.168274
 481427/1000000: episode: 4815, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 63.724, mean reward: 0.637 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.276, 10.264], loss: 0.001508, mae: 0.042237, mean_q: 1.169979
 481527/1000000: episode: 4816, duration: 1.296s, episode steps: 100, steps per second: 77, episode reward: 58.569, mean reward: 0.586 [0.512, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.745, 10.098], loss: 0.001513, mae: 0.042105, mean_q: 1.170832
 481627/1000000: episode: 4817, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 58.546, mean reward: 0.585 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.098], loss: 0.001438, mae: 0.041496, mean_q: 1.164566
 481727/1000000: episode: 4818, duration: 1.329s, episode steps: 100, steps per second: 75, episode reward: 58.822, mean reward: 0.588 [0.512, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.001426, mae: 0.040920, mean_q: 1.167707
 481827/1000000: episode: 4819, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 57.654, mean reward: 0.577 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.070, 10.262], loss: 0.001371, mae: 0.040068, mean_q: 1.166264
 481927/1000000: episode: 4820, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 57.802, mean reward: 0.578 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.213, 10.098], loss: 0.001323, mae: 0.040003, mean_q: 1.163768
 482027/1000000: episode: 4821, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.504, mean reward: 0.595 [0.507, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.991, 10.166], loss: 0.001404, mae: 0.040501, mean_q: 1.166375
 482127/1000000: episode: 4822, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.644, mean reward: 0.576 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.788, 10.340], loss: 0.001453, mae: 0.041760, mean_q: 1.166660
 482227/1000000: episode: 4823, duration: 1.261s, episode steps: 100, steps per second: 79, episode reward: 58.739, mean reward: 0.587 [0.499, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.108, 10.098], loss: 0.001443, mae: 0.041458, mean_q: 1.165217
 482327/1000000: episode: 4824, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 60.543, mean reward: 0.605 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.950, 10.098], loss: 0.001375, mae: 0.039935, mean_q: 1.165281
 482427/1000000: episode: 4825, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 58.248, mean reward: 0.582 [0.503, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.452, 10.219], loss: 0.001525, mae: 0.041899, mean_q: 1.170952
 482527/1000000: episode: 4826, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 57.906, mean reward: 0.579 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.098], loss: 0.001519, mae: 0.042113, mean_q: 1.169084
 482627/1000000: episode: 4827, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 57.821, mean reward: 0.578 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.910, 10.151], loss: 0.001487, mae: 0.041039, mean_q: 1.162304
 482727/1000000: episode: 4828, duration: 1.299s, episode steps: 100, steps per second: 77, episode reward: 59.768, mean reward: 0.598 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.869, 10.291], loss: 0.001383, mae: 0.040133, mean_q: 1.164724
 482827/1000000: episode: 4829, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 61.935, mean reward: 0.619 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.321, 10.098], loss: 0.001403, mae: 0.040963, mean_q: 1.160891
 482927/1000000: episode: 4830, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 60.485, mean reward: 0.605 [0.512, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.079, 10.098], loss: 0.001479, mae: 0.041860, mean_q: 1.167046
 483027/1000000: episode: 4831, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 57.401, mean reward: 0.574 [0.498, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.305, 10.170], loss: 0.001468, mae: 0.041170, mean_q: 1.163597
 483127/1000000: episode: 4832, duration: 1.347s, episode steps: 100, steps per second: 74, episode reward: 61.500, mean reward: 0.615 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.841, 10.152], loss: 0.001582, mae: 0.042386, mean_q: 1.167182
 483227/1000000: episode: 4833, duration: 1.369s, episode steps: 100, steps per second: 73, episode reward: 65.538, mean reward: 0.655 [0.503, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.020, 10.432], loss: 0.001616, mae: 0.042953, mean_q: 1.167071
 483327/1000000: episode: 4834, duration: 1.333s, episode steps: 100, steps per second: 75, episode reward: 60.183, mean reward: 0.602 [0.514, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.990, 10.098], loss: 0.001576, mae: 0.042724, mean_q: 1.167622
 483427/1000000: episode: 4835, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.586, mean reward: 0.596 [0.520, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.739, 10.107], loss: 0.001496, mae: 0.042023, mean_q: 1.165287
 483527/1000000: episode: 4836, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.830, mean reward: 0.588 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.561, 10.098], loss: 0.001501, mae: 0.041846, mean_q: 1.170977
 483627/1000000: episode: 4837, duration: 1.015s, episode steps: 100, steps per second: 98, episode reward: 57.303, mean reward: 0.573 [0.511, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.122, 10.098], loss: 0.001533, mae: 0.042222, mean_q: 1.170561
 483727/1000000: episode: 4838, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.459, mean reward: 0.575 [0.500, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.459, 10.108], loss: 0.001538, mae: 0.042346, mean_q: 1.170216
 483827/1000000: episode: 4839, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 58.571, mean reward: 0.586 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.862, 10.201], loss: 0.001426, mae: 0.040623, mean_q: 1.168521
 483927/1000000: episode: 4840, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 57.782, mean reward: 0.578 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.351, 10.192], loss: 0.001560, mae: 0.042635, mean_q: 1.164913
 484027/1000000: episode: 4841, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 57.358, mean reward: 0.574 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.134, 10.205], loss: 0.001446, mae: 0.041337, mean_q: 1.167406
 484127/1000000: episode: 4842, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 58.883, mean reward: 0.589 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.426, 10.098], loss: 0.001641, mae: 0.043891, mean_q: 1.170257
 484227/1000000: episode: 4843, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.561, mean reward: 0.596 [0.514, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.505, 10.187], loss: 0.001501, mae: 0.041742, mean_q: 1.170304
 484327/1000000: episode: 4844, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 58.216, mean reward: 0.582 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.098], loss: 0.001573, mae: 0.042676, mean_q: 1.168141
 484427/1000000: episode: 4845, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 61.386, mean reward: 0.614 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.590, 10.098], loss: 0.001473, mae: 0.041752, mean_q: 1.168735
 484527/1000000: episode: 4846, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.291, mean reward: 0.583 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.862, 10.098], loss: 0.001424, mae: 0.040637, mean_q: 1.165388
 484627/1000000: episode: 4847, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 60.860, mean reward: 0.609 [0.499, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.361, 10.098], loss: 0.001490, mae: 0.042070, mean_q: 1.172164
 484727/1000000: episode: 4848, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.226, mean reward: 0.592 [0.515, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.659, 10.351], loss: 0.001621, mae: 0.042757, mean_q: 1.174259
 484827/1000000: episode: 4849, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.922, mean reward: 0.579 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.349, 10.098], loss: 0.001568, mae: 0.042400, mean_q: 1.169601
 484927/1000000: episode: 4850, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 57.948, mean reward: 0.579 [0.510, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.527, 10.098], loss: 0.001603, mae: 0.043144, mean_q: 1.170987
 485027/1000000: episode: 4851, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.303, mean reward: 0.583 [0.503, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.208, 10.098], loss: 0.001492, mae: 0.041575, mean_q: 1.170061
 485127/1000000: episode: 4852, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.801, mean reward: 0.588 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.751, 10.129], loss: 0.001523, mae: 0.042760, mean_q: 1.169463
 485227/1000000: episode: 4853, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.455, mean reward: 0.575 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.920, 10.098], loss: 0.001613, mae: 0.043019, mean_q: 1.168484
 485327/1000000: episode: 4854, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 60.713, mean reward: 0.607 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.735, 10.098], loss: 0.001518, mae: 0.041940, mean_q: 1.172372
 485427/1000000: episode: 4855, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 60.646, mean reward: 0.606 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.495, 10.245], loss: 0.001519, mae: 0.041837, mean_q: 1.169621
 485527/1000000: episode: 4856, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 58.034, mean reward: 0.580 [0.498, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.159, 10.098], loss: 0.001574, mae: 0.042468, mean_q: 1.172947
 485627/1000000: episode: 4857, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: 59.666, mean reward: 0.597 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.888, 10.413], loss: 0.001535, mae: 0.042676, mean_q: 1.173152
 485727/1000000: episode: 4858, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 59.107, mean reward: 0.591 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.874, 10.103], loss: 0.001503, mae: 0.041482, mean_q: 1.174872
 485827/1000000: episode: 4859, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 61.172, mean reward: 0.612 [0.514, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.995, 10.274], loss: 0.001564, mae: 0.042574, mean_q: 1.172166
 485927/1000000: episode: 4860, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.748, mean reward: 0.577 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.153, 10.116], loss: 0.001546, mae: 0.042820, mean_q: 1.172492
 486027/1000000: episode: 4861, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.450, mean reward: 0.585 [0.499, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.783, 10.137], loss: 0.001455, mae: 0.041526, mean_q: 1.174806
 486127/1000000: episode: 4862, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.122, mean reward: 0.591 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.562, 10.317], loss: 0.001623, mae: 0.042989, mean_q: 1.169160
 486227/1000000: episode: 4863, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 59.631, mean reward: 0.596 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.976, 10.208], loss: 0.001548, mae: 0.042678, mean_q: 1.172768
 486327/1000000: episode: 4864, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 57.928, mean reward: 0.579 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.084, 10.098], loss: 0.001600, mae: 0.043490, mean_q: 1.171552
 486427/1000000: episode: 4865, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 59.168, mean reward: 0.592 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.271], loss: 0.001586, mae: 0.043307, mean_q: 1.167986
 486527/1000000: episode: 4866, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.679, mean reward: 0.587 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.276], loss: 0.001570, mae: 0.042552, mean_q: 1.168956
 486627/1000000: episode: 4867, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: 56.430, mean reward: 0.564 [0.501, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.813, 10.145], loss: 0.001484, mae: 0.041101, mean_q: 1.168219
 486727/1000000: episode: 4868, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.589, mean reward: 0.586 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.321, 10.098], loss: 0.001460, mae: 0.041403, mean_q: 1.169938
 486827/1000000: episode: 4869, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 56.425, mean reward: 0.564 [0.498, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.490, 10.098], loss: 0.001493, mae: 0.041986, mean_q: 1.166203
 486927/1000000: episode: 4870, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 60.668, mean reward: 0.607 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.554, 10.388], loss: 0.001552, mae: 0.042429, mean_q: 1.164953
 487027/1000000: episode: 4871, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.704, mean reward: 0.587 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.618, 10.098], loss: 0.001628, mae: 0.043429, mean_q: 1.168759
 487127/1000000: episode: 4872, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.167, mean reward: 0.582 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.371, 10.193], loss: 0.001638, mae: 0.043649, mean_q: 1.168385
 487227/1000000: episode: 4873, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.612, mean reward: 0.586 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.303, 10.231], loss: 0.001586, mae: 0.042767, mean_q: 1.167915
 487327/1000000: episode: 4874, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 60.781, mean reward: 0.608 [0.509, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.616, 10.432], loss: 0.001564, mae: 0.042778, mean_q: 1.171252
 487427/1000000: episode: 4875, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 56.962, mean reward: 0.570 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.669, 10.160], loss: 0.001551, mae: 0.042684, mean_q: 1.171753
 487527/1000000: episode: 4876, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 56.091, mean reward: 0.561 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.474, 10.213], loss: 0.001567, mae: 0.042950, mean_q: 1.171217
 487627/1000000: episode: 4877, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 60.750, mean reward: 0.607 [0.504, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.688, 10.265], loss: 0.001503, mae: 0.041510, mean_q: 1.170933
 487727/1000000: episode: 4878, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 61.606, mean reward: 0.616 [0.515, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.383], loss: 0.001469, mae: 0.041576, mean_q: 1.168512
 487827/1000000: episode: 4879, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 61.177, mean reward: 0.612 [0.512, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.212, 10.232], loss: 0.001409, mae: 0.040588, mean_q: 1.165928
 487927/1000000: episode: 4880, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 60.746, mean reward: 0.607 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.863, 10.098], loss: 0.001454, mae: 0.041672, mean_q: 1.168649
 488027/1000000: episode: 4881, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.104, mean reward: 0.591 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.072, 10.106], loss: 0.001522, mae: 0.042582, mean_q: 1.172095
 488127/1000000: episode: 4882, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.523, mean reward: 0.585 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.980, 10.131], loss: 0.001416, mae: 0.040891, mean_q: 1.167260
 488227/1000000: episode: 4883, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.017, mean reward: 0.600 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.607, 10.099], loss: 0.001438, mae: 0.041258, mean_q: 1.165749
 488327/1000000: episode: 4884, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.902, mean reward: 0.589 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.051, 10.312], loss: 0.001486, mae: 0.042138, mean_q: 1.164595
 488427/1000000: episode: 4885, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 60.108, mean reward: 0.601 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.098], loss: 0.001482, mae: 0.042072, mean_q: 1.165751
 488527/1000000: episode: 4886, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.605, mean reward: 0.606 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.333, 10.098], loss: 0.001400, mae: 0.040779, mean_q: 1.168280
 488627/1000000: episode: 4887, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 59.308, mean reward: 0.593 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.988, 10.098], loss: 0.001486, mae: 0.041964, mean_q: 1.173121
 488727/1000000: episode: 4888, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.096, mean reward: 0.581 [0.511, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.906, 10.098], loss: 0.001443, mae: 0.041220, mean_q: 1.165865
 488827/1000000: episode: 4889, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 60.337, mean reward: 0.603 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.773, 10.098], loss: 0.001401, mae: 0.040463, mean_q: 1.165906
 488927/1000000: episode: 4890, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 60.806, mean reward: 0.608 [0.506, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.160], loss: 0.001467, mae: 0.041720, mean_q: 1.168980
 489027/1000000: episode: 4891, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 64.171, mean reward: 0.642 [0.522, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.051, 10.098], loss: 0.001480, mae: 0.041747, mean_q: 1.173218
 489127/1000000: episode: 4892, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.898, mean reward: 0.589 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.080, 10.098], loss: 0.001554, mae: 0.043068, mean_q: 1.169099
 489227/1000000: episode: 4893, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 64.261, mean reward: 0.643 [0.511, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.602, 10.400], loss: 0.001472, mae: 0.041698, mean_q: 1.170741
 489327/1000000: episode: 4894, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 58.860, mean reward: 0.589 [0.500, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.740, 10.098], loss: 0.001415, mae: 0.041158, mean_q: 1.175079
 489427/1000000: episode: 4895, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.134, mean reward: 0.581 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.914, 10.098], loss: 0.001460, mae: 0.041867, mean_q: 1.174139
 489527/1000000: episode: 4896, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 57.550, mean reward: 0.576 [0.499, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.294, 10.126], loss: 0.001340, mae: 0.039835, mean_q: 1.173943
 489627/1000000: episode: 4897, duration: 0.830s, episode steps: 100, steps per second: 121, episode reward: 56.537, mean reward: 0.565 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.178, 10.202], loss: 0.001383, mae: 0.040445, mean_q: 1.169971
 489727/1000000: episode: 4898, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.055, mean reward: 0.581 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.995, 10.149], loss: 0.001369, mae: 0.040492, mean_q: 1.172949
 489827/1000000: episode: 4899, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 57.744, mean reward: 0.577 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.984, 10.098], loss: 0.001427, mae: 0.041401, mean_q: 1.169824
 489927/1000000: episode: 4900, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 62.272, mean reward: 0.623 [0.506, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.256, 10.098], loss: 0.001419, mae: 0.041517, mean_q: 1.172168
 490027/1000000: episode: 4901, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.963, mean reward: 0.590 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.098], loss: 0.001402, mae: 0.040920, mean_q: 1.169462
 490127/1000000: episode: 4902, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.401, mean reward: 0.584 [0.501, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.171, 10.209], loss: 0.001418, mae: 0.041117, mean_q: 1.172051
 490227/1000000: episode: 4903, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 59.535, mean reward: 0.595 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.627, 10.098], loss: 0.001391, mae: 0.040699, mean_q: 1.176478
 490327/1000000: episode: 4904, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 60.026, mean reward: 0.600 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.877, 10.288], loss: 0.001346, mae: 0.040631, mean_q: 1.171760
 490427/1000000: episode: 4905, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 56.651, mean reward: 0.567 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.302, 10.098], loss: 0.001363, mae: 0.040369, mean_q: 1.171347
 490527/1000000: episode: 4906, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.130, mean reward: 0.581 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.483, 10.272], loss: 0.001345, mae: 0.040241, mean_q: 1.170303
 490627/1000000: episode: 4907, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 59.543, mean reward: 0.595 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.781, 10.098], loss: 0.001326, mae: 0.039919, mean_q: 1.169611
 490727/1000000: episode: 4908, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 60.211, mean reward: 0.602 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.755, 10.192], loss: 0.001511, mae: 0.042126, mean_q: 1.174844
 490827/1000000: episode: 4909, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 58.028, mean reward: 0.580 [0.499, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.954, 10.098], loss: 0.001390, mae: 0.041010, mean_q: 1.174003
 490927/1000000: episode: 4910, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 59.419, mean reward: 0.594 [0.508, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.932, 10.098], loss: 0.001446, mae: 0.041850, mean_q: 1.174074
 491027/1000000: episode: 4911, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 57.329, mean reward: 0.573 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.146, 10.098], loss: 0.001329, mae: 0.039662, mean_q: 1.172161
 491127/1000000: episode: 4912, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.609, mean reward: 0.586 [0.518, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.849, 10.222], loss: 0.001341, mae: 0.040078, mean_q: 1.170370
 491227/1000000: episode: 4913, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 57.531, mean reward: 0.575 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.963, 10.098], loss: 0.001320, mae: 0.039676, mean_q: 1.170820
 491327/1000000: episode: 4914, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 59.136, mean reward: 0.591 [0.498, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.881, 10.098], loss: 0.001309, mae: 0.039500, mean_q: 1.171747
 491427/1000000: episode: 4915, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.771, mean reward: 0.588 [0.506, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.228, 10.140], loss: 0.001454, mae: 0.041058, mean_q: 1.172314
 491527/1000000: episode: 4916, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.413, mean reward: 0.584 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.328], loss: 0.001397, mae: 0.039939, mean_q: 1.169743
 491627/1000000: episode: 4917, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.872, mean reward: 0.589 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.841, 10.259], loss: 0.001384, mae: 0.041115, mean_q: 1.172243
 491727/1000000: episode: 4918, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.296, mean reward: 0.593 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.254, 10.128], loss: 0.001493, mae: 0.042067, mean_q: 1.171968
 491827/1000000: episode: 4919, duration: 0.844s, episode steps: 100, steps per second: 119, episode reward: 59.880, mean reward: 0.599 [0.510, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.120, 10.098], loss: 0.001407, mae: 0.041030, mean_q: 1.173088
 491927/1000000: episode: 4920, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 60.456, mean reward: 0.605 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.068, 10.136], loss: 0.001390, mae: 0.040680, mean_q: 1.174893
 492027/1000000: episode: 4921, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 59.588, mean reward: 0.596 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.219, 10.098], loss: 0.001353, mae: 0.040414, mean_q: 1.174610
 492127/1000000: episode: 4922, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 59.371, mean reward: 0.594 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.686, 10.098], loss: 0.001442, mae: 0.041055, mean_q: 1.174652
 492227/1000000: episode: 4923, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 59.369, mean reward: 0.594 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.561, 10.212], loss: 0.001505, mae: 0.042097, mean_q: 1.179193
 492327/1000000: episode: 4924, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 60.579, mean reward: 0.606 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.779, 10.398], loss: 0.001444, mae: 0.041306, mean_q: 1.175372
 492427/1000000: episode: 4925, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 58.139, mean reward: 0.581 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.761, 10.098], loss: 0.001469, mae: 0.041189, mean_q: 1.176472
 492527/1000000: episode: 4926, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 57.999, mean reward: 0.580 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.968, 10.115], loss: 0.001480, mae: 0.042053, mean_q: 1.176208
 492627/1000000: episode: 4927, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 57.559, mean reward: 0.576 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.375, 10.098], loss: 0.001527, mae: 0.042367, mean_q: 1.176180
 492727/1000000: episode: 4928, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 58.806, mean reward: 0.588 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.226], loss: 0.001423, mae: 0.041044, mean_q: 1.174725
 492827/1000000: episode: 4929, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 57.122, mean reward: 0.571 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.254], loss: 0.001543, mae: 0.042389, mean_q: 1.172057
 492927/1000000: episode: 4930, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 59.396, mean reward: 0.594 [0.501, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.373, 10.098], loss: 0.001580, mae: 0.043033, mean_q: 1.171827
 493027/1000000: episode: 4931, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 58.667, mean reward: 0.587 [0.514, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.630, 10.205], loss: 0.001552, mae: 0.042485, mean_q: 1.174334
 493127/1000000: episode: 4932, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 56.937, mean reward: 0.569 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.922, 10.120], loss: 0.001527, mae: 0.042327, mean_q: 1.171959
 493227/1000000: episode: 4933, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.497, mean reward: 0.585 [0.511, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.132], loss: 0.001497, mae: 0.041910, mean_q: 1.168116
 493327/1000000: episode: 4934, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.176, mean reward: 0.572 [0.507, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.696, 10.282], loss: 0.001508, mae: 0.042061, mean_q: 1.167499
 493427/1000000: episode: 4935, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 57.495, mean reward: 0.575 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.404, 10.111], loss: 0.001483, mae: 0.042127, mean_q: 1.168882
 493527/1000000: episode: 4936, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 57.511, mean reward: 0.575 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.931, 10.098], loss: 0.001415, mae: 0.041280, mean_q: 1.167096
 493627/1000000: episode: 4937, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.507, mean reward: 0.575 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.131, 10.098], loss: 0.001513, mae: 0.041873, mean_q: 1.165420
 493727/1000000: episode: 4938, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 58.609, mean reward: 0.586 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.722, 10.098], loss: 0.001412, mae: 0.040984, mean_q: 1.164167
 493827/1000000: episode: 4939, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 57.490, mean reward: 0.575 [0.509, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.909, 10.098], loss: 0.001527, mae: 0.042225, mean_q: 1.167097
 493927/1000000: episode: 4940, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 60.042, mean reward: 0.600 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.788, 10.098], loss: 0.001432, mae: 0.041433, mean_q: 1.161950
 494027/1000000: episode: 4941, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.801, mean reward: 0.588 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.785, 10.098], loss: 0.001505, mae: 0.041363, mean_q: 1.161926
 494127/1000000: episode: 4942, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 59.388, mean reward: 0.594 [0.502, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.877, 10.098], loss: 0.001555, mae: 0.043267, mean_q: 1.163125
 494227/1000000: episode: 4943, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.452, mean reward: 0.595 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.196], loss: 0.001473, mae: 0.041830, mean_q: 1.160295
 494327/1000000: episode: 4944, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 60.383, mean reward: 0.604 [0.508, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.175, 10.098], loss: 0.001555, mae: 0.042622, mean_q: 1.159689
 494427/1000000: episode: 4945, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.198, mean reward: 0.592 [0.518, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.902, 10.098], loss: 0.001579, mae: 0.043106, mean_q: 1.160054
 494527/1000000: episode: 4946, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 57.888, mean reward: 0.579 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.985, 10.137], loss: 0.001542, mae: 0.042373, mean_q: 1.164253
 494627/1000000: episode: 4947, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 57.511, mean reward: 0.575 [0.509, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.111], loss: 0.001362, mae: 0.040398, mean_q: 1.162696
 494727/1000000: episode: 4948, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 58.698, mean reward: 0.587 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.042, 10.098], loss: 0.001520, mae: 0.042507, mean_q: 1.161500
 494827/1000000: episode: 4949, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 61.175, mean reward: 0.612 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.052, 10.098], loss: 0.001497, mae: 0.042179, mean_q: 1.165051
 494927/1000000: episode: 4950, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 58.122, mean reward: 0.581 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.829, 10.098], loss: 0.001549, mae: 0.042874, mean_q: 1.164753
 495027/1000000: episode: 4951, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 56.645, mean reward: 0.566 [0.503, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.082, 10.098], loss: 0.001453, mae: 0.041786, mean_q: 1.159924
 495127/1000000: episode: 4952, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 56.666, mean reward: 0.567 [0.500, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.594, 10.098], loss: 0.001457, mae: 0.041974, mean_q: 1.160052
 495227/1000000: episode: 4953, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 58.562, mean reward: 0.586 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.516, 10.098], loss: 0.001413, mae: 0.041170, mean_q: 1.159718
 495327/1000000: episode: 4954, duration: 0.985s, episode steps: 100, steps per second: 101, episode reward: 58.746, mean reward: 0.587 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.836, 10.098], loss: 0.001564, mae: 0.042916, mean_q: 1.158942
 495427/1000000: episode: 4955, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 56.128, mean reward: 0.561 [0.508, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.057, 10.110], loss: 0.001404, mae: 0.041103, mean_q: 1.157599
 495527/1000000: episode: 4956, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 59.185, mean reward: 0.592 [0.502, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.098], loss: 0.001455, mae: 0.041272, mean_q: 1.156475
 495627/1000000: episode: 4957, duration: 0.966s, episode steps: 100, steps per second: 103, episode reward: 59.529, mean reward: 0.595 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.318, 10.212], loss: 0.001489, mae: 0.042432, mean_q: 1.163613
 495727/1000000: episode: 4958, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 60.982, mean reward: 0.610 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.542, 10.246], loss: 0.001492, mae: 0.042365, mean_q: 1.160090
 495827/1000000: episode: 4959, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 57.887, mean reward: 0.579 [0.501, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.798, 10.164], loss: 0.001440, mae: 0.041489, mean_q: 1.160094
 495927/1000000: episode: 4960, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 58.554, mean reward: 0.586 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.421, 10.181], loss: 0.001448, mae: 0.041573, mean_q: 1.162304
 496027/1000000: episode: 4961, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 59.894, mean reward: 0.599 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.958, 10.152], loss: 0.001511, mae: 0.042576, mean_q: 1.161616
 496127/1000000: episode: 4962, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.677, mean reward: 0.597 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.098], loss: 0.001536, mae: 0.042547, mean_q: 1.160024
 496227/1000000: episode: 4963, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 62.212, mean reward: 0.622 [0.505, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.138, 10.098], loss: 0.001435, mae: 0.040946, mean_q: 1.161250
 496327/1000000: episode: 4964, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.563, mean reward: 0.586 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.690, 10.289], loss: 0.001505, mae: 0.042028, mean_q: 1.163939
 496427/1000000: episode: 4965, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: 60.005, mean reward: 0.600 [0.505, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.915, 10.098], loss: 0.001521, mae: 0.042589, mean_q: 1.163959
 496527/1000000: episode: 4966, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 56.212, mean reward: 0.562 [0.498, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.163, 10.098], loss: 0.001516, mae: 0.043065, mean_q: 1.167894
 496627/1000000: episode: 4967, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 61.060, mean reward: 0.611 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.866, 10.151], loss: 0.001558, mae: 0.043072, mean_q: 1.164247
 496727/1000000: episode: 4968, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 58.654, mean reward: 0.587 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.978, 10.098], loss: 0.001477, mae: 0.042064, mean_q: 1.158181
 496827/1000000: episode: 4969, duration: 0.966s, episode steps: 100, steps per second: 103, episode reward: 60.959, mean reward: 0.610 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.579, 10.300], loss: 0.001497, mae: 0.041988, mean_q: 1.162094
 496927/1000000: episode: 4970, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 59.151, mean reward: 0.592 [0.510, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.832, 10.107], loss: 0.001526, mae: 0.042388, mean_q: 1.162751
 497027/1000000: episode: 4971, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 61.030, mean reward: 0.610 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.800, 10.098], loss: 0.001493, mae: 0.042149, mean_q: 1.165240
 497127/1000000: episode: 4972, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 57.743, mean reward: 0.577 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.877, 10.369], loss: 0.001470, mae: 0.041411, mean_q: 1.158555
 497227/1000000: episode: 4973, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 56.927, mean reward: 0.569 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.956, 10.179], loss: 0.001589, mae: 0.042955, mean_q: 1.162723
 497327/1000000: episode: 4974, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 60.074, mean reward: 0.601 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.583, 10.220], loss: 0.001529, mae: 0.042645, mean_q: 1.160884
 497427/1000000: episode: 4975, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 60.625, mean reward: 0.606 [0.498, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.622, 10.098], loss: 0.001431, mae: 0.041393, mean_q: 1.163976
 497527/1000000: episode: 4976, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 57.854, mean reward: 0.579 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.678, 10.234], loss: 0.001425, mae: 0.040635, mean_q: 1.163496
 497627/1000000: episode: 4977, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.506, mean reward: 0.585 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.252, 10.105], loss: 0.001524, mae: 0.042433, mean_q: 1.161643
 497727/1000000: episode: 4978, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 59.642, mean reward: 0.596 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.560, 10.377], loss: 0.001525, mae: 0.041754, mean_q: 1.158820
 497827/1000000: episode: 4979, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 56.935, mean reward: 0.569 [0.508, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.071, 10.166], loss: 0.001421, mae: 0.041762, mean_q: 1.162186
 497927/1000000: episode: 4980, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 61.607, mean reward: 0.616 [0.506, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.281, 10.098], loss: 0.001353, mae: 0.040304, mean_q: 1.160343
 498027/1000000: episode: 4981, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 61.292, mean reward: 0.613 [0.506, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.791, 10.294], loss: 0.001468, mae: 0.041762, mean_q: 1.163815
 498127/1000000: episode: 4982, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 59.449, mean reward: 0.594 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.480, 10.289], loss: 0.001608, mae: 0.043500, mean_q: 1.166859
 498227/1000000: episode: 4983, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 59.489, mean reward: 0.595 [0.497, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.824, 10.346], loss: 0.001470, mae: 0.041747, mean_q: 1.169028
 498327/1000000: episode: 4984, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.243, mean reward: 0.582 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.098], loss: 0.001606, mae: 0.042797, mean_q: 1.162800
 498427/1000000: episode: 4985, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.672, mean reward: 0.577 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.014, 10.176], loss: 0.001477, mae: 0.041722, mean_q: 1.165487
 498527/1000000: episode: 4986, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 64.243, mean reward: 0.642 [0.501, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.760, 10.098], loss: 0.001419, mae: 0.041226, mean_q: 1.166723
 498627/1000000: episode: 4987, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 60.765, mean reward: 0.608 [0.506, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.809, 10.098], loss: 0.001510, mae: 0.042205, mean_q: 1.168165
 498727/1000000: episode: 4988, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 57.841, mean reward: 0.578 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.820, 10.180], loss: 0.001533, mae: 0.042699, mean_q: 1.168278
 498827/1000000: episode: 4989, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 57.515, mean reward: 0.575 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.174, 10.098], loss: 0.001637, mae: 0.043378, mean_q: 1.169514
 498927/1000000: episode: 4990, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 57.258, mean reward: 0.573 [0.504, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.200, 10.121], loss: 0.001547, mae: 0.042599, mean_q: 1.166999
 499027/1000000: episode: 4991, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 57.940, mean reward: 0.579 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.817, 10.098], loss: 0.001523, mae: 0.042487, mean_q: 1.164185
 499127/1000000: episode: 4992, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 57.576, mean reward: 0.576 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.699, 10.160], loss: 0.001386, mae: 0.041183, mean_q: 1.167343
 499227/1000000: episode: 4993, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: 57.535, mean reward: 0.575 [0.506, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.861, 10.098], loss: 0.001506, mae: 0.042651, mean_q: 1.166810
 499327/1000000: episode: 4994, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.412, mean reward: 0.584 [0.497, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.885, 10.302], loss: 0.001332, mae: 0.039760, mean_q: 1.167179
 499427/1000000: episode: 4995, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 61.893, mean reward: 0.619 [0.510, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.098], loss: 0.001496, mae: 0.041745, mean_q: 1.166974
 499527/1000000: episode: 4996, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 57.964, mean reward: 0.580 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.519, 10.262], loss: 0.001506, mae: 0.042124, mean_q: 1.166296
 499627/1000000: episode: 4997, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 56.806, mean reward: 0.568 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.098], loss: 0.001502, mae: 0.042143, mean_q: 1.170773
 499727/1000000: episode: 4998, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 59.003, mean reward: 0.590 [0.499, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.542, 10.229], loss: 0.001581, mae: 0.042608, mean_q: 1.167647
 499827/1000000: episode: 4999, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 59.796, mean reward: 0.598 [0.514, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.420, 10.250], loss: 0.001447, mae: 0.040711, mean_q: 1.164931
 499927/1000000: episode: 5000, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 57.548, mean reward: 0.575 [0.507, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.824, 10.113], loss: 0.001520, mae: 0.042151, mean_q: 1.170081
 500027/1000000: episode: 5001, duration: 0.866s, episode steps: 100, steps per second: 116, episode reward: 62.125, mean reward: 0.621 [0.513, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.098], loss: 0.001498, mae: 0.042240, mean_q: 1.169913
 500127/1000000: episode: 5002, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 57.282, mean reward: 0.573 [0.511, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.098], loss: 0.001497, mae: 0.042349, mean_q: 1.169370
 500227/1000000: episode: 5003, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.580, mean reward: 0.586 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.913, 10.170], loss: 0.001510, mae: 0.042364, mean_q: 1.168364
 500327/1000000: episode: 5004, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.531, mean reward: 0.585 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.196, 10.153], loss: 0.001556, mae: 0.042835, mean_q: 1.170515
 500427/1000000: episode: 5005, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.684, mean reward: 0.607 [0.521, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.052, 10.334], loss: 0.001438, mae: 0.041152, mean_q: 1.166735
 500527/1000000: episode: 5006, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 57.908, mean reward: 0.579 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.098], loss: 0.001510, mae: 0.042164, mean_q: 1.170593
 500627/1000000: episode: 5007, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 57.211, mean reward: 0.572 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.489, 10.098], loss: 0.001457, mae: 0.041558, mean_q: 1.170292
 500727/1000000: episode: 5008, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.782, mean reward: 0.588 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.827, 10.140], loss: 0.001491, mae: 0.041541, mean_q: 1.164343
 500827/1000000: episode: 5009, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 65.556, mean reward: 0.656 [0.502, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.758, 10.098], loss: 0.001615, mae: 0.043460, mean_q: 1.173284
 500927/1000000: episode: 5010, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 58.980, mean reward: 0.590 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.313, 10.098], loss: 0.001583, mae: 0.043371, mean_q: 1.172757
 501027/1000000: episode: 5011, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.128, mean reward: 0.581 [0.498, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.305, 10.098], loss: 0.001467, mae: 0.041593, mean_q: 1.170815
 501127/1000000: episode: 5012, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.809, mean reward: 0.588 [0.509, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.009, 10.098], loss: 0.001467, mae: 0.041519, mean_q: 1.170257
 501227/1000000: episode: 5013, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 58.920, mean reward: 0.589 [0.517, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.904, 10.287], loss: 0.001470, mae: 0.041749, mean_q: 1.169651
 501327/1000000: episode: 5014, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 57.042, mean reward: 0.570 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.157], loss: 0.001394, mae: 0.040728, mean_q: 1.168683
 501427/1000000: episode: 5015, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 58.189, mean reward: 0.582 [0.517, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.060, 10.098], loss: 0.001374, mae: 0.040340, mean_q: 1.169364
 501527/1000000: episode: 5016, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 60.119, mean reward: 0.601 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.719, 10.204], loss: 0.001493, mae: 0.041824, mean_q: 1.172708
 501627/1000000: episode: 5017, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.742, mean reward: 0.587 [0.512, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.674, 10.239], loss: 0.001406, mae: 0.040462, mean_q: 1.171177
 501727/1000000: episode: 5018, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 62.504, mean reward: 0.625 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.163, 10.098], loss: 0.001388, mae: 0.041060, mean_q: 1.171037
 501827/1000000: episode: 5019, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.878, mean reward: 0.599 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.307], loss: 0.001383, mae: 0.040958, mean_q: 1.171111
 501927/1000000: episode: 5020, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.176, mean reward: 0.572 [0.503, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.098], loss: 0.001487, mae: 0.042284, mean_q: 1.172109
 502027/1000000: episode: 5021, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 63.802, mean reward: 0.638 [0.557, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.335, 10.098], loss: 0.001549, mae: 0.042381, mean_q: 1.170354
 502127/1000000: episode: 5022, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 64.510, mean reward: 0.645 [0.500, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.034, 10.386], loss: 0.001379, mae: 0.040763, mean_q: 1.170515
 502227/1000000: episode: 5023, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 59.654, mean reward: 0.597 [0.513, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.856, 10.098], loss: 0.001439, mae: 0.041498, mean_q: 1.170944
 502327/1000000: episode: 5024, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 60.135, mean reward: 0.601 [0.498, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.639, 10.474], loss: 0.001448, mae: 0.041975, mean_q: 1.174212
 502427/1000000: episode: 5025, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.086, mean reward: 0.591 [0.497, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.370, 10.098], loss: 0.001518, mae: 0.042401, mean_q: 1.173499
 502527/1000000: episode: 5026, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.723, mean reward: 0.587 [0.509, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.986, 10.098], loss: 0.001428, mae: 0.040856, mean_q: 1.173600
 502627/1000000: episode: 5027, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 61.808, mean reward: 0.618 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.460, 10.319], loss: 0.001439, mae: 0.041696, mean_q: 1.176112
 502727/1000000: episode: 5028, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 58.645, mean reward: 0.586 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.219, 10.262], loss: 0.001484, mae: 0.041562, mean_q: 1.173598
 502827/1000000: episode: 5029, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.970, mean reward: 0.590 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.623, 10.098], loss: 0.001533, mae: 0.042721, mean_q: 1.177600
 502927/1000000: episode: 5030, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.989, mean reward: 0.600 [0.529, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.301, 10.266], loss: 0.001462, mae: 0.041549, mean_q: 1.173196
 503027/1000000: episode: 5031, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 59.832, mean reward: 0.598 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.530, 10.098], loss: 0.001407, mae: 0.040979, mean_q: 1.172548
 503127/1000000: episode: 5032, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 60.409, mean reward: 0.604 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.805, 10.428], loss: 0.001470, mae: 0.041508, mean_q: 1.174656
 503227/1000000: episode: 5033, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 58.213, mean reward: 0.582 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.098], loss: 0.001416, mae: 0.040803, mean_q: 1.175113
 503327/1000000: episode: 5034, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.463, mean reward: 0.575 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.716, 10.098], loss: 0.001413, mae: 0.040719, mean_q: 1.172071
 503427/1000000: episode: 5035, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.146, mean reward: 0.601 [0.521, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.908, 10.244], loss: 0.001345, mae: 0.039757, mean_q: 1.178021
 503527/1000000: episode: 5036, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.218, mean reward: 0.582 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.014, 10.136], loss: 0.001390, mae: 0.040833, mean_q: 1.171155
 503627/1000000: episode: 5037, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 59.277, mean reward: 0.593 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.451, 10.098], loss: 0.001389, mae: 0.040524, mean_q: 1.169651
 503727/1000000: episode: 5038, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 60.891, mean reward: 0.609 [0.520, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.438, 10.098], loss: 0.001315, mae: 0.039491, mean_q: 1.169014
 503827/1000000: episode: 5039, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.874, mean reward: 0.589 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.690, 10.098], loss: 0.001488, mae: 0.041776, mean_q: 1.176603
 503927/1000000: episode: 5040, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.090, mean reward: 0.581 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.261], loss: 0.001427, mae: 0.040986, mean_q: 1.174685
 504027/1000000: episode: 5041, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.712, mean reward: 0.597 [0.516, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.128, 10.201], loss: 0.001408, mae: 0.040587, mean_q: 1.174129
 504127/1000000: episode: 5042, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 59.260, mean reward: 0.593 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.513, 10.098], loss: 0.001415, mae: 0.040660, mean_q: 1.177641
 504227/1000000: episode: 5043, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.307, mean reward: 0.583 [0.498, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.114, 10.188], loss: 0.001452, mae: 0.041380, mean_q: 1.176232
 504327/1000000: episode: 5044, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 58.875, mean reward: 0.589 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.614, 10.098], loss: 0.001363, mae: 0.040009, mean_q: 1.176973
 504427/1000000: episode: 5045, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 65.312, mean reward: 0.653 [0.510, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.409 [-0.312, 10.098], loss: 0.001432, mae: 0.041334, mean_q: 1.180751
 504527/1000000: episode: 5046, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.260, mean reward: 0.583 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.161], loss: 0.001484, mae: 0.041550, mean_q: 1.177493
 504627/1000000: episode: 5047, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.786, mean reward: 0.578 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.926, 10.143], loss: 0.001448, mae: 0.041235, mean_q: 1.177948
 504727/1000000: episode: 5048, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 56.346, mean reward: 0.563 [0.503, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.411, 10.149], loss: 0.001478, mae: 0.041656, mean_q: 1.174658
 504827/1000000: episode: 5049, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 57.904, mean reward: 0.579 [0.497, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.797, 10.171], loss: 0.001415, mae: 0.040559, mean_q: 1.178236
 504927/1000000: episode: 5050, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 57.307, mean reward: 0.573 [0.504, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.684, 10.229], loss: 0.001492, mae: 0.041662, mean_q: 1.175681
 505027/1000000: episode: 5051, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.954, mean reward: 0.600 [0.514, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.098], loss: 0.001472, mae: 0.041294, mean_q: 1.176391
 505127/1000000: episode: 5052, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 61.112, mean reward: 0.611 [0.516, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.177, 10.098], loss: 0.001523, mae: 0.042876, mean_q: 1.178947
 505227/1000000: episode: 5053, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.809, mean reward: 0.588 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.952, 10.098], loss: 0.001429, mae: 0.040560, mean_q: 1.178796
 505327/1000000: episode: 5054, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.066, mean reward: 0.581 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.434, 10.098], loss: 0.001372, mae: 0.039838, mean_q: 1.171584
 505427/1000000: episode: 5055, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.838, mean reward: 0.588 [0.508, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.976, 10.234], loss: 0.001586, mae: 0.043228, mean_q: 1.177753
 505527/1000000: episode: 5056, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 58.080, mean reward: 0.581 [0.500, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.033, 10.115], loss: 0.001481, mae: 0.041941, mean_q: 1.176158
 505627/1000000: episode: 5057, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 59.125, mean reward: 0.591 [0.511, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.744, 10.098], loss: 0.001530, mae: 0.042324, mean_q: 1.179283
 505727/1000000: episode: 5058, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 61.611, mean reward: 0.616 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.856, 10.098], loss: 0.001491, mae: 0.041722, mean_q: 1.180628
 505827/1000000: episode: 5059, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.454, mean reward: 0.585 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.442, 10.140], loss: 0.001405, mae: 0.040352, mean_q: 1.171827
 505927/1000000: episode: 5060, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 59.321, mean reward: 0.593 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.125], loss: 0.001437, mae: 0.041024, mean_q: 1.173741
 506027/1000000: episode: 5061, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 57.448, mean reward: 0.574 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.480, 10.216], loss: 0.001534, mae: 0.041762, mean_q: 1.172667
 506127/1000000: episode: 5062, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 60.342, mean reward: 0.603 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.781, 10.098], loss: 0.001498, mae: 0.041543, mean_q: 1.172177
 506227/1000000: episode: 5063, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.590, mean reward: 0.576 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.098], loss: 0.001411, mae: 0.040739, mean_q: 1.176405
 506327/1000000: episode: 5064, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 57.777, mean reward: 0.578 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.290, 10.118], loss: 0.001386, mae: 0.039703, mean_q: 1.174693
 506427/1000000: episode: 5065, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 62.282, mean reward: 0.623 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.389, 10.295], loss: 0.001531, mae: 0.041690, mean_q: 1.175565
 506527/1000000: episode: 5066, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 61.298, mean reward: 0.613 [0.511, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.551, 10.261], loss: 0.001511, mae: 0.042039, mean_q: 1.174180
 506627/1000000: episode: 5067, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.358, mean reward: 0.584 [0.512, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.782, 10.211], loss: 0.001564, mae: 0.043235, mean_q: 1.177192
 506727/1000000: episode: 5068, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.752, mean reward: 0.578 [0.517, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.111, 10.098], loss: 0.001455, mae: 0.041122, mean_q: 1.172794
 506827/1000000: episode: 5069, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 58.779, mean reward: 0.588 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.167, 10.098], loss: 0.001512, mae: 0.041815, mean_q: 1.175922
 506927/1000000: episode: 5070, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 64.589, mean reward: 0.646 [0.508, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.866, 10.098], loss: 0.001474, mae: 0.041521, mean_q: 1.175096
 507027/1000000: episode: 5071, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 62.633, mean reward: 0.626 [0.520, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.020, 10.098], loss: 0.001468, mae: 0.041319, mean_q: 1.176260
 507127/1000000: episode: 5072, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 62.222, mean reward: 0.622 [0.514, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.854, 10.372], loss: 0.001490, mae: 0.041876, mean_q: 1.178689
 507227/1000000: episode: 5073, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.565, mean reward: 0.596 [0.512, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.593, 10.157], loss: 0.001567, mae: 0.042622, mean_q: 1.177030
 507327/1000000: episode: 5074, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 56.420, mean reward: 0.564 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.118, 10.098], loss: 0.001595, mae: 0.043104, mean_q: 1.177099
 507427/1000000: episode: 5075, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.386, mean reward: 0.584 [0.511, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.098], loss: 0.001590, mae: 0.042566, mean_q: 1.172356
 507527/1000000: episode: 5076, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 59.866, mean reward: 0.599 [0.515, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.512, 10.266], loss: 0.001576, mae: 0.042967, mean_q: 1.175544
 507627/1000000: episode: 5077, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 59.483, mean reward: 0.595 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.801, 10.359], loss: 0.001489, mae: 0.042000, mean_q: 1.174869
 507727/1000000: episode: 5078, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 60.941, mean reward: 0.609 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.637, 10.510], loss: 0.001539, mae: 0.042695, mean_q: 1.173788
 507827/1000000: episode: 5079, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 59.164, mean reward: 0.592 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.857, 10.340], loss: 0.001537, mae: 0.042752, mean_q: 1.176895
 507927/1000000: episode: 5080, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.467, mean reward: 0.595 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.262, 10.260], loss: 0.001460, mae: 0.041668, mean_q: 1.173757
 508027/1000000: episode: 5081, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 57.524, mean reward: 0.575 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.229, 10.112], loss: 0.001447, mae: 0.041574, mean_q: 1.174057
 508127/1000000: episode: 5082, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.945, mean reward: 0.579 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.098], loss: 0.001471, mae: 0.041971, mean_q: 1.169285
 508227/1000000: episode: 5083, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.628, mean reward: 0.576 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.120, 10.098], loss: 0.001510, mae: 0.041690, mean_q: 1.169101
 508327/1000000: episode: 5084, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.444, mean reward: 0.594 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.213, 10.098], loss: 0.001557, mae: 0.042893, mean_q: 1.172845
 508427/1000000: episode: 5085, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 58.955, mean reward: 0.590 [0.504, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.076, 10.098], loss: 0.001516, mae: 0.042341, mean_q: 1.171246
 508527/1000000: episode: 5086, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.568, mean reward: 0.576 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.188], loss: 0.001529, mae: 0.042205, mean_q: 1.174800
 508627/1000000: episode: 5087, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 61.748, mean reward: 0.617 [0.510, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.227, 10.369], loss: 0.001583, mae: 0.043182, mean_q: 1.170142
 508727/1000000: episode: 5088, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: 57.195, mean reward: 0.572 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.704, 10.098], loss: 0.001525, mae: 0.041842, mean_q: 1.171576
 508827/1000000: episode: 5089, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 61.064, mean reward: 0.611 [0.524, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.984, 10.280], loss: 0.001590, mae: 0.043294, mean_q: 1.172489
 508927/1000000: episode: 5090, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 57.772, mean reward: 0.578 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-2.395, 10.137], loss: 0.001575, mae: 0.042945, mean_q: 1.170813
 509027/1000000: episode: 5091, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 59.008, mean reward: 0.590 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.098], loss: 0.001459, mae: 0.041626, mean_q: 1.175829
 509127/1000000: episode: 5092, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 59.534, mean reward: 0.595 [0.515, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.268], loss: 0.001420, mae: 0.041116, mean_q: 1.170455
 509227/1000000: episode: 5093, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 57.706, mean reward: 0.577 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.540, 10.164], loss: 0.001509, mae: 0.041820, mean_q: 1.172438
 509327/1000000: episode: 5094, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 58.028, mean reward: 0.580 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.353, 10.098], loss: 0.001575, mae: 0.042792, mean_q: 1.172741
 509427/1000000: episode: 5095, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 56.603, mean reward: 0.566 [0.507, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.595, 10.140], loss: 0.001440, mae: 0.041294, mean_q: 1.171363
 509527/1000000: episode: 5096, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: 61.055, mean reward: 0.611 [0.506, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.609, 10.429], loss: 0.001542, mae: 0.042281, mean_q: 1.169667
 509627/1000000: episode: 5097, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 59.345, mean reward: 0.593 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.019, 10.098], loss: 0.001438, mae: 0.041469, mean_q: 1.169254
 509727/1000000: episode: 5098, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 60.442, mean reward: 0.604 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.499, 10.098], loss: 0.001507, mae: 0.042259, mean_q: 1.168262
 509827/1000000: episode: 5099, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 60.065, mean reward: 0.601 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.812, 10.098], loss: 0.001521, mae: 0.042428, mean_q: 1.174532
 509927/1000000: episode: 5100, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 58.131, mean reward: 0.581 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.833, 10.120], loss: 0.001414, mae: 0.041050, mean_q: 1.170301
 510027/1000000: episode: 5101, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 60.030, mean reward: 0.600 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.408, 10.098], loss: 0.001642, mae: 0.043799, mean_q: 1.177243
 510127/1000000: episode: 5102, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 62.365, mean reward: 0.624 [0.526, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.639, 10.518], loss: 0.001431, mae: 0.041654, mean_q: 1.173363
 510227/1000000: episode: 5103, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 60.748, mean reward: 0.607 [0.511, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.940, 10.098], loss: 0.001526, mae: 0.042258, mean_q: 1.172745
 510327/1000000: episode: 5104, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.322, mean reward: 0.593 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.143], loss: 0.001478, mae: 0.041863, mean_q: 1.176077
 510427/1000000: episode: 5105, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 57.274, mean reward: 0.573 [0.497, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.213, 10.201], loss: 0.001354, mae: 0.039995, mean_q: 1.176027
 510527/1000000: episode: 5106, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 58.277, mean reward: 0.583 [0.501, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.885, 10.098], loss: 0.001528, mae: 0.042654, mean_q: 1.175868
 510627/1000000: episode: 5107, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 61.450, mean reward: 0.615 [0.521, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.140, 10.133], loss: 0.001490, mae: 0.042048, mean_q: 1.175968
 510727/1000000: episode: 5108, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 60.455, mean reward: 0.605 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.651, 10.224], loss: 0.001464, mae: 0.041320, mean_q: 1.177049
 510827/1000000: episode: 5109, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 56.852, mean reward: 0.569 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.359, 10.117], loss: 0.001528, mae: 0.042389, mean_q: 1.171962
 510927/1000000: episode: 5110, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 60.356, mean reward: 0.604 [0.517, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.001, 10.098], loss: 0.001523, mae: 0.042547, mean_q: 1.171504
 511027/1000000: episode: 5111, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 62.733, mean reward: 0.627 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.077, 10.098], loss: 0.001557, mae: 0.043288, mean_q: 1.175731
 511127/1000000: episode: 5112, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 57.849, mean reward: 0.578 [0.499, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.335, 10.098], loss: 0.001526, mae: 0.042640, mean_q: 1.179167
 511227/1000000: episode: 5113, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 59.057, mean reward: 0.591 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.225, 10.245], loss: 0.001556, mae: 0.042383, mean_q: 1.176048
 511327/1000000: episode: 5114, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.347, mean reward: 0.593 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.233, 10.469], loss: 0.001459, mae: 0.041564, mean_q: 1.177305
 511427/1000000: episode: 5115, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 59.671, mean reward: 0.597 [0.519, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.123, 10.098], loss: 0.001386, mae: 0.040530, mean_q: 1.172014
 511527/1000000: episode: 5116, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 58.562, mean reward: 0.586 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.141, 10.321], loss: 0.001525, mae: 0.041726, mean_q: 1.178722
 511627/1000000: episode: 5117, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.725, mean reward: 0.587 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.771, 10.299], loss: 0.001477, mae: 0.041917, mean_q: 1.175328
 511727/1000000: episode: 5118, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 60.126, mean reward: 0.601 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.975, 10.098], loss: 0.001440, mae: 0.041193, mean_q: 1.172868
 511827/1000000: episode: 5119, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.997, mean reward: 0.590 [0.509, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.721, 10.098], loss: 0.001349, mae: 0.040449, mean_q: 1.176552
 511927/1000000: episode: 5120, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 61.161, mean reward: 0.612 [0.513, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.197, 10.098], loss: 0.001455, mae: 0.041962, mean_q: 1.174965
 512027/1000000: episode: 5121, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 58.527, mean reward: 0.585 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.098], loss: 0.001516, mae: 0.042407, mean_q: 1.175931
 512127/1000000: episode: 5122, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.228, mean reward: 0.582 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.070, 10.098], loss: 0.001468, mae: 0.041856, mean_q: 1.171762
 512227/1000000: episode: 5123, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 57.925, mean reward: 0.579 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.116, 10.098], loss: 0.001353, mae: 0.040229, mean_q: 1.165801
 512327/1000000: episode: 5124, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 58.627, mean reward: 0.586 [0.514, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.154, 10.098], loss: 0.001381, mae: 0.040778, mean_q: 1.169718
 512427/1000000: episode: 5125, duration: 1.349s, episode steps: 100, steps per second: 74, episode reward: 57.192, mean reward: 0.572 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.702, 10.098], loss: 0.001535, mae: 0.042557, mean_q: 1.170853
 512527/1000000: episode: 5126, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 58.724, mean reward: 0.587 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.244, 10.422], loss: 0.001507, mae: 0.042362, mean_q: 1.174567
 512627/1000000: episode: 5127, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 57.198, mean reward: 0.572 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.145], loss: 0.001414, mae: 0.040815, mean_q: 1.170302
 512727/1000000: episode: 5128, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 58.101, mean reward: 0.581 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.169, 10.279], loss: 0.001492, mae: 0.041667, mean_q: 1.168589
 512827/1000000: episode: 5129, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 58.397, mean reward: 0.584 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.239, 10.464], loss: 0.001371, mae: 0.040125, mean_q: 1.168569
 512927/1000000: episode: 5130, duration: 1.579s, episode steps: 100, steps per second: 63, episode reward: 57.664, mean reward: 0.577 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.250, 10.098], loss: 0.001571, mae: 0.042998, mean_q: 1.170000
 513027/1000000: episode: 5131, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 60.783, mean reward: 0.608 [0.512, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.805, 10.098], loss: 0.001359, mae: 0.040327, mean_q: 1.166309
 513127/1000000: episode: 5132, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 60.302, mean reward: 0.603 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.484, 10.098], loss: 0.001506, mae: 0.042039, mean_q: 1.169518
 513227/1000000: episode: 5133, duration: 1.362s, episode steps: 100, steps per second: 73, episode reward: 60.879, mean reward: 0.609 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.498, 10.444], loss: 0.001509, mae: 0.042536, mean_q: 1.170030
 513327/1000000: episode: 5134, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 58.008, mean reward: 0.580 [0.511, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.581, 10.212], loss: 0.001589, mae: 0.042923, mean_q: 1.171644
 513427/1000000: episode: 5135, duration: 1.388s, episode steps: 100, steps per second: 72, episode reward: 64.599, mean reward: 0.646 [0.512, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.870, 10.098], loss: 0.001495, mae: 0.042470, mean_q: 1.170690
 513527/1000000: episode: 5136, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 59.208, mean reward: 0.592 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.756, 10.098], loss: 0.001400, mae: 0.040572, mean_q: 1.170610
 513627/1000000: episode: 5137, duration: 1.248s, episode steps: 100, steps per second: 80, episode reward: 58.322, mean reward: 0.583 [0.501, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.882, 10.276], loss: 0.001463, mae: 0.041448, mean_q: 1.170242
 513727/1000000: episode: 5138, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 61.563, mean reward: 0.616 [0.499, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.098], loss: 0.001504, mae: 0.042736, mean_q: 1.169819
 513827/1000000: episode: 5139, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 63.876, mean reward: 0.639 [0.502, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.259, 10.186], loss: 0.001487, mae: 0.041648, mean_q: 1.172602
 513927/1000000: episode: 5140, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 60.774, mean reward: 0.608 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.984, 10.098], loss: 0.001442, mae: 0.041609, mean_q: 1.175945
 514027/1000000: episode: 5141, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.717, mean reward: 0.577 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.391, 10.143], loss: 0.001473, mae: 0.042114, mean_q: 1.174837
 514127/1000000: episode: 5142, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 58.808, mean reward: 0.588 [0.499, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.611, 10.105], loss: 0.001628, mae: 0.043892, mean_q: 1.175428
 514227/1000000: episode: 5143, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 56.142, mean reward: 0.561 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.001, 10.107], loss: 0.001548, mae: 0.042606, mean_q: 1.176370
 514327/1000000: episode: 5144, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 57.661, mean reward: 0.577 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.808, 10.098], loss: 0.001551, mae: 0.043282, mean_q: 1.173319
 514427/1000000: episode: 5145, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 59.535, mean reward: 0.595 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.778, 10.098], loss: 0.001493, mae: 0.041897, mean_q: 1.177239
 514527/1000000: episode: 5146, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 59.042, mean reward: 0.590 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.429, 10.224], loss: 0.001556, mae: 0.042697, mean_q: 1.177139
 514627/1000000: episode: 5147, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 60.324, mean reward: 0.603 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.611, 10.277], loss: 0.001431, mae: 0.040939, mean_q: 1.176374
 514727/1000000: episode: 5148, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.990, mean reward: 0.580 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.781, 10.098], loss: 0.001488, mae: 0.041915, mean_q: 1.177379
 514827/1000000: episode: 5149, duration: 0.862s, episode steps: 100, steps per second: 116, episode reward: 58.145, mean reward: 0.581 [0.509, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.098], loss: 0.001618, mae: 0.043335, mean_q: 1.175900
 514927/1000000: episode: 5150, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.857, mean reward: 0.599 [0.520, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.541, 10.098], loss: 0.001508, mae: 0.042163, mean_q: 1.177086
 515027/1000000: episode: 5151, duration: 0.841s, episode steps: 100, steps per second: 119, episode reward: 58.755, mean reward: 0.588 [0.510, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.137, 10.159], loss: 0.001481, mae: 0.041929, mean_q: 1.174995
 515127/1000000: episode: 5152, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 61.013, mean reward: 0.610 [0.512, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.756, 10.098], loss: 0.001434, mae: 0.040774, mean_q: 1.172320
 515227/1000000: episode: 5153, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 57.664, mean reward: 0.577 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.676, 10.098], loss: 0.001457, mae: 0.041997, mean_q: 1.172228
 515327/1000000: episode: 5154, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 58.148, mean reward: 0.581 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.114, 10.098], loss: 0.001416, mae: 0.040775, mean_q: 1.170618
 515427/1000000: episode: 5155, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 59.652, mean reward: 0.597 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.467, 10.098], loss: 0.001463, mae: 0.041153, mean_q: 1.171471
 515527/1000000: episode: 5156, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 60.313, mean reward: 0.603 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.278], loss: 0.001508, mae: 0.041902, mean_q: 1.176397
 515627/1000000: episode: 5157, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.201, mean reward: 0.572 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.788, 10.117], loss: 0.001426, mae: 0.041503, mean_q: 1.173432
 515727/1000000: episode: 5158, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 57.641, mean reward: 0.576 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.963, 10.098], loss: 0.001541, mae: 0.042507, mean_q: 1.172176
 515827/1000000: episode: 5159, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 58.230, mean reward: 0.582 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.758, 10.373], loss: 0.001416, mae: 0.040898, mean_q: 1.173002
 515927/1000000: episode: 5160, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 57.825, mean reward: 0.578 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.486, 10.098], loss: 0.001481, mae: 0.042146, mean_q: 1.173957
 516027/1000000: episode: 5161, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: 57.811, mean reward: 0.578 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.616, 10.098], loss: 0.001524, mae: 0.041640, mean_q: 1.165543
 516127/1000000: episode: 5162, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 61.218, mean reward: 0.612 [0.505, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.068, 10.098], loss: 0.001386, mae: 0.040872, mean_q: 1.171597
 516227/1000000: episode: 5163, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 60.197, mean reward: 0.602 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.111, 10.098], loss: 0.001543, mae: 0.042498, mean_q: 1.170825
 516327/1000000: episode: 5164, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 60.099, mean reward: 0.601 [0.502, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.321, 10.257], loss: 0.001298, mae: 0.038766, mean_q: 1.170086
 516427/1000000: episode: 5165, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.783, mean reward: 0.588 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.515, 10.098], loss: 0.001436, mae: 0.041385, mean_q: 1.168949
 516527/1000000: episode: 5166, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 60.294, mean reward: 0.603 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.397, 10.098], loss: 0.001473, mae: 0.041609, mean_q: 1.168849
 516627/1000000: episode: 5167, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 57.983, mean reward: 0.580 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.443, 10.214], loss: 0.001494, mae: 0.041752, mean_q: 1.167616
 516727/1000000: episode: 5168, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 58.568, mean reward: 0.586 [0.502, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.374, 10.202], loss: 0.001357, mae: 0.039911, mean_q: 1.168824
 516827/1000000: episode: 5169, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 58.712, mean reward: 0.587 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.068, 10.109], loss: 0.001514, mae: 0.042423, mean_q: 1.171936
 516927/1000000: episode: 5170, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 56.650, mean reward: 0.567 [0.499, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.001453, mae: 0.041586, mean_q: 1.169054
 517027/1000000: episode: 5171, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 57.103, mean reward: 0.571 [0.498, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.453, 10.236], loss: 0.001466, mae: 0.041493, mean_q: 1.173062
 517127/1000000: episode: 5172, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 62.859, mean reward: 0.629 [0.529, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.349, 10.098], loss: 0.001416, mae: 0.040743, mean_q: 1.167949
 517227/1000000: episode: 5173, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 59.245, mean reward: 0.592 [0.507, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.087, 10.098], loss: 0.001428, mae: 0.041012, mean_q: 1.168270
 517327/1000000: episode: 5174, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 57.957, mean reward: 0.580 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.929, 10.098], loss: 0.001440, mae: 0.040908, mean_q: 1.171547
 517427/1000000: episode: 5175, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 57.617, mean reward: 0.576 [0.498, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.163, 10.098], loss: 0.001465, mae: 0.041668, mean_q: 1.173158
 517527/1000000: episode: 5176, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 59.255, mean reward: 0.593 [0.504, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.106], loss: 0.001367, mae: 0.040214, mean_q: 1.165360
 517627/1000000: episode: 5177, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.254, mean reward: 0.583 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.203], loss: 0.001464, mae: 0.041354, mean_q: 1.170624
 517727/1000000: episode: 5178, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.435, mean reward: 0.584 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.137], loss: 0.001384, mae: 0.040607, mean_q: 1.172168
 517827/1000000: episode: 5179, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 60.138, mean reward: 0.601 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.705, 10.254], loss: 0.001394, mae: 0.040349, mean_q: 1.175106
 517927/1000000: episode: 5180, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 60.201, mean reward: 0.602 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.256], loss: 0.001564, mae: 0.042869, mean_q: 1.173919
 518027/1000000: episode: 5181, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 57.001, mean reward: 0.570 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.130], loss: 0.001459, mae: 0.041892, mean_q: 1.170677
 518127/1000000: episode: 5182, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 58.728, mean reward: 0.587 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.939, 10.247], loss: 0.001335, mae: 0.040060, mean_q: 1.172203
 518227/1000000: episode: 5183, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 59.698, mean reward: 0.597 [0.509, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.772, 10.332], loss: 0.001406, mae: 0.040692, mean_q: 1.164100
 518327/1000000: episode: 5184, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 60.363, mean reward: 0.604 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.419, 10.380], loss: 0.001381, mae: 0.040285, mean_q: 1.173518
 518427/1000000: episode: 5185, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 61.563, mean reward: 0.616 [0.504, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.645, 10.098], loss: 0.001338, mae: 0.039606, mean_q: 1.166635
 518527/1000000: episode: 5186, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 59.899, mean reward: 0.599 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.649, 10.098], loss: 0.001275, mae: 0.038582, mean_q: 1.167024
 518627/1000000: episode: 5187, duration: 1.354s, episode steps: 100, steps per second: 74, episode reward: 60.636, mean reward: 0.606 [0.516, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.919, 10.098], loss: 0.001402, mae: 0.040502, mean_q: 1.170051
 518727/1000000: episode: 5188, duration: 1.478s, episode steps: 100, steps per second: 68, episode reward: 57.567, mean reward: 0.576 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.058, 10.249], loss: 0.001380, mae: 0.040411, mean_q: 1.167122
 518827/1000000: episode: 5189, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 58.366, mean reward: 0.584 [0.518, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.789, 10.098], loss: 0.001408, mae: 0.040519, mean_q: 1.167061
 518927/1000000: episode: 5190, duration: 1.486s, episode steps: 100, steps per second: 67, episode reward: 57.473, mean reward: 0.575 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.371, 10.098], loss: 0.001451, mae: 0.041044, mean_q: 1.169660
 519027/1000000: episode: 5191, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 57.413, mean reward: 0.574 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.891, 10.167], loss: 0.001364, mae: 0.039786, mean_q: 1.166359
 519127/1000000: episode: 5192, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 57.740, mean reward: 0.577 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.160, 10.114], loss: 0.001376, mae: 0.040455, mean_q: 1.162905
 519227/1000000: episode: 5193, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.444, mean reward: 0.584 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.842, 10.098], loss: 0.001410, mae: 0.040627, mean_q: 1.166278
 519327/1000000: episode: 5194, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 58.455, mean reward: 0.585 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.071, 10.098], loss: 0.001370, mae: 0.039984, mean_q: 1.164568
 519427/1000000: episode: 5195, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 58.483, mean reward: 0.585 [0.511, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.006, 10.098], loss: 0.001390, mae: 0.039867, mean_q: 1.168045
 519527/1000000: episode: 5196, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 60.609, mean reward: 0.606 [0.512, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.405, 10.098], loss: 0.001392, mae: 0.040094, mean_q: 1.165391
 519627/1000000: episode: 5197, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 62.388, mean reward: 0.624 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.439, 10.098], loss: 0.001338, mae: 0.039574, mean_q: 1.167469
 519727/1000000: episode: 5198, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 62.715, mean reward: 0.627 [0.498, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.325, 10.098], loss: 0.001474, mae: 0.041525, mean_q: 1.171608
 519827/1000000: episode: 5199, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 58.714, mean reward: 0.587 [0.500, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.753, 10.098], loss: 0.001389, mae: 0.040260, mean_q: 1.169602
 519927/1000000: episode: 5200, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 59.629, mean reward: 0.596 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.007, 10.098], loss: 0.001302, mae: 0.039077, mean_q: 1.167997
 520027/1000000: episode: 5201, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 64.035, mean reward: 0.640 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.251, 10.098], loss: 0.001349, mae: 0.039742, mean_q: 1.170108
 520127/1000000: episode: 5202, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.244, mean reward: 0.572 [0.498, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.488, 10.152], loss: 0.001404, mae: 0.040726, mean_q: 1.171368
 520227/1000000: episode: 5203, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 56.718, mean reward: 0.567 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.327, 10.137], loss: 0.001379, mae: 0.040138, mean_q: 1.166555
 520327/1000000: episode: 5204, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 61.989, mean reward: 0.620 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.321, 10.098], loss: 0.001352, mae: 0.039915, mean_q: 1.169840
 520427/1000000: episode: 5205, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 58.153, mean reward: 0.582 [0.513, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.491, 10.098], loss: 0.001399, mae: 0.040689, mean_q: 1.170962
 520527/1000000: episode: 5206, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 60.632, mean reward: 0.606 [0.514, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.261, 10.098], loss: 0.001389, mae: 0.040212, mean_q: 1.169749
 520627/1000000: episode: 5207, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 59.933, mean reward: 0.599 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.084, 10.098], loss: 0.001467, mae: 0.041457, mean_q: 1.169450
 520727/1000000: episode: 5208, duration: 1.258s, episode steps: 100, steps per second: 79, episode reward: 58.199, mean reward: 0.582 [0.515, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.690, 10.270], loss: 0.001412, mae: 0.040509, mean_q: 1.169318
 520827/1000000: episode: 5209, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 57.993, mean reward: 0.580 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.968, 10.098], loss: 0.001578, mae: 0.042379, mean_q: 1.172175
 520927/1000000: episode: 5210, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 56.947, mean reward: 0.569 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.459, 10.177], loss: 0.001398, mae: 0.040390, mean_q: 1.172547
 521027/1000000: episode: 5211, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 57.982, mean reward: 0.580 [0.503, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.879, 10.098], loss: 0.001480, mae: 0.041818, mean_q: 1.170694
 521127/1000000: episode: 5212, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 59.151, mean reward: 0.592 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.593, 10.324], loss: 0.001396, mae: 0.040758, mean_q: 1.171444
 521227/1000000: episode: 5213, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 58.820, mean reward: 0.588 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.200, 10.183], loss: 0.001384, mae: 0.039891, mean_q: 1.170690
 521327/1000000: episode: 5214, duration: 1.395s, episode steps: 100, steps per second: 72, episode reward: 59.287, mean reward: 0.593 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.663, 10.168], loss: 0.001342, mae: 0.039208, mean_q: 1.169346
 521427/1000000: episode: 5215, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 61.889, mean reward: 0.619 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.264, 10.098], loss: 0.001377, mae: 0.040076, mean_q: 1.170263
 521527/1000000: episode: 5216, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 58.526, mean reward: 0.585 [0.507, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.977, 10.098], loss: 0.001421, mae: 0.040809, mean_q: 1.170754
 521627/1000000: episode: 5217, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.763, mean reward: 0.588 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.790, 10.098], loss: 0.001407, mae: 0.040207, mean_q: 1.170898
 521727/1000000: episode: 5218, duration: 1.275s, episode steps: 100, steps per second: 78, episode reward: 58.399, mean reward: 0.584 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.857, 10.144], loss: 0.001341, mae: 0.039780, mean_q: 1.168495
 521827/1000000: episode: 5219, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 57.671, mean reward: 0.577 [0.509, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.427, 10.098], loss: 0.001343, mae: 0.039526, mean_q: 1.167748
 521927/1000000: episode: 5220, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 60.162, mean reward: 0.602 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.260, 10.431], loss: 0.001364, mae: 0.040187, mean_q: 1.170351
 522027/1000000: episode: 5221, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 58.163, mean reward: 0.582 [0.505, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.255, 10.098], loss: 0.001339, mae: 0.039981, mean_q: 1.171781
 522127/1000000: episode: 5222, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.218, mean reward: 0.582 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.098], loss: 0.001307, mae: 0.039348, mean_q: 1.168848
 522227/1000000: episode: 5223, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 59.237, mean reward: 0.592 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.333, 10.363], loss: 0.001430, mae: 0.041143, mean_q: 1.169432
 522327/1000000: episode: 5224, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 58.815, mean reward: 0.588 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.883, 10.098], loss: 0.001364, mae: 0.040291, mean_q: 1.172536
 522427/1000000: episode: 5225, duration: 1.269s, episode steps: 100, steps per second: 79, episode reward: 59.746, mean reward: 0.597 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.148], loss: 0.001406, mae: 0.040355, mean_q: 1.171424
 522527/1000000: episode: 5226, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 63.540, mean reward: 0.635 [0.532, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.939, 10.292], loss: 0.001408, mae: 0.040857, mean_q: 1.170524
 522627/1000000: episode: 5227, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 58.606, mean reward: 0.586 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.003, 10.323], loss: 0.001346, mae: 0.039580, mean_q: 1.178492
 522727/1000000: episode: 5228, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 57.623, mean reward: 0.576 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.368, 10.179], loss: 0.001378, mae: 0.040430, mean_q: 1.171949
 522827/1000000: episode: 5229, duration: 1.489s, episode steps: 100, steps per second: 67, episode reward: 58.205, mean reward: 0.582 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.101, 10.226], loss: 0.001342, mae: 0.039673, mean_q: 1.167987
 522927/1000000: episode: 5230, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.007, mean reward: 0.580 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.631, 10.187], loss: 0.001335, mae: 0.039726, mean_q: 1.168298
 523027/1000000: episode: 5231, duration: 1.385s, episode steps: 100, steps per second: 72, episode reward: 57.817, mean reward: 0.578 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.828, 10.098], loss: 0.001367, mae: 0.040125, mean_q: 1.169134
 523127/1000000: episode: 5232, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.182, mean reward: 0.582 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.353, 10.098], loss: 0.001347, mae: 0.040537, mean_q: 1.173110
 523227/1000000: episode: 5233, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.922, mean reward: 0.579 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.399, 10.098], loss: 0.001376, mae: 0.040311, mean_q: 1.170811
 523327/1000000: episode: 5234, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 58.723, mean reward: 0.587 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.104], loss: 0.001421, mae: 0.040830, mean_q: 1.169728
 523427/1000000: episode: 5235, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 59.008, mean reward: 0.590 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.396, 10.146], loss: 0.001370, mae: 0.040619, mean_q: 1.168318
 523527/1000000: episode: 5236, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 57.312, mean reward: 0.573 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.920, 10.227], loss: 0.001389, mae: 0.040461, mean_q: 1.170021
 523627/1000000: episode: 5237, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 56.320, mean reward: 0.563 [0.503, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.339, 10.098], loss: 0.001388, mae: 0.040284, mean_q: 1.165109
 523727/1000000: episode: 5238, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 59.759, mean reward: 0.598 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.422, 10.102], loss: 0.001404, mae: 0.040967, mean_q: 1.168064
 523827/1000000: episode: 5239, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.982, mean reward: 0.600 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.752, 10.103], loss: 0.001355, mae: 0.039946, mean_q: 1.164326
 523927/1000000: episode: 5240, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 59.153, mean reward: 0.592 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.551, 10.484], loss: 0.001326, mae: 0.039881, mean_q: 1.165911
 524027/1000000: episode: 5241, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 61.312, mean reward: 0.613 [0.513, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.349], loss: 0.001402, mae: 0.040797, mean_q: 1.170590
 524127/1000000: episode: 5242, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.213, mean reward: 0.582 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.312, 10.163], loss: 0.001359, mae: 0.040830, mean_q: 1.169965
 524227/1000000: episode: 5243, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.779, mean reward: 0.598 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.098], loss: 0.001427, mae: 0.041264, mean_q: 1.165759
 524327/1000000: episode: 5244, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 57.942, mean reward: 0.579 [0.508, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.282, 10.098], loss: 0.001402, mae: 0.040923, mean_q: 1.168017
 524427/1000000: episode: 5245, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 60.823, mean reward: 0.608 [0.522, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.410, 10.098], loss: 0.001390, mae: 0.040680, mean_q: 1.169302
 524527/1000000: episode: 5246, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 59.693, mean reward: 0.597 [0.517, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.818, 10.098], loss: 0.001396, mae: 0.041814, mean_q: 1.172114
 524627/1000000: episode: 5247, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 59.245, mean reward: 0.592 [0.500, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.361, 10.189], loss: 0.001348, mae: 0.040207, mean_q: 1.169183
 524727/1000000: episode: 5248, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 60.127, mean reward: 0.601 [0.514, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.458, 10.098], loss: 0.001392, mae: 0.040606, mean_q: 1.167454
 524827/1000000: episode: 5249, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 61.231, mean reward: 0.612 [0.524, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.430, 10.098], loss: 0.001502, mae: 0.041912, mean_q: 1.168943
 524927/1000000: episode: 5250, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.261, mean reward: 0.573 [0.502, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.866, 10.098], loss: 0.001431, mae: 0.040931, mean_q: 1.167753
 525027/1000000: episode: 5251, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 57.130, mean reward: 0.571 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.151], loss: 0.001493, mae: 0.042014, mean_q: 1.167677
 525127/1000000: episode: 5252, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 58.986, mean reward: 0.590 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.969, 10.130], loss: 0.001476, mae: 0.041636, mean_q: 1.166112
 525227/1000000: episode: 5253, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 57.426, mean reward: 0.574 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.866, 10.098], loss: 0.001471, mae: 0.041534, mean_q: 1.166496
 525327/1000000: episode: 5254, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 59.684, mean reward: 0.597 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.559, 10.098], loss: 0.001474, mae: 0.041626, mean_q: 1.166048
 525427/1000000: episode: 5255, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.809, mean reward: 0.588 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.110], loss: 0.001496, mae: 0.041926, mean_q: 1.168690
 525527/1000000: episode: 5256, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 61.253, mean reward: 0.613 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.795, 10.098], loss: 0.001398, mae: 0.041014, mean_q: 1.167358
 525627/1000000: episode: 5257, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 56.815, mean reward: 0.568 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.483, 10.196], loss: 0.001436, mae: 0.041078, mean_q: 1.165234
 525727/1000000: episode: 5258, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 58.997, mean reward: 0.590 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.959, 10.155], loss: 0.001420, mae: 0.041398, mean_q: 1.166327
 525827/1000000: episode: 5259, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 57.081, mean reward: 0.571 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.011, 10.150], loss: 0.001467, mae: 0.042032, mean_q: 1.164204
 525927/1000000: episode: 5260, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 58.285, mean reward: 0.583 [0.509, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.496, 10.098], loss: 0.001355, mae: 0.040250, mean_q: 1.168370
 526027/1000000: episode: 5261, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 58.205, mean reward: 0.582 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.599, 10.098], loss: 0.001390, mae: 0.039838, mean_q: 1.164560
 526127/1000000: episode: 5262, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 62.934, mean reward: 0.629 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.060, 10.123], loss: 0.001457, mae: 0.041847, mean_q: 1.165480
 526227/1000000: episode: 5263, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.065, mean reward: 0.581 [0.499, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.653, 10.098], loss: 0.001437, mae: 0.041449, mean_q: 1.165648
 526327/1000000: episode: 5264, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.321, mean reward: 0.593 [0.521, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.112], loss: 0.001484, mae: 0.042106, mean_q: 1.165331
 526427/1000000: episode: 5265, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 59.076, mean reward: 0.591 [0.497, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.799, 10.098], loss: 0.001501, mae: 0.042595, mean_q: 1.162372
 526527/1000000: episode: 5266, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 57.341, mean reward: 0.573 [0.504, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.261, 10.098], loss: 0.001398, mae: 0.041340, mean_q: 1.162727
 526627/1000000: episode: 5267, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 58.883, mean reward: 0.589 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.430, 10.098], loss: 0.001491, mae: 0.041979, mean_q: 1.162957
 526727/1000000: episode: 5268, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 58.715, mean reward: 0.587 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.706, 10.251], loss: 0.001464, mae: 0.041805, mean_q: 1.165266
 526827/1000000: episode: 5269, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 57.931, mean reward: 0.579 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.649, 10.120], loss: 0.001489, mae: 0.042104, mean_q: 1.167750
 526927/1000000: episode: 5270, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 59.365, mean reward: 0.594 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.874, 10.260], loss: 0.001406, mae: 0.040980, mean_q: 1.162360
 527027/1000000: episode: 5271, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 59.721, mean reward: 0.597 [0.510, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.417, 10.162], loss: 0.001520, mae: 0.043298, mean_q: 1.169188
 527127/1000000: episode: 5272, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.169, mean reward: 0.592 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.467, 10.258], loss: 0.001613, mae: 0.043897, mean_q: 1.166425
 527227/1000000: episode: 5273, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 60.557, mean reward: 0.606 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.153, 10.098], loss: 0.001535, mae: 0.043012, mean_q: 1.165786
 527327/1000000: episode: 5274, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 56.248, mean reward: 0.562 [0.503, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.145], loss: 0.001556, mae: 0.043134, mean_q: 1.163082
 527427/1000000: episode: 5275, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 61.509, mean reward: 0.615 [0.510, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.481, 10.098], loss: 0.001423, mae: 0.041331, mean_q: 1.163889
 527527/1000000: episode: 5276, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.822, mean reward: 0.588 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.497, 10.116], loss: 0.001516, mae: 0.043071, mean_q: 1.165272
 527627/1000000: episode: 5277, duration: 1.005s, episode steps: 100, steps per second: 99, episode reward: 59.245, mean reward: 0.592 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.773, 10.140], loss: 0.001604, mae: 0.043642, mean_q: 1.166374
 527727/1000000: episode: 5278, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 59.773, mean reward: 0.598 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.838, 10.098], loss: 0.001527, mae: 0.042637, mean_q: 1.165646
 527827/1000000: episode: 5279, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 55.642, mean reward: 0.556 [0.501, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.020, 10.200], loss: 0.001330, mae: 0.040217, mean_q: 1.167186
 527927/1000000: episode: 5280, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 57.654, mean reward: 0.577 [0.498, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.077, 10.098], loss: 0.001475, mae: 0.041967, mean_q: 1.163423
 528027/1000000: episode: 5281, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 62.611, mean reward: 0.626 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.968, 10.098], loss: 0.001488, mae: 0.042373, mean_q: 1.163490
 528127/1000000: episode: 5282, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.344, mean reward: 0.593 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.207], loss: 0.001482, mae: 0.041853, mean_q: 1.165189
 528227/1000000: episode: 5283, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 64.035, mean reward: 0.640 [0.505, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.511, 10.611], loss: 0.001519, mae: 0.042902, mean_q: 1.164820
 528327/1000000: episode: 5284, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 56.185, mean reward: 0.562 [0.504, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.098], loss: 0.001517, mae: 0.042952, mean_q: 1.171397
 528427/1000000: episode: 5285, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.829, mean reward: 0.598 [0.516, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.265], loss: 0.001568, mae: 0.043376, mean_q: 1.171054
 528527/1000000: episode: 5286, duration: 0.851s, episode steps: 100, steps per second: 118, episode reward: 57.547, mean reward: 0.575 [0.507, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.327, 10.102], loss: 0.001515, mae: 0.042325, mean_q: 1.172131
 528627/1000000: episode: 5287, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 58.900, mean reward: 0.589 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.098], loss: 0.001586, mae: 0.043127, mean_q: 1.170684
 528727/1000000: episode: 5288, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 58.615, mean reward: 0.586 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.057, 10.239], loss: 0.001467, mae: 0.042331, mean_q: 1.168687
 528827/1000000: episode: 5289, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 59.916, mean reward: 0.599 [0.515, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.485, 10.313], loss: 0.001486, mae: 0.041757, mean_q: 1.168939
 528927/1000000: episode: 5290, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 62.485, mean reward: 0.625 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.980, 10.127], loss: 0.001497, mae: 0.042674, mean_q: 1.169309
 529027/1000000: episode: 5291, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 56.593, mean reward: 0.566 [0.502, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.835, 10.098], loss: 0.001481, mae: 0.042006, mean_q: 1.167525
 529127/1000000: episode: 5292, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 57.694, mean reward: 0.577 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.653, 10.195], loss: 0.001477, mae: 0.042444, mean_q: 1.169481
 529227/1000000: episode: 5293, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 60.394, mean reward: 0.604 [0.515, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.016, 10.098], loss: 0.001424, mae: 0.040940, mean_q: 1.169408
 529327/1000000: episode: 5294, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 57.221, mean reward: 0.572 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.817, 10.098], loss: 0.001461, mae: 0.042173, mean_q: 1.166633
 529427/1000000: episode: 5295, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.624, mean reward: 0.586 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.942, 10.178], loss: 0.001449, mae: 0.041823, mean_q: 1.170450
 529527/1000000: episode: 5296, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 57.644, mean reward: 0.576 [0.500, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.028, 10.142], loss: 0.001464, mae: 0.041996, mean_q: 1.166312
 529627/1000000: episode: 5297, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 59.239, mean reward: 0.592 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.642, 10.098], loss: 0.001444, mae: 0.041349, mean_q: 1.168652
 529727/1000000: episode: 5298, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.992, mean reward: 0.590 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.653, 10.175], loss: 0.001531, mae: 0.042627, mean_q: 1.169992
 529827/1000000: episode: 5299, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.327, mean reward: 0.583 [0.505, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.470, 10.098], loss: 0.001461, mae: 0.042020, mean_q: 1.165785
 529927/1000000: episode: 5300, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 57.206, mean reward: 0.572 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.001, 10.098], loss: 0.001413, mae: 0.040965, mean_q: 1.168512
 530027/1000000: episode: 5301, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 58.507, mean reward: 0.585 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.669, 10.113], loss: 0.001422, mae: 0.041696, mean_q: 1.166310
 530127/1000000: episode: 5302, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 62.066, mean reward: 0.621 [0.514, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.238, 10.098], loss: 0.001436, mae: 0.042022, mean_q: 1.168879
 530227/1000000: episode: 5303, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 60.162, mean reward: 0.602 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.520, 10.304], loss: 0.001440, mae: 0.041404, mean_q: 1.166245
 530327/1000000: episode: 5304, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 58.503, mean reward: 0.585 [0.500, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.158, 10.098], loss: 0.001494, mae: 0.042317, mean_q: 1.167645
 530427/1000000: episode: 5305, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 60.214, mean reward: 0.602 [0.510, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.188, 10.221], loss: 0.001438, mae: 0.041576, mean_q: 1.170174
 530527/1000000: episode: 5306, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 60.101, mean reward: 0.601 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.546, 10.186], loss: 0.001357, mae: 0.040586, mean_q: 1.166518
 530627/1000000: episode: 5307, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 58.146, mean reward: 0.581 [0.499, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.560, 10.124], loss: 0.001370, mae: 0.041107, mean_q: 1.166472
 530727/1000000: episode: 5308, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.192, mean reward: 0.582 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.573, 10.278], loss: 0.001467, mae: 0.041918, mean_q: 1.167934
 530827/1000000: episode: 5309, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.637, mean reward: 0.576 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.419, 10.199], loss: 0.001389, mae: 0.040515, mean_q: 1.169730
 530927/1000000: episode: 5310, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.192, mean reward: 0.592 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.344, 10.407], loss: 0.001354, mae: 0.040413, mean_q: 1.166488
 531027/1000000: episode: 5311, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.598, mean reward: 0.576 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.177], loss: 0.001462, mae: 0.042105, mean_q: 1.166916
 531127/1000000: episode: 5312, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 60.401, mean reward: 0.604 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.835, 10.098], loss: 0.001453, mae: 0.041790, mean_q: 1.168860
 531227/1000000: episode: 5313, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 56.709, mean reward: 0.567 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.165], loss: 0.001381, mae: 0.040975, mean_q: 1.166583
 531327/1000000: episode: 5314, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 59.373, mean reward: 0.594 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.157, 10.143], loss: 0.001456, mae: 0.041788, mean_q: 1.170467
 531427/1000000: episode: 5315, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.696, mean reward: 0.597 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.772, 10.139], loss: 0.001448, mae: 0.042231, mean_q: 1.166324
 531527/1000000: episode: 5316, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.797, mean reward: 0.578 [0.502, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.964, 10.209], loss: 0.001499, mae: 0.042441, mean_q: 1.164220
 531627/1000000: episode: 5317, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.082, mean reward: 0.601 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.098], loss: 0.001413, mae: 0.041159, mean_q: 1.165866
 531727/1000000: episode: 5318, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 58.307, mean reward: 0.583 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.522, 10.098], loss: 0.001482, mae: 0.042331, mean_q: 1.169584
 531827/1000000: episode: 5319, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 61.324, mean reward: 0.613 [0.506, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.127, 10.363], loss: 0.001477, mae: 0.042169, mean_q: 1.170211
 531927/1000000: episode: 5320, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 58.366, mean reward: 0.584 [0.512, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.001420, mae: 0.040991, mean_q: 1.164528
 532027/1000000: episode: 5321, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.198, mean reward: 0.582 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.881, 10.143], loss: 0.001475, mae: 0.042215, mean_q: 1.167748
 532127/1000000: episode: 5322, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 59.037, mean reward: 0.590 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.150], loss: 0.001373, mae: 0.040596, mean_q: 1.167410
 532227/1000000: episode: 5323, duration: 0.847s, episode steps: 100, steps per second: 118, episode reward: 59.115, mean reward: 0.591 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.747, 10.098], loss: 0.001402, mae: 0.041380, mean_q: 1.168648
 532327/1000000: episode: 5324, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 57.854, mean reward: 0.579 [0.508, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.462, 10.220], loss: 0.001327, mae: 0.039951, mean_q: 1.168064
 532427/1000000: episode: 5325, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 57.506, mean reward: 0.575 [0.506, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.496, 10.098], loss: 0.001418, mae: 0.040954, mean_q: 1.167877
 532527/1000000: episode: 5326, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 61.317, mean reward: 0.613 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.431, 10.181], loss: 0.001416, mae: 0.041054, mean_q: 1.163680
 532627/1000000: episode: 5327, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.175, mean reward: 0.592 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.912, 10.382], loss: 0.001380, mae: 0.040691, mean_q: 1.169624
 532727/1000000: episode: 5328, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 57.635, mean reward: 0.576 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.452, 10.098], loss: 0.001371, mae: 0.040615, mean_q: 1.166459
 532827/1000000: episode: 5329, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 57.702, mean reward: 0.577 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.098], loss: 0.001413, mae: 0.040885, mean_q: 1.166368
 532927/1000000: episode: 5330, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 60.551, mean reward: 0.606 [0.526, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.846, 10.098], loss: 0.001386, mae: 0.041009, mean_q: 1.168280
 533027/1000000: episode: 5331, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 58.495, mean reward: 0.585 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.564, 10.098], loss: 0.001336, mae: 0.039834, mean_q: 1.163313
 533127/1000000: episode: 5332, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 58.916, mean reward: 0.589 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.678, 10.264], loss: 0.001424, mae: 0.041364, mean_q: 1.169449
 533227/1000000: episode: 5333, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 60.832, mean reward: 0.608 [0.511, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.543, 10.098], loss: 0.001499, mae: 0.042423, mean_q: 1.167233
 533327/1000000: episode: 5334, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 57.053, mean reward: 0.571 [0.498, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.826, 10.098], loss: 0.001438, mae: 0.041336, mean_q: 1.165787
 533427/1000000: episode: 5335, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 58.816, mean reward: 0.588 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.268, 10.098], loss: 0.001470, mae: 0.041791, mean_q: 1.166971
 533527/1000000: episode: 5336, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.732, mean reward: 0.597 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.109, 10.337], loss: 0.001436, mae: 0.041891, mean_q: 1.166989
 533627/1000000: episode: 5337, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 61.210, mean reward: 0.612 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.749, 10.346], loss: 0.001485, mae: 0.042015, mean_q: 1.166913
 533727/1000000: episode: 5338, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 59.884, mean reward: 0.599 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.977, 10.098], loss: 0.001445, mae: 0.041068, mean_q: 1.167585
 533827/1000000: episode: 5339, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 60.733, mean reward: 0.607 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.140, 10.098], loss: 0.001383, mae: 0.040821, mean_q: 1.165676
 533927/1000000: episode: 5340, duration: 0.985s, episode steps: 100, steps per second: 101, episode reward: 57.790, mean reward: 0.578 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.136, 10.297], loss: 0.001476, mae: 0.042033, mean_q: 1.169273
 534027/1000000: episode: 5341, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 59.226, mean reward: 0.592 [0.514, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.685, 10.186], loss: 0.001438, mae: 0.041673, mean_q: 1.170069
 534127/1000000: episode: 5342, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 60.304, mean reward: 0.603 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.169, 10.098], loss: 0.001465, mae: 0.041413, mean_q: 1.169420
 534227/1000000: episode: 5343, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.863, mean reward: 0.589 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.606, 10.281], loss: 0.001468, mae: 0.042345, mean_q: 1.166782
 534327/1000000: episode: 5344, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 57.849, mean reward: 0.578 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.595, 10.166], loss: 0.001521, mae: 0.042215, mean_q: 1.168907
 534427/1000000: episode: 5345, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 58.206, mean reward: 0.582 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.742, 10.098], loss: 0.001394, mae: 0.041114, mean_q: 1.167877
 534527/1000000: episode: 5346, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.605, mean reward: 0.586 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.343, 10.098], loss: 0.001522, mae: 0.043017, mean_q: 1.169446
 534627/1000000: episode: 5347, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.405, mean reward: 0.594 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.382, 10.343], loss: 0.001487, mae: 0.042419, mean_q: 1.168666
 534727/1000000: episode: 5348, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 59.876, mean reward: 0.599 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.939, 10.098], loss: 0.001453, mae: 0.041909, mean_q: 1.167086
 534827/1000000: episode: 5349, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 59.151, mean reward: 0.592 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.281, 10.098], loss: 0.001505, mae: 0.041933, mean_q: 1.168225
 534927/1000000: episode: 5350, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 65.773, mean reward: 0.658 [0.509, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.098], loss: 0.001427, mae: 0.041289, mean_q: 1.171978
 535027/1000000: episode: 5351, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.316, mean reward: 0.573 [0.510, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.480, 10.098], loss: 0.001378, mae: 0.040808, mean_q: 1.169449
 535127/1000000: episode: 5352, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 59.378, mean reward: 0.594 [0.508, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.770, 10.148], loss: 0.001432, mae: 0.041346, mean_q: 1.172108
 535227/1000000: episode: 5353, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 58.981, mean reward: 0.590 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.109], loss: 0.001499, mae: 0.042111, mean_q: 1.171361
 535327/1000000: episode: 5354, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 59.066, mean reward: 0.591 [0.517, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.352, 10.126], loss: 0.001408, mae: 0.041022, mean_q: 1.172771
 535427/1000000: episode: 5355, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 60.387, mean reward: 0.604 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.698, 10.157], loss: 0.001447, mae: 0.041792, mean_q: 1.170184
 535527/1000000: episode: 5356, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 56.970, mean reward: 0.570 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.237], loss: 0.001488, mae: 0.041958, mean_q: 1.167900
 535627/1000000: episode: 5357, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.797, mean reward: 0.608 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.754, 10.299], loss: 0.001432, mae: 0.041283, mean_q: 1.172384
 535727/1000000: episode: 5358, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 58.062, mean reward: 0.581 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.406, 10.189], loss: 0.001392, mae: 0.040446, mean_q: 1.166191
 535827/1000000: episode: 5359, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 60.063, mean reward: 0.601 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.004, 10.098], loss: 0.001387, mae: 0.040734, mean_q: 1.167979
 535927/1000000: episode: 5360, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 59.754, mean reward: 0.598 [0.505, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.509, 10.098], loss: 0.001539, mae: 0.042790, mean_q: 1.170021
 536027/1000000: episode: 5361, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 60.079, mean reward: 0.601 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.586, 10.098], loss: 0.001506, mae: 0.041923, mean_q: 1.174586
 536127/1000000: episode: 5362, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.239, mean reward: 0.592 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.169], loss: 0.001503, mae: 0.042296, mean_q: 1.176855
 536227/1000000: episode: 5363, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 60.110, mean reward: 0.601 [0.507, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.063, 10.308], loss: 0.001515, mae: 0.042296, mean_q: 1.171781
 536327/1000000: episode: 5364, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 58.070, mean reward: 0.581 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.200], loss: 0.001492, mae: 0.041457, mean_q: 1.170328
 536427/1000000: episode: 5365, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 59.538, mean reward: 0.595 [0.517, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.242, 10.423], loss: 0.001537, mae: 0.042697, mean_q: 1.170592
 536527/1000000: episode: 5366, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.976, mean reward: 0.590 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.482, 10.098], loss: 0.001550, mae: 0.042494, mean_q: 1.174858
 536627/1000000: episode: 5367, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 60.535, mean reward: 0.605 [0.503, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.200], loss: 0.001458, mae: 0.041307, mean_q: 1.172648
 536727/1000000: episode: 5368, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.807, mean reward: 0.578 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.342, 10.098], loss: 0.001460, mae: 0.041268, mean_q: 1.172438
 536827/1000000: episode: 5369, duration: 0.899s, episode steps: 100, steps per second: 111, episode reward: 58.563, mean reward: 0.586 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.138], loss: 0.001509, mae: 0.041870, mean_q: 1.171984
 536927/1000000: episode: 5370, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 59.629, mean reward: 0.596 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.596, 10.098], loss: 0.001468, mae: 0.041661, mean_q: 1.172717
 537027/1000000: episode: 5371, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 59.948, mean reward: 0.599 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.264], loss: 0.001599, mae: 0.043250, mean_q: 1.174515
 537127/1000000: episode: 5372, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 58.516, mean reward: 0.585 [0.505, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.756, 10.152], loss: 0.001611, mae: 0.043019, mean_q: 1.173205
 537227/1000000: episode: 5373, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: 58.902, mean reward: 0.589 [0.512, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.222], loss: 0.001453, mae: 0.041457, mean_q: 1.170748
 537327/1000000: episode: 5374, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 59.754, mean reward: 0.598 [0.505, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.109, 10.098], loss: 0.001548, mae: 0.042775, mean_q: 1.172766
 537427/1000000: episode: 5375, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 57.688, mean reward: 0.577 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.258], loss: 0.001376, mae: 0.040729, mean_q: 1.170657
 537527/1000000: episode: 5376, duration: 0.865s, episode steps: 100, steps per second: 116, episode reward: 57.229, mean reward: 0.572 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.966, 10.098], loss: 0.001441, mae: 0.040969, mean_q: 1.168460
 537627/1000000: episode: 5377, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 59.774, mean reward: 0.598 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.455, 10.098], loss: 0.001512, mae: 0.042272, mean_q: 1.170622
 537727/1000000: episode: 5378, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.556, mean reward: 0.586 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.946, 10.098], loss: 0.001561, mae: 0.042556, mean_q: 1.170047
 537827/1000000: episode: 5379, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.145, mean reward: 0.581 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.711, 10.213], loss: 0.001555, mae: 0.043234, mean_q: 1.169661
 537927/1000000: episode: 5380, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 58.498, mean reward: 0.585 [0.498, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.194], loss: 0.001463, mae: 0.041880, mean_q: 1.169509
 538027/1000000: episode: 5381, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 58.351, mean reward: 0.584 [0.511, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.148, 10.098], loss: 0.001499, mae: 0.042000, mean_q: 1.174556
 538127/1000000: episode: 5382, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.019, mean reward: 0.580 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.138, 10.127], loss: 0.001529, mae: 0.041781, mean_q: 1.171103
 538227/1000000: episode: 5383, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 56.698, mean reward: 0.567 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.345, 10.098], loss: 0.001516, mae: 0.042213, mean_q: 1.168919
 538327/1000000: episode: 5384, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.491, mean reward: 0.585 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.359], loss: 0.001559, mae: 0.042953, mean_q: 1.169171
 538427/1000000: episode: 5385, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 61.208, mean reward: 0.612 [0.507, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.117, 10.523], loss: 0.001426, mae: 0.041339, mean_q: 1.169850
 538527/1000000: episode: 5386, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 59.379, mean reward: 0.594 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.433, 10.098], loss: 0.001589, mae: 0.043034, mean_q: 1.169663
 538627/1000000: episode: 5387, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 59.685, mean reward: 0.597 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.685, 10.135], loss: 0.001484, mae: 0.042083, mean_q: 1.171966
 538727/1000000: episode: 5388, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 59.422, mean reward: 0.594 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.237], loss: 0.001550, mae: 0.042429, mean_q: 1.170923
 538827/1000000: episode: 5389, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.368, mean reward: 0.574 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.195, 10.280], loss: 0.001500, mae: 0.041661, mean_q: 1.172190
 538927/1000000: episode: 5390, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 58.214, mean reward: 0.582 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.631, 10.098], loss: 0.001574, mae: 0.042896, mean_q: 1.171245
 539027/1000000: episode: 5391, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 63.272, mean reward: 0.633 [0.510, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.294, 10.098], loss: 0.001630, mae: 0.043832, mean_q: 1.173548
 539127/1000000: episode: 5392, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 59.943, mean reward: 0.599 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.098], loss: 0.001690, mae: 0.044371, mean_q: 1.171290
 539227/1000000: episode: 5393, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 59.765, mean reward: 0.598 [0.514, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.864, 10.133], loss: 0.001487, mae: 0.041757, mean_q: 1.172920
 539327/1000000: episode: 5394, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 60.528, mean reward: 0.605 [0.503, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.949, 10.098], loss: 0.001531, mae: 0.042107, mean_q: 1.173761
 539427/1000000: episode: 5395, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 58.596, mean reward: 0.586 [0.511, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.629, 10.251], loss: 0.001571, mae: 0.042895, mean_q: 1.171304
 539527/1000000: episode: 5396, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 58.013, mean reward: 0.580 [0.507, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.179], loss: 0.001470, mae: 0.041148, mean_q: 1.169008
 539627/1000000: episode: 5397, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 58.292, mean reward: 0.583 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.916, 10.136], loss: 0.001450, mae: 0.041028, mean_q: 1.171008
 539727/1000000: episode: 5398, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.850, mean reward: 0.579 [0.502, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.402, 10.098], loss: 0.001543, mae: 0.041707, mean_q: 1.168375
 539827/1000000: episode: 5399, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 60.871, mean reward: 0.609 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.959, 10.254], loss: 0.001545, mae: 0.042307, mean_q: 1.172194
 539927/1000000: episode: 5400, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.881, mean reward: 0.589 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.360, 10.098], loss: 0.001413, mae: 0.040252, mean_q: 1.166638
 540027/1000000: episode: 5401, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 56.931, mean reward: 0.569 [0.501, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.368, 10.098], loss: 0.001511, mae: 0.042042, mean_q: 1.171100
 540127/1000000: episode: 5402, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.686, mean reward: 0.587 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.079, 10.210], loss: 0.001552, mae: 0.042391, mean_q: 1.166688
 540227/1000000: episode: 5403, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.401, mean reward: 0.574 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.151, 10.098], loss: 0.001413, mae: 0.040664, mean_q: 1.167117
 540327/1000000: episode: 5404, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 57.391, mean reward: 0.574 [0.507, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.598, 10.098], loss: 0.001528, mae: 0.042261, mean_q: 1.167980
 540427/1000000: episode: 5405, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 60.934, mean reward: 0.609 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.387, 10.098], loss: 0.001521, mae: 0.041641, mean_q: 1.167053
 540527/1000000: episode: 5406, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 58.464, mean reward: 0.585 [0.516, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.468, 10.098], loss: 0.001434, mae: 0.041106, mean_q: 1.167678
 540627/1000000: episode: 5407, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 61.454, mean reward: 0.615 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.140, 10.219], loss: 0.001482, mae: 0.041240, mean_q: 1.168867
 540727/1000000: episode: 5408, duration: 0.846s, episode steps: 100, steps per second: 118, episode reward: 59.187, mean reward: 0.592 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.729, 10.144], loss: 0.001504, mae: 0.042205, mean_q: 1.168766
 540827/1000000: episode: 5409, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.579, mean reward: 0.586 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.604, 10.190], loss: 0.001492, mae: 0.041817, mean_q: 1.169305
 540927/1000000: episode: 5410, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 59.009, mean reward: 0.590 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.902, 10.103], loss: 0.001517, mae: 0.042253, mean_q: 1.163223
 541027/1000000: episode: 5411, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 60.434, mean reward: 0.604 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.769, 10.098], loss: 0.001563, mae: 0.042430, mean_q: 1.170455
 541127/1000000: episode: 5412, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 59.111, mean reward: 0.591 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.499, 10.098], loss: 0.001473, mae: 0.042085, mean_q: 1.169869
 541227/1000000: episode: 5413, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 59.962, mean reward: 0.600 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.841, 10.098], loss: 0.001469, mae: 0.041506, mean_q: 1.166624
 541327/1000000: episode: 5414, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 65.238, mean reward: 0.652 [0.502, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.840, 10.098], loss: 0.001414, mae: 0.041183, mean_q: 1.168213
 541427/1000000: episode: 5415, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 60.868, mean reward: 0.609 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.905, 10.098], loss: 0.001529, mae: 0.042666, mean_q: 1.169107
 541527/1000000: episode: 5416, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 63.558, mean reward: 0.636 [0.521, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.380, 10.098], loss: 0.001464, mae: 0.041645, mean_q: 1.171595
 541627/1000000: episode: 5417, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.548, mean reward: 0.585 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.543, 10.098], loss: 0.001425, mae: 0.041203, mean_q: 1.174909
 541727/1000000: episode: 5418, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.656, mean reward: 0.577 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.465, 10.133], loss: 0.001428, mae: 0.041619, mean_q: 1.169119
 541827/1000000: episode: 5419, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 59.872, mean reward: 0.599 [0.515, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.860, 10.390], loss: 0.001457, mae: 0.041231, mean_q: 1.171181
 541927/1000000: episode: 5420, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 64.573, mean reward: 0.646 [0.503, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.750, 10.098], loss: 0.001594, mae: 0.043349, mean_q: 1.174485
 542027/1000000: episode: 5421, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 59.957, mean reward: 0.600 [0.498, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.804, 10.298], loss: 0.001465, mae: 0.042161, mean_q: 1.174715
 542127/1000000: episode: 5422, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 56.745, mean reward: 0.567 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.052, 10.189], loss: 0.001470, mae: 0.041789, mean_q: 1.176497
 542227/1000000: episode: 5423, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 57.232, mean reward: 0.572 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.271, 10.272], loss: 0.001511, mae: 0.041945, mean_q: 1.174406
 542327/1000000: episode: 5424, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 59.565, mean reward: 0.596 [0.518, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.774, 10.180], loss: 0.001553, mae: 0.042865, mean_q: 1.175509
 542427/1000000: episode: 5425, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 63.741, mean reward: 0.637 [0.525, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.402, 10.361], loss: 0.001474, mae: 0.042531, mean_q: 1.173888
 542527/1000000: episode: 5426, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 59.929, mean reward: 0.599 [0.512, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.098], loss: 0.001589, mae: 0.043224, mean_q: 1.175107
 542627/1000000: episode: 5427, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 60.188, mean reward: 0.602 [0.512, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.265, 10.303], loss: 0.001462, mae: 0.041590, mean_q: 1.176461
 542727/1000000: episode: 5428, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.982, mean reward: 0.590 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.919, 10.252], loss: 0.001403, mae: 0.041003, mean_q: 1.175804
 542827/1000000: episode: 5429, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.527, mean reward: 0.575 [0.504, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.223, 10.197], loss: 0.001600, mae: 0.043131, mean_q: 1.179525
 542927/1000000: episode: 5430, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 57.983, mean reward: 0.580 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.043, 10.206], loss: 0.001531, mae: 0.042331, mean_q: 1.177529
 543027/1000000: episode: 5431, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 57.894, mean reward: 0.579 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.460, 10.098], loss: 0.001372, mae: 0.040691, mean_q: 1.180497
 543127/1000000: episode: 5432, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.520, mean reward: 0.585 [0.513, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.836, 10.098], loss: 0.001404, mae: 0.040855, mean_q: 1.179142
 543227/1000000: episode: 5433, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 62.187, mean reward: 0.622 [0.512, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.098], loss: 0.001376, mae: 0.040642, mean_q: 1.178119
 543327/1000000: episode: 5434, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.622, mean reward: 0.576 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.506, 10.121], loss: 0.001374, mae: 0.039979, mean_q: 1.177983
 543427/1000000: episode: 5435, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.018, mean reward: 0.590 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.098], loss: 0.001463, mae: 0.041181, mean_q: 1.179422
 543527/1000000: episode: 5436, duration: 1.268s, episode steps: 100, steps per second: 79, episode reward: 58.731, mean reward: 0.587 [0.505, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.200], loss: 0.001494, mae: 0.042039, mean_q: 1.173102
 543627/1000000: episode: 5437, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 57.295, mean reward: 0.573 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.856, 10.098], loss: 0.001478, mae: 0.041180, mean_q: 1.178713
 543727/1000000: episode: 5438, duration: 1.359s, episode steps: 100, steps per second: 74, episode reward: 57.758, mean reward: 0.578 [0.511, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.914, 10.098], loss: 0.001449, mae: 0.041154, mean_q: 1.173056
 543827/1000000: episode: 5439, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 58.611, mean reward: 0.586 [0.503, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.228, 10.098], loss: 0.001536, mae: 0.042213, mean_q: 1.175108
 543927/1000000: episode: 5440, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 56.383, mean reward: 0.564 [0.508, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.108], loss: 0.001525, mae: 0.042199, mean_q: 1.175650
 544027/1000000: episode: 5441, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 63.653, mean reward: 0.637 [0.502, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.337, 10.491], loss: 0.001555, mae: 0.042652, mean_q: 1.175516
 544127/1000000: episode: 5442, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 59.800, mean reward: 0.598 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.476, 10.291], loss: 0.001413, mae: 0.041376, mean_q: 1.171539
 544227/1000000: episode: 5443, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 56.769, mean reward: 0.568 [0.505, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.548, 10.232], loss: 0.001513, mae: 0.041919, mean_q: 1.173213
 544327/1000000: episode: 5444, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.655, mean reward: 0.587 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.891, 10.098], loss: 0.001544, mae: 0.042392, mean_q: 1.174695
 544427/1000000: episode: 5445, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.424, mean reward: 0.584 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.668, 10.114], loss: 0.001449, mae: 0.041117, mean_q: 1.173981
 544527/1000000: episode: 5446, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.788, mean reward: 0.578 [0.499, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.128, 10.204], loss: 0.001448, mae: 0.041610, mean_q: 1.171783
 544627/1000000: episode: 5447, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 57.199, mean reward: 0.572 [0.502, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.507, 10.205], loss: 0.001479, mae: 0.041566, mean_q: 1.173615
 544727/1000000: episode: 5448, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 56.919, mean reward: 0.569 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.639, 10.153], loss: 0.001465, mae: 0.041590, mean_q: 1.170740
 544827/1000000: episode: 5449, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 57.069, mean reward: 0.571 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.147, 10.098], loss: 0.001452, mae: 0.041585, mean_q: 1.172840
 544927/1000000: episode: 5450, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 60.074, mean reward: 0.601 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.738, 10.167], loss: 0.001540, mae: 0.042669, mean_q: 1.172899
 545027/1000000: episode: 5451, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 56.970, mean reward: 0.570 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.176, 10.127], loss: 0.001432, mae: 0.040676, mean_q: 1.172071
 545127/1000000: episode: 5452, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.627, mean reward: 0.586 [0.513, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.278, 10.098], loss: 0.001549, mae: 0.042400, mean_q: 1.173866
 545227/1000000: episode: 5453, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 59.234, mean reward: 0.592 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.070, 10.510], loss: 0.001530, mae: 0.041879, mean_q: 1.172193
 545327/1000000: episode: 5454, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 58.034, mean reward: 0.580 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.327, 10.285], loss: 0.001538, mae: 0.041983, mean_q: 1.171874
 545427/1000000: episode: 5455, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.123, mean reward: 0.571 [0.499, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.724, 10.175], loss: 0.001485, mae: 0.042149, mean_q: 1.175600
 545527/1000000: episode: 5456, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 57.738, mean reward: 0.577 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.016, 10.100], loss: 0.001400, mae: 0.040488, mean_q: 1.169541
 545627/1000000: episode: 5457, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 59.108, mean reward: 0.591 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.834, 10.109], loss: 0.001399, mae: 0.040454, mean_q: 1.166116
 545727/1000000: episode: 5458, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 56.782, mean reward: 0.568 [0.501, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.193, 10.098], loss: 0.001487, mae: 0.042017, mean_q: 1.172416
 545827/1000000: episode: 5459, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 59.140, mean reward: 0.591 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.275, 10.098], loss: 0.001444, mae: 0.041073, mean_q: 1.167770
 545927/1000000: episode: 5460, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: 59.411, mean reward: 0.594 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.284, 10.098], loss: 0.001537, mae: 0.042528, mean_q: 1.166699
 546027/1000000: episode: 5461, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 59.259, mean reward: 0.593 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.271], loss: 0.001417, mae: 0.040854, mean_q: 1.169073
 546127/1000000: episode: 5462, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 57.896, mean reward: 0.579 [0.513, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.801, 10.160], loss: 0.001510, mae: 0.041922, mean_q: 1.171155
 546227/1000000: episode: 5463, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.801, mean reward: 0.598 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.822, 10.098], loss: 0.001478, mae: 0.041179, mean_q: 1.166859
 546327/1000000: episode: 5464, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 58.325, mean reward: 0.583 [0.507, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.370, 10.299], loss: 0.001496, mae: 0.041633, mean_q: 1.168930
 546427/1000000: episode: 5465, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 59.142, mean reward: 0.591 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.840, 10.260], loss: 0.001444, mae: 0.041136, mean_q: 1.167151
 546527/1000000: episode: 5466, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 58.933, mean reward: 0.589 [0.517, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.994, 10.098], loss: 0.001585, mae: 0.042885, mean_q: 1.160029
 546627/1000000: episode: 5467, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.987, mean reward: 0.590 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.977, 10.130], loss: 0.001487, mae: 0.041381, mean_q: 1.161813
 546727/1000000: episode: 5468, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 64.385, mean reward: 0.644 [0.527, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.301], loss: 0.001431, mae: 0.041637, mean_q: 1.164453
 546827/1000000: episode: 5469, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 57.776, mean reward: 0.578 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.193, 10.098], loss: 0.001537, mae: 0.042339, mean_q: 1.165527
 546927/1000000: episode: 5470, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 61.803, mean reward: 0.618 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.383], loss: 0.001474, mae: 0.040929, mean_q: 1.162207
 547027/1000000: episode: 5471, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 62.303, mean reward: 0.623 [0.531, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.202], loss: 0.001566, mae: 0.042286, mean_q: 1.164251
 547127/1000000: episode: 5472, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 60.592, mean reward: 0.606 [0.523, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.994, 10.098], loss: 0.001491, mae: 0.042218, mean_q: 1.169499
 547227/1000000: episode: 5473, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 61.779, mean reward: 0.618 [0.516, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.332, 10.332], loss: 0.001558, mae: 0.042566, mean_q: 1.168213
 547327/1000000: episode: 5474, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 57.268, mean reward: 0.573 [0.501, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.963, 10.264], loss: 0.001519, mae: 0.041698, mean_q: 1.166732
 547427/1000000: episode: 5475, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 56.453, mean reward: 0.565 [0.500, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.771, 10.098], loss: 0.001501, mae: 0.041850, mean_q: 1.168046
 547527/1000000: episode: 5476, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 59.218, mean reward: 0.592 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.752, 10.098], loss: 0.001511, mae: 0.042408, mean_q: 1.166305
 547627/1000000: episode: 5477, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 60.704, mean reward: 0.607 [0.511, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.575, 10.189], loss: 0.001584, mae: 0.042529, mean_q: 1.167598
 547727/1000000: episode: 5478, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 59.150, mean reward: 0.592 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.799, 10.098], loss: 0.001513, mae: 0.042241, mean_q: 1.163992
 547827/1000000: episode: 5479, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 62.989, mean reward: 0.630 [0.500, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.264, 10.233], loss: 0.001415, mae: 0.040807, mean_q: 1.170497
 547927/1000000: episode: 5480, duration: 1.460s, episode steps: 100, steps per second: 68, episode reward: 62.205, mean reward: 0.622 [0.514, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.192, 10.459], loss: 0.001417, mae: 0.041294, mean_q: 1.166915
 548027/1000000: episode: 5481, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 60.517, mean reward: 0.605 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.663, 10.511], loss: 0.001402, mae: 0.040493, mean_q: 1.168265
 548127/1000000: episode: 5482, duration: 1.005s, episode steps: 100, steps per second: 99, episode reward: 58.025, mean reward: 0.580 [0.507, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.086, 10.115], loss: 0.001460, mae: 0.041484, mean_q: 1.169097
 548227/1000000: episode: 5483, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 60.676, mean reward: 0.607 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.156, 10.404], loss: 0.001477, mae: 0.041537, mean_q: 1.169655
 548327/1000000: episode: 5484, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 61.534, mean reward: 0.615 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.234, 10.098], loss: 0.001434, mae: 0.041120, mean_q: 1.167528
 548427/1000000: episode: 5485, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.836, mean reward: 0.588 [0.506, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.466, 10.234], loss: 0.001513, mae: 0.042414, mean_q: 1.168291
 548527/1000000: episode: 5486, duration: 1.378s, episode steps: 100, steps per second: 73, episode reward: 58.393, mean reward: 0.584 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.618, 10.098], loss: 0.001440, mae: 0.040738, mean_q: 1.170360
 548627/1000000: episode: 5487, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 57.959, mean reward: 0.580 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.107], loss: 0.001468, mae: 0.041456, mean_q: 1.172461
 548727/1000000: episode: 5488, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 57.427, mean reward: 0.574 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.591, 10.098], loss: 0.001425, mae: 0.041127, mean_q: 1.170387
 548827/1000000: episode: 5489, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 56.408, mean reward: 0.564 [0.501, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.206, 10.098], loss: 0.001432, mae: 0.041406, mean_q: 1.171257
 548927/1000000: episode: 5490, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.320, mean reward: 0.583 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.001, 10.098], loss: 0.001460, mae: 0.041526, mean_q: 1.170422
 549027/1000000: episode: 5491, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 58.069, mean reward: 0.581 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.920, 10.114], loss: 0.001503, mae: 0.041711, mean_q: 1.170656
 549127/1000000: episode: 5492, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 59.101, mean reward: 0.591 [0.513, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.942, 10.098], loss: 0.001368, mae: 0.040330, mean_q: 1.165619
 549227/1000000: episode: 5493, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 61.014, mean reward: 0.610 [0.511, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.856, 10.203], loss: 0.001336, mae: 0.039439, mean_q: 1.161730
 549327/1000000: episode: 5494, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 59.182, mean reward: 0.592 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.440, 10.098], loss: 0.001350, mae: 0.039385, mean_q: 1.167466
 549427/1000000: episode: 5495, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 59.079, mean reward: 0.591 [0.502, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.377], loss: 0.001410, mae: 0.040919, mean_q: 1.169219
 549527/1000000: episode: 5496, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 56.847, mean reward: 0.568 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.885, 10.219], loss: 0.001318, mae: 0.039262, mean_q: 1.168122
 549627/1000000: episode: 5497, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 60.374, mean reward: 0.604 [0.516, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.531, 10.218], loss: 0.001416, mae: 0.040371, mean_q: 1.166632
 549727/1000000: episode: 5498, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: 58.496, mean reward: 0.585 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.170, 10.098], loss: 0.001325, mae: 0.039825, mean_q: 1.169364
 549827/1000000: episode: 5499, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.881, mean reward: 0.579 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.101], loss: 0.001386, mae: 0.040380, mean_q: 1.171126
 549927/1000000: episode: 5500, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.956, mean reward: 0.590 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.124, 10.347], loss: 0.001399, mae: 0.041061, mean_q: 1.171970
 550027/1000000: episode: 5501, duration: 0.873s, episode steps: 100, steps per second: 114, episode reward: 59.856, mean reward: 0.599 [0.508, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.309, 10.178], loss: 0.001366, mae: 0.040091, mean_q: 1.167022
 550127/1000000: episode: 5502, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.819, mean reward: 0.588 [0.512, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.167], loss: 0.001429, mae: 0.041226, mean_q: 1.173373
 550227/1000000: episode: 5503, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 59.539, mean reward: 0.595 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.584, 10.426], loss: 0.001467, mae: 0.041674, mean_q: 1.174653
 550327/1000000: episode: 5504, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.800, mean reward: 0.578 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.743, 10.143], loss: 0.001424, mae: 0.040819, mean_q: 1.173427
 550427/1000000: episode: 5505, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 61.492, mean reward: 0.615 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.176, 10.318], loss: 0.001396, mae: 0.040871, mean_q: 1.173292
 550527/1000000: episode: 5506, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 58.107, mean reward: 0.581 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.187], loss: 0.001459, mae: 0.041261, mean_q: 1.176405
 550627/1000000: episode: 5507, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 59.089, mean reward: 0.591 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.115, 10.207], loss: 0.001439, mae: 0.040954, mean_q: 1.172084
 550727/1000000: episode: 5508, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 57.651, mean reward: 0.577 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.968, 10.306], loss: 0.001385, mae: 0.040969, mean_q: 1.172254
 550827/1000000: episode: 5509, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 57.480, mean reward: 0.575 [0.507, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.098], loss: 0.001391, mae: 0.040603, mean_q: 1.175873
 550927/1000000: episode: 5510, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 60.547, mean reward: 0.605 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.193, 10.098], loss: 0.001458, mae: 0.041272, mean_q: 1.176069
 551027/1000000: episode: 5511, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 61.708, mean reward: 0.617 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.890, 10.313], loss: 0.001426, mae: 0.040740, mean_q: 1.175875
 551127/1000000: episode: 5512, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.967, mean reward: 0.580 [0.510, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.439, 10.121], loss: 0.001345, mae: 0.040200, mean_q: 1.174318
 551227/1000000: episode: 5513, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 62.300, mean reward: 0.623 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.153, 10.098], loss: 0.001296, mae: 0.039806, mean_q: 1.171748
 551327/1000000: episode: 5514, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 56.296, mean reward: 0.563 [0.500, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.098], loss: 0.001302, mae: 0.039537, mean_q: 1.177015
 551427/1000000: episode: 5515, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 59.952, mean reward: 0.600 [0.518, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.726, 10.098], loss: 0.001285, mae: 0.038854, mean_q: 1.172507
 551527/1000000: episode: 5516, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 60.174, mean reward: 0.602 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.098], loss: 0.001396, mae: 0.040757, mean_q: 1.175526
 551627/1000000: episode: 5517, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 59.426, mean reward: 0.594 [0.518, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.773, 10.098], loss: 0.001392, mae: 0.040691, mean_q: 1.178042
 551727/1000000: episode: 5518, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 60.968, mean reward: 0.610 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.979, 10.098], loss: 0.001379, mae: 0.040609, mean_q: 1.175409
 551827/1000000: episode: 5519, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 60.283, mean reward: 0.603 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.413, 10.098], loss: 0.001393, mae: 0.040751, mean_q: 1.177638
 551927/1000000: episode: 5520, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 60.599, mean reward: 0.606 [0.505, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.551, 10.141], loss: 0.001266, mae: 0.039286, mean_q: 1.178211
 552027/1000000: episode: 5521, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 59.098, mean reward: 0.591 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.236, 10.098], loss: 0.001391, mae: 0.040290, mean_q: 1.175774
 552127/1000000: episode: 5522, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 58.145, mean reward: 0.581 [0.500, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.480, 10.098], loss: 0.001260, mae: 0.038479, mean_q: 1.171400
 552227/1000000: episode: 5523, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 56.221, mean reward: 0.562 [0.502, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.497, 10.144], loss: 0.001415, mae: 0.040891, mean_q: 1.172922
 552327/1000000: episode: 5524, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.908, mean reward: 0.589 [0.499, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.982, 10.098], loss: 0.001556, mae: 0.042264, mean_q: 1.172254
 552427/1000000: episode: 5525, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 58.735, mean reward: 0.587 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.828, 10.378], loss: 0.001410, mae: 0.041032, mean_q: 1.172120
 552527/1000000: episode: 5526, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 56.816, mean reward: 0.568 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.980, 10.205], loss: 0.001419, mae: 0.041602, mean_q: 1.170391
 552627/1000000: episode: 5527, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 59.555, mean reward: 0.596 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.681, 10.123], loss: 0.001413, mae: 0.040666, mean_q: 1.169384
 552727/1000000: episode: 5528, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 56.606, mean reward: 0.566 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.113], loss: 0.001512, mae: 0.041953, mean_q: 1.170653
 552827/1000000: episode: 5529, duration: 0.881s, episode steps: 100, steps per second: 113, episode reward: 58.042, mean reward: 0.580 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.848, 10.160], loss: 0.001361, mae: 0.039813, mean_q: 1.166631
 552927/1000000: episode: 5530, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 59.203, mean reward: 0.592 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.987, 10.098], loss: 0.001433, mae: 0.040611, mean_q: 1.164948
 553027/1000000: episode: 5531, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.661, mean reward: 0.587 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.566, 10.216], loss: 0.001506, mae: 0.041659, mean_q: 1.167033
 553127/1000000: episode: 5532, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 58.551, mean reward: 0.586 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.243], loss: 0.001439, mae: 0.041184, mean_q: 1.166295
 553227/1000000: episode: 5533, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.253, mean reward: 0.583 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.212, 10.098], loss: 0.001433, mae: 0.041013, mean_q: 1.163013
 553327/1000000: episode: 5534, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 59.278, mean reward: 0.593 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.350, 10.344], loss: 0.001402, mae: 0.040587, mean_q: 1.162874
 553427/1000000: episode: 5535, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.282, mean reward: 0.573 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.667, 10.110], loss: 0.001380, mae: 0.040615, mean_q: 1.164867
 553527/1000000: episode: 5536, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 57.050, mean reward: 0.570 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.148], loss: 0.001453, mae: 0.041399, mean_q: 1.162738
 553627/1000000: episode: 5537, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.391, mean reward: 0.584 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.769, 10.430], loss: 0.001406, mae: 0.040158, mean_q: 1.162502
 553727/1000000: episode: 5538, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.479, mean reward: 0.585 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.360, 10.240], loss: 0.001404, mae: 0.040362, mean_q: 1.162278
 553827/1000000: episode: 5539, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.257, mean reward: 0.593 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.379, 10.098], loss: 0.001373, mae: 0.040618, mean_q: 1.164498
 553927/1000000: episode: 5540, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 56.993, mean reward: 0.570 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.446, 10.098], loss: 0.001320, mae: 0.039747, mean_q: 1.163419
 554027/1000000: episode: 5541, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 60.154, mean reward: 0.602 [0.516, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.187], loss: 0.001418, mae: 0.041164, mean_q: 1.164752
 554127/1000000: episode: 5542, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 59.826, mean reward: 0.598 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.129, 10.098], loss: 0.001345, mae: 0.039980, mean_q: 1.167318
 554227/1000000: episode: 5543, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.614, mean reward: 0.596 [0.506, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.163, 10.142], loss: 0.001436, mae: 0.040737, mean_q: 1.165778
 554327/1000000: episode: 5544, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 56.662, mean reward: 0.567 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.136, 10.102], loss: 0.001448, mae: 0.041184, mean_q: 1.167544
 554427/1000000: episode: 5545, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 57.563, mean reward: 0.576 [0.509, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.503, 10.098], loss: 0.001429, mae: 0.041201, mean_q: 1.165834
 554527/1000000: episode: 5546, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 61.849, mean reward: 0.618 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.657, 10.098], loss: 0.001387, mae: 0.040812, mean_q: 1.165452
 554627/1000000: episode: 5547, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.992, mean reward: 0.580 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.386, 10.098], loss: 0.001484, mae: 0.041755, mean_q: 1.162058
 554727/1000000: episode: 5548, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 58.457, mean reward: 0.585 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.595, 10.279], loss: 0.001439, mae: 0.040833, mean_q: 1.166892
 554827/1000000: episode: 5549, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.304, mean reward: 0.583 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.548, 10.121], loss: 0.001397, mae: 0.040760, mean_q: 1.162239
 554927/1000000: episode: 5550, duration: 0.877s, episode steps: 100, steps per second: 114, episode reward: 58.289, mean reward: 0.583 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.310, 10.098], loss: 0.001546, mae: 0.042424, mean_q: 1.165836
 555027/1000000: episode: 5551, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 62.992, mean reward: 0.630 [0.506, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.311, 10.247], loss: 0.001490, mae: 0.041750, mean_q: 1.164508
 555127/1000000: episode: 5552, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 57.398, mean reward: 0.574 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.839, 10.141], loss: 0.001526, mae: 0.041676, mean_q: 1.166538
 555227/1000000: episode: 5553, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 61.243, mean reward: 0.612 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.561, 10.128], loss: 0.001411, mae: 0.040552, mean_q: 1.163089
 555327/1000000: episode: 5554, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 60.632, mean reward: 0.606 [0.522, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.210, 10.297], loss: 0.001464, mae: 0.041082, mean_q: 1.165915
 555427/1000000: episode: 5555, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.094, mean reward: 0.581 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.645, 10.129], loss: 0.001445, mae: 0.041221, mean_q: 1.163696
 555527/1000000: episode: 5556, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 60.833, mean reward: 0.608 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.290, 10.113], loss: 0.001478, mae: 0.041750, mean_q: 1.163715
 555627/1000000: episode: 5557, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.932, mean reward: 0.609 [0.513, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.105, 10.098], loss: 0.001449, mae: 0.041480, mean_q: 1.164250
 555727/1000000: episode: 5558, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.654, mean reward: 0.587 [0.502, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.793, 10.197], loss: 0.001468, mae: 0.041344, mean_q: 1.167250
 555827/1000000: episode: 5559, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 59.282, mean reward: 0.593 [0.513, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.965, 10.158], loss: 0.001539, mae: 0.042154, mean_q: 1.166547
 555927/1000000: episode: 5560, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 60.296, mean reward: 0.603 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.873, 10.098], loss: 0.001541, mae: 0.042785, mean_q: 1.167293
 556027/1000000: episode: 5561, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.906, mean reward: 0.589 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.098], loss: 0.001534, mae: 0.042825, mean_q: 1.168393
 556127/1000000: episode: 5562, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 60.001, mean reward: 0.600 [0.503, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.861, 10.310], loss: 0.001460, mae: 0.041283, mean_q: 1.164138
 556227/1000000: episode: 5563, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 60.170, mean reward: 0.602 [0.509, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.068, 10.098], loss: 0.001533, mae: 0.041850, mean_q: 1.164170
 556327/1000000: episode: 5564, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 59.937, mean reward: 0.599 [0.515, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.770, 10.098], loss: 0.001544, mae: 0.042288, mean_q: 1.167300
 556427/1000000: episode: 5565, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.527, mean reward: 0.585 [0.509, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.830, 10.098], loss: 0.001501, mae: 0.041544, mean_q: 1.167052
 556527/1000000: episode: 5566, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 57.380, mean reward: 0.574 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.887, 10.202], loss: 0.001547, mae: 0.042353, mean_q: 1.168554
 556627/1000000: episode: 5567, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 57.513, mean reward: 0.575 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.818, 10.098], loss: 0.001575, mae: 0.042728, mean_q: 1.165846
 556727/1000000: episode: 5568, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 60.210, mean reward: 0.602 [0.516, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.223, 10.255], loss: 0.001511, mae: 0.041724, mean_q: 1.161863
 556827/1000000: episode: 5569, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.537, mean reward: 0.575 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.857, 10.149], loss: 0.001536, mae: 0.042397, mean_q: 1.163462
 556927/1000000: episode: 5570, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.613, mean reward: 0.596 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.999, 10.098], loss: 0.001562, mae: 0.042388, mean_q: 1.166677
 557027/1000000: episode: 5571, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 57.072, mean reward: 0.571 [0.498, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.790, 10.144], loss: 0.001499, mae: 0.041728, mean_q: 1.164082
 557127/1000000: episode: 5572, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 57.016, mean reward: 0.570 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.730, 10.109], loss: 0.001436, mae: 0.040873, mean_q: 1.163899
 557227/1000000: episode: 5573, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 63.960, mean reward: 0.640 [0.516, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.103, 10.469], loss: 0.001457, mae: 0.041232, mean_q: 1.164052
 557327/1000000: episode: 5574, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 56.972, mean reward: 0.570 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.260, 10.223], loss: 0.001417, mae: 0.041014, mean_q: 1.167069
 557427/1000000: episode: 5575, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.522, mean reward: 0.585 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.000, 10.130], loss: 0.001493, mae: 0.041689, mean_q: 1.165810
 557527/1000000: episode: 5576, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 57.089, mean reward: 0.571 [0.498, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.179, 10.143], loss: 0.001504, mae: 0.042086, mean_q: 1.165501
 557627/1000000: episode: 5577, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 58.593, mean reward: 0.586 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.217], loss: 0.001377, mae: 0.040274, mean_q: 1.162713
 557727/1000000: episode: 5578, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 57.173, mean reward: 0.572 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.397, 10.127], loss: 0.001426, mae: 0.040918, mean_q: 1.162817
 557827/1000000: episode: 5579, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 58.361, mean reward: 0.584 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.191], loss: 0.001467, mae: 0.041375, mean_q: 1.163862
 557927/1000000: episode: 5580, duration: 0.855s, episode steps: 100, steps per second: 117, episode reward: 63.801, mean reward: 0.638 [0.509, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.225, 10.312], loss: 0.001603, mae: 0.043405, mean_q: 1.167183
 558027/1000000: episode: 5581, duration: 0.863s, episode steps: 100, steps per second: 116, episode reward: 61.456, mean reward: 0.615 [0.520, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.872, 10.098], loss: 0.001442, mae: 0.041118, mean_q: 1.169133
 558127/1000000: episode: 5582, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 57.906, mean reward: 0.579 [0.513, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.177], loss: 0.001420, mae: 0.041598, mean_q: 1.166428
 558227/1000000: episode: 5583, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 61.299, mean reward: 0.613 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.753, 10.098], loss: 0.001363, mae: 0.039992, mean_q: 1.168136
 558327/1000000: episode: 5584, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 58.099, mean reward: 0.581 [0.501, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.850, 10.356], loss: 0.001420, mae: 0.041080, mean_q: 1.169944
 558427/1000000: episode: 5585, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 62.329, mean reward: 0.623 [0.509, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.654, 10.098], loss: 0.001473, mae: 0.041539, mean_q: 1.173628
 558527/1000000: episode: 5586, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 58.396, mean reward: 0.584 [0.504, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.123, 10.201], loss: 0.001366, mae: 0.040208, mean_q: 1.167268
 558627/1000000: episode: 5587, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 60.627, mean reward: 0.606 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.393], loss: 0.001421, mae: 0.041248, mean_q: 1.170920
 558727/1000000: episode: 5588, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 61.097, mean reward: 0.611 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.337, 10.162], loss: 0.001570, mae: 0.043164, mean_q: 1.174693
 558827/1000000: episode: 5589, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 60.032, mean reward: 0.600 [0.502, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.694, 10.138], loss: 0.001600, mae: 0.043656, mean_q: 1.172827
 558927/1000000: episode: 5590, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.233, mean reward: 0.572 [0.498, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.249, 10.192], loss: 0.001556, mae: 0.043116, mean_q: 1.169375
 559027/1000000: episode: 5591, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 58.191, mean reward: 0.582 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.301], loss: 0.001575, mae: 0.042986, mean_q: 1.174614
 559127/1000000: episode: 5592, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 57.489, mean reward: 0.575 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.870, 10.098], loss: 0.001468, mae: 0.042020, mean_q: 1.173700
 559227/1000000: episode: 5593, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 57.848, mean reward: 0.578 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.405, 10.144], loss: 0.001501, mae: 0.042097, mean_q: 1.172418
 559327/1000000: episode: 5594, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 56.781, mean reward: 0.568 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.106], loss: 0.001488, mae: 0.041788, mean_q: 1.172898
 559427/1000000: episode: 5595, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: 59.142, mean reward: 0.591 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.240, 10.226], loss: 0.001516, mae: 0.042195, mean_q: 1.173080
 559527/1000000: episode: 5596, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 58.648, mean reward: 0.586 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.176, 10.134], loss: 0.001423, mae: 0.041545, mean_q: 1.171496
 559627/1000000: episode: 5597, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 59.155, mean reward: 0.592 [0.513, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.281, 10.098], loss: 0.001336, mae: 0.039641, mean_q: 1.171671
 559727/1000000: episode: 5598, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.491, mean reward: 0.585 [0.499, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.578, 10.218], loss: 0.001424, mae: 0.041175, mean_q: 1.171645
 559827/1000000: episode: 5599, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 61.446, mean reward: 0.614 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.098], loss: 0.001522, mae: 0.041676, mean_q: 1.171433
 559927/1000000: episode: 5600, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 57.511, mean reward: 0.575 [0.510, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.780, 10.310], loss: 0.001469, mae: 0.041683, mean_q: 1.172413
 560027/1000000: episode: 5601, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 56.824, mean reward: 0.568 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.280, 10.098], loss: 0.001463, mae: 0.041479, mean_q: 1.171255
 560127/1000000: episode: 5602, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 60.146, mean reward: 0.601 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.560, 10.098], loss: 0.001502, mae: 0.041250, mean_q: 1.171633
 560227/1000000: episode: 5603, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 59.357, mean reward: 0.594 [0.513, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.327, 10.245], loss: 0.001520, mae: 0.041876, mean_q: 1.170956
 560327/1000000: episode: 5604, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.073, mean reward: 0.591 [0.506, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.098], loss: 0.001512, mae: 0.042019, mean_q: 1.171886
 560427/1000000: episode: 5605, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 57.247, mean reward: 0.572 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.506, 10.098], loss: 0.001469, mae: 0.041364, mean_q: 1.169422
 560527/1000000: episode: 5606, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 61.981, mean reward: 0.620 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.340, 10.286], loss: 0.001501, mae: 0.041585, mean_q: 1.169975
 560627/1000000: episode: 5607, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 59.963, mean reward: 0.600 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.301, 10.098], loss: 0.001551, mae: 0.042172, mean_q: 1.167953
 560727/1000000: episode: 5608, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.189, mean reward: 0.582 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.730, 10.144], loss: 0.001369, mae: 0.040055, mean_q: 1.165279
 560827/1000000: episode: 5609, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 58.352, mean reward: 0.584 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.371, 10.261], loss: 0.001544, mae: 0.042349, mean_q: 1.166141
 560927/1000000: episode: 5610, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 58.675, mean reward: 0.587 [0.501, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.117], loss: 0.001535, mae: 0.042302, mean_q: 1.167604
 561027/1000000: episode: 5611, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.909, mean reward: 0.589 [0.511, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.142, 10.098], loss: 0.001585, mae: 0.042597, mean_q: 1.169297
 561127/1000000: episode: 5612, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 62.946, mean reward: 0.629 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.237, 10.182], loss: 0.001499, mae: 0.042134, mean_q: 1.167521
 561227/1000000: episode: 5613, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 58.806, mean reward: 0.588 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.645, 10.357], loss: 0.001435, mae: 0.040808, mean_q: 1.164691
 561327/1000000: episode: 5614, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.099, mean reward: 0.601 [0.507, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.978, 10.098], loss: 0.001527, mae: 0.041923, mean_q: 1.165463
 561427/1000000: episode: 5615, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.177, mean reward: 0.582 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.800, 10.098], loss: 0.001461, mae: 0.041226, mean_q: 1.167076
 561527/1000000: episode: 5616, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.180, mean reward: 0.582 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.098], loss: 0.001397, mae: 0.040603, mean_q: 1.168485
 561627/1000000: episode: 5617, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 58.583, mean reward: 0.586 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.525, 10.098], loss: 0.001549, mae: 0.041967, mean_q: 1.168155
 561727/1000000: episode: 5618, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 58.452, mean reward: 0.585 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.381, 10.098], loss: 0.001562, mae: 0.043096, mean_q: 1.169052
 561827/1000000: episode: 5619, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 57.883, mean reward: 0.579 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.072, 10.292], loss: 0.001511, mae: 0.041971, mean_q: 1.168600
 561927/1000000: episode: 5620, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 60.004, mean reward: 0.600 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.109, 10.303], loss: 0.001541, mae: 0.042393, mean_q: 1.169478
 562027/1000000: episode: 5621, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 59.600, mean reward: 0.596 [0.511, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.038, 10.098], loss: 0.001471, mae: 0.041434, mean_q: 1.165475
 562127/1000000: episode: 5622, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 59.022, mean reward: 0.590 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.791, 10.212], loss: 0.001552, mae: 0.042092, mean_q: 1.167713
 562227/1000000: episode: 5623, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 60.982, mean reward: 0.610 [0.516, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.353, 10.202], loss: 0.001497, mae: 0.041574, mean_q: 1.165687
 562327/1000000: episode: 5624, duration: 0.866s, episode steps: 100, steps per second: 116, episode reward: 59.624, mean reward: 0.596 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.612, 10.098], loss: 0.001589, mae: 0.042627, mean_q: 1.174146
 562427/1000000: episode: 5625, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 61.559, mean reward: 0.616 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.470, 10.098], loss: 0.001478, mae: 0.041402, mean_q: 1.174009
 562527/1000000: episode: 5626, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 60.883, mean reward: 0.609 [0.509, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.041, 10.366], loss: 0.001425, mae: 0.040763, mean_q: 1.169662
 562627/1000000: episode: 5627, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 56.785, mean reward: 0.568 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.655, 10.098], loss: 0.001685, mae: 0.043361, mean_q: 1.175220
 562727/1000000: episode: 5628, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 57.097, mean reward: 0.571 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.001526, mae: 0.042277, mean_q: 1.172152
 562827/1000000: episode: 5629, duration: 0.861s, episode steps: 100, steps per second: 116, episode reward: 60.280, mean reward: 0.603 [0.505, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.734, 10.098], loss: 0.001436, mae: 0.040409, mean_q: 1.171529
 562927/1000000: episode: 5630, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 57.925, mean reward: 0.579 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.805, 10.098], loss: 0.001525, mae: 0.041823, mean_q: 1.171436
 563027/1000000: episode: 5631, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 57.309, mean reward: 0.573 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.165, 10.161], loss: 0.001489, mae: 0.041980, mean_q: 1.168423
 563127/1000000: episode: 5632, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 59.888, mean reward: 0.599 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.098], loss: 0.001508, mae: 0.041716, mean_q: 1.170437
 563227/1000000: episode: 5633, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 57.916, mean reward: 0.579 [0.508, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.102, 10.162], loss: 0.001528, mae: 0.042325, mean_q: 1.168968
 563327/1000000: episode: 5634, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.059, mean reward: 0.581 [0.498, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.557, 10.335], loss: 0.001549, mae: 0.042200, mean_q: 1.170761
 563427/1000000: episode: 5635, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.457, mean reward: 0.585 [0.497, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.184, 10.098], loss: 0.001405, mae: 0.040567, mean_q: 1.166904
 563527/1000000: episode: 5636, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 57.013, mean reward: 0.570 [0.498, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.128, 10.127], loss: 0.001480, mae: 0.041090, mean_q: 1.168351
 563627/1000000: episode: 5637, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 60.373, mean reward: 0.604 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.142, 10.098], loss: 0.001430, mae: 0.040867, mean_q: 1.166379
 563727/1000000: episode: 5638, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 57.525, mean reward: 0.575 [0.500, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.377, 10.098], loss: 0.001459, mae: 0.040822, mean_q: 1.167366
 563827/1000000: episode: 5639, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 57.407, mean reward: 0.574 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.115, 10.125], loss: 0.001467, mae: 0.041177, mean_q: 1.166071
 563927/1000000: episode: 5640, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 58.895, mean reward: 0.589 [0.502, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.725, 10.098], loss: 0.001479, mae: 0.041614, mean_q: 1.165440
 564027/1000000: episode: 5641, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 59.646, mean reward: 0.596 [0.499, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.803, 10.314], loss: 0.001378, mae: 0.039770, mean_q: 1.163066
 564127/1000000: episode: 5642, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 61.478, mean reward: 0.615 [0.516, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.579, 10.311], loss: 0.001442, mae: 0.040430, mean_q: 1.167017
 564227/1000000: episode: 5643, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 58.411, mean reward: 0.584 [0.506, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.838, 10.098], loss: 0.001468, mae: 0.041299, mean_q: 1.166780
 564327/1000000: episode: 5644, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 58.490, mean reward: 0.585 [0.500, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.248, 10.299], loss: 0.001486, mae: 0.041221, mean_q: 1.170646
 564427/1000000: episode: 5645, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.779, mean reward: 0.578 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.609, 10.098], loss: 0.001455, mae: 0.041038, mean_q: 1.169360
 564527/1000000: episode: 5646, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: 60.713, mean reward: 0.607 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.927, 10.179], loss: 0.001415, mae: 0.040927, mean_q: 1.169423
 564627/1000000: episode: 5647, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 62.892, mean reward: 0.629 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.560, 10.098], loss: 0.001396, mae: 0.040041, mean_q: 1.172462
 564727/1000000: episode: 5648, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 58.319, mean reward: 0.583 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.096, 10.188], loss: 0.001346, mae: 0.039242, mean_q: 1.170281
 564827/1000000: episode: 5649, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 58.907, mean reward: 0.589 [0.511, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.222, 10.134], loss: 0.001419, mae: 0.040054, mean_q: 1.169160
 564927/1000000: episode: 5650, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 58.230, mean reward: 0.582 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.330, 10.098], loss: 0.001447, mae: 0.040632, mean_q: 1.170218
 565027/1000000: episode: 5651, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 60.878, mean reward: 0.609 [0.514, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.767, 10.098], loss: 0.001548, mae: 0.041733, mean_q: 1.173439
 565127/1000000: episode: 5652, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 59.609, mean reward: 0.596 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.323, 10.098], loss: 0.001472, mae: 0.041718, mean_q: 1.170906
 565227/1000000: episode: 5653, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.223, mean reward: 0.582 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.916, 10.137], loss: 0.001429, mae: 0.040729, mean_q: 1.167329
 565327/1000000: episode: 5654, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 60.080, mean reward: 0.601 [0.498, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.628, 10.189], loss: 0.001451, mae: 0.040391, mean_q: 1.168432
 565427/1000000: episode: 5655, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 60.102, mean reward: 0.601 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.359], loss: 0.001434, mae: 0.040488, mean_q: 1.170056
 565527/1000000: episode: 5656, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 59.954, mean reward: 0.600 [0.513, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.546, 10.286], loss: 0.001491, mae: 0.041717, mean_q: 1.172132
 565627/1000000: episode: 5657, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 58.965, mean reward: 0.590 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.856, 10.098], loss: 0.001479, mae: 0.041327, mean_q: 1.170444
 565727/1000000: episode: 5658, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 58.497, mean reward: 0.585 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.794, 10.098], loss: 0.001498, mae: 0.041154, mean_q: 1.172775
 565827/1000000: episode: 5659, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 61.001, mean reward: 0.610 [0.502, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.790, 10.446], loss: 0.001487, mae: 0.041597, mean_q: 1.170491
 565927/1000000: episode: 5660, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 60.951, mean reward: 0.610 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.549, 10.098], loss: 0.001460, mae: 0.040948, mean_q: 1.173561
 566027/1000000: episode: 5661, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 56.929, mean reward: 0.569 [0.498, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.054, 10.140], loss: 0.001523, mae: 0.041893, mean_q: 1.171115
 566127/1000000: episode: 5662, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 57.549, mean reward: 0.575 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.098], loss: 0.001469, mae: 0.040960, mean_q: 1.169599
 566227/1000000: episode: 5663, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.355, mean reward: 0.584 [0.512, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.406, 10.098], loss: 0.001459, mae: 0.040481, mean_q: 1.173780
 566327/1000000: episode: 5664, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 56.641, mean reward: 0.566 [0.498, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.634, 10.206], loss: 0.001369, mae: 0.040204, mean_q: 1.168092
 566427/1000000: episode: 5665, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 56.775, mean reward: 0.568 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.117], loss: 0.001478, mae: 0.041505, mean_q: 1.170026
 566527/1000000: episode: 5666, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 56.889, mean reward: 0.569 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.602, 10.232], loss: 0.001452, mae: 0.040814, mean_q: 1.166606
 566627/1000000: episode: 5667, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 60.917, mean reward: 0.609 [0.511, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.815, 10.347], loss: 0.001401, mae: 0.040086, mean_q: 1.165838
 566727/1000000: episode: 5668, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 57.564, mean reward: 0.576 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.983, 10.098], loss: 0.001499, mae: 0.041621, mean_q: 1.169021
 566827/1000000: episode: 5669, duration: 1.459s, episode steps: 100, steps per second: 69, episode reward: 59.593, mean reward: 0.596 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.275, 10.294], loss: 0.001471, mae: 0.041590, mean_q: 1.168723
 566927/1000000: episode: 5670, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 59.517, mean reward: 0.595 [0.497, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.996, 10.098], loss: 0.001522, mae: 0.042193, mean_q: 1.169571
 567027/1000000: episode: 5671, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 57.097, mean reward: 0.571 [0.509, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.635, 10.098], loss: 0.001428, mae: 0.040269, mean_q: 1.170022
 567127/1000000: episode: 5672, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 57.816, mean reward: 0.578 [0.516, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.098], loss: 0.001390, mae: 0.040017, mean_q: 1.166620
 567227/1000000: episode: 5673, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 61.233, mean reward: 0.612 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.619, 10.098], loss: 0.001486, mae: 0.041844, mean_q: 1.167702
 567327/1000000: episode: 5674, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.291, mean reward: 0.583 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.902, 10.239], loss: 0.001531, mae: 0.042004, mean_q: 1.165903
 567427/1000000: episode: 5675, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.870, mean reward: 0.579 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.608, 10.161], loss: 0.001463, mae: 0.041078, mean_q: 1.168444
 567527/1000000: episode: 5676, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.443, mean reward: 0.594 [0.511, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.832, 10.218], loss: 0.001568, mae: 0.042180, mean_q: 1.165567
 567627/1000000: episode: 5677, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 61.014, mean reward: 0.610 [0.500, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.775, 10.286], loss: 0.001548, mae: 0.042068, mean_q: 1.162230
 567727/1000000: episode: 5678, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 56.304, mean reward: 0.563 [0.499, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.058, 10.109], loss: 0.001438, mae: 0.040586, mean_q: 1.166510
 567827/1000000: episode: 5679, duration: 1.081s, episode steps: 100, steps per second: 92, episode reward: 60.162, mean reward: 0.602 [0.510, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.204, 10.186], loss: 0.001493, mae: 0.041422, mean_q: 1.163620
 567927/1000000: episode: 5680, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: 56.289, mean reward: 0.563 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.899, 10.158], loss: 0.001473, mae: 0.041182, mean_q: 1.162900
 568027/1000000: episode: 5681, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 55.936, mean reward: 0.559 [0.498, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.095, 10.262], loss: 0.001436, mae: 0.040559, mean_q: 1.164305
 568127/1000000: episode: 5682, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 57.510, mean reward: 0.575 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.176, 10.098], loss: 0.001479, mae: 0.040449, mean_q: 1.162148
 568227/1000000: episode: 5683, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.738, mean reward: 0.587 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.076, 10.160], loss: 0.001496, mae: 0.041512, mean_q: 1.161103
 568327/1000000: episode: 5684, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 58.164, mean reward: 0.582 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.556, 10.140], loss: 0.001473, mae: 0.040853, mean_q: 1.163988
 568427/1000000: episode: 5685, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.542, mean reward: 0.585 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.666, 10.135], loss: 0.001549, mae: 0.042168, mean_q: 1.163651
 568527/1000000: episode: 5686, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.520, mean reward: 0.585 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.655, 10.098], loss: 0.001550, mae: 0.042267, mean_q: 1.165038
 568627/1000000: episode: 5687, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 57.270, mean reward: 0.573 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.147, 10.108], loss: 0.001573, mae: 0.042462, mean_q: 1.161853
 568727/1000000: episode: 5688, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 59.893, mean reward: 0.599 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.777, 10.196], loss: 0.001499, mae: 0.041414, mean_q: 1.161729
 568827/1000000: episode: 5689, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 58.801, mean reward: 0.588 [0.504, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.740, 10.098], loss: 0.001428, mae: 0.040145, mean_q: 1.162312
 568927/1000000: episode: 5690, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.971, mean reward: 0.590 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.439, 10.202], loss: 0.001481, mae: 0.040906, mean_q: 1.165050
 569027/1000000: episode: 5691, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 59.213, mean reward: 0.592 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.956, 10.300], loss: 0.001470, mae: 0.040987, mean_q: 1.163478
 569127/1000000: episode: 5692, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 60.662, mean reward: 0.607 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.503, 10.135], loss: 0.001373, mae: 0.039702, mean_q: 1.162051
 569227/1000000: episode: 5693, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 57.791, mean reward: 0.578 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.410, 10.243], loss: 0.001531, mae: 0.040784, mean_q: 1.164646
 569327/1000000: episode: 5694, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 60.670, mean reward: 0.607 [0.502, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.418, 10.279], loss: 0.001524, mae: 0.041297, mean_q: 1.161263
 569427/1000000: episode: 5695, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 57.582, mean reward: 0.576 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.186, 10.098], loss: 0.001554, mae: 0.042149, mean_q: 1.163380
 569527/1000000: episode: 5696, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 63.559, mean reward: 0.636 [0.518, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.068, 10.306], loss: 0.001652, mae: 0.043257, mean_q: 1.165420
 569627/1000000: episode: 5697, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.036, mean reward: 0.570 [0.498, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.685, 10.110], loss: 0.001509, mae: 0.041516, mean_q: 1.164226
 569727/1000000: episode: 5698, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 61.216, mean reward: 0.612 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.321, 10.098], loss: 0.001476, mae: 0.041130, mean_q: 1.159481
 569827/1000000: episode: 5699, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 59.414, mean reward: 0.594 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.199, 10.098], loss: 0.001523, mae: 0.041423, mean_q: 1.164102
 569927/1000000: episode: 5700, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 58.866, mean reward: 0.589 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.442, 10.295], loss: 0.001558, mae: 0.042183, mean_q: 1.165003
 570027/1000000: episode: 5701, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 60.631, mean reward: 0.606 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.299], loss: 0.001540, mae: 0.041950, mean_q: 1.162561
 570127/1000000: episode: 5702, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.139, mean reward: 0.581 [0.505, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.957, 10.098], loss: 0.001457, mae: 0.041417, mean_q: 1.164309
 570227/1000000: episode: 5703, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 59.012, mean reward: 0.590 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.098], loss: 0.001531, mae: 0.041533, mean_q: 1.163146
 570327/1000000: episode: 5704, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.661, mean reward: 0.577 [0.497, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.393, 10.205], loss: 0.001585, mae: 0.042575, mean_q: 1.162435
 570427/1000000: episode: 5705, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 58.559, mean reward: 0.586 [0.501, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.066, 10.294], loss: 0.001581, mae: 0.042825, mean_q: 1.165082
 570527/1000000: episode: 5706, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 57.555, mean reward: 0.576 [0.513, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.700, 10.127], loss: 0.001389, mae: 0.040431, mean_q: 1.164748
 570627/1000000: episode: 5707, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 56.936, mean reward: 0.569 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.138], loss: 0.001509, mae: 0.041271, mean_q: 1.161806
 570727/1000000: episode: 5708, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 58.413, mean reward: 0.584 [0.498, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.249], loss: 0.001484, mae: 0.040881, mean_q: 1.158910
 570827/1000000: episode: 5709, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 60.777, mean reward: 0.608 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.415, 10.098], loss: 0.001544, mae: 0.042161, mean_q: 1.162666
 570927/1000000: episode: 5710, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.101, mean reward: 0.591 [0.514, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.477, 10.098], loss: 0.001440, mae: 0.040475, mean_q: 1.161575
 571027/1000000: episode: 5711, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 59.242, mean reward: 0.592 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.391, 10.098], loss: 0.001417, mae: 0.040152, mean_q: 1.159727
 571127/1000000: episode: 5712, duration: 1.329s, episode steps: 100, steps per second: 75, episode reward: 59.205, mean reward: 0.592 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.716, 10.300], loss: 0.001419, mae: 0.040531, mean_q: 1.161116
 571227/1000000: episode: 5713, duration: 1.366s, episode steps: 100, steps per second: 73, episode reward: 59.448, mean reward: 0.594 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.388, 10.196], loss: 0.001471, mae: 0.040996, mean_q: 1.161958
 571327/1000000: episode: 5714, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 59.474, mean reward: 0.595 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.184, 10.194], loss: 0.001534, mae: 0.041956, mean_q: 1.162861
 571427/1000000: episode: 5715, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 62.882, mean reward: 0.629 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.483, 10.098], loss: 0.001438, mae: 0.040476, mean_q: 1.161894
 571527/1000000: episode: 5716, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.499, mean reward: 0.585 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.129], loss: 0.001508, mae: 0.041807, mean_q: 1.168669
 571627/1000000: episode: 5717, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 61.776, mean reward: 0.618 [0.519, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.053, 10.098], loss: 0.001455, mae: 0.040562, mean_q: 1.165578
 571727/1000000: episode: 5718, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 59.542, mean reward: 0.595 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.563, 10.098], loss: 0.001436, mae: 0.040518, mean_q: 1.168823
 571827/1000000: episode: 5719, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 57.979, mean reward: 0.580 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.283, 10.371], loss: 0.001413, mae: 0.040234, mean_q: 1.159412
 571927/1000000: episode: 5720, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 57.220, mean reward: 0.572 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.943, 10.182], loss: 0.001369, mae: 0.039619, mean_q: 1.164981
 572027/1000000: episode: 5721, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 61.162, mean reward: 0.612 [0.504, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.711, 10.498], loss: 0.001512, mae: 0.041726, mean_q: 1.166051
 572127/1000000: episode: 5722, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 57.676, mean reward: 0.577 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.079, 10.098], loss: 0.001422, mae: 0.040647, mean_q: 1.165854
 572227/1000000: episode: 5723, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 58.022, mean reward: 0.580 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.531, 10.098], loss: 0.001459, mae: 0.041478, mean_q: 1.163557
 572327/1000000: episode: 5724, duration: 1.322s, episode steps: 100, steps per second: 76, episode reward: 59.561, mean reward: 0.596 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.900, 10.122], loss: 0.001466, mae: 0.041442, mean_q: 1.170501
 572427/1000000: episode: 5725, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 58.140, mean reward: 0.581 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.878, 10.098], loss: 0.001439, mae: 0.040917, mean_q: 1.164719
 572527/1000000: episode: 5726, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 61.397, mean reward: 0.614 [0.498, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.098], loss: 0.001355, mae: 0.040255, mean_q: 1.165754
 572627/1000000: episode: 5727, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.360, mean reward: 0.574 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.844, 10.121], loss: 0.001502, mae: 0.041418, mean_q: 1.166785
 572727/1000000: episode: 5728, duration: 1.293s, episode steps: 100, steps per second: 77, episode reward: 57.640, mean reward: 0.576 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.627, 10.098], loss: 0.001451, mae: 0.041370, mean_q: 1.166614
 572827/1000000: episode: 5729, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 59.268, mean reward: 0.593 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.638, 10.098], loss: 0.001497, mae: 0.041980, mean_q: 1.167432
 572927/1000000: episode: 5730, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 59.163, mean reward: 0.592 [0.514, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.751, 10.257], loss: 0.001608, mae: 0.043192, mean_q: 1.171037
 573027/1000000: episode: 5731, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 57.764, mean reward: 0.578 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.857, 10.126], loss: 0.001475, mae: 0.041149, mean_q: 1.167319
 573127/1000000: episode: 5732, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 59.738, mean reward: 0.597 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.520, 10.101], loss: 0.001510, mae: 0.041804, mean_q: 1.169385
 573227/1000000: episode: 5733, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.127, mean reward: 0.571 [0.513, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.436, 10.190], loss: 0.001505, mae: 0.041851, mean_q: 1.170379
 573327/1000000: episode: 5734, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 57.456, mean reward: 0.575 [0.508, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.985, 10.098], loss: 0.001461, mae: 0.042261, mean_q: 1.167490
 573427/1000000: episode: 5735, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 58.671, mean reward: 0.587 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.157, 10.251], loss: 0.001425, mae: 0.041054, mean_q: 1.169008
 573527/1000000: episode: 5736, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 61.845, mean reward: 0.618 [0.516, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.098], loss: 0.001565, mae: 0.042069, mean_q: 1.169222
 573627/1000000: episode: 5737, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 56.944, mean reward: 0.569 [0.510, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.898, 10.113], loss: 0.001438, mae: 0.041217, mean_q: 1.171144
 573727/1000000: episode: 5738, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 61.199, mean reward: 0.612 [0.521, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.508, 10.098], loss: 0.001439, mae: 0.041164, mean_q: 1.166118
 573827/1000000: episode: 5739, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.402, mean reward: 0.584 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.098], loss: 0.001426, mae: 0.040552, mean_q: 1.168000
 573927/1000000: episode: 5740, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 62.045, mean reward: 0.620 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.697, 10.411], loss: 0.001464, mae: 0.041058, mean_q: 1.168685
 574027/1000000: episode: 5741, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 59.672, mean reward: 0.597 [0.511, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.118, 10.171], loss: 0.001416, mae: 0.040554, mean_q: 1.168852
 574127/1000000: episode: 5742, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 58.526, mean reward: 0.585 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.853, 10.098], loss: 0.001494, mae: 0.041802, mean_q: 1.168569
 574227/1000000: episode: 5743, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.896, mean reward: 0.599 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.975, 10.122], loss: 0.001494, mae: 0.042519, mean_q: 1.170445
 574327/1000000: episode: 5744, duration: 0.911s, episode steps: 100, steps per second: 110, episode reward: 58.334, mean reward: 0.583 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.583, 10.155], loss: 0.001482, mae: 0.041843, mean_q: 1.168144
 574427/1000000: episode: 5745, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 58.399, mean reward: 0.584 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.378, 10.148], loss: 0.001407, mae: 0.041434, mean_q: 1.167380
 574527/1000000: episode: 5746, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 58.004, mean reward: 0.580 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.254, 10.117], loss: 0.001451, mae: 0.040992, mean_q: 1.166402
 574627/1000000: episode: 5747, duration: 1.280s, episode steps: 100, steps per second: 78, episode reward: 57.828, mean reward: 0.578 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.098], loss: 0.001392, mae: 0.040582, mean_q: 1.169444
 574727/1000000: episode: 5748, duration: 0.879s, episode steps: 100, steps per second: 114, episode reward: 58.409, mean reward: 0.584 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.982, 10.152], loss: 0.001441, mae: 0.041121, mean_q: 1.166658
 574827/1000000: episode: 5749, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 63.017, mean reward: 0.630 [0.501, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.428, 10.390], loss: 0.001351, mae: 0.039941, mean_q: 1.165853
 574927/1000000: episode: 5750, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 56.955, mean reward: 0.570 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.096, 10.118], loss: 0.001549, mae: 0.042160, mean_q: 1.168756
 575027/1000000: episode: 5751, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 60.268, mean reward: 0.603 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.943, 10.328], loss: 0.001531, mae: 0.042030, mean_q: 1.167849
 575127/1000000: episode: 5752, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 59.754, mean reward: 0.598 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.555, 10.098], loss: 0.001431, mae: 0.040692, mean_q: 1.166509
 575227/1000000: episode: 5753, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.070, mean reward: 0.581 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.209], loss: 0.001461, mae: 0.041253, mean_q: 1.166686
 575327/1000000: episode: 5754, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 58.012, mean reward: 0.580 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.452, 10.098], loss: 0.001501, mae: 0.042010, mean_q: 1.165640
 575427/1000000: episode: 5755, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 58.340, mean reward: 0.583 [0.505, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.274, 10.203], loss: 0.001528, mae: 0.042119, mean_q: 1.167473
 575527/1000000: episode: 5756, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 57.157, mean reward: 0.572 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.098], loss: 0.001503, mae: 0.041627, mean_q: 1.165997
 575627/1000000: episode: 5757, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 60.691, mean reward: 0.607 [0.510, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.116, 10.282], loss: 0.001451, mae: 0.040642, mean_q: 1.167364
 575727/1000000: episode: 5758, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 58.378, mean reward: 0.584 [0.499, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.098], loss: 0.001376, mae: 0.040515, mean_q: 1.166807
 575827/1000000: episode: 5759, duration: 1.284s, episode steps: 100, steps per second: 78, episode reward: 59.293, mean reward: 0.593 [0.498, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.098], loss: 0.001608, mae: 0.043146, mean_q: 1.168835
 575927/1000000: episode: 5760, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 58.477, mean reward: 0.585 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.964, 10.098], loss: 0.001568, mae: 0.042605, mean_q: 1.170596
 576027/1000000: episode: 5761, duration: 0.889s, episode steps: 100, steps per second: 112, episode reward: 59.112, mean reward: 0.591 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.438, 10.191], loss: 0.001434, mae: 0.041004, mean_q: 1.165841
 576127/1000000: episode: 5762, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 61.791, mean reward: 0.618 [0.508, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.448, 10.098], loss: 0.001489, mae: 0.041457, mean_q: 1.170316
 576227/1000000: episode: 5763, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.568, mean reward: 0.576 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.470, 10.319], loss: 0.001466, mae: 0.041888, mean_q: 1.167868
 576327/1000000: episode: 5764, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.858, mean reward: 0.589 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.610, 10.398], loss: 0.001518, mae: 0.041675, mean_q: 1.171234
 576427/1000000: episode: 5765, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.936, mean reward: 0.579 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.007, 10.220], loss: 0.001515, mae: 0.041502, mean_q: 1.166173
 576527/1000000: episode: 5766, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 58.049, mean reward: 0.580 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.434, 10.098], loss: 0.001628, mae: 0.042872, mean_q: 1.169989
 576627/1000000: episode: 5767, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 59.497, mean reward: 0.595 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.341, 10.098], loss: 0.001485, mae: 0.041029, mean_q: 1.163836
 576727/1000000: episode: 5768, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 59.856, mean reward: 0.599 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.679, 10.098], loss: 0.001425, mae: 0.040803, mean_q: 1.165381
 576827/1000000: episode: 5769, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 59.512, mean reward: 0.595 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.512, 10.200], loss: 0.001531, mae: 0.041993, mean_q: 1.167183
 576927/1000000: episode: 5770, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 59.809, mean reward: 0.598 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.655, 10.212], loss: 0.001491, mae: 0.041985, mean_q: 1.165274
 577027/1000000: episode: 5771, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.731, mean reward: 0.587 [0.508, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.626, 10.167], loss: 0.001509, mae: 0.042524, mean_q: 1.166968
 577127/1000000: episode: 5772, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 58.381, mean reward: 0.584 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.162], loss: 0.001428, mae: 0.040582, mean_q: 1.161958
 577227/1000000: episode: 5773, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 61.364, mean reward: 0.614 [0.510, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.045, 10.138], loss: 0.001542, mae: 0.041863, mean_q: 1.166916
 577327/1000000: episode: 5774, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.058, mean reward: 0.591 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.128], loss: 0.001424, mae: 0.040633, mean_q: 1.168864
 577427/1000000: episode: 5775, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.728, mean reward: 0.597 [0.515, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.306, 10.412], loss: 0.001465, mae: 0.041237, mean_q: 1.163579
 577527/1000000: episode: 5776, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.145, mean reward: 0.581 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.987, 10.112], loss: 0.001488, mae: 0.041362, mean_q: 1.166581
 577627/1000000: episode: 5777, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.801, mean reward: 0.578 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.702, 10.371], loss: 0.001396, mae: 0.040084, mean_q: 1.168469
 577727/1000000: episode: 5778, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.509, mean reward: 0.595 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.277, 10.098], loss: 0.001379, mae: 0.040135, mean_q: 1.167419
 577827/1000000: episode: 5779, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.679, mean reward: 0.587 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.177, 10.350], loss: 0.001546, mae: 0.042216, mean_q: 1.167614
 577927/1000000: episode: 5780, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 57.835, mean reward: 0.578 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.703, 10.098], loss: 0.001380, mae: 0.039822, mean_q: 1.169295
 578027/1000000: episode: 5781, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 62.876, mean reward: 0.629 [0.525, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.645, 10.161], loss: 0.001460, mae: 0.040877, mean_q: 1.166248
 578127/1000000: episode: 5782, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 60.763, mean reward: 0.608 [0.509, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.792, 10.098], loss: 0.001429, mae: 0.040642, mean_q: 1.164556
 578227/1000000: episode: 5783, duration: 0.874s, episode steps: 100, steps per second: 114, episode reward: 61.275, mean reward: 0.613 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.098], loss: 0.001332, mae: 0.039407, mean_q: 1.167141
 578327/1000000: episode: 5784, duration: 0.858s, episode steps: 100, steps per second: 117, episode reward: 60.254, mean reward: 0.603 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.098], loss: 0.001494, mae: 0.040998, mean_q: 1.168253
 578427/1000000: episode: 5785, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 56.831, mean reward: 0.568 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.576, 10.159], loss: 0.001540, mae: 0.042331, mean_q: 1.172022
 578527/1000000: episode: 5786, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.759, mean reward: 0.598 [0.507, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.019, 10.281], loss: 0.001608, mae: 0.042824, mean_q: 1.174619
 578627/1000000: episode: 5787, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.633, mean reward: 0.596 [0.509, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.637, 10.098], loss: 0.001518, mae: 0.041741, mean_q: 1.169417
 578727/1000000: episode: 5788, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 61.302, mean reward: 0.613 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.266, 10.098], loss: 0.001531, mae: 0.042329, mean_q: 1.172509
 578827/1000000: episode: 5789, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 60.472, mean reward: 0.605 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.838, 10.456], loss: 0.001583, mae: 0.042443, mean_q: 1.173099
 578927/1000000: episode: 5790, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 62.866, mean reward: 0.629 [0.509, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.968, 10.104], loss: 0.001472, mae: 0.041602, mean_q: 1.171735
 579027/1000000: episode: 5791, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 57.798, mean reward: 0.578 [0.512, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.250, 10.098], loss: 0.001527, mae: 0.042458, mean_q: 1.173176
 579127/1000000: episode: 5792, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 61.313, mean reward: 0.613 [0.509, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.860, 10.197], loss: 0.001539, mae: 0.042505, mean_q: 1.174163
 579227/1000000: episode: 5793, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 59.775, mean reward: 0.598 [0.512, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.333, 10.098], loss: 0.001538, mae: 0.042407, mean_q: 1.173260
 579327/1000000: episode: 5794, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 56.883, mean reward: 0.569 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.940, 10.098], loss: 0.001579, mae: 0.042285, mean_q: 1.175343
 579427/1000000: episode: 5795, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.325, mean reward: 0.573 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.612, 10.173], loss: 0.001465, mae: 0.041048, mean_q: 1.169079
 579527/1000000: episode: 5796, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.089, mean reward: 0.571 [0.506, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.976, 10.098], loss: 0.001550, mae: 0.042112, mean_q: 1.172194
 579627/1000000: episode: 5797, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 57.316, mean reward: 0.573 [0.506, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.789, 10.103], loss: 0.001654, mae: 0.043789, mean_q: 1.174876
 579727/1000000: episode: 5798, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.784, mean reward: 0.598 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.662, 10.098], loss: 0.001573, mae: 0.042611, mean_q: 1.169790
 579827/1000000: episode: 5799, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 57.895, mean reward: 0.579 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.697, 10.098], loss: 0.001623, mae: 0.042810, mean_q: 1.172701
 579927/1000000: episode: 5800, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 59.070, mean reward: 0.591 [0.509, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.614, 10.353], loss: 0.001489, mae: 0.041232, mean_q: 1.173552
 580027/1000000: episode: 5801, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 58.159, mean reward: 0.582 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.655, 10.280], loss: 0.001521, mae: 0.042089, mean_q: 1.172518
 580127/1000000: episode: 5802, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 59.223, mean reward: 0.592 [0.508, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.484, 10.098], loss: 0.001568, mae: 0.042276, mean_q: 1.169936
 580227/1000000: episode: 5803, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 58.459, mean reward: 0.585 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.794, 10.326], loss: 0.001477, mae: 0.040873, mean_q: 1.171041
 580327/1000000: episode: 5804, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 57.632, mean reward: 0.576 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.319, 10.098], loss: 0.001486, mae: 0.041280, mean_q: 1.171183
 580427/1000000: episode: 5805, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.969, mean reward: 0.600 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.836, 10.249], loss: 0.001393, mae: 0.039693, mean_q: 1.170313
 580527/1000000: episode: 5806, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 58.212, mean reward: 0.582 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.283, 10.120], loss: 0.001496, mae: 0.041492, mean_q: 1.171376
 580627/1000000: episode: 5807, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.866, mean reward: 0.589 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.733, 10.098], loss: 0.001479, mae: 0.041349, mean_q: 1.167572
 580727/1000000: episode: 5808, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 61.242, mean reward: 0.612 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.517, 10.114], loss: 0.001526, mae: 0.040736, mean_q: 1.167195
 580827/1000000: episode: 5809, duration: 1.343s, episode steps: 100, steps per second: 74, episode reward: 56.715, mean reward: 0.567 [0.499, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.098], loss: 0.001490, mae: 0.041338, mean_q: 1.171462
 580927/1000000: episode: 5810, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 58.259, mean reward: 0.583 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.656, 10.098], loss: 0.001431, mae: 0.040619, mean_q: 1.168351
 581027/1000000: episode: 5811, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.560, mean reward: 0.586 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.817, 10.400], loss: 0.001422, mae: 0.040708, mean_q: 1.167481
 581127/1000000: episode: 5812, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.419, mean reward: 0.584 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.629, 10.216], loss: 0.001428, mae: 0.040160, mean_q: 1.169370
 581227/1000000: episode: 5813, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 58.739, mean reward: 0.587 [0.502, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.372, 10.103], loss: 0.001482, mae: 0.041146, mean_q: 1.170546
 581327/1000000: episode: 5814, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 62.654, mean reward: 0.627 [0.530, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.352, 10.196], loss: 0.001529, mae: 0.041690, mean_q: 1.168904
 581427/1000000: episode: 5815, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 61.154, mean reward: 0.612 [0.518, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.435, 10.098], loss: 0.001499, mae: 0.041774, mean_q: 1.174718
 581527/1000000: episode: 5816, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 57.483, mean reward: 0.575 [0.501, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.187, 10.098], loss: 0.001473, mae: 0.040524, mean_q: 1.171489
 581627/1000000: episode: 5817, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 56.659, mean reward: 0.567 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.856, 10.098], loss: 0.001565, mae: 0.042237, mean_q: 1.173710
 581727/1000000: episode: 5818, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 61.731, mean reward: 0.617 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.255, 10.271], loss: 0.001466, mae: 0.041544, mean_q: 1.171841
 581827/1000000: episode: 5819, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 59.104, mean reward: 0.591 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.409, 10.187], loss: 0.001489, mae: 0.041463, mean_q: 1.173976
 581927/1000000: episode: 5820, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 60.183, mean reward: 0.602 [0.497, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.571, 10.306], loss: 0.001465, mae: 0.040918, mean_q: 1.172122
 582027/1000000: episode: 5821, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 59.320, mean reward: 0.593 [0.508, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.587, 10.185], loss: 0.001475, mae: 0.041338, mean_q: 1.172693
 582127/1000000: episode: 5822, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 58.593, mean reward: 0.586 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.525, 10.098], loss: 0.001469, mae: 0.041323, mean_q: 1.174627
 582227/1000000: episode: 5823, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 58.405, mean reward: 0.584 [0.509, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.758, 10.098], loss: 0.001431, mae: 0.041078, mean_q: 1.171421
 582327/1000000: episode: 5824, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.031, mean reward: 0.580 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.446, 10.308], loss: 0.001442, mae: 0.041305, mean_q: 1.168511
 582427/1000000: episode: 5825, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 61.327, mean reward: 0.613 [0.511, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.098], loss: 0.001440, mae: 0.040890, mean_q: 1.170539
 582527/1000000: episode: 5826, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 56.522, mean reward: 0.565 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 0.001427, mae: 0.040674, mean_q: 1.170333
 582627/1000000: episode: 5827, duration: 1.408s, episode steps: 100, steps per second: 71, episode reward: 60.576, mean reward: 0.606 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.098], loss: 0.001514, mae: 0.042058, mean_q: 1.171057
 582727/1000000: episode: 5828, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 61.068, mean reward: 0.611 [0.510, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.437], loss: 0.001463, mae: 0.041463, mean_q: 1.171111
 582827/1000000: episode: 5829, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 58.709, mean reward: 0.587 [0.515, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.267, 10.249], loss: 0.001394, mae: 0.040503, mean_q: 1.171049
 582927/1000000: episode: 5830, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 56.743, mean reward: 0.567 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.789, 10.144], loss: 0.001436, mae: 0.040107, mean_q: 1.170314
 583027/1000000: episode: 5831, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 56.688, mean reward: 0.567 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.113], loss: 0.001411, mae: 0.040395, mean_q: 1.170076
 583127/1000000: episode: 5832, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 58.759, mean reward: 0.588 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.586, 10.408], loss: 0.001434, mae: 0.040776, mean_q: 1.165262
 583227/1000000: episode: 5833, duration: 1.069s, episode steps: 100, steps per second: 94, episode reward: 59.226, mean reward: 0.592 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.312, 10.098], loss: 0.001458, mae: 0.041295, mean_q: 1.165792
 583327/1000000: episode: 5834, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 59.854, mean reward: 0.599 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.098], loss: 0.001414, mae: 0.040746, mean_q: 1.165165
 583427/1000000: episode: 5835, duration: 1.421s, episode steps: 100, steps per second: 70, episode reward: 57.515, mean reward: 0.575 [0.510, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.349, 10.143], loss: 0.001524, mae: 0.041914, mean_q: 1.167848
 583527/1000000: episode: 5836, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 58.252, mean reward: 0.583 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.762, 10.151], loss: 0.001384, mae: 0.040347, mean_q: 1.167166
 583627/1000000: episode: 5837, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 61.401, mean reward: 0.614 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.579, 10.162], loss: 0.001481, mae: 0.041266, mean_q: 1.167942
 583727/1000000: episode: 5838, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 58.920, mean reward: 0.589 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.098], loss: 0.001419, mae: 0.041272, mean_q: 1.167411
 583827/1000000: episode: 5839, duration: 1.081s, episode steps: 100, steps per second: 92, episode reward: 58.298, mean reward: 0.583 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.607, 10.098], loss: 0.001426, mae: 0.041285, mean_q: 1.166573
 583927/1000000: episode: 5840, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 62.186, mean reward: 0.622 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.409, 10.204], loss: 0.001357, mae: 0.040363, mean_q: 1.163507
 584027/1000000: episode: 5841, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.608, mean reward: 0.576 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.441, 10.194], loss: 0.001362, mae: 0.040281, mean_q: 1.165393
 584127/1000000: episode: 5842, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.550, mean reward: 0.576 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.153], loss: 0.001393, mae: 0.040677, mean_q: 1.164303
 584227/1000000: episode: 5843, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 58.551, mean reward: 0.586 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.731, 10.145], loss: 0.001392, mae: 0.040008, mean_q: 1.162746
 584327/1000000: episode: 5844, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.179, mean reward: 0.582 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.135, 10.149], loss: 0.001333, mae: 0.040158, mean_q: 1.168490
 584427/1000000: episode: 5845, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 59.693, mean reward: 0.597 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.527, 10.098], loss: 0.001380, mae: 0.039833, mean_q: 1.163259
 584527/1000000: episode: 5846, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 61.599, mean reward: 0.616 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.990, 10.301], loss: 0.001380, mae: 0.040286, mean_q: 1.164150
 584627/1000000: episode: 5847, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 59.055, mean reward: 0.591 [0.518, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.233, 10.188], loss: 0.001327, mae: 0.039524, mean_q: 1.168948
 584727/1000000: episode: 5848, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 57.906, mean reward: 0.579 [0.509, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.392, 10.192], loss: 0.001410, mae: 0.040866, mean_q: 1.166293
 584827/1000000: episode: 5849, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.666, mean reward: 0.587 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.456, 10.098], loss: 0.001403, mae: 0.040405, mean_q: 1.166195
 584927/1000000: episode: 5850, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.356, mean reward: 0.584 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.920, 10.283], loss: 0.001286, mae: 0.038895, mean_q: 1.164194
 585027/1000000: episode: 5851, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 57.571, mean reward: 0.576 [0.500, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.098], loss: 0.001380, mae: 0.040215, mean_q: 1.167884
 585127/1000000: episode: 5852, duration: 0.852s, episode steps: 100, steps per second: 117, episode reward: 62.683, mean reward: 0.627 [0.517, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.126, 10.098], loss: 0.001349, mae: 0.040400, mean_q: 1.164463
 585227/1000000: episode: 5853, duration: 0.876s, episode steps: 100, steps per second: 114, episode reward: 58.617, mean reward: 0.586 [0.497, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.298, 10.098], loss: 0.001329, mae: 0.039862, mean_q: 1.167433
 585327/1000000: episode: 5854, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 60.135, mean reward: 0.601 [0.502, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.497, 10.213], loss: 0.001367, mae: 0.040129, mean_q: 1.172874
 585427/1000000: episode: 5855, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.743, mean reward: 0.587 [0.506, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.661, 10.098], loss: 0.001377, mae: 0.039995, mean_q: 1.170138
 585527/1000000: episode: 5856, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 58.553, mean reward: 0.586 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.108], loss: 0.001328, mae: 0.039647, mean_q: 1.167173
 585627/1000000: episode: 5857, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 60.332, mean reward: 0.603 [0.511, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.739, 10.098], loss: 0.001388, mae: 0.040296, mean_q: 1.172870
 585727/1000000: episode: 5858, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 62.738, mean reward: 0.627 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.006, 10.518], loss: 0.001383, mae: 0.040613, mean_q: 1.170107
 585827/1000000: episode: 5859, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.196, mean reward: 0.582 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.084, 10.098], loss: 0.001422, mae: 0.040818, mean_q: 1.173041
 585927/1000000: episode: 5860, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 58.341, mean reward: 0.583 [0.498, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.875, 10.290], loss: 0.001423, mae: 0.040720, mean_q: 1.167605
 586027/1000000: episode: 5861, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 57.151, mean reward: 0.572 [0.504, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.898, 10.111], loss: 0.001335, mae: 0.039658, mean_q: 1.169833
 586127/1000000: episode: 5862, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 58.087, mean reward: 0.581 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.136, 10.182], loss: 0.001330, mae: 0.039112, mean_q: 1.172699
 586227/1000000: episode: 5863, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 62.695, mean reward: 0.627 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.647, 10.098], loss: 0.001297, mae: 0.039220, mean_q: 1.167694
 586327/1000000: episode: 5864, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 61.544, mean reward: 0.615 [0.507, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.111, 10.321], loss: 0.001442, mae: 0.041493, mean_q: 1.171400
 586427/1000000: episode: 5865, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 59.416, mean reward: 0.594 [0.499, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.742, 10.342], loss: 0.001254, mae: 0.038676, mean_q: 1.167499
 586527/1000000: episode: 5866, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 62.269, mean reward: 0.623 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.441], loss: 0.001359, mae: 0.040296, mean_q: 1.168662
 586627/1000000: episode: 5867, duration: 1.324s, episode steps: 100, steps per second: 76, episode reward: 59.309, mean reward: 0.593 [0.513, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.343], loss: 0.001271, mae: 0.038972, mean_q: 1.173645
 586727/1000000: episode: 5868, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.258, mean reward: 0.593 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.256], loss: 0.001234, mae: 0.038422, mean_q: 1.173118
 586827/1000000: episode: 5869, duration: 0.887s, episode steps: 100, steps per second: 113, episode reward: 60.676, mean reward: 0.607 [0.506, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.349, 10.152], loss: 0.001231, mae: 0.038535, mean_q: 1.170322
 586927/1000000: episode: 5870, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 63.760, mean reward: 0.638 [0.525, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.603, 10.098], loss: 0.001360, mae: 0.040517, mean_q: 1.174408
 587027/1000000: episode: 5871, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 59.980, mean reward: 0.600 [0.517, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.311], loss: 0.001388, mae: 0.040836, mean_q: 1.174219
 587127/1000000: episode: 5872, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.890, mean reward: 0.579 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.627, 10.164], loss: 0.001290, mae: 0.038862, mean_q: 1.174657
 587227/1000000: episode: 5873, duration: 1.275s, episode steps: 100, steps per second: 78, episode reward: 58.556, mean reward: 0.586 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.182, 10.252], loss: 0.001326, mae: 0.039584, mean_q: 1.177305
 587327/1000000: episode: 5874, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 58.222, mean reward: 0.582 [0.507, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.559, 10.271], loss: 0.001357, mae: 0.039726, mean_q: 1.178973
 587427/1000000: episode: 5875, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 57.130, mean reward: 0.571 [0.508, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.905, 10.152], loss: 0.001399, mae: 0.040487, mean_q: 1.172765
 587527/1000000: episode: 5876, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.368, mean reward: 0.574 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.624, 10.098], loss: 0.001357, mae: 0.039512, mean_q: 1.171083
 587627/1000000: episode: 5877, duration: 0.872s, episode steps: 100, steps per second: 115, episode reward: 59.278, mean reward: 0.593 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.334, 10.098], loss: 0.001316, mae: 0.038970, mean_q: 1.174346
 587727/1000000: episode: 5878, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 58.236, mean reward: 0.582 [0.499, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.798, 10.263], loss: 0.001338, mae: 0.039260, mean_q: 1.174969
 587827/1000000: episode: 5879, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.208, mean reward: 0.572 [0.506, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.195, 10.241], loss: 0.001300, mae: 0.039015, mean_q: 1.167796
 587927/1000000: episode: 5880, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 59.366, mean reward: 0.594 [0.499, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.691, 10.098], loss: 0.001331, mae: 0.039542, mean_q: 1.171417
 588027/1000000: episode: 5881, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 57.930, mean reward: 0.579 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.910, 10.177], loss: 0.001337, mae: 0.039851, mean_q: 1.172472
 588127/1000000: episode: 5882, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 59.771, mean reward: 0.598 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.002, 10.098], loss: 0.001421, mae: 0.040666, mean_q: 1.174727
 588227/1000000: episode: 5883, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.882, mean reward: 0.579 [0.497, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.650, 10.110], loss: 0.001288, mae: 0.039001, mean_q: 1.168906
 588327/1000000: episode: 5884, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 60.062, mean reward: 0.601 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.809, 10.098], loss: 0.001388, mae: 0.039961, mean_q: 1.173902
 588427/1000000: episode: 5885, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.753, mean reward: 0.588 [0.512, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.575, 10.098], loss: 0.001324, mae: 0.039254, mean_q: 1.172559
 588527/1000000: episode: 5886, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 57.915, mean reward: 0.579 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.734, 10.140], loss: 0.001480, mae: 0.041519, mean_q: 1.172460
 588627/1000000: episode: 5887, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 59.293, mean reward: 0.593 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.186, 10.098], loss: 0.001413, mae: 0.040968, mean_q: 1.172421
 588727/1000000: episode: 5888, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.818, mean reward: 0.598 [0.523, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.175, 10.098], loss: 0.001400, mae: 0.040663, mean_q: 1.169998
 588827/1000000: episode: 5889, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 60.363, mean reward: 0.604 [0.518, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.939, 10.098], loss: 0.001293, mae: 0.039257, mean_q: 1.173509
 588927/1000000: episode: 5890, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.820, mean reward: 0.578 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.115], loss: 0.001488, mae: 0.040965, mean_q: 1.171118
 589027/1000000: episode: 5891, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.833, mean reward: 0.578 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.844, 10.209], loss: 0.001396, mae: 0.040387, mean_q: 1.172832
 589127/1000000: episode: 5892, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.265, mean reward: 0.573 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.758, 10.143], loss: 0.001382, mae: 0.039747, mean_q: 1.171662
 589227/1000000: episode: 5893, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 60.180, mean reward: 0.602 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.137, 10.098], loss: 0.001323, mae: 0.039717, mean_q: 1.170647
 589327/1000000: episode: 5894, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 58.133, mean reward: 0.581 [0.500, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.363, 10.098], loss: 0.001334, mae: 0.039937, mean_q: 1.172979
 589427/1000000: episode: 5895, duration: 1.370s, episode steps: 100, steps per second: 73, episode reward: 59.019, mean reward: 0.590 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.101, 10.098], loss: 0.001457, mae: 0.040393, mean_q: 1.173296
 589527/1000000: episode: 5896, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 56.540, mean reward: 0.565 [0.502, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.511, 10.098], loss: 0.001349, mae: 0.039279, mean_q: 1.169696
 589627/1000000: episode: 5897, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 60.300, mean reward: 0.603 [0.514, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.140, 10.098], loss: 0.001367, mae: 0.040547, mean_q: 1.169490
 589727/1000000: episode: 5898, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 60.742, mean reward: 0.607 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.698, 10.098], loss: 0.001392, mae: 0.039792, mean_q: 1.171118
 589827/1000000: episode: 5899, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 60.993, mean reward: 0.610 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.430], loss: 0.001521, mae: 0.042096, mean_q: 1.175264
 589927/1000000: episode: 5900, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 63.232, mean reward: 0.632 [0.509, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.737, 10.335], loss: 0.001325, mae: 0.039369, mean_q: 1.174164
 590027/1000000: episode: 5901, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 59.810, mean reward: 0.598 [0.513, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.982, 10.115], loss: 0.001455, mae: 0.040991, mean_q: 1.179346
 590127/1000000: episode: 5902, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.233, mean reward: 0.582 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.804, 10.098], loss: 0.001366, mae: 0.039794, mean_q: 1.171989
 590227/1000000: episode: 5903, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 59.941, mean reward: 0.599 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.811, 10.098], loss: 0.001425, mae: 0.040092, mean_q: 1.175055
 590327/1000000: episode: 5904, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 59.646, mean reward: 0.596 [0.506, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.238, 10.098], loss: 0.001385, mae: 0.040047, mean_q: 1.173246
 590427/1000000: episode: 5905, duration: 1.212s, episode steps: 100, steps per second: 82, episode reward: 59.198, mean reward: 0.592 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.388, 10.437], loss: 0.001417, mae: 0.040217, mean_q: 1.178533
 590527/1000000: episode: 5906, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.113, mean reward: 0.571 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.918, 10.151], loss: 0.001434, mae: 0.040672, mean_q: 1.172189
 590627/1000000: episode: 5907, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 60.893, mean reward: 0.609 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.213, 10.331], loss: 0.001362, mae: 0.039956, mean_q: 1.176034
 590727/1000000: episode: 5908, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 61.050, mean reward: 0.611 [0.511, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.065, 10.098], loss: 0.001410, mae: 0.040748, mean_q: 1.172740
 590827/1000000: episode: 5909, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 62.612, mean reward: 0.626 [0.511, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.507, 10.456], loss: 0.001358, mae: 0.039969, mean_q: 1.172959
 590927/1000000: episode: 5910, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 60.947, mean reward: 0.609 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.511, 10.130], loss: 0.001422, mae: 0.040418, mean_q: 1.176465
 591027/1000000: episode: 5911, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 61.160, mean reward: 0.612 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.662, 10.105], loss: 0.001338, mae: 0.039103, mean_q: 1.172805
 591127/1000000: episode: 5912, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.002, mean reward: 0.580 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.302, 10.350], loss: 0.001379, mae: 0.040344, mean_q: 1.175705
 591227/1000000: episode: 5913, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 57.269, mean reward: 0.573 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.160], loss: 0.001429, mae: 0.040094, mean_q: 1.173051
 591327/1000000: episode: 5914, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 62.524, mean reward: 0.625 [0.520, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.929, 10.098], loss: 0.001593, mae: 0.042990, mean_q: 1.178173
 591427/1000000: episode: 5915, duration: 0.904s, episode steps: 100, steps per second: 111, episode reward: 60.822, mean reward: 0.608 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.148, 10.228], loss: 0.001366, mae: 0.039945, mean_q: 1.176352
 591527/1000000: episode: 5916, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.394, mean reward: 0.574 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.134], loss: 0.001419, mae: 0.040876, mean_q: 1.176559
 591627/1000000: episode: 5917, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 60.193, mean reward: 0.602 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.531, 10.098], loss: 0.001475, mae: 0.041224, mean_q: 1.173313
 591727/1000000: episode: 5918, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 57.674, mean reward: 0.577 [0.505, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.184], loss: 0.001456, mae: 0.040918, mean_q: 1.176104
 591827/1000000: episode: 5919, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 59.001, mean reward: 0.590 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-3.002, 10.098], loss: 0.001540, mae: 0.042149, mean_q: 1.177012
 591927/1000000: episode: 5920, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 58.265, mean reward: 0.583 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.630, 10.293], loss: 0.001426, mae: 0.040524, mean_q: 1.171934
 592027/1000000: episode: 5921, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.804, mean reward: 0.578 [0.509, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.154], loss: 0.001434, mae: 0.040906, mean_q: 1.171725
 592127/1000000: episode: 5922, duration: 1.026s, episode steps: 100, steps per second: 98, episode reward: 59.468, mean reward: 0.595 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.341], loss: 0.001448, mae: 0.041028, mean_q: 1.169914
 592227/1000000: episode: 5923, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 58.136, mean reward: 0.581 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.522, 10.222], loss: 0.001391, mae: 0.040834, mean_q: 1.173004
 592327/1000000: episode: 5924, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 57.912, mean reward: 0.579 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.611, 10.132], loss: 0.001522, mae: 0.041895, mean_q: 1.172493
 592427/1000000: episode: 5925, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 58.562, mean reward: 0.586 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.297, 10.125], loss: 0.001450, mae: 0.041574, mean_q: 1.171727
 592527/1000000: episode: 5926, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 58.194, mean reward: 0.582 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.682, 10.098], loss: 0.001493, mae: 0.041781, mean_q: 1.170566
 592627/1000000: episode: 5927, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 60.539, mean reward: 0.605 [0.503, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.684, 10.183], loss: 0.001452, mae: 0.041137, mean_q: 1.169672
 592727/1000000: episode: 5928, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 58.946, mean reward: 0.589 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.649, 10.144], loss: 0.001449, mae: 0.041070, mean_q: 1.170704
 592827/1000000: episode: 5929, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 56.557, mean reward: 0.566 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.654, 10.100], loss: 0.001539, mae: 0.042856, mean_q: 1.171369
 592927/1000000: episode: 5930, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 59.620, mean reward: 0.596 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.732, 10.342], loss: 0.001531, mae: 0.041942, mean_q: 1.171048
 593027/1000000: episode: 5931, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.387, mean reward: 0.584 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.357, 10.282], loss: 0.001418, mae: 0.041198, mean_q: 1.172162
 593127/1000000: episode: 5932, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 57.077, mean reward: 0.571 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.257, 10.171], loss: 0.001415, mae: 0.040661, mean_q: 1.168477
 593227/1000000: episode: 5933, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 58.168, mean reward: 0.582 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.164, 10.190], loss: 0.001514, mae: 0.042075, mean_q: 1.171069
 593327/1000000: episode: 5934, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 61.576, mean reward: 0.616 [0.518, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.125, 10.098], loss: 0.001575, mae: 0.042546, mean_q: 1.174003
 593427/1000000: episode: 5935, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 57.373, mean reward: 0.574 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.456, 10.155], loss: 0.001372, mae: 0.040372, mean_q: 1.170907
 593527/1000000: episode: 5936, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 57.176, mean reward: 0.572 [0.502, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.957, 10.116], loss: 0.001468, mae: 0.041311, mean_q: 1.173202
 593627/1000000: episode: 5937, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 60.057, mean reward: 0.601 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.104, 10.098], loss: 0.001469, mae: 0.041574, mean_q: 1.170238
 593727/1000000: episode: 5938, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.749, mean reward: 0.587 [0.502, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.981, 10.098], loss: 0.001582, mae: 0.042894, mean_q: 1.175777
 593827/1000000: episode: 5939, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 58.493, mean reward: 0.585 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.349, 10.098], loss: 0.001538, mae: 0.042119, mean_q: 1.172675
 593927/1000000: episode: 5940, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 61.078, mean reward: 0.611 [0.519, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.676, 10.098], loss: 0.001562, mae: 0.043020, mean_q: 1.170831
 594027/1000000: episode: 5941, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 57.774, mean reward: 0.578 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.132, 10.173], loss: 0.001482, mae: 0.041439, mean_q: 1.170341
 594127/1000000: episode: 5942, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 56.835, mean reward: 0.568 [0.504, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.788, 10.127], loss: 0.001499, mae: 0.041185, mean_q: 1.169300
 594227/1000000: episode: 5943, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.913, mean reward: 0.579 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.067, 10.098], loss: 0.001469, mae: 0.040907, mean_q: 1.167107
 594327/1000000: episode: 5944, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.068, mean reward: 0.581 [0.503, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.921, 10.098], loss: 0.001564, mae: 0.042558, mean_q: 1.166974
 594427/1000000: episode: 5945, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 59.288, mean reward: 0.593 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.083, 10.193], loss: 0.001487, mae: 0.041766, mean_q: 1.170513
 594527/1000000: episode: 5946, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 58.760, mean reward: 0.588 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.392, 10.098], loss: 0.001537, mae: 0.043032, mean_q: 1.171105
 594627/1000000: episode: 5947, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.945, mean reward: 0.599 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.674, 10.187], loss: 0.001433, mae: 0.040809, mean_q: 1.170235
 594727/1000000: episode: 5948, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 60.023, mean reward: 0.600 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.888, 10.196], loss: 0.001580, mae: 0.042742, mean_q: 1.171603
 594827/1000000: episode: 5949, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.381, mean reward: 0.594 [0.513, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.761, 10.118], loss: 0.001528, mae: 0.042690, mean_q: 1.172282
 594927/1000000: episode: 5950, duration: 0.878s, episode steps: 100, steps per second: 114, episode reward: 58.197, mean reward: 0.582 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.859, 10.098], loss: 0.001502, mae: 0.041980, mean_q: 1.165941
 595027/1000000: episode: 5951, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 59.991, mean reward: 0.600 [0.514, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.209, 10.098], loss: 0.001594, mae: 0.043390, mean_q: 1.168285
 595127/1000000: episode: 5952, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 62.609, mean reward: 0.626 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.135, 10.098], loss: 0.001543, mae: 0.042727, mean_q: 1.167001
 595227/1000000: episode: 5953, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 58.980, mean reward: 0.590 [0.517, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.522, 10.098], loss: 0.001551, mae: 0.042950, mean_q: 1.170106
 595327/1000000: episode: 5954, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 60.816, mean reward: 0.608 [0.510, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.088, 10.098], loss: 0.001590, mae: 0.043574, mean_q: 1.172889
 595427/1000000: episode: 5955, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 57.586, mean reward: 0.576 [0.504, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.888, 10.135], loss: 0.001488, mae: 0.041699, mean_q: 1.172281
 595527/1000000: episode: 5956, duration: 0.888s, episode steps: 100, steps per second: 113, episode reward: 64.430, mean reward: 0.644 [0.506, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.750, 10.534], loss: 0.001540, mae: 0.042405, mean_q: 1.168798
 595627/1000000: episode: 5957, duration: 0.885s, episode steps: 100, steps per second: 113, episode reward: 60.258, mean reward: 0.603 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.730, 10.369], loss: 0.001457, mae: 0.041629, mean_q: 1.172203
 595727/1000000: episode: 5958, duration: 0.886s, episode steps: 100, steps per second: 113, episode reward: 59.295, mean reward: 0.593 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.577, 10.381], loss: 0.001496, mae: 0.042067, mean_q: 1.168622
 595827/1000000: episode: 5959, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 59.147, mean reward: 0.591 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.753, 10.098], loss: 0.001536, mae: 0.042515, mean_q: 1.169337
 595927/1000000: episode: 5960, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 59.428, mean reward: 0.594 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.306], loss: 0.001517, mae: 0.042336, mean_q: 1.168510
 596027/1000000: episode: 5961, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 60.046, mean reward: 0.600 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.098], loss: 0.001639, mae: 0.043605, mean_q: 1.169608
 596127/1000000: episode: 5962, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: 58.270, mean reward: 0.583 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.152, 10.164], loss: 0.001556, mae: 0.042628, mean_q: 1.171968
 596227/1000000: episode: 5963, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 56.886, mean reward: 0.569 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.349, 10.098], loss: 0.001503, mae: 0.042381, mean_q: 1.168715
 596327/1000000: episode: 5964, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 56.751, mean reward: 0.568 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.681, 10.114], loss: 0.001516, mae: 0.041932, mean_q: 1.166383
 596427/1000000: episode: 5965, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.260, mean reward: 0.573 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.409, 10.198], loss: 0.001540, mae: 0.042339, mean_q: 1.169682
 596527/1000000: episode: 5966, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 58.943, mean reward: 0.589 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.289, 10.098], loss: 0.001348, mae: 0.040241, mean_q: 1.165429
 596627/1000000: episode: 5967, duration: 1.588s, episode steps: 100, steps per second: 63, episode reward: 59.908, mean reward: 0.599 [0.513, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.769, 10.297], loss: 0.001496, mae: 0.041964, mean_q: 1.167572
 596727/1000000: episode: 5968, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 62.682, mean reward: 0.627 [0.509, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.405, 10.404], loss: 0.001685, mae: 0.044183, mean_q: 1.168257
 596827/1000000: episode: 5969, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 57.464, mean reward: 0.575 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.123, 10.318], loss: 0.001407, mae: 0.040867, mean_q: 1.168337
 596927/1000000: episode: 5970, duration: 1.590s, episode steps: 100, steps per second: 63, episode reward: 59.324, mean reward: 0.593 [0.514, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.275, 10.098], loss: 0.001480, mae: 0.042532, mean_q: 1.169677
 597027/1000000: episode: 5971, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 59.033, mean reward: 0.590 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.614, 10.229], loss: 0.001513, mae: 0.042350, mean_q: 1.169632
 597127/1000000: episode: 5972, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 56.659, mean reward: 0.567 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.098], loss: 0.001427, mae: 0.041784, mean_q: 1.166013
 597227/1000000: episode: 5973, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 55.973, mean reward: 0.560 [0.498, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.861, 10.117], loss: 0.001415, mae: 0.041263, mean_q: 1.164748
 597327/1000000: episode: 5974, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 56.880, mean reward: 0.569 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.098], loss: 0.001425, mae: 0.041562, mean_q: 1.161955
 597427/1000000: episode: 5975, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 56.484, mean reward: 0.565 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.474, 10.147], loss: 0.001413, mae: 0.040800, mean_q: 1.168011
 597527/1000000: episode: 5976, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.426, mean reward: 0.594 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.642, 10.293], loss: 0.001448, mae: 0.041692, mean_q: 1.167300
 597627/1000000: episode: 5977, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 58.752, mean reward: 0.588 [0.502, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.374, 10.098], loss: 0.001380, mae: 0.040111, mean_q: 1.162514
 597727/1000000: episode: 5978, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 56.814, mean reward: 0.568 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.412, 10.216], loss: 0.001448, mae: 0.041690, mean_q: 1.166792
 597827/1000000: episode: 5979, duration: 0.929s, episode steps: 100, steps per second: 108, episode reward: 58.320, mean reward: 0.583 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.435, 10.125], loss: 0.001487, mae: 0.042235, mean_q: 1.166746
 597927/1000000: episode: 5980, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 59.311, mean reward: 0.593 [0.514, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.887, 10.244], loss: 0.001493, mae: 0.042100, mean_q: 1.166152
 598027/1000000: episode: 5981, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 60.266, mean reward: 0.603 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.229], loss: 0.001335, mae: 0.040221, mean_q: 1.167745
 598127/1000000: episode: 5982, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.140, mean reward: 0.571 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.346, 10.098], loss: 0.001548, mae: 0.042863, mean_q: 1.166500
 598227/1000000: episode: 5983, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 59.204, mean reward: 0.592 [0.517, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.614, 10.098], loss: 0.001318, mae: 0.039255, mean_q: 1.162858
 598327/1000000: episode: 5984, duration: 1.564s, episode steps: 100, steps per second: 64, episode reward: 58.000, mean reward: 0.580 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.192, 10.098], loss: 0.001309, mae: 0.039804, mean_q: 1.163231
 598427/1000000: episode: 5985, duration: 1.641s, episode steps: 100, steps per second: 61, episode reward: 63.282, mean reward: 0.633 [0.542, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.690, 10.355], loss: 0.001339, mae: 0.039842, mean_q: 1.167446
 598527/1000000: episode: 5986, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.839, mean reward: 0.588 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.373, 10.098], loss: 0.001489, mae: 0.041578, mean_q: 1.164987
 598627/1000000: episode: 5987, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 57.407, mean reward: 0.574 [0.513, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.495, 10.179], loss: 0.001373, mae: 0.040170, mean_q: 1.167018
 598727/1000000: episode: 5988, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 60.846, mean reward: 0.608 [0.515, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.459, 10.098], loss: 0.001312, mae: 0.039589, mean_q: 1.165102
 598827/1000000: episode: 5989, duration: 0.897s, episode steps: 100, steps per second: 112, episode reward: 62.768, mean reward: 0.628 [0.509, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.935, 10.098], loss: 0.001357, mae: 0.040124, mean_q: 1.164770
 598927/1000000: episode: 5990, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.041, mean reward: 0.580 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.573, 10.204], loss: 0.001353, mae: 0.040058, mean_q: 1.166709
 599027/1000000: episode: 5991, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 57.920, mean reward: 0.579 [0.502, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.406, 10.217], loss: 0.001383, mae: 0.040909, mean_q: 1.167545
 599127/1000000: episode: 5992, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.217, mean reward: 0.582 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.478, 10.240], loss: 0.001433, mae: 0.041029, mean_q: 1.169659
 599227/1000000: episode: 5993, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.521, mean reward: 0.575 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.972, 10.098], loss: 0.001320, mae: 0.039800, mean_q: 1.168157
 599327/1000000: episode: 5994, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 56.910, mean reward: 0.569 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.162, 10.403], loss: 0.001402, mae: 0.041028, mean_q: 1.167548
 599427/1000000: episode: 5995, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.982, mean reward: 0.600 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.742, 10.101], loss: 0.001237, mae: 0.039012, mean_q: 1.165182
 599527/1000000: episode: 5996, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.431, mean reward: 0.594 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.002, 10.277], loss: 0.001416, mae: 0.040747, mean_q: 1.168914
 599627/1000000: episode: 5997, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 58.083, mean reward: 0.581 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.805, 10.244], loss: 0.001298, mae: 0.039316, mean_q: 1.167600
 599727/1000000: episode: 5998, duration: 0.905s, episode steps: 100, steps per second: 111, episode reward: 58.070, mean reward: 0.581 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.079, 10.313], loss: 0.001288, mae: 0.038873, mean_q: 1.166774
 599827/1000000: episode: 5999, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.194, mean reward: 0.582 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.162, 10.098], loss: 0.001320, mae: 0.039827, mean_q: 1.165533
 599927/1000000: episode: 6000, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 57.698, mean reward: 0.577 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.713, 10.146], loss: 0.001287, mae: 0.039655, mean_q: 1.167563
 600027/1000000: episode: 6001, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.189, mean reward: 0.582 [0.507, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.064, 10.189], loss: 0.001341, mae: 0.039775, mean_q: 1.163433
 600127/1000000: episode: 6002, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 59.101, mean reward: 0.591 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.617, 10.203], loss: 0.001338, mae: 0.040315, mean_q: 1.166075
 600227/1000000: episode: 6003, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 61.723, mean reward: 0.617 [0.522, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.501, 10.550], loss: 0.001343, mae: 0.039948, mean_q: 1.161350
 600327/1000000: episode: 6004, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.832, mean reward: 0.598 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.707, 10.098], loss: 0.001347, mae: 0.040291, mean_q: 1.163826
 600427/1000000: episode: 6005, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.360, mean reward: 0.594 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.688, 10.172], loss: 0.001391, mae: 0.041130, mean_q: 1.162959
 600527/1000000: episode: 6006, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 56.505, mean reward: 0.565 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.853, 10.200], loss: 0.001357, mae: 0.040148, mean_q: 1.162638
 600627/1000000: episode: 6007, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 60.030, mean reward: 0.600 [0.509, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.309, 10.237], loss: 0.001487, mae: 0.042191, mean_q: 1.160729
 600727/1000000: episode: 6008, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 57.629, mean reward: 0.576 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.302, 10.098], loss: 0.001389, mae: 0.040591, mean_q: 1.162618
 600827/1000000: episode: 6009, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.478, mean reward: 0.585 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.110, 10.098], loss: 0.001372, mae: 0.040925, mean_q: 1.162704
 600927/1000000: episode: 6010, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 56.614, mean reward: 0.566 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.257, 10.165], loss: 0.001385, mae: 0.040409, mean_q: 1.159751
 601027/1000000: episode: 6011, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 60.106, mean reward: 0.601 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.030, 10.098], loss: 0.001377, mae: 0.040463, mean_q: 1.160326
 601127/1000000: episode: 6012, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 58.140, mean reward: 0.581 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.997, 10.147], loss: 0.001403, mae: 0.040423, mean_q: 1.157777
 601227/1000000: episode: 6013, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 57.537, mean reward: 0.575 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.897, 10.098], loss: 0.001462, mae: 0.041366, mean_q: 1.161613
 601327/1000000: episode: 6014, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 59.952, mean reward: 0.600 [0.516, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.098], loss: 0.001401, mae: 0.041181, mean_q: 1.163444
 601427/1000000: episode: 6015, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 56.572, mean reward: 0.566 [0.507, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.274], loss: 0.001428, mae: 0.040977, mean_q: 1.166496
 601527/1000000: episode: 6016, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.288, mean reward: 0.593 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.904, 10.098], loss: 0.001426, mae: 0.040564, mean_q: 1.162515
 601627/1000000: episode: 6017, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 59.065, mean reward: 0.591 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.513, 10.098], loss: 0.001422, mae: 0.041185, mean_q: 1.161713
 601727/1000000: episode: 6018, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.794, mean reward: 0.588 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.123, 10.098], loss: 0.001350, mae: 0.040130, mean_q: 1.159817
 601827/1000000: episode: 6019, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 59.540, mean reward: 0.595 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.422, 10.417], loss: 0.001352, mae: 0.039926, mean_q: 1.159255
 601927/1000000: episode: 6020, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.687, mean reward: 0.587 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.666, 10.110], loss: 0.001407, mae: 0.040810, mean_q: 1.163680
 602027/1000000: episode: 6021, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 59.410, mean reward: 0.594 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.380], loss: 0.001371, mae: 0.039915, mean_q: 1.155301
 602127/1000000: episode: 6022, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 57.837, mean reward: 0.578 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.752, 10.243], loss: 0.001410, mae: 0.040912, mean_q: 1.160735
 602227/1000000: episode: 6023, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 58.362, mean reward: 0.584 [0.512, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.949, 10.098], loss: 0.001490, mae: 0.041980, mean_q: 1.160948
 602327/1000000: episode: 6024, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.559, mean reward: 0.586 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.611, 10.098], loss: 0.001368, mae: 0.040034, mean_q: 1.159146
 602427/1000000: episode: 6025, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 56.782, mean reward: 0.568 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.098], loss: 0.001318, mae: 0.039595, mean_q: 1.159374
 602527/1000000: episode: 6026, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 58.023, mean reward: 0.580 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.954, 10.098], loss: 0.001477, mae: 0.042033, mean_q: 1.161312
 602627/1000000: episode: 6027, duration: 0.909s, episode steps: 100, steps per second: 110, episode reward: 61.305, mean reward: 0.613 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.168], loss: 0.001369, mae: 0.040515, mean_q: 1.163090
 602727/1000000: episode: 6028, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 57.835, mean reward: 0.578 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.327, 10.098], loss: 0.001380, mae: 0.039906, mean_q: 1.163618
 602827/1000000: episode: 6029, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 57.474, mean reward: 0.575 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.564, 10.148], loss: 0.001408, mae: 0.040520, mean_q: 1.166133
 602927/1000000: episode: 6030, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 59.811, mean reward: 0.598 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.589, 10.244], loss: 0.001491, mae: 0.041750, mean_q: 1.165060
 603027/1000000: episode: 6031, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 58.609, mean reward: 0.586 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.665, 10.187], loss: 0.001397, mae: 0.041202, mean_q: 1.166174
 603127/1000000: episode: 6032, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 59.498, mean reward: 0.595 [0.500, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.543, 10.242], loss: 0.001340, mae: 0.039688, mean_q: 1.161870
 603227/1000000: episode: 6033, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 58.956, mean reward: 0.590 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.800, 10.098], loss: 0.001368, mae: 0.040299, mean_q: 1.162912
 603327/1000000: episode: 6034, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 60.635, mean reward: 0.606 [0.512, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.314, 10.265], loss: 0.001330, mae: 0.039710, mean_q: 1.161671
 603427/1000000: episode: 6035, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 60.226, mean reward: 0.602 [0.514, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.478, 10.098], loss: 0.001307, mae: 0.040058, mean_q: 1.160437
 603527/1000000: episode: 6036, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 58.738, mean reward: 0.587 [0.515, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.843, 10.098], loss: 0.001268, mae: 0.039003, mean_q: 1.158447
 603627/1000000: episode: 6037, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 62.057, mean reward: 0.621 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.385, 10.472], loss: 0.001427, mae: 0.040945, mean_q: 1.164306
 603727/1000000: episode: 6038, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 58.917, mean reward: 0.589 [0.512, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.887, 10.098], loss: 0.001457, mae: 0.041369, mean_q: 1.164658
 603827/1000000: episode: 6039, duration: 1.326s, episode steps: 100, steps per second: 75, episode reward: 61.340, mean reward: 0.613 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.708, 10.098], loss: 0.001414, mae: 0.040495, mean_q: 1.163224
 603927/1000000: episode: 6040, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 60.484, mean reward: 0.605 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.232, 10.098], loss: 0.001347, mae: 0.039901, mean_q: 1.164323
 604027/1000000: episode: 6041, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 61.374, mean reward: 0.614 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.549, 10.098], loss: 0.001400, mae: 0.040507, mean_q: 1.165272
 604127/1000000: episode: 6042, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 56.869, mean reward: 0.569 [0.497, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.098], loss: 0.001329, mae: 0.039168, mean_q: 1.166919
 604227/1000000: episode: 6043, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 56.474, mean reward: 0.565 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.288, 10.183], loss: 0.001280, mae: 0.038885, mean_q: 1.165032
 604327/1000000: episode: 6044, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 57.251, mean reward: 0.573 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.001321, mae: 0.039355, mean_q: 1.162392
 604427/1000000: episode: 6045, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 60.482, mean reward: 0.605 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.212, 10.304], loss: 0.001307, mae: 0.039585, mean_q: 1.164235
 604527/1000000: episode: 6046, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 58.097, mean reward: 0.581 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.618, 10.098], loss: 0.001332, mae: 0.039651, mean_q: 1.165369
 604627/1000000: episode: 6047, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 60.971, mean reward: 0.610 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.512, 10.098], loss: 0.001471, mae: 0.041073, mean_q: 1.167175
 604727/1000000: episode: 6048, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 59.533, mean reward: 0.595 [0.522, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.351, 10.351], loss: 0.001443, mae: 0.040897, mean_q: 1.166004
 604827/1000000: episode: 6049, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.039, mean reward: 0.590 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.029, 10.098], loss: 0.001396, mae: 0.040581, mean_q: 1.163853
 604927/1000000: episode: 6050, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 63.988, mean reward: 0.640 [0.546, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.338, 10.098], loss: 0.001396, mae: 0.040250, mean_q: 1.170289
 605027/1000000: episode: 6051, duration: 0.896s, episode steps: 100, steps per second: 112, episode reward: 59.090, mean reward: 0.591 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.509, 10.098], loss: 0.001293, mae: 0.039659, mean_q: 1.164968
 605127/1000000: episode: 6052, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 57.151, mean reward: 0.572 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.303, 10.098], loss: 0.001353, mae: 0.040077, mean_q: 1.166247
 605227/1000000: episode: 6053, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 59.650, mean reward: 0.597 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.375, 10.194], loss: 0.001366, mae: 0.039946, mean_q: 1.165281
 605327/1000000: episode: 6054, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 56.885, mean reward: 0.569 [0.503, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.613, 10.224], loss: 0.001361, mae: 0.040052, mean_q: 1.167696
 605427/1000000: episode: 6055, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.419, mean reward: 0.574 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.586, 10.098], loss: 0.001341, mae: 0.039534, mean_q: 1.169513
 605527/1000000: episode: 6056, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 62.441, mean reward: 0.624 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.285, 10.407], loss: 0.001417, mae: 0.040304, mean_q: 1.168510
 605627/1000000: episode: 6057, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 58.693, mean reward: 0.587 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.224], loss: 0.001405, mae: 0.040147, mean_q: 1.165677
 605727/1000000: episode: 6058, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.920, mean reward: 0.599 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.649, 10.410], loss: 0.001325, mae: 0.039798, mean_q: 1.166355
 605827/1000000: episode: 6059, duration: 1.339s, episode steps: 100, steps per second: 75, episode reward: 57.161, mean reward: 0.572 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.677, 10.123], loss: 0.001421, mae: 0.040503, mean_q: 1.169034
 605927/1000000: episode: 6060, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 58.398, mean reward: 0.584 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.267, 10.098], loss: 0.001413, mae: 0.040950, mean_q: 1.171338
 606027/1000000: episode: 6061, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 57.870, mean reward: 0.579 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.711, 10.098], loss: 0.001354, mae: 0.040181, mean_q: 1.167477
 606127/1000000: episode: 6062, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 60.160, mean reward: 0.602 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.960, 10.098], loss: 0.001352, mae: 0.039714, mean_q: 1.170556
 606227/1000000: episode: 6063, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.503, mean reward: 0.585 [0.515, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.626, 10.218], loss: 0.001294, mae: 0.039598, mean_q: 1.167911
 606327/1000000: episode: 6064, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 57.820, mean reward: 0.578 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.750, 10.155], loss: 0.001402, mae: 0.040458, mean_q: 1.170055
 606427/1000000: episode: 6065, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 57.601, mean reward: 0.576 [0.509, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.041, 10.115], loss: 0.001390, mae: 0.039877, mean_q: 1.169065
 606527/1000000: episode: 6066, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.615, mean reward: 0.586 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.205], loss: 0.001410, mae: 0.041160, mean_q: 1.169417
 606627/1000000: episode: 6067, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 59.752, mean reward: 0.598 [0.509, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.572, 10.135], loss: 0.001337, mae: 0.040046, mean_q: 1.169280
 606727/1000000: episode: 6068, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.104, mean reward: 0.591 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.708, 10.371], loss: 0.001457, mae: 0.041517, mean_q: 1.169256
 606827/1000000: episode: 6069, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 59.063, mean reward: 0.591 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.815, 10.374], loss: 0.001403, mae: 0.040220, mean_q: 1.169017
 606927/1000000: episode: 6070, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 61.458, mean reward: 0.615 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.633, 10.098], loss: 0.001480, mae: 0.041378, mean_q: 1.173773
 607027/1000000: episode: 6071, duration: 1.385s, episode steps: 100, steps per second: 72, episode reward: 58.271, mean reward: 0.583 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.495, 10.281], loss: 0.001510, mae: 0.041906, mean_q: 1.169796
 607127/1000000: episode: 6072, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.992, mean reward: 0.600 [0.517, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.098], loss: 0.001450, mae: 0.041775, mean_q: 1.169535
 607227/1000000: episode: 6073, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 58.194, mean reward: 0.582 [0.499, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.765, 10.109], loss: 0.001450, mae: 0.041798, mean_q: 1.170391
 607327/1000000: episode: 6074, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.436, mean reward: 0.594 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.010, 10.141], loss: 0.001521, mae: 0.042581, mean_q: 1.168215
 607427/1000000: episode: 6075, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 61.229, mean reward: 0.612 [0.519, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.437, 10.318], loss: 0.001427, mae: 0.041358, mean_q: 1.170019
 607527/1000000: episode: 6076, duration: 1.357s, episode steps: 100, steps per second: 74, episode reward: 59.473, mean reward: 0.595 [0.509, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.281, 10.162], loss: 0.001510, mae: 0.041923, mean_q: 1.173681
 607627/1000000: episode: 6077, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 58.444, mean reward: 0.584 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.695, 10.098], loss: 0.001436, mae: 0.041492, mean_q: 1.175672
 607727/1000000: episode: 6078, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 57.486, mean reward: 0.575 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.311, 10.098], loss: 0.001460, mae: 0.041726, mean_q: 1.172269
 607827/1000000: episode: 6079, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 61.518, mean reward: 0.615 [0.509, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.616, 10.098], loss: 0.001482, mae: 0.041615, mean_q: 1.173469
 607927/1000000: episode: 6080, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.265, mean reward: 0.573 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.379, 10.110], loss: 0.001491, mae: 0.041806, mean_q: 1.173418
 608027/1000000: episode: 6081, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 55.923, mean reward: 0.559 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.052, 10.098], loss: 0.001516, mae: 0.042297, mean_q: 1.169858
 608127/1000000: episode: 6082, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 56.761, mean reward: 0.568 [0.498, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.762, 10.191], loss: 0.001415, mae: 0.041178, mean_q: 1.171447
 608227/1000000: episode: 6083, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 59.299, mean reward: 0.593 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.098], loss: 0.001480, mae: 0.042153, mean_q: 1.169973
 608327/1000000: episode: 6084, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 59.525, mean reward: 0.595 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.962, 10.271], loss: 0.001441, mae: 0.041796, mean_q: 1.173789
 608427/1000000: episode: 6085, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 56.286, mean reward: 0.563 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.551, 10.122], loss: 0.001437, mae: 0.041316, mean_q: 1.168160
 608527/1000000: episode: 6086, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 60.054, mean reward: 0.601 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.518, 10.294], loss: 0.001380, mae: 0.040540, mean_q: 1.168960
 608627/1000000: episode: 6087, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 58.548, mean reward: 0.585 [0.510, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.816, 10.119], loss: 0.001477, mae: 0.042386, mean_q: 1.168386
 608727/1000000: episode: 6088, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 59.267, mean reward: 0.593 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.286, 10.098], loss: 0.001557, mae: 0.042774, mean_q: 1.165611
 608827/1000000: episode: 6089, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 58.882, mean reward: 0.589 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.515, 10.098], loss: 0.001436, mae: 0.041278, mean_q: 1.166871
 608927/1000000: episode: 6090, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 61.581, mean reward: 0.616 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.457, 10.098], loss: 0.001400, mae: 0.041126, mean_q: 1.167520
 609027/1000000: episode: 6091, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 58.932, mean reward: 0.589 [0.497, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.250, 10.232], loss: 0.001485, mae: 0.042087, mean_q: 1.167715
 609127/1000000: episode: 6092, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 58.589, mean reward: 0.586 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.749, 10.142], loss: 0.001467, mae: 0.041913, mean_q: 1.165591
 609227/1000000: episode: 6093, duration: 1.262s, episode steps: 100, steps per second: 79, episode reward: 58.022, mean reward: 0.580 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.950, 10.098], loss: 0.001492, mae: 0.042188, mean_q: 1.169217
 609327/1000000: episode: 6094, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 58.565, mean reward: 0.586 [0.507, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.072, 10.098], loss: 0.001541, mae: 0.042691, mean_q: 1.169890
 609427/1000000: episode: 6095, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 58.879, mean reward: 0.589 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.098], loss: 0.001520, mae: 0.042127, mean_q: 1.170516
 609527/1000000: episode: 6096, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.519, mean reward: 0.585 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.378, 10.138], loss: 0.001462, mae: 0.041735, mean_q: 1.164211
 609627/1000000: episode: 6097, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 56.633, mean reward: 0.566 [0.502, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.151], loss: 0.001471, mae: 0.041973, mean_q: 1.165089
 609727/1000000: episode: 6098, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 59.533, mean reward: 0.595 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.625, 10.098], loss: 0.001450, mae: 0.041539, mean_q: 1.163596
 609827/1000000: episode: 6099, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 58.502, mean reward: 0.585 [0.505, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.181, 10.198], loss: 0.001432, mae: 0.040797, mean_q: 1.168040
 609927/1000000: episode: 6100, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 58.011, mean reward: 0.580 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.117, 10.098], loss: 0.001459, mae: 0.041762, mean_q: 1.162569
 610027/1000000: episode: 6101, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 61.616, mean reward: 0.616 [0.513, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.600, 10.151], loss: 0.001363, mae: 0.040521, mean_q: 1.164573
 610127/1000000: episode: 6102, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 57.925, mean reward: 0.579 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.098], loss: 0.001462, mae: 0.041975, mean_q: 1.164780
 610227/1000000: episode: 6103, duration: 1.575s, episode steps: 100, steps per second: 63, episode reward: 60.841, mean reward: 0.608 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.822, 10.098], loss: 0.001336, mae: 0.040291, mean_q: 1.165340
 610327/1000000: episode: 6104, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 62.359, mean reward: 0.624 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.237], loss: 0.001406, mae: 0.040788, mean_q: 1.167518
 610427/1000000: episode: 6105, duration: 1.280s, episode steps: 100, steps per second: 78, episode reward: 58.972, mean reward: 0.590 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.946, 10.413], loss: 0.001401, mae: 0.040782, mean_q: 1.166864
 610527/1000000: episode: 6106, duration: 1.272s, episode steps: 100, steps per second: 79, episode reward: 59.932, mean reward: 0.599 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.373, 10.106], loss: 0.001530, mae: 0.042036, mean_q: 1.168672
 610627/1000000: episode: 6107, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.063, mean reward: 0.581 [0.512, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.313, 10.248], loss: 0.001424, mae: 0.040984, mean_q: 1.165700
 610727/1000000: episode: 6108, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 61.589, mean reward: 0.616 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.863, 10.181], loss: 0.001464, mae: 0.041493, mean_q: 1.167625
 610827/1000000: episode: 6109, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 57.850, mean reward: 0.578 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.316, 10.098], loss: 0.001504, mae: 0.042773, mean_q: 1.167668
 610927/1000000: episode: 6110, duration: 1.283s, episode steps: 100, steps per second: 78, episode reward: 59.250, mean reward: 0.593 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.472, 10.143], loss: 0.001406, mae: 0.041077, mean_q: 1.166587
 611027/1000000: episode: 6111, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 59.115, mean reward: 0.591 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.701, 10.098], loss: 0.001355, mae: 0.040593, mean_q: 1.169993
 611127/1000000: episode: 6112, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 58.817, mean reward: 0.588 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.962, 10.100], loss: 0.001379, mae: 0.040271, mean_q: 1.165725
 611227/1000000: episode: 6113, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 59.624, mean reward: 0.596 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.209, 10.098], loss: 0.001421, mae: 0.041032, mean_q: 1.167393
 611327/1000000: episode: 6114, duration: 1.286s, episode steps: 100, steps per second: 78, episode reward: 59.186, mean reward: 0.592 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.644, 10.129], loss: 0.001521, mae: 0.042595, mean_q: 1.165902
 611427/1000000: episode: 6115, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 64.555, mean reward: 0.646 [0.510, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.849, 10.396], loss: 0.001623, mae: 0.042913, mean_q: 1.165601
 611527/1000000: episode: 6116, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.992, mean reward: 0.580 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.098], loss: 0.001512, mae: 0.042840, mean_q: 1.173014
 611627/1000000: episode: 6117, duration: 1.257s, episode steps: 100, steps per second: 80, episode reward: 58.633, mean reward: 0.586 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.435, 10.268], loss: 0.001487, mae: 0.042268, mean_q: 1.172328
 611727/1000000: episode: 6118, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 64.473, mean reward: 0.645 [0.513, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.089, 10.098], loss: 0.001580, mae: 0.043655, mean_q: 1.171347
 611827/1000000: episode: 6119, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 58.980, mean reward: 0.590 [0.508, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.196, 10.200], loss: 0.001438, mae: 0.041499, mean_q: 1.171995
 611927/1000000: episode: 6120, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.273, mean reward: 0.573 [0.504, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.245, 10.164], loss: 0.001462, mae: 0.041616, mean_q: 1.174378
 612027/1000000: episode: 6121, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 63.140, mean reward: 0.631 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.359], loss: 0.001460, mae: 0.041443, mean_q: 1.170016
 612127/1000000: episode: 6122, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 58.822, mean reward: 0.588 [0.502, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.115, 10.179], loss: 0.001436, mae: 0.041314, mean_q: 1.175980
 612227/1000000: episode: 6123, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 57.356, mean reward: 0.574 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.267, 10.227], loss: 0.001476, mae: 0.041826, mean_q: 1.172952
 612327/1000000: episode: 6124, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 59.657, mean reward: 0.597 [0.507, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.019, 10.431], loss: 0.001550, mae: 0.043010, mean_q: 1.172764
 612427/1000000: episode: 6125, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 58.850, mean reward: 0.588 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.195, 10.175], loss: 0.001494, mae: 0.042191, mean_q: 1.174339
 612527/1000000: episode: 6126, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 58.179, mean reward: 0.582 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.990, 10.261], loss: 0.001328, mae: 0.039865, mean_q: 1.168882
 612627/1000000: episode: 6127, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 57.290, mean reward: 0.573 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.098], loss: 0.001480, mae: 0.042017, mean_q: 1.168316
 612727/1000000: episode: 6128, duration: 0.895s, episode steps: 100, steps per second: 112, episode reward: 57.781, mean reward: 0.578 [0.515, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.403, 10.217], loss: 0.001445, mae: 0.042062, mean_q: 1.175264
 612827/1000000: episode: 6129, duration: 1.015s, episode steps: 100, steps per second: 98, episode reward: 59.628, mean reward: 0.596 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.889, 10.098], loss: 0.001397, mae: 0.040659, mean_q: 1.171033
 612927/1000000: episode: 6130, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 60.362, mean reward: 0.604 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.839, 10.098], loss: 0.001413, mae: 0.041614, mean_q: 1.170429
 613027/1000000: episode: 6131, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.346, mean reward: 0.573 [0.512, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.433, 10.119], loss: 0.001395, mae: 0.040919, mean_q: 1.170483
 613127/1000000: episode: 6132, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.588, mean reward: 0.576 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.218, 10.162], loss: 0.001454, mae: 0.042035, mean_q: 1.175064
 613227/1000000: episode: 6133, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 62.608, mean reward: 0.626 [0.501, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.644, 10.254], loss: 0.001476, mae: 0.041698, mean_q: 1.173892
 613327/1000000: episode: 6134, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 57.900, mean reward: 0.579 [0.498, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.135], loss: 0.001482, mae: 0.041832, mean_q: 1.172475
 613427/1000000: episode: 6135, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.491, mean reward: 0.575 [0.504, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.098], loss: 0.001454, mae: 0.041298, mean_q: 1.172472
 613527/1000000: episode: 6136, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 60.191, mean reward: 0.602 [0.517, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.888, 10.098], loss: 0.001496, mae: 0.042282, mean_q: 1.172663
 613627/1000000: episode: 6137, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 56.645, mean reward: 0.566 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.765, 10.099], loss: 0.001460, mae: 0.041932, mean_q: 1.173743
 613727/1000000: episode: 6138, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 57.550, mean reward: 0.575 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.557, 10.098], loss: 0.001440, mae: 0.041203, mean_q: 1.168664
 613827/1000000: episode: 6139, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.654, mean reward: 0.587 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.545, 10.120], loss: 0.001444, mae: 0.041330, mean_q: 1.169443
 613927/1000000: episode: 6140, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 57.973, mean reward: 0.580 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.422, 10.098], loss: 0.001454, mae: 0.041589, mean_q: 1.170918
 614027/1000000: episode: 6141, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.482, mean reward: 0.585 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.728, 10.098], loss: 0.001411, mae: 0.041010, mean_q: 1.170570
 614127/1000000: episode: 6142, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 59.631, mean reward: 0.596 [0.512, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.834, 10.114], loss: 0.001475, mae: 0.041794, mean_q: 1.169859
 614227/1000000: episode: 6143, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 62.214, mean reward: 0.622 [0.515, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.141, 10.098], loss: 0.001492, mae: 0.042280, mean_q: 1.171738
 614327/1000000: episode: 6144, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 58.550, mean reward: 0.586 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.110], loss: 0.001531, mae: 0.042707, mean_q: 1.173409
 614427/1000000: episode: 6145, duration: 1.393s, episode steps: 100, steps per second: 72, episode reward: 56.999, mean reward: 0.570 [0.513, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.412, 10.098], loss: 0.001417, mae: 0.041491, mean_q: 1.172367
 614527/1000000: episode: 6146, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 59.252, mean reward: 0.593 [0.498, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.370, 10.098], loss: 0.001408, mae: 0.041136, mean_q: 1.172874
 614627/1000000: episode: 6147, duration: 1.315s, episode steps: 100, steps per second: 76, episode reward: 58.373, mean reward: 0.584 [0.499, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.432, 10.098], loss: 0.001392, mae: 0.040800, mean_q: 1.174577
 614727/1000000: episode: 6148, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 58.852, mean reward: 0.589 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.782, 10.098], loss: 0.001440, mae: 0.041325, mean_q: 1.170092
 614827/1000000: episode: 6149, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 61.914, mean reward: 0.619 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.616, 10.098], loss: 0.001585, mae: 0.042843, mean_q: 1.173313
 614927/1000000: episode: 6150, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 59.358, mean reward: 0.594 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.166, 10.098], loss: 0.001452, mae: 0.041574, mean_q: 1.171986
 615027/1000000: episode: 6151, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 59.682, mean reward: 0.597 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.040, 10.203], loss: 0.001475, mae: 0.041805, mean_q: 1.172085
 615127/1000000: episode: 6152, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 61.615, mean reward: 0.616 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.342, 10.250], loss: 0.001532, mae: 0.042814, mean_q: 1.173028
 615227/1000000: episode: 6153, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 58.028, mean reward: 0.580 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.123, 10.098], loss: 0.001554, mae: 0.042969, mean_q: 1.175070
 615327/1000000: episode: 6154, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.246, mean reward: 0.572 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.911, 10.213], loss: 0.001496, mae: 0.042563, mean_q: 1.172501
 615427/1000000: episode: 6155, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 56.422, mean reward: 0.564 [0.500, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.092, 10.098], loss: 0.001495, mae: 0.041844, mean_q: 1.171092
 615527/1000000: episode: 6156, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 58.588, mean reward: 0.586 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.903, 10.271], loss: 0.001441, mae: 0.041770, mean_q: 1.168557
 615627/1000000: episode: 6157, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 58.812, mean reward: 0.588 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.957, 10.098], loss: 0.001481, mae: 0.041422, mean_q: 1.169542
 615727/1000000: episode: 6158, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.927, mean reward: 0.599 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.170], loss: 0.001510, mae: 0.042275, mean_q: 1.169156
 615827/1000000: episode: 6159, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 60.218, mean reward: 0.602 [0.498, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.471, 10.098], loss: 0.001470, mae: 0.041951, mean_q: 1.166934
 615927/1000000: episode: 6160, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 58.802, mean reward: 0.588 [0.510, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.095, 10.163], loss: 0.001528, mae: 0.042439, mean_q: 1.171838
 616027/1000000: episode: 6161, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 58.179, mean reward: 0.582 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.738, 10.098], loss: 0.001494, mae: 0.041867, mean_q: 1.169152
 616127/1000000: episode: 6162, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 60.586, mean reward: 0.606 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.332, 10.112], loss: 0.001512, mae: 0.042786, mean_q: 1.169395
 616227/1000000: episode: 6163, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 61.340, mean reward: 0.613 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.747, 10.098], loss: 0.001465, mae: 0.041614, mean_q: 1.170712
 616327/1000000: episode: 6164, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.642, mean reward: 0.586 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.406, 10.098], loss: 0.001518, mae: 0.042164, mean_q: 1.171273
 616427/1000000: episode: 6165, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 56.173, mean reward: 0.562 [0.499, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.389, 10.124], loss: 0.001562, mae: 0.042641, mean_q: 1.172488
 616527/1000000: episode: 6166, duration: 0.884s, episode steps: 100, steps per second: 113, episode reward: 58.878, mean reward: 0.589 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.342, 10.241], loss: 0.001413, mae: 0.041029, mean_q: 1.169037
 616627/1000000: episode: 6167, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.091, mean reward: 0.581 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.292, 10.243], loss: 0.001392, mae: 0.040627, mean_q: 1.168022
 616727/1000000: episode: 6168, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 59.269, mean reward: 0.593 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.098], loss: 0.001472, mae: 0.042058, mean_q: 1.164585
 616827/1000000: episode: 6169, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 59.917, mean reward: 0.599 [0.501, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.619, 10.253], loss: 0.001395, mae: 0.040989, mean_q: 1.163917
 616927/1000000: episode: 6170, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 58.139, mean reward: 0.581 [0.501, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.105], loss: 0.001544, mae: 0.042095, mean_q: 1.166016
 617027/1000000: episode: 6171, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 58.297, mean reward: 0.583 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.514, 10.098], loss: 0.001476, mae: 0.041492, mean_q: 1.164887
 617127/1000000: episode: 6172, duration: 1.405s, episode steps: 100, steps per second: 71, episode reward: 57.501, mean reward: 0.575 [0.512, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.210, 10.174], loss: 0.001503, mae: 0.041653, mean_q: 1.164439
 617227/1000000: episode: 6173, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 60.351, mean reward: 0.604 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.478, 10.098], loss: 0.001529, mae: 0.042280, mean_q: 1.165781
 617327/1000000: episode: 6174, duration: 1.378s, episode steps: 100, steps per second: 73, episode reward: 60.275, mean reward: 0.603 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.208, 10.188], loss: 0.001614, mae: 0.043148, mean_q: 1.166348
 617427/1000000: episode: 6175, duration: 1.484s, episode steps: 100, steps per second: 67, episode reward: 58.927, mean reward: 0.589 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.539, 10.098], loss: 0.001425, mae: 0.040831, mean_q: 1.164047
 617527/1000000: episode: 6176, duration: 1.454s, episode steps: 100, steps per second: 69, episode reward: 56.973, mean reward: 0.570 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.890, 10.164], loss: 0.001448, mae: 0.041811, mean_q: 1.165592
 617627/1000000: episode: 6177, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 57.931, mean reward: 0.579 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.293, 10.127], loss: 0.001527, mae: 0.042256, mean_q: 1.166282
 617727/1000000: episode: 6178, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 57.896, mean reward: 0.579 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.009, 10.278], loss: 0.001482, mae: 0.041799, mean_q: 1.167484
 617827/1000000: episode: 6179, duration: 1.307s, episode steps: 100, steps per second: 77, episode reward: 58.438, mean reward: 0.584 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.927, 10.267], loss: 0.001513, mae: 0.042378, mean_q: 1.164841
 617927/1000000: episode: 6180, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 59.727, mean reward: 0.597 [0.500, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.834, 10.098], loss: 0.001485, mae: 0.041511, mean_q: 1.164408
 618027/1000000: episode: 6181, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 58.042, mean reward: 0.580 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.275, 10.242], loss: 0.001434, mae: 0.041345, mean_q: 1.164312
 618127/1000000: episode: 6182, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 60.346, mean reward: 0.603 [0.503, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.149, 10.460], loss: 0.001516, mae: 0.042532, mean_q: 1.166310
 618227/1000000: episode: 6183, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 66.371, mean reward: 0.664 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.704, 10.305], loss: 0.001518, mae: 0.042183, mean_q: 1.167986
 618327/1000000: episode: 6184, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 59.110, mean reward: 0.591 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.830, 10.321], loss: 0.001472, mae: 0.041587, mean_q: 1.171035
 618427/1000000: episode: 6185, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.367, mean reward: 0.604 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.914, 10.098], loss: 0.001442, mae: 0.041386, mean_q: 1.167884
 618527/1000000: episode: 6186, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.913, mean reward: 0.589 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.147, 10.275], loss: 0.001484, mae: 0.041714, mean_q: 1.170871
 618627/1000000: episode: 6187, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 61.026, mean reward: 0.610 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.528, 10.098], loss: 0.001494, mae: 0.041337, mean_q: 1.166426
 618727/1000000: episode: 6188, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.559, mean reward: 0.576 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.098], loss: 0.001494, mae: 0.042024, mean_q: 1.170601
 618827/1000000: episode: 6189, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 64.886, mean reward: 0.649 [0.513, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.358, 10.098], loss: 0.001468, mae: 0.041050, mean_q: 1.170760
 618927/1000000: episode: 6190, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 57.846, mean reward: 0.578 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.229], loss: 0.001578, mae: 0.042297, mean_q: 1.174576
 619027/1000000: episode: 6191, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 59.271, mean reward: 0.593 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.515, 10.098], loss: 0.001602, mae: 0.043556, mean_q: 1.173014
 619127/1000000: episode: 6192, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 59.497, mean reward: 0.595 [0.506, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.788, 10.212], loss: 0.001601, mae: 0.043645, mean_q: 1.177684
 619227/1000000: episode: 6193, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 58.224, mean reward: 0.582 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.234], loss: 0.001475, mae: 0.041557, mean_q: 1.168830
 619327/1000000: episode: 6194, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 60.048, mean reward: 0.600 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.173, 10.171], loss: 0.001549, mae: 0.041900, mean_q: 1.170008
 619427/1000000: episode: 6195, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.313, mean reward: 0.573 [0.498, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.182, 10.149], loss: 0.001515, mae: 0.042098, mean_q: 1.174006
 619527/1000000: episode: 6196, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 59.277, mean reward: 0.593 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.146, 10.098], loss: 0.001510, mae: 0.041857, mean_q: 1.170593
 619627/1000000: episode: 6197, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 57.597, mean reward: 0.576 [0.509, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.028, 10.119], loss: 0.001548, mae: 0.042266, mean_q: 1.172551
 619727/1000000: episode: 6198, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 59.478, mean reward: 0.595 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.355, 10.171], loss: 0.001541, mae: 0.042610, mean_q: 1.174968
 619827/1000000: episode: 6199, duration: 0.985s, episode steps: 100, steps per second: 101, episode reward: 57.536, mean reward: 0.575 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.098], loss: 0.001529, mae: 0.042471, mean_q: 1.171201
 619927/1000000: episode: 6200, duration: 1.482s, episode steps: 100, steps per second: 67, episode reward: 59.639, mean reward: 0.596 [0.521, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.741, 10.252], loss: 0.001514, mae: 0.042392, mean_q: 1.170646
 620027/1000000: episode: 6201, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 59.684, mean reward: 0.597 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.057, 10.262], loss: 0.001520, mae: 0.041927, mean_q: 1.169901
 620127/1000000: episode: 6202, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 58.620, mean reward: 0.586 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.349, 10.098], loss: 0.001373, mae: 0.040263, mean_q: 1.168617
 620227/1000000: episode: 6203, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 59.551, mean reward: 0.596 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.394], loss: 0.001436, mae: 0.041116, mean_q: 1.169578
 620327/1000000: episode: 6204, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 60.770, mean reward: 0.608 [0.516, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.719, 10.346], loss: 0.001479, mae: 0.041193, mean_q: 1.168569
 620427/1000000: episode: 6205, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 61.138, mean reward: 0.611 [0.503, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.647, 10.098], loss: 0.001482, mae: 0.040550, mean_q: 1.169798
 620527/1000000: episode: 6206, duration: 1.321s, episode steps: 100, steps per second: 76, episode reward: 58.029, mean reward: 0.580 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.098], loss: 0.001495, mae: 0.041806, mean_q: 1.172023
 620627/1000000: episode: 6207, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 57.079, mean reward: 0.571 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.451, 10.271], loss: 0.001474, mae: 0.041316, mean_q: 1.170601
 620727/1000000: episode: 6208, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 57.618, mean reward: 0.576 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.213, 10.098], loss: 0.001500, mae: 0.041771, mean_q: 1.171125
 620827/1000000: episode: 6209, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 59.911, mean reward: 0.599 [0.517, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.066, 10.098], loss: 0.001515, mae: 0.042097, mean_q: 1.168802
 620927/1000000: episode: 6210, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 59.602, mean reward: 0.596 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.173, 10.098], loss: 0.001553, mae: 0.042771, mean_q: 1.170545
 621027/1000000: episode: 6211, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 58.944, mean reward: 0.589 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.049, 10.099], loss: 0.001409, mae: 0.040306, mean_q: 1.171632
 621127/1000000: episode: 6212, duration: 1.404s, episode steps: 100, steps per second: 71, episode reward: 58.122, mean reward: 0.581 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.327, 10.286], loss: 0.001446, mae: 0.040675, mean_q: 1.171135
 621227/1000000: episode: 6213, duration: 1.248s, episode steps: 100, steps per second: 80, episode reward: 56.846, mean reward: 0.568 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.663, 10.114], loss: 0.001463, mae: 0.041976, mean_q: 1.167848
 621327/1000000: episode: 6214, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 59.258, mean reward: 0.593 [0.509, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.532, 10.098], loss: 0.001505, mae: 0.041576, mean_q: 1.167106
 621427/1000000: episode: 6215, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 56.618, mean reward: 0.566 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.137, 10.151], loss: 0.001569, mae: 0.042559, mean_q: 1.171070
 621527/1000000: episode: 6216, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 56.699, mean reward: 0.567 [0.499, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.116], loss: 0.001593, mae: 0.042789, mean_q: 1.171759
 621627/1000000: episode: 6217, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 62.155, mean reward: 0.622 [0.501, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.271, 10.541], loss: 0.001591, mae: 0.042700, mean_q: 1.172727
 621727/1000000: episode: 6218, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 61.813, mean reward: 0.618 [0.512, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.743, 10.098], loss: 0.001531, mae: 0.042213, mean_q: 1.169684
 621827/1000000: episode: 6219, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 60.549, mean reward: 0.605 [0.498, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.728, 10.260], loss: 0.001466, mae: 0.041431, mean_q: 1.171885
 621927/1000000: episode: 6220, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 58.627, mean reward: 0.586 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.801, 10.249], loss: 0.001602, mae: 0.042812, mean_q: 1.172986
 622027/1000000: episode: 6221, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 60.209, mean reward: 0.602 [0.514, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.787, 10.176], loss: 0.001516, mae: 0.041701, mean_q: 1.173083
 622127/1000000: episode: 6222, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 58.329, mean reward: 0.583 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.362], loss: 0.001606, mae: 0.043739, mean_q: 1.176541
 622227/1000000: episode: 6223, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.838, mean reward: 0.598 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.155], loss: 0.001532, mae: 0.042347, mean_q: 1.173155
 622327/1000000: episode: 6224, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 60.473, mean reward: 0.605 [0.515, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.072, 10.098], loss: 0.001403, mae: 0.040614, mean_q: 1.167787
 622427/1000000: episode: 6225, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 60.821, mean reward: 0.608 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.792, 10.308], loss: 0.001647, mae: 0.042868, mean_q: 1.169508
 622527/1000000: episode: 6226, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 56.650, mean reward: 0.566 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.323, 10.098], loss: 0.001612, mae: 0.043847, mean_q: 1.176194
 622627/1000000: episode: 6227, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.671, mean reward: 0.597 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.940, 10.265], loss: 0.001577, mae: 0.042366, mean_q: 1.176096
 622727/1000000: episode: 6228, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.386, mean reward: 0.584 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 0.001527, mae: 0.042441, mean_q: 1.173386
 622827/1000000: episode: 6229, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 58.592, mean reward: 0.586 [0.498, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.895, 10.121], loss: 0.001515, mae: 0.041581, mean_q: 1.173771
 622927/1000000: episode: 6230, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.699, mean reward: 0.597 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.793, 10.396], loss: 0.001477, mae: 0.041574, mean_q: 1.172302
 623027/1000000: episode: 6231, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 59.686, mean reward: 0.597 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.062, 10.098], loss: 0.001607, mae: 0.042883, mean_q: 1.182162
 623127/1000000: episode: 6232, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.533, mean reward: 0.585 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.249], loss: 0.001573, mae: 0.043715, mean_q: 1.182058
 623227/1000000: episode: 6233, duration: 1.058s, episode steps: 100, steps per second: 94, episode reward: 58.277, mean reward: 0.583 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.729, 10.206], loss: 0.001550, mae: 0.042126, mean_q: 1.171723
 623327/1000000: episode: 6234, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 60.299, mean reward: 0.603 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.144, 10.430], loss: 0.001449, mae: 0.041545, mean_q: 1.173236
 623427/1000000: episode: 6235, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 56.523, mean reward: 0.565 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.142], loss: 0.001457, mae: 0.040985, mean_q: 1.172119
 623527/1000000: episode: 6236, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 59.209, mean reward: 0.592 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.769, 10.238], loss: 0.001441, mae: 0.040989, mean_q: 1.167863
 623627/1000000: episode: 6237, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 61.337, mean reward: 0.613 [0.509, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.907, 10.289], loss: 0.001448, mae: 0.041049, mean_q: 1.168481
 623727/1000000: episode: 6238, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.189, mean reward: 0.572 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.167], loss: 0.001450, mae: 0.041244, mean_q: 1.171441
 623827/1000000: episode: 6239, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 60.554, mean reward: 0.606 [0.522, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.661, 10.355], loss: 0.001410, mae: 0.040737, mean_q: 1.166173
 623927/1000000: episode: 6240, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 62.436, mean reward: 0.624 [0.511, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.103, 10.098], loss: 0.001609, mae: 0.043463, mean_q: 1.172876
 624027/1000000: episode: 6241, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 58.933, mean reward: 0.589 [0.510, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.484, 10.098], loss: 0.001508, mae: 0.041892, mean_q: 1.171771
 624127/1000000: episode: 6242, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 56.844, mean reward: 0.568 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.558, 10.098], loss: 0.001500, mae: 0.041900, mean_q: 1.170173
 624227/1000000: episode: 6243, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 58.717, mean reward: 0.587 [0.499, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.740, 10.105], loss: 0.001498, mae: 0.041545, mean_q: 1.170001
 624327/1000000: episode: 6244, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 57.389, mean reward: 0.574 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.304, 10.098], loss: 0.001478, mae: 0.041218, mean_q: 1.167627
 624427/1000000: episode: 6245, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.712, mean reward: 0.587 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.098], loss: 0.001405, mae: 0.040665, mean_q: 1.169160
 624527/1000000: episode: 6246, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.607, mean reward: 0.586 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.370, 10.120], loss: 0.001409, mae: 0.040484, mean_q: 1.166397
 624627/1000000: episode: 6247, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 59.999, mean reward: 0.600 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.913, 10.159], loss: 0.001489, mae: 0.041825, mean_q: 1.168054
 624727/1000000: episode: 6248, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 62.893, mean reward: 0.629 [0.505, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.154, 10.098], loss: 0.001499, mae: 0.041597, mean_q: 1.170485
 624827/1000000: episode: 6249, duration: 1.806s, episode steps: 100, steps per second: 55, episode reward: 58.175, mean reward: 0.582 [0.503, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.844, 10.356], loss: 0.001441, mae: 0.041245, mean_q: 1.168578
 624927/1000000: episode: 6250, duration: 1.520s, episode steps: 100, steps per second: 66, episode reward: 58.042, mean reward: 0.580 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.002, 10.099], loss: 0.001476, mae: 0.041425, mean_q: 1.172209
 625027/1000000: episode: 6251, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 59.876, mean reward: 0.599 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.933, 10.147], loss: 0.001481, mae: 0.041883, mean_q: 1.170205
 625127/1000000: episode: 6252, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.752, mean reward: 0.588 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.693, 10.098], loss: 0.001415, mae: 0.040471, mean_q: 1.170394
 625227/1000000: episode: 6253, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 57.890, mean reward: 0.579 [0.498, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 0.001376, mae: 0.040860, mean_q: 1.168894
 625327/1000000: episode: 6254, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.198, mean reward: 0.572 [0.509, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.892, 10.233], loss: 0.001465, mae: 0.041682, mean_q: 1.168630
 625427/1000000: episode: 6255, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 58.674, mean reward: 0.587 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.915, 10.098], loss: 0.001306, mae: 0.039294, mean_q: 1.168801
 625527/1000000: episode: 6256, duration: 0.891s, episode steps: 100, steps per second: 112, episode reward: 59.191, mean reward: 0.592 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.649, 10.132], loss: 0.001363, mae: 0.040046, mean_q: 1.163777
 625627/1000000: episode: 6257, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.448, mean reward: 0.584 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.626, 10.098], loss: 0.001412, mae: 0.040921, mean_q: 1.168699
 625727/1000000: episode: 6258, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 60.422, mean reward: 0.604 [0.517, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.638, 10.098], loss: 0.001407, mae: 0.040471, mean_q: 1.166646
 625827/1000000: episode: 6259, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.951, mean reward: 0.580 [0.498, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.169], loss: 0.001331, mae: 0.039597, mean_q: 1.169563
 625927/1000000: episode: 6260, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 59.205, mean reward: 0.592 [0.503, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.986, 10.290], loss: 0.001399, mae: 0.040148, mean_q: 1.170529
 626027/1000000: episode: 6261, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 58.332, mean reward: 0.583 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.669, 10.154], loss: 0.001429, mae: 0.041253, mean_q: 1.167206
 626127/1000000: episode: 6262, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 61.037, mean reward: 0.610 [0.515, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.477, 10.098], loss: 0.001409, mae: 0.040493, mean_q: 1.167926
 626227/1000000: episode: 6263, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 59.339, mean reward: 0.593 [0.511, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.763, 10.098], loss: 0.001362, mae: 0.039818, mean_q: 1.169975
 626327/1000000: episode: 6264, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.072, mean reward: 0.571 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.010, 10.188], loss: 0.001402, mae: 0.040663, mean_q: 1.170437
 626427/1000000: episode: 6265, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 58.451, mean reward: 0.585 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.911, 10.178], loss: 0.001399, mae: 0.040890, mean_q: 1.174755
 626527/1000000: episode: 6266, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 59.285, mean reward: 0.593 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.678, 10.293], loss: 0.001510, mae: 0.042121, mean_q: 1.172720
 626627/1000000: episode: 6267, duration: 1.768s, episode steps: 100, steps per second: 57, episode reward: 60.538, mean reward: 0.605 [0.513, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.437, 10.098], loss: 0.001387, mae: 0.040384, mean_q: 1.171204
 626727/1000000: episode: 6268, duration: 1.883s, episode steps: 100, steps per second: 53, episode reward: 60.366, mean reward: 0.604 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.586, 10.098], loss: 0.001393, mae: 0.039584, mean_q: 1.170074
 626827/1000000: episode: 6269, duration: 1.494s, episode steps: 100, steps per second: 67, episode reward: 61.415, mean reward: 0.614 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.098], loss: 0.001319, mae: 0.038930, mean_q: 1.167363
 626927/1000000: episode: 6270, duration: 1.665s, episode steps: 100, steps per second: 60, episode reward: 60.279, mean reward: 0.603 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.237, 10.143], loss: 0.001336, mae: 0.039582, mean_q: 1.169958
 627027/1000000: episode: 6271, duration: 1.510s, episode steps: 100, steps per second: 66, episode reward: 63.262, mean reward: 0.633 [0.508, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.886, 10.098], loss: 0.001452, mae: 0.041000, mean_q: 1.169211
 627127/1000000: episode: 6272, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 58.167, mean reward: 0.582 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.098], loss: 0.001513, mae: 0.041829, mean_q: 1.174289
 627227/1000000: episode: 6273, duration: 1.371s, episode steps: 100, steps per second: 73, episode reward: 58.130, mean reward: 0.581 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.878, 10.098], loss: 0.001301, mae: 0.039521, mean_q: 1.174039
 627327/1000000: episode: 6274, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 60.976, mean reward: 0.610 [0.514, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.767, 10.146], loss: 0.001381, mae: 0.040730, mean_q: 1.172576
 627427/1000000: episode: 6275, duration: 1.566s, episode steps: 100, steps per second: 64, episode reward: 58.637, mean reward: 0.586 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.198, 10.266], loss: 0.001438, mae: 0.040770, mean_q: 1.173853
 627527/1000000: episode: 6276, duration: 1.688s, episode steps: 100, steps per second: 59, episode reward: 57.165, mean reward: 0.572 [0.508, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.505, 10.098], loss: 0.001345, mae: 0.039389, mean_q: 1.172302
 627627/1000000: episode: 6277, duration: 1.375s, episode steps: 100, steps per second: 73, episode reward: 61.532, mean reward: 0.615 [0.507, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.743, 10.451], loss: 0.001370, mae: 0.039538, mean_q: 1.174012
 627727/1000000: episode: 6278, duration: 1.549s, episode steps: 100, steps per second: 65, episode reward: 58.852, mean reward: 0.589 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.124, 10.098], loss: 0.001390, mae: 0.040325, mean_q: 1.174681
 627827/1000000: episode: 6279, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 58.955, mean reward: 0.590 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.253, 10.146], loss: 0.001437, mae: 0.040760, mean_q: 1.170067
 627927/1000000: episode: 6280, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 59.870, mean reward: 0.599 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.199, 10.098], loss: 0.001412, mae: 0.040384, mean_q: 1.171498
 628027/1000000: episode: 6281, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 57.327, mean reward: 0.573 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.963, 10.098], loss: 0.001446, mae: 0.041074, mean_q: 1.172400
 628127/1000000: episode: 6282, duration: 1.130s, episode steps: 100, steps per second: 88, episode reward: 59.244, mean reward: 0.592 [0.509, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.766, 10.308], loss: 0.001381, mae: 0.040118, mean_q: 1.168373
 628227/1000000: episode: 6283, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.467, mean reward: 0.585 [0.500, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.237, 10.098], loss: 0.001478, mae: 0.041728, mean_q: 1.171716
 628327/1000000: episode: 6284, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.956, mean reward: 0.590 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.098], loss: 0.001417, mae: 0.040194, mean_q: 1.172033
 628427/1000000: episode: 6285, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 61.944, mean reward: 0.619 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.491, 10.098], loss: 0.001357, mae: 0.039803, mean_q: 1.168622
 628527/1000000: episode: 6286, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 57.705, mean reward: 0.577 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.143], loss: 0.001390, mae: 0.040341, mean_q: 1.174108
 628627/1000000: episode: 6287, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 62.888, mean reward: 0.629 [0.506, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.030, 10.376], loss: 0.001495, mae: 0.041805, mean_q: 1.171071
 628727/1000000: episode: 6288, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 59.403, mean reward: 0.594 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.269, 10.257], loss: 0.001472, mae: 0.040897, mean_q: 1.170292
 628827/1000000: episode: 6289, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 56.741, mean reward: 0.567 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.426, 10.133], loss: 0.001531, mae: 0.042225, mean_q: 1.173732
 628927/1000000: episode: 6290, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 61.952, mean reward: 0.620 [0.509, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.627, 10.189], loss: 0.001314, mae: 0.039213, mean_q: 1.171709
 629027/1000000: episode: 6291, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.046, mean reward: 0.570 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.229], loss: 0.001402, mae: 0.040614, mean_q: 1.168344
 629127/1000000: episode: 6292, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 59.272, mean reward: 0.593 [0.516, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.046, 10.194], loss: 0.001458, mae: 0.041640, mean_q: 1.172857
 629227/1000000: episode: 6293, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.902, mean reward: 0.599 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.444, 10.198], loss: 0.001350, mae: 0.040021, mean_q: 1.172103
 629327/1000000: episode: 6294, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.559, mean reward: 0.586 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.421, 10.195], loss: 0.001427, mae: 0.041139, mean_q: 1.172868
 629427/1000000: episode: 6295, duration: 1.081s, episode steps: 100, steps per second: 92, episode reward: 59.432, mean reward: 0.594 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.308, 10.098], loss: 0.001433, mae: 0.041076, mean_q: 1.173203
 629527/1000000: episode: 6296, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 59.065, mean reward: 0.591 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.098], loss: 0.001489, mae: 0.041545, mean_q: 1.172516
 629627/1000000: episode: 6297, duration: 0.905s, episode steps: 100, steps per second: 110, episode reward: 61.024, mean reward: 0.610 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.098], loss: 0.001449, mae: 0.041115, mean_q: 1.172593
 629727/1000000: episode: 6298, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: 58.392, mean reward: 0.584 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.199, 10.109], loss: 0.001368, mae: 0.040628, mean_q: 1.173143
 629827/1000000: episode: 6299, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.765, mean reward: 0.588 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.734, 10.098], loss: 0.001492, mae: 0.041909, mean_q: 1.169442
 629927/1000000: episode: 6300, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 59.879, mean reward: 0.599 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.450, 10.394], loss: 0.001505, mae: 0.041467, mean_q: 1.177702
 630027/1000000: episode: 6301, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 58.217, mean reward: 0.582 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.110], loss: 0.001446, mae: 0.040856, mean_q: 1.172630
 630127/1000000: episode: 6302, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.812, mean reward: 0.588 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.693, 10.347], loss: 0.001381, mae: 0.040455, mean_q: 1.172541
 630227/1000000: episode: 6303, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.785, mean reward: 0.578 [0.503, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.098], loss: 0.001520, mae: 0.042195, mean_q: 1.172890
 630327/1000000: episode: 6304, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 61.438, mean reward: 0.614 [0.513, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.727, 10.269], loss: 0.001477, mae: 0.041355, mean_q: 1.174871
 630427/1000000: episode: 6305, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.679, mean reward: 0.577 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.505, 10.239], loss: 0.001575, mae: 0.042345, mean_q: 1.173672
 630527/1000000: episode: 6306, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 59.937, mean reward: 0.599 [0.515, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.965, 10.138], loss: 0.001411, mae: 0.040779, mean_q: 1.171825
 630627/1000000: episode: 6307, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 60.302, mean reward: 0.603 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.903, 10.098], loss: 0.001370, mae: 0.040143, mean_q: 1.175005
 630727/1000000: episode: 6308, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.295, mean reward: 0.593 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.508, 10.194], loss: 0.001431, mae: 0.040761, mean_q: 1.174013
 630827/1000000: episode: 6309, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 61.807, mean reward: 0.618 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.893, 10.398], loss: 0.001477, mae: 0.041375, mean_q: 1.174014
 630927/1000000: episode: 6310, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 59.030, mean reward: 0.590 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.209, 10.098], loss: 0.001454, mae: 0.041158, mean_q: 1.177443
 631027/1000000: episode: 6311, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 59.700, mean reward: 0.597 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.358, 10.189], loss: 0.001396, mae: 0.040690, mean_q: 1.176437
 631127/1000000: episode: 6312, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 58.888, mean reward: 0.589 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.791, 10.098], loss: 0.001513, mae: 0.042061, mean_q: 1.178150
 631227/1000000: episode: 6313, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.923, mean reward: 0.589 [0.515, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.001396, mae: 0.040775, mean_q: 1.179645
 631327/1000000: episode: 6314, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 59.216, mean reward: 0.592 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.306], loss: 0.001560, mae: 0.042750, mean_q: 1.178147
 631427/1000000: episode: 6315, duration: 1.611s, episode steps: 100, steps per second: 62, episode reward: 58.407, mean reward: 0.584 [0.508, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.564, 10.098], loss: 0.001488, mae: 0.041681, mean_q: 1.178820
 631527/1000000: episode: 6316, duration: 1.475s, episode steps: 100, steps per second: 68, episode reward: 60.101, mean reward: 0.601 [0.512, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.437], loss: 0.001406, mae: 0.041434, mean_q: 1.175734
 631627/1000000: episode: 6317, duration: 1.424s, episode steps: 100, steps per second: 70, episode reward: 60.466, mean reward: 0.605 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.254, 10.098], loss: 0.001491, mae: 0.042195, mean_q: 1.179909
 631727/1000000: episode: 6318, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 59.042, mean reward: 0.590 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.806, 10.123], loss: 0.001382, mae: 0.040626, mean_q: 1.176647
 631827/1000000: episode: 6319, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.899, mean reward: 0.589 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.175, 10.539], loss: 0.001431, mae: 0.041549, mean_q: 1.176149
 631927/1000000: episode: 6320, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 59.139, mean reward: 0.591 [0.505, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.108, 10.098], loss: 0.001525, mae: 0.042646, mean_q: 1.173752
 632027/1000000: episode: 6321, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 59.936, mean reward: 0.599 [0.508, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.995, 10.098], loss: 0.001410, mae: 0.041274, mean_q: 1.176063
 632127/1000000: episode: 6322, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.773, mean reward: 0.598 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.671, 10.321], loss: 0.001524, mae: 0.042544, mean_q: 1.170207
 632227/1000000: episode: 6323, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.132, mean reward: 0.581 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.297, 10.098], loss: 0.001408, mae: 0.041297, mean_q: 1.174929
 632327/1000000: episode: 6324, duration: 1.401s, episode steps: 100, steps per second: 71, episode reward: 59.477, mean reward: 0.595 [0.509, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.631, 10.098], loss: 0.001482, mae: 0.042128, mean_q: 1.172942
 632427/1000000: episode: 6325, duration: 1.431s, episode steps: 100, steps per second: 70, episode reward: 61.019, mean reward: 0.610 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.698, 10.100], loss: 0.001450, mae: 0.041892, mean_q: 1.179050
 632527/1000000: episode: 6326, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 59.562, mean reward: 0.596 [0.508, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.899, 10.098], loss: 0.001391, mae: 0.040791, mean_q: 1.178842
 632627/1000000: episode: 6327, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 57.643, mean reward: 0.576 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.601, 10.292], loss: 0.001457, mae: 0.042045, mean_q: 1.176786
 632727/1000000: episode: 6328, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 58.556, mean reward: 0.586 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.302, 10.098], loss: 0.001378, mae: 0.040772, mean_q: 1.178021
 632827/1000000: episode: 6329, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 55.999, mean reward: 0.560 [0.498, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.313, 10.178], loss: 0.001435, mae: 0.041413, mean_q: 1.173270
 632927/1000000: episode: 6330, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 71.119, mean reward: 0.711 [0.504, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.869, 10.358], loss: 0.001375, mae: 0.040540, mean_q: 1.173791
 633027/1000000: episode: 6331, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.822, mean reward: 0.578 [0.511, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.644, 10.173], loss: 0.001462, mae: 0.041731, mean_q: 1.175462
 633127/1000000: episode: 6332, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 62.519, mean reward: 0.625 [0.511, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.855, 10.388], loss: 0.001418, mae: 0.041747, mean_q: 1.177317
 633227/1000000: episode: 6333, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 63.233, mean reward: 0.632 [0.510, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.904, 10.546], loss: 0.001556, mae: 0.043256, mean_q: 1.177036
 633327/1000000: episode: 6334, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 60.145, mean reward: 0.601 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.043, 10.326], loss: 0.001445, mae: 0.041812, mean_q: 1.180405
 633427/1000000: episode: 6335, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 59.181, mean reward: 0.592 [0.515, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.648, 10.318], loss: 0.001482, mae: 0.042217, mean_q: 1.183385
 633527/1000000: episode: 6336, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 61.555, mean reward: 0.616 [0.517, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.501, 10.376], loss: 0.001490, mae: 0.042363, mean_q: 1.180629
 633627/1000000: episode: 6337, duration: 1.428s, episode steps: 100, steps per second: 70, episode reward: 57.761, mean reward: 0.578 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.655, 10.142], loss: 0.001407, mae: 0.041324, mean_q: 1.177660
 633727/1000000: episode: 6338, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: 59.318, mean reward: 0.593 [0.509, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.158, 10.273], loss: 0.001553, mae: 0.042895, mean_q: 1.181455
 633827/1000000: episode: 6339, duration: 1.657s, episode steps: 100, steps per second: 60, episode reward: 60.238, mean reward: 0.602 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.816, 10.098], loss: 0.001507, mae: 0.041985, mean_q: 1.180061
 633927/1000000: episode: 6340, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 57.324, mean reward: 0.573 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.098], loss: 0.001284, mae: 0.039060, mean_q: 1.177691
 634027/1000000: episode: 6341, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.261, mean reward: 0.583 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.068, 10.098], loss: 0.001435, mae: 0.041570, mean_q: 1.180432
 634127/1000000: episode: 6342, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 61.586, mean reward: 0.616 [0.514, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.275, 10.098], loss: 0.001414, mae: 0.041019, mean_q: 1.179453
 634227/1000000: episode: 6343, duration: 1.360s, episode steps: 100, steps per second: 74, episode reward: 61.330, mean reward: 0.613 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.866, 10.143], loss: 0.001452, mae: 0.041185, mean_q: 1.181632
 634327/1000000: episode: 6344, duration: 1.366s, episode steps: 100, steps per second: 73, episode reward: 58.724, mean reward: 0.587 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.263, 10.195], loss: 0.001444, mae: 0.041447, mean_q: 1.180994
 634427/1000000: episode: 6345, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: 58.056, mean reward: 0.581 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.109], loss: 0.001510, mae: 0.042379, mean_q: 1.184171
 634527/1000000: episode: 6346, duration: 1.516s, episode steps: 100, steps per second: 66, episode reward: 57.412, mean reward: 0.574 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.665, 10.166], loss: 0.001419, mae: 0.041079, mean_q: 1.176170
 634627/1000000: episode: 6347, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.445, mean reward: 0.574 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.819, 10.160], loss: 0.001479, mae: 0.041919, mean_q: 1.178393
 634727/1000000: episode: 6348, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.120, mean reward: 0.571 [0.499, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.150], loss: 0.001449, mae: 0.041611, mean_q: 1.182016
 634827/1000000: episode: 6349, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 57.957, mean reward: 0.580 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.928, 10.146], loss: 0.001472, mae: 0.041892, mean_q: 1.178379
 634927/1000000: episode: 6350, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: 58.430, mean reward: 0.584 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.418, 10.098], loss: 0.001468, mae: 0.042298, mean_q: 1.180799
 635027/1000000: episode: 6351, duration: 1.363s, episode steps: 100, steps per second: 73, episode reward: 57.560, mean reward: 0.576 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.231, 10.098], loss: 0.001495, mae: 0.042003, mean_q: 1.179965
 635127/1000000: episode: 6352, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 56.806, mean reward: 0.568 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.370, 10.161], loss: 0.001324, mae: 0.040224, mean_q: 1.176081
 635227/1000000: episode: 6353, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 57.143, mean reward: 0.571 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.877, 10.098], loss: 0.001339, mae: 0.039963, mean_q: 1.175557
 635327/1000000: episode: 6354, duration: 1.490s, episode steps: 100, steps per second: 67, episode reward: 59.867, mean reward: 0.599 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.702, 10.098], loss: 0.001436, mae: 0.041120, mean_q: 1.172421
 635427/1000000: episode: 6355, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 57.674, mean reward: 0.577 [0.509, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.200, 10.098], loss: 0.001412, mae: 0.041204, mean_q: 1.176323
 635527/1000000: episode: 6356, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 58.228, mean reward: 0.582 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.150], loss: 0.001488, mae: 0.042413, mean_q: 1.173262
 635627/1000000: episode: 6357, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 57.107, mean reward: 0.571 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.633, 10.098], loss: 0.001435, mae: 0.040931, mean_q: 1.175872
 635727/1000000: episode: 6358, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.907, mean reward: 0.599 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.868, 10.188], loss: 0.001546, mae: 0.043172, mean_q: 1.174736
 635827/1000000: episode: 6359, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.718, mean reward: 0.587 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.244, 10.275], loss: 0.001389, mae: 0.040597, mean_q: 1.176305
 635927/1000000: episode: 6360, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 62.303, mean reward: 0.623 [0.510, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.624, 10.115], loss: 0.001532, mae: 0.042775, mean_q: 1.173115
 636027/1000000: episode: 6361, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 61.627, mean reward: 0.616 [0.501, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.008, 10.375], loss: 0.001475, mae: 0.042240, mean_q: 1.175065
 636127/1000000: episode: 6362, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 64.060, mean reward: 0.641 [0.508, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.056, 10.098], loss: 0.001463, mae: 0.041979, mean_q: 1.178822
 636227/1000000: episode: 6363, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 60.022, mean reward: 0.600 [0.497, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.449, 10.098], loss: 0.001467, mae: 0.041904, mean_q: 1.176908
 636327/1000000: episode: 6364, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 57.847, mean reward: 0.578 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.997, 10.333], loss: 0.001404, mae: 0.041249, mean_q: 1.176957
 636427/1000000: episode: 6365, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 59.137, mean reward: 0.591 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.643, 10.180], loss: 0.001345, mae: 0.040788, mean_q: 1.175206
 636527/1000000: episode: 6366, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.668, mean reward: 0.587 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.758, 10.256], loss: 0.001413, mae: 0.041177, mean_q: 1.174582
 636627/1000000: episode: 6367, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.135, mean reward: 0.581 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.846, 10.173], loss: 0.001443, mae: 0.041322, mean_q: 1.175922
 636727/1000000: episode: 6368, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.215, mean reward: 0.582 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.410, 10.182], loss: 0.001495, mae: 0.042163, mean_q: 1.175255
 636827/1000000: episode: 6369, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 57.725, mean reward: 0.577 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.201, 10.098], loss: 0.001469, mae: 0.041588, mean_q: 1.174125
 636927/1000000: episode: 6370, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 58.588, mean reward: 0.586 [0.512, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.546, 10.132], loss: 0.001517, mae: 0.042407, mean_q: 1.176180
 637027/1000000: episode: 6371, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 57.464, mean reward: 0.575 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.754, 10.262], loss: 0.001501, mae: 0.041819, mean_q: 1.173720
 637127/1000000: episode: 6372, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 62.194, mean reward: 0.622 [0.515, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.356, 10.098], loss: 0.001463, mae: 0.041999, mean_q: 1.173717
 637227/1000000: episode: 6373, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 57.329, mean reward: 0.573 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.123, 10.098], loss: 0.001427, mae: 0.041216, mean_q: 1.178060
 637327/1000000: episode: 6374, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 60.329, mean reward: 0.603 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.924, 10.098], loss: 0.001371, mae: 0.040369, mean_q: 1.170852
 637427/1000000: episode: 6375, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 59.154, mean reward: 0.592 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.633, 10.098], loss: 0.001446, mae: 0.041577, mean_q: 1.177679
 637527/1000000: episode: 6376, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.497, mean reward: 0.595 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.145], loss: 0.001555, mae: 0.042169, mean_q: 1.176811
 637627/1000000: episode: 6377, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.049, mean reward: 0.580 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.692, 10.240], loss: 0.001413, mae: 0.041428, mean_q: 1.171390
 637727/1000000: episode: 6378, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 56.139, mean reward: 0.561 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.098], loss: 0.001470, mae: 0.041577, mean_q: 1.177854
 637827/1000000: episode: 6379, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 57.662, mean reward: 0.577 [0.509, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.481, 10.128], loss: 0.001440, mae: 0.041330, mean_q: 1.176021
 637927/1000000: episode: 6380, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 59.118, mean reward: 0.591 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.680, 10.127], loss: 0.001398, mae: 0.040233, mean_q: 1.170333
 638027/1000000: episode: 6381, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 60.141, mean reward: 0.601 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.517, 10.163], loss: 0.001442, mae: 0.040971, mean_q: 1.169566
 638127/1000000: episode: 6382, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 58.748, mean reward: 0.587 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.384, 10.198], loss: 0.001409, mae: 0.040650, mean_q: 1.170437
 638227/1000000: episode: 6383, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 58.063, mean reward: 0.581 [0.509, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.644, 10.200], loss: 0.001428, mae: 0.041408, mean_q: 1.165936
 638327/1000000: episode: 6384, duration: 1.334s, episode steps: 100, steps per second: 75, episode reward: 57.739, mean reward: 0.577 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.229, 10.211], loss: 0.001466, mae: 0.041334, mean_q: 1.164949
 638427/1000000: episode: 6385, duration: 1.467s, episode steps: 100, steps per second: 68, episode reward: 58.793, mean reward: 0.588 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.932, 10.098], loss: 0.001384, mae: 0.040454, mean_q: 1.162892
 638527/1000000: episode: 6386, duration: 1.674s, episode steps: 100, steps per second: 60, episode reward: 58.423, mean reward: 0.584 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.098], loss: 0.001400, mae: 0.040540, mean_q: 1.161831
 638627/1000000: episode: 6387, duration: 1.495s, episode steps: 100, steps per second: 67, episode reward: 60.545, mean reward: 0.605 [0.500, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.993, 10.131], loss: 0.001353, mae: 0.040236, mean_q: 1.163153
 638727/1000000: episode: 6388, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 60.956, mean reward: 0.610 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.239, 10.098], loss: 0.001368, mae: 0.040503, mean_q: 1.161107
 638827/1000000: episode: 6389, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 57.901, mean reward: 0.579 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.288, 10.202], loss: 0.001467, mae: 0.041637, mean_q: 1.169205
 638927/1000000: episode: 6390, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 58.234, mean reward: 0.582 [0.504, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.026, 10.098], loss: 0.001378, mae: 0.040640, mean_q: 1.162375
 639027/1000000: episode: 6391, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 57.763, mean reward: 0.578 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.266], loss: 0.001403, mae: 0.041058, mean_q: 1.161083
 639127/1000000: episode: 6392, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.818, mean reward: 0.588 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.098], loss: 0.001370, mae: 0.040490, mean_q: 1.163157
 639227/1000000: episode: 6393, duration: 1.268s, episode steps: 100, steps per second: 79, episode reward: 58.395, mean reward: 0.584 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.495, 10.098], loss: 0.001343, mae: 0.040165, mean_q: 1.165592
 639327/1000000: episode: 6394, duration: 1.405s, episode steps: 100, steps per second: 71, episode reward: 57.120, mean reward: 0.571 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.248, 10.098], loss: 0.001383, mae: 0.040430, mean_q: 1.163356
 639427/1000000: episode: 6395, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 63.259, mean reward: 0.633 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.468, 10.101], loss: 0.001392, mae: 0.040863, mean_q: 1.163269
 639527/1000000: episode: 6396, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 59.950, mean reward: 0.599 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.552, 10.111], loss: 0.001340, mae: 0.040092, mean_q: 1.162779
 639627/1000000: episode: 6397, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 57.523, mean reward: 0.575 [0.511, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.337, 10.098], loss: 0.001315, mae: 0.040292, mean_q: 1.162086
 639727/1000000: episode: 6398, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 58.307, mean reward: 0.583 [0.507, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.303, 10.161], loss: 0.001432, mae: 0.041371, mean_q: 1.162745
 639827/1000000: episode: 6399, duration: 1.288s, episode steps: 100, steps per second: 78, episode reward: 59.485, mean reward: 0.595 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.235, 10.098], loss: 0.001332, mae: 0.039813, mean_q: 1.166263
 639927/1000000: episode: 6400, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 61.110, mean reward: 0.611 [0.512, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.506, 10.098], loss: 0.001278, mae: 0.039009, mean_q: 1.165444
 640027/1000000: episode: 6401, duration: 1.258s, episode steps: 100, steps per second: 80, episode reward: 61.709, mean reward: 0.617 [0.508, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.913, 10.098], loss: 0.001470, mae: 0.041759, mean_q: 1.169027
 640127/1000000: episode: 6402, duration: 1.258s, episode steps: 100, steps per second: 79, episode reward: 61.772, mean reward: 0.618 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.372, 10.098], loss: 0.001408, mae: 0.041019, mean_q: 1.168517
 640227/1000000: episode: 6403, duration: 1.415s, episode steps: 100, steps per second: 71, episode reward: 57.533, mean reward: 0.575 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.417, 10.251], loss: 0.001351, mae: 0.039978, mean_q: 1.169805
 640327/1000000: episode: 6404, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 57.530, mean reward: 0.575 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.202, 10.142], loss: 0.001363, mae: 0.040857, mean_q: 1.171125
 640427/1000000: episode: 6405, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 58.026, mean reward: 0.580 [0.500, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.001363, mae: 0.039980, mean_q: 1.167414
 640527/1000000: episode: 6406, duration: 1.493s, episode steps: 100, steps per second: 67, episode reward: 60.669, mean reward: 0.607 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.357, 10.098], loss: 0.001385, mae: 0.040445, mean_q: 1.172010
 640627/1000000: episode: 6407, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 58.600, mean reward: 0.586 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.056, 10.182], loss: 0.001454, mae: 0.041220, mean_q: 1.171425
 640727/1000000: episode: 6408, duration: 1.363s, episode steps: 100, steps per second: 73, episode reward: 65.065, mean reward: 0.651 [0.505, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.377, 10.098], loss: 0.001399, mae: 0.040824, mean_q: 1.171381
 640827/1000000: episode: 6409, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 60.538, mean reward: 0.605 [0.505, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.699, 10.565], loss: 0.001344, mae: 0.040086, mean_q: 1.175215
 640927/1000000: episode: 6410, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 58.347, mean reward: 0.583 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.305], loss: 0.001329, mae: 0.039770, mean_q: 1.172093
 641027/1000000: episode: 6411, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 60.309, mean reward: 0.603 [0.527, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.127, 10.142], loss: 0.001434, mae: 0.040656, mean_q: 1.172037
 641127/1000000: episode: 6412, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 57.127, mean reward: 0.571 [0.507, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.847, 10.142], loss: 0.001464, mae: 0.041695, mean_q: 1.168395
 641227/1000000: episode: 6413, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 59.841, mean reward: 0.598 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.587, 10.309], loss: 0.001285, mae: 0.039070, mean_q: 1.166898
 641327/1000000: episode: 6414, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.402, mean reward: 0.584 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.161], loss: 0.001419, mae: 0.040407, mean_q: 1.167130
 641427/1000000: episode: 6415, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 60.394, mean reward: 0.604 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.571, 10.098], loss: 0.001350, mae: 0.039889, mean_q: 1.168076
 641527/1000000: episode: 6416, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 57.359, mean reward: 0.574 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.235, 10.098], loss: 0.001415, mae: 0.040860, mean_q: 1.168843
 641627/1000000: episode: 6417, duration: 0.901s, episode steps: 100, steps per second: 111, episode reward: 56.211, mean reward: 0.562 [0.502, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.243, 10.098], loss: 0.001287, mae: 0.038544, mean_q: 1.166667
 641727/1000000: episode: 6418, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 58.865, mean reward: 0.589 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.899, 10.410], loss: 0.001298, mae: 0.038647, mean_q: 1.169887
 641827/1000000: episode: 6419, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 57.526, mean reward: 0.575 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.510, 10.152], loss: 0.001437, mae: 0.040871, mean_q: 1.168591
 641927/1000000: episode: 6420, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 59.550, mean reward: 0.596 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.197, 10.098], loss: 0.001392, mae: 0.039852, mean_q: 1.165579
 642027/1000000: episode: 6421, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 59.561, mean reward: 0.596 [0.497, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.626, 10.190], loss: 0.001394, mae: 0.040354, mean_q: 1.173381
 642127/1000000: episode: 6422, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 58.281, mean reward: 0.583 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.247], loss: 0.001472, mae: 0.041219, mean_q: 1.167921
 642227/1000000: episode: 6423, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.356, mean reward: 0.584 [0.520, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.265, 10.200], loss: 0.001309, mae: 0.039395, mean_q: 1.165049
 642327/1000000: episode: 6424, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.422, mean reward: 0.594 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.009, 10.098], loss: 0.001396, mae: 0.039542, mean_q: 1.166581
 642427/1000000: episode: 6425, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 59.142, mean reward: 0.591 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.697, 10.296], loss: 0.001395, mae: 0.040683, mean_q: 1.170945
 642527/1000000: episode: 6426, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.814, mean reward: 0.578 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.561, 10.098], loss: 0.001285, mae: 0.039250, mean_q: 1.165214
 642627/1000000: episode: 6427, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 62.159, mean reward: 0.622 [0.502, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.907, 10.098], loss: 0.001414, mae: 0.040568, mean_q: 1.164923
 642727/1000000: episode: 6428, duration: 0.900s, episode steps: 100, steps per second: 111, episode reward: 57.400, mean reward: 0.574 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.600, 10.098], loss: 0.001368, mae: 0.039872, mean_q: 1.167095
 642827/1000000: episode: 6429, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 60.483, mean reward: 0.605 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.251], loss: 0.001395, mae: 0.040065, mean_q: 1.170342
 642927/1000000: episode: 6430, duration: 1.327s, episode steps: 100, steps per second: 75, episode reward: 57.980, mean reward: 0.580 [0.500, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.360, 10.115], loss: 0.001483, mae: 0.041268, mean_q: 1.171454
 643027/1000000: episode: 6431, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.378, mean reward: 0.584 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.098], loss: 0.001424, mae: 0.040633, mean_q: 1.170669
 643127/1000000: episode: 6432, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 58.496, mean reward: 0.585 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.784, 10.230], loss: 0.001353, mae: 0.039515, mean_q: 1.168889
 643227/1000000: episode: 6433, duration: 0.893s, episode steps: 100, steps per second: 112, episode reward: 57.932, mean reward: 0.579 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.430, 10.098], loss: 0.001399, mae: 0.039831, mean_q: 1.171209
 643327/1000000: episode: 6434, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 60.887, mean reward: 0.609 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.424, 10.293], loss: 0.001323, mae: 0.039384, mean_q: 1.167504
 643427/1000000: episode: 6435, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 56.718, mean reward: 0.567 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.338, 10.098], loss: 0.001369, mae: 0.040253, mean_q: 1.171643
 643527/1000000: episode: 6436, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 58.733, mean reward: 0.587 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.687, 10.098], loss: 0.001470, mae: 0.041307, mean_q: 1.170583
 643627/1000000: episode: 6437, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 58.856, mean reward: 0.589 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.402, 10.098], loss: 0.001369, mae: 0.040261, mean_q: 1.169170
 643727/1000000: episode: 6438, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.651, mean reward: 0.587 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.682, 10.117], loss: 0.001484, mae: 0.040904, mean_q: 1.168493
 643827/1000000: episode: 6439, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 58.711, mean reward: 0.587 [0.504, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.812, 10.266], loss: 0.001378, mae: 0.040332, mean_q: 1.169218
 643927/1000000: episode: 6440, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 62.126, mean reward: 0.621 [0.513, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.621, 10.548], loss: 0.001441, mae: 0.040980, mean_q: 1.169356
 644027/1000000: episode: 6441, duration: 1.361s, episode steps: 100, steps per second: 73, episode reward: 57.307, mean reward: 0.573 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.228, 10.098], loss: 0.001434, mae: 0.041223, mean_q: 1.173654
 644127/1000000: episode: 6442, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 60.029, mean reward: 0.600 [0.512, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.528, 10.382], loss: 0.001338, mae: 0.039704, mean_q: 1.172022
 644227/1000000: episode: 6443, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 59.106, mean reward: 0.591 [0.498, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.464, 10.098], loss: 0.001438, mae: 0.040971, mean_q: 1.171019
 644327/1000000: episode: 6444, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 60.973, mean reward: 0.610 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.961, 10.098], loss: 0.001478, mae: 0.041967, mean_q: 1.173090
 644427/1000000: episode: 6445, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 61.968, mean reward: 0.620 [0.511, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.868, 10.222], loss: 0.001384, mae: 0.040621, mean_q: 1.171844
 644527/1000000: episode: 6446, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 60.664, mean reward: 0.607 [0.501, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.360, 10.134], loss: 0.001478, mae: 0.041605, mean_q: 1.169968
 644627/1000000: episode: 6447, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 61.446, mean reward: 0.614 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.358], loss: 0.001504, mae: 0.042035, mean_q: 1.171105
 644727/1000000: episode: 6448, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 57.696, mean reward: 0.577 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.392, 10.151], loss: 0.001464, mae: 0.041291, mean_q: 1.172957
 644827/1000000: episode: 6449, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 56.289, mean reward: 0.563 [0.500, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.426, 10.144], loss: 0.001447, mae: 0.041406, mean_q: 1.172673
 644927/1000000: episode: 6450, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 58.739, mean reward: 0.587 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.521, 10.098], loss: 0.001392, mae: 0.040500, mean_q: 1.170299
 645027/1000000: episode: 6451, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 60.224, mean reward: 0.602 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.958, 10.129], loss: 0.001479, mae: 0.041732, mean_q: 1.166644
 645127/1000000: episode: 6452, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 61.937, mean reward: 0.619 [0.515, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.706, 10.098], loss: 0.001507, mae: 0.042510, mean_q: 1.170137
 645227/1000000: episode: 6453, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 59.194, mean reward: 0.592 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.281, 10.098], loss: 0.001349, mae: 0.039750, mean_q: 1.170098
 645327/1000000: episode: 6454, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.028, mean reward: 0.580 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.556, 10.098], loss: 0.001545, mae: 0.042422, mean_q: 1.171029
 645427/1000000: episode: 6455, duration: 0.906s, episode steps: 100, steps per second: 110, episode reward: 61.573, mean reward: 0.616 [0.518, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.929, 10.236], loss: 0.001628, mae: 0.043435, mean_q: 1.174293
 645527/1000000: episode: 6456, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 61.106, mean reward: 0.611 [0.500, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.311], loss: 0.001475, mae: 0.042256, mean_q: 1.174133
 645627/1000000: episode: 6457, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 58.623, mean reward: 0.586 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.929, 10.098], loss: 0.001551, mae: 0.042404, mean_q: 1.172225
 645727/1000000: episode: 6458, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 60.142, mean reward: 0.601 [0.519, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.899, 10.098], loss: 0.001522, mae: 0.042688, mean_q: 1.174444
 645827/1000000: episode: 6459, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 58.358, mean reward: 0.584 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.336, 10.098], loss: 0.001393, mae: 0.040650, mean_q: 1.167334
 645927/1000000: episode: 6460, duration: 1.346s, episode steps: 100, steps per second: 74, episode reward: 61.196, mean reward: 0.612 [0.509, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.135, 10.186], loss: 0.001429, mae: 0.041394, mean_q: 1.171874
 646027/1000000: episode: 6461, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.253, mean reward: 0.583 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.813, 10.175], loss: 0.001528, mae: 0.042368, mean_q: 1.171027
 646127/1000000: episode: 6462, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 59.741, mean reward: 0.597 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.731, 10.428], loss: 0.001507, mae: 0.042333, mean_q: 1.171882
 646227/1000000: episode: 6463, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 57.955, mean reward: 0.580 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.645, 10.112], loss: 0.001496, mae: 0.042396, mean_q: 1.170563
 646327/1000000: episode: 6464, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 61.175, mean reward: 0.612 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.423, 10.239], loss: 0.001566, mae: 0.043555, mean_q: 1.171706
 646427/1000000: episode: 6465, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 60.602, mean reward: 0.606 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.327, 10.098], loss: 0.001472, mae: 0.041188, mean_q: 1.170626
 646527/1000000: episode: 6466, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 61.048, mean reward: 0.610 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 0.001535, mae: 0.043008, mean_q: 1.173003
 646627/1000000: episode: 6467, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 57.093, mean reward: 0.571 [0.501, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.558, 10.244], loss: 0.001614, mae: 0.043237, mean_q: 1.175637
 646727/1000000: episode: 6468, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.163, mean reward: 0.592 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.898, 10.179], loss: 0.001506, mae: 0.042813, mean_q: 1.174144
 646827/1000000: episode: 6469, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 57.511, mean reward: 0.575 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.829, 10.332], loss: 0.001563, mae: 0.042725, mean_q: 1.175946
 646927/1000000: episode: 6470, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 59.299, mean reward: 0.593 [0.513, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.924, 10.098], loss: 0.001501, mae: 0.042069, mean_q: 1.175333
 647027/1000000: episode: 6471, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 60.279, mean reward: 0.603 [0.498, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.541, 10.203], loss: 0.001650, mae: 0.044113, mean_q: 1.178608
 647127/1000000: episode: 6472, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 57.729, mean reward: 0.577 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.858, 10.149], loss: 0.001532, mae: 0.042353, mean_q: 1.175418
 647227/1000000: episode: 6473, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 58.689, mean reward: 0.587 [0.513, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.667, 10.098], loss: 0.001408, mae: 0.040623, mean_q: 1.174530
 647327/1000000: episode: 6474, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 60.066, mean reward: 0.601 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.712, 10.567], loss: 0.001544, mae: 0.043103, mean_q: 1.171816
 647427/1000000: episode: 6475, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.709, mean reward: 0.587 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.090, 10.279], loss: 0.001543, mae: 0.043084, mean_q: 1.176814
 647527/1000000: episode: 6476, duration: 1.441s, episode steps: 100, steps per second: 69, episode reward: 59.404, mean reward: 0.594 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.496, 10.098], loss: 0.001583, mae: 0.042974, mean_q: 1.175107
 647627/1000000: episode: 6477, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 57.573, mean reward: 0.576 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.655, 10.217], loss: 0.001571, mae: 0.043080, mean_q: 1.175956
 647727/1000000: episode: 6478, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 60.773, mean reward: 0.608 [0.503, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.144, 10.161], loss: 0.001501, mae: 0.042347, mean_q: 1.172855
 647827/1000000: episode: 6479, duration: 1.300s, episode steps: 100, steps per second: 77, episode reward: 59.312, mean reward: 0.593 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.198], loss: 0.001529, mae: 0.042716, mean_q: 1.176205
 647927/1000000: episode: 6480, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 58.992, mean reward: 0.590 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.668, 10.229], loss: 0.001506, mae: 0.042976, mean_q: 1.176428
 648027/1000000: episode: 6481, duration: 1.269s, episode steps: 100, steps per second: 79, episode reward: 61.729, mean reward: 0.617 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.757, 10.110], loss: 0.001635, mae: 0.043975, mean_q: 1.177952
 648127/1000000: episode: 6482, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 57.455, mean reward: 0.575 [0.502, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.571, 10.336], loss: 0.001673, mae: 0.044026, mean_q: 1.180208
 648227/1000000: episode: 6483, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.889, mean reward: 0.589 [0.516, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.578, 10.098], loss: 0.001653, mae: 0.044296, mean_q: 1.177783
 648327/1000000: episode: 6484, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 56.363, mean reward: 0.564 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.098], loss: 0.001596, mae: 0.043441, mean_q: 1.174083
 648427/1000000: episode: 6485, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 60.744, mean reward: 0.607 [0.518, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.277], loss: 0.001574, mae: 0.043394, mean_q: 1.174391
 648527/1000000: episode: 6486, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 58.202, mean reward: 0.582 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.330, 10.098], loss: 0.001609, mae: 0.043749, mean_q: 1.176596
 648627/1000000: episode: 6487, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 58.482, mean reward: 0.585 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.250, 10.098], loss: 0.001665, mae: 0.044586, mean_q: 1.177917
 648727/1000000: episode: 6488, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 58.289, mean reward: 0.583 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.290, 10.098], loss: 0.001555, mae: 0.043173, mean_q: 1.178363
 648827/1000000: episode: 6489, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 61.225, mean reward: 0.612 [0.498, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.815, 10.098], loss: 0.001579, mae: 0.043206, mean_q: 1.177524
 648927/1000000: episode: 6490, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.912, mean reward: 0.589 [0.505, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.811, 10.247], loss: 0.001686, mae: 0.044878, mean_q: 1.176868
 649027/1000000: episode: 6491, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.870, mean reward: 0.579 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.098], loss: 0.001594, mae: 0.043277, mean_q: 1.175825
 649127/1000000: episode: 6492, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 57.081, mean reward: 0.571 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.098], loss: 0.001579, mae: 0.043709, mean_q: 1.172930
 649227/1000000: episode: 6493, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 57.891, mean reward: 0.579 [0.500, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.480, 10.257], loss: 0.001748, mae: 0.044763, mean_q: 1.177098
 649327/1000000: episode: 6494, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 59.871, mean reward: 0.599 [0.513, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.126, 10.098], loss: 0.001544, mae: 0.043080, mean_q: 1.177087
 649427/1000000: episode: 6495, duration: 1.474s, episode steps: 100, steps per second: 68, episode reward: 57.548, mean reward: 0.575 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.630, 10.098], loss: 0.001528, mae: 0.042096, mean_q: 1.172560
 649527/1000000: episode: 6496, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 57.270, mean reward: 0.573 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.842, 10.098], loss: 0.001450, mae: 0.041798, mean_q: 1.168939
 649627/1000000: episode: 6497, duration: 1.274s, episode steps: 100, steps per second: 78, episode reward: 57.350, mean reward: 0.573 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.164, 10.098], loss: 0.001498, mae: 0.042136, mean_q: 1.169970
 649727/1000000: episode: 6498, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 60.567, mean reward: 0.606 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.626, 10.108], loss: 0.001608, mae: 0.042927, mean_q: 1.169048
 649827/1000000: episode: 6499, duration: 1.355s, episode steps: 100, steps per second: 74, episode reward: 58.669, mean reward: 0.587 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.381, 10.164], loss: 0.001460, mae: 0.041901, mean_q: 1.170556
 649927/1000000: episode: 6500, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 57.079, mean reward: 0.571 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.782, 10.191], loss: 0.001513, mae: 0.042328, mean_q: 1.172127
 650027/1000000: episode: 6501, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 57.605, mean reward: 0.576 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.482, 10.098], loss: 0.001598, mae: 0.043294, mean_q: 1.169968
 650127/1000000: episode: 6502, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 58.739, mean reward: 0.587 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.871, 10.111], loss: 0.001467, mae: 0.042145, mean_q: 1.167466
 650227/1000000: episode: 6503, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: 57.871, mean reward: 0.579 [0.515, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.886, 10.156], loss: 0.001481, mae: 0.041296, mean_q: 1.167212
 650327/1000000: episode: 6504, duration: 1.680s, episode steps: 100, steps per second: 60, episode reward: 59.104, mean reward: 0.591 [0.511, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.784, 10.198], loss: 0.001480, mae: 0.041720, mean_q: 1.165028
 650427/1000000: episode: 6505, duration: 1.625s, episode steps: 100, steps per second: 62, episode reward: 60.194, mean reward: 0.602 [0.503, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.232, 10.098], loss: 0.001460, mae: 0.041141, mean_q: 1.166173
 650527/1000000: episode: 6506, duration: 1.517s, episode steps: 100, steps per second: 66, episode reward: 57.913, mean reward: 0.579 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.544, 10.174], loss: 0.001523, mae: 0.042230, mean_q: 1.166162
 650627/1000000: episode: 6507, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 58.143, mean reward: 0.581 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.852, 10.118], loss: 0.001540, mae: 0.042392, mean_q: 1.165479
 650727/1000000: episode: 6508, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 58.150, mean reward: 0.581 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.146, 10.098], loss: 0.001546, mae: 0.043167, mean_q: 1.168860
 650827/1000000: episode: 6509, duration: 1.349s, episode steps: 100, steps per second: 74, episode reward: 59.138, mean reward: 0.591 [0.511, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.417, 10.098], loss: 0.001616, mae: 0.042711, mean_q: 1.165123
 650927/1000000: episode: 6510, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 59.613, mean reward: 0.596 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.829, 10.143], loss: 0.001483, mae: 0.042112, mean_q: 1.164712
 651027/1000000: episode: 6511, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 59.279, mean reward: 0.593 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.053, 10.211], loss: 0.001524, mae: 0.042402, mean_q: 1.161046
 651127/1000000: episode: 6512, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 59.972, mean reward: 0.600 [0.510, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.212], loss: 0.001620, mae: 0.042763, mean_q: 1.165720
 651227/1000000: episode: 6513, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 58.546, mean reward: 0.585 [0.517, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.214, 10.197], loss: 0.001514, mae: 0.041814, mean_q: 1.165225
 651327/1000000: episode: 6514, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 58.644, mean reward: 0.586 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.167, 10.098], loss: 0.001451, mae: 0.041233, mean_q: 1.163670
 651427/1000000: episode: 6515, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 60.282, mean reward: 0.603 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.180, 10.098], loss: 0.001439, mae: 0.041269, mean_q: 1.165605
 651527/1000000: episode: 6516, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 59.984, mean reward: 0.600 [0.498, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.075, 10.098], loss: 0.001535, mae: 0.042424, mean_q: 1.163919
 651627/1000000: episode: 6517, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.879, mean reward: 0.579 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.989, 10.193], loss: 0.001547, mae: 0.042985, mean_q: 1.165038
 651727/1000000: episode: 6518, duration: 0.926s, episode steps: 100, steps per second: 108, episode reward: 59.578, mean reward: 0.596 [0.507, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.573, 10.098], loss: 0.001437, mae: 0.041067, mean_q: 1.164904
 651827/1000000: episode: 6519, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.636, mean reward: 0.586 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.173], loss: 0.001419, mae: 0.041145, mean_q: 1.165764
 651927/1000000: episode: 6520, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.757, mean reward: 0.598 [0.510, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.349, 10.098], loss: 0.001369, mae: 0.039785, mean_q: 1.161725
 652027/1000000: episode: 6521, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 58.613, mean reward: 0.586 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.299, 10.336], loss: 0.001454, mae: 0.041779, mean_q: 1.163757
 652127/1000000: episode: 6522, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 60.287, mean reward: 0.603 [0.516, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.997, 10.118], loss: 0.001453, mae: 0.041684, mean_q: 1.163893
 652227/1000000: episode: 6523, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 63.019, mean reward: 0.630 [0.523, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.644, 10.335], loss: 0.001532, mae: 0.042392, mean_q: 1.166775
 652327/1000000: episode: 6524, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 61.100, mean reward: 0.611 [0.515, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.098], loss: 0.001488, mae: 0.042094, mean_q: 1.166531
 652427/1000000: episode: 6525, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.064, mean reward: 0.581 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.286, 10.254], loss: 0.001442, mae: 0.040683, mean_q: 1.167400
 652527/1000000: episode: 6526, duration: 0.914s, episode steps: 100, steps per second: 109, episode reward: 57.981, mean reward: 0.580 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.131, 10.160], loss: 0.001499, mae: 0.042013, mean_q: 1.167722
 652627/1000000: episode: 6527, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 58.934, mean reward: 0.589 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.258, 10.149], loss: 0.001356, mae: 0.040499, mean_q: 1.166295
 652727/1000000: episode: 6528, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 57.313, mean reward: 0.573 [0.498, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.048, 10.098], loss: 0.001502, mae: 0.042376, mean_q: 1.166985
 652827/1000000: episode: 6529, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 60.081, mean reward: 0.601 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.979, 10.098], loss: 0.001503, mae: 0.041805, mean_q: 1.167387
 652927/1000000: episode: 6530, duration: 0.902s, episode steps: 100, steps per second: 111, episode reward: 60.480, mean reward: 0.605 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.916, 10.258], loss: 0.001437, mae: 0.040884, mean_q: 1.165040
 653027/1000000: episode: 6531, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.157, mean reward: 0.582 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.570, 10.098], loss: 0.001490, mae: 0.041438, mean_q: 1.165862
 653127/1000000: episode: 6532, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 57.588, mean reward: 0.576 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.047, 10.098], loss: 0.001468, mae: 0.041621, mean_q: 1.163401
 653227/1000000: episode: 6533, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 57.159, mean reward: 0.572 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.334, 10.116], loss: 0.001354, mae: 0.039896, mean_q: 1.161592
 653327/1000000: episode: 6534, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.388, mean reward: 0.594 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.374, 10.290], loss: 0.001476, mae: 0.041308, mean_q: 1.163666
 653427/1000000: episode: 6535, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 56.680, mean reward: 0.567 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.661, 10.236], loss: 0.001333, mae: 0.039356, mean_q: 1.163498
 653527/1000000: episode: 6536, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.791, mean reward: 0.588 [0.515, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.560, 10.149], loss: 0.001397, mae: 0.041072, mean_q: 1.162471
 653627/1000000: episode: 6537, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 60.175, mean reward: 0.602 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.975, 10.098], loss: 0.001408, mae: 0.041073, mean_q: 1.167269
 653727/1000000: episode: 6538, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 62.890, mean reward: 0.629 [0.507, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.258, 10.421], loss: 0.001453, mae: 0.040219, mean_q: 1.163071
 653827/1000000: episode: 6539, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.346, mean reward: 0.583 [0.501, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.618, 10.098], loss: 0.001385, mae: 0.040444, mean_q: 1.165874
 653927/1000000: episode: 6540, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 60.272, mean reward: 0.603 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.651, 10.098], loss: 0.001515, mae: 0.042091, mean_q: 1.166303
 654027/1000000: episode: 6541, duration: 1.512s, episode steps: 100, steps per second: 66, episode reward: 58.289, mean reward: 0.583 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.098], loss: 0.001485, mae: 0.040756, mean_q: 1.165093
 654127/1000000: episode: 6542, duration: 1.660s, episode steps: 100, steps per second: 60, episode reward: 60.065, mean reward: 0.601 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.834, 10.098], loss: 0.001524, mae: 0.041833, mean_q: 1.166784
 654227/1000000: episode: 6543, duration: 1.568s, episode steps: 100, steps per second: 64, episode reward: 60.330, mean reward: 0.603 [0.509, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.752, 10.326], loss: 0.001453, mae: 0.041018, mean_q: 1.166455
 654327/1000000: episode: 6544, duration: 1.464s, episode steps: 100, steps per second: 68, episode reward: 59.650, mean reward: 0.596 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.619, 10.291], loss: 0.001488, mae: 0.042062, mean_q: 1.169040
 654427/1000000: episode: 6545, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.180, mean reward: 0.572 [0.512, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.263, 10.098], loss: 0.001425, mae: 0.040972, mean_q: 1.166399
 654527/1000000: episode: 6546, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 59.705, mean reward: 0.597 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.027, 10.098], loss: 0.001600, mae: 0.043322, mean_q: 1.171656
 654627/1000000: episode: 6547, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 64.807, mean reward: 0.648 [0.520, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.422, 10.098], loss: 0.001448, mae: 0.041696, mean_q: 1.172220
 654727/1000000: episode: 6548, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 58.537, mean reward: 0.585 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.971, 10.274], loss: 0.001434, mae: 0.040684, mean_q: 1.170992
 654827/1000000: episode: 6549, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 59.051, mean reward: 0.591 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.098], loss: 0.001465, mae: 0.041154, mean_q: 1.172489
 654927/1000000: episode: 6550, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 59.099, mean reward: 0.591 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.636, 10.098], loss: 0.001445, mae: 0.041321, mean_q: 1.171320
 655027/1000000: episode: 6551, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 62.211, mean reward: 0.622 [0.516, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.304, 10.109], loss: 0.001472, mae: 0.041863, mean_q: 1.171306
 655127/1000000: episode: 6552, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 58.414, mean reward: 0.584 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.773, 10.116], loss: 0.001383, mae: 0.039900, mean_q: 1.174273
 655227/1000000: episode: 6553, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.232, mean reward: 0.592 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.704, 10.234], loss: 0.001549, mae: 0.042510, mean_q: 1.174455
 655327/1000000: episode: 6554, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 59.520, mean reward: 0.595 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.501, 10.098], loss: 0.001478, mae: 0.041286, mean_q: 1.173851
 655427/1000000: episode: 6555, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.860, mean reward: 0.579 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.956, 10.179], loss: 0.001477, mae: 0.041911, mean_q: 1.176314
 655527/1000000: episode: 6556, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.698, mean reward: 0.587 [0.500, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.699, 10.273], loss: 0.001489, mae: 0.040808, mean_q: 1.173149
 655627/1000000: episode: 6557, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 57.426, mean reward: 0.574 [0.516, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.156, 10.098], loss: 0.001508, mae: 0.041351, mean_q: 1.172774
 655727/1000000: episode: 6558, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 58.474, mean reward: 0.585 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.098], loss: 0.001554, mae: 0.042086, mean_q: 1.172624
 655827/1000000: episode: 6559, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 60.029, mean reward: 0.600 [0.501, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.672, 10.483], loss: 0.001497, mae: 0.041277, mean_q: 1.172425
 655927/1000000: episode: 6560, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 58.309, mean reward: 0.583 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.607, 10.243], loss: 0.001444, mae: 0.040912, mean_q: 1.171164
 656027/1000000: episode: 6561, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 59.409, mean reward: 0.594 [0.505, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.389, 10.246], loss: 0.001524, mae: 0.041607, mean_q: 1.174434
 656127/1000000: episode: 6562, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 59.457, mean reward: 0.595 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.184, 10.098], loss: 0.001537, mae: 0.042367, mean_q: 1.171113
 656227/1000000: episode: 6563, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 61.636, mean reward: 0.616 [0.508, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.822, 10.132], loss: 0.001549, mae: 0.041943, mean_q: 1.176815
 656327/1000000: episode: 6564, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 57.699, mean reward: 0.577 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.224, 10.171], loss: 0.001440, mae: 0.041046, mean_q: 1.173107
 656427/1000000: episode: 6565, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 63.280, mean reward: 0.633 [0.518, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.911, 10.098], loss: 0.001467, mae: 0.041039, mean_q: 1.175767
 656527/1000000: episode: 6566, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 60.026, mean reward: 0.600 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.734, 10.152], loss: 0.001488, mae: 0.041236, mean_q: 1.176244
 656627/1000000: episode: 6567, duration: 1.310s, episode steps: 100, steps per second: 76, episode reward: 58.844, mean reward: 0.588 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.247], loss: 0.001473, mae: 0.041303, mean_q: 1.173970
 656727/1000000: episode: 6568, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.858, mean reward: 0.589 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.975, 10.098], loss: 0.001505, mae: 0.041973, mean_q: 1.173308
 656827/1000000: episode: 6569, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 59.287, mean reward: 0.593 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.110, 10.318], loss: 0.001438, mae: 0.040594, mean_q: 1.173405
 656927/1000000: episode: 6570, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.328, mean reward: 0.583 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.257, 10.109], loss: 0.001540, mae: 0.042332, mean_q: 1.173107
 657027/1000000: episode: 6571, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 56.393, mean reward: 0.564 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.098], loss: 0.001506, mae: 0.041388, mean_q: 1.172875
 657127/1000000: episode: 6572, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 60.036, mean reward: 0.600 [0.500, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.476, 10.106], loss: 0.001407, mae: 0.040110, mean_q: 1.173573
 657227/1000000: episode: 6573, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 57.553, mean reward: 0.576 [0.499, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.040, 10.098], loss: 0.001357, mae: 0.039555, mean_q: 1.171966
 657327/1000000: episode: 6574, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 58.768, mean reward: 0.588 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.753, 10.103], loss: 0.001523, mae: 0.041181, mean_q: 1.171696
 657427/1000000: episode: 6575, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 68.147, mean reward: 0.681 [0.515, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.574, 10.098], loss: 0.001549, mae: 0.041826, mean_q: 1.169672
 657527/1000000: episode: 6576, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 57.101, mean reward: 0.571 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.225], loss: 0.001633, mae: 0.043126, mean_q: 1.173264
 657627/1000000: episode: 6577, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 57.207, mean reward: 0.572 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.923, 10.160], loss: 0.001548, mae: 0.042178, mean_q: 1.174826
 657727/1000000: episode: 6578, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.836, mean reward: 0.588 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.944, 10.098], loss: 0.001493, mae: 0.042089, mean_q: 1.179995
 657827/1000000: episode: 6579, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 61.196, mean reward: 0.612 [0.508, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.542, 10.098], loss: 0.001541, mae: 0.042137, mean_q: 1.175935
 657927/1000000: episode: 6580, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 57.584, mean reward: 0.576 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.642, 10.198], loss: 0.001427, mae: 0.040604, mean_q: 1.174571
 658027/1000000: episode: 6581, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 61.748, mean reward: 0.617 [0.511, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.675, 10.439], loss: 0.001476, mae: 0.041608, mean_q: 1.175796
 658127/1000000: episode: 6582, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.843, mean reward: 0.598 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.353, 10.283], loss: 0.001562, mae: 0.042339, mean_q: 1.173087
 658227/1000000: episode: 6583, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.012, mean reward: 0.570 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.497, 10.252], loss: 0.001552, mae: 0.041917, mean_q: 1.175427
 658327/1000000: episode: 6584, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 57.761, mean reward: 0.578 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.401, 10.229], loss: 0.001556, mae: 0.042303, mean_q: 1.173701
 658427/1000000: episode: 6585, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 59.755, mean reward: 0.598 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.242, 10.183], loss: 0.001487, mae: 0.041438, mean_q: 1.177801
 658527/1000000: episode: 6586, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 59.303, mean reward: 0.593 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.658, 10.110], loss: 0.001490, mae: 0.041708, mean_q: 1.179122
 658627/1000000: episode: 6587, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 58.946, mean reward: 0.589 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.135, 10.145], loss: 0.001591, mae: 0.042907, mean_q: 1.177103
 658727/1000000: episode: 6588, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 58.353, mean reward: 0.584 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.360, 10.098], loss: 0.001557, mae: 0.042379, mean_q: 1.178418
 658827/1000000: episode: 6589, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 57.321, mean reward: 0.573 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.819, 10.098], loss: 0.001557, mae: 0.042323, mean_q: 1.174802
 658927/1000000: episode: 6590, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 59.455, mean reward: 0.595 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.255, 10.098], loss: 0.001558, mae: 0.042862, mean_q: 1.175314
 659027/1000000: episode: 6591, duration: 1.407s, episode steps: 100, steps per second: 71, episode reward: 59.203, mean reward: 0.592 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.102, 10.098], loss: 0.001636, mae: 0.043184, mean_q: 1.174011
 659127/1000000: episode: 6592, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 57.751, mean reward: 0.578 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.563, 10.098], loss: 0.001404, mae: 0.040866, mean_q: 1.169662
 659227/1000000: episode: 6593, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 59.780, mean reward: 0.598 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.984, 10.115], loss: 0.001671, mae: 0.043528, mean_q: 1.175281
 659327/1000000: episode: 6594, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 58.829, mean reward: 0.588 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.082, 10.265], loss: 0.001538, mae: 0.041952, mean_q: 1.170701
 659427/1000000: episode: 6595, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 58.700, mean reward: 0.587 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.155, 10.205], loss: 0.001499, mae: 0.041790, mean_q: 1.173344
 659527/1000000: episode: 6596, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 59.071, mean reward: 0.591 [0.499, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.869, 10.098], loss: 0.001502, mae: 0.042038, mean_q: 1.173286
 659627/1000000: episode: 6597, duration: 1.366s, episode steps: 100, steps per second: 73, episode reward: 59.702, mean reward: 0.597 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.308, 10.275], loss: 0.001483, mae: 0.041676, mean_q: 1.169159
 659727/1000000: episode: 6598, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 58.070, mean reward: 0.581 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.880, 10.098], loss: 0.001455, mae: 0.041157, mean_q: 1.168337
 659827/1000000: episode: 6599, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 59.530, mean reward: 0.595 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.900, 10.098], loss: 0.001509, mae: 0.041610, mean_q: 1.172259
 659927/1000000: episode: 6600, duration: 1.310s, episode steps: 100, steps per second: 76, episode reward: 59.752, mean reward: 0.598 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.608, 10.138], loss: 0.001522, mae: 0.041928, mean_q: 1.170352
 660027/1000000: episode: 6601, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 57.674, mean reward: 0.577 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.201], loss: 0.001478, mae: 0.041725, mean_q: 1.173273
 660127/1000000: episode: 6602, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 58.246, mean reward: 0.582 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.469, 10.098], loss: 0.001445, mae: 0.041126, mean_q: 1.171699
 660227/1000000: episode: 6603, duration: 1.242s, episode steps: 100, steps per second: 81, episode reward: 59.381, mean reward: 0.594 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.960, 10.098], loss: 0.001530, mae: 0.042070, mean_q: 1.170151
 660327/1000000: episode: 6604, duration: 1.292s, episode steps: 100, steps per second: 77, episode reward: 58.951, mean reward: 0.590 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.399, 10.131], loss: 0.001469, mae: 0.041445, mean_q: 1.173248
 660427/1000000: episode: 6605, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 62.218, mean reward: 0.622 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.725, 10.172], loss: 0.001496, mae: 0.042071, mean_q: 1.169270
 660527/1000000: episode: 6606, duration: 1.504s, episode steps: 100, steps per second: 66, episode reward: 57.014, mean reward: 0.570 [0.505, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.207, 10.223], loss: 0.001441, mae: 0.040382, mean_q: 1.172906
 660627/1000000: episode: 6607, duration: 1.550s, episode steps: 100, steps per second: 65, episode reward: 57.386, mean reward: 0.574 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.948, 10.098], loss: 0.001465, mae: 0.041727, mean_q: 1.172661
 660727/1000000: episode: 6608, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 61.530, mean reward: 0.615 [0.504, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.311], loss: 0.001407, mae: 0.040966, mean_q: 1.171345
 660827/1000000: episode: 6609, duration: 1.308s, episode steps: 100, steps per second: 76, episode reward: 58.570, mean reward: 0.586 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.953, 10.106], loss: 0.001489, mae: 0.041643, mean_q: 1.171474
 660927/1000000: episode: 6610, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 57.424, mean reward: 0.574 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.037, 10.136], loss: 0.001510, mae: 0.042175, mean_q: 1.170313
 661027/1000000: episode: 6611, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.865, mean reward: 0.589 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.672, 10.121], loss: 0.001457, mae: 0.041716, mean_q: 1.171157
 661127/1000000: episode: 6612, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 58.683, mean reward: 0.587 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.984, 10.136], loss: 0.001607, mae: 0.043303, mean_q: 1.172498
 661227/1000000: episode: 6613, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 57.196, mean reward: 0.572 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.246, 10.176], loss: 0.001451, mae: 0.041564, mean_q: 1.169426
 661327/1000000: episode: 6614, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 60.201, mean reward: 0.602 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.318], loss: 0.001391, mae: 0.040230, mean_q: 1.171703
 661427/1000000: episode: 6615, duration: 1.411s, episode steps: 100, steps per second: 71, episode reward: 57.534, mean reward: 0.575 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.175, 10.098], loss: 0.001440, mae: 0.041287, mean_q: 1.171716
 661527/1000000: episode: 6616, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 62.021, mean reward: 0.620 [0.518, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.244, 10.098], loss: 0.001562, mae: 0.042697, mean_q: 1.168344
 661627/1000000: episode: 6617, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 64.350, mean reward: 0.644 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.528, 10.414], loss: 0.001447, mae: 0.040930, mean_q: 1.169179
 661727/1000000: episode: 6618, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.141, mean reward: 0.581 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.986, 10.098], loss: 0.001441, mae: 0.041309, mean_q: 1.174414
 661827/1000000: episode: 6619, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 58.162, mean reward: 0.582 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.134], loss: 0.001417, mae: 0.040505, mean_q: 1.170055
 661927/1000000: episode: 6620, duration: 1.376s, episode steps: 100, steps per second: 73, episode reward: 58.489, mean reward: 0.585 [0.503, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.342, 10.122], loss: 0.001545, mae: 0.042633, mean_q: 1.169469
 662027/1000000: episode: 6621, duration: 1.531s, episode steps: 100, steps per second: 65, episode reward: 59.910, mean reward: 0.599 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.463, 10.289], loss: 0.001456, mae: 0.041424, mean_q: 1.171905
 662127/1000000: episode: 6622, duration: 1.613s, episode steps: 100, steps per second: 62, episode reward: 59.219, mean reward: 0.592 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.903, 10.174], loss: 0.001645, mae: 0.043567, mean_q: 1.173523
 662227/1000000: episode: 6623, duration: 1.563s, episode steps: 100, steps per second: 64, episode reward: 55.960, mean reward: 0.560 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.725, 10.098], loss: 0.001449, mae: 0.040658, mean_q: 1.169581
 662327/1000000: episode: 6624, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 57.265, mean reward: 0.573 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.348, 10.120], loss: 0.001629, mae: 0.043433, mean_q: 1.172304
 662427/1000000: episode: 6625, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 58.927, mean reward: 0.589 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.714, 10.263], loss: 0.001429, mae: 0.040966, mean_q: 1.165887
 662527/1000000: episode: 6626, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 55.540, mean reward: 0.555 [0.503, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.543, 10.098], loss: 0.001524, mae: 0.042686, mean_q: 1.165259
 662627/1000000: episode: 6627, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 62.040, mean reward: 0.620 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.829, 10.176], loss: 0.001481, mae: 0.041502, mean_q: 1.166817
 662727/1000000: episode: 6628, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 59.191, mean reward: 0.592 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.930, 10.098], loss: 0.001443, mae: 0.041517, mean_q: 1.164934
 662827/1000000: episode: 6629, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 56.877, mean reward: 0.569 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.462, 10.098], loss: 0.001468, mae: 0.041313, mean_q: 1.164088
 662927/1000000: episode: 6630, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.205, mean reward: 0.582 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.098], loss: 0.001591, mae: 0.043026, mean_q: 1.166462
 663027/1000000: episode: 6631, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.310, mean reward: 0.573 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.391, 10.098], loss: 0.001495, mae: 0.041128, mean_q: 1.164662
 663127/1000000: episode: 6632, duration: 1.610s, episode steps: 100, steps per second: 62, episode reward: 58.082, mean reward: 0.581 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.920, 10.273], loss: 0.001654, mae: 0.043718, mean_q: 1.165037
 663227/1000000: episode: 6633, duration: 1.612s, episode steps: 100, steps per second: 62, episode reward: 58.211, mean reward: 0.582 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.669, 10.232], loss: 0.001405, mae: 0.040537, mean_q: 1.162673
 663327/1000000: episode: 6634, duration: 1.384s, episode steps: 100, steps per second: 72, episode reward: 56.790, mean reward: 0.568 [0.514, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.243], loss: 0.001431, mae: 0.040923, mean_q: 1.165011
 663427/1000000: episode: 6635, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 57.690, mean reward: 0.577 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.947, 10.098], loss: 0.001395, mae: 0.040354, mean_q: 1.165586
 663527/1000000: episode: 6636, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 61.456, mean reward: 0.615 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.113, 10.451], loss: 0.001515, mae: 0.042149, mean_q: 1.162437
 663627/1000000: episode: 6637, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.194, mean reward: 0.592 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.926, 10.172], loss: 0.001403, mae: 0.040368, mean_q: 1.161155
 663727/1000000: episode: 6638, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 59.060, mean reward: 0.591 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.860, 10.098], loss: 0.001562, mae: 0.042972, mean_q: 1.165027
 663827/1000000: episode: 6639, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 60.549, mean reward: 0.605 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.098], loss: 0.001541, mae: 0.042348, mean_q: 1.163130
 663927/1000000: episode: 6640, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 59.910, mean reward: 0.599 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.588, 10.355], loss: 0.001420, mae: 0.040270, mean_q: 1.160866
 664027/1000000: episode: 6641, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 58.015, mean reward: 0.580 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.522, 10.098], loss: 0.001446, mae: 0.041095, mean_q: 1.166137
 664127/1000000: episode: 6642, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.972, mean reward: 0.590 [0.519, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.243, 10.098], loss: 0.001458, mae: 0.041600, mean_q: 1.162500
 664227/1000000: episode: 6643, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.853, mean reward: 0.599 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.615, 10.098], loss: 0.001477, mae: 0.041301, mean_q: 1.168229
 664327/1000000: episode: 6644, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.431, mean reward: 0.594 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.968, 10.098], loss: 0.001356, mae: 0.039792, mean_q: 1.164753
 664427/1000000: episode: 6645, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 57.337, mean reward: 0.573 [0.509, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.327, 10.130], loss: 0.001513, mae: 0.042206, mean_q: 1.165034
 664527/1000000: episode: 6646, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.943, mean reward: 0.589 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.324], loss: 0.001416, mae: 0.040772, mean_q: 1.160199
 664627/1000000: episode: 6647, duration: 1.382s, episode steps: 100, steps per second: 72, episode reward: 62.599, mean reward: 0.626 [0.511, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.251, 10.212], loss: 0.001372, mae: 0.040082, mean_q: 1.165022
 664727/1000000: episode: 6648, duration: 1.701s, episode steps: 100, steps per second: 59, episode reward: 58.965, mean reward: 0.590 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.574, 10.127], loss: 0.001573, mae: 0.042958, mean_q: 1.168192
 664827/1000000: episode: 6649, duration: 1.608s, episode steps: 100, steps per second: 62, episode reward: 59.876, mean reward: 0.599 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.320], loss: 0.001453, mae: 0.041169, mean_q: 1.167189
 664927/1000000: episode: 6650, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 57.407, mean reward: 0.574 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.076, 10.138], loss: 0.001444, mae: 0.040936, mean_q: 1.163723
 665027/1000000: episode: 6651, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 58.652, mean reward: 0.587 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.360], loss: 0.001529, mae: 0.042077, mean_q: 1.166210
 665127/1000000: episode: 6652, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.999, mean reward: 0.580 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.925, 10.098], loss: 0.001452, mae: 0.041313, mean_q: 1.163985
 665227/1000000: episode: 6653, duration: 1.070s, episode steps: 100, steps per second: 94, episode reward: 60.444, mean reward: 0.604 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.631, 10.098], loss: 0.001432, mae: 0.040406, mean_q: 1.164815
 665327/1000000: episode: 6654, duration: 0.912s, episode steps: 100, steps per second: 110, episode reward: 59.444, mean reward: 0.594 [0.524, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.287], loss: 0.001488, mae: 0.041966, mean_q: 1.164414
 665427/1000000: episode: 6655, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 56.375, mean reward: 0.564 [0.499, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.635, 10.098], loss: 0.001497, mae: 0.041782, mean_q: 1.169197
 665527/1000000: episode: 6656, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 60.752, mean reward: 0.608 [0.513, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.117], loss: 0.001516, mae: 0.042433, mean_q: 1.161949
 665627/1000000: episode: 6657, duration: 0.890s, episode steps: 100, steps per second: 112, episode reward: 59.551, mean reward: 0.596 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.203, 10.098], loss: 0.001590, mae: 0.043137, mean_q: 1.167309
 665727/1000000: episode: 6658, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 58.148, mean reward: 0.581 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.428, 10.213], loss: 0.001385, mae: 0.040309, mean_q: 1.161903
 665827/1000000: episode: 6659, duration: 1.069s, episode steps: 100, steps per second: 94, episode reward: 62.302, mean reward: 0.623 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.027, 10.098], loss: 0.001526, mae: 0.041691, mean_q: 1.164442
 665927/1000000: episode: 6660, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 62.175, mean reward: 0.622 [0.514, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.377], loss: 0.001555, mae: 0.042665, mean_q: 1.165818
 666027/1000000: episode: 6661, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 57.278, mean reward: 0.573 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.049, 10.106], loss: 0.001452, mae: 0.041230, mean_q: 1.167558
 666127/1000000: episode: 6662, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 59.491, mean reward: 0.595 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.976, 10.098], loss: 0.001550, mae: 0.042168, mean_q: 1.164945
 666227/1000000: episode: 6663, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 59.333, mean reward: 0.593 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.759, 10.141], loss: 0.001457, mae: 0.041226, mean_q: 1.168013
 666327/1000000: episode: 6664, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.642, mean reward: 0.596 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.332, 10.098], loss: 0.001482, mae: 0.041858, mean_q: 1.168634
 666427/1000000: episode: 6665, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 57.875, mean reward: 0.579 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.540, 10.227], loss: 0.001452, mae: 0.041170, mean_q: 1.169759
 666527/1000000: episode: 6666, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 58.658, mean reward: 0.587 [0.504, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.074, 10.098], loss: 0.001516, mae: 0.042367, mean_q: 1.168571
 666627/1000000: episode: 6667, duration: 1.400s, episode steps: 100, steps per second: 71, episode reward: 57.591, mean reward: 0.576 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.999, 10.208], loss: 0.001443, mae: 0.041595, mean_q: 1.168359
 666727/1000000: episode: 6668, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 58.395, mean reward: 0.584 [0.513, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.809, 10.302], loss: 0.001527, mae: 0.042437, mean_q: 1.166073
 666827/1000000: episode: 6669, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.695, mean reward: 0.577 [0.498, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.544, 10.120], loss: 0.001516, mae: 0.042702, mean_q: 1.165680
 666927/1000000: episode: 6670, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 58.474, mean reward: 0.585 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.019, 10.098], loss: 0.001466, mae: 0.041624, mean_q: 1.163436
 667027/1000000: episode: 6671, duration: 1.509s, episode steps: 100, steps per second: 66, episode reward: 58.412, mean reward: 0.584 [0.506, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.107, 10.098], loss: 0.001459, mae: 0.041074, mean_q: 1.163801
 667127/1000000: episode: 6672, duration: 1.795s, episode steps: 100, steps per second: 56, episode reward: 60.025, mean reward: 0.600 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.294], loss: 0.001401, mae: 0.040986, mean_q: 1.163760
 667227/1000000: episode: 6673, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 58.347, mean reward: 0.583 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.406, 10.098], loss: 0.001470, mae: 0.041758, mean_q: 1.164406
 667327/1000000: episode: 6674, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 59.786, mean reward: 0.598 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.738, 10.269], loss: 0.001453, mae: 0.041364, mean_q: 1.166607
 667427/1000000: episode: 6675, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 59.028, mean reward: 0.590 [0.504, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.595, 10.133], loss: 0.001560, mae: 0.042507, mean_q: 1.168239
 667527/1000000: episode: 6676, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 59.606, mean reward: 0.596 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.376, 10.098], loss: 0.001486, mae: 0.042513, mean_q: 1.170307
 667627/1000000: episode: 6677, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 60.166, mean reward: 0.602 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.952, 10.175], loss: 0.001518, mae: 0.042500, mean_q: 1.167698
 667727/1000000: episode: 6678, duration: 1.420s, episode steps: 100, steps per second: 70, episode reward: 61.300, mean reward: 0.613 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.465], loss: 0.001496, mae: 0.041971, mean_q: 1.167141
 667827/1000000: episode: 6679, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.007, mean reward: 0.580 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.622, 10.098], loss: 0.001526, mae: 0.042415, mean_q: 1.170704
 667927/1000000: episode: 6680, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 59.249, mean reward: 0.592 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.403, 10.098], loss: 0.001491, mae: 0.041812, mean_q: 1.168031
 668027/1000000: episode: 6681, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 62.411, mean reward: 0.624 [0.506, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.716, 10.098], loss: 0.001509, mae: 0.042077, mean_q: 1.170550
 668127/1000000: episode: 6682, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.478, mean reward: 0.585 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.145], loss: 0.001519, mae: 0.041704, mean_q: 1.174047
 668227/1000000: episode: 6683, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 58.539, mean reward: 0.585 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.062, 10.098], loss: 0.001443, mae: 0.041376, mean_q: 1.169198
 668327/1000000: episode: 6684, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.486, mean reward: 0.575 [0.500, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.658, 10.098], loss: 0.001385, mae: 0.039982, mean_q: 1.168764
 668427/1000000: episode: 6685, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 59.036, mean reward: 0.590 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.740, 10.223], loss: 0.001546, mae: 0.042742, mean_q: 1.169062
 668527/1000000: episode: 6686, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.165, mean reward: 0.582 [0.505, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.098], loss: 0.001435, mae: 0.041468, mean_q: 1.166412
 668627/1000000: episode: 6687, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 57.036, mean reward: 0.570 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.985, 10.159], loss: 0.001556, mae: 0.043099, mean_q: 1.169132
 668727/1000000: episode: 6688, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 59.330, mean reward: 0.593 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.680, 10.156], loss: 0.001471, mae: 0.041536, mean_q: 1.166523
 668827/1000000: episode: 6689, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 59.195, mean reward: 0.592 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.093, 10.098], loss: 0.001525, mae: 0.042861, mean_q: 1.168302
 668927/1000000: episode: 6690, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 58.289, mean reward: 0.583 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.469, 10.295], loss: 0.001482, mae: 0.042147, mean_q: 1.171172
 669027/1000000: episode: 6691, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 58.193, mean reward: 0.582 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.298, 10.285], loss: 0.001492, mae: 0.042278, mean_q: 1.169130
 669127/1000000: episode: 6692, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.113, mean reward: 0.591 [0.509, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.077, 10.219], loss: 0.001611, mae: 0.043507, mean_q: 1.165738
 669227/1000000: episode: 6693, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.700, mean reward: 0.597 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.235, 10.236], loss: 0.001570, mae: 0.042970, mean_q: 1.166788
 669327/1000000: episode: 6694, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 59.850, mean reward: 0.599 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.629, 10.195], loss: 0.001591, mae: 0.043481, mean_q: 1.166546
 669427/1000000: episode: 6695, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 58.729, mean reward: 0.587 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.745, 10.098], loss: 0.001539, mae: 0.042915, mean_q: 1.171595
 669527/1000000: episode: 6696, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.312, mean reward: 0.573 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.646, 10.189], loss: 0.001521, mae: 0.042305, mean_q: 1.170549
 669627/1000000: episode: 6697, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 60.146, mean reward: 0.601 [0.520, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.181, 10.098], loss: 0.001646, mae: 0.044104, mean_q: 1.166381
 669727/1000000: episode: 6698, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.417, mean reward: 0.584 [0.498, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.728, 10.098], loss: 0.001474, mae: 0.041682, mean_q: 1.166726
 669827/1000000: episode: 6699, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 60.836, mean reward: 0.608 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.477], loss: 0.001524, mae: 0.042491, mean_q: 1.171560
 669927/1000000: episode: 6700, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 59.122, mean reward: 0.591 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.695, 10.098], loss: 0.001594, mae: 0.042972, mean_q: 1.168574
 670027/1000000: episode: 6701, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 61.344, mean reward: 0.613 [0.513, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.027, 10.272], loss: 0.001607, mae: 0.043616, mean_q: 1.167235
 670127/1000000: episode: 6702, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.914, mean reward: 0.579 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.809, 10.146], loss: 0.001501, mae: 0.042135, mean_q: 1.168600
 670227/1000000: episode: 6703, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 60.044, mean reward: 0.600 [0.514, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.861, 10.264], loss: 0.001618, mae: 0.043262, mean_q: 1.172980
 670327/1000000: episode: 6704, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 57.883, mean reward: 0.579 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.152, 10.137], loss: 0.001598, mae: 0.043621, mean_q: 1.168287
 670427/1000000: episode: 6705, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 58.255, mean reward: 0.583 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.935, 10.098], loss: 0.001512, mae: 0.042710, mean_q: 1.170001
 670527/1000000: episode: 6706, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.299, mean reward: 0.593 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.745, 10.151], loss: 0.001724, mae: 0.044448, mean_q: 1.170662
 670627/1000000: episode: 6707, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.349, mean reward: 0.573 [0.513, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.866, 10.142], loss: 0.001617, mae: 0.043422, mean_q: 1.168230
 670727/1000000: episode: 6708, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 56.849, mean reward: 0.568 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.492, 10.236], loss: 0.001542, mae: 0.042406, mean_q: 1.166273
 670827/1000000: episode: 6709, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 61.561, mean reward: 0.616 [0.515, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.693, 10.320], loss: 0.001653, mae: 0.043754, mean_q: 1.170756
 670927/1000000: episode: 6710, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.652, mean reward: 0.587 [0.510, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.098], loss: 0.001480, mae: 0.041988, mean_q: 1.167804
 671027/1000000: episode: 6711, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 59.725, mean reward: 0.597 [0.501, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.292, 10.098], loss: 0.001510, mae: 0.042509, mean_q: 1.166663
 671127/1000000: episode: 6712, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 60.746, mean reward: 0.607 [0.515, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.304, 10.098], loss: 0.001553, mae: 0.042214, mean_q: 1.168491
 671227/1000000: episode: 6713, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.211, mean reward: 0.582 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.198, 10.194], loss: 0.001568, mae: 0.043295, mean_q: 1.169647
 671327/1000000: episode: 6714, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.432, mean reward: 0.584 [0.508, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.725, 10.098], loss: 0.001554, mae: 0.042545, mean_q: 1.165044
 671427/1000000: episode: 6715, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.562, mean reward: 0.586 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.023, 10.271], loss: 0.001441, mae: 0.041441, mean_q: 1.168929
 671527/1000000: episode: 6716, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 60.808, mean reward: 0.608 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.329, 10.212], loss: 0.001572, mae: 0.043377, mean_q: 1.166207
 671627/1000000: episode: 6717, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.444, mean reward: 0.584 [0.500, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.867, 10.200], loss: 0.001551, mae: 0.043071, mean_q: 1.166958
 671727/1000000: episode: 6718, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.368, mean reward: 0.574 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.964, 10.133], loss: 0.001547, mae: 0.042379, mean_q: 1.169024
 671827/1000000: episode: 6719, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 62.171, mean reward: 0.622 [0.498, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.510, 10.289], loss: 0.001715, mae: 0.044926, mean_q: 1.172700
 671927/1000000: episode: 6720, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 59.852, mean reward: 0.599 [0.502, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.564, 10.279], loss: 0.001590, mae: 0.042899, mean_q: 1.170225
 672027/1000000: episode: 6721, duration: 1.406s, episode steps: 100, steps per second: 71, episode reward: 58.445, mean reward: 0.584 [0.506, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.461, 10.311], loss: 0.001630, mae: 0.043423, mean_q: 1.171250
 672127/1000000: episode: 6722, duration: 1.590s, episode steps: 100, steps per second: 63, episode reward: 61.390, mean reward: 0.614 [0.512, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.240, 10.140], loss: 0.001688, mae: 0.044080, mean_q: 1.171404
 672227/1000000: episode: 6723, duration: 1.399s, episode steps: 100, steps per second: 72, episode reward: 61.099, mean reward: 0.611 [0.510, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.997, 10.098], loss: 0.001553, mae: 0.043149, mean_q: 1.169217
 672327/1000000: episode: 6724, duration: 1.446s, episode steps: 100, steps per second: 69, episode reward: 57.146, mean reward: 0.571 [0.498, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.966, 10.254], loss: 0.001526, mae: 0.042460, mean_q: 1.170851
 672427/1000000: episode: 6725, duration: 1.505s, episode steps: 100, steps per second: 66, episode reward: 58.424, mean reward: 0.584 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.202, 10.134], loss: 0.001532, mae: 0.042574, mean_q: 1.169891
 672527/1000000: episode: 6726, duration: 1.321s, episode steps: 100, steps per second: 76, episode reward: 59.486, mean reward: 0.595 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.652, 10.261], loss: 0.001586, mae: 0.042760, mean_q: 1.167994
 672627/1000000: episode: 6727, duration: 1.310s, episode steps: 100, steps per second: 76, episode reward: 58.694, mean reward: 0.587 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.098], loss: 0.001711, mae: 0.044947, mean_q: 1.168681
 672727/1000000: episode: 6728, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 60.242, mean reward: 0.602 [0.515, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.098], loss: 0.001501, mae: 0.042732, mean_q: 1.168707
 672827/1000000: episode: 6729, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 57.688, mean reward: 0.577 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.121], loss: 0.001678, mae: 0.044610, mean_q: 1.169980
 672927/1000000: episode: 6730, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 56.902, mean reward: 0.569 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.042, 10.240], loss: 0.001573, mae: 0.043204, mean_q: 1.168448
 673027/1000000: episode: 6731, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 60.173, mean reward: 0.602 [0.501, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.218, 10.463], loss: 0.001568, mae: 0.042939, mean_q: 1.168389
 673127/1000000: episode: 6732, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 60.919, mean reward: 0.609 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.433, 10.098], loss: 0.001589, mae: 0.043285, mean_q: 1.168382
 673227/1000000: episode: 6733, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 58.675, mean reward: 0.587 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.252], loss: 0.001658, mae: 0.044671, mean_q: 1.169961
 673327/1000000: episode: 6734, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 59.669, mean reward: 0.597 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.320], loss: 0.001714, mae: 0.044211, mean_q: 1.169261
 673427/1000000: episode: 6735, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 60.611, mean reward: 0.606 [0.500, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.499, 10.098], loss: 0.001633, mae: 0.043953, mean_q: 1.168123
 673527/1000000: episode: 6736, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 60.440, mean reward: 0.604 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.365, 10.314], loss: 0.001495, mae: 0.042135, mean_q: 1.170822
 673627/1000000: episode: 6737, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 56.997, mean reward: 0.570 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.223], loss: 0.001577, mae: 0.043177, mean_q: 1.171931
 673727/1000000: episode: 6738, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 59.253, mean reward: 0.593 [0.501, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.546, 10.507], loss: 0.001523, mae: 0.042084, mean_q: 1.167825
 673827/1000000: episode: 6739, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 55.965, mean reward: 0.560 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.955, 10.098], loss: 0.001661, mae: 0.044391, mean_q: 1.171258
 673927/1000000: episode: 6740, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 58.322, mean reward: 0.583 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.215], loss: 0.001625, mae: 0.043874, mean_q: 1.171269
 674027/1000000: episode: 6741, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 57.152, mean reward: 0.572 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.246, 10.098], loss: 0.001556, mae: 0.043049, mean_q: 1.170257
 674127/1000000: episode: 6742, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 59.916, mean reward: 0.599 [0.498, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.188, 10.163], loss: 0.001550, mae: 0.042732, mean_q: 1.170275
 674227/1000000: episode: 6743, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.822, mean reward: 0.578 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.319, 10.151], loss: 0.001612, mae: 0.043482, mean_q: 1.172339
 674327/1000000: episode: 6744, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 61.456, mean reward: 0.615 [0.504, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.277, 10.098], loss: 0.001515, mae: 0.042183, mean_q: 1.168841
 674427/1000000: episode: 6745, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 59.380, mean reward: 0.594 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.001521, mae: 0.042133, mean_q: 1.167282
 674527/1000000: episode: 6746, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.726, mean reward: 0.587 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.953, 10.115], loss: 0.001607, mae: 0.043610, mean_q: 1.168025
 674627/1000000: episode: 6747, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 58.896, mean reward: 0.589 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.385, 10.353], loss: 0.001622, mae: 0.043366, mean_q: 1.170425
 674727/1000000: episode: 6748, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 59.603, mean reward: 0.596 [0.519, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.277, 10.098], loss: 0.001578, mae: 0.043402, mean_q: 1.172244
 674827/1000000: episode: 6749, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 58.910, mean reward: 0.589 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.542, 10.098], loss: 0.001512, mae: 0.042269, mean_q: 1.169596
 674927/1000000: episode: 6750, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 59.870, mean reward: 0.599 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.442, 10.336], loss: 0.001549, mae: 0.042166, mean_q: 1.168288
 675027/1000000: episode: 6751, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 58.786, mean reward: 0.588 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.699, 10.232], loss: 0.001469, mae: 0.041859, mean_q: 1.168676
 675127/1000000: episode: 6752, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 58.265, mean reward: 0.583 [0.500, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.539, 10.098], loss: 0.001572, mae: 0.043074, mean_q: 1.167539
 675227/1000000: episode: 6753, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 58.067, mean reward: 0.581 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.714, 10.098], loss: 0.001524, mae: 0.042485, mean_q: 1.166919
 675327/1000000: episode: 6754, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 62.512, mean reward: 0.625 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.788, 10.098], loss: 0.001489, mae: 0.042019, mean_q: 1.172439
 675427/1000000: episode: 6755, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 59.237, mean reward: 0.592 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.239], loss: 0.001439, mae: 0.041077, mean_q: 1.168625
 675527/1000000: episode: 6756, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 57.459, mean reward: 0.575 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.917, 10.305], loss: 0.001442, mae: 0.041366, mean_q: 1.170470
 675627/1000000: episode: 6757, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 57.733, mean reward: 0.577 [0.499, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.283], loss: 0.001551, mae: 0.042977, mean_q: 1.172529
 675727/1000000: episode: 6758, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.812, mean reward: 0.598 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.513, 10.098], loss: 0.001511, mae: 0.042419, mean_q: 1.169872
 675827/1000000: episode: 6759, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 57.727, mean reward: 0.577 [0.514, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.162, 10.312], loss: 0.001468, mae: 0.042012, mean_q: 1.168468
 675927/1000000: episode: 6760, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 60.993, mean reward: 0.610 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.082, 10.098], loss: 0.001569, mae: 0.043740, mean_q: 1.169105
 676027/1000000: episode: 6761, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 57.427, mean reward: 0.574 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.142, 10.175], loss: 0.001444, mae: 0.041651, mean_q: 1.170361
 676127/1000000: episode: 6762, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 59.447, mean reward: 0.594 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.625, 10.138], loss: 0.001480, mae: 0.042084, mean_q: 1.171772
 676227/1000000: episode: 6763, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 60.638, mean reward: 0.606 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.756, 10.098], loss: 0.001468, mae: 0.041424, mean_q: 1.168771
 676327/1000000: episode: 6764, duration: 1.515s, episode steps: 100, steps per second: 66, episode reward: 58.523, mean reward: 0.585 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.575, 10.152], loss: 0.001460, mae: 0.041299, mean_q: 1.168744
 676427/1000000: episode: 6765, duration: 1.621s, episode steps: 100, steps per second: 62, episode reward: 58.112, mean reward: 0.581 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.001434, mae: 0.041399, mean_q: 1.172376
 676527/1000000: episode: 6766, duration: 1.351s, episode steps: 100, steps per second: 74, episode reward: 59.709, mean reward: 0.597 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.282], loss: 0.001475, mae: 0.041462, mean_q: 1.168079
 676627/1000000: episode: 6767, duration: 1.421s, episode steps: 100, steps per second: 70, episode reward: 60.988, mean reward: 0.610 [0.507, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.296, 10.098], loss: 0.001463, mae: 0.041593, mean_q: 1.171166
 676727/1000000: episode: 6768, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.752, mean reward: 0.588 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.201], loss: 0.001457, mae: 0.041513, mean_q: 1.168665
 676827/1000000: episode: 6769, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 59.114, mean reward: 0.591 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.633, 10.098], loss: 0.001367, mae: 0.040752, mean_q: 1.166867
 676927/1000000: episode: 6770, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 58.252, mean reward: 0.583 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.517, 10.355], loss: 0.001445, mae: 0.041428, mean_q: 1.168956
 677027/1000000: episode: 6771, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 59.902, mean reward: 0.599 [0.498, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.230, 10.100], loss: 0.001388, mae: 0.040256, mean_q: 1.168287
 677127/1000000: episode: 6772, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 59.766, mean reward: 0.598 [0.510, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.109, 10.249], loss: 0.001365, mae: 0.040237, mean_q: 1.168944
 677227/1000000: episode: 6773, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 57.678, mean reward: 0.577 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.889, 10.173], loss: 0.001463, mae: 0.041881, mean_q: 1.170136
 677327/1000000: episode: 6774, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 58.684, mean reward: 0.587 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.055, 10.098], loss: 0.001432, mae: 0.041453, mean_q: 1.169622
 677427/1000000: episode: 6775, duration: 1.518s, episode steps: 100, steps per second: 66, episode reward: 57.836, mean reward: 0.578 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.674, 10.162], loss: 0.001442, mae: 0.042054, mean_q: 1.170199
 677527/1000000: episode: 6776, duration: 1.513s, episode steps: 100, steps per second: 66, episode reward: 61.637, mean reward: 0.616 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.692, 10.098], loss: 0.001499, mae: 0.042318, mean_q: 1.167879
 677627/1000000: episode: 6777, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 58.286, mean reward: 0.583 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.650, 10.098], loss: 0.001435, mae: 0.041612, mean_q: 1.165469
 677727/1000000: episode: 6778, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.539, mean reward: 0.585 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.434, 10.219], loss: 0.001438, mae: 0.041680, mean_q: 1.170654
 677827/1000000: episode: 6779, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 57.443, mean reward: 0.574 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.525, 10.212], loss: 0.001479, mae: 0.042541, mean_q: 1.172569
 677927/1000000: episode: 6780, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 59.004, mean reward: 0.590 [0.507, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.098], loss: 0.001378, mae: 0.040602, mean_q: 1.169023
 678027/1000000: episode: 6781, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.612, mean reward: 0.576 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.380, 10.110], loss: 0.001532, mae: 0.042983, mean_q: 1.171490
 678127/1000000: episode: 6782, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 63.702, mean reward: 0.637 [0.505, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.990, 10.444], loss: 0.001543, mae: 0.042427, mean_q: 1.169587
 678227/1000000: episode: 6783, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 58.309, mean reward: 0.583 [0.510, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.135], loss: 0.001457, mae: 0.041556, mean_q: 1.169996
 678327/1000000: episode: 6784, duration: 1.522s, episode steps: 100, steps per second: 66, episode reward: 60.142, mean reward: 0.601 [0.511, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.223, 10.266], loss: 0.001395, mae: 0.041097, mean_q: 1.166182
 678427/1000000: episode: 6785, duration: 1.387s, episode steps: 100, steps per second: 72, episode reward: 57.683, mean reward: 0.577 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.131], loss: 0.001420, mae: 0.041363, mean_q: 1.167592
 678527/1000000: episode: 6786, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 60.114, mean reward: 0.601 [0.509, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.680, 10.098], loss: 0.001427, mae: 0.041276, mean_q: 1.162337
 678627/1000000: episode: 6787, duration: 1.599s, episode steps: 100, steps per second: 63, episode reward: 59.663, mean reward: 0.597 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.787, 10.098], loss: 0.001377, mae: 0.040612, mean_q: 1.163859
 678727/1000000: episode: 6788, duration: 1.383s, episode steps: 100, steps per second: 72, episode reward: 59.433, mean reward: 0.594 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.404, 10.142], loss: 0.001429, mae: 0.041264, mean_q: 1.169220
 678827/1000000: episode: 6789, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 56.829, mean reward: 0.568 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.594, 10.253], loss: 0.001561, mae: 0.043334, mean_q: 1.170253
 678927/1000000: episode: 6790, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.992, mean reward: 0.590 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.262, 10.343], loss: 0.001508, mae: 0.042181, mean_q: 1.170906
 679027/1000000: episode: 6791, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 58.082, mean reward: 0.581 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.853, 10.098], loss: 0.001386, mae: 0.040519, mean_q: 1.168262
 679127/1000000: episode: 6792, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 58.518, mean reward: 0.585 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.208, 10.098], loss: 0.001441, mae: 0.041253, mean_q: 1.167772
 679227/1000000: episode: 6793, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 57.455, mean reward: 0.575 [0.500, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.098], loss: 0.001335, mae: 0.039829, mean_q: 1.166860
 679327/1000000: episode: 6794, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 59.076, mean reward: 0.591 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.449, 10.098], loss: 0.001354, mae: 0.040434, mean_q: 1.166726
 679427/1000000: episode: 6795, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 60.553, mean reward: 0.606 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.098], loss: 0.001463, mae: 0.041922, mean_q: 1.167976
 679527/1000000: episode: 6796, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 61.625, mean reward: 0.616 [0.502, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.293], loss: 0.001401, mae: 0.041175, mean_q: 1.166805
 679627/1000000: episode: 6797, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 62.030, mean reward: 0.620 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.226, 10.216], loss: 0.001359, mae: 0.040147, mean_q: 1.167251
 679727/1000000: episode: 6798, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.408, mean reward: 0.574 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.617, 10.269], loss: 0.001408, mae: 0.040861, mean_q: 1.166477
 679827/1000000: episode: 6799, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.681, mean reward: 0.577 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.528, 10.154], loss: 0.001520, mae: 0.042676, mean_q: 1.171654
 679927/1000000: episode: 6800, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 57.710, mean reward: 0.577 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.475, 10.141], loss: 0.001414, mae: 0.040992, mean_q: 1.167406
 680027/1000000: episode: 6801, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.325, mean reward: 0.573 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.472, 10.098], loss: 0.001456, mae: 0.041486, mean_q: 1.166539
 680127/1000000: episode: 6802, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.271, mean reward: 0.593 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.470, 10.145], loss: 0.001483, mae: 0.041716, mean_q: 1.167490
 680227/1000000: episode: 6803, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.760, mean reward: 0.588 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.302, 10.264], loss: 0.001520, mae: 0.042063, mean_q: 1.168040
 680327/1000000: episode: 6804, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 59.715, mean reward: 0.597 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.220, 10.166], loss: 0.001382, mae: 0.040523, mean_q: 1.166005
 680427/1000000: episode: 6805, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 58.145, mean reward: 0.581 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.704, 10.208], loss: 0.001517, mae: 0.042257, mean_q: 1.164543
 680527/1000000: episode: 6806, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 57.520, mean reward: 0.575 [0.497, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.098], loss: 0.001521, mae: 0.042303, mean_q: 1.167777
 680627/1000000: episode: 6807, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.442, mean reward: 0.584 [0.504, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.372, 10.199], loss: 0.001540, mae: 0.042445, mean_q: 1.167002
 680727/1000000: episode: 6808, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 58.546, mean reward: 0.585 [0.512, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.774, 10.098], loss: 0.001488, mae: 0.042094, mean_q: 1.165609
 680827/1000000: episode: 6809, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 58.688, mean reward: 0.587 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.142], loss: 0.001452, mae: 0.041666, mean_q: 1.169155
 680927/1000000: episode: 6810, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 57.845, mean reward: 0.578 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.177, 10.098], loss: 0.001489, mae: 0.041264, mean_q: 1.168059
 681027/1000000: episode: 6811, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 59.893, mean reward: 0.599 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.120, 10.098], loss: 0.001572, mae: 0.042520, mean_q: 1.164924
 681127/1000000: episode: 6812, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 57.805, mean reward: 0.578 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.180, 10.261], loss: 0.001523, mae: 0.042413, mean_q: 1.166508
 681227/1000000: episode: 6813, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 59.107, mean reward: 0.591 [0.514, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.098], loss: 0.001443, mae: 0.041670, mean_q: 1.161541
 681327/1000000: episode: 6814, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 57.522, mean reward: 0.575 [0.505, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.098], loss: 0.001479, mae: 0.041915, mean_q: 1.165771
 681427/1000000: episode: 6815, duration: 1.549s, episode steps: 100, steps per second: 65, episode reward: 59.226, mean reward: 0.592 [0.515, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.438, 10.284], loss: 0.001492, mae: 0.041615, mean_q: 1.162201
 681527/1000000: episode: 6816, duration: 1.578s, episode steps: 100, steps per second: 63, episode reward: 57.645, mean reward: 0.576 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.837, 10.098], loss: 0.001511, mae: 0.042469, mean_q: 1.165809
 681627/1000000: episode: 6817, duration: 1.340s, episode steps: 100, steps per second: 75, episode reward: 59.823, mean reward: 0.598 [0.515, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.991, 10.316], loss: 0.001525, mae: 0.042770, mean_q: 1.167809
 681727/1000000: episode: 6818, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 59.740, mean reward: 0.597 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.493, 10.098], loss: 0.001561, mae: 0.042760, mean_q: 1.165434
 681827/1000000: episode: 6819, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 60.152, mean reward: 0.602 [0.502, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.779, 10.301], loss: 0.001507, mae: 0.042018, mean_q: 1.169246
 681927/1000000: episode: 6820, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 59.443, mean reward: 0.594 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.417, 10.249], loss: 0.001510, mae: 0.042028, mean_q: 1.164599
 682027/1000000: episode: 6821, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 60.038, mean reward: 0.600 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.618, 10.197], loss: 0.001507, mae: 0.042398, mean_q: 1.167451
 682127/1000000: episode: 6822, duration: 1.483s, episode steps: 100, steps per second: 67, episode reward: 58.378, mean reward: 0.584 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.117, 10.260], loss: 0.001519, mae: 0.042499, mean_q: 1.168149
 682227/1000000: episode: 6823, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 57.911, mean reward: 0.579 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.112, 10.158], loss: 0.001465, mae: 0.041666, mean_q: 1.164489
 682327/1000000: episode: 6824, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 58.739, mean reward: 0.587 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.423, 10.103], loss: 0.001485, mae: 0.042036, mean_q: 1.163220
 682427/1000000: episode: 6825, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 59.398, mean reward: 0.594 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.433, 10.098], loss: 0.001425, mae: 0.040866, mean_q: 1.166906
 682527/1000000: episode: 6826, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 61.598, mean reward: 0.616 [0.512, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.185, 10.098], loss: 0.001390, mae: 0.041018, mean_q: 1.164265
 682627/1000000: episode: 6827, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 57.695, mean reward: 0.577 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.192, 10.187], loss: 0.001443, mae: 0.041145, mean_q: 1.167588
 682727/1000000: episode: 6828, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.466, mean reward: 0.595 [0.512, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.229, 10.181], loss: 0.001532, mae: 0.042384, mean_q: 1.166752
 682827/1000000: episode: 6829, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 61.064, mean reward: 0.611 [0.503, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.882, 10.098], loss: 0.001412, mae: 0.041126, mean_q: 1.168425
 682927/1000000: episode: 6830, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 62.859, mean reward: 0.629 [0.516, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.457, 10.352], loss: 0.001403, mae: 0.040027, mean_q: 1.169436
 683027/1000000: episode: 6831, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 57.593, mean reward: 0.576 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.462, 10.098], loss: 0.001507, mae: 0.041502, mean_q: 1.172357
 683127/1000000: episode: 6832, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 57.781, mean reward: 0.578 [0.498, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.260, 10.238], loss: 0.001478, mae: 0.041532, mean_q: 1.169364
 683227/1000000: episode: 6833, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 58.939, mean reward: 0.589 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.100, 10.098], loss: 0.001430, mae: 0.041154, mean_q: 1.171829
 683327/1000000: episode: 6834, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 57.687, mean reward: 0.577 [0.507, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.440, 10.154], loss: 0.001504, mae: 0.042241, mean_q: 1.168255
 683427/1000000: episode: 6835, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 58.623, mean reward: 0.586 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.232, 10.098], loss: 0.001514, mae: 0.041615, mean_q: 1.165598
 683527/1000000: episode: 6836, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 59.493, mean reward: 0.595 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.492, 10.159], loss: 0.001417, mae: 0.041257, mean_q: 1.165880
 683627/1000000: episode: 6837, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 59.273, mean reward: 0.593 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.506, 10.098], loss: 0.001451, mae: 0.041168, mean_q: 1.162438
 683727/1000000: episode: 6838, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 60.176, mean reward: 0.602 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.485, 10.259], loss: 0.001365, mae: 0.040200, mean_q: 1.164184
 683827/1000000: episode: 6839, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 57.367, mean reward: 0.574 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.195, 10.100], loss: 0.001466, mae: 0.041810, mean_q: 1.167727
 683927/1000000: episode: 6840, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.003, mean reward: 0.590 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.098], loss: 0.001409, mae: 0.040006, mean_q: 1.165218
 684027/1000000: episode: 6841, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 59.795, mean reward: 0.598 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.172, 10.098], loss: 0.001535, mae: 0.042241, mean_q: 1.164383
 684127/1000000: episode: 6842, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 56.968, mean reward: 0.570 [0.512, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.003, 10.176], loss: 0.001464, mae: 0.041346, mean_q: 1.165984
 684227/1000000: episode: 6843, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.712, mean reward: 0.577 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.932, 10.098], loss: 0.001448, mae: 0.041455, mean_q: 1.166645
 684327/1000000: episode: 6844, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.413, mean reward: 0.594 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.210], loss: 0.001551, mae: 0.042449, mean_q: 1.167895
 684427/1000000: episode: 6845, duration: 0.910s, episode steps: 100, steps per second: 110, episode reward: 58.002, mean reward: 0.580 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.831, 10.192], loss: 0.001608, mae: 0.043330, mean_q: 1.167477
 684527/1000000: episode: 6846, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 57.079, mean reward: 0.571 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.939, 10.098], loss: 0.001468, mae: 0.041690, mean_q: 1.164469
 684627/1000000: episode: 6847, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 58.660, mean reward: 0.587 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.088, 10.098], loss: 0.001429, mae: 0.040582, mean_q: 1.160674
 684727/1000000: episode: 6848, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.022, mean reward: 0.580 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.546, 10.267], loss: 0.001539, mae: 0.042009, mean_q: 1.165359
 684827/1000000: episode: 6849, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 57.259, mean reward: 0.573 [0.501, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.201, 10.186], loss: 0.001404, mae: 0.040251, mean_q: 1.165470
 684927/1000000: episode: 6850, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 58.538, mean reward: 0.585 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.403, 10.186], loss: 0.001433, mae: 0.040611, mean_q: 1.164827
 685027/1000000: episode: 6851, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 58.739, mean reward: 0.587 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.715, 10.243], loss: 0.001453, mae: 0.041189, mean_q: 1.161998
 685127/1000000: episode: 6852, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.761, mean reward: 0.598 [0.517, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.976, 10.460], loss: 0.001401, mae: 0.041285, mean_q: 1.163449
 685227/1000000: episode: 6853, duration: 1.316s, episode steps: 100, steps per second: 76, episode reward: 58.149, mean reward: 0.581 [0.502, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.649, 10.098], loss: 0.001351, mae: 0.039823, mean_q: 1.161158
 685327/1000000: episode: 6854, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 59.068, mean reward: 0.591 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.759, 10.098], loss: 0.001347, mae: 0.040348, mean_q: 1.161005
 685427/1000000: episode: 6855, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 58.648, mean reward: 0.586 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.128, 10.098], loss: 0.001376, mae: 0.039932, mean_q: 1.162846
 685527/1000000: episode: 6856, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 57.004, mean reward: 0.570 [0.500, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.183, 10.098], loss: 0.001445, mae: 0.041120, mean_q: 1.161838
 685627/1000000: episode: 6857, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 59.846, mean reward: 0.598 [0.522, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.476, 10.209], loss: 0.001403, mae: 0.040604, mean_q: 1.162725
 685727/1000000: episode: 6858, duration: 1.498s, episode steps: 100, steps per second: 67, episode reward: 58.590, mean reward: 0.586 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.460, 10.098], loss: 0.001437, mae: 0.041088, mean_q: 1.163907
 685827/1000000: episode: 6859, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.314, mean reward: 0.573 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.665, 10.167], loss: 0.001421, mae: 0.040433, mean_q: 1.160052
 685927/1000000: episode: 6860, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.739, mean reward: 0.587 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.681, 10.098], loss: 0.001389, mae: 0.040718, mean_q: 1.162728
 686027/1000000: episode: 6861, duration: 1.275s, episode steps: 100, steps per second: 78, episode reward: 60.471, mean reward: 0.605 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.661, 10.151], loss: 0.001407, mae: 0.040714, mean_q: 1.162660
 686127/1000000: episode: 6862, duration: 1.227s, episode steps: 100, steps per second: 81, episode reward: 59.165, mean reward: 0.592 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.643, 10.164], loss: 0.001460, mae: 0.040968, mean_q: 1.162944
 686227/1000000: episode: 6863, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 63.201, mean reward: 0.632 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.770, 10.098], loss: 0.001445, mae: 0.041206, mean_q: 1.165722
 686327/1000000: episode: 6864, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 62.977, mean reward: 0.630 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.158, 10.098], loss: 0.001465, mae: 0.041734, mean_q: 1.165843
 686427/1000000: episode: 6865, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 59.656, mean reward: 0.597 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.950, 10.098], loss: 0.001474, mae: 0.041688, mean_q: 1.167898
 686527/1000000: episode: 6866, duration: 1.058s, episode steps: 100, steps per second: 94, episode reward: 59.390, mean reward: 0.594 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.810, 10.098], loss: 0.001372, mae: 0.040634, mean_q: 1.166607
 686627/1000000: episode: 6867, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.730, mean reward: 0.577 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.840, 10.158], loss: 0.001523, mae: 0.042320, mean_q: 1.169816
 686727/1000000: episode: 6868, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 58.454, mean reward: 0.585 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.619, 10.158], loss: 0.001506, mae: 0.042481, mean_q: 1.168124
 686827/1000000: episode: 6869, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 59.729, mean reward: 0.597 [0.514, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.262, 10.230], loss: 0.001559, mae: 0.043015, mean_q: 1.168368
 686927/1000000: episode: 6870, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 57.605, mean reward: 0.576 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.292, 10.180], loss: 0.001450, mae: 0.041495, mean_q: 1.171423
 687027/1000000: episode: 6871, duration: 1.510s, episode steps: 100, steps per second: 66, episode reward: 60.453, mean reward: 0.605 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.708, 10.416], loss: 0.001445, mae: 0.041175, mean_q: 1.163325
 687127/1000000: episode: 6872, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.303, mean reward: 0.583 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.218, 10.110], loss: 0.001477, mae: 0.041316, mean_q: 1.170818
 687227/1000000: episode: 6873, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 61.638, mean reward: 0.616 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.439, 10.173], loss: 0.001352, mae: 0.040670, mean_q: 1.163815
 687327/1000000: episode: 6874, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 57.732, mean reward: 0.577 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.406, 10.106], loss: 0.001382, mae: 0.040712, mean_q: 1.169097
 687427/1000000: episode: 6875, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.992, mean reward: 0.580 [0.512, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.887, 10.098], loss: 0.001512, mae: 0.042289, mean_q: 1.168432
 687527/1000000: episode: 6876, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.250, mean reward: 0.582 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.700, 10.113], loss: 0.001430, mae: 0.040859, mean_q: 1.166023
 687627/1000000: episode: 6877, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 57.259, mean reward: 0.573 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.517, 10.098], loss: 0.001508, mae: 0.042431, mean_q: 1.166657
 687727/1000000: episode: 6878, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 60.462, mean reward: 0.605 [0.513, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.582, 10.219], loss: 0.001500, mae: 0.041754, mean_q: 1.166514
 687827/1000000: episode: 6879, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 61.494, mean reward: 0.615 [0.516, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.857, 10.098], loss: 0.001435, mae: 0.040783, mean_q: 1.165468
 687927/1000000: episode: 6880, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 58.803, mean reward: 0.588 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.363, 10.098], loss: 0.001558, mae: 0.043465, mean_q: 1.165927
 688027/1000000: episode: 6881, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 57.552, mean reward: 0.576 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.767, 10.123], loss: 0.001481, mae: 0.041852, mean_q: 1.164149
 688127/1000000: episode: 6882, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 61.439, mean reward: 0.614 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.553, 10.098], loss: 0.001450, mae: 0.041415, mean_q: 1.168020
 688227/1000000: episode: 6883, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 58.858, mean reward: 0.589 [0.513, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.395, 10.250], loss: 0.001465, mae: 0.041725, mean_q: 1.167839
 688327/1000000: episode: 6884, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 59.895, mean reward: 0.599 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.194, 10.098], loss: 0.001519, mae: 0.042106, mean_q: 1.169722
 688427/1000000: episode: 6885, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 57.906, mean reward: 0.579 [0.510, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.457, 10.204], loss: 0.001533, mae: 0.042128, mean_q: 1.166395
 688527/1000000: episode: 6886, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 57.109, mean reward: 0.571 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.472, 10.098], loss: 0.001531, mae: 0.042359, mean_q: 1.168888
 688627/1000000: episode: 6887, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 57.069, mean reward: 0.571 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.990, 10.103], loss: 0.001339, mae: 0.040039, mean_q: 1.162173
 688727/1000000: episode: 6888, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 57.811, mean reward: 0.578 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.087, 10.102], loss: 0.001445, mae: 0.041467, mean_q: 1.167391
 688827/1000000: episode: 6889, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.580, mean reward: 0.576 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.098], loss: 0.001369, mae: 0.040252, mean_q: 1.168593
 688927/1000000: episode: 6890, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 57.166, mean reward: 0.572 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.614, 10.124], loss: 0.001463, mae: 0.041620, mean_q: 1.165215
 689027/1000000: episode: 6891, duration: 1.390s, episode steps: 100, steps per second: 72, episode reward: 59.104, mean reward: 0.591 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.098], loss: 0.001415, mae: 0.040714, mean_q: 1.162657
 689127/1000000: episode: 6892, duration: 1.538s, episode steps: 100, steps per second: 65, episode reward: 56.822, mean reward: 0.568 [0.506, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.220, 10.222], loss: 0.001444, mae: 0.041392, mean_q: 1.164178
 689227/1000000: episode: 6893, duration: 1.394s, episode steps: 100, steps per second: 72, episode reward: 59.020, mean reward: 0.590 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.313, 10.316], loss: 0.001357, mae: 0.040596, mean_q: 1.163203
 689327/1000000: episode: 6894, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 58.138, mean reward: 0.581 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.734, 10.098], loss: 0.001378, mae: 0.039744, mean_q: 1.159071
 689427/1000000: episode: 6895, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 57.616, mean reward: 0.576 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.438, 10.182], loss: 0.001404, mae: 0.040863, mean_q: 1.164460
 689527/1000000: episode: 6896, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 60.391, mean reward: 0.604 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.599, 10.098], loss: 0.001433, mae: 0.040917, mean_q: 1.167329
 689627/1000000: episode: 6897, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: 57.881, mean reward: 0.579 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.300, 10.098], loss: 0.001402, mae: 0.040656, mean_q: 1.167349
 689727/1000000: episode: 6898, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 57.675, mean reward: 0.577 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.812, 10.098], loss: 0.001369, mae: 0.040498, mean_q: 1.165662
 689827/1000000: episode: 6899, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 59.456, mean reward: 0.595 [0.512, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.375, 10.354], loss: 0.001398, mae: 0.039975, mean_q: 1.161372
 689927/1000000: episode: 6900, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.826, mean reward: 0.588 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.951, 10.255], loss: 0.001440, mae: 0.041287, mean_q: 1.164041
 690027/1000000: episode: 6901, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 58.658, mean reward: 0.587 [0.509, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.098], loss: 0.001409, mae: 0.040798, mean_q: 1.166396
 690127/1000000: episode: 6902, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 59.347, mean reward: 0.593 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.987, 10.190], loss: 0.001501, mae: 0.041996, mean_q: 1.166471
 690227/1000000: episode: 6903, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 58.580, mean reward: 0.586 [0.507, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.816, 10.148], loss: 0.001382, mae: 0.040109, mean_q: 1.161034
 690327/1000000: episode: 6904, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 60.397, mean reward: 0.604 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.983, 10.098], loss: 0.001467, mae: 0.042032, mean_q: 1.161500
 690427/1000000: episode: 6905, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 58.602, mean reward: 0.586 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.469, 10.354], loss: 0.001449, mae: 0.041284, mean_q: 1.165181
 690527/1000000: episode: 6906, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 58.394, mean reward: 0.584 [0.499, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.588, 10.098], loss: 0.001527, mae: 0.042456, mean_q: 1.167889
 690627/1000000: episode: 6907, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 58.559, mean reward: 0.586 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.186, 10.328], loss: 0.001388, mae: 0.040840, mean_q: 1.163673
 690727/1000000: episode: 6908, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 59.732, mean reward: 0.597 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.905, 10.347], loss: 0.001383, mae: 0.040428, mean_q: 1.162909
 690827/1000000: episode: 6909, duration: 1.497s, episode steps: 100, steps per second: 67, episode reward: 58.912, mean reward: 0.589 [0.513, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.801, 10.147], loss: 0.001458, mae: 0.041300, mean_q: 1.165264
 690927/1000000: episode: 6910, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 61.401, mean reward: 0.614 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.190, 10.098], loss: 0.001496, mae: 0.041926, mean_q: 1.169113
 691027/1000000: episode: 6911, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 59.500, mean reward: 0.595 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.491, 10.098], loss: 0.001540, mae: 0.042659, mean_q: 1.169453
 691127/1000000: episode: 6912, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.464, mean reward: 0.585 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.487, 10.243], loss: 0.001473, mae: 0.041792, mean_q: 1.167893
 691227/1000000: episode: 6913, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 58.503, mean reward: 0.585 [0.508, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.901, 10.098], loss: 0.001392, mae: 0.040872, mean_q: 1.166783
 691327/1000000: episode: 6914, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 59.023, mean reward: 0.590 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.545, 10.304], loss: 0.001411, mae: 0.040121, mean_q: 1.164197
 691427/1000000: episode: 6915, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.642, mean reward: 0.576 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.458, 10.098], loss: 0.001385, mae: 0.039991, mean_q: 1.162046
 691527/1000000: episode: 6916, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.916, mean reward: 0.589 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.577, 10.098], loss: 0.001392, mae: 0.040497, mean_q: 1.164188
 691627/1000000: episode: 6917, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 58.313, mean reward: 0.583 [0.498, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.236, 10.277], loss: 0.001550, mae: 0.042251, mean_q: 1.167165
 691727/1000000: episode: 6918, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.902, mean reward: 0.589 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.484, 10.286], loss: 0.001410, mae: 0.040680, mean_q: 1.166720
 691827/1000000: episode: 6919, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.610, mean reward: 0.596 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.617, 10.435], loss: 0.001538, mae: 0.042370, mean_q: 1.164050
 691927/1000000: episode: 6920, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.938, mean reward: 0.589 [0.511, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.515, 10.143], loss: 0.001471, mae: 0.041736, mean_q: 1.164125
 692027/1000000: episode: 6921, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 59.344, mean reward: 0.593 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.338, 10.098], loss: 0.001527, mae: 0.042104, mean_q: 1.164575
 692127/1000000: episode: 6922, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 60.395, mean reward: 0.604 [0.513, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.347, 10.115], loss: 0.001486, mae: 0.041518, mean_q: 1.163609
 692227/1000000: episode: 6923, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 59.138, mean reward: 0.591 [0.513, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.775, 10.098], loss: 0.001565, mae: 0.042059, mean_q: 1.164913
 692327/1000000: episode: 6924, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.038, mean reward: 0.580 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.963, 10.218], loss: 0.001396, mae: 0.040796, mean_q: 1.163747
 692427/1000000: episode: 6925, duration: 1.340s, episode steps: 100, steps per second: 75, episode reward: 58.081, mean reward: 0.581 [0.514, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.039, 10.098], loss: 0.001421, mae: 0.041036, mean_q: 1.162898
 692527/1000000: episode: 6926, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: 58.807, mean reward: 0.588 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.686, 10.098], loss: 0.001441, mae: 0.040617, mean_q: 1.159848
 692627/1000000: episode: 6927, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 59.398, mean reward: 0.594 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.853, 10.281], loss: 0.001481, mae: 0.041855, mean_q: 1.165890
 692727/1000000: episode: 6928, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 57.446, mean reward: 0.574 [0.515, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.001429, mae: 0.041069, mean_q: 1.161602
 692827/1000000: episode: 6929, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 59.734, mean reward: 0.597 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.098], loss: 0.001374, mae: 0.040672, mean_q: 1.161988
 692927/1000000: episode: 6930, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 59.173, mean reward: 0.592 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.735, 10.098], loss: 0.001471, mae: 0.041453, mean_q: 1.162673
 693027/1000000: episode: 6931, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 60.659, mean reward: 0.607 [0.511, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.098], loss: 0.001403, mae: 0.040732, mean_q: 1.161005
 693127/1000000: episode: 6932, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 58.863, mean reward: 0.589 [0.514, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.372, 10.098], loss: 0.001337, mae: 0.039590, mean_q: 1.163392
 693227/1000000: episode: 6933, duration: 0.915s, episode steps: 100, steps per second: 109, episode reward: 58.808, mean reward: 0.588 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.695, 10.098], loss: 0.001494, mae: 0.042345, mean_q: 1.165937
 693327/1000000: episode: 6934, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 61.915, mean reward: 0.619 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.330], loss: 0.001374, mae: 0.040432, mean_q: 1.160620
 693427/1000000: episode: 6935, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 58.243, mean reward: 0.582 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.795, 10.116], loss: 0.001455, mae: 0.041476, mean_q: 1.166703
 693527/1000000: episode: 6936, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 60.576, mean reward: 0.606 [0.508, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.362, 10.098], loss: 0.001424, mae: 0.040608, mean_q: 1.165575
 693627/1000000: episode: 6937, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.839, mean reward: 0.588 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.732, 10.098], loss: 0.001486, mae: 0.042026, mean_q: 1.168533
 693727/1000000: episode: 6938, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 63.908, mean reward: 0.639 [0.511, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.688, 10.535], loss: 0.001464, mae: 0.041638, mean_q: 1.167550
 693827/1000000: episode: 6939, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 59.921, mean reward: 0.599 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.803, 10.098], loss: 0.001371, mae: 0.040739, mean_q: 1.168095
 693927/1000000: episode: 6940, duration: 1.507s, episode steps: 100, steps per second: 66, episode reward: 60.458, mean reward: 0.605 [0.506, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.001372, mae: 0.039902, mean_q: 1.168418
 694027/1000000: episode: 6941, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 59.312, mean reward: 0.593 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.273, 10.098], loss: 0.001325, mae: 0.039427, mean_q: 1.171241
 694127/1000000: episode: 6942, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 57.439, mean reward: 0.574 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.400, 10.325], loss: 0.001349, mae: 0.039690, mean_q: 1.168495
 694227/1000000: episode: 6943, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.240, mean reward: 0.592 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.723, 10.100], loss: 0.001382, mae: 0.040277, mean_q: 1.166977
 694327/1000000: episode: 6944, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 58.927, mean reward: 0.589 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.184], loss: 0.001340, mae: 0.039664, mean_q: 1.167898
 694427/1000000: episode: 6945, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 59.343, mean reward: 0.593 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.612, 10.098], loss: 0.001436, mae: 0.041143, mean_q: 1.171590
 694527/1000000: episode: 6946, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 59.945, mean reward: 0.599 [0.498, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.094, 10.197], loss: 0.001337, mae: 0.040052, mean_q: 1.169045
 694627/1000000: episode: 6947, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.770, mean reward: 0.588 [0.497, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.667, 10.337], loss: 0.001346, mae: 0.039594, mean_q: 1.173617
 694727/1000000: episode: 6948, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 61.611, mean reward: 0.616 [0.510, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.954, 10.098], loss: 0.001447, mae: 0.041101, mean_q: 1.166321
 694827/1000000: episode: 6949, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 59.265, mean reward: 0.593 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.539, 10.315], loss: 0.001462, mae: 0.041088, mean_q: 1.173973
 694927/1000000: episode: 6950, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.013, mean reward: 0.590 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.504, 10.131], loss: 0.001389, mae: 0.040433, mean_q: 1.170967
 695027/1000000: episode: 6951, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 59.786, mean reward: 0.598 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.568, 10.098], loss: 0.001412, mae: 0.041044, mean_q: 1.173546
 695127/1000000: episode: 6952, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 58.260, mean reward: 0.583 [0.501, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.824, 10.098], loss: 0.001358, mae: 0.039656, mean_q: 1.170847
 695227/1000000: episode: 6953, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 59.960, mean reward: 0.600 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.121, 10.302], loss: 0.001368, mae: 0.040652, mean_q: 1.173046
 695327/1000000: episode: 6954, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 59.726, mean reward: 0.597 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.098], loss: 0.001495, mae: 0.041681, mean_q: 1.176579
 695427/1000000: episode: 6955, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 60.559, mean reward: 0.606 [0.514, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.525, 10.247], loss: 0.001351, mae: 0.040136, mean_q: 1.172792
 695527/1000000: episode: 6956, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 59.661, mean reward: 0.597 [0.519, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.883, 10.098], loss: 0.001425, mae: 0.040849, mean_q: 1.172213
 695627/1000000: episode: 6957, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 57.891, mean reward: 0.579 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.061, 10.275], loss: 0.001363, mae: 0.039992, mean_q: 1.174268
 695727/1000000: episode: 6958, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 60.511, mean reward: 0.605 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.256], loss: 0.001398, mae: 0.040965, mean_q: 1.176335
 695827/1000000: episode: 6959, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 63.521, mean reward: 0.635 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.395, 10.339], loss: 0.001383, mae: 0.040872, mean_q: 1.174913
 695927/1000000: episode: 6960, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.293, mean reward: 0.593 [0.511, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.692, 10.098], loss: 0.001328, mae: 0.039918, mean_q: 1.176349
 696027/1000000: episode: 6961, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 63.587, mean reward: 0.636 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.089, 10.311], loss: 0.001399, mae: 0.040858, mean_q: 1.178350
 696127/1000000: episode: 6962, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 56.561, mean reward: 0.566 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.170], loss: 0.001456, mae: 0.041452, mean_q: 1.179775
 696227/1000000: episode: 6963, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 57.504, mean reward: 0.575 [0.503, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.645, 10.335], loss: 0.001450, mae: 0.041130, mean_q: 1.175659
 696327/1000000: episode: 6964, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.184, mean reward: 0.582 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.098], loss: 0.001319, mae: 0.039867, mean_q: 1.175496
 696427/1000000: episode: 6965, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.750, mean reward: 0.587 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.606, 10.098], loss: 0.001471, mae: 0.042102, mean_q: 1.177022
 696527/1000000: episode: 6966, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 59.938, mean reward: 0.599 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.345, 10.098], loss: 0.001342, mae: 0.039570, mean_q: 1.174882
 696627/1000000: episode: 6967, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 57.709, mean reward: 0.577 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.250, 10.098], loss: 0.001374, mae: 0.040807, mean_q: 1.177400
 696727/1000000: episode: 6968, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 56.934, mean reward: 0.569 [0.497, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.765, 10.207], loss: 0.001395, mae: 0.040804, mean_q: 1.173325
 696827/1000000: episode: 6969, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 58.756, mean reward: 0.588 [0.516, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.552, 10.249], loss: 0.001398, mae: 0.040991, mean_q: 1.177699
 696927/1000000: episode: 6970, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.068, mean reward: 0.591 [0.506, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.239], loss: 0.001390, mae: 0.039992, mean_q: 1.178391
 697027/1000000: episode: 6971, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 61.660, mean reward: 0.617 [0.525, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.569, 10.308], loss: 0.001335, mae: 0.039848, mean_q: 1.176569
 697127/1000000: episode: 6972, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 57.337, mean reward: 0.573 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.435, 10.098], loss: 0.001400, mae: 0.040607, mean_q: 1.174984
 697227/1000000: episode: 6973, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 57.751, mean reward: 0.578 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.586, 10.098], loss: 0.001349, mae: 0.040332, mean_q: 1.173696
 697327/1000000: episode: 6974, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.554, mean reward: 0.586 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.083, 10.164], loss: 0.001401, mae: 0.040808, mean_q: 1.175592
 697427/1000000: episode: 6975, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 57.838, mean reward: 0.578 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.564, 10.098], loss: 0.001346, mae: 0.039966, mean_q: 1.173357
 697527/1000000: episode: 6976, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 56.445, mean reward: 0.564 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.546, 10.098], loss: 0.001335, mae: 0.040093, mean_q: 1.174352
 697627/1000000: episode: 6977, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.947, mean reward: 0.589 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.311, 10.098], loss: 0.001351, mae: 0.039729, mean_q: 1.173180
 697727/1000000: episode: 6978, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 58.253, mean reward: 0.583 [0.501, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.720, 10.129], loss: 0.001543, mae: 0.042010, mean_q: 1.175238
 697827/1000000: episode: 6979, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 57.023, mean reward: 0.570 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.251, 10.098], loss: 0.001510, mae: 0.042300, mean_q: 1.177320
 697927/1000000: episode: 6980, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 57.836, mean reward: 0.578 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.155], loss: 0.001425, mae: 0.041120, mean_q: 1.172727
 698027/1000000: episode: 6981, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 56.059, mean reward: 0.561 [0.503, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.098], loss: 0.001416, mae: 0.040990, mean_q: 1.170226
 698127/1000000: episode: 6982, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.145, mean reward: 0.571 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.641, 10.245], loss: 0.001343, mae: 0.039759, mean_q: 1.170781
 698227/1000000: episode: 6983, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 58.215, mean reward: 0.582 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.136, 10.311], loss: 0.001452, mae: 0.041446, mean_q: 1.173203
 698327/1000000: episode: 6984, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 59.016, mean reward: 0.590 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.570, 10.129], loss: 0.001453, mae: 0.041163, mean_q: 1.170778
 698427/1000000: episode: 6985, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 60.155, mean reward: 0.602 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.098], loss: 0.001400, mae: 0.040344, mean_q: 1.167026
 698527/1000000: episode: 6986, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 61.838, mean reward: 0.618 [0.515, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.844, 10.098], loss: 0.001373, mae: 0.040199, mean_q: 1.164095
 698627/1000000: episode: 6987, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 58.058, mean reward: 0.581 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.587, 10.311], loss: 0.001468, mae: 0.041701, mean_q: 1.171215
 698727/1000000: episode: 6988, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.259, mean reward: 0.583 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.143], loss: 0.001457, mae: 0.041199, mean_q: 1.168905
 698827/1000000: episode: 6989, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.617, mean reward: 0.576 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.140, 10.098], loss: 0.001449, mae: 0.041319, mean_q: 1.168949
 698927/1000000: episode: 6990, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 56.256, mean reward: 0.563 [0.504, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.344, 10.098], loss: 0.001361, mae: 0.039959, mean_q: 1.165816
 699027/1000000: episode: 6991, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 58.630, mean reward: 0.586 [0.513, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.579, 10.272], loss: 0.001334, mae: 0.039954, mean_q: 1.165590
 699127/1000000: episode: 6992, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.449, mean reward: 0.584 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.723, 10.098], loss: 0.001472, mae: 0.041407, mean_q: 1.165977
 699227/1000000: episode: 6993, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 58.388, mean reward: 0.584 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.589, 10.098], loss: 0.001389, mae: 0.040587, mean_q: 1.166887
 699327/1000000: episode: 6994, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 63.042, mean reward: 0.630 [0.504, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.138, 10.098], loss: 0.001390, mae: 0.040291, mean_q: 1.168615
 699427/1000000: episode: 6995, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 56.358, mean reward: 0.564 [0.499, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.429, 10.098], loss: 0.001368, mae: 0.040032, mean_q: 1.165564
 699527/1000000: episode: 6996, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 58.297, mean reward: 0.583 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.098], loss: 0.001465, mae: 0.041500, mean_q: 1.164724
 699627/1000000: episode: 6997, duration: 1.366s, episode steps: 100, steps per second: 73, episode reward: 57.909, mean reward: 0.579 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.146], loss: 0.001420, mae: 0.040824, mean_q: 1.167376
 699727/1000000: episode: 6998, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 59.067, mean reward: 0.591 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.162, 10.272], loss: 0.001478, mae: 0.041449, mean_q: 1.163611
 699827/1000000: episode: 6999, duration: 1.484s, episode steps: 100, steps per second: 67, episode reward: 59.413, mean reward: 0.594 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: 0.001310, mae: 0.039619, mean_q: 1.165113
 699927/1000000: episode: 7000, duration: 1.573s, episode steps: 100, steps per second: 64, episode reward: 58.655, mean reward: 0.587 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.146], loss: 0.001487, mae: 0.041430, mean_q: 1.163442
 700027/1000000: episode: 7001, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 57.518, mean reward: 0.575 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.098], loss: 0.001403, mae: 0.040738, mean_q: 1.161741
 700127/1000000: episode: 7002, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 56.938, mean reward: 0.569 [0.501, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.367, 10.098], loss: 0.001362, mae: 0.040074, mean_q: 1.163990
 700227/1000000: episode: 7003, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 60.877, mean reward: 0.609 [0.524, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.706, 10.098], loss: 0.001324, mae: 0.039191, mean_q: 1.158601
 700327/1000000: episode: 7004, duration: 1.453s, episode steps: 100, steps per second: 69, episode reward: 58.085, mean reward: 0.581 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.174, 10.121], loss: 0.001369, mae: 0.040163, mean_q: 1.161219
 700427/1000000: episode: 7005, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 59.659, mean reward: 0.597 [0.515, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.465, 10.218], loss: 0.001385, mae: 0.040208, mean_q: 1.162740
 700527/1000000: episode: 7006, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 60.392, mean reward: 0.604 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.413, 10.320], loss: 0.001326, mae: 0.039703, mean_q: 1.160335
 700627/1000000: episode: 7007, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.678, mean reward: 0.577 [0.500, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.059, 10.179], loss: 0.001349, mae: 0.039413, mean_q: 1.162234
 700727/1000000: episode: 7008, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 59.902, mean reward: 0.599 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.475, 10.098], loss: 0.001361, mae: 0.039820, mean_q: 1.158641
 700827/1000000: episode: 7009, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 60.215, mean reward: 0.602 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.256, 10.283], loss: 0.001367, mae: 0.039759, mean_q: 1.159856
 700927/1000000: episode: 7010, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 57.380, mean reward: 0.574 [0.508, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.398], loss: 0.001312, mae: 0.039358, mean_q: 1.156730
 701027/1000000: episode: 7011, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 58.138, mean reward: 0.581 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.879, 10.098], loss: 0.001307, mae: 0.038982, mean_q: 1.158281
 701127/1000000: episode: 7012, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 60.127, mean reward: 0.601 [0.499, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.098], loss: 0.001343, mae: 0.039173, mean_q: 1.157421
 701227/1000000: episode: 7013, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 61.280, mean reward: 0.613 [0.524, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.175, 10.323], loss: 0.001385, mae: 0.039983, mean_q: 1.159065
 701327/1000000: episode: 7014, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.230, mean reward: 0.592 [0.510, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.001314, mae: 0.039700, mean_q: 1.161652
 701427/1000000: episode: 7015, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.922, mean reward: 0.579 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.315, 10.258], loss: 0.001328, mae: 0.039983, mean_q: 1.162668
 701527/1000000: episode: 7016, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.056, mean reward: 0.581 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.096, 10.098], loss: 0.001316, mae: 0.039839, mean_q: 1.159069
 701627/1000000: episode: 7017, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.724, mean reward: 0.577 [0.504, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.271], loss: 0.001387, mae: 0.040475, mean_q: 1.158326
 701727/1000000: episode: 7018, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 57.992, mean reward: 0.580 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.254, 10.098], loss: 0.001298, mae: 0.039277, mean_q: 1.157951
 701827/1000000: episode: 7019, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.689, mean reward: 0.587 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.055, 10.098], loss: 0.001409, mae: 0.041296, mean_q: 1.159209
 701927/1000000: episode: 7020, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 58.555, mean reward: 0.586 [0.510, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.398, 10.117], loss: 0.001363, mae: 0.040253, mean_q: 1.160329
 702027/1000000: episode: 7021, duration: 0.963s, episode steps: 100, steps per second: 104, episode reward: 61.537, mean reward: 0.615 [0.501, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.832, 10.098], loss: 0.001393, mae: 0.040489, mean_q: 1.159396
 702127/1000000: episode: 7022, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 59.796, mean reward: 0.598 [0.511, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.432, 10.098], loss: 0.001412, mae: 0.041324, mean_q: 1.160994
 702227/1000000: episode: 7023, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 62.488, mean reward: 0.625 [0.529, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.570, 10.399], loss: 0.001324, mae: 0.039388, mean_q: 1.159281
 702327/1000000: episode: 7024, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 57.122, mean reward: 0.571 [0.501, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.142, 10.098], loss: 0.001293, mae: 0.038945, mean_q: 1.160714
 702427/1000000: episode: 7025, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 59.763, mean reward: 0.598 [0.510, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.098], loss: 0.001328, mae: 0.039864, mean_q: 1.163041
 702527/1000000: episode: 7026, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 60.097, mean reward: 0.601 [0.508, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.029, 10.150], loss: 0.001417, mae: 0.040481, mean_q: 1.163359
 702627/1000000: episode: 7027, duration: 1.832s, episode steps: 100, steps per second: 55, episode reward: 58.602, mean reward: 0.586 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.344, 10.174], loss: 0.001321, mae: 0.039575, mean_q: 1.159669
 702727/1000000: episode: 7028, duration: 1.467s, episode steps: 100, steps per second: 68, episode reward: 58.618, mean reward: 0.586 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.098], loss: 0.001426, mae: 0.040851, mean_q: 1.162936
 702827/1000000: episode: 7029, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: 59.805, mean reward: 0.598 [0.498, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.951, 10.395], loss: 0.001358, mae: 0.039926, mean_q: 1.163601
 702927/1000000: episode: 7030, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.966, mean reward: 0.590 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.195, 10.108], loss: 0.001248, mae: 0.038775, mean_q: 1.164448
 703027/1000000: episode: 7031, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.049, mean reward: 0.590 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.098], loss: 0.001380, mae: 0.040425, mean_q: 1.167756
 703127/1000000: episode: 7032, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 57.542, mean reward: 0.575 [0.499, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.650, 10.137], loss: 0.001308, mae: 0.039474, mean_q: 1.166533
 703227/1000000: episode: 7033, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 57.880, mean reward: 0.579 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.181, 10.369], loss: 0.001346, mae: 0.040084, mean_q: 1.168674
 703327/1000000: episode: 7034, duration: 1.619s, episode steps: 100, steps per second: 62, episode reward: 60.294, mean reward: 0.603 [0.507, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.364, 10.098], loss: 0.001380, mae: 0.040459, mean_q: 1.168183
 703427/1000000: episode: 7035, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 58.704, mean reward: 0.587 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.361, 10.125], loss: 0.001346, mae: 0.040225, mean_q: 1.166498
 703527/1000000: episode: 7036, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 57.699, mean reward: 0.577 [0.512, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.828, 10.098], loss: 0.001429, mae: 0.040892, mean_q: 1.165572
 703627/1000000: episode: 7037, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 56.368, mean reward: 0.564 [0.498, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.338, 10.170], loss: 0.001324, mae: 0.039312, mean_q: 1.166204
 703727/1000000: episode: 7038, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 63.500, mean reward: 0.635 [0.503, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.432, 10.302], loss: 0.001376, mae: 0.040416, mean_q: 1.166331
 703827/1000000: episode: 7039, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 57.945, mean reward: 0.579 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.150, 10.249], loss: 0.001431, mae: 0.040877, mean_q: 1.165682
 703927/1000000: episode: 7040, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 60.887, mean reward: 0.609 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.869, 10.098], loss: 0.001392, mae: 0.040554, mean_q: 1.169316
 704027/1000000: episode: 7041, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 60.445, mean reward: 0.604 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.595, 10.098], loss: 0.001345, mae: 0.039882, mean_q: 1.168025
 704127/1000000: episode: 7042, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 58.639, mean reward: 0.586 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.625, 10.098], loss: 0.001358, mae: 0.040663, mean_q: 1.169223
 704227/1000000: episode: 7043, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 62.627, mean reward: 0.626 [0.509, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.839, 10.319], loss: 0.001336, mae: 0.040202, mean_q: 1.168278
 704327/1000000: episode: 7044, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 57.951, mean reward: 0.580 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.109], loss: 0.001357, mae: 0.039725, mean_q: 1.167934
 704427/1000000: episode: 7045, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.116, mean reward: 0.591 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.256, 10.098], loss: 0.001248, mae: 0.038422, mean_q: 1.164805
 704527/1000000: episode: 7046, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 57.921, mean reward: 0.579 [0.503, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.737, 10.239], loss: 0.001370, mae: 0.040359, mean_q: 1.167850
 704627/1000000: episode: 7047, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 61.135, mean reward: 0.611 [0.514, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.983, 10.098], loss: 0.001295, mae: 0.039205, mean_q: 1.167065
 704727/1000000: episode: 7048, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.638, mean reward: 0.586 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.583, 10.098], loss: 0.001317, mae: 0.039229, mean_q: 1.171439
 704827/1000000: episode: 7049, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 65.620, mean reward: 0.656 [0.512, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.155, 10.386], loss: 0.001453, mae: 0.040855, mean_q: 1.173171
 704927/1000000: episode: 7050, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 56.936, mean reward: 0.569 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.952, 10.200], loss: 0.001380, mae: 0.040305, mean_q: 1.171687
 705027/1000000: episode: 7051, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 58.823, mean reward: 0.588 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.785, 10.098], loss: 0.001312, mae: 0.039723, mean_q: 1.173085
 705127/1000000: episode: 7052, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 58.024, mean reward: 0.580 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.595, 10.098], loss: 0.001379, mae: 0.040768, mean_q: 1.173871
 705227/1000000: episode: 7053, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 57.799, mean reward: 0.578 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.411, 10.226], loss: 0.001369, mae: 0.040547, mean_q: 1.170488
 705327/1000000: episode: 7054, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.667, mean reward: 0.597 [0.513, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.595, 10.265], loss: 0.001351, mae: 0.040505, mean_q: 1.173468
 705427/1000000: episode: 7055, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 57.899, mean reward: 0.579 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.169, 10.098], loss: 0.001372, mae: 0.040760, mean_q: 1.174204
 705527/1000000: episode: 7056, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 58.568, mean reward: 0.586 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.171, 10.098], loss: 0.001493, mae: 0.042279, mean_q: 1.174502
 705627/1000000: episode: 7057, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.198, mean reward: 0.582 [0.511, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.260, 10.098], loss: 0.001470, mae: 0.041665, mean_q: 1.173496
 705727/1000000: episode: 7058, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 59.940, mean reward: 0.599 [0.508, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.804, 10.098], loss: 0.001506, mae: 0.042539, mean_q: 1.170718
 705827/1000000: episode: 7059, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 66.651, mean reward: 0.667 [0.504, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.478, 10.098], loss: 0.001432, mae: 0.041428, mean_q: 1.170785
 705927/1000000: episode: 7060, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 58.540, mean reward: 0.585 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.164], loss: 0.001538, mae: 0.043142, mean_q: 1.178818
 706027/1000000: episode: 7061, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 60.255, mean reward: 0.603 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.700, 10.171], loss: 0.001451, mae: 0.041738, mean_q: 1.172969
 706127/1000000: episode: 7062, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.716, mean reward: 0.577 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.736, 10.098], loss: 0.001467, mae: 0.041798, mean_q: 1.174707
 706227/1000000: episode: 7063, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 56.516, mean reward: 0.565 [0.510, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.341, 10.098], loss: 0.001504, mae: 0.042026, mean_q: 1.178650
 706327/1000000: episode: 7064, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 57.957, mean reward: 0.580 [0.510, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.947, 10.098], loss: 0.001514, mae: 0.042508, mean_q: 1.175928
 706427/1000000: episode: 7065, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 65.067, mean reward: 0.651 [0.503, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.098], loss: 0.001448, mae: 0.041395, mean_q: 1.178388
 706527/1000000: episode: 7066, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 60.108, mean reward: 0.601 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.120, 10.216], loss: 0.001510, mae: 0.042372, mean_q: 1.174765
 706627/1000000: episode: 7067, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 56.849, mean reward: 0.568 [0.500, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.728, 10.107], loss: 0.001356, mae: 0.040510, mean_q: 1.176550
 706727/1000000: episode: 7068, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 58.289, mean reward: 0.583 [0.511, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.098], loss: 0.001455, mae: 0.041057, mean_q: 1.176515
 706827/1000000: episode: 7069, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 61.541, mean reward: 0.615 [0.519, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.464, 10.322], loss: 0.001377, mae: 0.040440, mean_q: 1.172668
 706927/1000000: episode: 7070, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 60.262, mean reward: 0.603 [0.504, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.407], loss: 0.001577, mae: 0.042912, mean_q: 1.176932
 707027/1000000: episode: 7071, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.086, mean reward: 0.581 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.321, 10.190], loss: 0.001459, mae: 0.041404, mean_q: 1.176213
 707127/1000000: episode: 7072, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 58.956, mean reward: 0.590 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.626, 10.098], loss: 0.001427, mae: 0.040974, mean_q: 1.177496
 707227/1000000: episode: 7073, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 59.407, mean reward: 0.594 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.440, 10.098], loss: 0.001547, mae: 0.042895, mean_q: 1.173539
 707327/1000000: episode: 7074, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 61.780, mean reward: 0.618 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.570, 10.098], loss: 0.001512, mae: 0.042122, mean_q: 1.173246
 707427/1000000: episode: 7075, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 58.264, mean reward: 0.583 [0.513, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.224, 10.252], loss: 0.001400, mae: 0.040995, mean_q: 1.174206
 707527/1000000: episode: 7076, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 59.261, mean reward: 0.593 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.126], loss: 0.001477, mae: 0.041673, mean_q: 1.173957
 707627/1000000: episode: 7077, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 58.402, mean reward: 0.584 [0.498, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.403, 10.098], loss: 0.001495, mae: 0.041694, mean_q: 1.174968
 707727/1000000: episode: 7078, duration: 1.279s, episode steps: 100, steps per second: 78, episode reward: 60.392, mean reward: 0.604 [0.527, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.156], loss: 0.001522, mae: 0.041804, mean_q: 1.176816
 707827/1000000: episode: 7079, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 59.023, mean reward: 0.590 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.098], loss: 0.001450, mae: 0.041543, mean_q: 1.179126
 707927/1000000: episode: 7080, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 57.080, mean reward: 0.571 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.034, 10.100], loss: 0.001591, mae: 0.042854, mean_q: 1.178403
 708027/1000000: episode: 7081, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.796, mean reward: 0.578 [0.514, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.220, 10.131], loss: 0.001467, mae: 0.041747, mean_q: 1.177062
 708127/1000000: episode: 7082, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 66.088, mean reward: 0.661 [0.507, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.961, 10.457], loss: 0.001467, mae: 0.041275, mean_q: 1.176480
 708227/1000000: episode: 7083, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.782, mean reward: 0.578 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.207], loss: 0.001468, mae: 0.041235, mean_q: 1.175238
 708327/1000000: episode: 7084, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 57.725, mean reward: 0.577 [0.499, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.284, 10.098], loss: 0.001478, mae: 0.041980, mean_q: 1.177824
 708427/1000000: episode: 7085, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.270, mean reward: 0.583 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.124], loss: 0.001481, mae: 0.041799, mean_q: 1.178717
 708527/1000000: episode: 7086, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 58.821, mean reward: 0.588 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.336], loss: 0.001461, mae: 0.041877, mean_q: 1.176879
 708627/1000000: episode: 7087, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 60.945, mean reward: 0.609 [0.512, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.412], loss: 0.001500, mae: 0.042222, mean_q: 1.180940
 708727/1000000: episode: 7088, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 58.880, mean reward: 0.589 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.837, 10.217], loss: 0.001482, mae: 0.041747, mean_q: 1.175058
 708827/1000000: episode: 7089, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 61.819, mean reward: 0.618 [0.518, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.665, 10.098], loss: 0.001484, mae: 0.041894, mean_q: 1.177155
 708927/1000000: episode: 7090, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 58.597, mean reward: 0.586 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.491, 10.278], loss: 0.001545, mae: 0.042533, mean_q: 1.175451
 709027/1000000: episode: 7091, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 59.008, mean reward: 0.590 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.130], loss: 0.001541, mae: 0.042646, mean_q: 1.177181
 709127/1000000: episode: 7092, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 58.167, mean reward: 0.582 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.367, 10.290], loss: 0.001449, mae: 0.041769, mean_q: 1.174485
 709227/1000000: episode: 7093, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 57.674, mean reward: 0.577 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.047, 10.221], loss: 0.001423, mae: 0.040891, mean_q: 1.173114
 709327/1000000: episode: 7094, duration: 1.311s, episode steps: 100, steps per second: 76, episode reward: 60.193, mean reward: 0.602 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.670, 10.407], loss: 0.001544, mae: 0.042967, mean_q: 1.175483
 709427/1000000: episode: 7095, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 57.262, mean reward: 0.573 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.219, 10.098], loss: 0.001374, mae: 0.040453, mean_q: 1.173894
 709527/1000000: episode: 7096, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 57.056, mean reward: 0.571 [0.508, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.774, 10.098], loss: 0.001335, mae: 0.040166, mean_q: 1.172359
 709627/1000000: episode: 7097, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 57.929, mean reward: 0.579 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.962, 10.098], loss: 0.001342, mae: 0.039571, mean_q: 1.173577
 709727/1000000: episode: 7098, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.211, mean reward: 0.582 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.098], loss: 0.001409, mae: 0.041045, mean_q: 1.174041
 709827/1000000: episode: 7099, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.636, mean reward: 0.596 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.151], loss: 0.001413, mae: 0.041068, mean_q: 1.168803
 709927/1000000: episode: 7100, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.567, mean reward: 0.586 [0.498, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.841, 10.098], loss: 0.001409, mae: 0.040422, mean_q: 1.171463
 710027/1000000: episode: 7101, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 59.282, mean reward: 0.593 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.635, 10.098], loss: 0.001443, mae: 0.041202, mean_q: 1.170063
 710127/1000000: episode: 7102, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 59.920, mean reward: 0.599 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.185, 10.188], loss: 0.001407, mae: 0.040106, mean_q: 1.171617
 710227/1000000: episode: 7103, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 61.896, mean reward: 0.619 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.754, 10.098], loss: 0.001366, mae: 0.040096, mean_q: 1.171155
 710327/1000000: episode: 7104, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 58.858, mean reward: 0.589 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 0.001368, mae: 0.039882, mean_q: 1.175385
 710427/1000000: episode: 7105, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 58.822, mean reward: 0.588 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.541, 10.441], loss: 0.001465, mae: 0.041834, mean_q: 1.173478
 710527/1000000: episode: 7106, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 60.942, mean reward: 0.609 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.154, 10.098], loss: 0.001323, mae: 0.040063, mean_q: 1.177611
 710627/1000000: episode: 7107, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 57.195, mean reward: 0.572 [0.497, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.912, 10.098], loss: 0.001341, mae: 0.039882, mean_q: 1.172068
 710727/1000000: episode: 7108, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 58.061, mean reward: 0.581 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.233, 10.160], loss: 0.001419, mae: 0.040924, mean_q: 1.168936
 710827/1000000: episode: 7109, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 62.112, mean reward: 0.621 [0.500, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.520, 10.098], loss: 0.001419, mae: 0.041192, mean_q: 1.173277
 710927/1000000: episode: 7110, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 62.712, mean reward: 0.627 [0.507, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.425, 10.098], loss: 0.001321, mae: 0.039990, mean_q: 1.171582
 711027/1000000: episode: 7111, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.292, mean reward: 0.573 [0.509, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.881, 10.098], loss: 0.001455, mae: 0.041218, mean_q: 1.170732
 711127/1000000: episode: 7112, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 60.141, mean reward: 0.601 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.098], loss: 0.001442, mae: 0.041058, mean_q: 1.174592
 711227/1000000: episode: 7113, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 57.876, mean reward: 0.579 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.339, 10.353], loss: 0.001392, mae: 0.041081, mean_q: 1.172060
 711327/1000000: episode: 7114, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 62.477, mean reward: 0.625 [0.502, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.558, 10.386], loss: 0.001439, mae: 0.041147, mean_q: 1.172701
 711427/1000000: episode: 7115, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 58.208, mean reward: 0.582 [0.510, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.893, 10.274], loss: 0.001474, mae: 0.041435, mean_q: 1.174050
 711527/1000000: episode: 7116, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.294, mean reward: 0.573 [0.503, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.695, 10.101], loss: 0.001395, mae: 0.040830, mean_q: 1.170840
 711627/1000000: episode: 7117, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 57.236, mean reward: 0.572 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.098], loss: 0.001330, mae: 0.040066, mean_q: 1.171081
 711727/1000000: episode: 7118, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 60.775, mean reward: 0.608 [0.524, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.320, 10.098], loss: 0.001468, mae: 0.041866, mean_q: 1.175219
 711827/1000000: episode: 7119, duration: 1.293s, episode steps: 100, steps per second: 77, episode reward: 57.650, mean reward: 0.576 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.984, 10.180], loss: 0.001382, mae: 0.040767, mean_q: 1.172680
 711927/1000000: episode: 7120, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 58.701, mean reward: 0.587 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.350, 10.098], loss: 0.001444, mae: 0.041048, mean_q: 1.171906
 712027/1000000: episode: 7121, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 57.397, mean reward: 0.574 [0.498, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.047, 10.325], loss: 0.001415, mae: 0.041509, mean_q: 1.173868
 712127/1000000: episode: 7122, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 59.850, mean reward: 0.598 [0.518, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.157, 10.281], loss: 0.001475, mae: 0.042080, mean_q: 1.173910
 712227/1000000: episode: 7123, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 58.528, mean reward: 0.585 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.829, 10.098], loss: 0.001475, mae: 0.041439, mean_q: 1.173524
 712327/1000000: episode: 7124, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.135, mean reward: 0.581 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.496, 10.098], loss: 0.001538, mae: 0.042640, mean_q: 1.172925
 712427/1000000: episode: 7125, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 57.801, mean reward: 0.578 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.982, 10.098], loss: 0.001462, mae: 0.041591, mean_q: 1.167523
 712527/1000000: episode: 7126, duration: 1.383s, episode steps: 100, steps per second: 72, episode reward: 56.803, mean reward: 0.568 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.769, 10.098], loss: 0.001406, mae: 0.041293, mean_q: 1.169312
 712627/1000000: episode: 7127, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 57.987, mean reward: 0.580 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.350, 10.098], loss: 0.001382, mae: 0.040791, mean_q: 1.167265
 712727/1000000: episode: 7128, duration: 1.362s, episode steps: 100, steps per second: 73, episode reward: 58.146, mean reward: 0.581 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.057, 10.113], loss: 0.001453, mae: 0.041904, mean_q: 1.168690
 712827/1000000: episode: 7129, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 56.444, mean reward: 0.564 [0.505, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.501, 10.166], loss: 0.001390, mae: 0.040260, mean_q: 1.166856
 712927/1000000: episode: 7130, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 58.477, mean reward: 0.585 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.278], loss: 0.001419, mae: 0.040957, mean_q: 1.167800
 713027/1000000: episode: 7131, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 60.209, mean reward: 0.602 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.235, 10.098], loss: 0.001488, mae: 0.042453, mean_q: 1.167092
 713127/1000000: episode: 7132, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 60.511, mean reward: 0.605 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.640, 10.098], loss: 0.001509, mae: 0.042544, mean_q: 1.166959
 713227/1000000: episode: 7133, duration: 0.985s, episode steps: 100, steps per second: 101, episode reward: 59.612, mean reward: 0.596 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.098], loss: 0.001393, mae: 0.040765, mean_q: 1.164718
 713327/1000000: episode: 7134, duration: 1.402s, episode steps: 100, steps per second: 71, episode reward: 58.305, mean reward: 0.583 [0.504, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.280, 10.106], loss: 0.001500, mae: 0.042242, mean_q: 1.163848
 713427/1000000: episode: 7135, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 63.395, mean reward: 0.634 [0.510, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.428, 10.558], loss: 0.001386, mae: 0.040863, mean_q: 1.161595
 713527/1000000: episode: 7136, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.342, mean reward: 0.583 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-2.165, 10.098], loss: 0.001478, mae: 0.041923, mean_q: 1.169687
 713627/1000000: episode: 7137, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.403, mean reward: 0.594 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.613, 10.098], loss: 0.001453, mae: 0.040944, mean_q: 1.164291
 713727/1000000: episode: 7138, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 58.706, mean reward: 0.587 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.895, 10.134], loss: 0.001500, mae: 0.042477, mean_q: 1.171091
 713827/1000000: episode: 7139, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 57.885, mean reward: 0.579 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.104, 10.141], loss: 0.001454, mae: 0.041879, mean_q: 1.167143
 713927/1000000: episode: 7140, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.494, mean reward: 0.575 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.418, 10.098], loss: 0.001526, mae: 0.043086, mean_q: 1.166443
 714027/1000000: episode: 7141, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.952, mean reward: 0.590 [0.513, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.618, 10.098], loss: 0.001441, mae: 0.041057, mean_q: 1.159421
 714127/1000000: episode: 7142, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 59.338, mean reward: 0.593 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.135, 10.098], loss: 0.001530, mae: 0.042375, mean_q: 1.165096
 714227/1000000: episode: 7143, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 57.931, mean reward: 0.579 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.941, 10.102], loss: 0.001456, mae: 0.041732, mean_q: 1.162597
 714327/1000000: episode: 7144, duration: 0.943s, episode steps: 100, steps per second: 106, episode reward: 58.722, mean reward: 0.587 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.335, 10.248], loss: 0.001487, mae: 0.042381, mean_q: 1.166530
 714427/1000000: episode: 7145, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.278, mean reward: 0.593 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.094, 10.133], loss: 0.001501, mae: 0.042507, mean_q: 1.167651
 714527/1000000: episode: 7146, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 59.568, mean reward: 0.596 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.739, 10.098], loss: 0.001531, mae: 0.042721, mean_q: 1.170133
 714627/1000000: episode: 7147, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 58.014, mean reward: 0.580 [0.498, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.785, 10.098], loss: 0.001503, mae: 0.042236, mean_q: 1.168000
 714727/1000000: episode: 7148, duration: 1.536s, episode steps: 100, steps per second: 65, episode reward: 59.850, mean reward: 0.598 [0.513, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.136, 10.098], loss: 0.001512, mae: 0.041944, mean_q: 1.165589
 714827/1000000: episode: 7149, duration: 1.788s, episode steps: 100, steps per second: 56, episode reward: 57.723, mean reward: 0.577 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.831, 10.160], loss: 0.001534, mae: 0.042201, mean_q: 1.165642
 714927/1000000: episode: 7150, duration: 1.461s, episode steps: 100, steps per second: 68, episode reward: 56.642, mean reward: 0.566 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.962, 10.147], loss: 0.001431, mae: 0.041666, mean_q: 1.169970
 715027/1000000: episode: 7151, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 58.955, mean reward: 0.590 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.118, 10.369], loss: 0.001457, mae: 0.041961, mean_q: 1.164530
 715127/1000000: episode: 7152, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 58.431, mean reward: 0.584 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.470, 10.347], loss: 0.001542, mae: 0.043525, mean_q: 1.169984
 715227/1000000: episode: 7153, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.424, mean reward: 0.574 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.195], loss: 0.001526, mae: 0.042691, mean_q: 1.164640
 715327/1000000: episode: 7154, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.654, mean reward: 0.577 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.098], loss: 0.001443, mae: 0.041495, mean_q: 1.165574
 715427/1000000: episode: 7155, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 56.834, mean reward: 0.568 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.028, 10.098], loss: 0.001578, mae: 0.043142, mean_q: 1.166183
 715527/1000000: episode: 7156, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 57.358, mean reward: 0.574 [0.509, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.747, 10.231], loss: 0.001575, mae: 0.042973, mean_q: 1.164026
 715627/1000000: episode: 7157, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 59.261, mean reward: 0.593 [0.505, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.141, 10.152], loss: 0.001586, mae: 0.043647, mean_q: 1.162980
 715727/1000000: episode: 7158, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 59.094, mean reward: 0.591 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.098], loss: 0.001478, mae: 0.041455, mean_q: 1.158945
 715827/1000000: episode: 7159, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 57.810, mean reward: 0.578 [0.512, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.635, 10.102], loss: 0.001573, mae: 0.042879, mean_q: 1.160084
 715927/1000000: episode: 7160, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 57.372, mean reward: 0.574 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.181], loss: 0.001441, mae: 0.041530, mean_q: 1.158723
 716027/1000000: episode: 7161, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 59.633, mean reward: 0.596 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.134, 10.098], loss: 0.001401, mae: 0.041136, mean_q: 1.157315
 716127/1000000: episode: 7162, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.434, mean reward: 0.584 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.979, 10.098], loss: 0.001499, mae: 0.042169, mean_q: 1.158670
 716227/1000000: episode: 7163, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 59.216, mean reward: 0.592 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.696, 10.233], loss: 0.001512, mae: 0.042894, mean_q: 1.157891
 716327/1000000: episode: 7164, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 57.595, mean reward: 0.576 [0.499, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.177, 10.159], loss: 0.001371, mae: 0.040534, mean_q: 1.155904
 716427/1000000: episode: 7165, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.257, mean reward: 0.583 [0.515, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.838, 10.168], loss: 0.001482, mae: 0.041987, mean_q: 1.155939
 716527/1000000: episode: 7166, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.998, mean reward: 0.580 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.642, 10.098], loss: 0.001528, mae: 0.042134, mean_q: 1.160803
 716627/1000000: episode: 7167, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.068, mean reward: 0.581 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.604, 10.137], loss: 0.001499, mae: 0.041837, mean_q: 1.156751
 716727/1000000: episode: 7168, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 61.491, mean reward: 0.615 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.724, 10.098], loss: 0.001508, mae: 0.042107, mean_q: 1.158817
 716827/1000000: episode: 7169, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 59.826, mean reward: 0.598 [0.517, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.654, 10.146], loss: 0.001427, mae: 0.041244, mean_q: 1.157889
 716927/1000000: episode: 7170, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 59.818, mean reward: 0.598 [0.515, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.235], loss: 0.001544, mae: 0.042166, mean_q: 1.156744
 717027/1000000: episode: 7171, duration: 0.924s, episode steps: 100, steps per second: 108, episode reward: 58.041, mean reward: 0.580 [0.499, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.519, 10.426], loss: 0.001523, mae: 0.042462, mean_q: 1.157626
 717127/1000000: episode: 7172, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 59.885, mean reward: 0.599 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.473, 10.098], loss: 0.001534, mae: 0.042764, mean_q: 1.160046
 717227/1000000: episode: 7173, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 59.489, mean reward: 0.595 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.248, 10.180], loss: 0.001436, mae: 0.041330, mean_q: 1.160015
 717327/1000000: episode: 7174, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 59.886, mean reward: 0.599 [0.500, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.233, 10.409], loss: 0.001531, mae: 0.042282, mean_q: 1.162150
 717427/1000000: episode: 7175, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 59.144, mean reward: 0.591 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.202, 10.098], loss: 0.001571, mae: 0.042905, mean_q: 1.158815
 717527/1000000: episode: 7176, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 60.004, mean reward: 0.600 [0.502, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.477, 10.098], loss: 0.001561, mae: 0.042348, mean_q: 1.163192
 717627/1000000: episode: 7177, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 57.623, mean reward: 0.576 [0.499, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.904, 10.098], loss: 0.001488, mae: 0.042065, mean_q: 1.162419
 717727/1000000: episode: 7178, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 61.359, mean reward: 0.614 [0.515, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.660, 10.252], loss: 0.001525, mae: 0.042564, mean_q: 1.160856
 717827/1000000: episode: 7179, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 58.913, mean reward: 0.589 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.338, 10.098], loss: 0.001533, mae: 0.042621, mean_q: 1.162728
 717927/1000000: episode: 7180, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 57.807, mean reward: 0.578 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.383, 10.098], loss: 0.001536, mae: 0.042200, mean_q: 1.165137
 718027/1000000: episode: 7181, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 58.889, mean reward: 0.589 [0.497, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.492, 10.406], loss: 0.001452, mae: 0.041382, mean_q: 1.166070
 718127/1000000: episode: 7182, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 61.408, mean reward: 0.614 [0.501, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.427, 10.564], loss: 0.001556, mae: 0.042337, mean_q: 1.161330
 718227/1000000: episode: 7183, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 63.364, mean reward: 0.634 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.374], loss: 0.001471, mae: 0.041619, mean_q: 1.166554
 718327/1000000: episode: 7184, duration: 0.939s, episode steps: 100, steps per second: 106, episode reward: 56.421, mean reward: 0.564 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.415, 10.342], loss: 0.001565, mae: 0.042865, mean_q: 1.166610
 718427/1000000: episode: 7185, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 57.366, mean reward: 0.574 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.730, 10.184], loss: 0.001525, mae: 0.041728, mean_q: 1.164640
 718527/1000000: episode: 7186, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 58.623, mean reward: 0.586 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.476, 10.153], loss: 0.001476, mae: 0.041979, mean_q: 1.162846
 718627/1000000: episode: 7187, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 61.067, mean reward: 0.611 [0.520, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.026, 10.351], loss: 0.001526, mae: 0.042150, mean_q: 1.162434
 718727/1000000: episode: 7188, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 59.422, mean reward: 0.594 [0.508, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.489, 10.098], loss: 0.001490, mae: 0.041638, mean_q: 1.163668
 718827/1000000: episode: 7189, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 58.536, mean reward: 0.585 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.801, 10.098], loss: 0.001570, mae: 0.042354, mean_q: 1.164386
 718927/1000000: episode: 7190, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 60.930, mean reward: 0.609 [0.502, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.145, 10.210], loss: 0.001444, mae: 0.040327, mean_q: 1.159949
 719027/1000000: episode: 7191, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 62.850, mean reward: 0.629 [0.513, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.031, 10.369], loss: 0.001536, mae: 0.042168, mean_q: 1.164837
 719127/1000000: episode: 7192, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 58.030, mean reward: 0.580 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.350, 10.098], loss: 0.001604, mae: 0.042510, mean_q: 1.166903
 719227/1000000: episode: 7193, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.232, mean reward: 0.582 [0.505, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.610, 10.098], loss: 0.001653, mae: 0.043526, mean_q: 1.165850
 719327/1000000: episode: 7194, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 60.331, mean reward: 0.603 [0.505, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.667, 10.098], loss: 0.001582, mae: 0.042548, mean_q: 1.165684
 719427/1000000: episode: 7195, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 58.632, mean reward: 0.586 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.041, 10.098], loss: 0.001534, mae: 0.042006, mean_q: 1.166624
 719527/1000000: episode: 7196, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 58.354, mean reward: 0.584 [0.505, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.098], loss: 0.001523, mae: 0.042554, mean_q: 1.165480
 719627/1000000: episode: 7197, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 61.880, mean reward: 0.619 [0.504, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.315, 10.257], loss: 0.001526, mae: 0.041553, mean_q: 1.170846
 719727/1000000: episode: 7198, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 57.698, mean reward: 0.577 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.996, 10.241], loss: 0.001601, mae: 0.043037, mean_q: 1.170117
 719827/1000000: episode: 7199, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 58.018, mean reward: 0.580 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.785, 10.243], loss: 0.001658, mae: 0.043590, mean_q: 1.168141
 719927/1000000: episode: 7200, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 60.018, mean reward: 0.600 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.971, 10.098], loss: 0.001633, mae: 0.043602, mean_q: 1.171338
 720027/1000000: episode: 7201, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 56.654, mean reward: 0.567 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.636, 10.098], loss: 0.001595, mae: 0.042863, mean_q: 1.169155
 720127/1000000: episode: 7202, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.354, mean reward: 0.574 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.243, 10.148], loss: 0.001489, mae: 0.041895, mean_q: 1.167951
 720227/1000000: episode: 7203, duration: 0.932s, episode steps: 100, steps per second: 107, episode reward: 57.771, mean reward: 0.578 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.712, 10.098], loss: 0.001607, mae: 0.043035, mean_q: 1.170957
 720327/1000000: episode: 7204, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 57.719, mean reward: 0.577 [0.497, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.048, 10.098], loss: 0.001509, mae: 0.041810, mean_q: 1.167749
 720427/1000000: episode: 7205, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 57.013, mean reward: 0.570 [0.499, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.560, 10.127], loss: 0.001476, mae: 0.041348, mean_q: 1.165436
 720527/1000000: episode: 7206, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 63.108, mean reward: 0.631 [0.513, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.366, 10.098], loss: 0.001531, mae: 0.042252, mean_q: 1.166178
 720627/1000000: episode: 7207, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: 60.243, mean reward: 0.602 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.732, 10.261], loss: 0.001594, mae: 0.042900, mean_q: 1.170208
 720727/1000000: episode: 7208, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 57.674, mean reward: 0.577 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.204, 10.343], loss: 0.001684, mae: 0.043650, mean_q: 1.171805
 720827/1000000: episode: 7209, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 56.124, mean reward: 0.561 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.261, 10.098], loss: 0.001567, mae: 0.042405, mean_q: 1.168620
 720927/1000000: episode: 7210, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.687, mean reward: 0.587 [0.509, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.545, 10.228], loss: 0.001481, mae: 0.040786, mean_q: 1.163818
 721027/1000000: episode: 7211, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 58.229, mean reward: 0.582 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.579, 10.270], loss: 0.001562, mae: 0.041899, mean_q: 1.167712
 721127/1000000: episode: 7212, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.893, mean reward: 0.589 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.234, 10.098], loss: 0.001596, mae: 0.042993, mean_q: 1.172775
 721227/1000000: episode: 7213, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 59.166, mean reward: 0.592 [0.514, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.986, 10.098], loss: 0.001466, mae: 0.041224, mean_q: 1.167712
 721327/1000000: episode: 7214, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.173, mean reward: 0.572 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.931, 10.098], loss: 0.001485, mae: 0.041419, mean_q: 1.169640
 721427/1000000: episode: 7215, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 58.794, mean reward: 0.588 [0.508, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.348, 10.109], loss: 0.001467, mae: 0.041885, mean_q: 1.167487
 721527/1000000: episode: 7216, duration: 0.923s, episode steps: 100, steps per second: 108, episode reward: 58.358, mean reward: 0.584 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.939, 10.098], loss: 0.001550, mae: 0.042614, mean_q: 1.171166
 721627/1000000: episode: 7217, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 58.033, mean reward: 0.580 [0.513, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.105], loss: 0.001444, mae: 0.041020, mean_q: 1.168781
 721727/1000000: episode: 7218, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 57.393, mean reward: 0.574 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.856, 10.189], loss: 0.001591, mae: 0.042632, mean_q: 1.167809
 721827/1000000: episode: 7219, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 57.250, mean reward: 0.572 [0.504, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.159, 10.203], loss: 0.001538, mae: 0.042430, mean_q: 1.165428
 721927/1000000: episode: 7220, duration: 0.920s, episode steps: 100, steps per second: 109, episode reward: 56.721, mean reward: 0.567 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.791, 10.098], loss: 0.001571, mae: 0.042340, mean_q: 1.168400
 722027/1000000: episode: 7221, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 60.228, mean reward: 0.602 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.199, 10.098], loss: 0.001504, mae: 0.042452, mean_q: 1.171324
 722127/1000000: episode: 7222, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 62.984, mean reward: 0.630 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.098], loss: 0.001441, mae: 0.040722, mean_q: 1.162983
 722227/1000000: episode: 7223, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.714, mean reward: 0.597 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.640, 10.098], loss: 0.001486, mae: 0.040721, mean_q: 1.168983
 722327/1000000: episode: 7224, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 59.221, mean reward: 0.592 [0.501, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.958, 10.129], loss: 0.001477, mae: 0.041848, mean_q: 1.166237
 722427/1000000: episode: 7225, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.827, mean reward: 0.578 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.866, 10.098], loss: 0.001503, mae: 0.041616, mean_q: 1.165467
 722527/1000000: episode: 7226, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.071, mean reward: 0.591 [0.497, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.626, 10.258], loss: 0.001521, mae: 0.042334, mean_q: 1.166228
 722627/1000000: episode: 7227, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 62.760, mean reward: 0.628 [0.514, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.658, 10.098], loss: 0.001570, mae: 0.042977, mean_q: 1.167062
 722727/1000000: episode: 7228, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 60.546, mean reward: 0.605 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.451, 10.125], loss: 0.001540, mae: 0.041764, mean_q: 1.166613
 722827/1000000: episode: 7229, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 56.974, mean reward: 0.570 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.705, 10.183], loss: 0.001634, mae: 0.043089, mean_q: 1.172429
 722927/1000000: episode: 7230, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 60.063, mean reward: 0.601 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.494, 10.274], loss: 0.001488, mae: 0.041501, mean_q: 1.165266
 723027/1000000: episode: 7231, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 59.816, mean reward: 0.598 [0.522, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.813, 10.098], loss: 0.001401, mae: 0.040885, mean_q: 1.171174
 723127/1000000: episode: 7232, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 56.900, mean reward: 0.569 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.960, 10.156], loss: 0.001499, mae: 0.042266, mean_q: 1.168270
 723227/1000000: episode: 7233, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 60.338, mean reward: 0.603 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.443, 10.406], loss: 0.001509, mae: 0.041217, mean_q: 1.166517
 723327/1000000: episode: 7234, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.724, mean reward: 0.597 [0.508, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.572, 10.101], loss: 0.001437, mae: 0.040674, mean_q: 1.165667
 723427/1000000: episode: 7235, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 55.299, mean reward: 0.553 [0.504, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.331, 10.098], loss: 0.001350, mae: 0.039551, mean_q: 1.165577
 723527/1000000: episode: 7236, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 60.193, mean reward: 0.602 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.219], loss: 0.001480, mae: 0.041195, mean_q: 1.167528
 723627/1000000: episode: 7237, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 58.510, mean reward: 0.585 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.120, 10.151], loss: 0.001481, mae: 0.041148, mean_q: 1.165074
 723727/1000000: episode: 7238, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 62.821, mean reward: 0.628 [0.513, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.694, 10.161], loss: 0.001510, mae: 0.042707, mean_q: 1.164258
 723827/1000000: episode: 7239, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 59.549, mean reward: 0.595 [0.511, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.235, 10.098], loss: 0.001421, mae: 0.040659, mean_q: 1.165994
 723927/1000000: episode: 7240, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 60.895, mean reward: 0.609 [0.508, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.891, 10.098], loss: 0.001507, mae: 0.041320, mean_q: 1.164794
 724027/1000000: episode: 7241, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.731, mean reward: 0.577 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.764, 10.113], loss: 0.001401, mae: 0.040480, mean_q: 1.167527
 724127/1000000: episode: 7242, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.702, mean reward: 0.577 [0.508, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.098], loss: 0.001369, mae: 0.040372, mean_q: 1.165858
 724227/1000000: episode: 7243, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.746, mean reward: 0.587 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.334, 10.098], loss: 0.001392, mae: 0.040771, mean_q: 1.167496
 724327/1000000: episode: 7244, duration: 0.934s, episode steps: 100, steps per second: 107, episode reward: 61.070, mean reward: 0.611 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.538, 10.098], loss: 0.001461, mae: 0.040949, mean_q: 1.168785
 724427/1000000: episode: 7245, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 58.465, mean reward: 0.585 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.779, 10.113], loss: 0.001355, mae: 0.040043, mean_q: 1.166022
 724527/1000000: episode: 7246, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.658, mean reward: 0.577 [0.505, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.162], loss: 0.001519, mae: 0.041797, mean_q: 1.166476
 724627/1000000: episode: 7247, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 59.847, mean reward: 0.598 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.977, 10.098], loss: 0.001384, mae: 0.039905, mean_q: 1.163555
 724727/1000000: episode: 7248, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.776, mean reward: 0.578 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.697, 10.098], loss: 0.001535, mae: 0.041790, mean_q: 1.164301
 724827/1000000: episode: 7249, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 60.891, mean reward: 0.609 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.040, 10.098], loss: 0.001459, mae: 0.040337, mean_q: 1.168571
 724927/1000000: episode: 7250, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 57.773, mean reward: 0.578 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.453, 10.321], loss: 0.001369, mae: 0.039997, mean_q: 1.165811
 725027/1000000: episode: 7251, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 58.481, mean reward: 0.585 [0.513, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.231, 10.161], loss: 0.001452, mae: 0.040808, mean_q: 1.167521
 725127/1000000: episode: 7252, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 58.040, mean reward: 0.580 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.425, 10.257], loss: 0.001408, mae: 0.040802, mean_q: 1.166859
 725227/1000000: episode: 7253, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 60.560, mean reward: 0.606 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.018, 10.124], loss: 0.001372, mae: 0.039479, mean_q: 1.165573
 725327/1000000: episode: 7254, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 62.239, mean reward: 0.622 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.763, 10.098], loss: 0.001361, mae: 0.040114, mean_q: 1.165575
 725427/1000000: episode: 7255, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 57.844, mean reward: 0.578 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.691, 10.175], loss: 0.001379, mae: 0.040044, mean_q: 1.166381
 725527/1000000: episode: 7256, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 62.118, mean reward: 0.621 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.132, 10.266], loss: 0.001494, mae: 0.041410, mean_q: 1.172393
 725627/1000000: episode: 7257, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 58.056, mean reward: 0.581 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.469, 10.310], loss: 0.001539, mae: 0.041972, mean_q: 1.168740
 725727/1000000: episode: 7258, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.284, mean reward: 0.583 [0.510, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.389, 10.098], loss: 0.001394, mae: 0.040350, mean_q: 1.166416
 725827/1000000: episode: 7259, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.997, mean reward: 0.600 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.629, 10.277], loss: 0.001436, mae: 0.041125, mean_q: 1.168258
 725927/1000000: episode: 7260, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 62.022, mean reward: 0.620 [0.508, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.535, 10.166], loss: 0.001363, mae: 0.039734, mean_q: 1.169801
 726027/1000000: episode: 7261, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.953, mean reward: 0.580 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.419, 10.098], loss: 0.001450, mae: 0.041193, mean_q: 1.174490
 726127/1000000: episode: 7262, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 57.831, mean reward: 0.578 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.012, 10.195], loss: 0.001470, mae: 0.041309, mean_q: 1.170281
 726227/1000000: episode: 7263, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.674, mean reward: 0.577 [0.509, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.788, 10.098], loss: 0.001491, mae: 0.041439, mean_q: 1.173329
 726327/1000000: episode: 7264, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 59.852, mean reward: 0.599 [0.503, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.731, 10.098], loss: 0.001456, mae: 0.040800, mean_q: 1.174539
 726427/1000000: episode: 7265, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 58.900, mean reward: 0.589 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.792, 10.388], loss: 0.001477, mae: 0.041150, mean_q: 1.172233
 726527/1000000: episode: 7266, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 61.092, mean reward: 0.611 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.574, 10.098], loss: 0.001478, mae: 0.041335, mean_q: 1.171331
 726627/1000000: episode: 7267, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 57.586, mean reward: 0.576 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.184, 10.109], loss: 0.001554, mae: 0.042548, mean_q: 1.172823
 726727/1000000: episode: 7268, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 56.945, mean reward: 0.569 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.018, 10.098], loss: 0.001363, mae: 0.039814, mean_q: 1.172187
 726827/1000000: episode: 7269, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 59.630, mean reward: 0.596 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.321], loss: 0.001388, mae: 0.040358, mean_q: 1.169770
 726927/1000000: episode: 7270, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 59.380, mean reward: 0.594 [0.499, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.870, 10.224], loss: 0.001505, mae: 0.041807, mean_q: 1.172703
 727027/1000000: episode: 7271, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 60.047, mean reward: 0.600 [0.511, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.017, 10.098], loss: 0.001534, mae: 0.041635, mean_q: 1.173837
 727127/1000000: episode: 7272, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.545, mean reward: 0.575 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.098], loss: 0.001615, mae: 0.042834, mean_q: 1.173027
 727227/1000000: episode: 7273, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: 58.741, mean reward: 0.587 [0.502, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.809, 10.098], loss: 0.001565, mae: 0.042994, mean_q: 1.170367
 727327/1000000: episode: 7274, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 57.664, mean reward: 0.577 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.116], loss: 0.001494, mae: 0.041375, mean_q: 1.173775
 727427/1000000: episode: 7275, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 58.306, mean reward: 0.583 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.666, 10.240], loss: 0.001528, mae: 0.041584, mean_q: 1.168970
 727527/1000000: episode: 7276, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.927, mean reward: 0.589 [0.509, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.124, 10.336], loss: 0.001533, mae: 0.042109, mean_q: 1.170537
 727627/1000000: episode: 7277, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.081, mean reward: 0.571 [0.511, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.399, 10.207], loss: 0.001547, mae: 0.042312, mean_q: 1.171001
 727727/1000000: episode: 7278, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: 56.574, mean reward: 0.566 [0.497, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.127], loss: 0.001493, mae: 0.042133, mean_q: 1.169582
 727827/1000000: episode: 7279, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: 62.043, mean reward: 0.620 [0.508, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.696, 10.098], loss: 0.001611, mae: 0.042963, mean_q: 1.169085
 727927/1000000: episode: 7280, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 60.858, mean reward: 0.609 [0.508, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.098], loss: 0.001557, mae: 0.042132, mean_q: 1.172194
 728027/1000000: episode: 7281, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 59.686, mean reward: 0.597 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.967, 10.098], loss: 0.001540, mae: 0.042542, mean_q: 1.175974
 728127/1000000: episode: 7282, duration: 0.928s, episode steps: 100, steps per second: 108, episode reward: 56.564, mean reward: 0.566 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.098], loss: 0.001575, mae: 0.042897, mean_q: 1.169451
 728227/1000000: episode: 7283, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.051, mean reward: 0.581 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.388, 10.276], loss: 0.001550, mae: 0.042154, mean_q: 1.170082
 728327/1000000: episode: 7284, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.724, mean reward: 0.577 [0.501, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.383, 10.098], loss: 0.001481, mae: 0.041412, mean_q: 1.167587
 728427/1000000: episode: 7285, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 57.876, mean reward: 0.579 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.898, 10.462], loss: 0.001486, mae: 0.041650, mean_q: 1.169044
 728527/1000000: episode: 7286, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 60.070, mean reward: 0.601 [0.504, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.848, 10.098], loss: 0.001514, mae: 0.042103, mean_q: 1.167648
 728627/1000000: episode: 7287, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 58.516, mean reward: 0.585 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.888, 10.175], loss: 0.001493, mae: 0.041707, mean_q: 1.168585
 728727/1000000: episode: 7288, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 60.607, mean reward: 0.606 [0.511, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.340, 10.099], loss: 0.001559, mae: 0.042564, mean_q: 1.171152
 728827/1000000: episode: 7289, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 58.845, mean reward: 0.588 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.626, 10.382], loss: 0.001438, mae: 0.040863, mean_q: 1.164827
 728927/1000000: episode: 7290, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 58.975, mean reward: 0.590 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.591, 10.098], loss: 0.001572, mae: 0.043210, mean_q: 1.173970
 729027/1000000: episode: 7291, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 57.179, mean reward: 0.572 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.550, 10.299], loss: 0.001481, mae: 0.041457, mean_q: 1.163899
 729127/1000000: episode: 7292, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 56.671, mean reward: 0.567 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.895, 10.151], loss: 0.001504, mae: 0.041958, mean_q: 1.166609
 729227/1000000: episode: 7293, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 57.744, mean reward: 0.577 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.270], loss: 0.001485, mae: 0.041313, mean_q: 1.164367
 729327/1000000: episode: 7294, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 60.962, mean reward: 0.610 [0.498, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.692, 10.492], loss: 0.001511, mae: 0.042389, mean_q: 1.164084
 729427/1000000: episode: 7295, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 58.231, mean reward: 0.582 [0.509, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.191], loss: 0.001415, mae: 0.041350, mean_q: 1.167432
 729527/1000000: episode: 7296, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 61.228, mean reward: 0.612 [0.510, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.649, 10.101], loss: 0.001587, mae: 0.042953, mean_q: 1.167243
 729627/1000000: episode: 7297, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 59.011, mean reward: 0.590 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.223], loss: 0.001486, mae: 0.042330, mean_q: 1.164743
 729727/1000000: episode: 7298, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 57.196, mean reward: 0.572 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.219, 10.098], loss: 0.001491, mae: 0.042125, mean_q: 1.165870
 729827/1000000: episode: 7299, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 58.368, mean reward: 0.584 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.052, 10.098], loss: 0.001491, mae: 0.042320, mean_q: 1.164388
 729927/1000000: episode: 7300, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 57.020, mean reward: 0.570 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.203], loss: 0.001513, mae: 0.042164, mean_q: 1.167945
 730027/1000000: episode: 7301, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.550, mean reward: 0.595 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.289], loss: 0.001517, mae: 0.042509, mean_q: 1.161494
 730127/1000000: episode: 7302, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.379, mean reward: 0.594 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.931, 10.098], loss: 0.001540, mae: 0.042800, mean_q: 1.165934
 730227/1000000: episode: 7303, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 59.108, mean reward: 0.591 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.683, 10.098], loss: 0.001559, mae: 0.043168, mean_q: 1.165485
 730327/1000000: episode: 7304, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 59.180, mean reward: 0.592 [0.519, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.127, 10.098], loss: 0.001435, mae: 0.041153, mean_q: 1.161451
 730427/1000000: episode: 7305, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 58.822, mean reward: 0.588 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.666, 10.153], loss: 0.001489, mae: 0.042158, mean_q: 1.164591
 730527/1000000: episode: 7306, duration: 0.945s, episode steps: 100, steps per second: 106, episode reward: 58.724, mean reward: 0.587 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.054, 10.098], loss: 0.001499, mae: 0.042122, mean_q: 1.163811
 730627/1000000: episode: 7307, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 60.282, mean reward: 0.603 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.857, 10.254], loss: 0.001440, mae: 0.041585, mean_q: 1.163661
 730727/1000000: episode: 7308, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 57.485, mean reward: 0.575 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.098], loss: 0.001476, mae: 0.041557, mean_q: 1.164868
 730827/1000000: episode: 7309, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 59.778, mean reward: 0.598 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.916, 10.261], loss: 0.001433, mae: 0.041261, mean_q: 1.160889
 730927/1000000: episode: 7310, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 59.922, mean reward: 0.599 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.016, 10.098], loss: 0.001545, mae: 0.042601, mean_q: 1.164849
 731027/1000000: episode: 7311, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 58.928, mean reward: 0.589 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.365, 10.211], loss: 0.001572, mae: 0.042560, mean_q: 1.162893
 731127/1000000: episode: 7312, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.258, mean reward: 0.583 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.745, 10.190], loss: 0.001395, mae: 0.041214, mean_q: 1.163571
 731227/1000000: episode: 7313, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 57.655, mean reward: 0.577 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.089, 10.098], loss: 0.001455, mae: 0.041443, mean_q: 1.162849
 731327/1000000: episode: 7314, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 59.724, mean reward: 0.597 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.098], loss: 0.001490, mae: 0.041979, mean_q: 1.161255
 731427/1000000: episode: 7315, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 63.793, mean reward: 0.638 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.343, 10.098], loss: 0.001500, mae: 0.042664, mean_q: 1.162817
 731527/1000000: episode: 7316, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.399, mean reward: 0.584 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.115, 10.098], loss: 0.001421, mae: 0.041534, mean_q: 1.167365
 731627/1000000: episode: 7317, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 59.962, mean reward: 0.600 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.151, 10.098], loss: 0.001459, mae: 0.041178, mean_q: 1.163555
 731727/1000000: episode: 7318, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.302, mean reward: 0.583 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.640, 10.098], loss: 0.001395, mae: 0.040651, mean_q: 1.162031
 731827/1000000: episode: 7319, duration: 0.936s, episode steps: 100, steps per second: 107, episode reward: 57.626, mean reward: 0.576 [0.505, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.350, 10.272], loss: 0.001479, mae: 0.042541, mean_q: 1.166573
 731927/1000000: episode: 7320, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.164, mean reward: 0.582 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.135, 10.274], loss: 0.001474, mae: 0.041325, mean_q: 1.162855
 732027/1000000: episode: 7321, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 60.235, mean reward: 0.602 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.277, 10.235], loss: 0.001503, mae: 0.042713, mean_q: 1.165033
 732127/1000000: episode: 7322, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 59.777, mean reward: 0.598 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.995, 10.098], loss: 0.001462, mae: 0.041333, mean_q: 1.162492
 732227/1000000: episode: 7323, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 58.249, mean reward: 0.582 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.246, 10.143], loss: 0.001452, mae: 0.041433, mean_q: 1.161697
 732327/1000000: episode: 7324, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.511, mean reward: 0.575 [0.502, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.371, 10.235], loss: 0.001478, mae: 0.042314, mean_q: 1.165158
 732427/1000000: episode: 7325, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 58.764, mean reward: 0.588 [0.508, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.541, 10.098], loss: 0.001530, mae: 0.042789, mean_q: 1.166165
 732527/1000000: episode: 7326, duration: 0.949s, episode steps: 100, steps per second: 105, episode reward: 60.080, mean reward: 0.601 [0.504, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.929, 10.098], loss: 0.001585, mae: 0.043094, mean_q: 1.162632
 732627/1000000: episode: 7327, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 59.455, mean reward: 0.595 [0.515, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.590, 10.255], loss: 0.001580, mae: 0.043136, mean_q: 1.166190
 732727/1000000: episode: 7328, duration: 1.460s, episode steps: 100, steps per second: 69, episode reward: 62.429, mean reward: 0.624 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.376, 10.098], loss: 0.001484, mae: 0.041611, mean_q: 1.165705
 732827/1000000: episode: 7329, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: 62.590, mean reward: 0.626 [0.500, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.820, 10.098], loss: 0.001540, mae: 0.043335, mean_q: 1.167922
 732927/1000000: episode: 7330, duration: 1.702s, episode steps: 100, steps per second: 59, episode reward: 60.466, mean reward: 0.605 [0.512, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.312, 10.170], loss: 0.001534, mae: 0.042533, mean_q: 1.167497
 733027/1000000: episode: 7331, duration: 1.629s, episode steps: 100, steps per second: 61, episode reward: 57.906, mean reward: 0.579 [0.499, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.425, 10.297], loss: 0.001611, mae: 0.043443, mean_q: 1.172808
 733127/1000000: episode: 7332, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 58.879, mean reward: 0.589 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.394, 10.287], loss: 0.001538, mae: 0.043064, mean_q: 1.169108
 733227/1000000: episode: 7333, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.744, mean reward: 0.587 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.291, 10.098], loss: 0.001630, mae: 0.044044, mean_q: 1.172733
 733327/1000000: episode: 7334, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 57.427, mean reward: 0.574 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.361], loss: 0.001566, mae: 0.043085, mean_q: 1.171382
 733427/1000000: episode: 7335, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 59.619, mean reward: 0.596 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.210, 10.098], loss: 0.001588, mae: 0.043165, mean_q: 1.166276
 733527/1000000: episode: 7336, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 57.967, mean reward: 0.580 [0.500, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.921, 10.235], loss: 0.001656, mae: 0.044548, mean_q: 1.169383
 733627/1000000: episode: 7337, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 58.602, mean reward: 0.586 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.499, 10.105], loss: 0.001482, mae: 0.042384, mean_q: 1.169401
 733727/1000000: episode: 7338, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.435, mean reward: 0.574 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.368, 10.178], loss: 0.001544, mae: 0.042632, mean_q: 1.168205
 733827/1000000: episode: 7339, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 59.040, mean reward: 0.590 [0.509, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.554, 10.372], loss: 0.001497, mae: 0.042288, mean_q: 1.168571
 733927/1000000: episode: 7340, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 61.594, mean reward: 0.616 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.394, 10.493], loss: 0.001653, mae: 0.044204, mean_q: 1.169575
 734027/1000000: episode: 7341, duration: 1.385s, episode steps: 100, steps per second: 72, episode reward: 58.761, mean reward: 0.588 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.535, 10.191], loss: 0.001550, mae: 0.043392, mean_q: 1.169984
 734127/1000000: episode: 7342, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 62.207, mean reward: 0.622 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.771, 10.098], loss: 0.001493, mae: 0.041990, mean_q: 1.170448
 734227/1000000: episode: 7343, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 58.242, mean reward: 0.582 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.098], loss: 0.001510, mae: 0.041933, mean_q: 1.169743
 734327/1000000: episode: 7344, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 57.997, mean reward: 0.580 [0.503, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.272], loss: 0.001462, mae: 0.041723, mean_q: 1.170770
 734427/1000000: episode: 7345, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 57.507, mean reward: 0.575 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.640, 10.098], loss: 0.001533, mae: 0.042613, mean_q: 1.168581
 734527/1000000: episode: 7346, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 61.423, mean reward: 0.614 [0.505, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.264], loss: 0.001578, mae: 0.042916, mean_q: 1.169292
 734627/1000000: episode: 7347, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 58.174, mean reward: 0.582 [0.509, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.162, 10.098], loss: 0.001535, mae: 0.041942, mean_q: 1.170293
 734727/1000000: episode: 7348, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 56.383, mean reward: 0.564 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.764, 10.098], loss: 0.001488, mae: 0.041594, mean_q: 1.171087
 734827/1000000: episode: 7349, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 60.031, mean reward: 0.600 [0.503, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.898, 10.421], loss: 0.001466, mae: 0.041064, mean_q: 1.170021
 734927/1000000: episode: 7350, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 57.267, mean reward: 0.573 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.175], loss: 0.001436, mae: 0.041832, mean_q: 1.172742
 735027/1000000: episode: 7351, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 58.951, mean reward: 0.590 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.459, 10.202], loss: 0.001423, mae: 0.041643, mean_q: 1.170820
 735127/1000000: episode: 7352, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 58.695, mean reward: 0.587 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.039, 10.280], loss: 0.001431, mae: 0.041498, mean_q: 1.170390
 735227/1000000: episode: 7353, duration: 0.937s, episode steps: 100, steps per second: 107, episode reward: 57.496, mean reward: 0.575 [0.511, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.818, 10.098], loss: 0.001497, mae: 0.042113, mean_q: 1.168862
 735327/1000000: episode: 7354, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 58.249, mean reward: 0.582 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.306], loss: 0.001473, mae: 0.041483, mean_q: 1.166957
 735427/1000000: episode: 7355, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 57.879, mean reward: 0.579 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.965, 10.098], loss: 0.001583, mae: 0.042743, mean_q: 1.170306
 735527/1000000: episode: 7356, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 60.792, mean reward: 0.608 [0.498, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.742, 10.314], loss: 0.001585, mae: 0.042954, mean_q: 1.167923
 735627/1000000: episode: 7357, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.696, mean reward: 0.587 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.554, 10.315], loss: 0.001595, mae: 0.043192, mean_q: 1.169093
 735727/1000000: episode: 7358, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.228, mean reward: 0.582 [0.499, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.363, 10.098], loss: 0.001494, mae: 0.041689, mean_q: 1.166906
 735827/1000000: episode: 7359, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 57.589, mean reward: 0.576 [0.500, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.288], loss: 0.001435, mae: 0.041451, mean_q: 1.170981
 735927/1000000: episode: 7360, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 60.566, mean reward: 0.606 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.497, 10.356], loss: 0.001548, mae: 0.042287, mean_q: 1.169796
 736027/1000000: episode: 7361, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 58.905, mean reward: 0.589 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.616, 10.098], loss: 0.001462, mae: 0.041306, mean_q: 1.168092
 736127/1000000: episode: 7362, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 56.466, mean reward: 0.565 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.496, 10.098], loss: 0.001570, mae: 0.042813, mean_q: 1.171940
 736227/1000000: episode: 7363, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 60.709, mean reward: 0.607 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.685, 10.320], loss: 0.001427, mae: 0.041169, mean_q: 1.166718
 736327/1000000: episode: 7364, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 60.394, mean reward: 0.604 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.098], loss: 0.001523, mae: 0.041916, mean_q: 1.167848
 736427/1000000: episode: 7365, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 57.024, mean reward: 0.570 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.755, 10.098], loss: 0.001497, mae: 0.041838, mean_q: 1.168792
 736527/1000000: episode: 7366, duration: 1.242s, episode steps: 100, steps per second: 81, episode reward: 58.385, mean reward: 0.584 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.511, 10.098], loss: 0.001382, mae: 0.040829, mean_q: 1.165702
 736627/1000000: episode: 7367, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 61.405, mean reward: 0.614 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.526, 10.302], loss: 0.001504, mae: 0.041981, mean_q: 1.170116
 736727/1000000: episode: 7368, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 59.284, mean reward: 0.593 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.928, 10.152], loss: 0.001451, mae: 0.041589, mean_q: 1.168966
 736827/1000000: episode: 7369, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 60.760, mean reward: 0.608 [0.515, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.098], loss: 0.001490, mae: 0.041530, mean_q: 1.166627
 736927/1000000: episode: 7370, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 60.086, mean reward: 0.601 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.186, 10.098], loss: 0.001519, mae: 0.042543, mean_q: 1.172625
 737027/1000000: episode: 7371, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.006, mean reward: 0.570 [0.502, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.109, 10.132], loss: 0.001478, mae: 0.041327, mean_q: 1.169279
 737127/1000000: episode: 7372, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 57.958, mean reward: 0.580 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.132, 10.128], loss: 0.001404, mae: 0.040624, mean_q: 1.165619
 737227/1000000: episode: 7373, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 59.928, mean reward: 0.599 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.478, 10.122], loss: 0.001365, mae: 0.040125, mean_q: 1.163423
 737327/1000000: episode: 7374, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 58.532, mean reward: 0.585 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.098], loss: 0.001407, mae: 0.040567, mean_q: 1.169023
 737427/1000000: episode: 7375, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 58.863, mean reward: 0.589 [0.513, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.046, 10.185], loss: 0.001332, mae: 0.039790, mean_q: 1.171567
 737527/1000000: episode: 7376, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 55.890, mean reward: 0.559 [0.499, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.688, 10.113], loss: 0.001393, mae: 0.040767, mean_q: 1.169624
 737627/1000000: episode: 7377, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 60.815, mean reward: 0.608 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.398, 10.098], loss: 0.001380, mae: 0.040895, mean_q: 1.169904
 737727/1000000: episode: 7378, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.688, mean reward: 0.587 [0.498, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.453, 10.237], loss: 0.001305, mae: 0.039777, mean_q: 1.165519
 737827/1000000: episode: 7379, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 57.421, mean reward: 0.574 [0.498, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.133], loss: 0.001381, mae: 0.040485, mean_q: 1.164929
 737927/1000000: episode: 7380, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.430, mean reward: 0.594 [0.498, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.741, 10.201], loss: 0.001479, mae: 0.041729, mean_q: 1.165911
 738027/1000000: episode: 7381, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 58.921, mean reward: 0.589 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.682, 10.371], loss: 0.001423, mae: 0.040756, mean_q: 1.163624
 738127/1000000: episode: 7382, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 59.902, mean reward: 0.599 [0.516, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.993, 10.098], loss: 0.001376, mae: 0.040548, mean_q: 1.161573
 738227/1000000: episode: 7383, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 58.914, mean reward: 0.589 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.988, 10.098], loss: 0.001350, mae: 0.040539, mean_q: 1.162918
 738327/1000000: episode: 7384, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.881, mean reward: 0.589 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.345, 10.268], loss: 0.001461, mae: 0.041264, mean_q: 1.168602
 738427/1000000: episode: 7385, duration: 0.950s, episode steps: 100, steps per second: 105, episode reward: 57.386, mean reward: 0.574 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.649, 10.311], loss: 0.001548, mae: 0.042754, mean_q: 1.164612
 738527/1000000: episode: 7386, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 62.162, mean reward: 0.622 [0.515, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.282, 10.098], loss: 0.001432, mae: 0.041749, mean_q: 1.162791
 738627/1000000: episode: 7387, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 58.943, mean reward: 0.589 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.098], loss: 0.001417, mae: 0.041057, mean_q: 1.166310
 738727/1000000: episode: 7388, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.370, mean reward: 0.584 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.883, 10.098], loss: 0.001380, mae: 0.040781, mean_q: 1.163653
 738827/1000000: episode: 7389, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 59.027, mean reward: 0.590 [0.512, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.441, 10.167], loss: 0.001385, mae: 0.041042, mean_q: 1.163845
 738927/1000000: episode: 7390, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 58.432, mean reward: 0.584 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.287], loss: 0.001378, mae: 0.041141, mean_q: 1.165902
 739027/1000000: episode: 7391, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.488, mean reward: 0.585 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.982, 10.201], loss: 0.001397, mae: 0.041096, mean_q: 1.167176
 739127/1000000: episode: 7392, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 58.660, mean reward: 0.587 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.177, 10.300], loss: 0.001331, mae: 0.040329, mean_q: 1.163887
 739227/1000000: episode: 7393, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 60.146, mean reward: 0.601 [0.522, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.353, 10.276], loss: 0.001415, mae: 0.041083, mean_q: 1.164946
 739327/1000000: episode: 7394, duration: 1.858s, episode steps: 100, steps per second: 54, episode reward: 58.356, mean reward: 0.584 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.608, 10.098], loss: 0.001290, mae: 0.039435, mean_q: 1.165399
 739427/1000000: episode: 7395, duration: 1.505s, episode steps: 100, steps per second: 66, episode reward: 58.209, mean reward: 0.582 [0.510, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.210, 10.259], loss: 0.001336, mae: 0.040336, mean_q: 1.167578
 739527/1000000: episode: 7396, duration: 1.630s, episode steps: 100, steps per second: 61, episode reward: 58.326, mean reward: 0.583 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.036, 10.153], loss: 0.001345, mae: 0.040009, mean_q: 1.163791
 739627/1000000: episode: 7397, duration: 0.985s, episode steps: 100, steps per second: 101, episode reward: 60.789, mean reward: 0.608 [0.500, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.573, 10.459], loss: 0.001349, mae: 0.040447, mean_q: 1.165325
 739727/1000000: episode: 7398, duration: 1.301s, episode steps: 100, steps per second: 77, episode reward: 59.069, mean reward: 0.591 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.699, 10.371], loss: 0.001437, mae: 0.042067, mean_q: 1.167607
 739827/1000000: episode: 7399, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 61.740, mean reward: 0.617 [0.533, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.450, 10.338], loss: 0.001295, mae: 0.040015, mean_q: 1.164264
 739927/1000000: episode: 7400, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.873, mean reward: 0.599 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.149], loss: 0.001293, mae: 0.039423, mean_q: 1.165713
 740027/1000000: episode: 7401, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 57.961, mean reward: 0.580 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.212], loss: 0.001409, mae: 0.041399, mean_q: 1.163937
 740127/1000000: episode: 7402, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 57.222, mean reward: 0.572 [0.500, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.781, 10.098], loss: 0.001329, mae: 0.039790, mean_q: 1.168059
 740227/1000000: episode: 7403, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 59.898, mean reward: 0.599 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.757, 10.306], loss: 0.001314, mae: 0.040312, mean_q: 1.168972
 740327/1000000: episode: 7404, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 59.184, mean reward: 0.592 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.427, 10.223], loss: 0.001372, mae: 0.040504, mean_q: 1.166761
 740427/1000000: episode: 7405, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 60.028, mean reward: 0.600 [0.509, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.789, 10.098], loss: 0.001313, mae: 0.039841, mean_q: 1.164725
 740527/1000000: episode: 7406, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 59.180, mean reward: 0.592 [0.499, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.774, 10.098], loss: 0.001394, mae: 0.041019, mean_q: 1.171877
 740627/1000000: episode: 7407, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 58.793, mean reward: 0.588 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.816, 10.228], loss: 0.001367, mae: 0.040399, mean_q: 1.171411
 740727/1000000: episode: 7408, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 58.591, mean reward: 0.586 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.776, 10.098], loss: 0.001350, mae: 0.040376, mean_q: 1.170459
 740827/1000000: episode: 7409, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 60.930, mean reward: 0.609 [0.514, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.634, 10.098], loss: 0.001369, mae: 0.040940, mean_q: 1.168495
 740927/1000000: episode: 7410, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 59.406, mean reward: 0.594 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.528, 10.293], loss: 0.001327, mae: 0.039838, mean_q: 1.168063
 741027/1000000: episode: 7411, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 59.325, mean reward: 0.593 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.488, 10.236], loss: 0.001241, mae: 0.038957, mean_q: 1.168518
 741127/1000000: episode: 7412, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 62.634, mean reward: 0.626 [0.520, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.655, 10.098], loss: 0.001321, mae: 0.039657, mean_q: 1.171081
 741227/1000000: episode: 7413, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 59.086, mean reward: 0.591 [0.510, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.077, 10.168], loss: 0.001416, mae: 0.041616, mean_q: 1.173258
 741327/1000000: episode: 7414, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 59.517, mean reward: 0.595 [0.499, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.285], loss: 0.001395, mae: 0.041002, mean_q: 1.171377
 741427/1000000: episode: 7415, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 57.241, mean reward: 0.572 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.524, 10.098], loss: 0.001413, mae: 0.041538, mean_q: 1.168492
 741527/1000000: episode: 7416, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 58.450, mean reward: 0.584 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.820, 10.271], loss: 0.001313, mae: 0.039967, mean_q: 1.171100
 741627/1000000: episode: 7417, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 61.796, mean reward: 0.618 [0.499, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.810, 10.510], loss: 0.001376, mae: 0.040731, mean_q: 1.172030
 741727/1000000: episode: 7418, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 60.400, mean reward: 0.604 [0.514, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.793, 10.367], loss: 0.001424, mae: 0.040922, mean_q: 1.171114
 741827/1000000: episode: 7419, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 60.382, mean reward: 0.604 [0.513, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.523, 10.208], loss: 0.001458, mae: 0.041404, mean_q: 1.173514
 741927/1000000: episode: 7420, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 62.382, mean reward: 0.624 [0.516, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.995, 10.098], loss: 0.001447, mae: 0.041629, mean_q: 1.169511
 742027/1000000: episode: 7421, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.231, mean reward: 0.582 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.982, 10.383], loss: 0.001439, mae: 0.042018, mean_q: 1.169849
 742127/1000000: episode: 7422, duration: 1.588s, episode steps: 100, steps per second: 63, episode reward: 59.874, mean reward: 0.599 [0.500, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.446, 10.098], loss: 0.001420, mae: 0.041497, mean_q: 1.169705
 742227/1000000: episode: 7423, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 58.169, mean reward: 0.582 [0.508, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.495, 10.185], loss: 0.001384, mae: 0.040374, mean_q: 1.173000
 742327/1000000: episode: 7424, duration: 1.520s, episode steps: 100, steps per second: 66, episode reward: 59.311, mean reward: 0.593 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.649, 10.426], loss: 0.001409, mae: 0.040921, mean_q: 1.171247
 742427/1000000: episode: 7425, duration: 1.759s, episode steps: 100, steps per second: 57, episode reward: 57.731, mean reward: 0.577 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.351, 10.231], loss: 0.001394, mae: 0.041211, mean_q: 1.171415
 742527/1000000: episode: 7426, duration: 1.649s, episode steps: 100, steps per second: 61, episode reward: 60.387, mean reward: 0.604 [0.505, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.575, 10.183], loss: 0.001435, mae: 0.040729, mean_q: 1.173357
 742627/1000000: episode: 7427, duration: 1.879s, episode steps: 100, steps per second: 53, episode reward: 58.142, mean reward: 0.581 [0.502, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.400, 10.177], loss: 0.001434, mae: 0.041583, mean_q: 1.171515
 742727/1000000: episode: 7428, duration: 1.614s, episode steps: 100, steps per second: 62, episode reward: 58.831, mean reward: 0.588 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.516, 10.163], loss: 0.001579, mae: 0.042747, mean_q: 1.176715
 742827/1000000: episode: 7429, duration: 1.513s, episode steps: 100, steps per second: 66, episode reward: 58.293, mean reward: 0.583 [0.507, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.051, 10.275], loss: 0.001453, mae: 0.041562, mean_q: 1.176214
 742927/1000000: episode: 7430, duration: 1.655s, episode steps: 100, steps per second: 60, episode reward: 58.154, mean reward: 0.582 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.404, 10.130], loss: 0.001496, mae: 0.042628, mean_q: 1.175799
 743027/1000000: episode: 7431, duration: 1.466s, episode steps: 100, steps per second: 68, episode reward: 59.319, mean reward: 0.593 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.367, 10.098], loss: 0.001489, mae: 0.041556, mean_q: 1.173745
 743127/1000000: episode: 7432, duration: 1.771s, episode steps: 100, steps per second: 56, episode reward: 56.506, mean reward: 0.565 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.016, 10.102], loss: 0.001418, mae: 0.040822, mean_q: 1.169453
 743227/1000000: episode: 7433, duration: 1.724s, episode steps: 100, steps per second: 58, episode reward: 56.307, mean reward: 0.563 [0.505, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.137, 10.218], loss: 0.001385, mae: 0.040387, mean_q: 1.168675
 743327/1000000: episode: 7434, duration: 1.318s, episode steps: 100, steps per second: 76, episode reward: 58.907, mean reward: 0.589 [0.515, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.814, 10.098], loss: 0.001460, mae: 0.040842, mean_q: 1.170840
 743427/1000000: episode: 7435, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 56.943, mean reward: 0.569 [0.507, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.382, 10.252], loss: 0.001463, mae: 0.041803, mean_q: 1.170815
 743527/1000000: episode: 7436, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 63.165, mean reward: 0.632 [0.514, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.098], loss: 0.001412, mae: 0.040765, mean_q: 1.169457
 743627/1000000: episode: 7437, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 59.155, mean reward: 0.592 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.856, 10.098], loss: 0.001563, mae: 0.043053, mean_q: 1.175240
 743727/1000000: episode: 7438, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 57.105, mean reward: 0.571 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.098], loss: 0.001408, mae: 0.040518, mean_q: 1.173415
 743827/1000000: episode: 7439, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 57.808, mean reward: 0.578 [0.512, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.565, 10.098], loss: 0.001442, mae: 0.041346, mean_q: 1.171012
 743927/1000000: episode: 7440, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 57.948, mean reward: 0.579 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.866, 10.098], loss: 0.001594, mae: 0.042851, mean_q: 1.171102
 744027/1000000: episode: 7441, duration: 1.005s, episode steps: 100, steps per second: 99, episode reward: 57.117, mean reward: 0.571 [0.503, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.098], loss: 0.001512, mae: 0.041602, mean_q: 1.167402
 744127/1000000: episode: 7442, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 60.223, mean reward: 0.602 [0.514, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.116, 10.506], loss: 0.001517, mae: 0.042459, mean_q: 1.167721
 744227/1000000: episode: 7443, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 57.292, mean reward: 0.573 [0.500, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.755, 10.190], loss: 0.001517, mae: 0.042157, mean_q: 1.170844
 744327/1000000: episode: 7444, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 60.052, mean reward: 0.601 [0.515, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.259, 10.098], loss: 0.001527, mae: 0.042597, mean_q: 1.168836
 744427/1000000: episode: 7445, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 58.149, mean reward: 0.581 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.710, 10.155], loss: 0.001478, mae: 0.041417, mean_q: 1.170182
 744527/1000000: episode: 7446, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.165, mean reward: 0.592 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.299, 10.098], loss: 0.001513, mae: 0.041893, mean_q: 1.169673
 744627/1000000: episode: 7447, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 57.956, mean reward: 0.580 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.109, 10.286], loss: 0.001514, mae: 0.042272, mean_q: 1.170073
 744727/1000000: episode: 7448, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 59.383, mean reward: 0.594 [0.512, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.704, 10.098], loss: 0.001513, mae: 0.041734, mean_q: 1.165364
 744827/1000000: episode: 7449, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 58.897, mean reward: 0.589 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.722, 10.269], loss: 0.001441, mae: 0.040704, mean_q: 1.166885
 744927/1000000: episode: 7450, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 62.591, mean reward: 0.626 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.019, 10.377], loss: 0.001454, mae: 0.041468, mean_q: 1.167399
 745027/1000000: episode: 7451, duration: 1.281s, episode steps: 100, steps per second: 78, episode reward: 59.369, mean reward: 0.594 [0.516, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.542, 10.098], loss: 0.001495, mae: 0.041561, mean_q: 1.167739
 745127/1000000: episode: 7452, duration: 1.415s, episode steps: 100, steps per second: 71, episode reward: 59.379, mean reward: 0.594 [0.503, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.301, 10.443], loss: 0.001588, mae: 0.042646, mean_q: 1.171759
 745227/1000000: episode: 7453, duration: 1.436s, episode steps: 100, steps per second: 70, episode reward: 60.132, mean reward: 0.601 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.908, 10.126], loss: 0.001456, mae: 0.041472, mean_q: 1.172402
 745327/1000000: episode: 7454, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 57.125, mean reward: 0.571 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.801, 10.246], loss: 0.001350, mae: 0.039477, mean_q: 1.168910
 745427/1000000: episode: 7455, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 58.201, mean reward: 0.582 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.578, 10.104], loss: 0.001629, mae: 0.043380, mean_q: 1.171904
 745527/1000000: episode: 7456, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.481, mean reward: 0.575 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.720, 10.098], loss: 0.001584, mae: 0.042279, mean_q: 1.172354
 745627/1000000: episode: 7457, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 58.057, mean reward: 0.581 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.448, 10.230], loss: 0.001583, mae: 0.042880, mean_q: 1.166747
 745727/1000000: episode: 7458, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 58.934, mean reward: 0.589 [0.512, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.450, 10.098], loss: 0.001471, mae: 0.041130, mean_q: 1.167485
 745827/1000000: episode: 7459, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 57.016, mean reward: 0.570 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.833, 10.098], loss: 0.001458, mae: 0.040910, mean_q: 1.166282
 745927/1000000: episode: 7460, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 60.594, mean reward: 0.606 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.965, 10.105], loss: 0.001543, mae: 0.042048, mean_q: 1.164959
 746027/1000000: episode: 7461, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 59.491, mean reward: 0.595 [0.509, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.691, 10.366], loss: 0.001514, mae: 0.041906, mean_q: 1.166089
 746127/1000000: episode: 7462, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 58.766, mean reward: 0.588 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.342], loss: 0.001605, mae: 0.043287, mean_q: 1.166080
 746227/1000000: episode: 7463, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 57.899, mean reward: 0.579 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.969, 10.098], loss: 0.001536, mae: 0.041941, mean_q: 1.163926
 746327/1000000: episode: 7464, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 56.631, mean reward: 0.566 [0.504, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.098], loss: 0.001595, mae: 0.043161, mean_q: 1.166379
 746427/1000000: episode: 7465, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 59.082, mean reward: 0.591 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.513, 10.331], loss: 0.001529, mae: 0.042299, mean_q: 1.164584
 746527/1000000: episode: 7466, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 57.723, mean reward: 0.577 [0.499, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.439, 10.149], loss: 0.001533, mae: 0.042217, mean_q: 1.163200
 746627/1000000: episode: 7467, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 57.903, mean reward: 0.579 [0.504, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.160, 10.107], loss: 0.001488, mae: 0.041537, mean_q: 1.165397
 746727/1000000: episode: 7468, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.766, mean reward: 0.598 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.327, 10.309], loss: 0.001600, mae: 0.042573, mean_q: 1.162791
 746827/1000000: episode: 7469, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 59.339, mean reward: 0.593 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.233, 10.460], loss: 0.001543, mae: 0.042347, mean_q: 1.163839
 746927/1000000: episode: 7470, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 58.546, mean reward: 0.585 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.183, 10.098], loss: 0.001490, mae: 0.041376, mean_q: 1.159799
 747027/1000000: episode: 7471, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 56.955, mean reward: 0.570 [0.506, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.612, 10.292], loss: 0.001456, mae: 0.040746, mean_q: 1.161689
 747127/1000000: episode: 7472, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 58.478, mean reward: 0.585 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.018, 10.098], loss: 0.001497, mae: 0.040670, mean_q: 1.160354
 747227/1000000: episode: 7473, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 59.573, mean reward: 0.596 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.957, 10.098], loss: 0.001542, mae: 0.041855, mean_q: 1.161983
 747327/1000000: episode: 7474, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 61.105, mean reward: 0.611 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.429, 10.327], loss: 0.001501, mae: 0.041901, mean_q: 1.159513
 747427/1000000: episode: 7475, duration: 1.143s, episode steps: 100, steps per second: 88, episode reward: 57.901, mean reward: 0.579 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.501, 10.187], loss: 0.001440, mae: 0.040387, mean_q: 1.158653
 747527/1000000: episode: 7476, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 58.176, mean reward: 0.582 [0.497, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.632, 10.416], loss: 0.001371, mae: 0.039788, mean_q: 1.160335
 747627/1000000: episode: 7477, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 60.616, mean reward: 0.606 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.632, 10.361], loss: 0.001469, mae: 0.041241, mean_q: 1.161351
 747727/1000000: episode: 7478, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 56.853, mean reward: 0.569 [0.508, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.783, 10.098], loss: 0.001375, mae: 0.039112, mean_q: 1.159319
 747827/1000000: episode: 7479, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 60.770, mean reward: 0.608 [0.516, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.635, 10.230], loss: 0.001421, mae: 0.040699, mean_q: 1.162975
 747927/1000000: episode: 7480, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 58.532, mean reward: 0.585 [0.499, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.634, 10.098], loss: 0.001384, mae: 0.039963, mean_q: 1.161244
 748027/1000000: episode: 7481, duration: 0.973s, episode steps: 100, steps per second: 103, episode reward: 64.874, mean reward: 0.649 [0.505, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.239, 10.098], loss: 0.001506, mae: 0.041508, mean_q: 1.164495
 748127/1000000: episode: 7482, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 58.312, mean reward: 0.583 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.276, 10.325], loss: 0.001353, mae: 0.039534, mean_q: 1.163494
 748227/1000000: episode: 7483, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 57.407, mean reward: 0.574 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.663, 10.154], loss: 0.001489, mae: 0.041432, mean_q: 1.167539
 748327/1000000: episode: 7484, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 57.681, mean reward: 0.577 [0.502, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.568, 10.107], loss: 0.001367, mae: 0.040080, mean_q: 1.166107
 748427/1000000: episode: 7485, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 56.912, mean reward: 0.569 [0.509, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.862, 10.243], loss: 0.001383, mae: 0.040118, mean_q: 1.164491
 748527/1000000: episode: 7486, duration: 1.156s, episode steps: 100, steps per second: 87, episode reward: 59.467, mean reward: 0.595 [0.511, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.423, 10.098], loss: 0.001576, mae: 0.041834, mean_q: 1.165559
 748627/1000000: episode: 7487, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 57.427, mean reward: 0.574 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.114], loss: 0.001415, mae: 0.040365, mean_q: 1.165782
 748727/1000000: episode: 7488, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.945, mean reward: 0.599 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.715, 10.396], loss: 0.001369, mae: 0.039824, mean_q: 1.163200
 748827/1000000: episode: 7489, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.100, mean reward: 0.581 [0.499, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.628, 10.182], loss: 0.001460, mae: 0.040877, mean_q: 1.164804
 748927/1000000: episode: 7490, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 59.865, mean reward: 0.599 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.349, 10.098], loss: 0.001411, mae: 0.040475, mean_q: 1.163668
 749027/1000000: episode: 7491, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 59.460, mean reward: 0.595 [0.517, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.700, 10.261], loss: 0.001375, mae: 0.039829, mean_q: 1.165340
 749127/1000000: episode: 7492, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 57.889, mean reward: 0.579 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.380, 10.147], loss: 0.001449, mae: 0.041261, mean_q: 1.167394
 749227/1000000: episode: 7493, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 57.625, mean reward: 0.576 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.474, 10.116], loss: 0.001476, mae: 0.040815, mean_q: 1.168163
 749327/1000000: episode: 7494, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 58.055, mean reward: 0.581 [0.499, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.493, 10.098], loss: 0.001444, mae: 0.040402, mean_q: 1.164081
 749427/1000000: episode: 7495, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.682, mean reward: 0.587 [0.511, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.591, 10.098], loss: 0.001413, mae: 0.040488, mean_q: 1.164083
 749527/1000000: episode: 7496, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 57.676, mean reward: 0.577 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.627, 10.098], loss: 0.001402, mae: 0.040539, mean_q: 1.165026
 749627/1000000: episode: 7497, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 58.317, mean reward: 0.583 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.582, 10.227], loss: 0.001419, mae: 0.040462, mean_q: 1.163877
 749727/1000000: episode: 7498, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 58.789, mean reward: 0.588 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.113], loss: 0.001454, mae: 0.040602, mean_q: 1.166269
 749827/1000000: episode: 7499, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 58.414, mean reward: 0.584 [0.503, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.811, 10.098], loss: 0.001419, mae: 0.040592, mean_q: 1.162383
 749927/1000000: episode: 7500, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 59.291, mean reward: 0.593 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.675, 10.098], loss: 0.001311, mae: 0.038789, mean_q: 1.160720
 750027/1000000: episode: 7501, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 59.292, mean reward: 0.593 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.861, 10.109], loss: 0.001396, mae: 0.040371, mean_q: 1.164854
 750127/1000000: episode: 7502, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.318, mean reward: 0.593 [0.498, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.983, 10.426], loss: 0.001399, mae: 0.040118, mean_q: 1.164477
 750227/1000000: episode: 7503, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 62.255, mean reward: 0.623 [0.507, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.901, 10.098], loss: 0.001399, mae: 0.040278, mean_q: 1.164818
 750327/1000000: episode: 7504, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 60.317, mean reward: 0.603 [0.512, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.327, 10.098], loss: 0.001438, mae: 0.040635, mean_q: 1.165232
 750427/1000000: episode: 7505, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 60.383, mean reward: 0.604 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.510, 10.098], loss: 0.001342, mae: 0.039929, mean_q: 1.163477
 750527/1000000: episode: 7506, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.037, mean reward: 0.580 [0.509, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.913, 10.098], loss: 0.001437, mae: 0.040676, mean_q: 1.164412
 750627/1000000: episode: 7507, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 57.079, mean reward: 0.571 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.475, 10.124], loss: 0.001365, mae: 0.040169, mean_q: 1.162154
 750727/1000000: episode: 7508, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 59.060, mean reward: 0.591 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.063, 10.098], loss: 0.001493, mae: 0.041089, mean_q: 1.164726
 750827/1000000: episode: 7509, duration: 1.387s, episode steps: 100, steps per second: 72, episode reward: 59.026, mean reward: 0.590 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.784, 10.098], loss: 0.001320, mae: 0.039624, mean_q: 1.163627
 750927/1000000: episode: 7510, duration: 1.392s, episode steps: 100, steps per second: 72, episode reward: 58.603, mean reward: 0.586 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.943, 10.098], loss: 0.001389, mae: 0.039948, mean_q: 1.165545
 751027/1000000: episode: 7511, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 56.938, mean reward: 0.569 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.351, 10.098], loss: 0.001283, mae: 0.038913, mean_q: 1.162740
 751127/1000000: episode: 7512, duration: 1.280s, episode steps: 100, steps per second: 78, episode reward: 57.634, mean reward: 0.576 [0.507, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.449, 10.182], loss: 0.001367, mae: 0.039771, mean_q: 1.164062
 751227/1000000: episode: 7513, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 60.090, mean reward: 0.601 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.271, 10.098], loss: 0.001551, mae: 0.042253, mean_q: 1.166973
 751327/1000000: episode: 7514, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 58.352, mean reward: 0.584 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.903, 10.098], loss: 0.001430, mae: 0.040867, mean_q: 1.162652
 751427/1000000: episode: 7515, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 57.219, mean reward: 0.572 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.523, 10.196], loss: 0.001433, mae: 0.040681, mean_q: 1.165666
 751527/1000000: episode: 7516, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 56.732, mean reward: 0.567 [0.501, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.171], loss: 0.001392, mae: 0.040613, mean_q: 1.161771
 751627/1000000: episode: 7517, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 59.370, mean reward: 0.594 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.592, 10.190], loss: 0.001402, mae: 0.039891, mean_q: 1.165147
 751727/1000000: episode: 7518, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 60.623, mean reward: 0.606 [0.499, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.656, 10.098], loss: 0.001451, mae: 0.041081, mean_q: 1.169236
 751827/1000000: episode: 7519, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 59.733, mean reward: 0.597 [0.506, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.535, 10.217], loss: 0.001318, mae: 0.039346, mean_q: 1.161183
 751927/1000000: episode: 7520, duration: 0.942s, episode steps: 100, steps per second: 106, episode reward: 59.531, mean reward: 0.595 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.467, 10.267], loss: 0.001392, mae: 0.040341, mean_q: 1.162160
 752027/1000000: episode: 7521, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 57.705, mean reward: 0.577 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.433, 10.251], loss: 0.001447, mae: 0.040809, mean_q: 1.165297
 752127/1000000: episode: 7522, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 57.560, mean reward: 0.576 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.961, 10.192], loss: 0.001390, mae: 0.040024, mean_q: 1.164046
 752227/1000000: episode: 7523, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 61.862, mean reward: 0.619 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.134, 10.098], loss: 0.001370, mae: 0.040302, mean_q: 1.166609
 752327/1000000: episode: 7524, duration: 0.948s, episode steps: 100, steps per second: 106, episode reward: 58.559, mean reward: 0.586 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.098], loss: 0.001435, mae: 0.040705, mean_q: 1.164604
 752427/1000000: episode: 7525, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 61.768, mean reward: 0.618 [0.511, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.535, 10.183], loss: 0.001353, mae: 0.039646, mean_q: 1.165420
 752527/1000000: episode: 7526, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 56.514, mean reward: 0.565 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.932, 10.114], loss: 0.001396, mae: 0.040826, mean_q: 1.166635
 752627/1000000: episode: 7527, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 57.853, mean reward: 0.579 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.324, 10.213], loss: 0.001425, mae: 0.040959, mean_q: 1.162811
 752727/1000000: episode: 7528, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 57.304, mean reward: 0.573 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.037, 10.098], loss: 0.001562, mae: 0.041921, mean_q: 1.164314
 752827/1000000: episode: 7529, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 59.279, mean reward: 0.593 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.002, 10.378], loss: 0.001303, mae: 0.039323, mean_q: 1.160873
 752927/1000000: episode: 7530, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 59.451, mean reward: 0.595 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.740, 10.098], loss: 0.001447, mae: 0.041424, mean_q: 1.164662
 753027/1000000: episode: 7531, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.721, mean reward: 0.577 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.539, 10.098], loss: 0.001571, mae: 0.042582, mean_q: 1.165593
 753127/1000000: episode: 7532, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 59.547, mean reward: 0.595 [0.500, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.406, 10.098], loss: 0.001449, mae: 0.041047, mean_q: 1.162176
 753227/1000000: episode: 7533, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 61.717, mean reward: 0.617 [0.531, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.098], loss: 0.001501, mae: 0.041468, mean_q: 1.160802
 753327/1000000: episode: 7534, duration: 0.941s, episode steps: 100, steps per second: 106, episode reward: 58.235, mean reward: 0.582 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.897, 10.098], loss: 0.001450, mae: 0.040707, mean_q: 1.162241
 753427/1000000: episode: 7535, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 57.345, mean reward: 0.573 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.736, 10.206], loss: 0.001404, mae: 0.040840, mean_q: 1.161599
 753527/1000000: episode: 7536, duration: 0.948s, episode steps: 100, steps per second: 105, episode reward: 58.351, mean reward: 0.584 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.511, 10.098], loss: 0.001523, mae: 0.042043, mean_q: 1.165507
 753627/1000000: episode: 7537, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 59.928, mean reward: 0.599 [0.512, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.892, 10.345], loss: 0.001554, mae: 0.041846, mean_q: 1.164856
 753727/1000000: episode: 7538, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 58.353, mean reward: 0.584 [0.499, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.240], loss: 0.001404, mae: 0.040370, mean_q: 1.163071
 753827/1000000: episode: 7539, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 59.025, mean reward: 0.590 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.715, 10.098], loss: 0.001538, mae: 0.041964, mean_q: 1.168187
 753927/1000000: episode: 7540, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 60.013, mean reward: 0.600 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.765, 10.098], loss: 0.001476, mae: 0.041163, mean_q: 1.164567
 754027/1000000: episode: 7541, duration: 0.957s, episode steps: 100, steps per second: 105, episode reward: 57.369, mean reward: 0.574 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.359, 10.206], loss: 0.001406, mae: 0.040362, mean_q: 1.163782
 754127/1000000: episode: 7542, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 61.476, mean reward: 0.615 [0.514, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.190], loss: 0.001456, mae: 0.041217, mean_q: 1.163696
 754227/1000000: episode: 7543, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 57.430, mean reward: 0.574 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.098], loss: 0.001408, mae: 0.040318, mean_q: 1.160330
 754327/1000000: episode: 7544, duration: 0.939s, episode steps: 100, steps per second: 107, episode reward: 58.379, mean reward: 0.584 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.568, 10.232], loss: 0.001507, mae: 0.042069, mean_q: 1.166173
 754427/1000000: episode: 7545, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 57.061, mean reward: 0.571 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.290, 10.167], loss: 0.001511, mae: 0.041804, mean_q: 1.166531
 754527/1000000: episode: 7546, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 60.761, mean reward: 0.608 [0.502, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.760, 10.098], loss: 0.001429, mae: 0.040724, mean_q: 1.166819
 754627/1000000: episode: 7547, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 56.972, mean reward: 0.570 [0.497, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.164], loss: 0.001411, mae: 0.040723, mean_q: 1.164223
 754727/1000000: episode: 7548, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 59.747, mean reward: 0.597 [0.502, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.614, 10.098], loss: 0.001512, mae: 0.041592, mean_q: 1.165491
 754827/1000000: episode: 7549, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 57.681, mean reward: 0.577 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.098], loss: 0.001439, mae: 0.040848, mean_q: 1.162141
 754927/1000000: episode: 7550, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 61.075, mean reward: 0.611 [0.519, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.290, 10.344], loss: 0.001449, mae: 0.040697, mean_q: 1.162850
 755027/1000000: episode: 7551, duration: 0.955s, episode steps: 100, steps per second: 105, episode reward: 58.547, mean reward: 0.585 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.029, 10.098], loss: 0.001526, mae: 0.042252, mean_q: 1.168555
 755127/1000000: episode: 7552, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 58.145, mean reward: 0.581 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.920, 10.121], loss: 0.001548, mae: 0.041824, mean_q: 1.166412
 755227/1000000: episode: 7553, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 61.238, mean reward: 0.612 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.337, 10.353], loss: 0.001538, mae: 0.041482, mean_q: 1.162969
 755327/1000000: episode: 7554, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 59.736, mean reward: 0.597 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.080, 10.367], loss: 0.001497, mae: 0.041154, mean_q: 1.162102
 755427/1000000: episode: 7555, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 58.431, mean reward: 0.584 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.074, 10.098], loss: 0.001644, mae: 0.043725, mean_q: 1.167531
 755527/1000000: episode: 7556, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 58.363, mean reward: 0.584 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.577, 10.098], loss: 0.001526, mae: 0.042177, mean_q: 1.165407
 755627/1000000: episode: 7557, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 57.835, mean reward: 0.578 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.078, 10.328], loss: 0.001469, mae: 0.041110, mean_q: 1.163084
 755727/1000000: episode: 7558, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 60.183, mean reward: 0.602 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.990, 10.331], loss: 0.001443, mae: 0.041218, mean_q: 1.166443
 755827/1000000: episode: 7559, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 63.380, mean reward: 0.634 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.621, 10.415], loss: 0.001464, mae: 0.041046, mean_q: 1.166535
 755927/1000000: episode: 7560, duration: 0.972s, episode steps: 100, steps per second: 103, episode reward: 57.295, mean reward: 0.573 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.574, 10.098], loss: 0.001553, mae: 0.042225, mean_q: 1.164893
 756027/1000000: episode: 7561, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 58.926, mean reward: 0.589 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.031, 10.098], loss: 0.001619, mae: 0.043013, mean_q: 1.169302
 756127/1000000: episode: 7562, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 60.335, mean reward: 0.603 [0.501, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.521, 10.355], loss: 0.001303, mae: 0.038952, mean_q: 1.165406
 756227/1000000: episode: 7563, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 57.697, mean reward: 0.577 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.482, 10.098], loss: 0.001429, mae: 0.041101, mean_q: 1.169429
 756327/1000000: episode: 7564, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 61.279, mean reward: 0.613 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.602, 10.380], loss: 0.001477, mae: 0.041575, mean_q: 1.168448
 756427/1000000: episode: 7565, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.464, mean reward: 0.595 [0.512, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.679, 10.098], loss: 0.001490, mae: 0.041253, mean_q: 1.170920
 756527/1000000: episode: 7566, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 60.267, mean reward: 0.603 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.872, 10.517], loss: 0.001456, mae: 0.041005, mean_q: 1.174114
 756627/1000000: episode: 7567, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 57.980, mean reward: 0.580 [0.516, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.413, 10.098], loss: 0.001486, mae: 0.041737, mean_q: 1.168001
 756727/1000000: episode: 7568, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 62.451, mean reward: 0.625 [0.523, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.644, 10.098], loss: 0.001419, mae: 0.040766, mean_q: 1.169735
 756827/1000000: episode: 7569, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 59.259, mean reward: 0.593 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.098], loss: 0.001434, mae: 0.040430, mean_q: 1.172918
 756927/1000000: episode: 7570, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 60.981, mean reward: 0.610 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.978, 10.098], loss: 0.001488, mae: 0.041416, mean_q: 1.172622
 757027/1000000: episode: 7571, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 59.171, mean reward: 0.592 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.132], loss: 0.001400, mae: 0.040414, mean_q: 1.171647
 757127/1000000: episode: 7572, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 60.393, mean reward: 0.604 [0.511, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.080, 10.373], loss: 0.001503, mae: 0.041361, mean_q: 1.173006
 757227/1000000: episode: 7573, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 59.420, mean reward: 0.594 [0.501, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.194, 10.170], loss: 0.001486, mae: 0.041296, mean_q: 1.175680
 757327/1000000: episode: 7574, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.489, mean reward: 0.575 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.855, 10.130], loss: 0.001405, mae: 0.040297, mean_q: 1.168525
 757427/1000000: episode: 7575, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 59.077, mean reward: 0.591 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.977, 10.234], loss: 0.001447, mae: 0.040778, mean_q: 1.171402
 757527/1000000: episode: 7576, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 58.086, mean reward: 0.581 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.911, 10.246], loss: 0.001479, mae: 0.040832, mean_q: 1.174044
 757627/1000000: episode: 7577, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 57.427, mean reward: 0.574 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.762, 10.169], loss: 0.001531, mae: 0.041707, mean_q: 1.171809
 757727/1000000: episode: 7578, duration: 0.925s, episode steps: 100, steps per second: 108, episode reward: 58.254, mean reward: 0.583 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.833, 10.098], loss: 0.001430, mae: 0.041175, mean_q: 1.168216
 757827/1000000: episode: 7579, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.178, mean reward: 0.582 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.140], loss: 0.001561, mae: 0.042662, mean_q: 1.170615
 757927/1000000: episode: 7580, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 59.184, mean reward: 0.592 [0.519, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.225, 10.098], loss: 0.001560, mae: 0.042077, mean_q: 1.172523
 758027/1000000: episode: 7581, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 57.885, mean reward: 0.579 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.077, 10.098], loss: 0.001567, mae: 0.042384, mean_q: 1.175597
 758127/1000000: episode: 7582, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 59.951, mean reward: 0.600 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.012, 10.306], loss: 0.001569, mae: 0.042326, mean_q: 1.168395
 758227/1000000: episode: 7583, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 57.850, mean reward: 0.579 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.034, 10.098], loss: 0.001578, mae: 0.042391, mean_q: 1.171021
 758327/1000000: episode: 7584, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 57.035, mean reward: 0.570 [0.503, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.131, 10.131], loss: 0.001403, mae: 0.040320, mean_q: 1.169387
 758427/1000000: episode: 7585, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 58.468, mean reward: 0.585 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.361, 10.203], loss: 0.001480, mae: 0.041300, mean_q: 1.171159
 758527/1000000: episode: 7586, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.902, mean reward: 0.599 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.376, 10.098], loss: 0.001425, mae: 0.040825, mean_q: 1.167265
 758627/1000000: episode: 7587, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 60.368, mean reward: 0.604 [0.511, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.294, 10.223], loss: 0.001472, mae: 0.041363, mean_q: 1.168427
 758727/1000000: episode: 7588, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: 57.418, mean reward: 0.574 [0.498, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.113, 10.133], loss: 0.001415, mae: 0.040615, mean_q: 1.167995
 758827/1000000: episode: 7589, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 65.008, mean reward: 0.650 [0.507, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.915, 10.404], loss: 0.001549, mae: 0.042001, mean_q: 1.170405
 758927/1000000: episode: 7590, duration: 0.954s, episode steps: 100, steps per second: 105, episode reward: 59.038, mean reward: 0.590 [0.498, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.494, 10.119], loss: 0.001560, mae: 0.041910, mean_q: 1.169427
 759027/1000000: episode: 7591, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: 58.088, mean reward: 0.581 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.351, 10.384], loss: 0.001486, mae: 0.041135, mean_q: 1.166219
 759127/1000000: episode: 7592, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 60.657, mean reward: 0.607 [0.511, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.946, 10.098], loss: 0.001429, mae: 0.040611, mean_q: 1.168859
 759227/1000000: episode: 7593, duration: 0.944s, episode steps: 100, steps per second: 106, episode reward: 56.639, mean reward: 0.566 [0.505, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.733, 10.098], loss: 0.001529, mae: 0.041878, mean_q: 1.173014
 759327/1000000: episode: 7594, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.496, mean reward: 0.585 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.482, 10.138], loss: 0.001483, mae: 0.041631, mean_q: 1.173973
 759427/1000000: episode: 7595, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 58.059, mean reward: 0.581 [0.498, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.732, 10.098], loss: 0.001557, mae: 0.042873, mean_q: 1.172174
 759527/1000000: episode: 7596, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 57.219, mean reward: 0.572 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.696, 10.200], loss: 0.001448, mae: 0.041221, mean_q: 1.171200
 759627/1000000: episode: 7597, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.050, mean reward: 0.571 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.906, 10.139], loss: 0.001540, mae: 0.042249, mean_q: 1.169043
 759727/1000000: episode: 7598, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 60.292, mean reward: 0.603 [0.510, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.429, 10.327], loss: 0.001523, mae: 0.041980, mean_q: 1.167785
 759827/1000000: episode: 7599, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.549, mean reward: 0.585 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.163], loss: 0.001547, mae: 0.042264, mean_q: 1.171592
 759927/1000000: episode: 7600, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 62.367, mean reward: 0.624 [0.503, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.935, 10.098], loss: 0.001491, mae: 0.041485, mean_q: 1.173201
 760027/1000000: episode: 7601, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 58.185, mean reward: 0.582 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.826, 10.101], loss: 0.001496, mae: 0.041093, mean_q: 1.168454
 760127/1000000: episode: 7602, duration: 0.953s, episode steps: 100, steps per second: 105, episode reward: 58.551, mean reward: 0.586 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.874, 10.212], loss: 0.001524, mae: 0.041961, mean_q: 1.171768
 760227/1000000: episode: 7603, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 59.109, mean reward: 0.591 [0.497, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-3.065, 10.125], loss: 0.001632, mae: 0.043149, mean_q: 1.167926
 760327/1000000: episode: 7604, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 58.426, mean reward: 0.584 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.791, 10.105], loss: 0.001629, mae: 0.043156, mean_q: 1.173350
 760427/1000000: episode: 7605, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 57.279, mean reward: 0.573 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.085, 10.098], loss: 0.001570, mae: 0.042881, mean_q: 1.169097
 760527/1000000: episode: 7606, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 58.366, mean reward: 0.584 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.109, 10.098], loss: 0.001545, mae: 0.042285, mean_q: 1.168136
 760627/1000000: episode: 7607, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 64.262, mean reward: 0.643 [0.546, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.608, 10.448], loss: 0.001464, mae: 0.041471, mean_q: 1.172386
 760727/1000000: episode: 7608, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.797, mean reward: 0.598 [0.513, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.212, 10.098], loss: 0.001522, mae: 0.041389, mean_q: 1.170317
 760827/1000000: episode: 7609, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 58.681, mean reward: 0.587 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.114, 10.238], loss: 0.001590, mae: 0.042303, mean_q: 1.164947
 760927/1000000: episode: 7610, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 59.440, mean reward: 0.594 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.281, 10.223], loss: 0.001506, mae: 0.041900, mean_q: 1.167265
 761027/1000000: episode: 7611, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 58.688, mean reward: 0.587 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.629, 10.098], loss: 0.001524, mae: 0.042335, mean_q: 1.168593
 761127/1000000: episode: 7612, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 59.238, mean reward: 0.592 [0.498, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.930, 10.098], loss: 0.001671, mae: 0.043386, mean_q: 1.168193
 761227/1000000: episode: 7613, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: 60.079, mean reward: 0.601 [0.503, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.484, 10.099], loss: 0.001688, mae: 0.044027, mean_q: 1.168615
 761327/1000000: episode: 7614, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 58.765, mean reward: 0.588 [0.508, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.156, 10.098], loss: 0.001646, mae: 0.043500, mean_q: 1.169253
 761427/1000000: episode: 7615, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 60.203, mean reward: 0.602 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.872, 10.098], loss: 0.001686, mae: 0.044145, mean_q: 1.169666
 761527/1000000: episode: 7616, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 61.570, mean reward: 0.616 [0.517, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.479, 10.098], loss: 0.001622, mae: 0.042816, mean_q: 1.168012
 761627/1000000: episode: 7617, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.509, mean reward: 0.585 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.484, 10.197], loss: 0.001573, mae: 0.042468, mean_q: 1.170640
 761727/1000000: episode: 7618, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 56.949, mean reward: 0.569 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.992, 10.103], loss: 0.001656, mae: 0.043872, mean_q: 1.167147
 761827/1000000: episode: 7619, duration: 1.117s, episode steps: 100, steps per second: 89, episode reward: 56.981, mean reward: 0.570 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.160, 10.098], loss: 0.001688, mae: 0.044627, mean_q: 1.166923
 761927/1000000: episode: 7620, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 57.765, mean reward: 0.578 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.713, 10.098], loss: 0.001588, mae: 0.043172, mean_q: 1.165907
 762027/1000000: episode: 7621, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 57.187, mean reward: 0.572 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.022, 10.139], loss: 0.001620, mae: 0.042982, mean_q: 1.167287
 762127/1000000: episode: 7622, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 61.704, mean reward: 0.617 [0.519, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.022, 10.098], loss: 0.001666, mae: 0.043008, mean_q: 1.165159
 762227/1000000: episode: 7623, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 56.806, mean reward: 0.568 [0.503, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.868, 10.098], loss: 0.001523, mae: 0.042273, mean_q: 1.164516
 762327/1000000: episode: 7624, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 61.335, mean reward: 0.613 [0.498, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.095, 10.255], loss: 0.001586, mae: 0.043186, mean_q: 1.166554
 762427/1000000: episode: 7625, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 57.259, mean reward: 0.573 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.168, 10.134], loss: 0.001665, mae: 0.043836, mean_q: 1.169154
 762527/1000000: episode: 7626, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 58.605, mean reward: 0.586 [0.516, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.715, 10.098], loss: 0.001591, mae: 0.043110, mean_q: 1.164640
 762627/1000000: episode: 7627, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 60.476, mean reward: 0.605 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.801, 10.481], loss: 0.001678, mae: 0.044103, mean_q: 1.166186
 762727/1000000: episode: 7628, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 57.614, mean reward: 0.576 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.099, 10.098], loss: 0.001505, mae: 0.041692, mean_q: 1.165666
 762827/1000000: episode: 7629, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 60.731, mean reward: 0.607 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.271, 10.098], loss: 0.001494, mae: 0.042139, mean_q: 1.169278
 762927/1000000: episode: 7630, duration: 1.471s, episode steps: 100, steps per second: 68, episode reward: 56.253, mean reward: 0.563 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.576, 10.118], loss: 0.001515, mae: 0.041787, mean_q: 1.165262
 763027/1000000: episode: 7631, duration: 1.796s, episode steps: 100, steps per second: 56, episode reward: 59.725, mean reward: 0.597 [0.517, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.365, 10.187], loss: 0.001505, mae: 0.041649, mean_q: 1.165176
 763127/1000000: episode: 7632, duration: 1.416s, episode steps: 100, steps per second: 71, episode reward: 57.790, mean reward: 0.578 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.376, 10.098], loss: 0.001499, mae: 0.041992, mean_q: 1.164471
 763227/1000000: episode: 7633, duration: 1.550s, episode steps: 100, steps per second: 65, episode reward: 58.070, mean reward: 0.581 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.301], loss: 0.001493, mae: 0.041108, mean_q: 1.164439
 763327/1000000: episode: 7634, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 57.866, mean reward: 0.579 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.843, 10.311], loss: 0.001579, mae: 0.042875, mean_q: 1.165805
 763427/1000000: episode: 7635, duration: 1.273s, episode steps: 100, steps per second: 79, episode reward: 57.885, mean reward: 0.579 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.302, 10.192], loss: 0.001680, mae: 0.043929, mean_q: 1.168710
 763527/1000000: episode: 7636, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 60.564, mean reward: 0.606 [0.509, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.772, 10.099], loss: 0.001562, mae: 0.042519, mean_q: 1.169134
 763627/1000000: episode: 7637, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 59.498, mean reward: 0.595 [0.510, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.151, 10.098], loss: 0.001472, mae: 0.041357, mean_q: 1.168024
 763727/1000000: episode: 7638, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 58.431, mean reward: 0.584 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.394, 10.098], loss: 0.001511, mae: 0.041942, mean_q: 1.166471
 763827/1000000: episode: 7639, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.187, mean reward: 0.582 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.230], loss: 0.001619, mae: 0.043283, mean_q: 1.164675
 763927/1000000: episode: 7640, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 58.405, mean reward: 0.584 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.448, 10.098], loss: 0.001632, mae: 0.043983, mean_q: 1.165206
 764027/1000000: episode: 7641, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 60.557, mean reward: 0.606 [0.518, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.500, 10.098], loss: 0.001453, mae: 0.041640, mean_q: 1.163652
 764127/1000000: episode: 7642, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 60.438, mean reward: 0.604 [0.500, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.240], loss: 0.001600, mae: 0.042669, mean_q: 1.162357
 764227/1000000: episode: 7643, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 58.176, mean reward: 0.582 [0.500, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.664, 10.098], loss: 0.001600, mae: 0.042601, mean_q: 1.166124
 764327/1000000: episode: 7644, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.917, mean reward: 0.579 [0.507, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.427, 10.146], loss: 0.001540, mae: 0.042160, mean_q: 1.164263
 764427/1000000: episode: 7645, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.226, mean reward: 0.582 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.408, 10.232], loss: 0.001670, mae: 0.043525, mean_q: 1.166908
 764527/1000000: episode: 7646, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.456, mean reward: 0.585 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.098], loss: 0.001588, mae: 0.042539, mean_q: 1.164753
 764627/1000000: episode: 7647, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 61.156, mean reward: 0.612 [0.504, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.502, 10.098], loss: 0.001639, mae: 0.043549, mean_q: 1.170572
 764727/1000000: episode: 7648, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 59.239, mean reward: 0.592 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.211, 10.098], loss: 0.001649, mae: 0.043694, mean_q: 1.168701
 764827/1000000: episode: 7649, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 57.377, mean reward: 0.574 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.743, 10.098], loss: 0.001601, mae: 0.043246, mean_q: 1.170749
 764927/1000000: episode: 7650, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: 60.001, mean reward: 0.600 [0.517, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.798, 10.203], loss: 0.001576, mae: 0.042767, mean_q: 1.166745
 765027/1000000: episode: 7651, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 59.043, mean reward: 0.590 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.510, 10.098], loss: 0.001555, mae: 0.042082, mean_q: 1.165980
 765127/1000000: episode: 7652, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 62.259, mean reward: 0.623 [0.498, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.671, 10.183], loss: 0.001426, mae: 0.040937, mean_q: 1.165239
 765227/1000000: episode: 7653, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 62.108, mean reward: 0.621 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.478, 10.530], loss: 0.001538, mae: 0.042151, mean_q: 1.168850
 765327/1000000: episode: 7654, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 57.374, mean reward: 0.574 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.465, 10.118], loss: 0.001522, mae: 0.042323, mean_q: 1.168589
 765427/1000000: episode: 7655, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.315, mean reward: 0.573 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.647, 10.231], loss: 0.001517, mae: 0.042237, mean_q: 1.166394
 765527/1000000: episode: 7656, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 57.036, mean reward: 0.570 [0.502, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.740, 10.106], loss: 0.001577, mae: 0.042815, mean_q: 1.168565
 765627/1000000: episode: 7657, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 62.469, mean reward: 0.625 [0.510, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.413, 10.287], loss: 0.001496, mae: 0.041690, mean_q: 1.168348
 765727/1000000: episode: 7658, duration: 1.691s, episode steps: 100, steps per second: 59, episode reward: 58.901, mean reward: 0.589 [0.502, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.253, 10.270], loss: 0.001662, mae: 0.043395, mean_q: 1.166282
 765827/1000000: episode: 7659, duration: 1.755s, episode steps: 100, steps per second: 57, episode reward: 58.083, mean reward: 0.581 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.313, 10.236], loss: 0.001401, mae: 0.040400, mean_q: 1.170039
 765927/1000000: episode: 7660, duration: 1.486s, episode steps: 100, steps per second: 67, episode reward: 56.779, mean reward: 0.568 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.690, 10.120], loss: 0.001475, mae: 0.041624, mean_q: 1.167580
 766027/1000000: episode: 7661, duration: 1.279s, episode steps: 100, steps per second: 78, episode reward: 58.744, mean reward: 0.587 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.182, 10.388], loss: 0.001499, mae: 0.041283, mean_q: 1.164376
 766127/1000000: episode: 7662, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 57.846, mean reward: 0.578 [0.505, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.902, 10.327], loss: 0.001460, mae: 0.041355, mean_q: 1.165038
 766227/1000000: episode: 7663, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 57.255, mean reward: 0.573 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.042, 10.148], loss: 0.001433, mae: 0.040724, mean_q: 1.166829
 766327/1000000: episode: 7664, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.421, mean reward: 0.574 [0.508, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.256, 10.241], loss: 0.001390, mae: 0.040075, mean_q: 1.165775
 766427/1000000: episode: 7665, duration: 0.966s, episode steps: 100, steps per second: 104, episode reward: 60.111, mean reward: 0.601 [0.508, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.765, 10.249], loss: 0.001523, mae: 0.041188, mean_q: 1.165551
 766527/1000000: episode: 7666, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 57.381, mean reward: 0.574 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.398, 10.115], loss: 0.001462, mae: 0.041368, mean_q: 1.160114
 766627/1000000: episode: 7667, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 58.363, mean reward: 0.584 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.525, 10.233], loss: 0.001437, mae: 0.041180, mean_q: 1.159818
 766727/1000000: episode: 7668, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 57.667, mean reward: 0.577 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.140], loss: 0.001411, mae: 0.040149, mean_q: 1.155200
 766827/1000000: episode: 7669, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 58.360, mean reward: 0.584 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.328], loss: 0.001439, mae: 0.040823, mean_q: 1.163715
 766927/1000000: episode: 7670, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 63.045, mean reward: 0.630 [0.510, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.449, 10.365], loss: 0.001437, mae: 0.040925, mean_q: 1.163135
 767027/1000000: episode: 7671, duration: 1.156s, episode steps: 100, steps per second: 87, episode reward: 59.588, mean reward: 0.596 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.098], loss: 0.001470, mae: 0.040905, mean_q: 1.165299
 767127/1000000: episode: 7672, duration: 1.610s, episode steps: 100, steps per second: 62, episode reward: 62.398, mean reward: 0.624 [0.517, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.822, 10.098], loss: 0.001416, mae: 0.040739, mean_q: 1.160619
 767227/1000000: episode: 7673, duration: 1.619s, episode steps: 100, steps per second: 62, episode reward: 57.600, mean reward: 0.576 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.516, 10.098], loss: 0.001473, mae: 0.041128, mean_q: 1.163675
 767327/1000000: episode: 7674, duration: 1.477s, episode steps: 100, steps per second: 68, episode reward: 57.700, mean reward: 0.577 [0.508, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.313, 10.118], loss: 0.001492, mae: 0.042048, mean_q: 1.162992
 767427/1000000: episode: 7675, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: 60.917, mean reward: 0.609 [0.505, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.024, 10.098], loss: 0.001456, mae: 0.041115, mean_q: 1.164830
 767527/1000000: episode: 7676, duration: 1.594s, episode steps: 100, steps per second: 63, episode reward: 58.009, mean reward: 0.580 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.098], loss: 0.001593, mae: 0.042792, mean_q: 1.169620
 767627/1000000: episode: 7677, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.685, mean reward: 0.587 [0.508, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.678, 10.284], loss: 0.001555, mae: 0.043009, mean_q: 1.169458
 767727/1000000: episode: 7678, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.330, mean reward: 0.583 [0.498, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.981, 10.290], loss: 0.001484, mae: 0.041405, mean_q: 1.164609
 767827/1000000: episode: 7679, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 58.404, mean reward: 0.584 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.936, 10.098], loss: 0.001404, mae: 0.040642, mean_q: 1.165244
 767927/1000000: episode: 7680, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 59.922, mean reward: 0.599 [0.519, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.513, 10.330], loss: 0.001433, mae: 0.040788, mean_q: 1.167497
 768027/1000000: episode: 7681, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 58.234, mean reward: 0.582 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.220, 10.324], loss: 0.001381, mae: 0.040461, mean_q: 1.164539
 768127/1000000: episode: 7682, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 60.654, mean reward: 0.607 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.128], loss: 0.001562, mae: 0.042849, mean_q: 1.170061
 768227/1000000: episode: 7683, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 60.304, mean reward: 0.603 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.472, 10.359], loss: 0.001462, mae: 0.041372, mean_q: 1.164220
 768327/1000000: episode: 7684, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.148, mean reward: 0.581 [0.518, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.173, 10.098], loss: 0.001442, mae: 0.041033, mean_q: 1.169369
 768427/1000000: episode: 7685, duration: 1.005s, episode steps: 100, steps per second: 99, episode reward: 59.148, mean reward: 0.591 [0.497, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.782, 10.149], loss: 0.001332, mae: 0.040104, mean_q: 1.166320
 768527/1000000: episode: 7686, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 59.734, mean reward: 0.597 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.479, 10.153], loss: 0.001418, mae: 0.040999, mean_q: 1.165031
 768627/1000000: episode: 7687, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 56.568, mean reward: 0.566 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.216], loss: 0.001429, mae: 0.041474, mean_q: 1.168427
 768727/1000000: episode: 7688, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 61.037, mean reward: 0.610 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.406, 10.370], loss: 0.001349, mae: 0.040221, mean_q: 1.165942
 768827/1000000: episode: 7689, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 56.879, mean reward: 0.569 [0.507, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.182, 10.101], loss: 0.001526, mae: 0.042903, mean_q: 1.167619
 768927/1000000: episode: 7690, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 58.386, mean reward: 0.584 [0.503, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.029, 10.249], loss: 0.001411, mae: 0.041123, mean_q: 1.170877
 769027/1000000: episode: 7691, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 59.595, mean reward: 0.596 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.857, 10.098], loss: 0.001414, mae: 0.041077, mean_q: 1.165545
 769127/1000000: episode: 7692, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 59.322, mean reward: 0.593 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.397, 10.098], loss: 0.001436, mae: 0.041422, mean_q: 1.171027
 769227/1000000: episode: 7693, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 56.882, mean reward: 0.569 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.855, 10.098], loss: 0.001385, mae: 0.041034, mean_q: 1.165617
 769327/1000000: episode: 7694, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 62.538, mean reward: 0.625 [0.521, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.242, 10.098], loss: 0.001345, mae: 0.039506, mean_q: 1.165485
 769427/1000000: episode: 7695, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 60.649, mean reward: 0.606 [0.501, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.509, 10.098], loss: 0.001419, mae: 0.041229, mean_q: 1.167240
 769527/1000000: episode: 7696, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 58.207, mean reward: 0.582 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.037, 10.098], loss: 0.001388, mae: 0.040649, mean_q: 1.167601
 769627/1000000: episode: 7697, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.856, mean reward: 0.599 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.295, 10.365], loss: 0.001359, mae: 0.040404, mean_q: 1.170676
 769727/1000000: episode: 7698, duration: 1.143s, episode steps: 100, steps per second: 88, episode reward: 57.085, mean reward: 0.571 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.047, 10.184], loss: 0.001373, mae: 0.040638, mean_q: 1.167966
 769827/1000000: episode: 7699, duration: 0.947s, episode steps: 100, steps per second: 106, episode reward: 59.909, mean reward: 0.599 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.248, 10.098], loss: 0.001488, mae: 0.042007, mean_q: 1.167785
 769927/1000000: episode: 7700, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 58.571, mean reward: 0.586 [0.510, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.441], loss: 0.001388, mae: 0.040538, mean_q: 1.169482
 770027/1000000: episode: 7701, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 58.160, mean reward: 0.582 [0.497, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.205], loss: 0.001419, mae: 0.040809, mean_q: 1.169165
 770127/1000000: episode: 7702, duration: 0.952s, episode steps: 100, steps per second: 105, episode reward: 57.686, mean reward: 0.577 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.845, 10.098], loss: 0.001431, mae: 0.041393, mean_q: 1.171185
 770227/1000000: episode: 7703, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 62.759, mean reward: 0.628 [0.517, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.896, 10.204], loss: 0.001371, mae: 0.040750, mean_q: 1.166931
 770327/1000000: episode: 7704, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 61.692, mean reward: 0.617 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.408, 10.426], loss: 0.001514, mae: 0.042096, mean_q: 1.169924
 770427/1000000: episode: 7705, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 59.176, mean reward: 0.592 [0.498, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.559, 10.104], loss: 0.001453, mae: 0.041576, mean_q: 1.169724
 770527/1000000: episode: 7706, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 57.981, mean reward: 0.580 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.805, 10.098], loss: 0.001420, mae: 0.040696, mean_q: 1.162138
 770627/1000000: episode: 7707, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 59.303, mean reward: 0.593 [0.505, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.973, 10.098], loss: 0.001400, mae: 0.041196, mean_q: 1.163320
 770727/1000000: episode: 7708, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 62.267, mean reward: 0.623 [0.499, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.399, 10.098], loss: 0.001475, mae: 0.041501, mean_q: 1.167130
 770827/1000000: episode: 7709, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 57.333, mean reward: 0.573 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.098], loss: 0.001502, mae: 0.041902, mean_q: 1.169155
 770927/1000000: episode: 7710, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 58.849, mean reward: 0.588 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.525, 10.219], loss: 0.001395, mae: 0.040538, mean_q: 1.170982
 771027/1000000: episode: 7711, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 59.352, mean reward: 0.594 [0.507, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.557, 10.098], loss: 0.001387, mae: 0.040780, mean_q: 1.166744
 771127/1000000: episode: 7712, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 58.913, mean reward: 0.589 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.098], loss: 0.001419, mae: 0.041412, mean_q: 1.170919
 771227/1000000: episode: 7713, duration: 1.643s, episode steps: 100, steps per second: 61, episode reward: 58.904, mean reward: 0.589 [0.499, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.406, 10.252], loss: 0.001466, mae: 0.041772, mean_q: 1.171706
 771327/1000000: episode: 7714, duration: 1.780s, episode steps: 100, steps per second: 56, episode reward: 58.100, mean reward: 0.581 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.135], loss: 0.001420, mae: 0.041049, mean_q: 1.172960
 771427/1000000: episode: 7715, duration: 1.638s, episode steps: 100, steps per second: 61, episode reward: 57.338, mean reward: 0.573 [0.515, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.701, 10.111], loss: 0.001508, mae: 0.042279, mean_q: 1.170380
 771527/1000000: episode: 7716, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 62.072, mean reward: 0.621 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.810, 10.098], loss: 0.001396, mae: 0.040791, mean_q: 1.169146
 771627/1000000: episode: 7717, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.509, mean reward: 0.585 [0.498, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.636, 10.114], loss: 0.001410, mae: 0.041050, mean_q: 1.172979
 771727/1000000: episode: 7718, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.015, mean reward: 0.580 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.616, 10.152], loss: 0.001392, mae: 0.040631, mean_q: 1.174476
 771827/1000000: episode: 7719, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 56.990, mean reward: 0.570 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.466, 10.237], loss: 0.001404, mae: 0.040673, mean_q: 1.170915
 771927/1000000: episode: 7720, duration: 1.442s, episode steps: 100, steps per second: 69, episode reward: 58.052, mean reward: 0.581 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.098], loss: 0.001418, mae: 0.040951, mean_q: 1.172545
 772027/1000000: episode: 7721, duration: 1.513s, episode steps: 100, steps per second: 66, episode reward: 58.955, mean reward: 0.590 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.740, 10.313], loss: 0.001367, mae: 0.040483, mean_q: 1.166491
 772127/1000000: episode: 7722, duration: 1.716s, episode steps: 100, steps per second: 58, episode reward: 59.540, mean reward: 0.595 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.282, 10.179], loss: 0.001436, mae: 0.041256, mean_q: 1.170882
 772227/1000000: episode: 7723, duration: 1.642s, episode steps: 100, steps per second: 61, episode reward: 65.732, mean reward: 0.657 [0.508, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.932, 10.098], loss: 0.001446, mae: 0.040765, mean_q: 1.170270
 772327/1000000: episode: 7724, duration: 1.673s, episode steps: 100, steps per second: 60, episode reward: 59.158, mean reward: 0.592 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.976, 10.238], loss: 0.001490, mae: 0.041575, mean_q: 1.174056
 772427/1000000: episode: 7725, duration: 1.673s, episode steps: 100, steps per second: 60, episode reward: 58.491, mean reward: 0.585 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.576, 10.190], loss: 0.001360, mae: 0.039870, mean_q: 1.170329
 772527/1000000: episode: 7726, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 57.500, mean reward: 0.575 [0.498, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.781, 10.098], loss: 0.001457, mae: 0.041460, mean_q: 1.172156
 772627/1000000: episode: 7727, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 59.448, mean reward: 0.594 [0.505, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.390, 10.098], loss: 0.001325, mae: 0.040002, mean_q: 1.170786
 772727/1000000: episode: 7728, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.600, mean reward: 0.586 [0.515, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.098], loss: 0.001470, mae: 0.041229, mean_q: 1.171632
 772827/1000000: episode: 7729, duration: 0.971s, episode steps: 100, steps per second: 103, episode reward: 57.592, mean reward: 0.576 [0.508, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.512, 10.176], loss: 0.001546, mae: 0.042617, mean_q: 1.173943
 772927/1000000: episode: 7730, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 58.865, mean reward: 0.589 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.650, 10.098], loss: 0.001548, mae: 0.042653, mean_q: 1.172143
 773027/1000000: episode: 7731, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 58.662, mean reward: 0.587 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.179], loss: 0.001587, mae: 0.043129, mean_q: 1.174414
 773127/1000000: episode: 7732, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 58.993, mean reward: 0.590 [0.508, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.485, 10.199], loss: 0.001504, mae: 0.041904, mean_q: 1.169431
 773227/1000000: episode: 7733, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 63.590, mean reward: 0.636 [0.536, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.436, 10.098], loss: 0.001517, mae: 0.041821, mean_q: 1.168815
 773327/1000000: episode: 7734, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 61.308, mean reward: 0.613 [0.509, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.850, 10.151], loss: 0.001416, mae: 0.040717, mean_q: 1.169636
 773427/1000000: episode: 7735, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 57.778, mean reward: 0.578 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.212, 10.098], loss: 0.001546, mae: 0.042439, mean_q: 1.172420
 773527/1000000: episode: 7736, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.334, mean reward: 0.593 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.888, 10.143], loss: 0.001565, mae: 0.042550, mean_q: 1.175131
 773627/1000000: episode: 7737, duration: 1.614s, episode steps: 100, steps per second: 62, episode reward: 60.351, mean reward: 0.604 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.680, 10.321], loss: 0.001479, mae: 0.042125, mean_q: 1.172452
 773727/1000000: episode: 7738, duration: 1.329s, episode steps: 100, steps per second: 75, episode reward: 60.324, mean reward: 0.603 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.400, 10.437], loss: 0.001492, mae: 0.042470, mean_q: 1.171782
 773827/1000000: episode: 7739, duration: 1.820s, episode steps: 100, steps per second: 55, episode reward: 58.048, mean reward: 0.580 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.742, 10.098], loss: 0.001474, mae: 0.041404, mean_q: 1.171120
 773927/1000000: episode: 7740, duration: 1.596s, episode steps: 100, steps per second: 63, episode reward: 58.677, mean reward: 0.587 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.278, 10.116], loss: 0.001454, mae: 0.041203, mean_q: 1.174046
 774027/1000000: episode: 7741, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.649, mean reward: 0.576 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.382], loss: 0.001458, mae: 0.041720, mean_q: 1.174286
 774127/1000000: episode: 7742, duration: 1.517s, episode steps: 100, steps per second: 66, episode reward: 58.314, mean reward: 0.583 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.400, 10.274], loss: 0.001558, mae: 0.043154, mean_q: 1.174671
 774227/1000000: episode: 7743, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 59.006, mean reward: 0.590 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.880, 10.098], loss: 0.001456, mae: 0.041182, mean_q: 1.174019
 774327/1000000: episode: 7744, duration: 0.970s, episode steps: 100, steps per second: 103, episode reward: 56.767, mean reward: 0.568 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.452, 10.160], loss: 0.001569, mae: 0.042946, mean_q: 1.175373
 774427/1000000: episode: 7745, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 64.262, mean reward: 0.643 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.098], loss: 0.001579, mae: 0.043223, mean_q: 1.174122
 774527/1000000: episode: 7746, duration: 1.326s, episode steps: 100, steps per second: 75, episode reward: 61.263, mean reward: 0.613 [0.501, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.394, 10.098], loss: 0.001556, mae: 0.042919, mean_q: 1.175953
 774627/1000000: episode: 7747, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 58.655, mean reward: 0.587 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.157, 10.115], loss: 0.001492, mae: 0.042058, mean_q: 1.174857
 774727/1000000: episode: 7748, duration: 1.264s, episode steps: 100, steps per second: 79, episode reward: 58.998, mean reward: 0.590 [0.505, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.773, 10.098], loss: 0.001446, mae: 0.041680, mean_q: 1.173513
 774827/1000000: episode: 7749, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 58.282, mean reward: 0.583 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.501, 10.101], loss: 0.001527, mae: 0.042431, mean_q: 1.171028
 774927/1000000: episode: 7750, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 61.698, mean reward: 0.617 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.139, 10.098], loss: 0.001497, mae: 0.042009, mean_q: 1.174909
 775027/1000000: episode: 7751, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 58.355, mean reward: 0.584 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.247, 10.247], loss: 0.001503, mae: 0.041642, mean_q: 1.175724
 775127/1000000: episode: 7752, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 58.854, mean reward: 0.589 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.881, 10.127], loss: 0.001464, mae: 0.041694, mean_q: 1.172795
 775227/1000000: episode: 7753, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 57.599, mean reward: 0.576 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.641, 10.142], loss: 0.001458, mae: 0.041584, mean_q: 1.170786
 775327/1000000: episode: 7754, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 59.655, mean reward: 0.597 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.098], loss: 0.001570, mae: 0.042691, mean_q: 1.173585
 775427/1000000: episode: 7755, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.691, mean reward: 0.587 [0.503, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.107], loss: 0.001589, mae: 0.042864, mean_q: 1.172260
 775527/1000000: episode: 7756, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 58.606, mean reward: 0.586 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.098], loss: 0.001485, mae: 0.041657, mean_q: 1.172713
 775627/1000000: episode: 7757, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 64.434, mean reward: 0.644 [0.505, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.426, 10.479], loss: 0.001602, mae: 0.043432, mean_q: 1.173717
 775727/1000000: episode: 7758, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 59.150, mean reward: 0.592 [0.510, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.615, 10.098], loss: 0.001416, mae: 0.040816, mean_q: 1.172717
 775827/1000000: episode: 7759, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 58.856, mean reward: 0.589 [0.500, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.370, 10.168], loss: 0.001410, mae: 0.040671, mean_q: 1.176302
 775927/1000000: episode: 7760, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 57.683, mean reward: 0.577 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.264, 10.098], loss: 0.001539, mae: 0.042516, mean_q: 1.176194
 776027/1000000: episode: 7761, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.971, mean reward: 0.590 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.107, 10.170], loss: 0.001452, mae: 0.041623, mean_q: 1.171894
 776127/1000000: episode: 7762, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 59.782, mean reward: 0.598 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.098], loss: 0.001383, mae: 0.040456, mean_q: 1.172785
 776227/1000000: episode: 7763, duration: 1.282s, episode steps: 100, steps per second: 78, episode reward: 59.706, mean reward: 0.597 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.191], loss: 0.001451, mae: 0.041405, mean_q: 1.172436
 776327/1000000: episode: 7764, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 57.839, mean reward: 0.578 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.732, 10.244], loss: 0.001423, mae: 0.041196, mean_q: 1.175378
 776427/1000000: episode: 7765, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 57.936, mean reward: 0.579 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.534, 10.160], loss: 0.001476, mae: 0.041588, mean_q: 1.174053
 776527/1000000: episode: 7766, duration: 1.488s, episode steps: 100, steps per second: 67, episode reward: 61.895, mean reward: 0.619 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.177, 10.145], loss: 0.001437, mae: 0.041021, mean_q: 1.173397
 776627/1000000: episode: 7767, duration: 1.586s, episode steps: 100, steps per second: 63, episode reward: 58.212, mean reward: 0.582 [0.508, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.179], loss: 0.001502, mae: 0.042254, mean_q: 1.172737
 776727/1000000: episode: 7768, duration: 1.906s, episode steps: 100, steps per second: 52, episode reward: 58.632, mean reward: 0.586 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.739, 10.243], loss: 0.001458, mae: 0.041576, mean_q: 1.171525
 776827/1000000: episode: 7769, duration: 1.643s, episode steps: 100, steps per second: 61, episode reward: 59.597, mean reward: 0.596 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.780, 10.460], loss: 0.001554, mae: 0.042867, mean_q: 1.174383
 776927/1000000: episode: 7770, duration: 1.595s, episode steps: 100, steps per second: 63, episode reward: 61.761, mean reward: 0.618 [0.501, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.989, 10.265], loss: 0.001569, mae: 0.042868, mean_q: 1.175115
 777027/1000000: episode: 7771, duration: 1.809s, episode steps: 100, steps per second: 55, episode reward: 60.802, mean reward: 0.608 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.273], loss: 0.001628, mae: 0.043508, mean_q: 1.180419
 777127/1000000: episode: 7772, duration: 1.831s, episode steps: 100, steps per second: 55, episode reward: 59.464, mean reward: 0.595 [0.506, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.516, 10.098], loss: 0.001507, mae: 0.042282, mean_q: 1.178718
 777227/1000000: episode: 7773, duration: 1.603s, episode steps: 100, steps per second: 62, episode reward: 57.322, mean reward: 0.573 [0.500, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.281, 10.133], loss: 0.001534, mae: 0.042557, mean_q: 1.178179
 777327/1000000: episode: 7774, duration: 1.359s, episode steps: 100, steps per second: 74, episode reward: 58.477, mean reward: 0.585 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.152, 10.098], loss: 0.001582, mae: 0.043255, mean_q: 1.176210
 777427/1000000: episode: 7775, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 58.459, mean reward: 0.585 [0.499, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.188, 10.098], loss: 0.001562, mae: 0.043319, mean_q: 1.172956
 777527/1000000: episode: 7776, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 57.827, mean reward: 0.578 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.491, 10.098], loss: 0.001470, mae: 0.041842, mean_q: 1.175739
 777627/1000000: episode: 7777, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.656, mean reward: 0.587 [0.511, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.689, 10.274], loss: 0.001644, mae: 0.043517, mean_q: 1.177134
 777727/1000000: episode: 7778, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 56.855, mean reward: 0.569 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.979, 10.144], loss: 0.001496, mae: 0.042240, mean_q: 1.174311
 777827/1000000: episode: 7779, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 57.693, mean reward: 0.577 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.950, 10.098], loss: 0.001466, mae: 0.042155, mean_q: 1.170643
 777927/1000000: episode: 7780, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.249, mean reward: 0.582 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.098], loss: 0.001557, mae: 0.042677, mean_q: 1.170394
 778027/1000000: episode: 7781, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 59.389, mean reward: 0.594 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.906, 10.098], loss: 0.001544, mae: 0.042892, mean_q: 1.175113
 778127/1000000: episode: 7782, duration: 0.961s, episode steps: 100, steps per second: 104, episode reward: 58.774, mean reward: 0.588 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.541, 10.098], loss: 0.001500, mae: 0.042331, mean_q: 1.171352
 778227/1000000: episode: 7783, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 57.707, mean reward: 0.577 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.331, 10.098], loss: 0.001497, mae: 0.042449, mean_q: 1.172669
 778327/1000000: episode: 7784, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 58.317, mean reward: 0.583 [0.504, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.630, 10.178], loss: 0.001508, mae: 0.042590, mean_q: 1.167676
 778427/1000000: episode: 7785, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 59.446, mean reward: 0.594 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.790, 10.326], loss: 0.001580, mae: 0.043329, mean_q: 1.170718
 778527/1000000: episode: 7786, duration: 0.984s, episode steps: 100, steps per second: 102, episode reward: 58.969, mean reward: 0.590 [0.513, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.802, 10.244], loss: 0.001457, mae: 0.041784, mean_q: 1.168734
 778627/1000000: episode: 7787, duration: 1.277s, episode steps: 100, steps per second: 78, episode reward: 58.678, mean reward: 0.587 [0.515, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.633, 10.098], loss: 0.001450, mae: 0.041278, mean_q: 1.170813
 778727/1000000: episode: 7788, duration: 1.419s, episode steps: 100, steps per second: 70, episode reward: 58.020, mean reward: 0.580 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.554, 10.166], loss: 0.001655, mae: 0.043731, mean_q: 1.167180
 778827/1000000: episode: 7789, duration: 1.376s, episode steps: 100, steps per second: 73, episode reward: 58.368, mean reward: 0.584 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.515, 10.377], loss: 0.001459, mae: 0.041324, mean_q: 1.165664
 778927/1000000: episode: 7790, duration: 1.760s, episode steps: 100, steps per second: 57, episode reward: 57.704, mean reward: 0.577 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.182], loss: 0.001570, mae: 0.043059, mean_q: 1.169647
 779027/1000000: episode: 7791, duration: 1.888s, episode steps: 100, steps per second: 53, episode reward: 60.158, mean reward: 0.602 [0.510, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.182], loss: 0.001594, mae: 0.043452, mean_q: 1.169150
 779127/1000000: episode: 7792, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: 61.971, mean reward: 0.620 [0.500, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.547, 10.098], loss: 0.001487, mae: 0.041951, mean_q: 1.170734
 779227/1000000: episode: 7793, duration: 1.706s, episode steps: 100, steps per second: 59, episode reward: 57.860, mean reward: 0.579 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.095, 10.111], loss: 0.001545, mae: 0.042389, mean_q: 1.170177
 779327/1000000: episode: 7794, duration: 1.778s, episode steps: 100, steps per second: 56, episode reward: 58.623, mean reward: 0.586 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.404, 10.275], loss: 0.001493, mae: 0.042546, mean_q: 1.167468
 779427/1000000: episode: 7795, duration: 1.608s, episode steps: 100, steps per second: 62, episode reward: 59.984, mean reward: 0.600 [0.507, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.151], loss: 0.001500, mae: 0.042167, mean_q: 1.169039
 779527/1000000: episode: 7796, duration: 1.286s, episode steps: 100, steps per second: 78, episode reward: 59.489, mean reward: 0.595 [0.515, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.466, 10.127], loss: 0.001569, mae: 0.042943, mean_q: 1.169628
 779627/1000000: episode: 7797, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 62.809, mean reward: 0.628 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.985, 10.447], loss: 0.001469, mae: 0.041353, mean_q: 1.166100
 779727/1000000: episode: 7798, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 56.856, mean reward: 0.569 [0.503, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.666, 10.120], loss: 0.001554, mae: 0.042792, mean_q: 1.170239
 779827/1000000: episode: 7799, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 57.836, mean reward: 0.578 [0.508, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.586, 10.098], loss: 0.001622, mae: 0.043534, mean_q: 1.169873
 779927/1000000: episode: 7800, duration: 1.026s, episode steps: 100, steps per second: 98, episode reward: 60.572, mean reward: 0.606 [0.515, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.005, 10.098], loss: 0.001546, mae: 0.042246, mean_q: 1.166757
 780027/1000000: episode: 7801, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.775, mean reward: 0.588 [0.497, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.239, 10.098], loss: 0.001547, mae: 0.042841, mean_q: 1.167794
 780127/1000000: episode: 7802, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 63.478, mean reward: 0.635 [0.507, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.259], loss: 0.001548, mae: 0.042526, mean_q: 1.168398
 780227/1000000: episode: 7803, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 58.387, mean reward: 0.584 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.768, 10.196], loss: 0.001557, mae: 0.042409, mean_q: 1.172624
 780327/1000000: episode: 7804, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 58.766, mean reward: 0.588 [0.511, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.624, 10.244], loss: 0.001558, mae: 0.043500, mean_q: 1.171225
 780427/1000000: episode: 7805, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.971, mean reward: 0.580 [0.512, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.037, 10.140], loss: 0.001594, mae: 0.043286, mean_q: 1.166576
 780527/1000000: episode: 7806, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 65.615, mean reward: 0.656 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.352, 10.270], loss: 0.001533, mae: 0.042118, mean_q: 1.167601
 780627/1000000: episode: 7807, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 59.805, mean reward: 0.598 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.964, 10.204], loss: 0.001631, mae: 0.043655, mean_q: 1.173769
 780727/1000000: episode: 7808, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 60.360, mean reward: 0.604 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.457, 10.098], loss: 0.001550, mae: 0.043195, mean_q: 1.171237
 780827/1000000: episode: 7809, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 58.705, mean reward: 0.587 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.264, 10.098], loss: 0.001594, mae: 0.042946, mean_q: 1.171575
 780927/1000000: episode: 7810, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 62.198, mean reward: 0.622 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.348, 10.098], loss: 0.001604, mae: 0.044127, mean_q: 1.171787
 781027/1000000: episode: 7811, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 61.498, mean reward: 0.615 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.182, 10.098], loss: 0.001551, mae: 0.042771, mean_q: 1.169599
 781127/1000000: episode: 7812, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 57.194, mean reward: 0.572 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.276, 10.116], loss: 0.001532, mae: 0.042593, mean_q: 1.171028
 781227/1000000: episode: 7813, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 57.984, mean reward: 0.580 [0.501, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.561, 10.261], loss: 0.001514, mae: 0.041616, mean_q: 1.169762
 781327/1000000: episode: 7814, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 56.942, mean reward: 0.569 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.245, 10.098], loss: 0.001564, mae: 0.042506, mean_q: 1.175375
 781427/1000000: episode: 7815, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 61.607, mean reward: 0.616 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.170, 10.098], loss: 0.001573, mae: 0.042821, mean_q: 1.172654
 781527/1000000: episode: 7816, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 57.288, mean reward: 0.573 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.737, 10.098], loss: 0.001576, mae: 0.043369, mean_q: 1.174731
 781627/1000000: episode: 7817, duration: 1.152s, episode steps: 100, steps per second: 87, episode reward: 57.367, mean reward: 0.574 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.885, 10.098], loss: 0.001450, mae: 0.042137, mean_q: 1.172372
 781727/1000000: episode: 7818, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 58.045, mean reward: 0.580 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.610, 10.306], loss: 0.001423, mae: 0.041244, mean_q: 1.169044
 781827/1000000: episode: 7819, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 60.644, mean reward: 0.606 [0.514, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.241], loss: 0.001531, mae: 0.042334, mean_q: 1.171571
 781927/1000000: episode: 7820, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 60.239, mean reward: 0.602 [0.513, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.617, 10.392], loss: 0.001549, mae: 0.042042, mean_q: 1.170819
 782027/1000000: episode: 7821, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 59.101, mean reward: 0.591 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.394, 10.098], loss: 0.001565, mae: 0.042499, mean_q: 1.173311
 782127/1000000: episode: 7822, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 58.150, mean reward: 0.582 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.339, 10.123], loss: 0.001439, mae: 0.041774, mean_q: 1.170118
 782227/1000000: episode: 7823, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 60.273, mean reward: 0.603 [0.511, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.802, 10.098], loss: 0.001422, mae: 0.041391, mean_q: 1.168532
 782327/1000000: episode: 7824, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 57.047, mean reward: 0.570 [0.499, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.265], loss: 0.001468, mae: 0.041665, mean_q: 1.171777
 782427/1000000: episode: 7825, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 59.507, mean reward: 0.595 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.512, 10.098], loss: 0.001441, mae: 0.041102, mean_q: 1.171195
 782527/1000000: episode: 7826, duration: 0.980s, episode steps: 100, steps per second: 102, episode reward: 58.450, mean reward: 0.584 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.197, 10.098], loss: 0.001449, mae: 0.041390, mean_q: 1.170507
 782627/1000000: episode: 7827, duration: 0.959s, episode steps: 100, steps per second: 104, episode reward: 58.136, mean reward: 0.581 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.112, 10.169], loss: 0.001410, mae: 0.041553, mean_q: 1.171307
 782727/1000000: episode: 7828, duration: 1.227s, episode steps: 100, steps per second: 81, episode reward: 57.777, mean reward: 0.578 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.098], loss: 0.001464, mae: 0.040990, mean_q: 1.169598
 782827/1000000: episode: 7829, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.166, mean reward: 0.592 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.996, 10.316], loss: 0.001450, mae: 0.041441, mean_q: 1.172429
 782927/1000000: episode: 7830, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 61.353, mean reward: 0.614 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.178, 10.312], loss: 0.001524, mae: 0.041742, mean_q: 1.172543
 783027/1000000: episode: 7831, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 59.168, mean reward: 0.592 [0.513, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.706, 10.232], loss: 0.001491, mae: 0.042174, mean_q: 1.172207
 783127/1000000: episode: 7832, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 57.653, mean reward: 0.577 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.321, 10.120], loss: 0.001462, mae: 0.041852, mean_q: 1.172143
 783227/1000000: episode: 7833, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 59.199, mean reward: 0.592 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.491, 10.098], loss: 0.001464, mae: 0.041310, mean_q: 1.174094
 783327/1000000: episode: 7834, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 59.085, mean reward: 0.591 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.098], loss: 0.001410, mae: 0.040604, mean_q: 1.175778
 783427/1000000: episode: 7835, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 57.998, mean reward: 0.580 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.964, 10.189], loss: 0.001530, mae: 0.042259, mean_q: 1.178212
 783527/1000000: episode: 7836, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 62.090, mean reward: 0.621 [0.516, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.293, 10.098], loss: 0.001499, mae: 0.042410, mean_q: 1.169507
 783627/1000000: episode: 7837, duration: 1.597s, episode steps: 100, steps per second: 63, episode reward: 59.791, mean reward: 0.598 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.098], loss: 0.001458, mae: 0.041644, mean_q: 1.173517
 783727/1000000: episode: 7838, duration: 1.696s, episode steps: 100, steps per second: 59, episode reward: 61.253, mean reward: 0.613 [0.501, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.021, 10.121], loss: 0.001488, mae: 0.041836, mean_q: 1.172242
 783827/1000000: episode: 7839, duration: 1.755s, episode steps: 100, steps per second: 57, episode reward: 58.209, mean reward: 0.582 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.960, 10.098], loss: 0.001458, mae: 0.041797, mean_q: 1.174100
 783927/1000000: episode: 7840, duration: 1.606s, episode steps: 100, steps per second: 62, episode reward: 58.281, mean reward: 0.583 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.641, 10.216], loss: 0.001441, mae: 0.041137, mean_q: 1.176161
 784027/1000000: episode: 7841, duration: 1.269s, episode steps: 100, steps per second: 79, episode reward: 58.129, mean reward: 0.581 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.408, 10.098], loss: 0.001448, mae: 0.041763, mean_q: 1.174326
 784127/1000000: episode: 7842, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 60.276, mean reward: 0.603 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.682, 10.229], loss: 0.001525, mae: 0.041979, mean_q: 1.175733
 784227/1000000: episode: 7843, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 63.836, mean reward: 0.638 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.098, 10.434], loss: 0.001448, mae: 0.041067, mean_q: 1.177053
 784327/1000000: episode: 7844, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 57.371, mean reward: 0.574 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.192], loss: 0.001487, mae: 0.041854, mean_q: 1.175344
 784427/1000000: episode: 7845, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 60.593, mean reward: 0.606 [0.510, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.429, 10.098], loss: 0.001444, mae: 0.041693, mean_q: 1.174109
 784527/1000000: episode: 7846, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 59.539, mean reward: 0.595 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.370, 10.174], loss: 0.001476, mae: 0.041269, mean_q: 1.176530
 784627/1000000: episode: 7847, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 62.015, mean reward: 0.620 [0.509, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.098], loss: 0.001463, mae: 0.041579, mean_q: 1.174976
 784727/1000000: episode: 7848, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 59.270, mean reward: 0.593 [0.509, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.147], loss: 0.001292, mae: 0.039273, mean_q: 1.171606
 784827/1000000: episode: 7849, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 58.175, mean reward: 0.582 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.035, 10.098], loss: 0.001356, mae: 0.040431, mean_q: 1.174199
 784927/1000000: episode: 7850, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 58.583, mean reward: 0.586 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.574, 10.135], loss: 0.001412, mae: 0.041231, mean_q: 1.176253
 785027/1000000: episode: 7851, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 58.252, mean reward: 0.583 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.121], loss: 0.001480, mae: 0.042325, mean_q: 1.177651
 785127/1000000: episode: 7852, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 57.383, mean reward: 0.574 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.362, 10.098], loss: 0.001449, mae: 0.041655, mean_q: 1.175530
 785227/1000000: episode: 7853, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 57.946, mean reward: 0.579 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.347, 10.140], loss: 0.001463, mae: 0.041530, mean_q: 1.176752
 785327/1000000: episode: 7854, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 57.446, mean reward: 0.574 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.179], loss: 0.001462, mae: 0.041289, mean_q: 1.176421
 785427/1000000: episode: 7855, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 65.005, mean reward: 0.650 [0.527, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.643, 10.098], loss: 0.001418, mae: 0.041531, mean_q: 1.174304
 785527/1000000: episode: 7856, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 57.889, mean reward: 0.579 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.300, 10.190], loss: 0.001435, mae: 0.041435, mean_q: 1.171409
 785627/1000000: episode: 7857, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 58.957, mean reward: 0.590 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.410, 10.098], loss: 0.001392, mae: 0.040747, mean_q: 1.173340
 785727/1000000: episode: 7858, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 56.975, mean reward: 0.570 [0.505, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.556, 10.215], loss: 0.001414, mae: 0.040925, mean_q: 1.171006
 785827/1000000: episode: 7859, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 62.160, mean reward: 0.622 [0.512, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.520, 10.234], loss: 0.001354, mae: 0.040199, mean_q: 1.170843
 785927/1000000: episode: 7860, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 57.908, mean reward: 0.579 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.098], loss: 0.001405, mae: 0.040644, mean_q: 1.175134
 786027/1000000: episode: 7861, duration: 1.510s, episode steps: 100, steps per second: 66, episode reward: 60.487, mean reward: 0.605 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.628, 10.098], loss: 0.001488, mae: 0.042034, mean_q: 1.172808
 786127/1000000: episode: 7862, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 59.125, mean reward: 0.591 [0.516, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.821, 10.098], loss: 0.001521, mae: 0.041948, mean_q: 1.173651
 786227/1000000: episode: 7863, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 57.042, mean reward: 0.570 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.449, 10.098], loss: 0.001460, mae: 0.041291, mean_q: 1.170189
 786327/1000000: episode: 7864, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.081, mean reward: 0.581 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.890, 10.098], loss: 0.001440, mae: 0.041273, mean_q: 1.173988
 786427/1000000: episode: 7865, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 58.306, mean reward: 0.583 [0.502, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.649, 10.098], loss: 0.001452, mae: 0.040993, mean_q: 1.169120
 786527/1000000: episode: 7866, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 62.000, mean reward: 0.620 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.058, 10.098], loss: 0.001472, mae: 0.042086, mean_q: 1.170507
 786627/1000000: episode: 7867, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 57.284, mean reward: 0.573 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.905, 10.265], loss: 0.001520, mae: 0.042035, mean_q: 1.173450
 786727/1000000: episode: 7868, duration: 1.015s, episode steps: 100, steps per second: 98, episode reward: 58.834, mean reward: 0.588 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.876, 10.257], loss: 0.001455, mae: 0.041584, mean_q: 1.173572
 786827/1000000: episode: 7869, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.933, mean reward: 0.589 [0.510, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.422, 10.098], loss: 0.001445, mae: 0.041325, mean_q: 1.174583
 786927/1000000: episode: 7870, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 58.998, mean reward: 0.590 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.439, 10.098], loss: 0.001464, mae: 0.041430, mean_q: 1.171639
 787027/1000000: episode: 7871, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.827, mean reward: 0.578 [0.500, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.215], loss: 0.001403, mae: 0.040611, mean_q: 1.172445
 787127/1000000: episode: 7872, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 57.986, mean reward: 0.580 [0.510, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.274, 10.098], loss: 0.001492, mae: 0.042134, mean_q: 1.172974
 787227/1000000: episode: 7873, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 58.309, mean reward: 0.583 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.525, 10.164], loss: 0.001490, mae: 0.041893, mean_q: 1.172850
 787327/1000000: episode: 7874, duration: 1.227s, episode steps: 100, steps per second: 81, episode reward: 59.275, mean reward: 0.593 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.131], loss: 0.001405, mae: 0.040888, mean_q: 1.168783
 787427/1000000: episode: 7875, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.387, mean reward: 0.584 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.246, 10.219], loss: 0.001464, mae: 0.041840, mean_q: 1.172093
 787527/1000000: episode: 7876, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 58.938, mean reward: 0.589 [0.516, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.083, 10.244], loss: 0.001450, mae: 0.041735, mean_q: 1.167528
 787627/1000000: episode: 7877, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 58.414, mean reward: 0.584 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.530, 10.224], loss: 0.001419, mae: 0.041480, mean_q: 1.169863
 787727/1000000: episode: 7878, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 58.423, mean reward: 0.584 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.174, 10.152], loss: 0.001425, mae: 0.041326, mean_q: 1.171811
 787827/1000000: episode: 7879, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 60.335, mean reward: 0.603 [0.503, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.054, 10.098], loss: 0.001428, mae: 0.041290, mean_q: 1.170501
 787927/1000000: episode: 7880, duration: 1.117s, episode steps: 100, steps per second: 89, episode reward: 57.788, mean reward: 0.578 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.678, 10.098], loss: 0.001496, mae: 0.041731, mean_q: 1.167871
 788027/1000000: episode: 7881, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 60.804, mean reward: 0.608 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.061, 10.330], loss: 0.001468, mae: 0.041646, mean_q: 1.173385
 788127/1000000: episode: 7882, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 58.444, mean reward: 0.584 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.361, 10.098], loss: 0.001405, mae: 0.040959, mean_q: 1.172233
 788227/1000000: episode: 7883, duration: 1.117s, episode steps: 100, steps per second: 89, episode reward: 58.415, mean reward: 0.584 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.721, 10.202], loss: 0.001517, mae: 0.042978, mean_q: 1.175748
 788327/1000000: episode: 7884, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.561, mean reward: 0.586 [0.500, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.119, 10.098], loss: 0.001411, mae: 0.040973, mean_q: 1.170358
 788427/1000000: episode: 7885, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.719, mean reward: 0.587 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.099], loss: 0.001323, mae: 0.039953, mean_q: 1.167917
 788527/1000000: episode: 7886, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 57.788, mean reward: 0.578 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.836, 10.193], loss: 0.001522, mae: 0.043179, mean_q: 1.170051
 788627/1000000: episode: 7887, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 58.232, mean reward: 0.582 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.178], loss: 0.001468, mae: 0.041210, mean_q: 1.167614
 788727/1000000: episode: 7888, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 58.006, mean reward: 0.580 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.321, 10.106], loss: 0.001556, mae: 0.042837, mean_q: 1.165273
 788827/1000000: episode: 7889, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 60.143, mean reward: 0.601 [0.501, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.477, 10.303], loss: 0.001474, mae: 0.042007, mean_q: 1.168252
 788927/1000000: episode: 7890, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 60.418, mean reward: 0.604 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.392, 10.206], loss: 0.001509, mae: 0.042006, mean_q: 1.165190
 789027/1000000: episode: 7891, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 57.564, mean reward: 0.576 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.002, 10.098], loss: 0.001491, mae: 0.042392, mean_q: 1.169033
 789127/1000000: episode: 7892, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 57.956, mean reward: 0.580 [0.507, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.455, 10.391], loss: 0.001476, mae: 0.041668, mean_q: 1.166412
 789227/1000000: episode: 7893, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 59.436, mean reward: 0.594 [0.521, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.098], loss: 0.001520, mae: 0.042486, mean_q: 1.168018
 789327/1000000: episode: 7894, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 59.035, mean reward: 0.590 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.053, 10.098], loss: 0.001600, mae: 0.043076, mean_q: 1.165347
 789427/1000000: episode: 7895, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 57.699, mean reward: 0.577 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.494, 10.229], loss: 0.001526, mae: 0.042837, mean_q: 1.163378
 789527/1000000: episode: 7896, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.256, mean reward: 0.593 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.556, 10.098], loss: 0.001565, mae: 0.042981, mean_q: 1.164795
 789627/1000000: episode: 7897, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 60.718, mean reward: 0.607 [0.509, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.051, 10.098], loss: 0.001441, mae: 0.041297, mean_q: 1.161206
 789727/1000000: episode: 7898, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 60.646, mean reward: 0.606 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.758, 10.098], loss: 0.001468, mae: 0.041783, mean_q: 1.166290
 789827/1000000: episode: 7899, duration: 1.279s, episode steps: 100, steps per second: 78, episode reward: 56.142, mean reward: 0.561 [0.502, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.563, 10.098], loss: 0.001472, mae: 0.042267, mean_q: 1.160556
 789927/1000000: episode: 7900, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 57.333, mean reward: 0.573 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.303, 10.272], loss: 0.001552, mae: 0.042960, mean_q: 1.166475
 790027/1000000: episode: 7901, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 59.613, mean reward: 0.596 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.800, 10.277], loss: 0.001461, mae: 0.041175, mean_q: 1.163123
 790127/1000000: episode: 7902, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 57.018, mean reward: 0.570 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.667, 10.120], loss: 0.001610, mae: 0.043756, mean_q: 1.166047
 790227/1000000: episode: 7903, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 60.153, mean reward: 0.602 [0.510, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.774, 10.308], loss: 0.001523, mae: 0.042441, mean_q: 1.164497
 790327/1000000: episode: 7904, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.412, mean reward: 0.584 [0.512, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.305, 10.247], loss: 0.001489, mae: 0.041902, mean_q: 1.163298
 790427/1000000: episode: 7905, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 60.134, mean reward: 0.601 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.464, 10.098], loss: 0.001559, mae: 0.043056, mean_q: 1.164763
 790527/1000000: episode: 7906, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 58.575, mean reward: 0.586 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.304, 10.098], loss: 0.001591, mae: 0.043453, mean_q: 1.160487
 790627/1000000: episode: 7907, duration: 1.069s, episode steps: 100, steps per second: 94, episode reward: 57.672, mean reward: 0.577 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.215, 10.265], loss: 0.001491, mae: 0.041445, mean_q: 1.162344
 790727/1000000: episode: 7908, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 60.112, mean reward: 0.601 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.477], loss: 0.001477, mae: 0.042217, mean_q: 1.163237
 790827/1000000: episode: 7909, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 59.940, mean reward: 0.599 [0.514, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.058, 10.397], loss: 0.001459, mae: 0.041834, mean_q: 1.159912
 790927/1000000: episode: 7910, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 58.482, mean reward: 0.585 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.981, 10.171], loss: 0.001423, mae: 0.041172, mean_q: 1.163948
 791027/1000000: episode: 7911, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 58.613, mean reward: 0.586 [0.510, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.442, 10.098], loss: 0.001460, mae: 0.041573, mean_q: 1.162737
 791127/1000000: episode: 7912, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.439, mean reward: 0.594 [0.503, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.218], loss: 0.001428, mae: 0.041597, mean_q: 1.165770
 791227/1000000: episode: 7913, duration: 1.955s, episode steps: 100, steps per second: 51, episode reward: 57.944, mean reward: 0.579 [0.513, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.828, 10.098], loss: 0.001549, mae: 0.042777, mean_q: 1.167622
 791327/1000000: episode: 7914, duration: 1.782s, episode steps: 100, steps per second: 56, episode reward: 57.483, mean reward: 0.575 [0.503, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.967, 10.322], loss: 0.001404, mae: 0.041287, mean_q: 1.167300
 791427/1000000: episode: 7915, duration: 1.652s, episode steps: 100, steps per second: 61, episode reward: 57.878, mean reward: 0.579 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.098], loss: 0.001417, mae: 0.041591, mean_q: 1.161686
 791527/1000000: episode: 7916, duration: 1.418s, episode steps: 100, steps per second: 71, episode reward: 60.863, mean reward: 0.609 [0.518, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.098], loss: 0.001434, mae: 0.041776, mean_q: 1.159798
 791627/1000000: episode: 7917, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 57.228, mean reward: 0.572 [0.499, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.767, 10.098], loss: 0.001516, mae: 0.042463, mean_q: 1.164193
 791727/1000000: episode: 7918, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.360, mean reward: 0.584 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.271, 10.145], loss: 0.001428, mae: 0.041042, mean_q: 1.162006
 791827/1000000: episode: 7919, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 60.197, mean reward: 0.602 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.170, 10.098], loss: 0.001526, mae: 0.042446, mean_q: 1.161160
 791927/1000000: episode: 7920, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 57.833, mean reward: 0.578 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.491, 10.098], loss: 0.001413, mae: 0.040921, mean_q: 1.164160
 792027/1000000: episode: 7921, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 59.418, mean reward: 0.594 [0.508, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.527, 10.147], loss: 0.001454, mae: 0.041572, mean_q: 1.165618
 792127/1000000: episode: 7922, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 68.074, mean reward: 0.681 [0.525, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.152, 10.098], loss: 0.001486, mae: 0.042102, mean_q: 1.162765
 792227/1000000: episode: 7923, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 57.811, mean reward: 0.578 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.818, 10.098], loss: 0.001458, mae: 0.041710, mean_q: 1.164345
 792327/1000000: episode: 7924, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 59.852, mean reward: 0.599 [0.519, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.268, 10.098], loss: 0.001425, mae: 0.041224, mean_q: 1.165928
 792427/1000000: episode: 7925, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 61.396, mean reward: 0.614 [0.520, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.864, 10.098], loss: 0.001501, mae: 0.042301, mean_q: 1.166357
 792527/1000000: episode: 7926, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 57.652, mean reward: 0.577 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.784, 10.098], loss: 0.001396, mae: 0.041198, mean_q: 1.166196
 792627/1000000: episode: 7927, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.529, mean reward: 0.585 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.943, 10.225], loss: 0.001478, mae: 0.041398, mean_q: 1.165240
 792727/1000000: episode: 7928, duration: 1.143s, episode steps: 100, steps per second: 88, episode reward: 57.718, mean reward: 0.577 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.635, 10.098], loss: 0.001491, mae: 0.042248, mean_q: 1.165558
 792827/1000000: episode: 7929, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 57.495, mean reward: 0.575 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.639, 10.177], loss: 0.001380, mae: 0.040627, mean_q: 1.162821
 792927/1000000: episode: 7930, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 57.914, mean reward: 0.579 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.522, 10.098], loss: 0.001472, mae: 0.041738, mean_q: 1.166070
 793027/1000000: episode: 7931, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 58.614, mean reward: 0.586 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.493, 10.299], loss: 0.001453, mae: 0.041572, mean_q: 1.166230
 793127/1000000: episode: 7932, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 57.456, mean reward: 0.575 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.782, 10.179], loss: 0.001501, mae: 0.041565, mean_q: 1.163243
 793227/1000000: episode: 7933, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 56.926, mean reward: 0.569 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.618, 10.098], loss: 0.001550, mae: 0.042933, mean_q: 1.166191
 793327/1000000: episode: 7934, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 58.850, mean reward: 0.588 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.724, 10.187], loss: 0.001368, mae: 0.040564, mean_q: 1.162956
 793427/1000000: episode: 7935, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 57.589, mean reward: 0.576 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.780, 10.098], loss: 0.001420, mae: 0.041123, mean_q: 1.162319
 793527/1000000: episode: 7936, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 58.906, mean reward: 0.589 [0.526, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.351], loss: 0.001364, mae: 0.040621, mean_q: 1.163218
 793627/1000000: episode: 7937, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 56.633, mean reward: 0.566 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.807, 10.098], loss: 0.001390, mae: 0.040553, mean_q: 1.163120
 793727/1000000: episode: 7938, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 58.312, mean reward: 0.583 [0.508, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.258, 10.380], loss: 0.001468, mae: 0.041524, mean_q: 1.165946
 793827/1000000: episode: 7939, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 58.101, mean reward: 0.581 [0.497, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.996, 10.098], loss: 0.001344, mae: 0.040253, mean_q: 1.166905
 793927/1000000: episode: 7940, duration: 1.274s, episode steps: 100, steps per second: 78, episode reward: 59.489, mean reward: 0.595 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.979, 10.301], loss: 0.001353, mae: 0.040164, mean_q: 1.165603
 794027/1000000: episode: 7941, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 59.280, mean reward: 0.593 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.006, 10.098], loss: 0.001415, mae: 0.040786, mean_q: 1.161770
 794127/1000000: episode: 7942, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 59.141, mean reward: 0.591 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.936, 10.098], loss: 0.001417, mae: 0.040900, mean_q: 1.164451
 794227/1000000: episode: 7943, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 58.451, mean reward: 0.585 [0.510, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.973, 10.281], loss: 0.001397, mae: 0.040641, mean_q: 1.162817
 794327/1000000: episode: 7944, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 58.144, mean reward: 0.581 [0.502, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.150, 10.286], loss: 0.001391, mae: 0.040961, mean_q: 1.161974
 794427/1000000: episode: 7945, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 60.445, mean reward: 0.604 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.322, 10.133], loss: 0.001377, mae: 0.040634, mean_q: 1.163095
 794527/1000000: episode: 7946, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 58.433, mean reward: 0.584 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.802, 10.184], loss: 0.001389, mae: 0.041002, mean_q: 1.164451
 794627/1000000: episode: 7947, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.620, mean reward: 0.586 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.695, 10.366], loss: 0.001401, mae: 0.040702, mean_q: 1.163810
 794727/1000000: episode: 7948, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 58.036, mean reward: 0.580 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.411, 10.158], loss: 0.001392, mae: 0.040503, mean_q: 1.159850
 794827/1000000: episode: 7949, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.307, mean reward: 0.583 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.693, 10.119], loss: 0.001367, mae: 0.040735, mean_q: 1.163547
 794927/1000000: episode: 7950, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 57.022, mean reward: 0.570 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.534, 10.383], loss: 0.001390, mae: 0.040409, mean_q: 1.162735
 795027/1000000: episode: 7951, duration: 1.143s, episode steps: 100, steps per second: 88, episode reward: 57.865, mean reward: 0.579 [0.503, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.228, 10.098], loss: 0.001376, mae: 0.039948, mean_q: 1.159109
 795127/1000000: episode: 7952, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.540, mean reward: 0.595 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.878, 10.098], loss: 0.001395, mae: 0.040953, mean_q: 1.159120
 795227/1000000: episode: 7953, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 58.558, mean reward: 0.586 [0.498, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.489, 10.268], loss: 0.001301, mae: 0.039755, mean_q: 1.162471
 795327/1000000: episode: 7954, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 58.581, mean reward: 0.586 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.939, 10.175], loss: 0.001358, mae: 0.040690, mean_q: 1.161585
 795427/1000000: episode: 7955, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 59.885, mean reward: 0.599 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.211, 10.098], loss: 0.001465, mae: 0.042268, mean_q: 1.164025
 795527/1000000: episode: 7956, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 63.603, mean reward: 0.636 [0.507, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.929, 10.098], loss: 0.001329, mae: 0.039846, mean_q: 1.165939
 795627/1000000: episode: 7957, duration: 1.143s, episode steps: 100, steps per second: 88, episode reward: 62.225, mean reward: 0.622 [0.507, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.890, 10.359], loss: 0.001406, mae: 0.041731, mean_q: 1.168242
 795727/1000000: episode: 7958, duration: 0.976s, episode steps: 100, steps per second: 102, episode reward: 59.105, mean reward: 0.591 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.500, 10.174], loss: 0.001390, mae: 0.041320, mean_q: 1.168560
 795827/1000000: episode: 7959, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 58.892, mean reward: 0.589 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.571, 10.098], loss: 0.001356, mae: 0.040724, mean_q: 1.167394
 795927/1000000: episode: 7960, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 57.421, mean reward: 0.574 [0.509, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.151, 10.224], loss: 0.001370, mae: 0.040473, mean_q: 1.167981
 796027/1000000: episode: 7961, duration: 1.770s, episode steps: 100, steps per second: 56, episode reward: 55.574, mean reward: 0.556 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.739, 10.157], loss: 0.001513, mae: 0.042274, mean_q: 1.167937
 796127/1000000: episode: 7962, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 58.260, mean reward: 0.583 [0.508, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.793, 10.098], loss: 0.001454, mae: 0.041625, mean_q: 1.165049
 796227/1000000: episode: 7963, duration: 1.390s, episode steps: 100, steps per second: 72, episode reward: 58.063, mean reward: 0.581 [0.510, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.832, 10.139], loss: 0.001430, mae: 0.041534, mean_q: 1.162698
 796327/1000000: episode: 7964, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.320, mean reward: 0.573 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.438, 10.123], loss: 0.001423, mae: 0.041166, mean_q: 1.167834
 796427/1000000: episode: 7965, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 58.863, mean reward: 0.589 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.473, 10.183], loss: 0.001442, mae: 0.041853, mean_q: 1.164346
 796527/1000000: episode: 7966, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 64.996, mean reward: 0.650 [0.508, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.568, 10.098], loss: 0.001397, mae: 0.040789, mean_q: 1.163201
 796627/1000000: episode: 7967, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 61.134, mean reward: 0.611 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.336, 10.291], loss: 0.001422, mae: 0.041277, mean_q: 1.165987
 796727/1000000: episode: 7968, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 59.101, mean reward: 0.591 [0.499, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.098], loss: 0.001391, mae: 0.040699, mean_q: 1.168155
 796827/1000000: episode: 7969, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 61.759, mean reward: 0.618 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.290, 10.228], loss: 0.001413, mae: 0.040813, mean_q: 1.166696
 796927/1000000: episode: 7970, duration: 1.282s, episode steps: 100, steps per second: 78, episode reward: 59.226, mean reward: 0.592 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.976, 10.244], loss: 0.001384, mae: 0.040386, mean_q: 1.166498
 797027/1000000: episode: 7971, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 57.921, mean reward: 0.579 [0.515, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.530, 10.367], loss: 0.001358, mae: 0.040615, mean_q: 1.171127
 797127/1000000: episode: 7972, duration: 1.344s, episode steps: 100, steps per second: 74, episode reward: 58.804, mean reward: 0.588 [0.504, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.098], loss: 0.001338, mae: 0.039736, mean_q: 1.168886
 797227/1000000: episode: 7973, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 61.195, mean reward: 0.612 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.216], loss: 0.001359, mae: 0.040301, mean_q: 1.168427
 797327/1000000: episode: 7974, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 57.747, mean reward: 0.577 [0.501, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.725, 10.118], loss: 0.001367, mae: 0.040174, mean_q: 1.166369
 797427/1000000: episode: 7975, duration: 1.870s, episode steps: 100, steps per second: 53, episode reward: 59.313, mean reward: 0.593 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.728, 10.098], loss: 0.001356, mae: 0.040366, mean_q: 1.162010
 797527/1000000: episode: 7976, duration: 1.578s, episode steps: 100, steps per second: 63, episode reward: 59.203, mean reward: 0.592 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.310, 10.363], loss: 0.001360, mae: 0.040089, mean_q: 1.161489
 797627/1000000: episode: 7977, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 62.669, mean reward: 0.627 [0.509, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.744, 10.098], loss: 0.001341, mae: 0.039775, mean_q: 1.162717
 797727/1000000: episode: 7978, duration: 1.431s, episode steps: 100, steps per second: 70, episode reward: 57.939, mean reward: 0.579 [0.500, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.098], loss: 0.001382, mae: 0.039816, mean_q: 1.165544
 797827/1000000: episode: 7979, duration: 2.126s, episode steps: 100, steps per second: 47, episode reward: 62.250, mean reward: 0.622 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.640, 10.098], loss: 0.001273, mae: 0.038859, mean_q: 1.163482
 797927/1000000: episode: 7980, duration: 1.736s, episode steps: 100, steps per second: 58, episode reward: 63.462, mean reward: 0.635 [0.504, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.379, 10.485], loss: 0.001419, mae: 0.041205, mean_q: 1.174089
 798027/1000000: episode: 7981, duration: 1.806s, episode steps: 100, steps per second: 55, episode reward: 57.874, mean reward: 0.579 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.649, 10.151], loss: 0.001372, mae: 0.040518, mean_q: 1.170662
 798127/1000000: episode: 7982, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: 59.506, mean reward: 0.595 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.511, 10.098], loss: 0.001452, mae: 0.041527, mean_q: 1.174146
 798227/1000000: episode: 7983, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 59.753, mean reward: 0.598 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.583, 10.209], loss: 0.001395, mae: 0.041016, mean_q: 1.171552
 798327/1000000: episode: 7984, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 58.077, mean reward: 0.581 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.098], loss: 0.001413, mae: 0.041246, mean_q: 1.175422
 798427/1000000: episode: 7985, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 57.986, mean reward: 0.580 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.216, 10.174], loss: 0.001330, mae: 0.039846, mean_q: 1.167753
 798527/1000000: episode: 7986, duration: 1.334s, episode steps: 100, steps per second: 75, episode reward: 62.336, mean reward: 0.623 [0.516, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.099, 10.406], loss: 0.001441, mae: 0.041534, mean_q: 1.176243
 798627/1000000: episode: 7987, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 56.384, mean reward: 0.564 [0.497, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.772, 10.242], loss: 0.001420, mae: 0.041366, mean_q: 1.173030
 798727/1000000: episode: 7988, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 57.517, mean reward: 0.575 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.609, 10.186], loss: 0.001327, mae: 0.039911, mean_q: 1.172775
 798827/1000000: episode: 7989, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 57.209, mean reward: 0.572 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.742, 10.098], loss: 0.001346, mae: 0.040172, mean_q: 1.173473
 798927/1000000: episode: 7990, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 58.176, mean reward: 0.582 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.626, 10.098], loss: 0.001362, mae: 0.040372, mean_q: 1.172210
 799027/1000000: episode: 7991, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 62.246, mean reward: 0.622 [0.513, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.557, 10.098], loss: 0.001366, mae: 0.040097, mean_q: 1.174135
 799127/1000000: episode: 7992, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 62.775, mean reward: 0.628 [0.502, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.545, 10.206], loss: 0.001392, mae: 0.040587, mean_q: 1.177095
 799227/1000000: episode: 7993, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 61.440, mean reward: 0.614 [0.512, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.326, 10.319], loss: 0.001460, mae: 0.041750, mean_q: 1.175284
 799327/1000000: episode: 7994, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 60.165, mean reward: 0.602 [0.508, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.449, 10.130], loss: 0.001458, mae: 0.041261, mean_q: 1.176631
 799427/1000000: episode: 7995, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 60.432, mean reward: 0.604 [0.508, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.154, 10.143], loss: 0.001448, mae: 0.041833, mean_q: 1.180701
 799527/1000000: episode: 7996, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 61.143, mean reward: 0.611 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.440, 10.192], loss: 0.001342, mae: 0.040466, mean_q: 1.177691
 799627/1000000: episode: 7997, duration: 1.096s, episode steps: 100, steps per second: 91, episode reward: 58.808, mean reward: 0.588 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.385, 10.098], loss: 0.001449, mae: 0.041069, mean_q: 1.177003
 799727/1000000: episode: 7998, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 61.754, mean reward: 0.618 [0.511, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.012, 10.298], loss: 0.001520, mae: 0.042347, mean_q: 1.182292
 799827/1000000: episode: 7999, duration: 1.369s, episode steps: 100, steps per second: 73, episode reward: 57.368, mean reward: 0.574 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.216, 10.098], loss: 0.001396, mae: 0.040789, mean_q: 1.176317
 799927/1000000: episode: 8000, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 57.888, mean reward: 0.579 [0.504, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.949, 10.098], loss: 0.001395, mae: 0.040360, mean_q: 1.179408
 800027/1000000: episode: 8001, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 57.409, mean reward: 0.574 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.799, 10.175], loss: 0.001418, mae: 0.041158, mean_q: 1.180334
 800127/1000000: episode: 8002, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 57.726, mean reward: 0.577 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.239], loss: 0.001467, mae: 0.041734, mean_q: 1.178853
 800227/1000000: episode: 8003, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 61.150, mean reward: 0.612 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.251, 10.164], loss: 0.001429, mae: 0.040842, mean_q: 1.178822
 800327/1000000: episode: 8004, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.895, mean reward: 0.599 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.365, 10.098], loss: 0.001503, mae: 0.042423, mean_q: 1.183816
 800427/1000000: episode: 8005, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 57.010, mean reward: 0.570 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.516, 10.151], loss: 0.001419, mae: 0.041300, mean_q: 1.176880
 800527/1000000: episode: 8006, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 59.498, mean reward: 0.595 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.359, 10.098], loss: 0.001423, mae: 0.041647, mean_q: 1.179836
 800627/1000000: episode: 8007, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 56.965, mean reward: 0.570 [0.506, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.924, 10.284], loss: 0.001353, mae: 0.039845, mean_q: 1.173037
 800727/1000000: episode: 8008, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 59.932, mean reward: 0.599 [0.517, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.688, 10.098], loss: 0.001406, mae: 0.040886, mean_q: 1.174032
 800827/1000000: episode: 8009, duration: 1.258s, episode steps: 100, steps per second: 80, episode reward: 61.021, mean reward: 0.610 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.110, 10.206], loss: 0.001459, mae: 0.041571, mean_q: 1.176857
 800927/1000000: episode: 8010, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.594, mean reward: 0.586 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.124, 10.098], loss: 0.001569, mae: 0.042554, mean_q: 1.179699
 801027/1000000: episode: 8011, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 60.276, mean reward: 0.603 [0.509, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.235, 10.551], loss: 0.001377, mae: 0.040029, mean_q: 1.177545
 801127/1000000: episode: 8012, duration: 1.594s, episode steps: 100, steps per second: 63, episode reward: 57.414, mean reward: 0.574 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.248, 10.116], loss: 0.001485, mae: 0.041697, mean_q: 1.176029
 801227/1000000: episode: 8013, duration: 1.650s, episode steps: 100, steps per second: 61, episode reward: 58.706, mean reward: 0.587 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.656, 10.171], loss: 0.001452, mae: 0.040764, mean_q: 1.179171
 801327/1000000: episode: 8014, duration: 1.705s, episode steps: 100, steps per second: 59, episode reward: 63.137, mean reward: 0.631 [0.529, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.045, 10.098], loss: 0.001508, mae: 0.041673, mean_q: 1.181354
 801427/1000000: episode: 8015, duration: 1.609s, episode steps: 100, steps per second: 62, episode reward: 59.056, mean reward: 0.591 [0.506, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.195, 10.373], loss: 0.001471, mae: 0.041772, mean_q: 1.179499
 801527/1000000: episode: 8016, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 60.918, mean reward: 0.609 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.420, 10.098], loss: 0.001575, mae: 0.042870, mean_q: 1.179423
 801627/1000000: episode: 8017, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 57.729, mean reward: 0.577 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.264, 10.306], loss: 0.001547, mae: 0.042460, mean_q: 1.178796
 801727/1000000: episode: 8018, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 58.787, mean reward: 0.588 [0.500, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.152, 10.172], loss: 0.001526, mae: 0.041832, mean_q: 1.174249
 801827/1000000: episode: 8019, duration: 1.293s, episode steps: 100, steps per second: 77, episode reward: 59.495, mean reward: 0.595 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.332, 10.098], loss: 0.001507, mae: 0.041919, mean_q: 1.176536
 801927/1000000: episode: 8020, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 56.184, mean reward: 0.562 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.479, 10.098], loss: 0.001571, mae: 0.042029, mean_q: 1.176096
 802027/1000000: episode: 8021, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 59.360, mean reward: 0.594 [0.505, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.912, 10.220], loss: 0.001455, mae: 0.041174, mean_q: 1.175270
 802127/1000000: episode: 8022, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 58.752, mean reward: 0.588 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.194, 10.098], loss: 0.001351, mae: 0.040045, mean_q: 1.173779
 802227/1000000: episode: 8023, duration: 1.269s, episode steps: 100, steps per second: 79, episode reward: 58.553, mean reward: 0.586 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.394, 10.098], loss: 0.001399, mae: 0.040233, mean_q: 1.171600
 802327/1000000: episode: 8024, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 61.363, mean reward: 0.614 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.857, 10.098], loss: 0.001417, mae: 0.041282, mean_q: 1.177647
 802427/1000000: episode: 8025, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 59.168, mean reward: 0.592 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.316, 10.174], loss: 0.001490, mae: 0.041837, mean_q: 1.177524
 802527/1000000: episode: 8026, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 59.135, mean reward: 0.591 [0.513, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.099, 10.098], loss: 0.001508, mae: 0.042450, mean_q: 1.178625
 802627/1000000: episode: 8027, duration: 1.286s, episode steps: 100, steps per second: 78, episode reward: 57.287, mean reward: 0.573 [0.510, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.193, 10.244], loss: 0.001493, mae: 0.041474, mean_q: 1.176634
 802727/1000000: episode: 8028, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 64.173, mean reward: 0.642 [0.512, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.098], loss: 0.001501, mae: 0.041685, mean_q: 1.177788
 802827/1000000: episode: 8029, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 59.492, mean reward: 0.595 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.098], loss: 0.001382, mae: 0.040354, mean_q: 1.177610
 802927/1000000: episode: 8030, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 57.654, mean reward: 0.577 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.098], loss: 0.001514, mae: 0.041798, mean_q: 1.176947
 803027/1000000: episode: 8031, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.467, mean reward: 0.585 [0.498, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.098], loss: 0.001478, mae: 0.040943, mean_q: 1.177912
 803127/1000000: episode: 8032, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 56.840, mean reward: 0.568 [0.506, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.751, 10.098], loss: 0.001415, mae: 0.040586, mean_q: 1.172993
 803227/1000000: episode: 8033, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.226, mean reward: 0.582 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.429, 10.099], loss: 0.001436, mae: 0.041352, mean_q: 1.168657
 803327/1000000: episode: 8034, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 56.954, mean reward: 0.570 [0.497, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.450, 10.098], loss: 0.001444, mae: 0.041311, mean_q: 1.170695
 803427/1000000: episode: 8035, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 60.123, mean reward: 0.601 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.098], loss: 0.001431, mae: 0.040715, mean_q: 1.171818
 803527/1000000: episode: 8036, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 59.189, mean reward: 0.592 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.914, 10.111], loss: 0.001496, mae: 0.041974, mean_q: 1.173229
 803627/1000000: episode: 8037, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 57.873, mean reward: 0.579 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.007, 10.098], loss: 0.001490, mae: 0.041522, mean_q: 1.172459
 803727/1000000: episode: 8038, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.326, mean reward: 0.583 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.822, 10.296], loss: 0.001472, mae: 0.041073, mean_q: 1.170588
 803827/1000000: episode: 8039, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 59.599, mean reward: 0.596 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.864, 10.271], loss: 0.001434, mae: 0.040909, mean_q: 1.168267
 803927/1000000: episode: 8040, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 56.701, mean reward: 0.567 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.545, 10.098], loss: 0.001468, mae: 0.041889, mean_q: 1.172711
 804027/1000000: episode: 8041, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 58.905, mean reward: 0.589 [0.513, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.638, 10.098], loss: 0.001391, mae: 0.040507, mean_q: 1.169303
 804127/1000000: episode: 8042, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 60.263, mean reward: 0.603 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.885, 10.353], loss: 0.001492, mae: 0.041451, mean_q: 1.168557
 804227/1000000: episode: 8043, duration: 0.990s, episode steps: 100, steps per second: 101, episode reward: 56.536, mean reward: 0.565 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.990, 10.098], loss: 0.001579, mae: 0.042782, mean_q: 1.170997
 804327/1000000: episode: 8044, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.921, mean reward: 0.589 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.289, 10.098], loss: 0.001432, mae: 0.041083, mean_q: 1.167339
 804427/1000000: episode: 8045, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 56.634, mean reward: 0.566 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.884, 10.098], loss: 0.001555, mae: 0.042558, mean_q: 1.167803
 804527/1000000: episode: 8046, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 57.938, mean reward: 0.579 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.262, 10.098], loss: 0.001287, mae: 0.038906, mean_q: 1.164530
 804627/1000000: episode: 8047, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 59.192, mean reward: 0.592 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.636, 10.098], loss: 0.001432, mae: 0.041103, mean_q: 1.166283
 804727/1000000: episode: 8048, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 60.393, mean reward: 0.604 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.927, 10.098], loss: 0.001434, mae: 0.040993, mean_q: 1.162330
 804827/1000000: episode: 8049, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.941, mean reward: 0.589 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.385, 10.098], loss: 0.001410, mae: 0.040355, mean_q: 1.164616
 804927/1000000: episode: 8050, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.765, mean reward: 0.588 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.763, 10.098], loss: 0.001533, mae: 0.042328, mean_q: 1.166981
 805027/1000000: episode: 8051, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 57.994, mean reward: 0.580 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.308], loss: 0.001474, mae: 0.041177, mean_q: 1.168562
 805127/1000000: episode: 8052, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 59.745, mean reward: 0.597 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.771, 10.098], loss: 0.001526, mae: 0.041906, mean_q: 1.166260
 805227/1000000: episode: 8053, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 57.651, mean reward: 0.577 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.997, 10.104], loss: 0.001468, mae: 0.040804, mean_q: 1.167160
 805327/1000000: episode: 8054, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 58.981, mean reward: 0.590 [0.510, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.466, 10.260], loss: 0.001389, mae: 0.040073, mean_q: 1.160467
 805427/1000000: episode: 8055, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 59.455, mean reward: 0.595 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.705, 10.098], loss: 0.001536, mae: 0.041637, mean_q: 1.162924
 805527/1000000: episode: 8056, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.374, mean reward: 0.584 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.449, 10.098], loss: 0.001441, mae: 0.040242, mean_q: 1.164914
 805627/1000000: episode: 8057, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 58.354, mean reward: 0.584 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.119], loss: 0.001453, mae: 0.040930, mean_q: 1.164532
 805727/1000000: episode: 8058, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 58.751, mean reward: 0.588 [0.507, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.835, 10.358], loss: 0.001334, mae: 0.039590, mean_q: 1.162490
 805827/1000000: episode: 8059, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 57.501, mean reward: 0.575 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.568, 10.149], loss: 0.001409, mae: 0.040116, mean_q: 1.161419
 805927/1000000: episode: 8060, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 61.176, mean reward: 0.612 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.048, 10.098], loss: 0.001501, mae: 0.041629, mean_q: 1.163808
 806027/1000000: episode: 8061, duration: 1.128s, episode steps: 100, steps per second: 89, episode reward: 58.703, mean reward: 0.587 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.488, 10.224], loss: 0.001456, mae: 0.040953, mean_q: 1.164537
 806127/1000000: episode: 8062, duration: 1.283s, episode steps: 100, steps per second: 78, episode reward: 62.301, mean reward: 0.623 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.843, 10.142], loss: 0.001422, mae: 0.040301, mean_q: 1.162949
 806227/1000000: episode: 8063, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 56.688, mean reward: 0.567 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.428, 10.098], loss: 0.001515, mae: 0.041493, mean_q: 1.165612
 806327/1000000: episode: 8064, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 57.791, mean reward: 0.578 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.169], loss: 0.001429, mae: 0.040704, mean_q: 1.160980
 806427/1000000: episode: 8065, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 58.309, mean reward: 0.583 [0.503, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.082, 10.176], loss: 0.001388, mae: 0.040358, mean_q: 1.164094
 806527/1000000: episode: 8066, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.389, mean reward: 0.584 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.195, 10.098], loss: 0.001451, mae: 0.041283, mean_q: 1.162516
 806627/1000000: episode: 8067, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 60.258, mean reward: 0.603 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.634, 10.098], loss: 0.001456, mae: 0.041118, mean_q: 1.165182
 806727/1000000: episode: 8068, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 57.478, mean reward: 0.575 [0.509, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.927, 10.137], loss: 0.001428, mae: 0.040631, mean_q: 1.165537
 806827/1000000: episode: 8069, duration: 1.626s, episode steps: 100, steps per second: 62, episode reward: 57.037, mean reward: 0.570 [0.500, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: 0.001381, mae: 0.039557, mean_q: 1.160812
 806927/1000000: episode: 8070, duration: 1.639s, episode steps: 100, steps per second: 61, episode reward: 62.852, mean reward: 0.629 [0.519, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.098], loss: 0.001288, mae: 0.038855, mean_q: 1.162075
 807027/1000000: episode: 8071, duration: 1.718s, episode steps: 100, steps per second: 58, episode reward: 57.998, mean reward: 0.580 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.098], loss: 0.001351, mae: 0.039105, mean_q: 1.161712
 807127/1000000: episode: 8072, duration: 1.366s, episode steps: 100, steps per second: 73, episode reward: 59.573, mean reward: 0.596 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.188, 10.098], loss: 0.001292, mae: 0.039208, mean_q: 1.160311
 807227/1000000: episode: 8073, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.760, mean reward: 0.578 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.478, 10.214], loss: 0.001407, mae: 0.040159, mean_q: 1.161458
 807327/1000000: episode: 8074, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 59.417, mean reward: 0.594 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.841, 10.098], loss: 0.001466, mae: 0.041020, mean_q: 1.160055
 807427/1000000: episode: 8075, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 61.304, mean reward: 0.613 [0.507, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.098], loss: 0.001392, mae: 0.040411, mean_q: 1.165545
 807527/1000000: episode: 8076, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 58.073, mean reward: 0.581 [0.497, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.514, 10.098], loss: 0.001550, mae: 0.042109, mean_q: 1.168265
 807627/1000000: episode: 8077, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 59.940, mean reward: 0.599 [0.513, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.502, 10.103], loss: 0.001434, mae: 0.041080, mean_q: 1.164846
 807727/1000000: episode: 8078, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 59.402, mean reward: 0.594 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.720, 10.098], loss: 0.001463, mae: 0.040898, mean_q: 1.162969
 807827/1000000: episode: 8079, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 60.172, mean reward: 0.602 [0.508, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.973, 10.359], loss: 0.001502, mae: 0.041839, mean_q: 1.166280
 807927/1000000: episode: 8080, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 57.900, mean reward: 0.579 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.738, 10.098], loss: 0.001461, mae: 0.041298, mean_q: 1.163385
 808027/1000000: episode: 8081, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 62.320, mean reward: 0.623 [0.503, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.368, 10.392], loss: 0.001513, mae: 0.041696, mean_q: 1.165003
 808127/1000000: episode: 8082, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 62.847, mean reward: 0.628 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.572, 10.467], loss: 0.001523, mae: 0.041693, mean_q: 1.167205
 808227/1000000: episode: 8083, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 58.189, mean reward: 0.582 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.924, 10.098], loss: 0.001529, mae: 0.041659, mean_q: 1.169240
 808327/1000000: episode: 8084, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 59.991, mean reward: 0.600 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.598, 10.098], loss: 0.001372, mae: 0.040209, mean_q: 1.166313
 808427/1000000: episode: 8085, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 59.773, mean reward: 0.598 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.896, 10.098], loss: 0.001460, mae: 0.041162, mean_q: 1.168826
 808527/1000000: episode: 8086, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 58.192, mean reward: 0.582 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.502, 10.269], loss: 0.001398, mae: 0.040600, mean_q: 1.165670
 808627/1000000: episode: 8087, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.140, mean reward: 0.581 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.890, 10.218], loss: 0.001424, mae: 0.040529, mean_q: 1.168200
 808727/1000000: episode: 8088, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 58.546, mean reward: 0.585 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.967, 10.147], loss: 0.001514, mae: 0.042283, mean_q: 1.167788
 808827/1000000: episode: 8089, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.305, mean reward: 0.573 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.938, 10.098], loss: 0.001534, mae: 0.041965, mean_q: 1.166407
 808927/1000000: episode: 8090, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 62.201, mean reward: 0.622 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.897, 10.098], loss: 0.001402, mae: 0.040928, mean_q: 1.165615
 809027/1000000: episode: 8091, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 57.810, mean reward: 0.578 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.098], loss: 0.001452, mae: 0.041213, mean_q: 1.167451
 809127/1000000: episode: 8092, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 56.317, mean reward: 0.563 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.979, 10.098], loss: 0.001475, mae: 0.041721, mean_q: 1.171944
 809227/1000000: episode: 8093, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.173, mean reward: 0.582 [0.497, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.650, 10.324], loss: 0.001458, mae: 0.041289, mean_q: 1.167193
 809327/1000000: episode: 8094, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 58.890, mean reward: 0.589 [0.508, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.508, 10.098], loss: 0.001375, mae: 0.040431, mean_q: 1.168882
 809427/1000000: episode: 8095, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 58.619, mean reward: 0.586 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.608, 10.183], loss: 0.001460, mae: 0.041151, mean_q: 1.170306
 809527/1000000: episode: 8096, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 60.302, mean reward: 0.603 [0.503, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.232, 10.098], loss: 0.001512, mae: 0.041926, mean_q: 1.171829
 809627/1000000: episode: 8097, duration: 1.450s, episode steps: 100, steps per second: 69, episode reward: 56.923, mean reward: 0.569 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.713, 10.098], loss: 0.001464, mae: 0.041571, mean_q: 1.168998
 809727/1000000: episode: 8098, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 61.678, mean reward: 0.617 [0.509, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.849, 10.234], loss: 0.001358, mae: 0.040124, mean_q: 1.168859
 809827/1000000: episode: 8099, duration: 1.143s, episode steps: 100, steps per second: 87, episode reward: 58.798, mean reward: 0.588 [0.510, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.429, 10.235], loss: 0.001561, mae: 0.042216, mean_q: 1.168909
 809927/1000000: episode: 8100, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 58.166, mean reward: 0.582 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.990, 10.248], loss: 0.001395, mae: 0.040017, mean_q: 1.168688
 810027/1000000: episode: 8101, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 57.542, mean reward: 0.575 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.586, 10.158], loss: 0.001401, mae: 0.040564, mean_q: 1.164693
 810127/1000000: episode: 8102, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 59.166, mean reward: 0.592 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.837, 10.199], loss: 0.001492, mae: 0.041840, mean_q: 1.171555
 810227/1000000: episode: 8103, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 58.339, mean reward: 0.583 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.860, 10.151], loss: 0.001497, mae: 0.041833, mean_q: 1.169486
 810327/1000000: episode: 8104, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 57.998, mean reward: 0.580 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.891, 10.264], loss: 0.001373, mae: 0.040370, mean_q: 1.168360
 810427/1000000: episode: 8105, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 58.490, mean reward: 0.585 [0.509, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.963, 10.098], loss: 0.001493, mae: 0.042612, mean_q: 1.168601
 810527/1000000: episode: 8106, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 61.209, mean reward: 0.612 [0.514, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.713, 10.462], loss: 0.001356, mae: 0.040112, mean_q: 1.170542
 810627/1000000: episode: 8107, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 56.296, mean reward: 0.563 [0.505, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.666, 10.098], loss: 0.001464, mae: 0.041313, mean_q: 1.166982
 810727/1000000: episode: 8108, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 58.519, mean reward: 0.585 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.312], loss: 0.001352, mae: 0.039902, mean_q: 1.164800
 810827/1000000: episode: 8109, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 57.153, mean reward: 0.572 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.159], loss: 0.001430, mae: 0.041356, mean_q: 1.172336
 810927/1000000: episode: 8110, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 60.346, mean reward: 0.603 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.721, 10.119], loss: 0.001445, mae: 0.040801, mean_q: 1.169256
 811027/1000000: episode: 8111, duration: 1.258s, episode steps: 100, steps per second: 80, episode reward: 56.839, mean reward: 0.568 [0.510, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.879, 10.235], loss: 0.001400, mae: 0.040749, mean_q: 1.168429
 811127/1000000: episode: 8112, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 57.126, mean reward: 0.571 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.707, 10.144], loss: 0.001296, mae: 0.039248, mean_q: 1.164438
 811227/1000000: episode: 8113, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 60.197, mean reward: 0.602 [0.501, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.420, 10.333], loss: 0.001384, mae: 0.040032, mean_q: 1.160801
 811327/1000000: episode: 8114, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 59.667, mean reward: 0.597 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.395, 10.176], loss: 0.001391, mae: 0.040069, mean_q: 1.165693
 811427/1000000: episode: 8115, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: 57.645, mean reward: 0.576 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.565, 10.178], loss: 0.001390, mae: 0.041016, mean_q: 1.165321
 811527/1000000: episode: 8116, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 58.309, mean reward: 0.583 [0.498, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.490, 10.098], loss: 0.001414, mae: 0.041426, mean_q: 1.169338
 811627/1000000: episode: 8117, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 59.531, mean reward: 0.595 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.350, 10.242], loss: 0.001431, mae: 0.041312, mean_q: 1.163530
 811727/1000000: episode: 8118, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 56.100, mean reward: 0.561 [0.498, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.420, 10.127], loss: 0.001353, mae: 0.040451, mean_q: 1.167090
 811827/1000000: episode: 8119, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 58.317, mean reward: 0.583 [0.498, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.041, 10.098], loss: 0.001356, mae: 0.039868, mean_q: 1.167594
 811927/1000000: episode: 8120, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 58.388, mean reward: 0.584 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.082, 10.194], loss: 0.001418, mae: 0.040641, mean_q: 1.162639
 812027/1000000: episode: 8121, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.986, mean reward: 0.590 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 0.001335, mae: 0.039798, mean_q: 1.164959
 812127/1000000: episode: 8122, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 61.069, mean reward: 0.611 [0.510, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.792, 10.372], loss: 0.001315, mae: 0.039556, mean_q: 1.164560
 812227/1000000: episode: 8123, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 60.408, mean reward: 0.604 [0.499, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.446, 10.098], loss: 0.001438, mae: 0.041599, mean_q: 1.162217
 812327/1000000: episode: 8124, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 60.003, mean reward: 0.600 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.774, 10.243], loss: 0.001422, mae: 0.040718, mean_q: 1.166044
 812427/1000000: episode: 8125, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 60.390, mean reward: 0.604 [0.498, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.341, 10.322], loss: 0.001429, mae: 0.040956, mean_q: 1.168408
 812527/1000000: episode: 8126, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 60.632, mean reward: 0.606 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.098], loss: 0.001533, mae: 0.042221, mean_q: 1.168739
 812627/1000000: episode: 8127, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 60.586, mean reward: 0.606 [0.512, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.812, 10.147], loss: 0.001399, mae: 0.040805, mean_q: 1.168326
 812727/1000000: episode: 8128, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 58.358, mean reward: 0.584 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.904, 10.387], loss: 0.001432, mae: 0.041144, mean_q: 1.168261
 812827/1000000: episode: 8129, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 61.054, mean reward: 0.611 [0.509, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.515, 10.272], loss: 0.001421, mae: 0.040910, mean_q: 1.169298
 812927/1000000: episode: 8130, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 57.721, mean reward: 0.577 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.104, 10.098], loss: 0.001432, mae: 0.040946, mean_q: 1.166766
 813027/1000000: episode: 8131, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 58.166, mean reward: 0.582 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.712, 10.098], loss: 0.001412, mae: 0.041003, mean_q: 1.167431
 813127/1000000: episode: 8132, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.544, mean reward: 0.595 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.002, 10.281], loss: 0.001429, mae: 0.040433, mean_q: 1.163545
 813227/1000000: episode: 8133, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 57.797, mean reward: 0.578 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.051, 10.098], loss: 0.001424, mae: 0.040619, mean_q: 1.168342
 813327/1000000: episode: 8134, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 60.252, mean reward: 0.603 [0.508, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.082, 10.407], loss: 0.001402, mae: 0.040544, mean_q: 1.162957
 813427/1000000: episode: 8135, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 60.978, mean reward: 0.610 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.939, 10.180], loss: 0.001419, mae: 0.040894, mean_q: 1.164610
 813527/1000000: episode: 8136, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 64.983, mean reward: 0.650 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.098], loss: 0.001402, mae: 0.039899, mean_q: 1.162527
 813627/1000000: episode: 8137, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 59.880, mean reward: 0.599 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.348], loss: 0.001419, mae: 0.040769, mean_q: 1.163491
 813727/1000000: episode: 8138, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 57.802, mean reward: 0.578 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.028, 10.098], loss: 0.001382, mae: 0.039700, mean_q: 1.170905
 813827/1000000: episode: 8139, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 58.350, mean reward: 0.583 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.733, 10.287], loss: 0.001397, mae: 0.040326, mean_q: 1.167753
 813927/1000000: episode: 8140, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 60.359, mean reward: 0.604 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.887, 10.425], loss: 0.001401, mae: 0.040244, mean_q: 1.166695
 814027/1000000: episode: 8141, duration: 1.688s, episode steps: 100, steps per second: 59, episode reward: 57.714, mean reward: 0.577 [0.510, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.543, 10.198], loss: 0.001471, mae: 0.041270, mean_q: 1.168303
 814127/1000000: episode: 8142, duration: 1.886s, episode steps: 100, steps per second: 53, episode reward: 62.601, mean reward: 0.626 [0.515, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.825, 10.098], loss: 0.001465, mae: 0.041352, mean_q: 1.168235
 814227/1000000: episode: 8143, duration: 1.467s, episode steps: 100, steps per second: 68, episode reward: 60.030, mean reward: 0.600 [0.506, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.192, 10.125], loss: 0.001398, mae: 0.039929, mean_q: 1.169088
 814327/1000000: episode: 8144, duration: 1.413s, episode steps: 100, steps per second: 71, episode reward: 58.144, mean reward: 0.581 [0.511, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.589, 10.098], loss: 0.001544, mae: 0.042266, mean_q: 1.170142
 814427/1000000: episode: 8145, duration: 1.321s, episode steps: 100, steps per second: 76, episode reward: 58.878, mean reward: 0.589 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.906, 10.318], loss: 0.001388, mae: 0.040130, mean_q: 1.168946
 814527/1000000: episode: 8146, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 58.653, mean reward: 0.587 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.541, 10.098], loss: 0.001509, mae: 0.041329, mean_q: 1.169426
 814627/1000000: episode: 8147, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 58.339, mean reward: 0.583 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.287, 10.186], loss: 0.001437, mae: 0.040733, mean_q: 1.173789
 814727/1000000: episode: 8148, duration: 1.338s, episode steps: 100, steps per second: 75, episode reward: 57.808, mean reward: 0.578 [0.504, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.098], loss: 0.001444, mae: 0.040974, mean_q: 1.167013
 814827/1000000: episode: 8149, duration: 1.429s, episode steps: 100, steps per second: 70, episode reward: 56.224, mean reward: 0.562 [0.505, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.098], loss: 0.001470, mae: 0.041073, mean_q: 1.171457
 814927/1000000: episode: 8150, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 61.941, mean reward: 0.619 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.133, 10.331], loss: 0.001340, mae: 0.039230, mean_q: 1.166068
 815027/1000000: episode: 8151, duration: 1.212s, episode steps: 100, steps per second: 82, episode reward: 58.110, mean reward: 0.581 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.109, 10.098], loss: 0.001425, mae: 0.040826, mean_q: 1.171552
 815127/1000000: episode: 8152, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 60.236, mean reward: 0.602 [0.500, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.093, 10.098], loss: 0.001537, mae: 0.042247, mean_q: 1.170189
 815227/1000000: episode: 8153, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 60.135, mean reward: 0.601 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.283, 10.125], loss: 0.001413, mae: 0.040540, mean_q: 1.172118
 815327/1000000: episode: 8154, duration: 1.130s, episode steps: 100, steps per second: 88, episode reward: 56.877, mean reward: 0.569 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.937, 10.104], loss: 0.001431, mae: 0.040304, mean_q: 1.171434
 815427/1000000: episode: 8155, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 57.501, mean reward: 0.575 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.686, 10.130], loss: 0.001478, mae: 0.041297, mean_q: 1.175175
 815527/1000000: episode: 8156, duration: 1.313s, episode steps: 100, steps per second: 76, episode reward: 57.887, mean reward: 0.579 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.992, 10.324], loss: 0.001437, mae: 0.040551, mean_q: 1.169138
 815627/1000000: episode: 8157, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 58.178, mean reward: 0.582 [0.499, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.184, 10.192], loss: 0.001350, mae: 0.039789, mean_q: 1.170134
 815727/1000000: episode: 8158, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 57.793, mean reward: 0.578 [0.498, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.686, 10.121], loss: 0.001539, mae: 0.041442, mean_q: 1.167483
 815827/1000000: episode: 8159, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 59.639, mean reward: 0.596 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.295, 10.514], loss: 0.001546, mae: 0.041516, mean_q: 1.169498
 815927/1000000: episode: 8160, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 64.685, mean reward: 0.647 [0.514, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.969, 10.098], loss: 0.001497, mae: 0.041790, mean_q: 1.171917
 816027/1000000: episode: 8161, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 57.494, mean reward: 0.575 [0.501, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.265, 10.098], loss: 0.001379, mae: 0.039889, mean_q: 1.172590
 816127/1000000: episode: 8162, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 58.128, mean reward: 0.581 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.321, 10.392], loss: 0.001476, mae: 0.041706, mean_q: 1.174007
 816227/1000000: episode: 8163, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 58.170, mean reward: 0.582 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.098], loss: 0.001433, mae: 0.040670, mean_q: 1.173054
 816327/1000000: episode: 8164, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 57.340, mean reward: 0.573 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.098], loss: 0.001485, mae: 0.041503, mean_q: 1.167873
 816427/1000000: episode: 8165, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 59.592, mean reward: 0.596 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.737, 10.190], loss: 0.001347, mae: 0.039946, mean_q: 1.173919
 816527/1000000: episode: 8166, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.545, mean reward: 0.585 [0.513, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.262], loss: 0.001559, mae: 0.042433, mean_q: 1.174371
 816627/1000000: episode: 8167, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 59.496, mean reward: 0.595 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.291, 10.191], loss: 0.001531, mae: 0.041929, mean_q: 1.174374
 816727/1000000: episode: 8168, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 61.720, mean reward: 0.617 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.081, 10.098], loss: 0.001445, mae: 0.040866, mean_q: 1.169984
 816827/1000000: episode: 8169, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 57.930, mean reward: 0.579 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.282], loss: 0.001486, mae: 0.041433, mean_q: 1.174286
 816927/1000000: episode: 8170, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 59.840, mean reward: 0.598 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.022, 10.358], loss: 0.001616, mae: 0.043172, mean_q: 1.174214
 817027/1000000: episode: 8171, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 57.663, mean reward: 0.577 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.241, 10.112], loss: 0.001622, mae: 0.043585, mean_q: 1.179021
 817127/1000000: episode: 8172, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 58.225, mean reward: 0.582 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.270, 10.098], loss: 0.001512, mae: 0.042059, mean_q: 1.176199
 817227/1000000: episode: 8173, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 58.428, mean reward: 0.584 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.491, 10.174], loss: 0.001409, mae: 0.040758, mean_q: 1.174521
 817327/1000000: episode: 8174, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 60.332, mean reward: 0.603 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.789, 10.301], loss: 0.001488, mae: 0.041683, mean_q: 1.173640
 817427/1000000: episode: 8175, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 59.834, mean reward: 0.598 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.111, 10.098], loss: 0.001410, mae: 0.041186, mean_q: 1.170142
 817527/1000000: episode: 8176, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 57.668, mean reward: 0.577 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.427, 10.207], loss: 0.001457, mae: 0.041291, mean_q: 1.171942
 817627/1000000: episode: 8177, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 57.478, mean reward: 0.575 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.727, 10.287], loss: 0.001484, mae: 0.041613, mean_q: 1.172117
 817727/1000000: episode: 8178, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 60.139, mean reward: 0.601 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.524, 10.253], loss: 0.001472, mae: 0.041062, mean_q: 1.168275
 817827/1000000: episode: 8179, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 57.622, mean reward: 0.576 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.538, 10.098], loss: 0.001451, mae: 0.041158, mean_q: 1.166362
 817927/1000000: episode: 8180, duration: 1.268s, episode steps: 100, steps per second: 79, episode reward: 60.570, mean reward: 0.606 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.885, 10.103], loss: 0.001484, mae: 0.041930, mean_q: 1.168973
 818027/1000000: episode: 8181, duration: 1.615s, episode steps: 100, steps per second: 62, episode reward: 57.948, mean reward: 0.579 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.838, 10.304], loss: 0.001546, mae: 0.042443, mean_q: 1.171050
 818127/1000000: episode: 8182, duration: 1.858s, episode steps: 100, steps per second: 54, episode reward: 57.069, mean reward: 0.571 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.105], loss: 0.001383, mae: 0.040553, mean_q: 1.168533
 818227/1000000: episode: 8183, duration: 2.017s, episode steps: 100, steps per second: 50, episode reward: 59.178, mean reward: 0.592 [0.513, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.889, 10.305], loss: 0.001386, mae: 0.040401, mean_q: 1.165712
 818327/1000000: episode: 8184, duration: 1.961s, episode steps: 100, steps per second: 51, episode reward: 61.241, mean reward: 0.612 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.859, 10.146], loss: 0.001505, mae: 0.041755, mean_q: 1.166410
 818427/1000000: episode: 8185, duration: 1.477s, episode steps: 100, steps per second: 68, episode reward: 57.823, mean reward: 0.578 [0.512, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.803, 10.107], loss: 0.001480, mae: 0.042076, mean_q: 1.167207
 818527/1000000: episode: 8186, duration: 1.398s, episode steps: 100, steps per second: 72, episode reward: 58.192, mean reward: 0.582 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.459, 10.098], loss: 0.001411, mae: 0.041218, mean_q: 1.161392
 818627/1000000: episode: 8187, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 57.679, mean reward: 0.577 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.540, 10.100], loss: 0.001488, mae: 0.041793, mean_q: 1.165357
 818727/1000000: episode: 8188, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 57.729, mean reward: 0.577 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.653, 10.139], loss: 0.001385, mae: 0.040588, mean_q: 1.163742
 818827/1000000: episode: 8189, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 59.208, mean reward: 0.592 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.152, 10.194], loss: 0.001517, mae: 0.041956, mean_q: 1.166428
 818927/1000000: episode: 8190, duration: 0.967s, episode steps: 100, steps per second: 103, episode reward: 57.274, mean reward: 0.573 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.294, 10.155], loss: 0.001483, mae: 0.041781, mean_q: 1.165401
 819027/1000000: episode: 8191, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.868, mean reward: 0.579 [0.519, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.664, 10.098], loss: 0.001481, mae: 0.041544, mean_q: 1.164633
 819127/1000000: episode: 8192, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 57.666, mean reward: 0.577 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.457, 10.245], loss: 0.001430, mae: 0.040787, mean_q: 1.162664
 819227/1000000: episode: 8193, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 57.148, mean reward: 0.571 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.934, 10.213], loss: 0.001440, mae: 0.041344, mean_q: 1.164263
 819327/1000000: episode: 8194, duration: 1.465s, episode steps: 100, steps per second: 68, episode reward: 57.114, mean reward: 0.571 [0.503, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.098], loss: 0.001428, mae: 0.041106, mean_q: 1.164327
 819427/1000000: episode: 8195, duration: 1.571s, episode steps: 100, steps per second: 64, episode reward: 60.916, mean reward: 0.609 [0.507, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.431, 10.257], loss: 0.001357, mae: 0.039669, mean_q: 1.160964
 819527/1000000: episode: 8196, duration: 1.882s, episode steps: 100, steps per second: 53, episode reward: 57.994, mean reward: 0.580 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.778, 10.098], loss: 0.001477, mae: 0.041755, mean_q: 1.161834
 819627/1000000: episode: 8197, duration: 1.258s, episode steps: 100, steps per second: 80, episode reward: 58.254, mean reward: 0.583 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.543, 10.098], loss: 0.001506, mae: 0.042603, mean_q: 1.161174
 819727/1000000: episode: 8198, duration: 1.553s, episode steps: 100, steps per second: 64, episode reward: 64.315, mean reward: 0.643 [0.510, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.980, 10.355], loss: 0.001531, mae: 0.042193, mean_q: 1.163957
 819827/1000000: episode: 8199, duration: 1.258s, episode steps: 100, steps per second: 79, episode reward: 58.620, mean reward: 0.586 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.148], loss: 0.001402, mae: 0.040543, mean_q: 1.165266
 819927/1000000: episode: 8200, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 59.268, mean reward: 0.593 [0.501, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.452, 10.306], loss: 0.001553, mae: 0.042571, mean_q: 1.167458
 820027/1000000: episode: 8201, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 60.242, mean reward: 0.602 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.285, 10.418], loss: 0.001413, mae: 0.041359, mean_q: 1.163666
 820127/1000000: episode: 8202, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 60.427, mean reward: 0.604 [0.516, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.437, 10.221], loss: 0.001431, mae: 0.041218, mean_q: 1.163778
 820227/1000000: episode: 8203, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 60.429, mean reward: 0.604 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.391, 10.288], loss: 0.001310, mae: 0.039595, mean_q: 1.160715
 820327/1000000: episode: 8204, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 60.212, mean reward: 0.602 [0.514, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.098], loss: 0.001491, mae: 0.041529, mean_q: 1.164409
 820427/1000000: episode: 8205, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 57.005, mean reward: 0.570 [0.500, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.547, 10.098], loss: 0.001394, mae: 0.040543, mean_q: 1.166245
 820527/1000000: episode: 8206, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.970, mean reward: 0.590 [0.506, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.043, 10.153], loss: 0.001455, mae: 0.041481, mean_q: 1.169948
 820627/1000000: episode: 8207, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 55.933, mean reward: 0.559 [0.506, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.839, 10.124], loss: 0.001530, mae: 0.042332, mean_q: 1.168998
 820727/1000000: episode: 8208, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 58.917, mean reward: 0.589 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.411, 10.345], loss: 0.001357, mae: 0.040201, mean_q: 1.163883
 820827/1000000: episode: 8209, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 57.436, mean reward: 0.574 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.862, 10.233], loss: 0.001449, mae: 0.041556, mean_q: 1.166073
 820927/1000000: episode: 8210, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 56.828, mean reward: 0.568 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.416, 10.098], loss: 0.001348, mae: 0.039978, mean_q: 1.162836
 821027/1000000: episode: 8211, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 58.485, mean reward: 0.585 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.273, 10.098], loss: 0.001458, mae: 0.041453, mean_q: 1.163920
 821127/1000000: episode: 8212, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 60.053, mean reward: 0.601 [0.512, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.345, 10.098], loss: 0.001335, mae: 0.039350, mean_q: 1.163625
 821227/1000000: episode: 8213, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: 59.478, mean reward: 0.595 [0.514, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.117, 10.098], loss: 0.001355, mae: 0.040166, mean_q: 1.164144
 821327/1000000: episode: 8214, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 57.512, mean reward: 0.575 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.011, 10.284], loss: 0.001354, mae: 0.040186, mean_q: 1.164539
 821427/1000000: episode: 8215, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 60.719, mean reward: 0.607 [0.508, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.497, 10.098], loss: 0.001337, mae: 0.039362, mean_q: 1.164150
 821527/1000000: episode: 8216, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 58.192, mean reward: 0.582 [0.506, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.195, 10.151], loss: 0.001366, mae: 0.040458, mean_q: 1.165267
 821627/1000000: episode: 8217, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.897, mean reward: 0.589 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.303], loss: 0.001397, mae: 0.040239, mean_q: 1.159608
 821727/1000000: episode: 8218, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 60.819, mean reward: 0.608 [0.505, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.958, 10.119], loss: 0.001360, mae: 0.039658, mean_q: 1.163899
 821827/1000000: episode: 8219, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 58.552, mean reward: 0.586 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.224], loss: 0.001329, mae: 0.039338, mean_q: 1.160352
 821927/1000000: episode: 8220, duration: 1.481s, episode steps: 100, steps per second: 68, episode reward: 57.034, mean reward: 0.570 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.986, 10.104], loss: 0.001406, mae: 0.040770, mean_q: 1.160149
 822027/1000000: episode: 8221, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 61.965, mean reward: 0.620 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.904, 10.098], loss: 0.001335, mae: 0.039872, mean_q: 1.162362
 822127/1000000: episode: 8222, duration: 1.283s, episode steps: 100, steps per second: 78, episode reward: 59.676, mean reward: 0.597 [0.521, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.098], loss: 0.001305, mae: 0.039489, mean_q: 1.164419
 822227/1000000: episode: 8223, duration: 1.351s, episode steps: 100, steps per second: 74, episode reward: 58.566, mean reward: 0.586 [0.509, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.431, 10.237], loss: 0.001427, mae: 0.040681, mean_q: 1.164390
 822327/1000000: episode: 8224, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 58.303, mean reward: 0.583 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.700, 10.120], loss: 0.001389, mae: 0.039875, mean_q: 1.164031
 822427/1000000: episode: 8225, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 60.464, mean reward: 0.605 [0.510, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.583, 10.295], loss: 0.001426, mae: 0.040357, mean_q: 1.163486
 822527/1000000: episode: 8226, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 58.570, mean reward: 0.586 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.287, 10.098], loss: 0.001382, mae: 0.040038, mean_q: 1.167412
 822627/1000000: episode: 8227, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 63.703, mean reward: 0.637 [0.509, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.347, 10.159], loss: 0.001374, mae: 0.040057, mean_q: 1.165617
 822727/1000000: episode: 8228, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.201, mean reward: 0.572 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.225], loss: 0.001373, mae: 0.039844, mean_q: 1.167413
 822827/1000000: episode: 8229, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 60.685, mean reward: 0.607 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.449, 10.098], loss: 0.001445, mae: 0.041116, mean_q: 1.168503
 822927/1000000: episode: 8230, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 59.437, mean reward: 0.594 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.389, 10.186], loss: 0.001310, mae: 0.039193, mean_q: 1.165235
 823027/1000000: episode: 8231, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 59.426, mean reward: 0.594 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.854, 10.098], loss: 0.001243, mae: 0.037820, mean_q: 1.166093
 823127/1000000: episode: 8232, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 56.795, mean reward: 0.568 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.954, 10.098], loss: 0.001397, mae: 0.040007, mean_q: 1.167018
 823227/1000000: episode: 8233, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 60.665, mean reward: 0.607 [0.511, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.137], loss: 0.001368, mae: 0.040278, mean_q: 1.173036
 823327/1000000: episode: 8234, duration: 1.636s, episode steps: 100, steps per second: 61, episode reward: 60.883, mean reward: 0.609 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.331, 10.198], loss: 0.001283, mae: 0.039062, mean_q: 1.167981
 823427/1000000: episode: 8235, duration: 1.283s, episode steps: 100, steps per second: 78, episode reward: 58.309, mean reward: 0.583 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.219, 10.271], loss: 0.001350, mae: 0.040007, mean_q: 1.165263
 823527/1000000: episode: 8236, duration: 1.639s, episode steps: 100, steps per second: 61, episode reward: 56.838, mean reward: 0.568 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.354, 10.098], loss: 0.001365, mae: 0.039750, mean_q: 1.166681
 823627/1000000: episode: 8237, duration: 1.679s, episode steps: 100, steps per second: 60, episode reward: 60.401, mean reward: 0.604 [0.510, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.521, 10.294], loss: 0.001437, mae: 0.040736, mean_q: 1.170819
 823727/1000000: episode: 8238, duration: 1.416s, episode steps: 100, steps per second: 71, episode reward: 58.331, mean reward: 0.583 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.390, 10.098], loss: 0.001364, mae: 0.039797, mean_q: 1.168102
 823827/1000000: episode: 8239, duration: 1.324s, episode steps: 100, steps per second: 76, episode reward: 57.548, mean reward: 0.575 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.195, 10.098], loss: 0.001397, mae: 0.040678, mean_q: 1.168225
 823927/1000000: episode: 8240, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 62.916, mean reward: 0.629 [0.519, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.080, 10.098], loss: 0.001337, mae: 0.039662, mean_q: 1.169623
 824027/1000000: episode: 8241, duration: 0.978s, episode steps: 100, steps per second: 102, episode reward: 58.171, mean reward: 0.582 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.017, 10.155], loss: 0.001379, mae: 0.039919, mean_q: 1.170569
 824127/1000000: episode: 8242, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 62.407, mean reward: 0.624 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.171, 10.406], loss: 0.001433, mae: 0.040933, mean_q: 1.170772
 824227/1000000: episode: 8243, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.542, mean reward: 0.575 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.307, 10.225], loss: 0.001398, mae: 0.040460, mean_q: 1.172951
 824327/1000000: episode: 8244, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 59.593, mean reward: 0.596 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.491, 10.098], loss: 0.001450, mae: 0.040972, mean_q: 1.169112
 824427/1000000: episode: 8245, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 61.298, mean reward: 0.613 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.546, 10.098], loss: 0.001476, mae: 0.041365, mean_q: 1.172675
 824527/1000000: episode: 8246, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 64.000, mean reward: 0.640 [0.500, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.743, 10.369], loss: 0.001394, mae: 0.040367, mean_q: 1.170419
 824627/1000000: episode: 8247, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 62.114, mean reward: 0.621 [0.502, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.455, 10.098], loss: 0.001568, mae: 0.043064, mean_q: 1.178144
 824727/1000000: episode: 8248, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 58.980, mean reward: 0.590 [0.510, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.302], loss: 0.001560, mae: 0.042776, mean_q: 1.172262
 824827/1000000: episode: 8249, duration: 0.962s, episode steps: 100, steps per second: 104, episode reward: 58.331, mean reward: 0.583 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.436, 10.130], loss: 0.001439, mae: 0.040744, mean_q: 1.173801
 824927/1000000: episode: 8250, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: 61.675, mean reward: 0.617 [0.526, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.002, 10.370], loss: 0.001376, mae: 0.039825, mean_q: 1.171337
 825027/1000000: episode: 8251, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 59.691, mean reward: 0.597 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.419, 10.098], loss: 0.001532, mae: 0.042129, mean_q: 1.176291
 825127/1000000: episode: 8252, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 58.680, mean reward: 0.587 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.202, 10.179], loss: 0.001528, mae: 0.042475, mean_q: 1.179019
 825227/1000000: episode: 8253, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 60.034, mean reward: 0.600 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.883, 10.222], loss: 0.001519, mae: 0.042555, mean_q: 1.181239
 825327/1000000: episode: 8254, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 57.585, mean reward: 0.576 [0.512, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.001445, mae: 0.041242, mean_q: 1.172543
 825427/1000000: episode: 8255, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 59.194, mean reward: 0.592 [0.515, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.958, 10.187], loss: 0.001525, mae: 0.042546, mean_q: 1.176506
 825527/1000000: episode: 8256, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 59.351, mean reward: 0.594 [0.526, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.227], loss: 0.001494, mae: 0.042037, mean_q: 1.175931
 825627/1000000: episode: 8257, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.598, mean reward: 0.576 [0.499, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.098], loss: 0.001476, mae: 0.041446, mean_q: 1.177580
 825727/1000000: episode: 8258, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 56.858, mean reward: 0.569 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.710, 10.243], loss: 0.001500, mae: 0.041913, mean_q: 1.178552
 825827/1000000: episode: 8259, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 56.398, mean reward: 0.564 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.475, 10.098], loss: 0.001488, mae: 0.042025, mean_q: 1.175884
 825927/1000000: episode: 8260, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 57.292, mean reward: 0.573 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.843, 10.202], loss: 0.001529, mae: 0.042073, mean_q: 1.177004
 826027/1000000: episode: 8261, duration: 1.332s, episode steps: 100, steps per second: 75, episode reward: 57.156, mean reward: 0.572 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.098], loss: 0.001489, mae: 0.041505, mean_q: 1.173976
 826127/1000000: episode: 8262, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 58.000, mean reward: 0.580 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.767, 10.098], loss: 0.001433, mae: 0.041333, mean_q: 1.175119
 826227/1000000: episode: 8263, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 60.256, mean reward: 0.603 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.072, 10.106], loss: 0.001471, mae: 0.041410, mean_q: 1.172387
 826327/1000000: episode: 8264, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: 62.492, mean reward: 0.625 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.548, 10.098], loss: 0.001492, mae: 0.041426, mean_q: 1.173911
 826427/1000000: episode: 8265, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 60.109, mean reward: 0.601 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.621, 10.204], loss: 0.001449, mae: 0.040946, mean_q: 1.175471
 826527/1000000: episode: 8266, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 58.968, mean reward: 0.590 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.154, 10.098], loss: 0.001531, mae: 0.041960, mean_q: 1.178422
 826627/1000000: episode: 8267, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 58.493, mean reward: 0.585 [0.517, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.330, 10.267], loss: 0.001510, mae: 0.041794, mean_q: 1.177972
 826727/1000000: episode: 8268, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 64.503, mean reward: 0.645 [0.527, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.045, 10.098], loss: 0.001484, mae: 0.041869, mean_q: 1.173933
 826827/1000000: episode: 8269, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 57.336, mean reward: 0.573 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.550, 10.098], loss: 0.001484, mae: 0.042079, mean_q: 1.175539
 826927/1000000: episode: 8270, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 58.960, mean reward: 0.590 [0.503, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.505, 10.174], loss: 0.001460, mae: 0.040874, mean_q: 1.177799
 827027/1000000: episode: 8271, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.759, mean reward: 0.588 [0.500, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.319, 10.122], loss: 0.001392, mae: 0.040424, mean_q: 1.175218
 827127/1000000: episode: 8272, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 58.032, mean reward: 0.580 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.964, 10.098], loss: 0.001498, mae: 0.041386, mean_q: 1.174397
 827227/1000000: episode: 8273, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 58.784, mean reward: 0.588 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.098], loss: 0.001550, mae: 0.042591, mean_q: 1.176308
 827327/1000000: episode: 8274, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 59.247, mean reward: 0.592 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.263, 10.187], loss: 0.001545, mae: 0.042517, mean_q: 1.176951
 827427/1000000: episode: 8275, duration: 1.156s, episode steps: 100, steps per second: 86, episode reward: 60.093, mean reward: 0.601 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.197], loss: 0.001461, mae: 0.040890, mean_q: 1.176449
 827527/1000000: episode: 8276, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.147, mean reward: 0.591 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.370, 10.162], loss: 0.001519, mae: 0.041699, mean_q: 1.173339
 827627/1000000: episode: 8277, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 59.800, mean reward: 0.598 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.615, 10.202], loss: 0.001549, mae: 0.042537, mean_q: 1.171955
 827727/1000000: episode: 8278, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.426, mean reward: 0.584 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.098], loss: 0.001520, mae: 0.042216, mean_q: 1.173214
 827827/1000000: episode: 8279, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.292, mean reward: 0.593 [0.509, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.579, 10.098], loss: 0.001487, mae: 0.041310, mean_q: 1.173956
 827927/1000000: episode: 8280, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.085, mean reward: 0.581 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.244, 10.130], loss: 0.001531, mae: 0.042245, mean_q: 1.175198
 828027/1000000: episode: 8281, duration: 0.998s, episode steps: 100, steps per second: 100, episode reward: 60.122, mean reward: 0.601 [0.501, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.098], loss: 0.001517, mae: 0.041886, mean_q: 1.175887
 828127/1000000: episode: 8282, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.926, mean reward: 0.579 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.006, 10.098], loss: 0.001575, mae: 0.042319, mean_q: 1.176586
 828227/1000000: episode: 8283, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 63.299, mean reward: 0.633 [0.527, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.775, 10.098], loss: 0.001536, mae: 0.041754, mean_q: 1.170102
 828327/1000000: episode: 8284, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 58.676, mean reward: 0.587 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.606, 10.098], loss: 0.001623, mae: 0.043325, mean_q: 1.175624
 828427/1000000: episode: 8285, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.561, mean reward: 0.586 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.923, 10.098], loss: 0.001472, mae: 0.041412, mean_q: 1.176207
 828527/1000000: episode: 8286, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 60.336, mean reward: 0.603 [0.500, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.327, 10.210], loss: 0.001510, mae: 0.041656, mean_q: 1.175493
 828627/1000000: episode: 8287, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 58.711, mean reward: 0.587 [0.505, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.067, 10.210], loss: 0.001671, mae: 0.043865, mean_q: 1.172077
 828727/1000000: episode: 8288, duration: 1.130s, episode steps: 100, steps per second: 88, episode reward: 58.842, mean reward: 0.588 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.495, 10.328], loss: 0.001668, mae: 0.043585, mean_q: 1.175997
 828827/1000000: episode: 8289, duration: 1.152s, episode steps: 100, steps per second: 87, episode reward: 59.388, mean reward: 0.594 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.209, 10.400], loss: 0.001520, mae: 0.042006, mean_q: 1.177079
 828927/1000000: episode: 8290, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 56.731, mean reward: 0.567 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.717, 10.098], loss: 0.001539, mae: 0.041990, mean_q: 1.179436
 829027/1000000: episode: 8291, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 58.417, mean reward: 0.584 [0.517, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.030, 10.098], loss: 0.001538, mae: 0.041832, mean_q: 1.172519
 829127/1000000: episode: 8292, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 60.308, mean reward: 0.603 [0.507, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.472, 10.098], loss: 0.001501, mae: 0.041983, mean_q: 1.172894
 829227/1000000: episode: 8293, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 57.302, mean reward: 0.573 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.740, 10.098], loss: 0.001470, mae: 0.041727, mean_q: 1.171480
 829327/1000000: episode: 8294, duration: 0.987s, episode steps: 100, steps per second: 101, episode reward: 57.807, mean reward: 0.578 [0.508, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.559, 10.261], loss: 0.001550, mae: 0.042095, mean_q: 1.173563
 829427/1000000: episode: 8295, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 59.497, mean reward: 0.595 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.536, 10.098], loss: 0.001621, mae: 0.043005, mean_q: 1.174792
 829527/1000000: episode: 8296, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 58.516, mean reward: 0.585 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.326, 10.199], loss: 0.001532, mae: 0.042062, mean_q: 1.169834
 829627/1000000: episode: 8297, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.973, mean reward: 0.590 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.076, 10.159], loss: 0.001641, mae: 0.043192, mean_q: 1.165877
 829727/1000000: episode: 8298, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 57.921, mean reward: 0.579 [0.505, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.869, 10.098], loss: 0.001446, mae: 0.041113, mean_q: 1.168242
 829827/1000000: episode: 8299, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 59.217, mean reward: 0.592 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.323, 10.103], loss: 0.001525, mae: 0.042257, mean_q: 1.164383
 829927/1000000: episode: 8300, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 58.861, mean reward: 0.589 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.906, 10.098], loss: 0.001535, mae: 0.042067, mean_q: 1.166820
 830027/1000000: episode: 8301, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 60.972, mean reward: 0.610 [0.503, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.207], loss: 0.001605, mae: 0.043238, mean_q: 1.169240
 830127/1000000: episode: 8302, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 57.204, mean reward: 0.572 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.434, 10.286], loss: 0.001523, mae: 0.042151, mean_q: 1.167165
 830227/1000000: episode: 8303, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: 56.916, mean reward: 0.569 [0.498, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.259, 10.258], loss: 0.001525, mae: 0.041921, mean_q: 1.165376
 830327/1000000: episode: 8304, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 58.106, mean reward: 0.581 [0.512, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.538, 10.227], loss: 0.001592, mae: 0.042803, mean_q: 1.165677
 830427/1000000: episode: 8305, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 58.507, mean reward: 0.585 [0.506, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.257, 10.098], loss: 0.001601, mae: 0.042730, mean_q: 1.166129
 830527/1000000: episode: 8306, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 60.091, mean reward: 0.601 [0.510, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.741, 10.357], loss: 0.001510, mae: 0.041047, mean_q: 1.165258
 830627/1000000: episode: 8307, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 59.483, mean reward: 0.595 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.333], loss: 0.001558, mae: 0.042447, mean_q: 1.166839
 830727/1000000: episode: 8308, duration: 1.362s, episode steps: 100, steps per second: 73, episode reward: 57.754, mean reward: 0.578 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.314], loss: 0.001533, mae: 0.041908, mean_q: 1.168985
 830827/1000000: episode: 8309, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 60.069, mean reward: 0.601 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.418, 10.098], loss: 0.001582, mae: 0.042669, mean_q: 1.165700
 830927/1000000: episode: 8310, duration: 1.047s, episode steps: 100, steps per second: 95, episode reward: 57.403, mean reward: 0.574 [0.501, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.188, 10.098], loss: 0.001449, mae: 0.041742, mean_q: 1.170940
 831027/1000000: episode: 8311, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.689, mean reward: 0.577 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.444, 10.098], loss: 0.001603, mae: 0.043349, mean_q: 1.167655
 831127/1000000: episode: 8312, duration: 1.001s, episode steps: 100, steps per second: 100, episode reward: 57.838, mean reward: 0.578 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.886, 10.278], loss: 0.001570, mae: 0.042421, mean_q: 1.168818
 831227/1000000: episode: 8313, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 59.793, mean reward: 0.598 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.706, 10.228], loss: 0.001511, mae: 0.041903, mean_q: 1.166499
 831327/1000000: episode: 8314, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 61.540, mean reward: 0.615 [0.511, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.861, 10.098], loss: 0.001479, mae: 0.041196, mean_q: 1.165707
 831427/1000000: episode: 8315, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 59.210, mean reward: 0.592 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.973, 10.098], loss: 0.001500, mae: 0.041659, mean_q: 1.168890
 831527/1000000: episode: 8316, duration: 1.345s, episode steps: 100, steps per second: 74, episode reward: 58.451, mean reward: 0.585 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.550, 10.124], loss: 0.001500, mae: 0.042167, mean_q: 1.165297
 831627/1000000: episode: 8317, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 58.546, mean reward: 0.585 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.400, 10.098], loss: 0.001502, mae: 0.041748, mean_q: 1.164876
 831727/1000000: episode: 8318, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 59.631, mean reward: 0.596 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.535, 10.294], loss: 0.001610, mae: 0.043400, mean_q: 1.166053
 831827/1000000: episode: 8319, duration: 1.261s, episode steps: 100, steps per second: 79, episode reward: 57.771, mean reward: 0.578 [0.512, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.233, 10.098], loss: 0.001475, mae: 0.041317, mean_q: 1.161868
 831927/1000000: episode: 8320, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.288, mean reward: 0.573 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.163], loss: 0.001468, mae: 0.041388, mean_q: 1.165050
 832027/1000000: episode: 8321, duration: 1.647s, episode steps: 100, steps per second: 61, episode reward: 57.635, mean reward: 0.576 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.445, 10.217], loss: 0.001531, mae: 0.041753, mean_q: 1.165557
 832127/1000000: episode: 8322, duration: 1.730s, episode steps: 100, steps per second: 58, episode reward: 58.962, mean reward: 0.590 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.756, 10.204], loss: 0.001541, mae: 0.042320, mean_q: 1.163432
 832227/1000000: episode: 8323, duration: 1.826s, episode steps: 100, steps per second: 55, episode reward: 59.279, mean reward: 0.593 [0.514, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.252, 10.466], loss: 0.001533, mae: 0.042809, mean_q: 1.164631
 832327/1000000: episode: 8324, duration: 1.322s, episode steps: 100, steps per second: 76, episode reward: 57.022, mean reward: 0.570 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.098], loss: 0.001558, mae: 0.042776, mean_q: 1.166071
 832427/1000000: episode: 8325, duration: 1.491s, episode steps: 100, steps per second: 67, episode reward: 60.068, mean reward: 0.601 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.407, 10.169], loss: 0.001518, mae: 0.042081, mean_q: 1.162395
 832527/1000000: episode: 8326, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.925, mean reward: 0.579 [0.509, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.363, 10.098], loss: 0.001571, mae: 0.042331, mean_q: 1.165916
 832627/1000000: episode: 8327, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 58.045, mean reward: 0.580 [0.506, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.522, 10.098], loss: 0.001501, mae: 0.041699, mean_q: 1.163888
 832727/1000000: episode: 8328, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 61.233, mean reward: 0.612 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.945, 10.098], loss: 0.001531, mae: 0.041860, mean_q: 1.162960
 832827/1000000: episode: 8329, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 57.354, mean reward: 0.574 [0.508, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.439, 10.098], loss: 0.001623, mae: 0.043111, mean_q: 1.166095
 832927/1000000: episode: 8330, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 57.864, mean reward: 0.579 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.688, 10.129], loss: 0.001456, mae: 0.041361, mean_q: 1.161062
 833027/1000000: episode: 8331, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 57.107, mean reward: 0.571 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.313, 10.118], loss: 0.001347, mae: 0.039684, mean_q: 1.158795
 833127/1000000: episode: 8332, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 60.571, mean reward: 0.606 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.295, 10.372], loss: 0.001505, mae: 0.041889, mean_q: 1.161862
 833227/1000000: episode: 8333, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.905, mean reward: 0.579 [0.511, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.456, 10.112], loss: 0.001569, mae: 0.042457, mean_q: 1.160296
 833327/1000000: episode: 8334, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 57.660, mean reward: 0.577 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.457, 10.098], loss: 0.001593, mae: 0.042607, mean_q: 1.162397
 833427/1000000: episode: 8335, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.366, mean reward: 0.584 [0.526, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.215, 10.171], loss: 0.001430, mae: 0.040705, mean_q: 1.161622
 833527/1000000: episode: 8336, duration: 1.486s, episode steps: 100, steps per second: 67, episode reward: 61.479, mean reward: 0.615 [0.512, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.819, 10.390], loss: 0.001367, mae: 0.039923, mean_q: 1.164745
 833627/1000000: episode: 8337, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 58.472, mean reward: 0.585 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.098], loss: 0.001509, mae: 0.041509, mean_q: 1.160050
 833727/1000000: episode: 8338, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 63.158, mean reward: 0.632 [0.507, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.268, 10.098], loss: 0.001392, mae: 0.040013, mean_q: 1.157660
 833827/1000000: episode: 8339, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 60.392, mean reward: 0.604 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.382, 10.098], loss: 0.001394, mae: 0.040343, mean_q: 1.166074
 833927/1000000: episode: 8340, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 58.649, mean reward: 0.586 [0.500, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.452, 10.312], loss: 0.001504, mae: 0.041852, mean_q: 1.164750
 834027/1000000: episode: 8341, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.815, mean reward: 0.578 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.464, 10.098], loss: 0.001415, mae: 0.041019, mean_q: 1.165165
 834127/1000000: episode: 8342, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 58.133, mean reward: 0.581 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.102, 10.098], loss: 0.001375, mae: 0.039843, mean_q: 1.161396
 834227/1000000: episode: 8343, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 57.918, mean reward: 0.579 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.383], loss: 0.001470, mae: 0.041175, mean_q: 1.164798
 834327/1000000: episode: 8344, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 58.019, mean reward: 0.580 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.734, 10.098], loss: 0.001375, mae: 0.040340, mean_q: 1.163251
 834427/1000000: episode: 8345, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 59.784, mean reward: 0.598 [0.515, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.098], loss: 0.001384, mae: 0.040283, mean_q: 1.165015
 834527/1000000: episode: 8346, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 58.305, mean reward: 0.583 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.834, 10.098], loss: 0.001410, mae: 0.040139, mean_q: 1.160242
 834627/1000000: episode: 8347, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 59.864, mean reward: 0.599 [0.507, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.751, 10.288], loss: 0.001366, mae: 0.039550, mean_q: 1.159055
 834727/1000000: episode: 8348, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 58.843, mean reward: 0.588 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.158, 10.098], loss: 0.001380, mae: 0.039530, mean_q: 1.160460
 834827/1000000: episode: 8349, duration: 1.296s, episode steps: 100, steps per second: 77, episode reward: 61.143, mean reward: 0.611 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.814, 10.098], loss: 0.001314, mae: 0.039125, mean_q: 1.165588
 834927/1000000: episode: 8350, duration: 1.471s, episode steps: 100, steps per second: 68, episode reward: 61.262, mean reward: 0.613 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.098], loss: 0.001305, mae: 0.039077, mean_q: 1.164439
 835027/1000000: episode: 8351, duration: 1.025s, episode steps: 100, steps per second: 98, episode reward: 61.003, mean reward: 0.610 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.348, 10.098], loss: 0.001417, mae: 0.040807, mean_q: 1.163708
 835127/1000000: episode: 8352, duration: 1.292s, episode steps: 100, steps per second: 77, episode reward: 57.882, mean reward: 0.579 [0.512, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.632, 10.114], loss: 0.001348, mae: 0.039361, mean_q: 1.164127
 835227/1000000: episode: 8353, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: 57.177, mean reward: 0.572 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.530, 10.098], loss: 0.001359, mae: 0.039908, mean_q: 1.166895
 835327/1000000: episode: 8354, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 57.595, mean reward: 0.576 [0.502, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.313, 10.147], loss: 0.001395, mae: 0.040052, mean_q: 1.163314
 835427/1000000: episode: 8355, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 59.747, mean reward: 0.597 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.683, 10.119], loss: 0.001397, mae: 0.040349, mean_q: 1.164609
 835527/1000000: episode: 8356, duration: 1.370s, episode steps: 100, steps per second: 73, episode reward: 57.662, mean reward: 0.577 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.744, 10.179], loss: 0.001354, mae: 0.039831, mean_q: 1.162916
 835627/1000000: episode: 8357, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.934, mean reward: 0.599 [0.502, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.498, 10.340], loss: 0.001385, mae: 0.040254, mean_q: 1.162247
 835727/1000000: episode: 8358, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 60.304, mean reward: 0.603 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.890, 10.098], loss: 0.001488, mae: 0.041218, mean_q: 1.164870
 835827/1000000: episode: 8359, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 64.236, mean reward: 0.642 [0.504, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.548, 10.417], loss: 0.001354, mae: 0.039550, mean_q: 1.164498
 835927/1000000: episode: 8360, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.622, mean reward: 0.586 [0.502, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.098], loss: 0.001380, mae: 0.040035, mean_q: 1.167329
 836027/1000000: episode: 8361, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 58.396, mean reward: 0.584 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.030, 10.199], loss: 0.001413, mae: 0.040039, mean_q: 1.167438
 836127/1000000: episode: 8362, duration: 1.352s, episode steps: 100, steps per second: 74, episode reward: 62.701, mean reward: 0.627 [0.500, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.257, 10.444], loss: 0.001481, mae: 0.041333, mean_q: 1.169125
 836227/1000000: episode: 8363, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 59.673, mean reward: 0.597 [0.503, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.861, 10.194], loss: 0.001529, mae: 0.041650, mean_q: 1.171633
 836327/1000000: episode: 8364, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 66.332, mean reward: 0.663 [0.522, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.468, 10.098], loss: 0.001582, mae: 0.042577, mean_q: 1.172280
 836427/1000000: episode: 8365, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 55.975, mean reward: 0.560 [0.504, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.805, 10.098], loss: 0.001477, mae: 0.040873, mean_q: 1.174459
 836527/1000000: episode: 8366, duration: 1.512s, episode steps: 100, steps per second: 66, episode reward: 58.799, mean reward: 0.588 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.705, 10.100], loss: 0.001525, mae: 0.041591, mean_q: 1.168566
 836627/1000000: episode: 8367, duration: 1.428s, episode steps: 100, steps per second: 70, episode reward: 58.191, mean reward: 0.582 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.803, 10.263], loss: 0.001476, mae: 0.041674, mean_q: 1.171877
 836727/1000000: episode: 8368, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: 56.917, mean reward: 0.569 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.230, 10.102], loss: 0.001506, mae: 0.041644, mean_q: 1.172705
 836827/1000000: episode: 8369, duration: 1.526s, episode steps: 100, steps per second: 66, episode reward: 58.113, mean reward: 0.581 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.122], loss: 0.001507, mae: 0.041085, mean_q: 1.172485
 836927/1000000: episode: 8370, duration: 1.530s, episode steps: 100, steps per second: 65, episode reward: 56.846, mean reward: 0.568 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.479, 10.098], loss: 0.001540, mae: 0.041430, mean_q: 1.171075
 837027/1000000: episode: 8371, duration: 1.556s, episode steps: 100, steps per second: 64, episode reward: 58.965, mean reward: 0.590 [0.506, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.475, 10.098], loss: 0.001546, mae: 0.041770, mean_q: 1.172322
 837127/1000000: episode: 8372, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 61.810, mean reward: 0.618 [0.514, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.263, 10.111], loss: 0.001611, mae: 0.042546, mean_q: 1.172635
 837227/1000000: episode: 8373, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.726, mean reward: 0.577 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.640, 10.150], loss: 0.001509, mae: 0.041512, mean_q: 1.170714
 837327/1000000: episode: 8374, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 58.917, mean reward: 0.589 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.784, 10.500], loss: 0.001544, mae: 0.042338, mean_q: 1.172256
 837427/1000000: episode: 8375, duration: 1.487s, episode steps: 100, steps per second: 67, episode reward: 62.694, mean reward: 0.627 [0.524, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.098], loss: 0.001450, mae: 0.040443, mean_q: 1.172587
 837527/1000000: episode: 8376, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 57.903, mean reward: 0.579 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.067, 10.167], loss: 0.001476, mae: 0.041029, mean_q: 1.172151
 837627/1000000: episode: 8377, duration: 1.418s, episode steps: 100, steps per second: 71, episode reward: 61.227, mean reward: 0.612 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.502, 10.098], loss: 0.001590, mae: 0.042120, mean_q: 1.175782
 837727/1000000: episode: 8378, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 58.347, mean reward: 0.583 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.098], loss: 0.001567, mae: 0.042103, mean_q: 1.175696
 837827/1000000: episode: 8379, duration: 1.489s, episode steps: 100, steps per second: 67, episode reward: 56.726, mean reward: 0.567 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.815, 10.203], loss: 0.001495, mae: 0.041254, mean_q: 1.174960
 837927/1000000: episode: 8380, duration: 1.342s, episode steps: 100, steps per second: 75, episode reward: 59.262, mean reward: 0.593 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.720, 10.137], loss: 0.001570, mae: 0.041880, mean_q: 1.173711
 838027/1000000: episode: 8381, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 59.065, mean reward: 0.591 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.098], loss: 0.001596, mae: 0.042129, mean_q: 1.172095
 838127/1000000: episode: 8382, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 61.520, mean reward: 0.615 [0.499, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.184, 10.098], loss: 0.001496, mae: 0.040942, mean_q: 1.173343
 838227/1000000: episode: 8383, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 57.643, mean reward: 0.576 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.420, 10.135], loss: 0.001633, mae: 0.043339, mean_q: 1.174475
 838327/1000000: episode: 8384, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 58.325, mean reward: 0.583 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.584, 10.098], loss: 0.001633, mae: 0.043479, mean_q: 1.172833
 838427/1000000: episode: 8385, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 56.887, mean reward: 0.569 [0.508, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.168, 10.188], loss: 0.001696, mae: 0.043778, mean_q: 1.173975
 838527/1000000: episode: 8386, duration: 1.081s, episode steps: 100, steps per second: 92, episode reward: 61.403, mean reward: 0.614 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.758, 10.098], loss: 0.001607, mae: 0.042872, mean_q: 1.176064
 838627/1000000: episode: 8387, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 59.929, mean reward: 0.599 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.280, 10.098], loss: 0.001549, mae: 0.042053, mean_q: 1.176979
 838727/1000000: episode: 8388, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 59.057, mean reward: 0.591 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.301, 10.098], loss: 0.001592, mae: 0.042705, mean_q: 1.174186
 838827/1000000: episode: 8389, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 59.518, mean reward: 0.595 [0.513, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.119], loss: 0.001602, mae: 0.043081, mean_q: 1.173448
 838927/1000000: episode: 8390, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 60.163, mean reward: 0.602 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.709, 10.399], loss: 0.001658, mae: 0.043635, mean_q: 1.174720
 839027/1000000: episode: 8391, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 59.003, mean reward: 0.590 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.951, 10.130], loss: 0.001696, mae: 0.043836, mean_q: 1.176276
 839127/1000000: episode: 8392, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.512, mean reward: 0.575 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.098], loss: 0.001663, mae: 0.043746, mean_q: 1.171954
 839227/1000000: episode: 8393, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.370, mean reward: 0.584 [0.500, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.971, 10.214], loss: 0.001629, mae: 0.043777, mean_q: 1.175476
 839327/1000000: episode: 8394, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 58.024, mean reward: 0.580 [0.511, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.439, 10.223], loss: 0.001634, mae: 0.042910, mean_q: 1.172873
 839427/1000000: episode: 8395, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.668, mean reward: 0.577 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.629, 10.209], loss: 0.001613, mae: 0.042907, mean_q: 1.174339
 839527/1000000: episode: 8396, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 60.893, mean reward: 0.609 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.495, 10.366], loss: 0.001538, mae: 0.042743, mean_q: 1.173052
 839627/1000000: episode: 8397, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.881, mean reward: 0.599 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.450, 10.098], loss: 0.001620, mae: 0.042992, mean_q: 1.173461
 839727/1000000: episode: 8398, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 58.603, mean reward: 0.586 [0.509, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.964, 10.125], loss: 0.001664, mae: 0.043302, mean_q: 1.169718
 839827/1000000: episode: 8399, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 59.507, mean reward: 0.595 [0.500, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.032, 10.098], loss: 0.001531, mae: 0.041937, mean_q: 1.167454
 839927/1000000: episode: 8400, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.523, mean reward: 0.585 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.363, 10.098], loss: 0.001566, mae: 0.042540, mean_q: 1.172104
 840027/1000000: episode: 8401, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 58.008, mean reward: 0.580 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.098], loss: 0.001487, mae: 0.041319, mean_q: 1.169239
 840127/1000000: episode: 8402, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 57.947, mean reward: 0.579 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.214], loss: 0.001660, mae: 0.043934, mean_q: 1.170837
 840227/1000000: episode: 8403, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.238, mean reward: 0.592 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.974, 10.124], loss: 0.001714, mae: 0.044333, mean_q: 1.177875
 840327/1000000: episode: 8404, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 57.537, mean reward: 0.575 [0.497, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.429, 10.098], loss: 0.001779, mae: 0.045428, mean_q: 1.175849
 840427/1000000: episode: 8405, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 58.606, mean reward: 0.586 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.098], loss: 0.001736, mae: 0.044644, mean_q: 1.173988
 840527/1000000: episode: 8406, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 59.986, mean reward: 0.600 [0.513, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.242, 10.098], loss: 0.001648, mae: 0.044115, mean_q: 1.172452
 840627/1000000: episode: 8407, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.040, mean reward: 0.580 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.490, 10.219], loss: 0.001579, mae: 0.042830, mean_q: 1.170967
 840727/1000000: episode: 8408, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 60.788, mean reward: 0.608 [0.507, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.243, 10.098], loss: 0.001511, mae: 0.042302, mean_q: 1.166501
 840827/1000000: episode: 8409, duration: 1.272s, episode steps: 100, steps per second: 79, episode reward: 59.696, mean reward: 0.597 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.386], loss: 0.001634, mae: 0.043423, mean_q: 1.171884
 840927/1000000: episode: 8410, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 59.007, mean reward: 0.590 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.438, 10.098], loss: 0.001724, mae: 0.044365, mean_q: 1.168373
 841027/1000000: episode: 8411, duration: 1.330s, episode steps: 100, steps per second: 75, episode reward: 63.267, mean reward: 0.633 [0.518, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.830, 10.392], loss: 0.001571, mae: 0.042790, mean_q: 1.173043
 841127/1000000: episode: 8412, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 60.662, mean reward: 0.607 [0.514, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.709, 10.098], loss: 0.001688, mae: 0.044000, mean_q: 1.170653
 841227/1000000: episode: 8413, duration: 1.140s, episode steps: 100, steps per second: 88, episode reward: 57.712, mean reward: 0.577 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.773, 10.171], loss: 0.001562, mae: 0.042788, mean_q: 1.172592
 841327/1000000: episode: 8414, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.898, mean reward: 0.579 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.292, 10.175], loss: 0.001595, mae: 0.042923, mean_q: 1.165833
 841427/1000000: episode: 8415, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 63.420, mean reward: 0.634 [0.516, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.161, 10.300], loss: 0.001621, mae: 0.043347, mean_q: 1.165563
 841527/1000000: episode: 8416, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 55.646, mean reward: 0.556 [0.505, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.179], loss: 0.001462, mae: 0.041462, mean_q: 1.172166
 841627/1000000: episode: 8417, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 60.529, mean reward: 0.605 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.548, 10.352], loss: 0.001605, mae: 0.042910, mean_q: 1.170048
 841727/1000000: episode: 8418, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 57.404, mean reward: 0.574 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.633, 10.173], loss: 0.001568, mae: 0.042607, mean_q: 1.169764
 841827/1000000: episode: 8419, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 58.249, mean reward: 0.582 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.795, 10.098], loss: 0.001565, mae: 0.042984, mean_q: 1.168747
 841927/1000000: episode: 8420, duration: 1.388s, episode steps: 100, steps per second: 72, episode reward: 57.602, mean reward: 0.576 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.619, 10.302], loss: 0.001480, mae: 0.041439, mean_q: 1.171153
 842027/1000000: episode: 8421, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 59.296, mean reward: 0.593 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.660, 10.123], loss: 0.001526, mae: 0.042528, mean_q: 1.168064
 842127/1000000: episode: 8422, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 58.480, mean reward: 0.585 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.192, 10.180], loss: 0.001501, mae: 0.042194, mean_q: 1.170485
 842227/1000000: episode: 8423, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 58.711, mean reward: 0.587 [0.502, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.113], loss: 0.001562, mae: 0.043660, mean_q: 1.169216
 842327/1000000: episode: 8424, duration: 1.303s, episode steps: 100, steps per second: 77, episode reward: 62.403, mean reward: 0.624 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.621, 10.308], loss: 0.001492, mae: 0.042266, mean_q: 1.166714
 842427/1000000: episode: 8425, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 57.591, mean reward: 0.576 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.645, 10.098], loss: 0.001396, mae: 0.041101, mean_q: 1.168786
 842527/1000000: episode: 8426, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 61.085, mean reward: 0.611 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.729, 10.317], loss: 0.001528, mae: 0.042926, mean_q: 1.173309
 842627/1000000: episode: 8427, duration: 1.264s, episode steps: 100, steps per second: 79, episode reward: 58.617, mean reward: 0.586 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.388, 10.166], loss: 0.001565, mae: 0.043413, mean_q: 1.167970
 842727/1000000: episode: 8428, duration: 1.338s, episode steps: 100, steps per second: 75, episode reward: 58.213, mean reward: 0.582 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.214, 10.138], loss: 0.001492, mae: 0.042349, mean_q: 1.172024
 842827/1000000: episode: 8429, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 58.763, mean reward: 0.588 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.083, 10.098], loss: 0.001412, mae: 0.041216, mean_q: 1.165514
 842927/1000000: episode: 8430, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 59.155, mean reward: 0.592 [0.506, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.406, 10.098], loss: 0.001535, mae: 0.042347, mean_q: 1.171001
 843027/1000000: episode: 8431, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 57.673, mean reward: 0.577 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.310, 10.182], loss: 0.001444, mae: 0.041741, mean_q: 1.169998
 843127/1000000: episode: 8432, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 57.731, mean reward: 0.577 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.144, 10.309], loss: 0.001526, mae: 0.042709, mean_q: 1.169374
 843227/1000000: episode: 8433, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 62.170, mean reward: 0.622 [0.507, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.706, 10.391], loss: 0.001432, mae: 0.041955, mean_q: 1.168676
 843327/1000000: episode: 8434, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 60.038, mean reward: 0.600 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.843, 10.098], loss: 0.001500, mae: 0.042486, mean_q: 1.168784
 843427/1000000: episode: 8435, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 62.408, mean reward: 0.624 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.633, 10.307], loss: 0.001489, mae: 0.041699, mean_q: 1.168370
 843527/1000000: episode: 8436, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 58.146, mean reward: 0.581 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.349, 10.133], loss: 0.001520, mae: 0.042461, mean_q: 1.173818
 843627/1000000: episode: 8437, duration: 1.670s, episode steps: 100, steps per second: 60, episode reward: 58.578, mean reward: 0.586 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.484, 10.137], loss: 0.001452, mae: 0.041715, mean_q: 1.172328
 843727/1000000: episode: 8438, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 57.069, mean reward: 0.571 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.178], loss: 0.001477, mae: 0.042024, mean_q: 1.170359
 843827/1000000: episode: 8439, duration: 1.308s, episode steps: 100, steps per second: 76, episode reward: 59.779, mean reward: 0.598 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.483, 10.098], loss: 0.001528, mae: 0.042483, mean_q: 1.168368
 843927/1000000: episode: 8440, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 57.620, mean reward: 0.576 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.886, 10.158], loss: 0.001399, mae: 0.040587, mean_q: 1.168817
 844027/1000000: episode: 8441, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.435, mean reward: 0.594 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.529, 10.098], loss: 0.001492, mae: 0.042078, mean_q: 1.168374
 844127/1000000: episode: 8442, duration: 1.392s, episode steps: 100, steps per second: 72, episode reward: 60.142, mean reward: 0.601 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.636, 10.098], loss: 0.001400, mae: 0.041098, mean_q: 1.170847
 844227/1000000: episode: 8443, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 56.837, mean reward: 0.568 [0.503, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.013, 10.109], loss: 0.001469, mae: 0.041633, mean_q: 1.171525
 844327/1000000: episode: 8444, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 57.407, mean reward: 0.574 [0.501, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.098], loss: 0.001354, mae: 0.040137, mean_q: 1.168673
 844427/1000000: episode: 8445, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 59.773, mean reward: 0.598 [0.500, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.920, 10.136], loss: 0.001393, mae: 0.040985, mean_q: 1.170256
 844527/1000000: episode: 8446, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 58.357, mean reward: 0.584 [0.513, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.236], loss: 0.001490, mae: 0.041681, mean_q: 1.167173
 844627/1000000: episode: 8447, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 57.706, mean reward: 0.577 [0.508, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.343, 10.098], loss: 0.001352, mae: 0.040401, mean_q: 1.166620
 844727/1000000: episode: 8448, duration: 1.388s, episode steps: 100, steps per second: 72, episode reward: 56.220, mean reward: 0.562 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.098], loss: 0.001430, mae: 0.041272, mean_q: 1.169112
 844827/1000000: episode: 8449, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 59.378, mean reward: 0.594 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.376, 10.132], loss: 0.001395, mae: 0.040522, mean_q: 1.170903
 844927/1000000: episode: 8450, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 61.602, mean reward: 0.616 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.601, 10.348], loss: 0.001404, mae: 0.040457, mean_q: 1.167202
 845027/1000000: episode: 8451, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 60.494, mean reward: 0.605 [0.521, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.069, 10.098], loss: 0.001398, mae: 0.040800, mean_q: 1.172074
 845127/1000000: episode: 8452, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.977, mean reward: 0.580 [0.508, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.453, 10.145], loss: 0.001481, mae: 0.041745, mean_q: 1.172652
 845227/1000000: episode: 8453, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 58.256, mean reward: 0.583 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.665, 10.246], loss: 0.001342, mae: 0.040077, mean_q: 1.170881
 845327/1000000: episode: 8454, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 58.349, mean reward: 0.583 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.098], loss: 0.001369, mae: 0.040433, mean_q: 1.169293
 845427/1000000: episode: 8455, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.814, mean reward: 0.578 [0.515, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.606, 10.126], loss: 0.001338, mae: 0.040221, mean_q: 1.168861
 845527/1000000: episode: 8456, duration: 1.415s, episode steps: 100, steps per second: 71, episode reward: 62.786, mean reward: 0.628 [0.523, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.650, 10.787], loss: 0.001396, mae: 0.040345, mean_q: 1.164614
 845627/1000000: episode: 8457, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.673, mean reward: 0.577 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.590, 10.098], loss: 0.001370, mae: 0.040600, mean_q: 1.170893
 845727/1000000: episode: 8458, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 57.168, mean reward: 0.572 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.319, 10.190], loss: 0.001308, mae: 0.039633, mean_q: 1.168751
 845827/1000000: episode: 8459, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 59.014, mean reward: 0.590 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.350, 10.348], loss: 0.001319, mae: 0.039543, mean_q: 1.166041
 845927/1000000: episode: 8460, duration: 1.507s, episode steps: 100, steps per second: 66, episode reward: 58.909, mean reward: 0.589 [0.521, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.570, 10.225], loss: 0.001366, mae: 0.039966, mean_q: 1.165444
 846027/1000000: episode: 8461, duration: 1.814s, episode steps: 100, steps per second: 55, episode reward: 62.969, mean reward: 0.630 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.776, 10.557], loss: 0.001392, mae: 0.040600, mean_q: 1.167579
 846127/1000000: episode: 8462, duration: 1.931s, episode steps: 100, steps per second: 52, episode reward: 59.713, mean reward: 0.597 [0.512, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.812, 10.098], loss: 0.001352, mae: 0.039975, mean_q: 1.168727
 846227/1000000: episode: 8463, duration: 1.685s, episode steps: 100, steps per second: 59, episode reward: 58.593, mean reward: 0.586 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.259, 10.104], loss: 0.001328, mae: 0.039566, mean_q: 1.168500
 846327/1000000: episode: 8464, duration: 1.714s, episode steps: 100, steps per second: 58, episode reward: 59.870, mean reward: 0.599 [0.504, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.094, 10.098], loss: 0.001393, mae: 0.040627, mean_q: 1.170387
 846427/1000000: episode: 8465, duration: 1.780s, episode steps: 100, steps per second: 56, episode reward: 57.822, mean reward: 0.578 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.740, 10.098], loss: 0.001332, mae: 0.040274, mean_q: 1.166849
 846527/1000000: episode: 8466, duration: 1.651s, episode steps: 100, steps per second: 61, episode reward: 58.993, mean reward: 0.590 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.802, 10.098], loss: 0.001320, mae: 0.039914, mean_q: 1.168076
 846627/1000000: episode: 8467, duration: 1.523s, episode steps: 100, steps per second: 66, episode reward: 56.742, mean reward: 0.567 [0.504, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.220, 10.160], loss: 0.001259, mae: 0.039166, mean_q: 1.165439
 846727/1000000: episode: 8468, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 58.672, mean reward: 0.587 [0.498, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.054, 10.098], loss: 0.001316, mae: 0.039343, mean_q: 1.167146
 846827/1000000: episode: 8469, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 57.685, mean reward: 0.577 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.666, 10.218], loss: 0.001361, mae: 0.039869, mean_q: 1.170660
 846927/1000000: episode: 8470, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 58.296, mean reward: 0.583 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.762, 10.262], loss: 0.001371, mae: 0.040472, mean_q: 1.167767
 847027/1000000: episode: 8471, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 56.885, mean reward: 0.569 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.668, 10.247], loss: 0.001324, mae: 0.039667, mean_q: 1.168295
 847127/1000000: episode: 8472, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 57.610, mean reward: 0.576 [0.502, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.098], loss: 0.001222, mae: 0.038156, mean_q: 1.164327
 847227/1000000: episode: 8473, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 59.074, mean reward: 0.591 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.686, 10.098], loss: 0.001285, mae: 0.039683, mean_q: 1.165428
 847327/1000000: episode: 8474, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: 60.647, mean reward: 0.606 [0.520, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.476, 10.098], loss: 0.001257, mae: 0.038743, mean_q: 1.162148
 847427/1000000: episode: 8475, duration: 1.441s, episode steps: 100, steps per second: 69, episode reward: 58.099, mean reward: 0.581 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.098], loss: 0.001292, mae: 0.039396, mean_q: 1.167243
 847527/1000000: episode: 8476, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 61.087, mean reward: 0.611 [0.508, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.058, 10.222], loss: 0.001279, mae: 0.039144, mean_q: 1.164329
 847627/1000000: episode: 8477, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 59.206, mean reward: 0.592 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.974, 10.200], loss: 0.001337, mae: 0.039929, mean_q: 1.168627
 847727/1000000: episode: 8478, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 63.364, mean reward: 0.634 [0.510, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.865, 10.392], loss: 0.001306, mae: 0.039587, mean_q: 1.168373
 847827/1000000: episode: 8479, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 57.699, mean reward: 0.577 [0.501, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.278, 10.282], loss: 0.001265, mae: 0.039235, mean_q: 1.166351
 847927/1000000: episode: 8480, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 57.533, mean reward: 0.575 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.932, 10.098], loss: 0.001324, mae: 0.039570, mean_q: 1.170977
 848027/1000000: episode: 8481, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 58.678, mean reward: 0.587 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.444, 10.171], loss: 0.001343, mae: 0.040392, mean_q: 1.168229
 848127/1000000: episode: 8482, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 59.118, mean reward: 0.591 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.970, 10.140], loss: 0.001272, mae: 0.038790, mean_q: 1.170312
 848227/1000000: episode: 8483, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 58.620, mean reward: 0.586 [0.511, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.736, 10.098], loss: 0.001322, mae: 0.039708, mean_q: 1.166572
 848327/1000000: episode: 8484, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 59.522, mean reward: 0.595 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.641, 10.312], loss: 0.001289, mae: 0.039402, mean_q: 1.163917
 848427/1000000: episode: 8485, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 60.880, mean reward: 0.609 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.759, 10.331], loss: 0.001227, mae: 0.038554, mean_q: 1.162106
 848527/1000000: episode: 8486, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.518, mean reward: 0.585 [0.499, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.762, 10.098], loss: 0.001371, mae: 0.040359, mean_q: 1.164560
 848627/1000000: episode: 8487, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 58.070, mean reward: 0.581 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.613, 10.098], loss: 0.001323, mae: 0.039882, mean_q: 1.164953
 848727/1000000: episode: 8488, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 63.215, mean reward: 0.632 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.992, 10.537], loss: 0.001250, mae: 0.038724, mean_q: 1.165630
 848827/1000000: episode: 8489, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 60.788, mean reward: 0.608 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.321], loss: 0.001332, mae: 0.039470, mean_q: 1.165913
 848927/1000000: episode: 8490, duration: 1.469s, episode steps: 100, steps per second: 68, episode reward: 60.352, mean reward: 0.604 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.406, 10.143], loss: 0.001344, mae: 0.040346, mean_q: 1.166525
 849027/1000000: episode: 8491, duration: 1.763s, episode steps: 100, steps per second: 57, episode reward: 62.696, mean reward: 0.627 [0.526, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.457, 10.098], loss: 0.001425, mae: 0.040418, mean_q: 1.167586
 849127/1000000: episode: 8492, duration: 1.668s, episode steps: 100, steps per second: 60, episode reward: 61.457, mean reward: 0.615 [0.513, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.501, 10.456], loss: 0.001328, mae: 0.039893, mean_q: 1.165889
 849227/1000000: episode: 8493, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 60.780, mean reward: 0.608 [0.515, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.198, 10.098], loss: 0.001334, mae: 0.039842, mean_q: 1.172049
 849327/1000000: episode: 8494, duration: 1.376s, episode steps: 100, steps per second: 73, episode reward: 61.110, mean reward: 0.611 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.332, 10.408], loss: 0.001370, mae: 0.040320, mean_q: 1.172422
 849427/1000000: episode: 8495, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 58.277, mean reward: 0.583 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.363, 10.099], loss: 0.001384, mae: 0.040667, mean_q: 1.174366
 849527/1000000: episode: 8496, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 57.386, mean reward: 0.574 [0.503, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.719, 10.098], loss: 0.001289, mae: 0.039279, mean_q: 1.171847
 849627/1000000: episode: 8497, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 56.866, mean reward: 0.569 [0.503, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.821, 10.119], loss: 0.001425, mae: 0.041028, mean_q: 1.174317
 849727/1000000: episode: 8498, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 58.345, mean reward: 0.583 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.535, 10.098], loss: 0.001341, mae: 0.039837, mean_q: 1.169421
 849827/1000000: episode: 8499, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.019, mean reward: 0.580 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.197, 10.177], loss: 0.001423, mae: 0.041540, mean_q: 1.175362
 849927/1000000: episode: 8500, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 59.007, mean reward: 0.590 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.616, 10.214], loss: 0.001447, mae: 0.041769, mean_q: 1.173411
 850027/1000000: episode: 8501, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 57.523, mean reward: 0.575 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.380, 10.098], loss: 0.001394, mae: 0.040813, mean_q: 1.173156
 850127/1000000: episode: 8502, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 61.759, mean reward: 0.618 [0.510, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.932, 10.315], loss: 0.001353, mae: 0.040341, mean_q: 1.169173
 850227/1000000: episode: 8503, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 57.137, mean reward: 0.571 [0.510, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.910, 10.098], loss: 0.001347, mae: 0.040032, mean_q: 1.170281
 850327/1000000: episode: 8504, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 57.355, mean reward: 0.574 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.391, 10.098], loss: 0.001366, mae: 0.040640, mean_q: 1.169584
 850427/1000000: episode: 8505, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 58.341, mean reward: 0.583 [0.497, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.958, 10.403], loss: 0.001439, mae: 0.041317, mean_q: 1.173155
 850527/1000000: episode: 8506, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 60.079, mean reward: 0.601 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.766, 10.098], loss: 0.001423, mae: 0.041413, mean_q: 1.172667
 850627/1000000: episode: 8507, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 57.491, mean reward: 0.575 [0.499, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.253, 10.180], loss: 0.001375, mae: 0.040813, mean_q: 1.172629
 850727/1000000: episode: 8508, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 59.469, mean reward: 0.595 [0.515, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.620, 10.098], loss: 0.001397, mae: 0.040827, mean_q: 1.170919
 850827/1000000: episode: 8509, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 63.895, mean reward: 0.639 [0.518, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.652, 10.098], loss: 0.001344, mae: 0.039926, mean_q: 1.172498
 850927/1000000: episode: 8510, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 61.518, mean reward: 0.615 [0.506, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.098], loss: 0.001406, mae: 0.041064, mean_q: 1.175469
 851027/1000000: episode: 8511, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 57.426, mean reward: 0.574 [0.515, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.876, 10.098], loss: 0.001419, mae: 0.040769, mean_q: 1.175676
 851127/1000000: episode: 8512, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 58.432, mean reward: 0.584 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.435, 10.100], loss: 0.001406, mae: 0.040541, mean_q: 1.173200
 851227/1000000: episode: 8513, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 61.296, mean reward: 0.613 [0.519, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.838, 10.339], loss: 0.001456, mae: 0.041610, mean_q: 1.170057
 851327/1000000: episode: 8514, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 59.420, mean reward: 0.594 [0.517, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.180, 10.098], loss: 0.001459, mae: 0.041247, mean_q: 1.172033
 851427/1000000: episode: 8515, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 58.414, mean reward: 0.584 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.824, 10.245], loss: 0.001456, mae: 0.041780, mean_q: 1.176031
 851527/1000000: episode: 8516, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 63.297, mean reward: 0.633 [0.507, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.217, 10.098], loss: 0.001464, mae: 0.041514, mean_q: 1.173871
 851627/1000000: episode: 8517, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 58.956, mean reward: 0.590 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.214, 10.208], loss: 0.001494, mae: 0.042104, mean_q: 1.174085
 851727/1000000: episode: 8518, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 58.030, mean reward: 0.580 [0.505, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.474, 10.098], loss: 0.001507, mae: 0.042368, mean_q: 1.175953
 851827/1000000: episode: 8519, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 59.537, mean reward: 0.595 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.324, 10.098], loss: 0.001397, mae: 0.041127, mean_q: 1.175878
 851927/1000000: episode: 8520, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 59.576, mean reward: 0.596 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.858, 10.098], loss: 0.001459, mae: 0.041444, mean_q: 1.175512
 852027/1000000: episode: 8521, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 60.849, mean reward: 0.608 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.180, 10.135], loss: 0.001359, mae: 0.040478, mean_q: 1.176193
 852127/1000000: episode: 8522, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 60.351, mean reward: 0.604 [0.512, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.903, 10.168], loss: 0.001436, mae: 0.041289, mean_q: 1.177361
 852227/1000000: episode: 8523, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.595, mean reward: 0.586 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.409, 10.098], loss: 0.001486, mae: 0.042294, mean_q: 1.181052
 852327/1000000: episode: 8524, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.819, mean reward: 0.588 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.106, 10.098], loss: 0.001432, mae: 0.041085, mean_q: 1.176232
 852427/1000000: episode: 8525, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 60.720, mean reward: 0.607 [0.501, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.762, 10.098], loss: 0.001459, mae: 0.041708, mean_q: 1.178175
 852527/1000000: episode: 8526, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 59.390, mean reward: 0.594 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.865, 10.201], loss: 0.001396, mae: 0.040531, mean_q: 1.177873
 852627/1000000: episode: 8527, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 59.497, mean reward: 0.595 [0.517, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.921, 10.180], loss: 0.001504, mae: 0.041711, mean_q: 1.172704
 852727/1000000: episode: 8528, duration: 1.412s, episode steps: 100, steps per second: 71, episode reward: 58.336, mean reward: 0.583 [0.503, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.975, 10.235], loss: 0.001495, mae: 0.041610, mean_q: 1.174789
 852827/1000000: episode: 8529, duration: 1.647s, episode steps: 100, steps per second: 61, episode reward: 58.059, mean reward: 0.581 [0.508, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.739, 10.098], loss: 0.001418, mae: 0.041666, mean_q: 1.172773
 852927/1000000: episode: 8530, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 62.805, mean reward: 0.628 [0.508, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.437], loss: 0.001491, mae: 0.042111, mean_q: 1.176041
 853027/1000000: episode: 8531, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 57.480, mean reward: 0.575 [0.507, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.740, 10.215], loss: 0.001536, mae: 0.042452, mean_q: 1.177279
 853127/1000000: episode: 8532, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 58.678, mean reward: 0.587 [0.512, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.195], loss: 0.001465, mae: 0.041565, mean_q: 1.179397
 853227/1000000: episode: 8533, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 59.739, mean reward: 0.597 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.815, 10.446], loss: 0.001506, mae: 0.041816, mean_q: 1.177017
 853327/1000000: episode: 8534, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 58.983, mean reward: 0.590 [0.504, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.409, 10.172], loss: 0.001481, mae: 0.041950, mean_q: 1.179627
 853427/1000000: episode: 8535, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.494, mean reward: 0.585 [0.510, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.568, 10.098], loss: 0.001492, mae: 0.041550, mean_q: 1.177549
 853527/1000000: episode: 8536, duration: 1.333s, episode steps: 100, steps per second: 75, episode reward: 57.863, mean reward: 0.579 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.820, 10.098], loss: 0.001418, mae: 0.041274, mean_q: 1.177464
 853627/1000000: episode: 8537, duration: 1.907s, episode steps: 100, steps per second: 52, episode reward: 57.342, mean reward: 0.573 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.877, 10.128], loss: 0.001360, mae: 0.040307, mean_q: 1.175802
 853727/1000000: episode: 8538, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 55.864, mean reward: 0.559 [0.501, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.063, 10.098], loss: 0.001386, mae: 0.040609, mean_q: 1.173954
 853827/1000000: episode: 8539, duration: 1.471s, episode steps: 100, steps per second: 68, episode reward: 61.072, mean reward: 0.611 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.885, 10.436], loss: 0.001513, mae: 0.041935, mean_q: 1.179122
 853927/1000000: episode: 8540, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 60.333, mean reward: 0.603 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.308, 10.098], loss: 0.001512, mae: 0.041510, mean_q: 1.172298
 854027/1000000: episode: 8541, duration: 1.668s, episode steps: 100, steps per second: 60, episode reward: 60.245, mean reward: 0.602 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.958, 10.098], loss: 0.001477, mae: 0.041443, mean_q: 1.174287
 854127/1000000: episode: 8542, duration: 1.527s, episode steps: 100, steps per second: 65, episode reward: 57.250, mean reward: 0.573 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.531, 10.098], loss: 0.001391, mae: 0.040390, mean_q: 1.169778
 854227/1000000: episode: 8543, duration: 1.597s, episode steps: 100, steps per second: 63, episode reward: 60.625, mean reward: 0.606 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.973, 10.098], loss: 0.001402, mae: 0.041038, mean_q: 1.170095
 854327/1000000: episode: 8544, duration: 1.563s, episode steps: 100, steps per second: 64, episode reward: 58.493, mean reward: 0.585 [0.510, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.048, 10.280], loss: 0.001379, mae: 0.040114, mean_q: 1.169032
 854427/1000000: episode: 8545, duration: 1.474s, episode steps: 100, steps per second: 68, episode reward: 57.070, mean reward: 0.571 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.658, 10.098], loss: 0.001519, mae: 0.041946, mean_q: 1.170784
 854527/1000000: episode: 8546, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 59.455, mean reward: 0.595 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.891, 10.098], loss: 0.001392, mae: 0.040197, mean_q: 1.169111
 854627/1000000: episode: 8547, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 60.710, mean reward: 0.607 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.235, 10.098], loss: 0.001447, mae: 0.041232, mean_q: 1.175478
 854727/1000000: episode: 8548, duration: 1.360s, episode steps: 100, steps per second: 74, episode reward: 59.254, mean reward: 0.593 [0.516, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.004, 10.329], loss: 0.001505, mae: 0.041727, mean_q: 1.173523
 854827/1000000: episode: 8549, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 58.308, mean reward: 0.583 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.256, 10.098], loss: 0.001443, mae: 0.040880, mean_q: 1.173875
 854927/1000000: episode: 8550, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 61.281, mean reward: 0.613 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.944, 10.098], loss: 0.001545, mae: 0.042034, mean_q: 1.180417
 855027/1000000: episode: 8551, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.998, mean reward: 0.600 [0.503, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.385, 10.214], loss: 0.001357, mae: 0.039969, mean_q: 1.174383
 855127/1000000: episode: 8552, duration: 1.397s, episode steps: 100, steps per second: 72, episode reward: 58.582, mean reward: 0.586 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.649, 10.121], loss: 0.001477, mae: 0.041284, mean_q: 1.177231
 855227/1000000: episode: 8553, duration: 1.820s, episode steps: 100, steps per second: 55, episode reward: 56.691, mean reward: 0.567 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.781, 10.183], loss: 0.001454, mae: 0.041313, mean_q: 1.175103
 855327/1000000: episode: 8554, duration: 1.645s, episode steps: 100, steps per second: 61, episode reward: 58.058, mean reward: 0.581 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.959, 10.326], loss: 0.001497, mae: 0.041081, mean_q: 1.176049
 855427/1000000: episode: 8555, duration: 1.452s, episode steps: 100, steps per second: 69, episode reward: 56.490, mean reward: 0.565 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.294, 10.105], loss: 0.001516, mae: 0.041775, mean_q: 1.175132
 855527/1000000: episode: 8556, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 60.744, mean reward: 0.607 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.920, 10.168], loss: 0.001643, mae: 0.043005, mean_q: 1.179278
 855627/1000000: episode: 8557, duration: 1.740s, episode steps: 100, steps per second: 57, episode reward: 59.898, mean reward: 0.599 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.495, 10.116], loss: 0.001492, mae: 0.041462, mean_q: 1.178108
 855727/1000000: episode: 8558, duration: 1.823s, episode steps: 100, steps per second: 55, episode reward: 60.434, mean reward: 0.604 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.115, 10.098], loss: 0.001363, mae: 0.040389, mean_q: 1.173358
 855827/1000000: episode: 8559, duration: 1.653s, episode steps: 100, steps per second: 60, episode reward: 59.228, mean reward: 0.592 [0.514, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.552, 10.223], loss: 0.001463, mae: 0.040739, mean_q: 1.171902
 855927/1000000: episode: 8560, duration: 1.691s, episode steps: 100, steps per second: 59, episode reward: 59.103, mean reward: 0.591 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.447, 10.214], loss: 0.001483, mae: 0.040762, mean_q: 1.173067
 856027/1000000: episode: 8561, duration: 1.731s, episode steps: 100, steps per second: 58, episode reward: 58.484, mean reward: 0.585 [0.503, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.024, 10.098], loss: 0.001434, mae: 0.040944, mean_q: 1.173410
 856127/1000000: episode: 8562, duration: 1.843s, episode steps: 100, steps per second: 54, episode reward: 57.912, mean reward: 0.579 [0.501, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.292, 10.098], loss: 0.001547, mae: 0.042341, mean_q: 1.172096
 856227/1000000: episode: 8563, duration: 1.595s, episode steps: 100, steps per second: 63, episode reward: 57.668, mean reward: 0.577 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.098], loss: 0.001489, mae: 0.040886, mean_q: 1.170888
 856327/1000000: episode: 8564, duration: 1.593s, episode steps: 100, steps per second: 63, episode reward: 59.817, mean reward: 0.598 [0.511, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.636, 10.098], loss: 0.001474, mae: 0.041089, mean_q: 1.168014
 856427/1000000: episode: 8565, duration: 1.592s, episode steps: 100, steps per second: 63, episode reward: 58.504, mean reward: 0.585 [0.511, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.814, 10.098], loss: 0.001576, mae: 0.042347, mean_q: 1.171351
 856527/1000000: episode: 8566, duration: 1.606s, episode steps: 100, steps per second: 62, episode reward: 57.300, mean reward: 0.573 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.678, 10.098], loss: 0.001534, mae: 0.041024, mean_q: 1.170472
 856627/1000000: episode: 8567, duration: 1.699s, episode steps: 100, steps per second: 59, episode reward: 58.143, mean reward: 0.581 [0.497, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.020, 10.098], loss: 0.001459, mae: 0.040640, mean_q: 1.167783
 856727/1000000: episode: 8568, duration: 1.786s, episode steps: 100, steps per second: 56, episode reward: 58.645, mean reward: 0.586 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.974, 10.176], loss: 0.001441, mae: 0.040326, mean_q: 1.168494
 856827/1000000: episode: 8569, duration: 1.557s, episode steps: 100, steps per second: 64, episode reward: 58.622, mean reward: 0.586 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.000, 10.284], loss: 0.001449, mae: 0.040538, mean_q: 1.165939
 856927/1000000: episode: 8570, duration: 1.568s, episode steps: 100, steps per second: 64, episode reward: 58.120, mean reward: 0.581 [0.500, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.212, 10.098], loss: 0.001509, mae: 0.040884, mean_q: 1.165545
 857027/1000000: episode: 8571, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 57.337, mean reward: 0.573 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.335, 10.098], loss: 0.001557, mae: 0.042007, mean_q: 1.167792
 857127/1000000: episode: 8572, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 57.348, mean reward: 0.573 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.225, 10.098], loss: 0.001498, mae: 0.040803, mean_q: 1.163478
 857227/1000000: episode: 8573, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 61.306, mean reward: 0.613 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.098], loss: 0.001504, mae: 0.041849, mean_q: 1.167692
 857327/1000000: episode: 8574, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 59.562, mean reward: 0.596 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.576, 10.098], loss: 0.001547, mae: 0.041732, mean_q: 1.166504
 857427/1000000: episode: 8575, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 58.329, mean reward: 0.583 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.878, 10.144], loss: 0.001418, mae: 0.040338, mean_q: 1.166731
 857527/1000000: episode: 8576, duration: 1.446s, episode steps: 100, steps per second: 69, episode reward: 61.537, mean reward: 0.615 [0.513, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.892, 10.098], loss: 0.001651, mae: 0.043024, mean_q: 1.167796
 857627/1000000: episode: 8577, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 59.592, mean reward: 0.596 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.290, 10.280], loss: 0.001515, mae: 0.041513, mean_q: 1.167551
 857727/1000000: episode: 8578, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 57.963, mean reward: 0.580 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.115, 10.098], loss: 0.001558, mae: 0.041847, mean_q: 1.167778
 857827/1000000: episode: 8579, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 58.255, mean reward: 0.583 [0.499, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.180], loss: 0.001580, mae: 0.040973, mean_q: 1.164351
 857927/1000000: episode: 8580, duration: 0.995s, episode steps: 100, steps per second: 100, episode reward: 62.125, mean reward: 0.621 [0.504, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.878, 10.296], loss: 0.001612, mae: 0.042705, mean_q: 1.165582
 858027/1000000: episode: 8581, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 56.884, mean reward: 0.569 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.639, 10.126], loss: 0.001590, mae: 0.042412, mean_q: 1.168169
 858127/1000000: episode: 8582, duration: 1.069s, episode steps: 100, steps per second: 94, episode reward: 60.067, mean reward: 0.601 [0.506, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.382, 10.098], loss: 0.001521, mae: 0.041671, mean_q: 1.164526
 858227/1000000: episode: 8583, duration: 1.697s, episode steps: 100, steps per second: 59, episode reward: 58.023, mean reward: 0.580 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.652, 10.145], loss: 0.001505, mae: 0.040994, mean_q: 1.162986
 858327/1000000: episode: 8584, duration: 1.848s, episode steps: 100, steps per second: 54, episode reward: 59.791, mean reward: 0.598 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.340, 10.098], loss: 0.001558, mae: 0.042023, mean_q: 1.166796
 858427/1000000: episode: 8585, duration: 1.747s, episode steps: 100, steps per second: 57, episode reward: 58.709, mean reward: 0.587 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.508, 10.098], loss: 0.001565, mae: 0.042457, mean_q: 1.164562
 858527/1000000: episode: 8586, duration: 1.709s, episode steps: 100, steps per second: 59, episode reward: 58.380, mean reward: 0.584 [0.511, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.505, 10.098], loss: 0.001614, mae: 0.042798, mean_q: 1.165164
 858627/1000000: episode: 8587, duration: 1.708s, episode steps: 100, steps per second: 59, episode reward: 59.386, mean reward: 0.594 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.262], loss: 0.001551, mae: 0.041749, mean_q: 1.165409
 858727/1000000: episode: 8588, duration: 1.704s, episode steps: 100, steps per second: 59, episode reward: 58.870, mean reward: 0.589 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.959, 10.151], loss: 0.001552, mae: 0.041338, mean_q: 1.163765
 858827/1000000: episode: 8589, duration: 1.624s, episode steps: 100, steps per second: 62, episode reward: 57.553, mean reward: 0.576 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.766, 10.195], loss: 0.001409, mae: 0.040580, mean_q: 1.163469
 858927/1000000: episode: 8590, duration: 1.605s, episode steps: 100, steps per second: 62, episode reward: 57.865, mean reward: 0.579 [0.501, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.063, 10.098], loss: 0.001567, mae: 0.042412, mean_q: 1.166712
 859027/1000000: episode: 8591, duration: 1.545s, episode steps: 100, steps per second: 65, episode reward: 59.755, mean reward: 0.598 [0.512, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.967, 10.098], loss: 0.001583, mae: 0.042509, mean_q: 1.166526
 859127/1000000: episode: 8592, duration: 1.523s, episode steps: 100, steps per second: 66, episode reward: 58.378, mean reward: 0.584 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.251], loss: 0.001603, mae: 0.042376, mean_q: 1.162572
 859227/1000000: episode: 8593, duration: 1.544s, episode steps: 100, steps per second: 65, episode reward: 59.091, mean reward: 0.591 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.052, 10.098], loss: 0.001586, mae: 0.042130, mean_q: 1.163393
 859327/1000000: episode: 8594, duration: 1.572s, episode steps: 100, steps per second: 64, episode reward: 58.350, mean reward: 0.584 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.416, 10.164], loss: 0.001658, mae: 0.043365, mean_q: 1.163887
 859427/1000000: episode: 8595, duration: 1.438s, episode steps: 100, steps per second: 70, episode reward: 62.442, mean reward: 0.624 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.252, 10.098], loss: 0.001631, mae: 0.042910, mean_q: 1.166634
 859527/1000000: episode: 8596, duration: 1.819s, episode steps: 100, steps per second: 55, episode reward: 58.840, mean reward: 0.588 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.295], loss: 0.001540, mae: 0.042429, mean_q: 1.166697
 859627/1000000: episode: 8597, duration: 1.526s, episode steps: 100, steps per second: 66, episode reward: 59.317, mean reward: 0.593 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.251, 10.098], loss: 0.001488, mae: 0.041456, mean_q: 1.165756
 859727/1000000: episode: 8598, duration: 1.264s, episode steps: 100, steps per second: 79, episode reward: 58.228, mean reward: 0.582 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.385, 10.098], loss: 0.001568, mae: 0.042796, mean_q: 1.165831
 859827/1000000: episode: 8599, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 61.603, mean reward: 0.616 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.779, 10.175], loss: 0.001681, mae: 0.043938, mean_q: 1.167452
 859927/1000000: episode: 8600, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 59.343, mean reward: 0.593 [0.520, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.000, 10.185], loss: 0.001599, mae: 0.042536, mean_q: 1.167979
 860027/1000000: episode: 8601, duration: 1.093s, episode steps: 100, steps per second: 91, episode reward: 58.008, mean reward: 0.580 [0.506, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.831, 10.154], loss: 0.001534, mae: 0.042092, mean_q: 1.163023
 860127/1000000: episode: 8602, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 57.452, mean reward: 0.575 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.078, 10.098], loss: 0.001573, mae: 0.042444, mean_q: 1.164987
 860227/1000000: episode: 8603, duration: 1.354s, episode steps: 100, steps per second: 74, episode reward: 57.495, mean reward: 0.575 [0.501, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.417, 10.193], loss: 0.001601, mae: 0.042985, mean_q: 1.165269
 860327/1000000: episode: 8604, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 64.029, mean reward: 0.640 [0.528, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.565, 10.098], loss: 0.001548, mae: 0.042409, mean_q: 1.164523
 860427/1000000: episode: 8605, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 60.624, mean reward: 0.606 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.159, 10.149], loss: 0.001599, mae: 0.043228, mean_q: 1.171158
 860527/1000000: episode: 8606, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 57.595, mean reward: 0.576 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.258, 10.098], loss: 0.001640, mae: 0.043070, mean_q: 1.168813
 860627/1000000: episode: 8607, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 59.599, mean reward: 0.596 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.321, 10.098], loss: 0.001591, mae: 0.042981, mean_q: 1.169521
 860727/1000000: episode: 8608, duration: 1.097s, episode steps: 100, steps per second: 91, episode reward: 58.372, mean reward: 0.584 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.248], loss: 0.001584, mae: 0.043091, mean_q: 1.168227
 860827/1000000: episode: 8609, duration: 1.594s, episode steps: 100, steps per second: 63, episode reward: 57.484, mean reward: 0.575 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.856, 10.204], loss: 0.001670, mae: 0.043332, mean_q: 1.168457
 860927/1000000: episode: 8610, duration: 1.105s, episode steps: 100, steps per second: 91, episode reward: 56.393, mean reward: 0.564 [0.505, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.415, 10.098], loss: 0.001706, mae: 0.043972, mean_q: 1.169602
 861027/1000000: episode: 8611, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 58.526, mean reward: 0.585 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.233, 10.108], loss: 0.001564, mae: 0.042749, mean_q: 1.166537
 861127/1000000: episode: 8612, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 57.426, mean reward: 0.574 [0.506, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.033, 10.098], loss: 0.001480, mae: 0.041380, mean_q: 1.168077
 861227/1000000: episode: 8613, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 57.269, mean reward: 0.573 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.962, 10.183], loss: 0.001603, mae: 0.043181, mean_q: 1.167838
 861327/1000000: episode: 8614, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 58.798, mean reward: 0.588 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.299, 10.098], loss: 0.001632, mae: 0.043856, mean_q: 1.166357
 861427/1000000: episode: 8615, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 60.935, mean reward: 0.609 [0.516, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.187, 10.098], loss: 0.001507, mae: 0.042218, mean_q: 1.166697
 861527/1000000: episode: 8616, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 58.541, mean reward: 0.585 [0.505, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.490, 10.320], loss: 0.001448, mae: 0.040936, mean_q: 1.165185
 861627/1000000: episode: 8617, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 57.224, mean reward: 0.572 [0.505, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.219], loss: 0.001486, mae: 0.041596, mean_q: 1.167570
 861727/1000000: episode: 8618, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.551, mean reward: 0.586 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.375, 10.098], loss: 0.001515, mae: 0.042527, mean_q: 1.167093
 861827/1000000: episode: 8619, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 59.740, mean reward: 0.597 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.978, 10.098], loss: 0.001566, mae: 0.042899, mean_q: 1.166348
 861927/1000000: episode: 8620, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 57.995, mean reward: 0.580 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.223, 10.326], loss: 0.001499, mae: 0.041966, mean_q: 1.168088
 862027/1000000: episode: 8621, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 57.707, mean reward: 0.577 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.945, 10.098], loss: 0.001397, mae: 0.040561, mean_q: 1.166374
 862127/1000000: episode: 8622, duration: 1.424s, episode steps: 100, steps per second: 70, episode reward: 63.603, mean reward: 0.636 [0.529, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.098], loss: 0.001548, mae: 0.042498, mean_q: 1.165898
 862227/1000000: episode: 8623, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 57.430, mean reward: 0.574 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.443, 10.098], loss: 0.001449, mae: 0.040908, mean_q: 1.166866
 862327/1000000: episode: 8624, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 57.429, mean reward: 0.574 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.438, 10.098], loss: 0.001463, mae: 0.041482, mean_q: 1.164862
 862427/1000000: episode: 8625, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 56.807, mean reward: 0.568 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.882, 10.098], loss: 0.001576, mae: 0.042658, mean_q: 1.164821
 862527/1000000: episode: 8626, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 59.382, mean reward: 0.594 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.403, 10.146], loss: 0.001478, mae: 0.041731, mean_q: 1.161573
 862627/1000000: episode: 8627, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 60.657, mean reward: 0.607 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.235], loss: 0.001499, mae: 0.041320, mean_q: 1.163000
 862727/1000000: episode: 8628, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 58.910, mean reward: 0.589 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.648, 10.101], loss: 0.001453, mae: 0.041979, mean_q: 1.164109
 862827/1000000: episode: 8629, duration: 1.122s, episode steps: 100, steps per second: 89, episode reward: 58.560, mean reward: 0.586 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.567, 10.241], loss: 0.001471, mae: 0.041796, mean_q: 1.164636
 862927/1000000: episode: 8630, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 60.904, mean reward: 0.609 [0.510, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.154, 10.267], loss: 0.001539, mae: 0.042220, mean_q: 1.167564
 863027/1000000: episode: 8631, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: 59.277, mean reward: 0.593 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.589, 10.196], loss: 0.001442, mae: 0.041295, mean_q: 1.166634
 863127/1000000: episode: 8632, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 61.947, mean reward: 0.619 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.472, 10.260], loss: 0.001478, mae: 0.041606, mean_q: 1.164359
 863227/1000000: episode: 8633, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 57.452, mean reward: 0.575 [0.503, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.986, 10.183], loss: 0.001463, mae: 0.041385, mean_q: 1.166695
 863327/1000000: episode: 8634, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 59.402, mean reward: 0.594 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.076, 10.098], loss: 0.001352, mae: 0.039883, mean_q: 1.164237
 863427/1000000: episode: 8635, duration: 1.317s, episode steps: 100, steps per second: 76, episode reward: 58.992, mean reward: 0.590 [0.500, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.213], loss: 0.001590, mae: 0.042482, mean_q: 1.169142
 863527/1000000: episode: 8636, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 62.107, mean reward: 0.621 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.938, 10.098], loss: 0.001505, mae: 0.041892, mean_q: 1.167415
 863627/1000000: episode: 8637, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 63.323, mean reward: 0.633 [0.511, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.325], loss: 0.001451, mae: 0.041148, mean_q: 1.166142
 863727/1000000: episode: 8638, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 57.194, mean reward: 0.572 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.227, 10.098], loss: 0.001453, mae: 0.041377, mean_q: 1.167853
 863827/1000000: episode: 8639, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 61.795, mean reward: 0.618 [0.502, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.781, 10.098], loss: 0.001599, mae: 0.043099, mean_q: 1.169448
 863927/1000000: episode: 8640, duration: 0.985s, episode steps: 100, steps per second: 102, episode reward: 59.015, mean reward: 0.590 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.013, 10.329], loss: 0.001456, mae: 0.041467, mean_q: 1.169408
 864027/1000000: episode: 8641, duration: 1.036s, episode steps: 100, steps per second: 96, episode reward: 57.135, mean reward: 0.571 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.167, 10.098], loss: 0.001569, mae: 0.042599, mean_q: 1.169489
 864127/1000000: episode: 8642, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 57.737, mean reward: 0.577 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.498, 10.098], loss: 0.001411, mae: 0.040531, mean_q: 1.169804
 864227/1000000: episode: 8643, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 58.330, mean reward: 0.583 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.872, 10.239], loss: 0.001553, mae: 0.042738, mean_q: 1.170326
 864327/1000000: episode: 8644, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 58.228, mean reward: 0.582 [0.511, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.087, 10.246], loss: 0.001547, mae: 0.042561, mean_q: 1.166024
 864427/1000000: episode: 8645, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 57.598, mean reward: 0.576 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.658, 10.098], loss: 0.001483, mae: 0.041610, mean_q: 1.168480
 864527/1000000: episode: 8646, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 58.086, mean reward: 0.581 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.869, 10.098], loss: 0.001430, mae: 0.041301, mean_q: 1.166554
 864627/1000000: episode: 8647, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 59.747, mean reward: 0.597 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.041, 10.098], loss: 0.001466, mae: 0.041891, mean_q: 1.169647
 864727/1000000: episode: 8648, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.057, mean reward: 0.581 [0.502, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.199, 10.141], loss: 0.001504, mae: 0.042412, mean_q: 1.169006
 864827/1000000: episode: 8649, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 62.427, mean reward: 0.624 [0.521, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.098], loss: 0.001438, mae: 0.040485, mean_q: 1.165841
 864927/1000000: episode: 8650, duration: 1.624s, episode steps: 100, steps per second: 62, episode reward: 59.790, mean reward: 0.598 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.854, 10.098], loss: 0.001593, mae: 0.042823, mean_q: 1.169447
 865027/1000000: episode: 8651, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 60.522, mean reward: 0.605 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.260, 10.098], loss: 0.001598, mae: 0.043375, mean_q: 1.167650
 865127/1000000: episode: 8652, duration: 1.825s, episode steps: 100, steps per second: 55, episode reward: 56.672, mean reward: 0.567 [0.502, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.386, 10.110], loss: 0.001595, mae: 0.042878, mean_q: 1.168222
 865227/1000000: episode: 8653, duration: 1.281s, episode steps: 100, steps per second: 78, episode reward: 59.143, mean reward: 0.591 [0.498, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.998, 10.200], loss: 0.001417, mae: 0.040862, mean_q: 1.166736
 865327/1000000: episode: 8654, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 60.262, mean reward: 0.603 [0.510, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.098], loss: 0.001532, mae: 0.042340, mean_q: 1.166801
 865427/1000000: episode: 8655, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 59.033, mean reward: 0.590 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.994, 10.138], loss: 0.001520, mae: 0.042316, mean_q: 1.168377
 865527/1000000: episode: 8656, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 59.138, mean reward: 0.591 [0.506, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.588, 10.117], loss: 0.001473, mae: 0.041650, mean_q: 1.169175
 865627/1000000: episode: 8657, duration: 1.478s, episode steps: 100, steps per second: 68, episode reward: 59.559, mean reward: 0.596 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.269, 10.165], loss: 0.001454, mae: 0.041241, mean_q: 1.167144
 865727/1000000: episode: 8658, duration: 1.826s, episode steps: 100, steps per second: 55, episode reward: 58.358, mean reward: 0.584 [0.514, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.290, 10.431], loss: 0.001472, mae: 0.041438, mean_q: 1.168798
 865827/1000000: episode: 8659, duration: 1.594s, episode steps: 100, steps per second: 63, episode reward: 59.160, mean reward: 0.592 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.386], loss: 0.001433, mae: 0.041029, mean_q: 1.164823
 865927/1000000: episode: 8660, duration: 1.683s, episode steps: 100, steps per second: 59, episode reward: 57.957, mean reward: 0.580 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.979, 10.262], loss: 0.001592, mae: 0.042930, mean_q: 1.171734
 866027/1000000: episode: 8661, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 58.932, mean reward: 0.589 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.282, 10.098], loss: 0.001474, mae: 0.041347, mean_q: 1.167370
 866127/1000000: episode: 8662, duration: 1.701s, episode steps: 100, steps per second: 59, episode reward: 63.197, mean reward: 0.632 [0.519, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.508, 10.473], loss: 0.001509, mae: 0.041980, mean_q: 1.173451
 866227/1000000: episode: 8663, duration: 1.547s, episode steps: 100, steps per second: 65, episode reward: 59.216, mean reward: 0.592 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.079, 10.195], loss: 0.001459, mae: 0.041414, mean_q: 1.169552
 866327/1000000: episode: 8664, duration: 1.377s, episode steps: 100, steps per second: 73, episode reward: 59.610, mean reward: 0.596 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.282, 10.249], loss: 0.001531, mae: 0.043077, mean_q: 1.173406
 866427/1000000: episode: 8665, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 59.496, mean reward: 0.595 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.830, 10.098], loss: 0.001475, mae: 0.041455, mean_q: 1.174471
 866527/1000000: episode: 8666, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 56.902, mean reward: 0.569 [0.512, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.095, 10.106], loss: 0.001487, mae: 0.041866, mean_q: 1.172620
 866627/1000000: episode: 8667, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 58.909, mean reward: 0.589 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.380, 10.098], loss: 0.001515, mae: 0.042350, mean_q: 1.171243
 866727/1000000: episode: 8668, duration: 1.604s, episode steps: 100, steps per second: 62, episode reward: 58.597, mean reward: 0.586 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.545, 10.259], loss: 0.001554, mae: 0.041857, mean_q: 1.170049
 866827/1000000: episode: 8669, duration: 1.630s, episode steps: 100, steps per second: 61, episode reward: 58.453, mean reward: 0.585 [0.516, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.294, 10.198], loss: 0.001561, mae: 0.042950, mean_q: 1.174195
 866927/1000000: episode: 8670, duration: 1.300s, episode steps: 100, steps per second: 77, episode reward: 58.939, mean reward: 0.589 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.008, 10.177], loss: 0.001505, mae: 0.041849, mean_q: 1.171820
 867027/1000000: episode: 8671, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 59.541, mean reward: 0.595 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.233, 10.098], loss: 0.001427, mae: 0.041487, mean_q: 1.173351
 867127/1000000: episode: 8672, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 58.297, mean reward: 0.583 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.415, 10.098], loss: 0.001492, mae: 0.042090, mean_q: 1.170740
 867227/1000000: episode: 8673, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 57.116, mean reward: 0.571 [0.501, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.981, 10.098], loss: 0.001452, mae: 0.041334, mean_q: 1.173152
 867327/1000000: episode: 8674, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 66.748, mean reward: 0.667 [0.511, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.143, 10.098], loss: 0.001456, mae: 0.041834, mean_q: 1.171720
 867427/1000000: episode: 8675, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 59.202, mean reward: 0.592 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.504, 10.098], loss: 0.001457, mae: 0.041784, mean_q: 1.178115
 867527/1000000: episode: 8676, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 56.794, mean reward: 0.568 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.997, 10.176], loss: 0.001422, mae: 0.041268, mean_q: 1.175730
 867627/1000000: episode: 8677, duration: 1.457s, episode steps: 100, steps per second: 69, episode reward: 58.966, mean reward: 0.590 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.784, 10.098], loss: 0.001498, mae: 0.042346, mean_q: 1.175613
 867727/1000000: episode: 8678, duration: 1.536s, episode steps: 100, steps per second: 65, episode reward: 60.150, mean reward: 0.602 [0.514, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.734, 10.098], loss: 0.001414, mae: 0.040944, mean_q: 1.174927
 867827/1000000: episode: 8679, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 60.773, mean reward: 0.608 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.278], loss: 0.001421, mae: 0.040595, mean_q: 1.170526
 867927/1000000: episode: 8680, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 57.873, mean reward: 0.579 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.165], loss: 0.001429, mae: 0.040924, mean_q: 1.172971
 868027/1000000: episode: 8681, duration: 1.572s, episode steps: 100, steps per second: 64, episode reward: 60.312, mean reward: 0.603 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.684, 10.336], loss: 0.001421, mae: 0.040709, mean_q: 1.169649
 868127/1000000: episode: 8682, duration: 1.709s, episode steps: 100, steps per second: 59, episode reward: 58.706, mean reward: 0.587 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.180, 10.098], loss: 0.001457, mae: 0.041997, mean_q: 1.167807
 868227/1000000: episode: 8683, duration: 1.767s, episode steps: 100, steps per second: 57, episode reward: 59.459, mean reward: 0.595 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.034, 10.229], loss: 0.001404, mae: 0.040533, mean_q: 1.168790
 868327/1000000: episode: 8684, duration: 2.000s, episode steps: 100, steps per second: 50, episode reward: 58.942, mean reward: 0.589 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.272, 10.121], loss: 0.001460, mae: 0.041601, mean_q: 1.176936
 868427/1000000: episode: 8685, duration: 2.002s, episode steps: 100, steps per second: 50, episode reward: 58.749, mean reward: 0.587 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.320], loss: 0.001380, mae: 0.040483, mean_q: 1.173051
 868527/1000000: episode: 8686, duration: 1.665s, episode steps: 100, steps per second: 60, episode reward: 58.716, mean reward: 0.587 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.358, 10.201], loss: 0.001359, mae: 0.040110, mean_q: 1.170677
 868627/1000000: episode: 8687, duration: 1.748s, episode steps: 100, steps per second: 57, episode reward: 57.510, mean reward: 0.575 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.422, 10.175], loss: 0.001495, mae: 0.042499, mean_q: 1.171338
 868727/1000000: episode: 8688, duration: 1.828s, episode steps: 100, steps per second: 55, episode reward: 59.465, mean reward: 0.595 [0.517, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.501, 10.098], loss: 0.001459, mae: 0.041412, mean_q: 1.172220
 868827/1000000: episode: 8689, duration: 1.619s, episode steps: 100, steps per second: 62, episode reward: 59.331, mean reward: 0.593 [0.506, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.152], loss: 0.001463, mae: 0.041433, mean_q: 1.167459
 868927/1000000: episode: 8690, duration: 1.790s, episode steps: 100, steps per second: 56, episode reward: 56.964, mean reward: 0.570 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.789, 10.098], loss: 0.001419, mae: 0.041131, mean_q: 1.170866
 869027/1000000: episode: 8691, duration: 1.609s, episode steps: 100, steps per second: 62, episode reward: 59.605, mean reward: 0.596 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.602, 10.278], loss: 0.001458, mae: 0.041781, mean_q: 1.168198
 869127/1000000: episode: 8692, duration: 1.486s, episode steps: 100, steps per second: 67, episode reward: 57.735, mean reward: 0.577 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.986, 10.139], loss: 0.001439, mae: 0.040807, mean_q: 1.170152
 869227/1000000: episode: 8693, duration: 1.693s, episode steps: 100, steps per second: 59, episode reward: 57.847, mean reward: 0.578 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.449, 10.226], loss: 0.001456, mae: 0.041508, mean_q: 1.169886
 869327/1000000: episode: 8694, duration: 1.498s, episode steps: 100, steps per second: 67, episode reward: 57.952, mean reward: 0.580 [0.507, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.070, 10.122], loss: 0.001463, mae: 0.041754, mean_q: 1.169007
 869427/1000000: episode: 8695, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 61.196, mean reward: 0.612 [0.512, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.098], loss: 0.001382, mae: 0.040641, mean_q: 1.168995
 869527/1000000: episode: 8696, duration: 1.047s, episode steps: 100, steps per second: 95, episode reward: 60.542, mean reward: 0.605 [0.517, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.650, 10.098], loss: 0.001385, mae: 0.040850, mean_q: 1.168322
 869627/1000000: episode: 8697, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 56.157, mean reward: 0.562 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.235, 10.102], loss: 0.001406, mae: 0.040928, mean_q: 1.171503
 869727/1000000: episode: 8698, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 57.566, mean reward: 0.576 [0.504, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.225, 10.119], loss: 0.001433, mae: 0.040957, mean_q: 1.169920
 869827/1000000: episode: 8699, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 61.796, mean reward: 0.618 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.808, 10.098], loss: 0.001406, mae: 0.040813, mean_q: 1.169538
 869927/1000000: episode: 8700, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 65.681, mean reward: 0.657 [0.541, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.074, 10.362], loss: 0.001349, mae: 0.039579, mean_q: 1.171861
 870027/1000000: episode: 8701, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 57.817, mean reward: 0.578 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.098], loss: 0.001534, mae: 0.041614, mean_q: 1.172381
 870127/1000000: episode: 8702, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 60.701, mean reward: 0.607 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.689, 10.098], loss: 0.001434, mae: 0.040479, mean_q: 1.172076
 870227/1000000: episode: 8703, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 58.745, mean reward: 0.587 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.622, 10.450], loss: 0.001535, mae: 0.041931, mean_q: 1.173988
 870327/1000000: episode: 8704, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.879, mean reward: 0.589 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.226, 10.189], loss: 0.001419, mae: 0.040462, mean_q: 1.172432
 870427/1000000: episode: 8705, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 59.887, mean reward: 0.599 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.666, 10.098], loss: 0.001353, mae: 0.040325, mean_q: 1.170550
 870527/1000000: episode: 8706, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 57.690, mean reward: 0.577 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.454, 10.098], loss: 0.001333, mae: 0.039770, mean_q: 1.170377
 870627/1000000: episode: 8707, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 57.155, mean reward: 0.572 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.801, 10.144], loss: 0.001407, mae: 0.041039, mean_q: 1.170248
 870727/1000000: episode: 8708, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 56.109, mean reward: 0.561 [0.504, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.225], loss: 0.001344, mae: 0.040135, mean_q: 1.166592
 870827/1000000: episode: 8709, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 59.924, mean reward: 0.599 [0.502, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.964, 10.099], loss: 0.001310, mae: 0.040158, mean_q: 1.168782
 870927/1000000: episode: 8710, duration: 1.212s, episode steps: 100, steps per second: 83, episode reward: 57.686, mean reward: 0.577 [0.513, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.032, 10.251], loss: 0.001478, mae: 0.041883, mean_q: 1.169956
 871027/1000000: episode: 8711, duration: 1.393s, episode steps: 100, steps per second: 72, episode reward: 57.606, mean reward: 0.576 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.098], loss: 0.001481, mae: 0.042229, mean_q: 1.170254
 871127/1000000: episode: 8712, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 59.173, mean reward: 0.592 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.719, 10.098], loss: 0.001500, mae: 0.041528, mean_q: 1.170085
 871227/1000000: episode: 8713, duration: 1.381s, episode steps: 100, steps per second: 72, episode reward: 58.552, mean reward: 0.586 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.644, 10.098], loss: 0.001396, mae: 0.040328, mean_q: 1.164837
 871327/1000000: episode: 8714, duration: 1.655s, episode steps: 100, steps per second: 60, episode reward: 57.396, mean reward: 0.574 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.113], loss: 0.001410, mae: 0.040534, mean_q: 1.168890
 871427/1000000: episode: 8715, duration: 1.775s, episode steps: 100, steps per second: 56, episode reward: 59.682, mean reward: 0.597 [0.504, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.504, 10.135], loss: 0.001385, mae: 0.040371, mean_q: 1.169027
 871527/1000000: episode: 8716, duration: 1.911s, episode steps: 100, steps per second: 52, episode reward: 59.127, mean reward: 0.591 [0.502, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.346, 10.599], loss: 0.001483, mae: 0.041926, mean_q: 1.169960
 871627/1000000: episode: 8717, duration: 1.612s, episode steps: 100, steps per second: 62, episode reward: 58.846, mean reward: 0.588 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.907, 10.098], loss: 0.001403, mae: 0.040479, mean_q: 1.168662
 871727/1000000: episode: 8718, duration: 1.641s, episode steps: 100, steps per second: 61, episode reward: 57.201, mean reward: 0.572 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.533, 10.193], loss: 0.001487, mae: 0.041695, mean_q: 1.169630
 871827/1000000: episode: 8719, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: 55.914, mean reward: 0.559 [0.501, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.537, 10.179], loss: 0.001423, mae: 0.040715, mean_q: 1.165959
 871927/1000000: episode: 8720, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 57.401, mean reward: 0.574 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.697, 10.172], loss: 0.001442, mae: 0.041515, mean_q: 1.165173
 872027/1000000: episode: 8721, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 58.300, mean reward: 0.583 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.371, 10.218], loss: 0.001472, mae: 0.041677, mean_q: 1.162264
 872127/1000000: episode: 8722, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 60.732, mean reward: 0.607 [0.500, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.904, 10.098], loss: 0.001570, mae: 0.042576, mean_q: 1.165857
 872227/1000000: episode: 8723, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 58.507, mean reward: 0.585 [0.506, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.654, 10.098], loss: 0.001613, mae: 0.043425, mean_q: 1.169831
 872327/1000000: episode: 8724, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 59.602, mean reward: 0.596 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.147, 10.324], loss: 0.001570, mae: 0.042772, mean_q: 1.164386
 872427/1000000: episode: 8725, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 58.126, mean reward: 0.581 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.227, 10.098], loss: 0.001490, mae: 0.042034, mean_q: 1.164519
 872527/1000000: episode: 8726, duration: 0.999s, episode steps: 100, steps per second: 100, episode reward: 60.739, mean reward: 0.607 [0.499, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.934, 10.098], loss: 0.001637, mae: 0.043501, mean_q: 1.163540
 872627/1000000: episode: 8727, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 61.301, mean reward: 0.613 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.803, 10.098], loss: 0.001517, mae: 0.041891, mean_q: 1.168908
 872727/1000000: episode: 8728, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 57.868, mean reward: 0.579 [0.510, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.640, 10.098], loss: 0.001404, mae: 0.040765, mean_q: 1.165549
 872827/1000000: episode: 8729, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 59.595, mean reward: 0.596 [0.504, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.886, 10.098], loss: 0.001544, mae: 0.041994, mean_q: 1.166875
 872927/1000000: episode: 8730, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 59.195, mean reward: 0.592 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.707, 10.143], loss: 0.001577, mae: 0.042889, mean_q: 1.166170
 873027/1000000: episode: 8731, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 60.621, mean reward: 0.606 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.098], loss: 0.001476, mae: 0.041860, mean_q: 1.166450
 873127/1000000: episode: 8732, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 58.976, mean reward: 0.590 [0.497, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.873, 10.279], loss: 0.001459, mae: 0.041989, mean_q: 1.168629
 873227/1000000: episode: 8733, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 58.401, mean reward: 0.584 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.228], loss: 0.001600, mae: 0.042958, mean_q: 1.163667
 873327/1000000: episode: 8734, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 61.988, mean reward: 0.620 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.582, 10.098], loss: 0.001572, mae: 0.042291, mean_q: 1.166332
 873427/1000000: episode: 8735, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 57.646, mean reward: 0.576 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.051, 10.192], loss: 0.001489, mae: 0.042152, mean_q: 1.163352
 873527/1000000: episode: 8736, duration: 1.015s, episode steps: 100, steps per second: 98, episode reward: 60.877, mean reward: 0.609 [0.520, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.046, 10.182], loss: 0.001477, mae: 0.041629, mean_q: 1.166000
 873627/1000000: episode: 8737, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 59.223, mean reward: 0.592 [0.520, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.798, 10.235], loss: 0.001545, mae: 0.042903, mean_q: 1.164792
 873727/1000000: episode: 8738, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.445, mean reward: 0.574 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.239, 10.098], loss: 0.001603, mae: 0.043097, mean_q: 1.167549
 873827/1000000: episode: 8739, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.493, mean reward: 0.585 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.480, 10.267], loss: 0.001494, mae: 0.041974, mean_q: 1.168031
 873927/1000000: episode: 8740, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 59.158, mean reward: 0.592 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.593, 10.106], loss: 0.001496, mae: 0.041891, mean_q: 1.162818
 874027/1000000: episode: 8741, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 56.642, mean reward: 0.566 [0.498, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.686, 10.098], loss: 0.001498, mae: 0.042108, mean_q: 1.166554
 874127/1000000: episode: 8742, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 59.589, mean reward: 0.596 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.633, 10.438], loss: 0.001517, mae: 0.042087, mean_q: 1.165450
 874227/1000000: episode: 8743, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 59.693, mean reward: 0.597 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.478, 10.098], loss: 0.001616, mae: 0.043560, mean_q: 1.168072
 874327/1000000: episode: 8744, duration: 1.019s, episode steps: 100, steps per second: 98, episode reward: 57.639, mean reward: 0.576 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.781, 10.117], loss: 0.001465, mae: 0.041488, mean_q: 1.167265
 874427/1000000: episode: 8745, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 59.103, mean reward: 0.591 [0.509, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.939, 10.277], loss: 0.001515, mae: 0.041660, mean_q: 1.164899
 874527/1000000: episode: 8746, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.291, mean reward: 0.583 [0.506, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.140], loss: 0.001565, mae: 0.042937, mean_q: 1.166354
 874627/1000000: episode: 8747, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.847, mean reward: 0.588 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.741, 10.098], loss: 0.001406, mae: 0.041036, mean_q: 1.163699
 874727/1000000: episode: 8748, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 59.672, mean reward: 0.597 [0.526, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.456, 10.098], loss: 0.001555, mae: 0.042885, mean_q: 1.168104
 874827/1000000: episode: 8749, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 59.004, mean reward: 0.590 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.731, 10.098], loss: 0.001518, mae: 0.042592, mean_q: 1.163661
 874927/1000000: episode: 8750, duration: 1.008s, episode steps: 100, steps per second: 99, episode reward: 57.517, mean reward: 0.575 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.206], loss: 0.001583, mae: 0.042439, mean_q: 1.164853
 875027/1000000: episode: 8751, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 59.680, mean reward: 0.597 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.590, 10.122], loss: 0.001658, mae: 0.043719, mean_q: 1.164667
 875127/1000000: episode: 8752, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 57.295, mean reward: 0.573 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.533, 10.098], loss: 0.001602, mae: 0.042984, mean_q: 1.162340
 875227/1000000: episode: 8753, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 58.418, mean reward: 0.584 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.277, 10.098], loss: 0.001510, mae: 0.041994, mean_q: 1.161948
 875327/1000000: episode: 8754, duration: 1.009s, episode steps: 100, steps per second: 99, episode reward: 58.127, mean reward: 0.581 [0.508, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.877, 10.236], loss: 0.001448, mae: 0.041498, mean_q: 1.158821
 875427/1000000: episode: 8755, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 61.401, mean reward: 0.614 [0.517, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.926, 10.098], loss: 0.001574, mae: 0.042424, mean_q: 1.162080
 875527/1000000: episode: 8756, duration: 1.152s, episode steps: 100, steps per second: 87, episode reward: 59.466, mean reward: 0.595 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.883, 10.109], loss: 0.001576, mae: 0.042534, mean_q: 1.163707
 875627/1000000: episode: 8757, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 59.558, mean reward: 0.596 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.792, 10.162], loss: 0.001632, mae: 0.043366, mean_q: 1.164887
 875727/1000000: episode: 8758, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 58.682, mean reward: 0.587 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.603, 10.098], loss: 0.001461, mae: 0.040972, mean_q: 1.162904
 875827/1000000: episode: 8759, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 63.599, mean reward: 0.636 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.055, 10.222], loss: 0.001448, mae: 0.041220, mean_q: 1.164094
 875927/1000000: episode: 8760, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 58.723, mean reward: 0.587 [0.503, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.940, 10.308], loss: 0.001632, mae: 0.044114, mean_q: 1.166752
 876027/1000000: episode: 8761, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 61.707, mean reward: 0.617 [0.512, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.943, 10.098], loss: 0.001549, mae: 0.042204, mean_q: 1.167849
 876127/1000000: episode: 8762, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 57.442, mean reward: 0.574 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.226], loss: 0.001455, mae: 0.041078, mean_q: 1.168881
 876227/1000000: episode: 8763, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 59.603, mean reward: 0.596 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.284, 10.098], loss: 0.001554, mae: 0.042296, mean_q: 1.168975
 876327/1000000: episode: 8764, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 58.045, mean reward: 0.580 [0.498, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.605, 10.246], loss: 0.001483, mae: 0.041903, mean_q: 1.166790
 876427/1000000: episode: 8765, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 56.526, mean reward: 0.565 [0.504, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.224, 10.098], loss: 0.001432, mae: 0.040556, mean_q: 1.165295
 876527/1000000: episode: 8766, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 60.571, mean reward: 0.606 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.966, 10.205], loss: 0.001671, mae: 0.043486, mean_q: 1.168729
 876627/1000000: episode: 8767, duration: 1.182s, episode steps: 100, steps per second: 85, episode reward: 58.174, mean reward: 0.582 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.389, 10.098], loss: 0.001567, mae: 0.042843, mean_q: 1.166992
 876727/1000000: episode: 8768, duration: 1.026s, episode steps: 100, steps per second: 98, episode reward: 57.829, mean reward: 0.578 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.468, 10.098], loss: 0.001427, mae: 0.040725, mean_q: 1.169436
 876827/1000000: episode: 8769, duration: 1.177s, episode steps: 100, steps per second: 85, episode reward: 59.426, mean reward: 0.594 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.295, 10.390], loss: 0.001541, mae: 0.042663, mean_q: 1.168140
 876927/1000000: episode: 8770, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 63.077, mean reward: 0.631 [0.504, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.710, 10.098], loss: 0.001533, mae: 0.041865, mean_q: 1.167995
 877027/1000000: episode: 8771, duration: 1.000s, episode steps: 100, steps per second: 100, episode reward: 62.338, mean reward: 0.623 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.682, 10.098], loss: 0.001492, mae: 0.041489, mean_q: 1.168347
 877127/1000000: episode: 8772, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.838, mean reward: 0.598 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.623, 10.098], loss: 0.001391, mae: 0.041108, mean_q: 1.171357
 877227/1000000: episode: 8773, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 58.337, mean reward: 0.583 [0.514, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.903, 10.127], loss: 0.001489, mae: 0.042207, mean_q: 1.171123
 877327/1000000: episode: 8774, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 59.469, mean reward: 0.595 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.328, 10.545], loss: 0.001569, mae: 0.042760, mean_q: 1.175988
 877427/1000000: episode: 8775, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 57.672, mean reward: 0.577 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.515, 10.231], loss: 0.001523, mae: 0.041979, mean_q: 1.172770
 877527/1000000: episode: 8776, duration: 1.137s, episode steps: 100, steps per second: 88, episode reward: 56.657, mean reward: 0.567 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.005, 10.135], loss: 0.001439, mae: 0.041231, mean_q: 1.168381
 877627/1000000: episode: 8777, duration: 1.013s, episode steps: 100, steps per second: 99, episode reward: 57.731, mean reward: 0.577 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.218], loss: 0.001516, mae: 0.041980, mean_q: 1.170375
 877727/1000000: episode: 8778, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 58.589, mean reward: 0.586 [0.512, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.619, 10.098], loss: 0.001480, mae: 0.041322, mean_q: 1.169964
 877827/1000000: episode: 8779, duration: 0.992s, episode steps: 100, steps per second: 101, episode reward: 60.527, mean reward: 0.605 [0.516, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.074, 10.098], loss: 0.001587, mae: 0.043283, mean_q: 1.167226
 877927/1000000: episode: 8780, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 60.332, mean reward: 0.603 [0.513, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.109, 10.098], loss: 0.001492, mae: 0.042576, mean_q: 1.172493
 878027/1000000: episode: 8781, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 58.992, mean reward: 0.590 [0.503, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.566, 10.140], loss: 0.001466, mae: 0.041981, mean_q: 1.169095
 878127/1000000: episode: 8782, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 57.315, mean reward: 0.573 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.121], loss: 0.001526, mae: 0.042512, mean_q: 1.171071
 878227/1000000: episode: 8783, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 57.138, mean reward: 0.571 [0.503, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.822, 10.277], loss: 0.001613, mae: 0.043575, mean_q: 1.172377
 878327/1000000: episode: 8784, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: 60.359, mean reward: 0.604 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.284, 10.278], loss: 0.001519, mae: 0.042133, mean_q: 1.170037
 878427/1000000: episode: 8785, duration: 1.011s, episode steps: 100, steps per second: 99, episode reward: 58.624, mean reward: 0.586 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.171, 10.305], loss: 0.001461, mae: 0.041536, mean_q: 1.164261
 878527/1000000: episode: 8786, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 58.140, mean reward: 0.581 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.546, 10.098], loss: 0.001345, mae: 0.040042, mean_q: 1.166046
 878627/1000000: episode: 8787, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 58.561, mean reward: 0.586 [0.499, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.086, 10.105], loss: 0.001407, mae: 0.040411, mean_q: 1.171841
 878727/1000000: episode: 8788, duration: 1.005s, episode steps: 100, steps per second: 99, episode reward: 57.973, mean reward: 0.580 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.098], loss: 0.001470, mae: 0.041483, mean_q: 1.168848
 878827/1000000: episode: 8789, duration: 0.997s, episode steps: 100, steps per second: 100, episode reward: 58.801, mean reward: 0.588 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.362, 10.229], loss: 0.001442, mae: 0.041014, mean_q: 1.168894
 878927/1000000: episode: 8790, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 59.630, mean reward: 0.596 [0.499, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.550, 10.098], loss: 0.001440, mae: 0.041465, mean_q: 1.166400
 879027/1000000: episode: 8791, duration: 1.012s, episode steps: 100, steps per second: 99, episode reward: 57.128, mean reward: 0.571 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.435, 10.115], loss: 0.001485, mae: 0.041368, mean_q: 1.169156
 879127/1000000: episode: 8792, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 59.778, mean reward: 0.598 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.666, 10.098], loss: 0.001471, mae: 0.041398, mean_q: 1.172390
 879227/1000000: episode: 8793, duration: 1.293s, episode steps: 100, steps per second: 77, episode reward: 56.275, mean reward: 0.563 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.924, 10.174], loss: 0.001543, mae: 0.042502, mean_q: 1.169105
 879327/1000000: episode: 8794, duration: 1.143s, episode steps: 100, steps per second: 87, episode reward: 60.399, mean reward: 0.604 [0.519, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.567, 10.098], loss: 0.001447, mae: 0.040503, mean_q: 1.166242
 879427/1000000: episode: 8795, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 61.706, mean reward: 0.617 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.910, 10.098], loss: 0.001460, mae: 0.041374, mean_q: 1.167109
 879527/1000000: episode: 8796, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 56.179, mean reward: 0.562 [0.498, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.312, 10.098], loss: 0.001420, mae: 0.041318, mean_q: 1.167125
 879627/1000000: episode: 8797, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 61.410, mean reward: 0.614 [0.512, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.882, 10.126], loss: 0.001431, mae: 0.041615, mean_q: 1.165980
 879727/1000000: episode: 8798, duration: 1.587s, episode steps: 100, steps per second: 63, episode reward: 59.056, mean reward: 0.591 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.251], loss: 0.001343, mae: 0.040394, mean_q: 1.164726
 879827/1000000: episode: 8799, duration: 1.629s, episode steps: 100, steps per second: 61, episode reward: 57.744, mean reward: 0.577 [0.508, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.978, 10.098], loss: 0.001374, mae: 0.040697, mean_q: 1.168467
 879927/1000000: episode: 8800, duration: 1.759s, episode steps: 100, steps per second: 57, episode reward: 60.311, mean reward: 0.603 [0.510, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.961, 10.388], loss: 0.001362, mae: 0.040577, mean_q: 1.168297
 880027/1000000: episode: 8801, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 59.729, mean reward: 0.597 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.322, 10.188], loss: 0.001496, mae: 0.041545, mean_q: 1.166603
 880127/1000000: episode: 8802, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 65.491, mean reward: 0.655 [0.509, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.645, 10.098], loss: 0.001515, mae: 0.042415, mean_q: 1.172002
 880227/1000000: episode: 8803, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 58.072, mean reward: 0.581 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.930, 10.098], loss: 0.001403, mae: 0.040635, mean_q: 1.171199
 880327/1000000: episode: 8804, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 58.187, mean reward: 0.582 [0.511, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.712, 10.139], loss: 0.001371, mae: 0.039786, mean_q: 1.171732
 880427/1000000: episode: 8805, duration: 1.361s, episode steps: 100, steps per second: 74, episode reward: 59.435, mean reward: 0.594 [0.513, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.050, 10.098], loss: 0.001439, mae: 0.041513, mean_q: 1.170110
 880527/1000000: episode: 8806, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 57.278, mean reward: 0.573 [0.506, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.642, 10.107], loss: 0.001419, mae: 0.041047, mean_q: 1.171638
 880627/1000000: episode: 8807, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 57.719, mean reward: 0.577 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.417, 10.098], loss: 0.001300, mae: 0.040108, mean_q: 1.172192
 880727/1000000: episode: 8808, duration: 1.540s, episode steps: 100, steps per second: 65, episode reward: 59.711, mean reward: 0.597 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.544, 10.352], loss: 0.001406, mae: 0.041090, mean_q: 1.171775
 880827/1000000: episode: 8809, duration: 1.499s, episode steps: 100, steps per second: 67, episode reward: 56.781, mean reward: 0.568 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.795, 10.218], loss: 0.001440, mae: 0.041576, mean_q: 1.168793
 880927/1000000: episode: 8810, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 59.438, mean reward: 0.594 [0.519, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.320, 10.098], loss: 0.001449, mae: 0.042163, mean_q: 1.171045
 881027/1000000: episode: 8811, duration: 1.444s, episode steps: 100, steps per second: 69, episode reward: 58.338, mean reward: 0.583 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.001395, mae: 0.040177, mean_q: 1.165379
 881127/1000000: episode: 8812, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 62.938, mean reward: 0.629 [0.519, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.622, 10.098], loss: 0.001442, mae: 0.041275, mean_q: 1.165919
 881227/1000000: episode: 8813, duration: 1.392s, episode steps: 100, steps per second: 72, episode reward: 58.880, mean reward: 0.589 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.483, 10.259], loss: 0.001426, mae: 0.040866, mean_q: 1.171081
 881327/1000000: episode: 8814, duration: 1.117s, episode steps: 100, steps per second: 89, episode reward: 58.405, mean reward: 0.584 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.400, 10.252], loss: 0.001403, mae: 0.040958, mean_q: 1.169905
 881427/1000000: episode: 8815, duration: 1.454s, episode steps: 100, steps per second: 69, episode reward: 60.098, mean reward: 0.601 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.835, 10.294], loss: 0.001423, mae: 0.041338, mean_q: 1.170501
 881527/1000000: episode: 8816, duration: 1.500s, episode steps: 100, steps per second: 67, episode reward: 59.062, mean reward: 0.591 [0.507, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.141], loss: 0.001412, mae: 0.040913, mean_q: 1.171657
 881627/1000000: episode: 8817, duration: 1.658s, episode steps: 100, steps per second: 60, episode reward: 58.750, mean reward: 0.587 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.356, 10.144], loss: 0.001332, mae: 0.039914, mean_q: 1.169702
 881727/1000000: episode: 8818, duration: 1.367s, episode steps: 100, steps per second: 73, episode reward: 62.745, mean reward: 0.627 [0.511, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.386, 10.254], loss: 0.001415, mae: 0.041007, mean_q: 1.170522
 881827/1000000: episode: 8819, duration: 1.369s, episode steps: 100, steps per second: 73, episode reward: 59.480, mean reward: 0.595 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.775, 10.098], loss: 0.001354, mae: 0.040129, mean_q: 1.172790
 881927/1000000: episode: 8820, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 66.904, mean reward: 0.669 [0.523, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.706, 10.428], loss: 0.001408, mae: 0.040603, mean_q: 1.173241
 882027/1000000: episode: 8821, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 57.604, mean reward: 0.576 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.033, 10.209], loss: 0.001407, mae: 0.040649, mean_q: 1.174983
 882127/1000000: episode: 8822, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 59.771, mean reward: 0.598 [0.514, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.743, 10.098], loss: 0.001368, mae: 0.040724, mean_q: 1.174127
 882227/1000000: episode: 8823, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 58.944, mean reward: 0.589 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.636, 10.098], loss: 0.001374, mae: 0.039783, mean_q: 1.170177
 882327/1000000: episode: 8824, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 59.683, mean reward: 0.597 [0.512, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.307, 10.098], loss: 0.001340, mae: 0.039652, mean_q: 1.170311
 882427/1000000: episode: 8825, duration: 1.227s, episode steps: 100, steps per second: 81, episode reward: 61.801, mean reward: 0.618 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.521, 10.098], loss: 0.001440, mae: 0.040973, mean_q: 1.173643
 882527/1000000: episode: 8826, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 57.682, mean reward: 0.577 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.098], loss: 0.001309, mae: 0.039510, mean_q: 1.172729
 882627/1000000: episode: 8827, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 58.089, mean reward: 0.581 [0.513, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.767, 10.166], loss: 0.001496, mae: 0.041881, mean_q: 1.173917
 882727/1000000: episode: 8828, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 56.926, mean reward: 0.569 [0.500, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.639, 10.111], loss: 0.001524, mae: 0.042036, mean_q: 1.173603
 882827/1000000: episode: 8829, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 62.417, mean reward: 0.624 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.743, 10.098], loss: 0.001396, mae: 0.040202, mean_q: 1.171939
 882927/1000000: episode: 8830, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 58.854, mean reward: 0.589 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.868, 10.098], loss: 0.001469, mae: 0.041435, mean_q: 1.175044
 883027/1000000: episode: 8831, duration: 1.355s, episode steps: 100, steps per second: 74, episode reward: 57.635, mean reward: 0.576 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.772, 10.098], loss: 0.001472, mae: 0.041773, mean_q: 1.174677
 883127/1000000: episode: 8832, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 59.223, mean reward: 0.592 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.584, 10.098], loss: 0.001411, mae: 0.040887, mean_q: 1.173900
 883227/1000000: episode: 8833, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 56.859, mean reward: 0.569 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.741, 10.098], loss: 0.001377, mae: 0.040551, mean_q: 1.171841
 883327/1000000: episode: 8834, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 58.653, mean reward: 0.587 [0.498, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.794, 10.107], loss: 0.001344, mae: 0.040022, mean_q: 1.168767
 883427/1000000: episode: 8835, duration: 1.475s, episode steps: 100, steps per second: 68, episode reward: 58.576, mean reward: 0.586 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.502, 10.098], loss: 0.001468, mae: 0.041917, mean_q: 1.171166
 883527/1000000: episode: 8836, duration: 1.827s, episode steps: 100, steps per second: 55, episode reward: 58.377, mean reward: 0.584 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.201, 10.098], loss: 0.001408, mae: 0.040688, mean_q: 1.172182
 883627/1000000: episode: 8837, duration: 1.725s, episode steps: 100, steps per second: 58, episode reward: 58.669, mean reward: 0.587 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.098], loss: 0.001312, mae: 0.039966, mean_q: 1.173090
 883727/1000000: episode: 8838, duration: 1.773s, episode steps: 100, steps per second: 56, episode reward: 58.740, mean reward: 0.587 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.292, 10.170], loss: 0.001442, mae: 0.041371, mean_q: 1.173999
 883827/1000000: episode: 8839, duration: 1.774s, episode steps: 100, steps per second: 56, episode reward: 58.917, mean reward: 0.589 [0.502, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.780, 10.192], loss: 0.001373, mae: 0.040922, mean_q: 1.172566
 883927/1000000: episode: 8840, duration: 1.540s, episode steps: 100, steps per second: 65, episode reward: 57.627, mean reward: 0.576 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.010, 10.098], loss: 0.001335, mae: 0.039409, mean_q: 1.171029
 884027/1000000: episode: 8841, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 58.927, mean reward: 0.589 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.307, 10.285], loss: 0.001369, mae: 0.040288, mean_q: 1.166085
 884127/1000000: episode: 8842, duration: 1.029s, episode steps: 100, steps per second: 97, episode reward: 58.407, mean reward: 0.584 [0.516, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.633, 10.098], loss: 0.001352, mae: 0.040283, mean_q: 1.172461
 884227/1000000: episode: 8843, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 58.504, mean reward: 0.585 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.541, 10.234], loss: 0.001494, mae: 0.041391, mean_q: 1.171418
 884327/1000000: episode: 8844, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 58.956, mean reward: 0.590 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.388, 10.098], loss: 0.001411, mae: 0.041155, mean_q: 1.172713
 884427/1000000: episode: 8845, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 58.521, mean reward: 0.585 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.988, 10.098], loss: 0.001425, mae: 0.041335, mean_q: 1.176291
 884527/1000000: episode: 8846, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 57.152, mean reward: 0.572 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.623, 10.128], loss: 0.001351, mae: 0.040221, mean_q: 1.173474
 884627/1000000: episode: 8847, duration: 1.036s, episode steps: 100, steps per second: 97, episode reward: 56.855, mean reward: 0.569 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.276], loss: 0.001440, mae: 0.041459, mean_q: 1.173742
 884727/1000000: episode: 8848, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 57.580, mean reward: 0.576 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.098], loss: 0.001349, mae: 0.040097, mean_q: 1.169563
 884827/1000000: episode: 8849, duration: 1.349s, episode steps: 100, steps per second: 74, episode reward: 60.212, mean reward: 0.602 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.770, 10.098], loss: 0.001396, mae: 0.040746, mean_q: 1.170457
 884927/1000000: episode: 8850, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.224, mean reward: 0.582 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.522, 10.098], loss: 0.001296, mae: 0.039033, mean_q: 1.166960
 885027/1000000: episode: 8851, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 58.150, mean reward: 0.582 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.791, 10.098], loss: 0.001357, mae: 0.040288, mean_q: 1.165273
 885127/1000000: episode: 8852, duration: 1.334s, episode steps: 100, steps per second: 75, episode reward: 58.075, mean reward: 0.581 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.536, 10.144], loss: 0.001306, mae: 0.039774, mean_q: 1.167826
 885227/1000000: episode: 8853, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 62.085, mean reward: 0.621 [0.502, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.358, 10.098], loss: 0.001330, mae: 0.039517, mean_q: 1.164102
 885327/1000000: episode: 8854, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 56.801, mean reward: 0.568 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.492, 10.135], loss: 0.001310, mae: 0.039803, mean_q: 1.166572
 885427/1000000: episode: 8855, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 59.490, mean reward: 0.595 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.431, 10.171], loss: 0.001332, mae: 0.039683, mean_q: 1.164829
 885527/1000000: episode: 8856, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.260, mean reward: 0.593 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.600, 10.252], loss: 0.001340, mae: 0.039983, mean_q: 1.167508
 885627/1000000: episode: 8857, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 59.683, mean reward: 0.597 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.183], loss: 0.001386, mae: 0.040542, mean_q: 1.168170
 885727/1000000: episode: 8858, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 63.926, mean reward: 0.639 [0.517, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.551, 10.098], loss: 0.001296, mae: 0.039310, mean_q: 1.169129
 885827/1000000: episode: 8859, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 57.467, mean reward: 0.575 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.330, 10.186], loss: 0.001275, mae: 0.038894, mean_q: 1.168248
 885927/1000000: episode: 8860, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 59.269, mean reward: 0.593 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.088, 10.098], loss: 0.001344, mae: 0.039858, mean_q: 1.170249
 886027/1000000: episode: 8861, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 57.494, mean reward: 0.575 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.134, 10.098], loss: 0.001246, mae: 0.038667, mean_q: 1.173288
 886127/1000000: episode: 8862, duration: 1.501s, episode steps: 100, steps per second: 67, episode reward: 57.225, mean reward: 0.572 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.256], loss: 0.001340, mae: 0.039750, mean_q: 1.167663
 886227/1000000: episode: 8863, duration: 1.803s, episode steps: 100, steps per second: 55, episode reward: 57.437, mean reward: 0.574 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.292, 10.121], loss: 0.001295, mae: 0.038773, mean_q: 1.168224
 886327/1000000: episode: 8864, duration: 1.662s, episode steps: 100, steps per second: 60, episode reward: 58.507, mean reward: 0.585 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.001387, mae: 0.040638, mean_q: 1.171006
 886427/1000000: episode: 8865, duration: 1.713s, episode steps: 100, steps per second: 58, episode reward: 60.096, mean reward: 0.601 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.920, 10.283], loss: 0.001395, mae: 0.040053, mean_q: 1.168594
 886527/1000000: episode: 8866, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 59.443, mean reward: 0.594 [0.499, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.751, 10.207], loss: 0.001231, mae: 0.038668, mean_q: 1.167359
 886627/1000000: episode: 8867, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 59.596, mean reward: 0.596 [0.505, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.150], loss: 0.001352, mae: 0.040031, mean_q: 1.166024
 886727/1000000: episode: 8868, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 56.423, mean reward: 0.564 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.320, 10.252], loss: 0.001304, mae: 0.039449, mean_q: 1.166839
 886827/1000000: episode: 8869, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 58.094, mean reward: 0.581 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.264, 10.183], loss: 0.001220, mae: 0.038129, mean_q: 1.162951
 886927/1000000: episode: 8870, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 60.374, mean reward: 0.604 [0.508, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.142], loss: 0.001280, mae: 0.039406, mean_q: 1.161338
 887027/1000000: episode: 8871, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 57.474, mean reward: 0.575 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.746, 10.098], loss: 0.001333, mae: 0.039527, mean_q: 1.165355
 887127/1000000: episode: 8872, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 58.499, mean reward: 0.585 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.212], loss: 0.001300, mae: 0.039318, mean_q: 1.166825
 887227/1000000: episode: 8873, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 58.140, mean reward: 0.581 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.175, 10.203], loss: 0.001313, mae: 0.039651, mean_q: 1.165036
 887327/1000000: episode: 8874, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.845, mean reward: 0.588 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.378, 10.098], loss: 0.001305, mae: 0.039202, mean_q: 1.160915
 887427/1000000: episode: 8875, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 60.992, mean reward: 0.610 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.260, 10.379], loss: 0.001301, mae: 0.038916, mean_q: 1.161790
 887527/1000000: episode: 8876, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 63.512, mean reward: 0.635 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.404, 10.493], loss: 0.001310, mae: 0.039473, mean_q: 1.162629
 887627/1000000: episode: 8877, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.046, mean reward: 0.580 [0.498, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.871, 10.204], loss: 0.001335, mae: 0.039454, mean_q: 1.162402
 887727/1000000: episode: 8878, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 57.528, mean reward: 0.575 [0.499, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.098], loss: 0.001433, mae: 0.041228, mean_q: 1.166406
 887827/1000000: episode: 8879, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 59.597, mean reward: 0.596 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.638, 10.227], loss: 0.001399, mae: 0.040375, mean_q: 1.164466
 887927/1000000: episode: 8880, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 56.358, mean reward: 0.564 [0.506, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.047, 10.098], loss: 0.001305, mae: 0.039389, mean_q: 1.163259
 888027/1000000: episode: 8881, duration: 1.006s, episode steps: 100, steps per second: 99, episode reward: 57.939, mean reward: 0.579 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.330, 10.404], loss: 0.001362, mae: 0.039920, mean_q: 1.161981
 888127/1000000: episode: 8882, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 68.708, mean reward: 0.687 [0.518, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.187, 10.117], loss: 0.001302, mae: 0.039194, mean_q: 1.160419
 888227/1000000: episode: 8883, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 58.356, mean reward: 0.584 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.117, 10.187], loss: 0.001353, mae: 0.040106, mean_q: 1.166340
 888327/1000000: episode: 8884, duration: 1.821s, episode steps: 100, steps per second: 55, episode reward: 59.245, mean reward: 0.592 [0.515, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.408, 10.098], loss: 0.001406, mae: 0.039993, mean_q: 1.165116
 888427/1000000: episode: 8885, duration: 1.763s, episode steps: 100, steps per second: 57, episode reward: 58.490, mean reward: 0.585 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.098], loss: 0.001363, mae: 0.040268, mean_q: 1.166227
 888527/1000000: episode: 8886, duration: 1.589s, episode steps: 100, steps per second: 63, episode reward: 57.580, mean reward: 0.576 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.098], loss: 0.001365, mae: 0.040099, mean_q: 1.166304
 888627/1000000: episode: 8887, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 59.370, mean reward: 0.594 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.610, 10.111], loss: 0.001314, mae: 0.039101, mean_q: 1.166564
 888727/1000000: episode: 8888, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 59.586, mean reward: 0.596 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.637, 10.098], loss: 0.001448, mae: 0.041090, mean_q: 1.165227
 888827/1000000: episode: 8889, duration: 1.018s, episode steps: 100, steps per second: 98, episode reward: 56.160, mean reward: 0.562 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.588, 10.098], loss: 0.001436, mae: 0.040613, mean_q: 1.167132
 888927/1000000: episode: 8890, duration: 1.295s, episode steps: 100, steps per second: 77, episode reward: 61.136, mean reward: 0.611 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.098], loss: 0.001464, mae: 0.041067, mean_q: 1.166800
 889027/1000000: episode: 8891, duration: 1.114s, episode steps: 100, steps per second: 90, episode reward: 61.287, mean reward: 0.613 [0.504, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.947, 10.098], loss: 0.001411, mae: 0.040603, mean_q: 1.165488
 889127/1000000: episode: 8892, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 58.438, mean reward: 0.584 [0.499, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.547, 10.098], loss: 0.001375, mae: 0.039926, mean_q: 1.169714
 889227/1000000: episode: 8893, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 57.208, mean reward: 0.572 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.921, 10.148], loss: 0.001407, mae: 0.040683, mean_q: 1.166993
 889327/1000000: episode: 8894, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 58.604, mean reward: 0.586 [0.511, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.023, 10.098], loss: 0.001444, mae: 0.040931, mean_q: 1.165423
 889427/1000000: episode: 8895, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.769, mean reward: 0.588 [0.515, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.139], loss: 0.001423, mae: 0.041168, mean_q: 1.170494
 889527/1000000: episode: 8896, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 58.629, mean reward: 0.586 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.430, 10.098], loss: 0.001462, mae: 0.041269, mean_q: 1.169568
 889627/1000000: episode: 8897, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 57.478, mean reward: 0.575 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.438, 10.131], loss: 0.001552, mae: 0.041985, mean_q: 1.172160
 889727/1000000: episode: 8898, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 58.171, mean reward: 0.582 [0.504, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.098], loss: 0.001526, mae: 0.042813, mean_q: 1.172035
 889827/1000000: episode: 8899, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.952, mean reward: 0.590 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.611, 10.334], loss: 0.001444, mae: 0.040624, mean_q: 1.170527
 889927/1000000: episode: 8900, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 57.132, mean reward: 0.571 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.395, 10.098], loss: 0.001493, mae: 0.041823, mean_q: 1.169070
 890027/1000000: episode: 8901, duration: 1.010s, episode steps: 100, steps per second: 99, episode reward: 57.925, mean reward: 0.579 [0.510, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.253, 10.098], loss: 0.001429, mae: 0.040644, mean_q: 1.170901
 890127/1000000: episode: 8902, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 57.746, mean reward: 0.577 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.519, 10.098], loss: 0.001467, mae: 0.041243, mean_q: 1.167104
 890227/1000000: episode: 8903, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 61.018, mean reward: 0.610 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.662, 10.362], loss: 0.001422, mae: 0.040490, mean_q: 1.166120
 890327/1000000: episode: 8904, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 57.998, mean reward: 0.580 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.678, 10.125], loss: 0.001529, mae: 0.042025, mean_q: 1.170978
 890427/1000000: episode: 8905, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 60.296, mean reward: 0.603 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.523, 10.111], loss: 0.001519, mae: 0.041968, mean_q: 1.165789
 890527/1000000: episode: 8906, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 57.340, mean reward: 0.573 [0.504, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.529, 10.098], loss: 0.001398, mae: 0.040563, mean_q: 1.165902
 890627/1000000: episode: 8907, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 56.403, mean reward: 0.564 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.959, 10.098], loss: 0.001521, mae: 0.041609, mean_q: 1.169276
 890727/1000000: episode: 8908, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 57.138, mean reward: 0.571 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.385, 10.168], loss: 0.001403, mae: 0.040931, mean_q: 1.165353
 890827/1000000: episode: 8909, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 62.092, mean reward: 0.621 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.244, 10.178], loss: 0.001444, mae: 0.040950, mean_q: 1.166263
 890927/1000000: episode: 8910, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 57.798, mean reward: 0.578 [0.510, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.222, 10.187], loss: 0.001477, mae: 0.041295, mean_q: 1.166241
 891027/1000000: episode: 8911, duration: 1.259s, episode steps: 100, steps per second: 79, episode reward: 59.026, mean reward: 0.590 [0.508, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.679, 10.098], loss: 0.001413, mae: 0.040556, mean_q: 1.164455
 891127/1000000: episode: 8912, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 61.106, mean reward: 0.611 [0.526, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.098], loss: 0.001430, mae: 0.041103, mean_q: 1.167161
 891227/1000000: episode: 8913, duration: 1.288s, episode steps: 100, steps per second: 78, episode reward: 58.966, mean reward: 0.590 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.098], loss: 0.001473, mae: 0.041548, mean_q: 1.163988
 891327/1000000: episode: 8914, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 60.378, mean reward: 0.604 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.067, 10.269], loss: 0.001495, mae: 0.041640, mean_q: 1.166730
 891427/1000000: episode: 8915, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 60.383, mean reward: 0.604 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.309, 10.098], loss: 0.001467, mae: 0.041551, mean_q: 1.165490
 891527/1000000: episode: 8916, duration: 1.428s, episode steps: 100, steps per second: 70, episode reward: 60.214, mean reward: 0.602 [0.508, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.996, 10.098], loss: 0.001478, mae: 0.041228, mean_q: 1.166974
 891627/1000000: episode: 8917, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 56.819, mean reward: 0.568 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.098], loss: 0.001555, mae: 0.042369, mean_q: 1.167109
 891727/1000000: episode: 8918, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.185, mean reward: 0.592 [0.497, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.444, 10.178], loss: 0.001570, mae: 0.042781, mean_q: 1.171627
 891827/1000000: episode: 8919, duration: 1.335s, episode steps: 100, steps per second: 75, episode reward: 58.608, mean reward: 0.586 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.887, 10.170], loss: 0.001434, mae: 0.041488, mean_q: 1.169648
 891927/1000000: episode: 8920, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 58.152, mean reward: 0.582 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.430, 10.098], loss: 0.001491, mae: 0.041845, mean_q: 1.166720
 892027/1000000: episode: 8921, duration: 1.293s, episode steps: 100, steps per second: 77, episode reward: 59.792, mean reward: 0.598 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.549, 10.299], loss: 0.001513, mae: 0.042365, mean_q: 1.167692
 892127/1000000: episode: 8922, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 57.359, mean reward: 0.574 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.256], loss: 0.001481, mae: 0.041872, mean_q: 1.171051
 892227/1000000: episode: 8923, duration: 1.407s, episode steps: 100, steps per second: 71, episode reward: 59.193, mean reward: 0.592 [0.518, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.382, 10.272], loss: 0.001496, mae: 0.042046, mean_q: 1.167296
 892327/1000000: episode: 8924, duration: 1.400s, episode steps: 100, steps per second: 71, episode reward: 59.799, mean reward: 0.598 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.766, 10.098], loss: 0.001558, mae: 0.042607, mean_q: 1.169150
 892427/1000000: episode: 8925, duration: 1.337s, episode steps: 100, steps per second: 75, episode reward: 57.110, mean reward: 0.571 [0.503, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.941, 10.237], loss: 0.001533, mae: 0.042536, mean_q: 1.166988
 892527/1000000: episode: 8926, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.261, mean reward: 0.583 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.606, 10.098], loss: 0.001474, mae: 0.041927, mean_q: 1.166693
 892627/1000000: episode: 8927, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 57.867, mean reward: 0.579 [0.504, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.635, 10.137], loss: 0.001495, mae: 0.042349, mean_q: 1.163981
 892727/1000000: episode: 8928, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 62.376, mean reward: 0.624 [0.502, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: 0.001414, mae: 0.041336, mean_q: 1.165719
 892827/1000000: episode: 8929, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 58.602, mean reward: 0.586 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.220, 10.098], loss: 0.001586, mae: 0.043326, mean_q: 1.167810
 892927/1000000: episode: 8930, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 61.456, mean reward: 0.615 [0.516, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.741, 10.173], loss: 0.001572, mae: 0.042557, mean_q: 1.169174
 893027/1000000: episode: 8931, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 59.234, mean reward: 0.592 [0.513, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.508, 10.098], loss: 0.001346, mae: 0.040415, mean_q: 1.170671
 893127/1000000: episode: 8932, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: 58.686, mean reward: 0.587 [0.510, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.584, 10.103], loss: 0.001478, mae: 0.042124, mean_q: 1.163759
 893227/1000000: episode: 8933, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 57.824, mean reward: 0.578 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.920, 10.098], loss: 0.001426, mae: 0.040826, mean_q: 1.163378
 893327/1000000: episode: 8934, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 57.896, mean reward: 0.579 [0.503, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.051, 10.115], loss: 0.001325, mae: 0.039870, mean_q: 1.160562
 893427/1000000: episode: 8935, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 57.461, mean reward: 0.575 [0.498, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.129, 10.114], loss: 0.001485, mae: 0.042154, mean_q: 1.164185
 893527/1000000: episode: 8936, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 59.899, mean reward: 0.599 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.719, 10.125], loss: 0.001426, mae: 0.040968, mean_q: 1.162577
 893627/1000000: episode: 8937, duration: 1.127s, episode steps: 100, steps per second: 89, episode reward: 59.034, mean reward: 0.590 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.489, 10.203], loss: 0.001473, mae: 0.041901, mean_q: 1.165997
 893727/1000000: episode: 8938, duration: 1.167s, episode steps: 100, steps per second: 86, episode reward: 60.301, mean reward: 0.603 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.867, 10.286], loss: 0.001350, mae: 0.039786, mean_q: 1.164728
 893827/1000000: episode: 8939, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 61.326, mean reward: 0.613 [0.503, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.273, 10.151], loss: 0.001455, mae: 0.040822, mean_q: 1.169791
 893927/1000000: episode: 8940, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 59.251, mean reward: 0.593 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.112, 10.098], loss: 0.001404, mae: 0.041262, mean_q: 1.165287
 894027/1000000: episode: 8941, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.432, mean reward: 0.584 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.154], loss: 0.001396, mae: 0.040998, mean_q: 1.165575
 894127/1000000: episode: 8942, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 58.018, mean reward: 0.580 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.161], loss: 0.001510, mae: 0.041720, mean_q: 1.166278
 894227/1000000: episode: 8943, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 57.819, mean reward: 0.578 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.594, 10.451], loss: 0.001483, mae: 0.041635, mean_q: 1.165632
 894327/1000000: episode: 8944, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 59.216, mean reward: 0.592 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.907, 10.098], loss: 0.001415, mae: 0.040964, mean_q: 1.161831
 894427/1000000: episode: 8945, duration: 1.004s, episode steps: 100, steps per second: 100, episode reward: 59.563, mean reward: 0.596 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.620, 10.098], loss: 0.001487, mae: 0.041580, mean_q: 1.163761
 894527/1000000: episode: 8946, duration: 0.994s, episode steps: 100, steps per second: 101, episode reward: 59.008, mean reward: 0.590 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.324, 10.320], loss: 0.001408, mae: 0.040709, mean_q: 1.164706
 894627/1000000: episode: 8947, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 57.106, mean reward: 0.571 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.761, 10.247], loss: 0.001437, mae: 0.041411, mean_q: 1.164988
 894727/1000000: episode: 8948, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 58.544, mean reward: 0.585 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.924, 10.101], loss: 0.001415, mae: 0.041267, mean_q: 1.163924
 894827/1000000: episode: 8949, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 59.947, mean reward: 0.599 [0.513, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.331, 10.296], loss: 0.001434, mae: 0.041207, mean_q: 1.168609
 894927/1000000: episode: 8950, duration: 1.024s, episode steps: 100, steps per second: 98, episode reward: 59.208, mean reward: 0.592 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.845, 10.124], loss: 0.001439, mae: 0.041689, mean_q: 1.168570
 895027/1000000: episode: 8951, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 57.812, mean reward: 0.578 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.550, 10.098], loss: 0.001581, mae: 0.042816, mean_q: 1.169870
 895127/1000000: episode: 8952, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 59.984, mean reward: 0.600 [0.525, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.992, 10.233], loss: 0.001433, mae: 0.041513, mean_q: 1.165259
 895227/1000000: episode: 8953, duration: 1.021s, episode steps: 100, steps per second: 98, episode reward: 63.238, mean reward: 0.632 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.467, 10.098], loss: 0.001495, mae: 0.042670, mean_q: 1.168195
 895327/1000000: episode: 8954, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 59.316, mean reward: 0.593 [0.511, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.401, 10.098], loss: 0.001401, mae: 0.040765, mean_q: 1.173938
 895427/1000000: episode: 8955, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 58.282, mean reward: 0.583 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.986, 10.282], loss: 0.001318, mae: 0.039792, mean_q: 1.166443
 895527/1000000: episode: 8956, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 59.167, mean reward: 0.592 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.249, 10.266], loss: 0.001418, mae: 0.041004, mean_q: 1.168472
 895627/1000000: episode: 8957, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 58.056, mean reward: 0.581 [0.504, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.098], loss: 0.001430, mae: 0.041297, mean_q: 1.170075
 895727/1000000: episode: 8958, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 58.569, mean reward: 0.586 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.428], loss: 0.001346, mae: 0.040398, mean_q: 1.168988
 895827/1000000: episode: 8959, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 58.258, mean reward: 0.583 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.105, 10.098], loss: 0.001363, mae: 0.040278, mean_q: 1.165180
 895927/1000000: episode: 8960, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 61.978, mean reward: 0.620 [0.516, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.551, 10.300], loss: 0.001485, mae: 0.041882, mean_q: 1.167469
 896027/1000000: episode: 8961, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 57.390, mean reward: 0.574 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.204, 10.098], loss: 0.001426, mae: 0.041712, mean_q: 1.167693
 896127/1000000: episode: 8962, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 56.676, mean reward: 0.567 [0.502, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.321, 10.099], loss: 0.001420, mae: 0.041150, mean_q: 1.166443
 896227/1000000: episode: 8963, duration: 1.326s, episode steps: 100, steps per second: 75, episode reward: 59.016, mean reward: 0.590 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.098], loss: 0.001553, mae: 0.042988, mean_q: 1.169390
 896327/1000000: episode: 8964, duration: 1.355s, episode steps: 100, steps per second: 74, episode reward: 58.194, mean reward: 0.582 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.509, 10.261], loss: 0.001475, mae: 0.041966, mean_q: 1.169811
 896427/1000000: episode: 8965, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 61.213, mean reward: 0.612 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.703, 10.129], loss: 0.001453, mae: 0.041646, mean_q: 1.169819
 896527/1000000: episode: 8966, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 57.437, mean reward: 0.574 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.558, 10.098], loss: 0.001451, mae: 0.041455, mean_q: 1.163888
 896627/1000000: episode: 8967, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 60.420, mean reward: 0.604 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.385, 10.140], loss: 0.001389, mae: 0.041144, mean_q: 1.168462
 896727/1000000: episode: 8968, duration: 1.488s, episode steps: 100, steps per second: 67, episode reward: 58.318, mean reward: 0.583 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.647, 10.098], loss: 0.001429, mae: 0.040736, mean_q: 1.167121
 896827/1000000: episode: 8969, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 59.713, mean reward: 0.597 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.952, 10.183], loss: 0.001370, mae: 0.040948, mean_q: 1.166913
 896927/1000000: episode: 8970, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 57.230, mean reward: 0.572 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.781, 10.098], loss: 0.001366, mae: 0.039957, mean_q: 1.165938
 897027/1000000: episode: 8971, duration: 1.417s, episode steps: 100, steps per second: 71, episode reward: 57.683, mean reward: 0.577 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.874, 10.098], loss: 0.001409, mae: 0.040497, mean_q: 1.165836
 897127/1000000: episode: 8972, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.245, mean reward: 0.582 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.239, 10.264], loss: 0.001436, mae: 0.040864, mean_q: 1.164100
 897227/1000000: episode: 8973, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 60.544, mean reward: 0.605 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.369, 10.098], loss: 0.001346, mae: 0.040081, mean_q: 1.167475
 897327/1000000: episode: 8974, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 58.985, mean reward: 0.590 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.664, 10.243], loss: 0.001425, mae: 0.040465, mean_q: 1.169338
 897427/1000000: episode: 8975, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 57.112, mean reward: 0.571 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.688, 10.098], loss: 0.001437, mae: 0.041611, mean_q: 1.168227
 897527/1000000: episode: 8976, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 56.380, mean reward: 0.564 [0.510, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.098], loss: 0.001441, mae: 0.041597, mean_q: 1.169861
 897627/1000000: episode: 8977, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 60.550, mean reward: 0.605 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.252], loss: 0.001269, mae: 0.038989, mean_q: 1.169097
 897727/1000000: episode: 8978, duration: 1.288s, episode steps: 100, steps per second: 78, episode reward: 59.539, mean reward: 0.595 [0.514, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.353, 10.174], loss: 0.001436, mae: 0.041155, mean_q: 1.167203
 897827/1000000: episode: 8979, duration: 1.308s, episode steps: 100, steps per second: 76, episode reward: 58.426, mean reward: 0.584 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.817, 10.144], loss: 0.001441, mae: 0.040935, mean_q: 1.166684
 897927/1000000: episode: 8980, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 60.837, mean reward: 0.608 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.263, 10.192], loss: 0.001447, mae: 0.040873, mean_q: 1.164516
 898027/1000000: episode: 8981, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 61.225, mean reward: 0.612 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.914, 10.098], loss: 0.001378, mae: 0.040613, mean_q: 1.168433
 898127/1000000: episode: 8982, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 60.396, mean reward: 0.604 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.049, 10.225], loss: 0.001351, mae: 0.040396, mean_q: 1.167423
 898227/1000000: episode: 8983, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 58.295, mean reward: 0.583 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.365, 10.098], loss: 0.001473, mae: 0.042002, mean_q: 1.174281
 898327/1000000: episode: 8984, duration: 1.474s, episode steps: 100, steps per second: 68, episode reward: 61.530, mean reward: 0.615 [0.512, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.731, 10.098], loss: 0.001518, mae: 0.042495, mean_q: 1.170293
 898427/1000000: episode: 8985, duration: 1.267s, episode steps: 100, steps per second: 79, episode reward: 58.530, mean reward: 0.585 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.441, 10.098], loss: 0.001427, mae: 0.041233, mean_q: 1.169279
 898527/1000000: episode: 8986, duration: 1.153s, episode steps: 100, steps per second: 87, episode reward: 57.281, mean reward: 0.573 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.475, 10.175], loss: 0.001506, mae: 0.042226, mean_q: 1.174293
 898627/1000000: episode: 8987, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 57.435, mean reward: 0.574 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.274], loss: 0.001390, mae: 0.040667, mean_q: 1.170875
 898727/1000000: episode: 8988, duration: 1.281s, episode steps: 100, steps per second: 78, episode reward: 59.178, mean reward: 0.592 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.214], loss: 0.001510, mae: 0.042144, mean_q: 1.169581
 898827/1000000: episode: 8989, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 63.286, mean reward: 0.633 [0.512, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.440], loss: 0.001409, mae: 0.041195, mean_q: 1.167753
 898927/1000000: episode: 8990, duration: 1.134s, episode steps: 100, steps per second: 88, episode reward: 57.754, mean reward: 0.578 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.935, 10.164], loss: 0.001415, mae: 0.040764, mean_q: 1.169004
 899027/1000000: episode: 8991, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 59.123, mean reward: 0.591 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.112], loss: 0.001446, mae: 0.041043, mean_q: 1.168242
 899127/1000000: episode: 8992, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 58.051, mean reward: 0.581 [0.512, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.791, 10.223], loss: 0.001448, mae: 0.041521, mean_q: 1.166221
 899227/1000000: episode: 8993, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 58.984, mean reward: 0.590 [0.509, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.870, 10.179], loss: 0.001342, mae: 0.039829, mean_q: 1.169086
 899327/1000000: episode: 8994, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 61.616, mean reward: 0.616 [0.509, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.110, 10.098], loss: 0.001373, mae: 0.041325, mean_q: 1.170105
 899427/1000000: episode: 8995, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 57.707, mean reward: 0.577 [0.524, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.934, 10.098], loss: 0.001535, mae: 0.042208, mean_q: 1.169924
 899527/1000000: episode: 8996, duration: 1.408s, episode steps: 100, steps per second: 71, episode reward: 58.371, mean reward: 0.584 [0.515, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.840, 10.236], loss: 0.001476, mae: 0.041621, mean_q: 1.170783
 899627/1000000: episode: 8997, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 60.893, mean reward: 0.609 [0.500, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.441, 10.236], loss: 0.001489, mae: 0.042392, mean_q: 1.173275
 899727/1000000: episode: 8998, duration: 1.440s, episode steps: 100, steps per second: 69, episode reward: 57.274, mean reward: 0.573 [0.509, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.942, 10.098], loss: 0.001464, mae: 0.041715, mean_q: 1.170787
 899827/1000000: episode: 8999, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 57.730, mean reward: 0.577 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.930, 10.098], loss: 0.001407, mae: 0.040732, mean_q: 1.171197
 899927/1000000: episode: 9000, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 60.766, mean reward: 0.608 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.235, 10.206], loss: 0.001335, mae: 0.039655, mean_q: 1.165261
 900027/1000000: episode: 9001, duration: 1.367s, episode steps: 100, steps per second: 73, episode reward: 59.276, mean reward: 0.593 [0.507, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.742, 10.533], loss: 0.001501, mae: 0.042395, mean_q: 1.165416
 900127/1000000: episode: 9002, duration: 1.261s, episode steps: 100, steps per second: 79, episode reward: 59.633, mean reward: 0.596 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.723, 10.115], loss: 0.001332, mae: 0.040117, mean_q: 1.170708
 900227/1000000: episode: 9003, duration: 1.261s, episode steps: 100, steps per second: 79, episode reward: 57.734, mean reward: 0.577 [0.509, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.773, 10.098], loss: 0.001434, mae: 0.041346, mean_q: 1.165580
 900327/1000000: episode: 9004, duration: 1.356s, episode steps: 100, steps per second: 74, episode reward: 56.008, mean reward: 0.560 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.384, 10.110], loss: 0.001482, mae: 0.041863, mean_q: 1.170036
 900427/1000000: episode: 9005, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 57.787, mean reward: 0.578 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.646, 10.137], loss: 0.001411, mae: 0.041525, mean_q: 1.167659
 900527/1000000: episode: 9006, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 61.125, mean reward: 0.611 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.376, 10.365], loss: 0.001412, mae: 0.040915, mean_q: 1.168076
 900627/1000000: episode: 9007, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 59.475, mean reward: 0.595 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.085, 10.098], loss: 0.001417, mae: 0.041568, mean_q: 1.168598
 900727/1000000: episode: 9008, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.809, mean reward: 0.588 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.209, 10.257], loss: 0.001328, mae: 0.040197, mean_q: 1.167565
 900827/1000000: episode: 9009, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 58.149, mean reward: 0.581 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.617, 10.394], loss: 0.001491, mae: 0.042051, mean_q: 1.166562
 900927/1000000: episode: 9010, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 57.254, mean reward: 0.573 [0.504, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.225, 10.098], loss: 0.001356, mae: 0.040422, mean_q: 1.165954
 901027/1000000: episode: 9011, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 62.060, mean reward: 0.621 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.285, 10.181], loss: 0.001388, mae: 0.040867, mean_q: 1.166207
 901127/1000000: episode: 9012, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 59.888, mean reward: 0.599 [0.498, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.279, 10.308], loss: 0.001375, mae: 0.040207, mean_q: 1.169215
 901227/1000000: episode: 9013, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 57.483, mean reward: 0.575 [0.498, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.741, 10.098], loss: 0.001389, mae: 0.040853, mean_q: 1.167014
 901327/1000000: episode: 9014, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 61.666, mean reward: 0.617 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.507, 10.098], loss: 0.001443, mae: 0.041254, mean_q: 1.167057
 901427/1000000: episode: 9015, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.033, mean reward: 0.580 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.583, 10.098], loss: 0.001389, mae: 0.041154, mean_q: 1.166617
 901527/1000000: episode: 9016, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 60.557, mean reward: 0.606 [0.498, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.537, 10.336], loss: 0.001355, mae: 0.040161, mean_q: 1.170271
 901627/1000000: episode: 9017, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 57.306, mean reward: 0.573 [0.501, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.567, 10.205], loss: 0.001516, mae: 0.042880, mean_q: 1.169793
 901727/1000000: episode: 9018, duration: 1.242s, episode steps: 100, steps per second: 81, episode reward: 61.797, mean reward: 0.618 [0.503, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.994, 10.098], loss: 0.001399, mae: 0.041180, mean_q: 1.170004
 901827/1000000: episode: 9019, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 59.283, mean reward: 0.593 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.368, 10.246], loss: 0.001406, mae: 0.041688, mean_q: 1.171537
 901927/1000000: episode: 9020, duration: 1.273s, episode steps: 100, steps per second: 79, episode reward: 58.103, mean reward: 0.581 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.216, 10.330], loss: 0.001391, mae: 0.040873, mean_q: 1.172783
 902027/1000000: episode: 9021, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 56.775, mean reward: 0.568 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.300, 10.219], loss: 0.001375, mae: 0.040929, mean_q: 1.171251
 902127/1000000: episode: 9022, duration: 1.198s, episode steps: 100, steps per second: 83, episode reward: 60.079, mean reward: 0.601 [0.511, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.098], loss: 0.001389, mae: 0.040421, mean_q: 1.167342
 902227/1000000: episode: 9023, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 59.315, mean reward: 0.593 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.018, 10.098], loss: 0.001385, mae: 0.040664, mean_q: 1.170790
 902327/1000000: episode: 9024, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 61.031, mean reward: 0.610 [0.516, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.367, 10.098], loss: 0.001549, mae: 0.042776, mean_q: 1.172138
 902427/1000000: episode: 9025, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 60.616, mean reward: 0.606 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.177, 10.120], loss: 0.001467, mae: 0.041692, mean_q: 1.174358
 902527/1000000: episode: 9026, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 61.058, mean reward: 0.611 [0.500, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.714, 10.098], loss: 0.001485, mae: 0.042145, mean_q: 1.171608
 902627/1000000: episode: 9027, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 59.488, mean reward: 0.595 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.554, 10.136], loss: 0.001433, mae: 0.041099, mean_q: 1.173030
 902727/1000000: episode: 9028, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 60.710, mean reward: 0.607 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.770, 10.120], loss: 0.001460, mae: 0.041445, mean_q: 1.175164
 902827/1000000: episode: 9029, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 57.958, mean reward: 0.580 [0.508, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.178, 10.159], loss: 0.001470, mae: 0.042027, mean_q: 1.172044
 902927/1000000: episode: 9030, duration: 1.262s, episode steps: 100, steps per second: 79, episode reward: 60.173, mean reward: 0.602 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.528, 10.098], loss: 0.001419, mae: 0.041219, mean_q: 1.168835
 903027/1000000: episode: 9031, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 60.758, mean reward: 0.608 [0.498, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.671, 10.209], loss: 0.001444, mae: 0.041740, mean_q: 1.173614
 903127/1000000: episode: 9032, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 58.714, mean reward: 0.587 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.859, 10.360], loss: 0.001469, mae: 0.041923, mean_q: 1.172932
 903227/1000000: episode: 9033, duration: 1.005s, episode steps: 100, steps per second: 100, episode reward: 58.946, mean reward: 0.589 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.429, 10.098], loss: 0.001619, mae: 0.043044, mean_q: 1.167863
 903327/1000000: episode: 9034, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 60.298, mean reward: 0.603 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.967, 10.148], loss: 0.001501, mae: 0.042071, mean_q: 1.171502
 903427/1000000: episode: 9035, duration: 1.028s, episode steps: 100, steps per second: 97, episode reward: 57.633, mean reward: 0.576 [0.507, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.191], loss: 0.001388, mae: 0.040733, mean_q: 1.170852
 903527/1000000: episode: 9036, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 57.829, mean reward: 0.578 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.813, 10.217], loss: 0.001441, mae: 0.041422, mean_q: 1.169506
 903627/1000000: episode: 9037, duration: 1.352s, episode steps: 100, steps per second: 74, episode reward: 59.561, mean reward: 0.596 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.668, 10.342], loss: 0.001396, mae: 0.040855, mean_q: 1.171954
 903727/1000000: episode: 9038, duration: 1.402s, episode steps: 100, steps per second: 71, episode reward: 61.174, mean reward: 0.612 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.985, 10.285], loss: 0.001531, mae: 0.042833, mean_q: 1.174666
 903827/1000000: episode: 9039, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 59.590, mean reward: 0.596 [0.498, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.154, 10.287], loss: 0.001477, mae: 0.041977, mean_q: 1.171174
 903927/1000000: episode: 9040, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 59.574, mean reward: 0.596 [0.502, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.924, 10.268], loss: 0.001516, mae: 0.042421, mean_q: 1.172502
 904027/1000000: episode: 9041, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 60.023, mean reward: 0.600 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.079, 10.098], loss: 0.001440, mae: 0.041356, mean_q: 1.168091
 904127/1000000: episode: 9042, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 59.880, mean reward: 0.599 [0.517, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.672, 10.098], loss: 0.001512, mae: 0.042486, mean_q: 1.172777
 904227/1000000: episode: 9043, duration: 1.278s, episode steps: 100, steps per second: 78, episode reward: 59.317, mean reward: 0.593 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.545, 10.170], loss: 0.001441, mae: 0.041232, mean_q: 1.168396
 904327/1000000: episode: 9044, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.739, mean reward: 0.597 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.062, 10.098], loss: 0.001462, mae: 0.041859, mean_q: 1.173063
 904427/1000000: episode: 9045, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 58.847, mean reward: 0.588 [0.504, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.815, 10.237], loss: 0.001512, mae: 0.042992, mean_q: 1.174836
 904527/1000000: episode: 9046, duration: 1.129s, episode steps: 100, steps per second: 89, episode reward: 60.204, mean reward: 0.602 [0.514, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.231], loss: 0.001456, mae: 0.042266, mean_q: 1.176382
 904627/1000000: episode: 9047, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 57.736, mean reward: 0.577 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.164], loss: 0.001397, mae: 0.040691, mean_q: 1.172297
 904727/1000000: episode: 9048, duration: 1.318s, episode steps: 100, steps per second: 76, episode reward: 59.944, mean reward: 0.599 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.334, 10.489], loss: 0.001400, mae: 0.041334, mean_q: 1.175231
 904827/1000000: episode: 9049, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 60.053, mean reward: 0.601 [0.508, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.920, 10.098], loss: 0.001435, mae: 0.041477, mean_q: 1.173274
 904927/1000000: episode: 9050, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 57.255, mean reward: 0.573 [0.503, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.924, 10.129], loss: 0.001497, mae: 0.042355, mean_q: 1.171642
 905027/1000000: episode: 9051, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 56.790, mean reward: 0.568 [0.498, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.067, 10.098], loss: 0.001434, mae: 0.041295, mean_q: 1.172421
 905127/1000000: episode: 9052, duration: 1.459s, episode steps: 100, steps per second: 69, episode reward: 58.599, mean reward: 0.586 [0.510, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.101], loss: 0.001435, mae: 0.040830, mean_q: 1.171303
 905227/1000000: episode: 9053, duration: 1.340s, episode steps: 100, steps per second: 75, episode reward: 56.713, mean reward: 0.567 [0.498, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.180], loss: 0.001381, mae: 0.040677, mean_q: 1.171201
 905327/1000000: episode: 9054, duration: 1.401s, episode steps: 100, steps per second: 71, episode reward: 59.900, mean reward: 0.599 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.157], loss: 0.001380, mae: 0.041305, mean_q: 1.173226
 905427/1000000: episode: 9055, duration: 1.460s, episode steps: 100, steps per second: 68, episode reward: 57.457, mean reward: 0.575 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.155, 10.098], loss: 0.001500, mae: 0.041847, mean_q: 1.170160
 905527/1000000: episode: 9056, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 59.791, mean reward: 0.598 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.562, 10.165], loss: 0.001520, mae: 0.042127, mean_q: 1.173800
 905627/1000000: episode: 9057, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: 59.076, mean reward: 0.591 [0.499, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.998, 10.098], loss: 0.001465, mae: 0.041593, mean_q: 1.171974
 905727/1000000: episode: 9058, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 59.786, mean reward: 0.598 [0.512, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.052, 10.098], loss: 0.001552, mae: 0.042896, mean_q: 1.171345
 905827/1000000: episode: 9059, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 57.202, mean reward: 0.572 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.792, 10.098], loss: 0.001458, mae: 0.041569, mean_q: 1.171781
 905927/1000000: episode: 9060, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 59.953, mean reward: 0.600 [0.513, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.770, 10.465], loss: 0.001527, mae: 0.042511, mean_q: 1.175087
 906027/1000000: episode: 9061, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 58.612, mean reward: 0.586 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.683, 10.098], loss: 0.001622, mae: 0.043536, mean_q: 1.174156
 906127/1000000: episode: 9062, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 60.252, mean reward: 0.603 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.543, 10.136], loss: 0.001561, mae: 0.042930, mean_q: 1.174056
 906227/1000000: episode: 9063, duration: 1.394s, episode steps: 100, steps per second: 72, episode reward: 57.512, mean reward: 0.575 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.766, 10.209], loss: 0.001403, mae: 0.040814, mean_q: 1.175463
 906327/1000000: episode: 9064, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 59.544, mean reward: 0.595 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.765, 10.333], loss: 0.001512, mae: 0.042221, mean_q: 1.174588
 906427/1000000: episode: 9065, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 58.267, mean reward: 0.583 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.098], loss: 0.001477, mae: 0.040970, mean_q: 1.172842
 906527/1000000: episode: 9066, duration: 1.429s, episode steps: 100, steps per second: 70, episode reward: 59.682, mean reward: 0.597 [0.500, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.344, 10.098], loss: 0.001453, mae: 0.041268, mean_q: 1.171942
 906627/1000000: episode: 9067, duration: 1.288s, episode steps: 100, steps per second: 78, episode reward: 58.598, mean reward: 0.586 [0.508, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.376, 10.155], loss: 0.001465, mae: 0.041749, mean_q: 1.174959
 906727/1000000: episode: 9068, duration: 1.375s, episode steps: 100, steps per second: 73, episode reward: 58.183, mean reward: 0.582 [0.503, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.074, 10.098], loss: 0.001538, mae: 0.043265, mean_q: 1.171761
 906827/1000000: episode: 9069, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 58.190, mean reward: 0.582 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.685, 10.263], loss: 0.001501, mae: 0.042703, mean_q: 1.170089
 906927/1000000: episode: 9070, duration: 1.388s, episode steps: 100, steps per second: 72, episode reward: 59.441, mean reward: 0.594 [0.514, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.327, 10.098], loss: 0.001553, mae: 0.043145, mean_q: 1.168901
 907027/1000000: episode: 9071, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 60.813, mean reward: 0.608 [0.505, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.257, 10.098], loss: 0.001508, mae: 0.042563, mean_q: 1.173494
 907127/1000000: episode: 9072, duration: 1.400s, episode steps: 100, steps per second: 71, episode reward: 58.906, mean reward: 0.589 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.815, 10.098], loss: 0.001503, mae: 0.041991, mean_q: 1.169755
 907227/1000000: episode: 9073, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 58.770, mean reward: 0.588 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.388, 10.098], loss: 0.001515, mae: 0.042096, mean_q: 1.169944
 907327/1000000: episode: 9074, duration: 1.324s, episode steps: 100, steps per second: 76, episode reward: 58.755, mean reward: 0.588 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.511, 10.098], loss: 0.001485, mae: 0.042491, mean_q: 1.175067
 907427/1000000: episode: 9075, duration: 1.301s, episode steps: 100, steps per second: 77, episode reward: 61.691, mean reward: 0.617 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.763, 10.490], loss: 0.001547, mae: 0.042843, mean_q: 1.170939
 907527/1000000: episode: 9076, duration: 1.399s, episode steps: 100, steps per second: 72, episode reward: 60.150, mean reward: 0.601 [0.506, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.731, 10.098], loss: 0.001524, mae: 0.042330, mean_q: 1.171542
 907627/1000000: episode: 9077, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 60.416, mean reward: 0.604 [0.508, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.384, 10.098], loss: 0.001556, mae: 0.042900, mean_q: 1.169239
 907727/1000000: episode: 9078, duration: 1.410s, episode steps: 100, steps per second: 71, episode reward: 57.221, mean reward: 0.572 [0.500, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.098], loss: 0.001608, mae: 0.042893, mean_q: 1.173399
 907827/1000000: episode: 9079, duration: 1.391s, episode steps: 100, steps per second: 72, episode reward: 58.944, mean reward: 0.589 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.598, 10.098], loss: 0.001586, mae: 0.043027, mean_q: 1.170971
 907927/1000000: episode: 9080, duration: 1.324s, episode steps: 100, steps per second: 76, episode reward: 58.072, mean reward: 0.581 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.426, 10.102], loss: 0.001475, mae: 0.041565, mean_q: 1.169574
 908027/1000000: episode: 9081, duration: 1.311s, episode steps: 100, steps per second: 76, episode reward: 58.813, mean reward: 0.588 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.102], loss: 0.001514, mae: 0.042005, mean_q: 1.171353
 908127/1000000: episode: 9082, duration: 1.494s, episode steps: 100, steps per second: 67, episode reward: 59.159, mean reward: 0.592 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.568, 10.098], loss: 0.001486, mae: 0.042217, mean_q: 1.168032
 908227/1000000: episode: 9083, duration: 1.412s, episode steps: 100, steps per second: 71, episode reward: 58.717, mean reward: 0.587 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.412, 10.375], loss: 0.001519, mae: 0.042704, mean_q: 1.167526
 908327/1000000: episode: 9084, duration: 1.386s, episode steps: 100, steps per second: 72, episode reward: 58.936, mean reward: 0.589 [0.509, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.364, 10.098], loss: 0.001588, mae: 0.042093, mean_q: 1.167554
 908427/1000000: episode: 9085, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 61.004, mean reward: 0.610 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.044, 10.098], loss: 0.001462, mae: 0.041383, mean_q: 1.166326
 908527/1000000: episode: 9086, duration: 1.460s, episode steps: 100, steps per second: 69, episode reward: 56.429, mean reward: 0.564 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.560, 10.098], loss: 0.001654, mae: 0.044190, mean_q: 1.174592
 908627/1000000: episode: 9087, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: 58.372, mean reward: 0.584 [0.517, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.283, 10.158], loss: 0.001573, mae: 0.042848, mean_q: 1.168232
 908727/1000000: episode: 9088, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 58.623, mean reward: 0.586 [0.515, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.298, 10.149], loss: 0.001437, mae: 0.040796, mean_q: 1.165349
 908827/1000000: episode: 9089, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 58.409, mean reward: 0.584 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.694, 10.208], loss: 0.001514, mae: 0.042841, mean_q: 1.169014
 908927/1000000: episode: 9090, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 56.770, mean reward: 0.568 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.949, 10.153], loss: 0.001517, mae: 0.042045, mean_q: 1.169976
 909027/1000000: episode: 9091, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 57.139, mean reward: 0.571 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.230, 10.140], loss: 0.001544, mae: 0.041567, mean_q: 1.167827
 909127/1000000: episode: 9092, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 60.247, mean reward: 0.602 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.207, 10.101], loss: 0.001540, mae: 0.042041, mean_q: 1.167517
 909227/1000000: episode: 9093, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 59.226, mean reward: 0.592 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.682, 10.098], loss: 0.001597, mae: 0.042514, mean_q: 1.164147
 909327/1000000: episode: 9094, duration: 1.393s, episode steps: 100, steps per second: 72, episode reward: 62.425, mean reward: 0.624 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.213, 10.447], loss: 0.001528, mae: 0.042470, mean_q: 1.165466
 909427/1000000: episode: 9095, duration: 1.330s, episode steps: 100, steps per second: 75, episode reward: 62.661, mean reward: 0.627 [0.501, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.198], loss: 0.001676, mae: 0.043559, mean_q: 1.167020
 909527/1000000: episode: 9096, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 60.217, mean reward: 0.602 [0.513, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.742, 10.098], loss: 0.001495, mae: 0.041226, mean_q: 1.165430
 909627/1000000: episode: 9097, duration: 1.370s, episode steps: 100, steps per second: 73, episode reward: 56.724, mean reward: 0.567 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.176, 10.129], loss: 0.001528, mae: 0.041056, mean_q: 1.166288
 909727/1000000: episode: 9098, duration: 1.420s, episode steps: 100, steps per second: 70, episode reward: 60.810, mean reward: 0.608 [0.499, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.181, 10.199], loss: 0.001637, mae: 0.043000, mean_q: 1.167792
 909827/1000000: episode: 9099, duration: 1.477s, episode steps: 100, steps per second: 68, episode reward: 58.864, mean reward: 0.589 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.586, 10.268], loss: 0.001504, mae: 0.041411, mean_q: 1.165293
 909927/1000000: episode: 9100, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: 58.331, mean reward: 0.583 [0.498, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.594, 10.123], loss: 0.001628, mae: 0.043564, mean_q: 1.169635
 910027/1000000: episode: 9101, duration: 1.823s, episode steps: 100, steps per second: 55, episode reward: 60.734, mean reward: 0.607 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.215, 10.098], loss: 0.001510, mae: 0.041600, mean_q: 1.167133
 910127/1000000: episode: 9102, duration: 1.554s, episode steps: 100, steps per second: 64, episode reward: 60.018, mean reward: 0.600 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.422, 10.126], loss: 0.001456, mae: 0.041080, mean_q: 1.167297
 910227/1000000: episode: 9103, duration: 1.447s, episode steps: 100, steps per second: 69, episode reward: 59.956, mean reward: 0.600 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.209, 10.135], loss: 0.001549, mae: 0.042200, mean_q: 1.170617
 910327/1000000: episode: 9104, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 59.412, mean reward: 0.594 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.696, 10.463], loss: 0.001474, mae: 0.041838, mean_q: 1.168029
 910427/1000000: episode: 9105, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 56.576, mean reward: 0.566 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.657, 10.165], loss: 0.001521, mae: 0.041775, mean_q: 1.168457
 910527/1000000: episode: 9106, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 58.721, mean reward: 0.587 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.356, 10.098], loss: 0.001461, mae: 0.041224, mean_q: 1.168983
 910627/1000000: episode: 9107, duration: 1.280s, episode steps: 100, steps per second: 78, episode reward: 61.341, mean reward: 0.613 [0.515, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.601, 10.098], loss: 0.001549, mae: 0.042408, mean_q: 1.168943
 910727/1000000: episode: 9108, duration: 1.291s, episode steps: 100, steps per second: 77, episode reward: 57.544, mean reward: 0.575 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.944, 10.178], loss: 0.001524, mae: 0.042272, mean_q: 1.171837
 910827/1000000: episode: 9109, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 58.644, mean reward: 0.586 [0.512, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.866, 10.308], loss: 0.001565, mae: 0.042919, mean_q: 1.172503
 910927/1000000: episode: 9110, duration: 1.194s, episode steps: 100, steps per second: 84, episode reward: 57.214, mean reward: 0.572 [0.509, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.450, 10.172], loss: 0.001534, mae: 0.042728, mean_q: 1.169146
 911027/1000000: episode: 9111, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.010, mean reward: 0.580 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.098], loss: 0.001581, mae: 0.042624, mean_q: 1.165979
 911127/1000000: episode: 9112, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 60.737, mean reward: 0.607 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.459, 10.098], loss: 0.001586, mae: 0.043018, mean_q: 1.169179
 911227/1000000: episode: 9113, duration: 1.410s, episode steps: 100, steps per second: 71, episode reward: 64.714, mean reward: 0.647 [0.512, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.419, 10.098], loss: 0.001534, mae: 0.042097, mean_q: 1.171366
 911327/1000000: episode: 9114, duration: 1.231s, episode steps: 100, steps per second: 81, episode reward: 60.148, mean reward: 0.601 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.856, 10.495], loss: 0.001542, mae: 0.042273, mean_q: 1.174118
 911427/1000000: episode: 9115, duration: 1.430s, episode steps: 100, steps per second: 70, episode reward: 59.724, mean reward: 0.597 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.716, 10.144], loss: 0.001529, mae: 0.042724, mean_q: 1.174954
 911527/1000000: episode: 9116, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 59.414, mean reward: 0.594 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.261], loss: 0.001523, mae: 0.041889, mean_q: 1.167703
 911627/1000000: episode: 9117, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 58.533, mean reward: 0.585 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.346, 10.177], loss: 0.001483, mae: 0.041900, mean_q: 1.170777
 911727/1000000: episode: 9118, duration: 1.353s, episode steps: 100, steps per second: 74, episode reward: 57.998, mean reward: 0.580 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.204, 10.098], loss: 0.001478, mae: 0.042317, mean_q: 1.171026
 911827/1000000: episode: 9119, duration: 1.621s, episode steps: 100, steps per second: 62, episode reward: 59.367, mean reward: 0.594 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.269], loss: 0.001569, mae: 0.042934, mean_q: 1.172428
 911927/1000000: episode: 9120, duration: 1.616s, episode steps: 100, steps per second: 62, episode reward: 58.285, mean reward: 0.583 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.494, 10.098], loss: 0.001552, mae: 0.042877, mean_q: 1.170412
 912027/1000000: episode: 9121, duration: 1.690s, episode steps: 100, steps per second: 59, episode reward: 56.859, mean reward: 0.569 [0.497, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.037, 10.134], loss: 0.001687, mae: 0.045127, mean_q: 1.173196
 912127/1000000: episode: 9122, duration: 1.603s, episode steps: 100, steps per second: 62, episode reward: 57.419, mean reward: 0.574 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.553, 10.098], loss: 0.001766, mae: 0.046124, mean_q: 1.172607
 912227/1000000: episode: 9123, duration: 1.418s, episode steps: 100, steps per second: 71, episode reward: 58.348, mean reward: 0.583 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.203], loss: 0.001528, mae: 0.042475, mean_q: 1.170994
 912327/1000000: episode: 9124, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 60.547, mean reward: 0.605 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.382, 10.356], loss: 0.001575, mae: 0.042881, mean_q: 1.170781
 912427/1000000: episode: 9125, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 58.064, mean reward: 0.581 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.372, 10.158], loss: 0.001517, mae: 0.042301, mean_q: 1.167854
 912527/1000000: episode: 9126, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 60.566, mean reward: 0.606 [0.510, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.078, 10.173], loss: 0.001539, mae: 0.042002, mean_q: 1.174282
 912627/1000000: episode: 9127, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 59.358, mean reward: 0.594 [0.510, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.461, 10.108], loss: 0.001563, mae: 0.042759, mean_q: 1.170626
 912727/1000000: episode: 9128, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 58.260, mean reward: 0.583 [0.511, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.179, 10.208], loss: 0.001490, mae: 0.041975, mean_q: 1.169681
 912827/1000000: episode: 9129, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 58.170, mean reward: 0.582 [0.510, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.411, 10.200], loss: 0.001581, mae: 0.043303, mean_q: 1.170175
 912927/1000000: episode: 9130, duration: 1.030s, episode steps: 100, steps per second: 97, episode reward: 60.701, mean reward: 0.607 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.098], loss: 0.001662, mae: 0.044137, mean_q: 1.167712
 913027/1000000: episode: 9131, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 61.899, mean reward: 0.619 [0.509, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.028, 10.098], loss: 0.001558, mae: 0.042984, mean_q: 1.170778
 913127/1000000: episode: 9132, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.912, mean reward: 0.589 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.155, 10.104], loss: 0.001576, mae: 0.042828, mean_q: 1.174546
 913227/1000000: episode: 9133, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 57.800, mean reward: 0.578 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.128], loss: 0.001565, mae: 0.042911, mean_q: 1.172855
 913327/1000000: episode: 9134, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 57.913, mean reward: 0.579 [0.503, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.395, 10.146], loss: 0.001565, mae: 0.042929, mean_q: 1.172368
 913427/1000000: episode: 9135, duration: 1.297s, episode steps: 100, steps per second: 77, episode reward: 59.052, mean reward: 0.591 [0.499, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.046, 10.098], loss: 0.001570, mae: 0.043190, mean_q: 1.169650
 913527/1000000: episode: 9136, duration: 1.394s, episode steps: 100, steps per second: 72, episode reward: 57.973, mean reward: 0.580 [0.504, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.098], loss: 0.001603, mae: 0.043756, mean_q: 1.173990
 913627/1000000: episode: 9137, duration: 1.782s, episode steps: 100, steps per second: 56, episode reward: 56.800, mean reward: 0.568 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.098], loss: 0.001664, mae: 0.044088, mean_q: 1.176898
 913727/1000000: episode: 9138, duration: 1.646s, episode steps: 100, steps per second: 61, episode reward: 56.797, mean reward: 0.568 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.414, 10.110], loss: 0.001517, mae: 0.042368, mean_q: 1.170581
 913827/1000000: episode: 9139, duration: 1.604s, episode steps: 100, steps per second: 62, episode reward: 59.125, mean reward: 0.591 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.745, 10.098], loss: 0.001540, mae: 0.042878, mean_q: 1.170509
 913927/1000000: episode: 9140, duration: 1.992s, episode steps: 100, steps per second: 50, episode reward: 58.881, mean reward: 0.589 [0.511, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.098], loss: 0.001631, mae: 0.043462, mean_q: 1.171072
 914027/1000000: episode: 9141, duration: 1.870s, episode steps: 100, steps per second: 53, episode reward: 60.206, mean reward: 0.602 [0.514, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.783, 10.098], loss: 0.001617, mae: 0.043474, mean_q: 1.167724
 914127/1000000: episode: 9142, duration: 1.475s, episode steps: 100, steps per second: 68, episode reward: 59.468, mean reward: 0.595 [0.511, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.931, 10.229], loss: 0.001656, mae: 0.043641, mean_q: 1.169156
 914227/1000000: episode: 9143, duration: 1.831s, episode steps: 100, steps per second: 55, episode reward: 60.033, mean reward: 0.600 [0.506, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.545, 10.342], loss: 0.001695, mae: 0.044554, mean_q: 1.171998
 914327/1000000: episode: 9144, duration: 1.491s, episode steps: 100, steps per second: 67, episode reward: 62.202, mean reward: 0.622 [0.520, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.517, 10.098], loss: 0.001671, mae: 0.044519, mean_q: 1.173064
 914427/1000000: episode: 9145, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 61.678, mean reward: 0.617 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.583, 10.098], loss: 0.001749, mae: 0.045266, mean_q: 1.173329
 914527/1000000: episode: 9146, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 57.224, mean reward: 0.572 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.836, 10.223], loss: 0.001776, mae: 0.045761, mean_q: 1.171718
 914627/1000000: episode: 9147, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 58.136, mean reward: 0.581 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.384, 10.098], loss: 0.001710, mae: 0.044127, mean_q: 1.170640
 914727/1000000: episode: 9148, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 60.190, mean reward: 0.602 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.003, 10.098], loss: 0.001690, mae: 0.044545, mean_q: 1.171052
 914827/1000000: episode: 9149, duration: 1.370s, episode steps: 100, steps per second: 73, episode reward: 58.343, mean reward: 0.583 [0.502, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.903, 10.098], loss: 0.001568, mae: 0.042550, mean_q: 1.168916
 914927/1000000: episode: 9150, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 57.791, mean reward: 0.578 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.756, 10.210], loss: 0.001610, mae: 0.043144, mean_q: 1.171000
 915027/1000000: episode: 9151, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 57.265, mean reward: 0.573 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.036, 10.266], loss: 0.001569, mae: 0.043356, mean_q: 1.168197
 915127/1000000: episode: 9152, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 59.486, mean reward: 0.595 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.098], loss: 0.001571, mae: 0.042741, mean_q: 1.166268
 915227/1000000: episode: 9153, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 59.685, mean reward: 0.597 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.184, 10.098], loss: 0.001689, mae: 0.043940, mean_q: 1.166333
 915327/1000000: episode: 9154, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 57.137, mean reward: 0.571 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.179, 10.098], loss: 0.001571, mae: 0.043402, mean_q: 1.167797
 915427/1000000: episode: 9155, duration: 1.257s, episode steps: 100, steps per second: 80, episode reward: 58.095, mean reward: 0.581 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.098], loss: 0.001631, mae: 0.043479, mean_q: 1.165210
 915527/1000000: episode: 9156, duration: 1.692s, episode steps: 100, steps per second: 59, episode reward: 57.227, mean reward: 0.572 [0.499, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.562, 10.108], loss: 0.001584, mae: 0.042223, mean_q: 1.166932
 915627/1000000: episode: 9157, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: 62.151, mean reward: 0.622 [0.501, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.529, 10.098], loss: 0.001535, mae: 0.042612, mean_q: 1.168679
 915727/1000000: episode: 9158, duration: 1.702s, episode steps: 100, steps per second: 59, episode reward: 59.145, mean reward: 0.591 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.557, 10.098], loss: 0.001601, mae: 0.042947, mean_q: 1.171328
 915827/1000000: episode: 9159, duration: 1.389s, episode steps: 100, steps per second: 72, episode reward: 59.336, mean reward: 0.593 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.477, 10.280], loss: 0.001718, mae: 0.044365, mean_q: 1.163948
 915927/1000000: episode: 9160, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 63.435, mean reward: 0.634 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.105, 10.098], loss: 0.001610, mae: 0.042973, mean_q: 1.172994
 916027/1000000: episode: 9161, duration: 1.316s, episode steps: 100, steps per second: 76, episode reward: 57.206, mean reward: 0.572 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.757, 10.125], loss: 0.001627, mae: 0.043226, mean_q: 1.171373
 916127/1000000: episode: 9162, duration: 1.414s, episode steps: 100, steps per second: 71, episode reward: 57.463, mean reward: 0.575 [0.498, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.934, 10.098], loss: 0.001611, mae: 0.042804, mean_q: 1.172421
 916227/1000000: episode: 9163, duration: 1.308s, episode steps: 100, steps per second: 76, episode reward: 59.081, mean reward: 0.591 [0.499, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.783, 10.388], loss: 0.001509, mae: 0.041543, mean_q: 1.167542
 916327/1000000: episode: 9164, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.833, mean reward: 0.598 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.667, 10.316], loss: 0.001617, mae: 0.043509, mean_q: 1.168381
 916427/1000000: episode: 9165, duration: 1.510s, episode steps: 100, steps per second: 66, episode reward: 59.717, mean reward: 0.597 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.360, 10.239], loss: 0.001565, mae: 0.042893, mean_q: 1.169616
 916527/1000000: episode: 9166, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 64.755, mean reward: 0.648 [0.502, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.917, 10.423], loss: 0.001470, mae: 0.041770, mean_q: 1.167373
 916627/1000000: episode: 9167, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 63.111, mean reward: 0.631 [0.514, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.413, 10.098], loss: 0.001538, mae: 0.042110, mean_q: 1.169421
 916727/1000000: episode: 9168, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 62.619, mean reward: 0.626 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.701, 10.098], loss: 0.001571, mae: 0.042446, mean_q: 1.168638
 916827/1000000: episode: 9169, duration: 1.178s, episode steps: 100, steps per second: 85, episode reward: 60.286, mean reward: 0.603 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.078, 10.246], loss: 0.001616, mae: 0.043094, mean_q: 1.174220
 916927/1000000: episode: 9170, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 59.891, mean reward: 0.599 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.941, 10.230], loss: 0.001503, mae: 0.041869, mean_q: 1.170957
 917027/1000000: episode: 9171, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 58.930, mean reward: 0.589 [0.502, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.714, 10.098], loss: 0.001518, mae: 0.042220, mean_q: 1.177214
 917127/1000000: episode: 9172, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 57.609, mean reward: 0.576 [0.502, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.388, 10.098], loss: 0.001563, mae: 0.042464, mean_q: 1.174790
 917227/1000000: episode: 9173, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 56.373, mean reward: 0.564 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.372, 10.098], loss: 0.001516, mae: 0.042236, mean_q: 1.177140
 917327/1000000: episode: 9174, duration: 1.588s, episode steps: 100, steps per second: 63, episode reward: 60.982, mean reward: 0.610 [0.514, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.108, 10.098], loss: 0.001564, mae: 0.042944, mean_q: 1.174788
 917427/1000000: episode: 9175, duration: 1.684s, episode steps: 100, steps per second: 59, episode reward: 62.116, mean reward: 0.621 [0.531, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.343], loss: 0.001625, mae: 0.043158, mean_q: 1.174443
 917527/1000000: episode: 9176, duration: 1.611s, episode steps: 100, steps per second: 62, episode reward: 59.606, mean reward: 0.596 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.328, 10.098], loss: 0.001426, mae: 0.040404, mean_q: 1.180004
 917627/1000000: episode: 9177, duration: 1.429s, episode steps: 100, steps per second: 70, episode reward: 58.485, mean reward: 0.585 [0.507, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.502, 10.098], loss: 0.001524, mae: 0.042104, mean_q: 1.178347
 917727/1000000: episode: 9178, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 58.603, mean reward: 0.586 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.210, 10.098], loss: 0.001485, mae: 0.041770, mean_q: 1.179547
 917827/1000000: episode: 9179, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 58.232, mean reward: 0.582 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.132], loss: 0.001500, mae: 0.041585, mean_q: 1.175087
 917927/1000000: episode: 9180, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 58.978, mean reward: 0.590 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.166, 10.211], loss: 0.001378, mae: 0.039249, mean_q: 1.173386
 918027/1000000: episode: 9181, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.534, mean reward: 0.595 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.286], loss: 0.001433, mae: 0.041027, mean_q: 1.174554
 918127/1000000: episode: 9182, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 59.955, mean reward: 0.600 [0.504, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.755, 10.307], loss: 0.001530, mae: 0.042455, mean_q: 1.174088
 918227/1000000: episode: 9183, duration: 1.296s, episode steps: 100, steps per second: 77, episode reward: 57.107, mean reward: 0.571 [0.511, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.440, 10.186], loss: 0.001598, mae: 0.042906, mean_q: 1.178168
 918327/1000000: episode: 9184, duration: 1.149s, episode steps: 100, steps per second: 87, episode reward: 62.883, mean reward: 0.629 [0.512, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.664, 10.098], loss: 0.001545, mae: 0.042238, mean_q: 1.173764
 918427/1000000: episode: 9185, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 58.686, mean reward: 0.587 [0.500, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.527, 10.098], loss: 0.001427, mae: 0.040995, mean_q: 1.172731
 918527/1000000: episode: 9186, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 64.619, mean reward: 0.646 [0.506, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.098], loss: 0.001524, mae: 0.041896, mean_q: 1.179521
 918627/1000000: episode: 9187, duration: 1.303s, episode steps: 100, steps per second: 77, episode reward: 58.552, mean reward: 0.586 [0.499, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.661, 10.098], loss: 0.001465, mae: 0.040909, mean_q: 1.179741
 918727/1000000: episode: 9188, duration: 1.290s, episode steps: 100, steps per second: 78, episode reward: 58.170, mean reward: 0.582 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.098], loss: 0.001467, mae: 0.041531, mean_q: 1.177820
 918827/1000000: episode: 9189, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 60.055, mean reward: 0.601 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.371, 10.200], loss: 0.001430, mae: 0.040796, mean_q: 1.179070
 918927/1000000: episode: 9190, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 61.335, mean reward: 0.613 [0.516, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.222], loss: 0.001574, mae: 0.042469, mean_q: 1.181981
 919027/1000000: episode: 9191, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 59.611, mean reward: 0.596 [0.514, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.518, 10.098], loss: 0.001606, mae: 0.043466, mean_q: 1.180641
 919127/1000000: episode: 9192, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.768, mean reward: 0.588 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.768, 10.234], loss: 0.001484, mae: 0.041321, mean_q: 1.180864
 919227/1000000: episode: 9193, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 58.429, mean reward: 0.584 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.098], loss: 0.001556, mae: 0.042108, mean_q: 1.180905
 919327/1000000: episode: 9194, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 62.564, mean reward: 0.626 [0.517, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.409 [-1.449, 10.098], loss: 0.001477, mae: 0.041635, mean_q: 1.179459
 919427/1000000: episode: 9195, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 57.202, mean reward: 0.572 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.978, 10.177], loss: 0.001514, mae: 0.042610, mean_q: 1.180254
 919527/1000000: episode: 9196, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 59.739, mean reward: 0.597 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.603, 10.098], loss: 0.001634, mae: 0.043303, mean_q: 1.181302
 919627/1000000: episode: 9197, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 59.433, mean reward: 0.594 [0.505, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.649, 10.138], loss: 0.001611, mae: 0.043497, mean_q: 1.182712
 919727/1000000: episode: 9198, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 58.251, mean reward: 0.583 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.375, 10.098], loss: 0.001522, mae: 0.041848, mean_q: 1.179865
 919827/1000000: episode: 9199, duration: 1.187s, episode steps: 100, steps per second: 84, episode reward: 56.705, mean reward: 0.567 [0.506, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.638, 10.176], loss: 0.001436, mae: 0.040834, mean_q: 1.180085
 919927/1000000: episode: 9200, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 56.346, mean reward: 0.563 [0.503, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.256, 10.130], loss: 0.001467, mae: 0.041213, mean_q: 1.181960
 920027/1000000: episode: 9201, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 65.424, mean reward: 0.654 [0.508, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.444, 10.520], loss: 0.001447, mae: 0.041321, mean_q: 1.176323
 920127/1000000: episode: 9202, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 65.052, mean reward: 0.651 [0.504, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.890, 10.538], loss: 0.001387, mae: 0.039425, mean_q: 1.179519
 920227/1000000: episode: 9203, duration: 1.032s, episode steps: 100, steps per second: 97, episode reward: 60.968, mean reward: 0.610 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.098], loss: 0.001484, mae: 0.041167, mean_q: 1.186487
 920327/1000000: episode: 9204, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.712, mean reward: 0.587 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.083, 10.098], loss: 0.001442, mae: 0.041181, mean_q: 1.183442
 920427/1000000: episode: 9205, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 57.709, mean reward: 0.577 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.280, 10.106], loss: 0.001434, mae: 0.041503, mean_q: 1.187204
 920527/1000000: episode: 9206, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 58.751, mean reward: 0.588 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.219], loss: 0.001528, mae: 0.042511, mean_q: 1.186086
 920627/1000000: episode: 9207, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 58.610, mean reward: 0.586 [0.512, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.309, 10.323], loss: 0.001620, mae: 0.042872, mean_q: 1.188643
 920727/1000000: episode: 9208, duration: 1.535s, episode steps: 100, steps per second: 65, episode reward: 59.878, mean reward: 0.599 [0.499, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.640, 10.098], loss: 0.001480, mae: 0.041726, mean_q: 1.181221
 920827/1000000: episode: 9209, duration: 1.674s, episode steps: 100, steps per second: 60, episode reward: 60.388, mean reward: 0.604 [0.516, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.285, 10.181], loss: 0.001464, mae: 0.041648, mean_q: 1.184478
 920927/1000000: episode: 9210, duration: 1.704s, episode steps: 100, steps per second: 59, episode reward: 56.343, mean reward: 0.563 [0.504, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.734, 10.241], loss: 0.001569, mae: 0.042411, mean_q: 1.183441
 921027/1000000: episode: 9211, duration: 1.628s, episode steps: 100, steps per second: 61, episode reward: 58.749, mean reward: 0.587 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.098], loss: 0.001537, mae: 0.042757, mean_q: 1.180946
 921127/1000000: episode: 9212, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 58.282, mean reward: 0.583 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.234, 10.249], loss: 0.001541, mae: 0.042418, mean_q: 1.181982
 921227/1000000: episode: 9213, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 57.217, mean reward: 0.572 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.109, 10.098], loss: 0.001475, mae: 0.041633, mean_q: 1.180928
 921327/1000000: episode: 9214, duration: 1.014s, episode steps: 100, steps per second: 99, episode reward: 57.481, mean reward: 0.575 [0.503, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.518, 10.173], loss: 0.001479, mae: 0.040778, mean_q: 1.182007
 921427/1000000: episode: 9215, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 60.921, mean reward: 0.609 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.051, 10.098], loss: 0.001509, mae: 0.041691, mean_q: 1.178427
 921527/1000000: episode: 9216, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 59.045, mean reward: 0.590 [0.508, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.173, 10.149], loss: 0.001544, mae: 0.042348, mean_q: 1.180020
 921627/1000000: episode: 9217, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 60.592, mean reward: 0.606 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.229, 10.098], loss: 0.001500, mae: 0.042393, mean_q: 1.179351
 921727/1000000: episode: 9218, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 59.587, mean reward: 0.596 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.514, 10.098], loss: 0.001474, mae: 0.041942, mean_q: 1.175969
 921827/1000000: episode: 9219, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 58.862, mean reward: 0.589 [0.498, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.808, 10.098], loss: 0.001518, mae: 0.042110, mean_q: 1.175468
 921927/1000000: episode: 9220, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 57.333, mean reward: 0.573 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.358, 10.098], loss: 0.001510, mae: 0.041807, mean_q: 1.178880
 922027/1000000: episode: 9221, duration: 1.766s, episode steps: 100, steps per second: 57, episode reward: 58.986, mean reward: 0.590 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.762, 10.098], loss: 0.001469, mae: 0.042039, mean_q: 1.177397
 922127/1000000: episode: 9222, duration: 1.708s, episode steps: 100, steps per second: 59, episode reward: 57.856, mean reward: 0.579 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.098], loss: 0.001513, mae: 0.041811, mean_q: 1.177790
 922227/1000000: episode: 9223, duration: 1.581s, episode steps: 100, steps per second: 63, episode reward: 60.298, mean reward: 0.603 [0.511, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.200, 10.308], loss: 0.001550, mae: 0.041968, mean_q: 1.176826
 922327/1000000: episode: 9224, duration: 1.666s, episode steps: 100, steps per second: 60, episode reward: 61.239, mean reward: 0.612 [0.506, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 0.001568, mae: 0.042736, mean_q: 1.180221
 922427/1000000: episode: 9225, duration: 1.787s, episode steps: 100, steps per second: 56, episode reward: 57.320, mean reward: 0.573 [0.505, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.098], loss: 0.001615, mae: 0.043690, mean_q: 1.175022
 922527/1000000: episode: 9226, duration: 1.620s, episode steps: 100, steps per second: 62, episode reward: 57.157, mean reward: 0.572 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.034, 10.098], loss: 0.001530, mae: 0.042596, mean_q: 1.178506
 922627/1000000: episode: 9227, duration: 1.560s, episode steps: 100, steps per second: 64, episode reward: 58.703, mean reward: 0.587 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.163], loss: 0.001428, mae: 0.040840, mean_q: 1.175139
 922727/1000000: episode: 9228, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 58.440, mean reward: 0.584 [0.510, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.898, 10.201], loss: 0.001566, mae: 0.042310, mean_q: 1.177828
 922827/1000000: episode: 9229, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 57.196, mean reward: 0.572 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.435, 10.158], loss: 0.001495, mae: 0.041667, mean_q: 1.176513
 922927/1000000: episode: 9230, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 57.342, mean reward: 0.573 [0.506, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.098], loss: 0.001447, mae: 0.040819, mean_q: 1.171902
 923027/1000000: episode: 9231, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 58.648, mean reward: 0.586 [0.523, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.826, 10.098], loss: 0.001481, mae: 0.042342, mean_q: 1.170263
 923127/1000000: episode: 9232, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 57.988, mean reward: 0.580 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.552, 10.168], loss: 0.001477, mae: 0.040992, mean_q: 1.171618
 923227/1000000: episode: 9233, duration: 1.183s, episode steps: 100, steps per second: 85, episode reward: 58.342, mean reward: 0.583 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.296, 10.098], loss: 0.001483, mae: 0.041522, mean_q: 1.171797
 923327/1000000: episode: 9234, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 57.885, mean reward: 0.579 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.552, 10.210], loss: 0.001485, mae: 0.041631, mean_q: 1.169442
 923427/1000000: episode: 9235, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 56.334, mean reward: 0.563 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.182], loss: 0.001510, mae: 0.041497, mean_q: 1.169024
 923527/1000000: episode: 9236, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 58.216, mean reward: 0.582 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.978, 10.098], loss: 0.001394, mae: 0.040737, mean_q: 1.167355
 923627/1000000: episode: 9237, duration: 1.081s, episode steps: 100, steps per second: 92, episode reward: 62.279, mean reward: 0.623 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.151, 10.368], loss: 0.001503, mae: 0.041598, mean_q: 1.166752
 923727/1000000: episode: 9238, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 61.961, mean reward: 0.620 [0.512, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.216, 10.098], loss: 0.001513, mae: 0.041251, mean_q: 1.169719
 923827/1000000: episode: 9239, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 57.322, mean reward: 0.573 [0.505, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.324, 10.251], loss: 0.001475, mae: 0.041321, mean_q: 1.168521
 923927/1000000: episode: 9240, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.103, mean reward: 0.581 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.282, 10.232], loss: 0.001373, mae: 0.040148, mean_q: 1.167494
 924027/1000000: episode: 9241, duration: 1.436s, episode steps: 100, steps per second: 70, episode reward: 59.219, mean reward: 0.592 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.565, 10.170], loss: 0.001436, mae: 0.040226, mean_q: 1.165134
 924127/1000000: episode: 9242, duration: 1.576s, episode steps: 100, steps per second: 63, episode reward: 59.207, mean reward: 0.592 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.645, 10.098], loss: 0.001414, mae: 0.040194, mean_q: 1.167190
 924227/1000000: episode: 9243, duration: 1.886s, episode steps: 100, steps per second: 53, episode reward: 60.934, mean reward: 0.609 [0.510, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.718, 10.271], loss: 0.001522, mae: 0.041552, mean_q: 1.171008
 924327/1000000: episode: 9244, duration: 1.431s, episode steps: 100, steps per second: 70, episode reward: 58.433, mean reward: 0.584 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.482, 10.098], loss: 0.001423, mae: 0.041004, mean_q: 1.167877
 924427/1000000: episode: 9245, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 57.417, mean reward: 0.574 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.183], loss: 0.001412, mae: 0.040432, mean_q: 1.166812
 924527/1000000: episode: 9246, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 60.584, mean reward: 0.606 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.664, 10.143], loss: 0.001466, mae: 0.041305, mean_q: 1.164762
 924627/1000000: episode: 9247, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 59.836, mean reward: 0.598 [0.512, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.213, 10.190], loss: 0.001552, mae: 0.041649, mean_q: 1.167245
 924727/1000000: episode: 9248, duration: 1.297s, episode steps: 100, steps per second: 77, episode reward: 59.004, mean reward: 0.590 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.039, 10.098], loss: 0.001438, mae: 0.040816, mean_q: 1.163653
 924827/1000000: episode: 9249, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 61.272, mean reward: 0.613 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.911, 10.098], loss: 0.001501, mae: 0.041187, mean_q: 1.168525
 924927/1000000: episode: 9250, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 57.920, mean reward: 0.579 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.563, 10.098], loss: 0.001528, mae: 0.041322, mean_q: 1.165645
 925027/1000000: episode: 9251, duration: 1.304s, episode steps: 100, steps per second: 77, episode reward: 60.571, mean reward: 0.606 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.240, 10.228], loss: 0.001466, mae: 0.040961, mean_q: 1.164763
 925127/1000000: episode: 9252, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 59.242, mean reward: 0.592 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.038, 10.098], loss: 0.001474, mae: 0.041457, mean_q: 1.166821
 925227/1000000: episode: 9253, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 58.009, mean reward: 0.580 [0.517, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.775, 10.186], loss: 0.001502, mae: 0.041770, mean_q: 1.163964
 925327/1000000: episode: 9254, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 58.761, mean reward: 0.588 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.065, 10.098], loss: 0.001406, mae: 0.040299, mean_q: 1.161904
 925427/1000000: episode: 9255, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 59.422, mean reward: 0.594 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.674, 10.098], loss: 0.001413, mae: 0.040659, mean_q: 1.163174
 925527/1000000: episode: 9256, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 59.583, mean reward: 0.596 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.433, 10.098], loss: 0.001474, mae: 0.041291, mean_q: 1.164804
 925627/1000000: episode: 9257, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 58.279, mean reward: 0.583 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.437, 10.176], loss: 0.001465, mae: 0.041476, mean_q: 1.164881
 925727/1000000: episode: 9258, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 59.120, mean reward: 0.591 [0.525, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.791, 10.315], loss: 0.001309, mae: 0.039580, mean_q: 1.164991
 925827/1000000: episode: 9259, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.360, mean reward: 0.584 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.259, 10.098], loss: 0.001508, mae: 0.041110, mean_q: 1.165983
 925927/1000000: episode: 9260, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.016, mean reward: 0.580 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.751, 10.098], loss: 0.001590, mae: 0.042979, mean_q: 1.166049
 926027/1000000: episode: 9261, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 59.390, mean reward: 0.594 [0.515, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.452, 10.296], loss: 0.001495, mae: 0.041934, mean_q: 1.166789
 926127/1000000: episode: 9262, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 59.693, mean reward: 0.597 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.098], loss: 0.001555, mae: 0.042389, mean_q: 1.165822
 926227/1000000: episode: 9263, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 59.820, mean reward: 0.598 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.239, 10.126], loss: 0.001476, mae: 0.041235, mean_q: 1.167286
 926327/1000000: episode: 9264, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 57.700, mean reward: 0.577 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.907, 10.414], loss: 0.001419, mae: 0.040654, mean_q: 1.162340
 926427/1000000: episode: 9265, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 60.829, mean reward: 0.608 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.839, 10.301], loss: 0.001622, mae: 0.043128, mean_q: 1.167333
 926527/1000000: episode: 9266, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 58.855, mean reward: 0.589 [0.507, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.825, 10.115], loss: 0.001445, mae: 0.040867, mean_q: 1.166305
 926627/1000000: episode: 9267, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 57.513, mean reward: 0.575 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.728, 10.234], loss: 0.001590, mae: 0.042651, mean_q: 1.166100
 926727/1000000: episode: 9268, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 58.501, mean reward: 0.585 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.772, 10.098], loss: 0.001579, mae: 0.042779, mean_q: 1.167988
 926827/1000000: episode: 9269, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 62.027, mean reward: 0.620 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.119, 10.098], loss: 0.001535, mae: 0.041920, mean_q: 1.164142
 926927/1000000: episode: 9270, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 57.969, mean reward: 0.580 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.282, 10.180], loss: 0.001615, mae: 0.042649, mean_q: 1.165637
 927027/1000000: episode: 9271, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 58.499, mean reward: 0.585 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.098], loss: 0.001506, mae: 0.042263, mean_q: 1.167212
 927127/1000000: episode: 9272, duration: 0.991s, episode steps: 100, steps per second: 101, episode reward: 58.070, mean reward: 0.581 [0.512, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.441, 10.098], loss: 0.001487, mae: 0.041329, mean_q: 1.166099
 927227/1000000: episode: 9273, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 57.993, mean reward: 0.580 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.800, 10.116], loss: 0.001516, mae: 0.042391, mean_q: 1.165927
 927327/1000000: episode: 9274, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 60.415, mean reward: 0.604 [0.517, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.503, 10.345], loss: 0.001553, mae: 0.042340, mean_q: 1.166590
 927427/1000000: episode: 9275, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 58.840, mean reward: 0.588 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.565, 10.100], loss: 0.001549, mae: 0.042342, mean_q: 1.167694
 927527/1000000: episode: 9276, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 58.363, mean reward: 0.584 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.805, 10.098], loss: 0.001445, mae: 0.041430, mean_q: 1.166870
 927627/1000000: episode: 9277, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 58.391, mean reward: 0.584 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.557, 10.098], loss: 0.001432, mae: 0.041077, mean_q: 1.164611
 927727/1000000: episode: 9278, duration: 1.101s, episode steps: 100, steps per second: 91, episode reward: 57.867, mean reward: 0.579 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.869, 10.098], loss: 0.001453, mae: 0.041571, mean_q: 1.167534
 927827/1000000: episode: 9279, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 56.913, mean reward: 0.569 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.651, 10.110], loss: 0.001523, mae: 0.043026, mean_q: 1.166918
 927927/1000000: episode: 9280, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 61.657, mean reward: 0.617 [0.501, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.843, 10.098], loss: 0.001431, mae: 0.041536, mean_q: 1.165094
 928027/1000000: episode: 9281, duration: 1.333s, episode steps: 100, steps per second: 75, episode reward: 59.918, mean reward: 0.599 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.138], loss: 0.001565, mae: 0.043020, mean_q: 1.170333
 928127/1000000: episode: 9282, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.002, mean reward: 0.590 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.919, 10.098], loss: 0.001473, mae: 0.042178, mean_q: 1.168143
 928227/1000000: episode: 9283, duration: 1.523s, episode steps: 100, steps per second: 66, episode reward: 57.847, mean reward: 0.578 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.459, 10.209], loss: 0.001502, mae: 0.042409, mean_q: 1.170076
 928327/1000000: episode: 9284, duration: 1.320s, episode steps: 100, steps per second: 76, episode reward: 59.038, mean reward: 0.590 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.841, 10.200], loss: 0.001456, mae: 0.041318, mean_q: 1.166579
 928427/1000000: episode: 9285, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 57.514, mean reward: 0.575 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.073, 10.098], loss: 0.001424, mae: 0.041295, mean_q: 1.168495
 928527/1000000: episode: 9286, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 57.362, mean reward: 0.574 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.314, 10.098], loss: 0.001475, mae: 0.041539, mean_q: 1.170370
 928627/1000000: episode: 9287, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 58.015, mean reward: 0.580 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.535, 10.155], loss: 0.001516, mae: 0.042061, mean_q: 1.170125
 928727/1000000: episode: 9288, duration: 1.533s, episode steps: 100, steps per second: 65, episode reward: 61.178, mean reward: 0.612 [0.506, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.126, 10.098], loss: 0.001520, mae: 0.042455, mean_q: 1.170221
 928827/1000000: episode: 9289, duration: 1.233s, episode steps: 100, steps per second: 81, episode reward: 58.584, mean reward: 0.586 [0.505, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.379, 10.170], loss: 0.001423, mae: 0.040965, mean_q: 1.166897
 928927/1000000: episode: 9290, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 59.678, mean reward: 0.597 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.660, 10.356], loss: 0.001454, mae: 0.040965, mean_q: 1.166951
 929027/1000000: episode: 9291, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 60.334, mean reward: 0.603 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.453, 10.290], loss: 0.001510, mae: 0.042119, mean_q: 1.165955
 929127/1000000: episode: 9292, duration: 1.865s, episode steps: 100, steps per second: 54, episode reward: 60.009, mean reward: 0.600 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.014, 10.191], loss: 0.001460, mae: 0.041049, mean_q: 1.168616
 929227/1000000: episode: 9293, duration: 1.922s, episode steps: 100, steps per second: 52, episode reward: 59.482, mean reward: 0.595 [0.511, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.172, 10.134], loss: 0.001425, mae: 0.041264, mean_q: 1.167705
 929327/1000000: episode: 9294, duration: 1.888s, episode steps: 100, steps per second: 53, episode reward: 57.894, mean reward: 0.579 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.545, 10.098], loss: 0.001507, mae: 0.041855, mean_q: 1.169849
 929427/1000000: episode: 9295, duration: 1.571s, episode steps: 100, steps per second: 64, episode reward: 57.526, mean reward: 0.575 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.415, 10.162], loss: 0.001557, mae: 0.043046, mean_q: 1.171431
 929527/1000000: episode: 9296, duration: 1.629s, episode steps: 100, steps per second: 61, episode reward: 57.898, mean reward: 0.579 [0.515, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.639, 10.134], loss: 0.001400, mae: 0.041046, mean_q: 1.166892
 929627/1000000: episode: 9297, duration: 1.431s, episode steps: 100, steps per second: 70, episode reward: 58.700, mean reward: 0.587 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.492, 10.098], loss: 0.001502, mae: 0.042282, mean_q: 1.165633
 929727/1000000: episode: 9298, duration: 1.589s, episode steps: 100, steps per second: 63, episode reward: 57.822, mean reward: 0.578 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.098], loss: 0.001511, mae: 0.041879, mean_q: 1.167390
 929827/1000000: episode: 9299, duration: 1.677s, episode steps: 100, steps per second: 60, episode reward: 58.661, mean reward: 0.587 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.071, 10.115], loss: 0.001456, mae: 0.041331, mean_q: 1.165778
 929927/1000000: episode: 9300, duration: 1.348s, episode steps: 100, steps per second: 74, episode reward: 56.676, mean reward: 0.567 [0.499, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.486, 10.172], loss: 0.001458, mae: 0.041397, mean_q: 1.166065
 930027/1000000: episode: 9301, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 60.425, mean reward: 0.604 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.840, 10.098], loss: 0.001342, mae: 0.040161, mean_q: 1.160207
 930127/1000000: episode: 9302, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 57.318, mean reward: 0.573 [0.497, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.639, 10.211], loss: 0.001386, mae: 0.041147, mean_q: 1.165311
 930227/1000000: episode: 9303, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 60.468, mean reward: 0.605 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.931, 10.267], loss: 0.001460, mae: 0.041570, mean_q: 1.164065
 930327/1000000: episode: 9304, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 58.888, mean reward: 0.589 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.847, 10.098], loss: 0.001525, mae: 0.042306, mean_q: 1.164696
 930427/1000000: episode: 9305, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 59.352, mean reward: 0.594 [0.514, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.137, 10.098], loss: 0.001366, mae: 0.039871, mean_q: 1.166002
 930527/1000000: episode: 9306, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 57.358, mean reward: 0.574 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.449, 10.098], loss: 0.001495, mae: 0.042143, mean_q: 1.162639
 930627/1000000: episode: 9307, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.885, mean reward: 0.599 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.155, 10.279], loss: 0.001453, mae: 0.041917, mean_q: 1.160764
 930727/1000000: episode: 9308, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 58.433, mean reward: 0.584 [0.512, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.124, 10.143], loss: 0.001398, mae: 0.040698, mean_q: 1.161462
 930827/1000000: episode: 9309, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 57.622, mean reward: 0.576 [0.512, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.371, 10.311], loss: 0.001470, mae: 0.041887, mean_q: 1.166707
 930927/1000000: episode: 9310, duration: 1.322s, episode steps: 100, steps per second: 76, episode reward: 59.271, mean reward: 0.593 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.161, 10.098], loss: 0.001355, mae: 0.040385, mean_q: 1.164069
 931027/1000000: episode: 9311, duration: 1.361s, episode steps: 100, steps per second: 73, episode reward: 61.421, mean reward: 0.614 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.859, 10.337], loss: 0.001514, mae: 0.042283, mean_q: 1.165670
 931127/1000000: episode: 9312, duration: 1.118s, episode steps: 100, steps per second: 89, episode reward: 60.969, mean reward: 0.610 [0.519, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.909, 10.242], loss: 0.001351, mae: 0.040182, mean_q: 1.162897
 931227/1000000: episode: 9313, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 63.361, mean reward: 0.634 [0.499, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.846, 10.098], loss: 0.001422, mae: 0.041306, mean_q: 1.164907
 931327/1000000: episode: 9314, duration: 1.347s, episode steps: 100, steps per second: 74, episode reward: 60.202, mean reward: 0.602 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.772, 10.098], loss: 0.001527, mae: 0.042387, mean_q: 1.168832
 931427/1000000: episode: 9315, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 59.142, mean reward: 0.591 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.968, 10.273], loss: 0.001512, mae: 0.042669, mean_q: 1.169609
 931527/1000000: episode: 9316, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 57.919, mean reward: 0.579 [0.508, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.013, 10.200], loss: 0.001335, mae: 0.040364, mean_q: 1.169263
 931627/1000000: episode: 9317, duration: 1.287s, episode steps: 100, steps per second: 78, episode reward: 58.375, mean reward: 0.584 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.921, 10.205], loss: 0.001334, mae: 0.040270, mean_q: 1.168034
 931727/1000000: episode: 9318, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 56.760, mean reward: 0.568 [0.504, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.751, 10.335], loss: 0.001384, mae: 0.040238, mean_q: 1.166631
 931827/1000000: episode: 9319, duration: 1.298s, episode steps: 100, steps per second: 77, episode reward: 62.200, mean reward: 0.622 [0.510, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.567, 10.225], loss: 0.001444, mae: 0.041485, mean_q: 1.162498
 931927/1000000: episode: 9320, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 58.538, mean reward: 0.585 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.887, 10.359], loss: 0.001396, mae: 0.041346, mean_q: 1.170245
 932027/1000000: episode: 9321, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 60.707, mean reward: 0.607 [0.521, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.220], loss: 0.001343, mae: 0.039633, mean_q: 1.166540
 932127/1000000: episode: 9322, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 56.792, mean reward: 0.568 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.214, 10.172], loss: 0.001404, mae: 0.040622, mean_q: 1.165916
 932227/1000000: episode: 9323, duration: 1.107s, episode steps: 100, steps per second: 90, episode reward: 66.694, mean reward: 0.667 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.077, 10.374], loss: 0.001368, mae: 0.040464, mean_q: 1.166306
 932327/1000000: episode: 9324, duration: 1.282s, episode steps: 100, steps per second: 78, episode reward: 57.874, mean reward: 0.579 [0.498, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.426, 10.098], loss: 0.001329, mae: 0.040249, mean_q: 1.168576
 932427/1000000: episode: 9325, duration: 1.117s, episode steps: 100, steps per second: 90, episode reward: 57.060, mean reward: 0.571 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.418, 10.098], loss: 0.001381, mae: 0.040962, mean_q: 1.172249
 932527/1000000: episode: 9326, duration: 1.109s, episode steps: 100, steps per second: 90, episode reward: 59.807, mean reward: 0.598 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.680, 10.098], loss: 0.001427, mae: 0.040863, mean_q: 1.169373
 932627/1000000: episode: 9327, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 60.354, mean reward: 0.604 [0.523, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.189, 10.098], loss: 0.001474, mae: 0.041575, mean_q: 1.171352
 932727/1000000: episode: 9328, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 58.893, mean reward: 0.589 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.098], loss: 0.001397, mae: 0.041047, mean_q: 1.172192
 932827/1000000: episode: 9329, duration: 1.272s, episode steps: 100, steps per second: 79, episode reward: 57.451, mean reward: 0.575 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.036, 10.283], loss: 0.001385, mae: 0.040368, mean_q: 1.169257
 932927/1000000: episode: 9330, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 59.632, mean reward: 0.596 [0.516, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.219, 10.166], loss: 0.001401, mae: 0.040782, mean_q: 1.167413
 933027/1000000: episode: 9331, duration: 1.190s, episode steps: 100, steps per second: 84, episode reward: 57.320, mean reward: 0.573 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.288, 10.098], loss: 0.001336, mae: 0.039870, mean_q: 1.168064
 933127/1000000: episode: 9332, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 57.841, mean reward: 0.578 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.859, 10.307], loss: 0.001260, mae: 0.039129, mean_q: 1.171154
 933227/1000000: episode: 9333, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 57.676, mean reward: 0.577 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.103], loss: 0.001343, mae: 0.040060, mean_q: 1.168231
 933327/1000000: episode: 9334, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 57.482, mean reward: 0.575 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.933, 10.130], loss: 0.001354, mae: 0.039904, mean_q: 1.166420
 933427/1000000: episode: 9335, duration: 1.091s, episode steps: 100, steps per second: 92, episode reward: 56.856, mean reward: 0.569 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.677, 10.098], loss: 0.001380, mae: 0.040428, mean_q: 1.168880
 933527/1000000: episode: 9336, duration: 1.189s, episode steps: 100, steps per second: 84, episode reward: 56.450, mean reward: 0.565 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.329, 10.098], loss: 0.001375, mae: 0.040175, mean_q: 1.167366
 933627/1000000: episode: 9337, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 58.671, mean reward: 0.587 [0.507, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.452, 10.215], loss: 0.001361, mae: 0.039970, mean_q: 1.167269
 933727/1000000: episode: 9338, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 57.066, mean reward: 0.571 [0.502, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.676, 10.158], loss: 0.001414, mae: 0.040949, mean_q: 1.167453
 933827/1000000: episode: 9339, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 58.255, mean reward: 0.583 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.704, 10.098], loss: 0.001323, mae: 0.040130, mean_q: 1.163679
 933927/1000000: episode: 9340, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 58.975, mean reward: 0.590 [0.512, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.539, 10.213], loss: 0.001400, mae: 0.040667, mean_q: 1.166638
 934027/1000000: episode: 9341, duration: 1.301s, episode steps: 100, steps per second: 77, episode reward: 59.851, mean reward: 0.599 [0.509, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.740, 10.098], loss: 0.001359, mae: 0.040267, mean_q: 1.162437
 934127/1000000: episode: 9342, duration: 1.533s, episode steps: 100, steps per second: 65, episode reward: 59.546, mean reward: 0.595 [0.515, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.298, 10.380], loss: 0.001363, mae: 0.040396, mean_q: 1.162407
 934227/1000000: episode: 9343, duration: 1.619s, episode steps: 100, steps per second: 62, episode reward: 57.361, mean reward: 0.574 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.282], loss: 0.001398, mae: 0.040698, mean_q: 1.165493
 934327/1000000: episode: 9344, duration: 1.262s, episode steps: 100, steps per second: 79, episode reward: 57.645, mean reward: 0.576 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.227, 10.288], loss: 0.001353, mae: 0.040160, mean_q: 1.165159
 934427/1000000: episode: 9345, duration: 1.647s, episode steps: 100, steps per second: 61, episode reward: 59.708, mean reward: 0.597 [0.514, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.797, 10.098], loss: 0.001361, mae: 0.040206, mean_q: 1.166046
 934527/1000000: episode: 9346, duration: 1.141s, episode steps: 100, steps per second: 88, episode reward: 56.867, mean reward: 0.569 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.905, 10.098], loss: 0.001274, mae: 0.039217, mean_q: 1.167175
 934627/1000000: episode: 9347, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 63.311, mean reward: 0.633 [0.508, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.738, 10.359], loss: 0.001277, mae: 0.039354, mean_q: 1.163239
 934727/1000000: episode: 9348, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 63.747, mean reward: 0.637 [0.500, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.380, 10.098], loss: 0.001385, mae: 0.040387, mean_q: 1.167019
 934827/1000000: episode: 9349, duration: 1.863s, episode steps: 100, steps per second: 54, episode reward: 58.888, mean reward: 0.589 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.146, 10.140], loss: 0.001406, mae: 0.040405, mean_q: 1.167646
 934927/1000000: episode: 9350, duration: 1.923s, episode steps: 100, steps per second: 52, episode reward: 60.086, mean reward: 0.601 [0.521, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.371, 10.143], loss: 0.001496, mae: 0.042172, mean_q: 1.168442
 935027/1000000: episode: 9351, duration: 1.449s, episode steps: 100, steps per second: 69, episode reward: 57.222, mean reward: 0.572 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.476, 10.098], loss: 0.001425, mae: 0.040605, mean_q: 1.168148
 935127/1000000: episode: 9352, duration: 1.632s, episode steps: 100, steps per second: 61, episode reward: 56.442, mean reward: 0.564 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.947, 10.098], loss: 0.001348, mae: 0.039731, mean_q: 1.172568
 935227/1000000: episode: 9353, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 58.122, mean reward: 0.581 [0.510, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.175, 10.098], loss: 0.001401, mae: 0.041328, mean_q: 1.169007
 935327/1000000: episode: 9354, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 57.979, mean reward: 0.580 [0.510, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.985, 10.098], loss: 0.001371, mae: 0.040503, mean_q: 1.171887
 935427/1000000: episode: 9355, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 58.817, mean reward: 0.588 [0.504, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.658, 10.098], loss: 0.001434, mae: 0.041054, mean_q: 1.168068
 935527/1000000: episode: 9356, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 58.252, mean reward: 0.583 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.098], loss: 0.001373, mae: 0.040344, mean_q: 1.166627
 935627/1000000: episode: 9357, duration: 1.212s, episode steps: 100, steps per second: 82, episode reward: 58.332, mean reward: 0.583 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.577, 10.098], loss: 0.001476, mae: 0.041913, mean_q: 1.167438
 935727/1000000: episode: 9358, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 58.989, mean reward: 0.590 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.430, 10.098], loss: 0.001460, mae: 0.041639, mean_q: 1.167333
 935827/1000000: episode: 9359, duration: 1.442s, episode steps: 100, steps per second: 69, episode reward: 58.652, mean reward: 0.587 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.122, 10.098], loss: 0.001440, mae: 0.041115, mean_q: 1.165905
 935927/1000000: episode: 9360, duration: 1.630s, episode steps: 100, steps per second: 61, episode reward: 58.592, mean reward: 0.586 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.121, 10.098], loss: 0.001451, mae: 0.040572, mean_q: 1.168056
 936027/1000000: episode: 9361, duration: 1.672s, episode steps: 100, steps per second: 60, episode reward: 59.751, mean reward: 0.598 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.904, 10.098], loss: 0.001443, mae: 0.041508, mean_q: 1.168103
 936127/1000000: episode: 9362, duration: 1.525s, episode steps: 100, steps per second: 66, episode reward: 59.207, mean reward: 0.592 [0.497, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.115, 10.295], loss: 0.001356, mae: 0.040320, mean_q: 1.166526
 936227/1000000: episode: 9363, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 57.924, mean reward: 0.579 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.494, 10.282], loss: 0.001392, mae: 0.040845, mean_q: 1.165019
 936327/1000000: episode: 9364, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 58.707, mean reward: 0.587 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.310, 10.098], loss: 0.001339, mae: 0.039912, mean_q: 1.164108
 936427/1000000: episode: 9365, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 58.553, mean reward: 0.586 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.098], loss: 0.001349, mae: 0.039576, mean_q: 1.162245
 936527/1000000: episode: 9366, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 58.351, mean reward: 0.584 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.108, 10.098], loss: 0.001450, mae: 0.041258, mean_q: 1.163880
 936627/1000000: episode: 9367, duration: 1.099s, episode steps: 100, steps per second: 91, episode reward: 59.267, mean reward: 0.593 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.896, 10.098], loss: 0.001389, mae: 0.041103, mean_q: 1.162349
 936727/1000000: episode: 9368, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.647, mean reward: 0.586 [0.498, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.572, 10.408], loss: 0.001392, mae: 0.041016, mean_q: 1.161098
 936827/1000000: episode: 9369, duration: 1.263s, episode steps: 100, steps per second: 79, episode reward: 58.328, mean reward: 0.583 [0.505, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.289, 10.098], loss: 0.001423, mae: 0.041095, mean_q: 1.162268
 936927/1000000: episode: 9370, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 61.625, mean reward: 0.616 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.110, 10.496], loss: 0.001413, mae: 0.041090, mean_q: 1.159380
 937027/1000000: episode: 9371, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 61.669, mean reward: 0.617 [0.518, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.417, 10.098], loss: 0.001375, mae: 0.040383, mean_q: 1.158511
 937127/1000000: episode: 9372, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 58.382, mean reward: 0.584 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.206, 10.098], loss: 0.001400, mae: 0.040801, mean_q: 1.167597
 937227/1000000: episode: 9373, duration: 1.016s, episode steps: 100, steps per second: 98, episode reward: 57.545, mean reward: 0.575 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.761, 10.098], loss: 0.001396, mae: 0.040042, mean_q: 1.160987
 937327/1000000: episode: 9374, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 58.469, mean reward: 0.585 [0.505, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.837, 10.341], loss: 0.001374, mae: 0.040170, mean_q: 1.163040
 937427/1000000: episode: 9375, duration: 1.205s, episode steps: 100, steps per second: 83, episode reward: 58.936, mean reward: 0.589 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.098], loss: 0.001366, mae: 0.040315, mean_q: 1.160997
 937527/1000000: episode: 9376, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 57.447, mean reward: 0.574 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.961, 10.098], loss: 0.001475, mae: 0.041865, mean_q: 1.161887
 937627/1000000: episode: 9377, duration: 1.138s, episode steps: 100, steps per second: 88, episode reward: 58.053, mean reward: 0.581 [0.513, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.242, 10.228], loss: 0.001405, mae: 0.040446, mean_q: 1.158683
 937727/1000000: episode: 9378, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 59.438, mean reward: 0.594 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.399, 10.166], loss: 0.001438, mae: 0.041104, mean_q: 1.160483
 937827/1000000: episode: 9379, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 58.850, mean reward: 0.588 [0.517, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.642, 10.098], loss: 0.001310, mae: 0.038654, mean_q: 1.162300
 937927/1000000: episode: 9380, duration: 1.106s, episode steps: 100, steps per second: 90, episode reward: 59.216, mean reward: 0.592 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.061, 10.311], loss: 0.001485, mae: 0.041851, mean_q: 1.160399
 938027/1000000: episode: 9381, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 60.385, mean reward: 0.604 [0.504, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.058, 10.098], loss: 0.001404, mae: 0.040242, mean_q: 1.161184
 938127/1000000: episode: 9382, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 60.574, mean reward: 0.606 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.147], loss: 0.001479, mae: 0.041496, mean_q: 1.163805
 938227/1000000: episode: 9383, duration: 1.206s, episode steps: 100, steps per second: 83, episode reward: 61.720, mean reward: 0.617 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.771, 10.098], loss: 0.001439, mae: 0.040850, mean_q: 1.165928
 938327/1000000: episode: 9384, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 58.122, mean reward: 0.581 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.163], loss: 0.001445, mae: 0.041256, mean_q: 1.170714
 938427/1000000: episode: 9385, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 59.076, mean reward: 0.591 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.212, 10.134], loss: 0.001442, mae: 0.041198, mean_q: 1.167436
 938527/1000000: episode: 9386, duration: 1.098s, episode steps: 100, steps per second: 91, episode reward: 60.483, mean reward: 0.605 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.752, 10.447], loss: 0.001364, mae: 0.040473, mean_q: 1.164819
 938627/1000000: episode: 9387, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 61.590, mean reward: 0.616 [0.512, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.480, 10.098], loss: 0.001454, mae: 0.041145, mean_q: 1.169899
 938727/1000000: episode: 9388, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 59.822, mean reward: 0.598 [0.510, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.671, 10.196], loss: 0.001480, mae: 0.041701, mean_q: 1.170590
 938827/1000000: episode: 9389, duration: 1.173s, episode steps: 100, steps per second: 85, episode reward: 57.172, mean reward: 0.572 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.910, 10.098], loss: 0.001399, mae: 0.040410, mean_q: 1.171014
 938927/1000000: episode: 9390, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 57.783, mean reward: 0.578 [0.520, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.186, 10.098], loss: 0.001414, mae: 0.041013, mean_q: 1.170733
 939027/1000000: episode: 9391, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 58.809, mean reward: 0.588 [0.498, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.129, 10.098], loss: 0.001381, mae: 0.040269, mean_q: 1.168810
 939127/1000000: episode: 9392, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.594, mean reward: 0.586 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.021, 10.193], loss: 0.001434, mae: 0.041175, mean_q: 1.166634
 939227/1000000: episode: 9393, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.944, mean reward: 0.589 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.544, 10.098], loss: 0.001460, mae: 0.041406, mean_q: 1.171577
 939327/1000000: episode: 9394, duration: 1.248s, episode steps: 100, steps per second: 80, episode reward: 58.010, mean reward: 0.580 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.868, 10.098], loss: 0.001459, mae: 0.041400, mean_q: 1.174710
 939427/1000000: episode: 9395, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 58.609, mean reward: 0.586 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.042, 10.098], loss: 0.001407, mae: 0.041015, mean_q: 1.171443
 939527/1000000: episode: 9396, duration: 1.227s, episode steps: 100, steps per second: 81, episode reward: 59.907, mean reward: 0.599 [0.514, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 0.001470, mae: 0.040979, mean_q: 1.171122
 939627/1000000: episode: 9397, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 60.811, mean reward: 0.608 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.798, 10.098], loss: 0.001451, mae: 0.041187, mean_q: 1.171090
 939727/1000000: episode: 9398, duration: 1.094s, episode steps: 100, steps per second: 91, episode reward: 59.800, mean reward: 0.598 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.788, 10.098], loss: 0.001472, mae: 0.041633, mean_q: 1.169851
 939827/1000000: episode: 9399, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 60.004, mean reward: 0.600 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.179, 10.404], loss: 0.001501, mae: 0.042253, mean_q: 1.169242
 939927/1000000: episode: 9400, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 57.252, mean reward: 0.573 [0.506, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.356, 10.179], loss: 0.001405, mae: 0.040351, mean_q: 1.162585
 940027/1000000: episode: 9401, duration: 1.022s, episode steps: 100, steps per second: 98, episode reward: 58.770, mean reward: 0.588 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.115], loss: 0.001438, mae: 0.040985, mean_q: 1.165032
 940127/1000000: episode: 9402, duration: 1.259s, episode steps: 100, steps per second: 79, episode reward: 57.448, mean reward: 0.574 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.036, 10.147], loss: 0.001414, mae: 0.040795, mean_q: 1.169607
 940227/1000000: episode: 9403, duration: 1.276s, episode steps: 100, steps per second: 78, episode reward: 57.565, mean reward: 0.576 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.541, 10.098], loss: 0.001379, mae: 0.040673, mean_q: 1.167501
 940327/1000000: episode: 9404, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 64.410, mean reward: 0.644 [0.501, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.297, 10.562], loss: 0.001439, mae: 0.041230, mean_q: 1.172015
 940427/1000000: episode: 9405, duration: 1.186s, episode steps: 100, steps per second: 84, episode reward: 59.617, mean reward: 0.596 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.274, 10.250], loss: 0.001471, mae: 0.041848, mean_q: 1.167804
 940527/1000000: episode: 9406, duration: 1.267s, episode steps: 100, steps per second: 79, episode reward: 56.026, mean reward: 0.560 [0.499, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.360, 10.098], loss: 0.001380, mae: 0.040297, mean_q: 1.165907
 940627/1000000: episode: 9407, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 57.167, mean reward: 0.572 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.124, 10.147], loss: 0.001409, mae: 0.040520, mean_q: 1.171652
 940727/1000000: episode: 9408, duration: 1.696s, episode steps: 100, steps per second: 59, episode reward: 58.823, mean reward: 0.588 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.447, 10.135], loss: 0.001367, mae: 0.040464, mean_q: 1.166499
 940827/1000000: episode: 9409, duration: 1.717s, episode steps: 100, steps per second: 58, episode reward: 59.542, mean reward: 0.595 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.098], loss: 0.001406, mae: 0.040999, mean_q: 1.169311
 940927/1000000: episode: 9410, duration: 1.803s, episode steps: 100, steps per second: 55, episode reward: 57.767, mean reward: 0.578 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.801, 10.216], loss: 0.001311, mae: 0.039330, mean_q: 1.167892
 941027/1000000: episode: 9411, duration: 1.558s, episode steps: 100, steps per second: 64, episode reward: 60.665, mean reward: 0.607 [0.510, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.991, 10.252], loss: 0.001416, mae: 0.040531, mean_q: 1.168096
 941127/1000000: episode: 9412, duration: 1.379s, episode steps: 100, steps per second: 72, episode reward: 58.422, mean reward: 0.584 [0.506, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.672, 10.098], loss: 0.001424, mae: 0.040665, mean_q: 1.168171
 941227/1000000: episode: 9413, duration: 1.251s, episode steps: 100, steps per second: 80, episode reward: 61.087, mean reward: 0.611 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.766, 10.322], loss: 0.001374, mae: 0.040245, mean_q: 1.167113
 941327/1000000: episode: 9414, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 57.468, mean reward: 0.575 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.273, 10.286], loss: 0.001376, mae: 0.039926, mean_q: 1.169244
 941427/1000000: episode: 9415, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 60.297, mean reward: 0.603 [0.503, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.143, 10.117], loss: 0.001421, mae: 0.040824, mean_q: 1.172043
 941527/1000000: episode: 9416, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 58.908, mean reward: 0.589 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.808, 10.098], loss: 0.001530, mae: 0.042041, mean_q: 1.170093
 941627/1000000: episode: 9417, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 60.089, mean reward: 0.601 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.098], loss: 0.001491, mae: 0.041652, mean_q: 1.172489
 941727/1000000: episode: 9418, duration: 1.290s, episode steps: 100, steps per second: 78, episode reward: 59.222, mean reward: 0.592 [0.508, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.479, 10.098], loss: 0.001480, mae: 0.041735, mean_q: 1.175572
 941827/1000000: episode: 9419, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 58.521, mean reward: 0.585 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.440, 10.100], loss: 0.001474, mae: 0.042221, mean_q: 1.175977
 941927/1000000: episode: 9420, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 58.107, mean reward: 0.581 [0.499, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.233], loss: 0.001463, mae: 0.040964, mean_q: 1.170631
 942027/1000000: episode: 9421, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 57.122, mean reward: 0.571 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.113, 10.324], loss: 0.001516, mae: 0.042136, mean_q: 1.167073
 942127/1000000: episode: 9422, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 58.177, mean reward: 0.582 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.269, 10.163], loss: 0.001467, mae: 0.041835, mean_q: 1.166543
 942227/1000000: episode: 9423, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 56.642, mean reward: 0.566 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.098], loss: 0.001500, mae: 0.042173, mean_q: 1.170771
 942327/1000000: episode: 9424, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 56.954, mean reward: 0.570 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.323, 10.098], loss: 0.001534, mae: 0.042234, mean_q: 1.166909
 942427/1000000: episode: 9425, duration: 1.310s, episode steps: 100, steps per second: 76, episode reward: 56.219, mean reward: 0.562 [0.501, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.417, 10.202], loss: 0.001387, mae: 0.040452, mean_q: 1.165088
 942527/1000000: episode: 9426, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 62.738, mean reward: 0.627 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.250, 10.348], loss: 0.001510, mae: 0.041430, mean_q: 1.167200
 942627/1000000: episode: 9427, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 60.547, mean reward: 0.605 [0.499, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.711, 10.584], loss: 0.001550, mae: 0.042475, mean_q: 1.168982
 942727/1000000: episode: 9428, duration: 1.131s, episode steps: 100, steps per second: 88, episode reward: 61.886, mean reward: 0.619 [0.518, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.108, 10.098], loss: 0.001398, mae: 0.040217, mean_q: 1.169525
 942827/1000000: episode: 9429, duration: 1.119s, episode steps: 100, steps per second: 89, episode reward: 55.743, mean reward: 0.557 [0.498, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.379, 10.098], loss: 0.001500, mae: 0.041817, mean_q: 1.166440
 942927/1000000: episode: 9430, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 61.644, mean reward: 0.616 [0.506, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.602, 10.266], loss: 0.001483, mae: 0.041912, mean_q: 1.168209
 943027/1000000: episode: 9431, duration: 1.358s, episode steps: 100, steps per second: 74, episode reward: 56.653, mean reward: 0.567 [0.501, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.913, 10.164], loss: 0.001403, mae: 0.040768, mean_q: 1.167931
 943127/1000000: episode: 9432, duration: 1.424s, episode steps: 100, steps per second: 70, episode reward: 62.261, mean reward: 0.623 [0.516, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.457], loss: 0.001466, mae: 0.041288, mean_q: 1.167390
 943227/1000000: episode: 9433, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 61.368, mean reward: 0.614 [0.514, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.833, 10.376], loss: 0.001449, mae: 0.041257, mean_q: 1.168259
 943327/1000000: episode: 9434, duration: 1.363s, episode steps: 100, steps per second: 73, episode reward: 58.842, mean reward: 0.588 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.903, 10.370], loss: 0.001412, mae: 0.040415, mean_q: 1.166326
 943427/1000000: episode: 9435, duration: 1.391s, episode steps: 100, steps per second: 72, episode reward: 61.926, mean reward: 0.619 [0.502, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.401, 10.264], loss: 0.001507, mae: 0.042114, mean_q: 1.172346
 943527/1000000: episode: 9436, duration: 1.636s, episode steps: 100, steps per second: 61, episode reward: 56.071, mean reward: 0.561 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.545, 10.098], loss: 0.001494, mae: 0.041616, mean_q: 1.171094
 943627/1000000: episode: 9437, duration: 1.553s, episode steps: 100, steps per second: 64, episode reward: 59.440, mean reward: 0.594 [0.500, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.381, 10.109], loss: 0.001536, mae: 0.041718, mean_q: 1.168350
 943727/1000000: episode: 9438, duration: 1.419s, episode steps: 100, steps per second: 70, episode reward: 58.617, mean reward: 0.586 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.413], loss: 0.001474, mae: 0.041191, mean_q: 1.164437
 943827/1000000: episode: 9439, duration: 1.400s, episode steps: 100, steps per second: 71, episode reward: 59.704, mean reward: 0.597 [0.513, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.965, 10.098], loss: 0.001513, mae: 0.042158, mean_q: 1.166795
 943927/1000000: episode: 9440, duration: 1.231s, episode steps: 100, steps per second: 81, episode reward: 59.315, mean reward: 0.593 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.098], loss: 0.001502, mae: 0.041594, mean_q: 1.168903
 944027/1000000: episode: 9441, duration: 1.607s, episode steps: 100, steps per second: 62, episode reward: 57.186, mean reward: 0.572 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.758, 10.098], loss: 0.001409, mae: 0.041100, mean_q: 1.166439
 944127/1000000: episode: 9442, duration: 1.875s, episode steps: 100, steps per second: 53, episode reward: 58.896, mean reward: 0.589 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.602, 10.222], loss: 0.001505, mae: 0.042369, mean_q: 1.172480
 944227/1000000: episode: 9443, duration: 1.714s, episode steps: 100, steps per second: 58, episode reward: 60.348, mean reward: 0.603 [0.513, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.645, 10.098], loss: 0.001486, mae: 0.041636, mean_q: 1.166040
 944327/1000000: episode: 9444, duration: 1.819s, episode steps: 100, steps per second: 55, episode reward: 57.579, mean reward: 0.576 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.408, 10.392], loss: 0.001423, mae: 0.041075, mean_q: 1.168694
 944427/1000000: episode: 9445, duration: 1.883s, episode steps: 100, steps per second: 53, episode reward: 59.762, mean reward: 0.598 [0.512, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.830, 10.098], loss: 0.001551, mae: 0.042100, mean_q: 1.170475
 944527/1000000: episode: 9446, duration: 1.680s, episode steps: 100, steps per second: 60, episode reward: 61.395, mean reward: 0.614 [0.513, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.544, 10.385], loss: 0.001517, mae: 0.042215, mean_q: 1.170145
 944627/1000000: episode: 9447, duration: 1.580s, episode steps: 100, steps per second: 63, episode reward: 59.432, mean reward: 0.594 [0.504, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.761, 10.241], loss: 0.001598, mae: 0.043035, mean_q: 1.167318
 944727/1000000: episode: 9448, duration: 1.676s, episode steps: 100, steps per second: 60, episode reward: 61.734, mean reward: 0.617 [0.510, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.412, 10.098], loss: 0.001531, mae: 0.042554, mean_q: 1.171466
 944827/1000000: episode: 9449, duration: 1.639s, episode steps: 100, steps per second: 61, episode reward: 60.166, mean reward: 0.602 [0.510, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.331, 10.098], loss: 0.001466, mae: 0.041088, mean_q: 1.172085
 944927/1000000: episode: 9450, duration: 1.879s, episode steps: 100, steps per second: 53, episode reward: 57.026, mean reward: 0.570 [0.498, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.154, 10.098], loss: 0.001466, mae: 0.041517, mean_q: 1.168733
 945027/1000000: episode: 9451, duration: 1.733s, episode steps: 100, steps per second: 58, episode reward: 61.143, mean reward: 0.611 [0.502, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.377, 10.140], loss: 0.001531, mae: 0.042174, mean_q: 1.165737
 945127/1000000: episode: 9452, duration: 1.699s, episode steps: 100, steps per second: 59, episode reward: 56.801, mean reward: 0.568 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.331, 10.098], loss: 0.001522, mae: 0.042031, mean_q: 1.168329
 945227/1000000: episode: 9453, duration: 1.553s, episode steps: 100, steps per second: 64, episode reward: 57.207, mean reward: 0.572 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.399, 10.243], loss: 0.001537, mae: 0.042164, mean_q: 1.169680
 945327/1000000: episode: 9454, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 60.680, mean reward: 0.607 [0.506, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.196, 10.133], loss: 0.001523, mae: 0.042016, mean_q: 1.167270
 945427/1000000: episode: 9455, duration: 1.285s, episode steps: 100, steps per second: 78, episode reward: 58.801, mean reward: 0.588 [0.517, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.842, 10.238], loss: 0.001602, mae: 0.043322, mean_q: 1.172753
 945527/1000000: episode: 9456, duration: 1.144s, episode steps: 100, steps per second: 87, episode reward: 58.063, mean reward: 0.581 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.404, 10.218], loss: 0.001674, mae: 0.043800, mean_q: 1.171591
 945627/1000000: episode: 9457, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 60.288, mean reward: 0.603 [0.525, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.354, 10.252], loss: 0.001568, mae: 0.042169, mean_q: 1.170705
 945727/1000000: episode: 9458, duration: 1.522s, episode steps: 100, steps per second: 66, episode reward: 63.007, mean reward: 0.630 [0.515, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.320, 10.441], loss: 0.001548, mae: 0.042777, mean_q: 1.171790
 945827/1000000: episode: 9459, duration: 1.846s, episode steps: 100, steps per second: 54, episode reward: 56.581, mean reward: 0.566 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.182, 10.098], loss: 0.001592, mae: 0.043182, mean_q: 1.171970
 945927/1000000: episode: 9460, duration: 1.582s, episode steps: 100, steps per second: 63, episode reward: 59.383, mean reward: 0.594 [0.513, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.338, 10.223], loss: 0.001560, mae: 0.042572, mean_q: 1.173896
 946027/1000000: episode: 9461, duration: 1.318s, episode steps: 100, steps per second: 76, episode reward: 58.277, mean reward: 0.583 [0.513, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.178], loss: 0.001532, mae: 0.042322, mean_q: 1.170149
 946127/1000000: episode: 9462, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 57.998, mean reward: 0.580 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.904, 10.110], loss: 0.001456, mae: 0.041606, mean_q: 1.167855
 946227/1000000: episode: 9463, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 58.512, mean reward: 0.585 [0.514, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.500, 10.098], loss: 0.001498, mae: 0.042446, mean_q: 1.169827
 946327/1000000: episode: 9464, duration: 1.245s, episode steps: 100, steps per second: 80, episode reward: 60.258, mean reward: 0.603 [0.516, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.297, 10.446], loss: 0.001468, mae: 0.040960, mean_q: 1.169790
 946427/1000000: episode: 9465, duration: 1.397s, episode steps: 100, steps per second: 72, episode reward: 59.727, mean reward: 0.597 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.286, 10.312], loss: 0.001553, mae: 0.043022, mean_q: 1.170202
 946527/1000000: episode: 9466, duration: 1.130s, episode steps: 100, steps per second: 89, episode reward: 57.642, mean reward: 0.576 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.432, 10.098], loss: 0.001472, mae: 0.042201, mean_q: 1.172332
 946627/1000000: episode: 9467, duration: 1.569s, episode steps: 100, steps per second: 64, episode reward: 57.927, mean reward: 0.579 [0.507, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.502, 10.098], loss: 0.001594, mae: 0.042962, mean_q: 1.168989
 946727/1000000: episode: 9468, duration: 1.363s, episode steps: 100, steps per second: 73, episode reward: 58.389, mean reward: 0.584 [0.507, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.320, 10.098], loss: 0.001511, mae: 0.042178, mean_q: 1.168037
 946827/1000000: episode: 9469, duration: 1.495s, episode steps: 100, steps per second: 67, episode reward: 57.680, mean reward: 0.577 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.336, 10.098], loss: 0.001478, mae: 0.041742, mean_q: 1.168830
 946927/1000000: episode: 9470, duration: 1.502s, episode steps: 100, steps per second: 67, episode reward: 57.974, mean reward: 0.580 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.260, 10.275], loss: 0.001424, mae: 0.040889, mean_q: 1.167359
 947027/1000000: episode: 9471, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 58.189, mean reward: 0.582 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.623, 10.138], loss: 0.001509, mae: 0.042096, mean_q: 1.167178
 947127/1000000: episode: 9472, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 57.669, mean reward: 0.577 [0.511, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.119], loss: 0.001504, mae: 0.041996, mean_q: 1.165280
 947227/1000000: episode: 9473, duration: 1.277s, episode steps: 100, steps per second: 78, episode reward: 57.551, mean reward: 0.576 [0.505, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.607, 10.138], loss: 0.001498, mae: 0.041796, mean_q: 1.169072
 947327/1000000: episode: 9474, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 58.785, mean reward: 0.588 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.162, 10.257], loss: 0.001499, mae: 0.041527, mean_q: 1.166996
 947427/1000000: episode: 9475, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 60.607, mean reward: 0.606 [0.520, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.528, 10.228], loss: 0.001623, mae: 0.043698, mean_q: 1.171081
 947527/1000000: episode: 9476, duration: 1.112s, episode steps: 100, steps per second: 90, episode reward: 58.401, mean reward: 0.584 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.746, 10.098], loss: 0.001601, mae: 0.043243, mean_q: 1.172542
 947627/1000000: episode: 9477, duration: 1.175s, episode steps: 100, steps per second: 85, episode reward: 59.359, mean reward: 0.594 [0.505, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.145, 10.311], loss: 0.001453, mae: 0.040955, mean_q: 1.168653
 947727/1000000: episode: 9478, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 58.456, mean reward: 0.585 [0.501, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.432], loss: 0.001506, mae: 0.041629, mean_q: 1.168867
 947827/1000000: episode: 9479, duration: 1.345s, episode steps: 100, steps per second: 74, episode reward: 57.177, mean reward: 0.572 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.806, 10.101], loss: 0.001573, mae: 0.042714, mean_q: 1.168405
 947927/1000000: episode: 9480, duration: 1.285s, episode steps: 100, steps per second: 78, episode reward: 60.855, mean reward: 0.609 [0.505, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.104, 10.184], loss: 0.001492, mae: 0.041377, mean_q: 1.165993
 948027/1000000: episode: 9481, duration: 1.379s, episode steps: 100, steps per second: 73, episode reward: 60.066, mean reward: 0.601 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.212, 10.098], loss: 0.001505, mae: 0.041763, mean_q: 1.168803
 948127/1000000: episode: 9482, duration: 1.385s, episode steps: 100, steps per second: 72, episode reward: 64.409, mean reward: 0.644 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.098], loss: 0.001511, mae: 0.042066, mean_q: 1.171157
 948227/1000000: episode: 9483, duration: 1.457s, episode steps: 100, steps per second: 69, episode reward: 58.398, mean reward: 0.584 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.866, 10.098], loss: 0.001508, mae: 0.041930, mean_q: 1.170606
 948327/1000000: episode: 9484, duration: 1.421s, episode steps: 100, steps per second: 70, episode reward: 57.700, mean reward: 0.577 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.098], loss: 0.001576, mae: 0.042275, mean_q: 1.169317
 948427/1000000: episode: 9485, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 57.539, mean reward: 0.575 [0.500, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.524, 10.098], loss: 0.001471, mae: 0.042061, mean_q: 1.166241
 948527/1000000: episode: 9486, duration: 1.389s, episode steps: 100, steps per second: 72, episode reward: 56.819, mean reward: 0.568 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.572, 10.135], loss: 0.001534, mae: 0.041807, mean_q: 1.167928
 948627/1000000: episode: 9487, duration: 1.325s, episode steps: 100, steps per second: 75, episode reward: 58.226, mean reward: 0.582 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.380, 10.228], loss: 0.001457, mae: 0.041197, mean_q: 1.166827
 948727/1000000: episode: 9488, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: 57.841, mean reward: 0.578 [0.505, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.580, 10.292], loss: 0.001437, mae: 0.040968, mean_q: 1.166806
 948827/1000000: episode: 9489, duration: 1.102s, episode steps: 100, steps per second: 91, episode reward: 61.174, mean reward: 0.612 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.759, 10.098], loss: 0.001518, mae: 0.041766, mean_q: 1.165879
 948927/1000000: episode: 9490, duration: 1.649s, episode steps: 100, steps per second: 61, episode reward: 57.352, mean reward: 0.574 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.003, 10.098], loss: 0.001542, mae: 0.042284, mean_q: 1.170661
 949027/1000000: episode: 9491, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: 61.096, mean reward: 0.611 [0.515, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.767, 10.321], loss: 0.001469, mae: 0.041842, mean_q: 1.168670
 949127/1000000: episode: 9492, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 58.473, mean reward: 0.585 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.941, 10.134], loss: 0.001463, mae: 0.041458, mean_q: 1.167645
 949227/1000000: episode: 9493, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 60.651, mean reward: 0.607 [0.510, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.465, 10.239], loss: 0.001526, mae: 0.042218, mean_q: 1.167442
 949327/1000000: episode: 9494, duration: 1.104s, episode steps: 100, steps per second: 91, episode reward: 62.166, mean reward: 0.622 [0.501, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.408, 10.098], loss: 0.001557, mae: 0.043301, mean_q: 1.170106
 949427/1000000: episode: 9495, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.197, mean reward: 0.582 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.647, 10.098], loss: 0.001483, mae: 0.041686, mean_q: 1.164402
 949527/1000000: episode: 9496, duration: 1.693s, episode steps: 100, steps per second: 59, episode reward: 57.669, mean reward: 0.577 [0.509, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.747, 10.117], loss: 0.001503, mae: 0.042149, mean_q: 1.171536
 949627/1000000: episode: 9497, duration: 1.619s, episode steps: 100, steps per second: 62, episode reward: 59.733, mean reward: 0.597 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.163, 10.098], loss: 0.001553, mae: 0.041861, mean_q: 1.170957
 949727/1000000: episode: 9498, duration: 1.667s, episode steps: 100, steps per second: 60, episode reward: 61.114, mean reward: 0.611 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.738, 10.098], loss: 0.001480, mae: 0.041788, mean_q: 1.167851
 949827/1000000: episode: 9499, duration: 1.714s, episode steps: 100, steps per second: 58, episode reward: 58.031, mean reward: 0.580 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.292], loss: 0.001501, mae: 0.042765, mean_q: 1.167705
 949927/1000000: episode: 9500, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 58.624, mean reward: 0.586 [0.497, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.962, 10.284], loss: 0.001525, mae: 0.042209, mean_q: 1.168051
 950027/1000000: episode: 9501, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 60.445, mean reward: 0.604 [0.509, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.044, 10.098], loss: 0.001447, mae: 0.040558, mean_q: 1.165157
 950127/1000000: episode: 9502, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 62.720, mean reward: 0.627 [0.510, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.992, 10.098], loss: 0.001432, mae: 0.041171, mean_q: 1.167370
 950227/1000000: episode: 9503, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 57.481, mean reward: 0.575 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.794, 10.400], loss: 0.001583, mae: 0.042411, mean_q: 1.169337
 950327/1000000: episode: 9504, duration: 1.026s, episode steps: 100, steps per second: 97, episode reward: 57.400, mean reward: 0.574 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.945, 10.109], loss: 0.001437, mae: 0.041019, mean_q: 1.170704
 950427/1000000: episode: 9505, duration: 1.264s, episode steps: 100, steps per second: 79, episode reward: 60.577, mean reward: 0.606 [0.515, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.240, 10.282], loss: 0.001613, mae: 0.043268, mean_q: 1.172789
 950527/1000000: episode: 9506, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 59.218, mean reward: 0.592 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.371, 10.242], loss: 0.001401, mae: 0.040607, mean_q: 1.167577
 950627/1000000: episode: 9507, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 58.433, mean reward: 0.584 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.486, 10.098], loss: 0.001492, mae: 0.041936, mean_q: 1.169951
 950727/1000000: episode: 9508, duration: 1.147s, episode steps: 100, steps per second: 87, episode reward: 57.151, mean reward: 0.572 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.348, 10.136], loss: 0.001476, mae: 0.041734, mean_q: 1.168533
 950827/1000000: episode: 9509, duration: 1.383s, episode steps: 100, steps per second: 72, episode reward: 57.466, mean reward: 0.575 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.359, 10.224], loss: 0.001507, mae: 0.041402, mean_q: 1.167398
 950927/1000000: episode: 9510, duration: 1.289s, episode steps: 100, steps per second: 78, episode reward: 58.579, mean reward: 0.586 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.724, 10.098], loss: 0.001458, mae: 0.041619, mean_q: 1.165660
 951027/1000000: episode: 9511, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 64.111, mean reward: 0.641 [0.505, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.273, 10.400], loss: 0.001419, mae: 0.041263, mean_q: 1.164323
 951127/1000000: episode: 9512, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 57.115, mean reward: 0.571 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.288, 10.300], loss: 0.001455, mae: 0.041217, mean_q: 1.170887
 951227/1000000: episode: 9513, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.935, mean reward: 0.589 [0.498, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.671, 10.098], loss: 0.001494, mae: 0.041940, mean_q: 1.165359
 951327/1000000: episode: 9514, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 59.113, mean reward: 0.591 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.825, 10.338], loss: 0.001529, mae: 0.042051, mean_q: 1.169827
 951427/1000000: episode: 9515, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 59.732, mean reward: 0.597 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.815, 10.230], loss: 0.001395, mae: 0.040350, mean_q: 1.165346
 951527/1000000: episode: 9516, duration: 1.246s, episode steps: 100, steps per second: 80, episode reward: 59.758, mean reward: 0.598 [0.510, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.599, 10.098], loss: 0.001561, mae: 0.042918, mean_q: 1.167790
 951627/1000000: episode: 9517, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 60.977, mean reward: 0.610 [0.500, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.427], loss: 0.001525, mae: 0.041515, mean_q: 1.165419
 951727/1000000: episode: 9518, duration: 1.383s, episode steps: 100, steps per second: 72, episode reward: 57.686, mean reward: 0.577 [0.509, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.899, 10.098], loss: 0.001644, mae: 0.044243, mean_q: 1.170816
 951827/1000000: episode: 9519, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 57.469, mean reward: 0.575 [0.503, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.964, 10.180], loss: 0.001490, mae: 0.041878, mean_q: 1.168883
 951927/1000000: episode: 9520, duration: 1.754s, episode steps: 100, steps per second: 57, episode reward: 62.522, mean reward: 0.625 [0.518, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.032, 10.098], loss: 0.001541, mae: 0.042184, mean_q: 1.171653
 952027/1000000: episode: 9521, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 60.880, mean reward: 0.609 [0.505, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.918, 10.098], loss: 0.001581, mae: 0.043475, mean_q: 1.175438
 952127/1000000: episode: 9522, duration: 1.049s, episode steps: 100, steps per second: 95, episode reward: 59.285, mean reward: 0.593 [0.500, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.904, 10.274], loss: 0.001503, mae: 0.041537, mean_q: 1.170040
 952227/1000000: episode: 9523, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 59.762, mean reward: 0.598 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.418, 10.450], loss: 0.001503, mae: 0.041689, mean_q: 1.168281
 952327/1000000: episode: 9524, duration: 1.089s, episode steps: 100, steps per second: 92, episode reward: 58.728, mean reward: 0.587 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.405, 10.327], loss: 0.001532, mae: 0.041754, mean_q: 1.177287
 952427/1000000: episode: 9525, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 58.404, mean reward: 0.584 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.406], loss: 0.001519, mae: 0.042384, mean_q: 1.171163
 952527/1000000: episode: 9526, duration: 1.752s, episode steps: 100, steps per second: 57, episode reward: 59.916, mean reward: 0.599 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.245, 10.186], loss: 0.001502, mae: 0.041955, mean_q: 1.174279
 952627/1000000: episode: 9527, duration: 1.636s, episode steps: 100, steps per second: 61, episode reward: 57.540, mean reward: 0.575 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.674, 10.098], loss: 0.001598, mae: 0.042889, mean_q: 1.173669
 952727/1000000: episode: 9528, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: 59.913, mean reward: 0.599 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.456], loss: 0.001567, mae: 0.042267, mean_q: 1.173486
 952827/1000000: episode: 9529, duration: 1.620s, episode steps: 100, steps per second: 62, episode reward: 59.849, mean reward: 0.598 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.014, 10.105], loss: 0.001440, mae: 0.040862, mean_q: 1.173871
 952927/1000000: episode: 9530, duration: 1.734s, episode steps: 100, steps per second: 58, episode reward: 61.707, mean reward: 0.617 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.825, 10.098], loss: 0.001543, mae: 0.042686, mean_q: 1.178246
 953027/1000000: episode: 9531, duration: 1.663s, episode steps: 100, steps per second: 60, episode reward: 58.833, mean reward: 0.588 [0.503, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.149, 10.098], loss: 0.001575, mae: 0.042567, mean_q: 1.172254
 953127/1000000: episode: 9532, duration: 1.355s, episode steps: 100, steps per second: 74, episode reward: 59.824, mean reward: 0.598 [0.513, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.296], loss: 0.001398, mae: 0.040556, mean_q: 1.171420
 953227/1000000: episode: 9533, duration: 1.405s, episode steps: 100, steps per second: 71, episode reward: 57.179, mean reward: 0.572 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.272, 10.098], loss: 0.001471, mae: 0.041290, mean_q: 1.169265
 953327/1000000: episode: 9534, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 57.477, mean reward: 0.575 [0.508, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.707, 10.098], loss: 0.001521, mae: 0.042366, mean_q: 1.173593
 953427/1000000: episode: 9535, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 58.152, mean reward: 0.582 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.384, 10.098], loss: 0.001465, mae: 0.041554, mean_q: 1.171210
 953527/1000000: episode: 9536, duration: 1.212s, episode steps: 100, steps per second: 83, episode reward: 56.373, mean reward: 0.564 [0.500, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.837, 10.237], loss: 0.001507, mae: 0.041852, mean_q: 1.172207
 953627/1000000: episode: 9537, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 59.963, mean reward: 0.600 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.425, 10.098], loss: 0.001482, mae: 0.042037, mean_q: 1.174971
 953727/1000000: episode: 9538, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 58.637, mean reward: 0.586 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.223, 10.098], loss: 0.001480, mae: 0.041706, mean_q: 1.175495
 953827/1000000: episode: 9539, duration: 1.053s, episode steps: 100, steps per second: 95, episode reward: 59.207, mean reward: 0.592 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.098], loss: 0.001615, mae: 0.043279, mean_q: 1.176171
 953927/1000000: episode: 9540, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 59.761, mean reward: 0.598 [0.518, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.622, 10.201], loss: 0.001482, mae: 0.041326, mean_q: 1.173139
 954027/1000000: episode: 9541, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 58.949, mean reward: 0.589 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.059, 10.098], loss: 0.001611, mae: 0.043597, mean_q: 1.173995
 954127/1000000: episode: 9542, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 59.879, mean reward: 0.599 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.387, 10.173], loss: 0.001578, mae: 0.042874, mean_q: 1.176358
 954227/1000000: episode: 9543, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 59.207, mean reward: 0.592 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.994, 10.148], loss: 0.001551, mae: 0.042433, mean_q: 1.172005
 954327/1000000: episode: 9544, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 56.999, mean reward: 0.570 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.471, 10.098], loss: 0.001565, mae: 0.042849, mean_q: 1.176089
 954427/1000000: episode: 9545, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 58.777, mean reward: 0.588 [0.520, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.389, 10.171], loss: 0.001652, mae: 0.043217, mean_q: 1.170231
 954527/1000000: episode: 9546, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 56.605, mean reward: 0.566 [0.502, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.098], loss: 0.001529, mae: 0.042444, mean_q: 1.171965
 954627/1000000: episode: 9547, duration: 1.508s, episode steps: 100, steps per second: 66, episode reward: 56.529, mean reward: 0.565 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.182, 10.098], loss: 0.001442, mae: 0.041087, mean_q: 1.165399
 954727/1000000: episode: 9548, duration: 1.651s, episode steps: 100, steps per second: 61, episode reward: 58.306, mean reward: 0.583 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.925, 10.098], loss: 0.001377, mae: 0.040321, mean_q: 1.166978
 954827/1000000: episode: 9549, duration: 1.554s, episode steps: 100, steps per second: 64, episode reward: 58.379, mean reward: 0.584 [0.501, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.353, 10.254], loss: 0.001456, mae: 0.041264, mean_q: 1.167210
 954927/1000000: episode: 9550, duration: 1.686s, episode steps: 100, steps per second: 59, episode reward: 57.559, mean reward: 0.576 [0.508, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.803, 10.178], loss: 0.001464, mae: 0.041341, mean_q: 1.167396
 955027/1000000: episode: 9551, duration: 1.382s, episode steps: 100, steps per second: 72, episode reward: 57.933, mean reward: 0.579 [0.511, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.287, 10.098], loss: 0.001464, mae: 0.041715, mean_q: 1.169380
 955127/1000000: episode: 9552, duration: 1.679s, episode steps: 100, steps per second: 60, episode reward: 59.054, mean reward: 0.591 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.834, 10.098], loss: 0.001618, mae: 0.043322, mean_q: 1.166000
 955227/1000000: episode: 9553, duration: 1.714s, episode steps: 100, steps per second: 58, episode reward: 59.299, mean reward: 0.593 [0.504, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.825, 10.098], loss: 0.001547, mae: 0.042127, mean_q: 1.167563
 955327/1000000: episode: 9554, duration: 1.582s, episode steps: 100, steps per second: 63, episode reward: 62.382, mean reward: 0.624 [0.501, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.098], loss: 0.001479, mae: 0.041135, mean_q: 1.166288
 955427/1000000: episode: 9555, duration: 1.325s, episode steps: 100, steps per second: 75, episode reward: 58.831, mean reward: 0.588 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.588, 10.230], loss: 0.001470, mae: 0.041481, mean_q: 1.169363
 955527/1000000: episode: 9556, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 59.577, mean reward: 0.596 [0.513, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.006, 10.208], loss: 0.001464, mae: 0.041953, mean_q: 1.166011
 955627/1000000: episode: 9557, duration: 1.350s, episode steps: 100, steps per second: 74, episode reward: 60.798, mean reward: 0.608 [0.511, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.978, 10.098], loss: 0.001509, mae: 0.041586, mean_q: 1.166044
 955727/1000000: episode: 9558, duration: 1.484s, episode steps: 100, steps per second: 67, episode reward: 61.274, mean reward: 0.613 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.412, 10.146], loss: 0.001431, mae: 0.041246, mean_q: 1.168541
 955827/1000000: episode: 9559, duration: 1.804s, episode steps: 100, steps per second: 55, episode reward: 57.389, mean reward: 0.574 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.484, 10.098], loss: 0.001460, mae: 0.041511, mean_q: 1.167651
 955927/1000000: episode: 9560, duration: 1.624s, episode steps: 100, steps per second: 62, episode reward: 64.020, mean reward: 0.640 [0.523, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.869, 10.323], loss: 0.001407, mae: 0.040771, mean_q: 1.170470
 956027/1000000: episode: 9561, duration: 1.562s, episode steps: 100, steps per second: 64, episode reward: 58.773, mean reward: 0.588 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.183, 10.218], loss: 0.001452, mae: 0.041349, mean_q: 1.170346
 956127/1000000: episode: 9562, duration: 1.416s, episode steps: 100, steps per second: 71, episode reward: 60.624, mean reward: 0.606 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.580, 10.253], loss: 0.001460, mae: 0.041598, mean_q: 1.170886
 956227/1000000: episode: 9563, duration: 1.835s, episode steps: 100, steps per second: 54, episode reward: 57.306, mean reward: 0.573 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.286, 10.120], loss: 0.001437, mae: 0.041117, mean_q: 1.171338
 956327/1000000: episode: 9564, duration: 1.677s, episode steps: 100, steps per second: 60, episode reward: 59.802, mean reward: 0.598 [0.505, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.347, 10.191], loss: 0.001525, mae: 0.042414, mean_q: 1.174013
 956427/1000000: episode: 9565, duration: 1.548s, episode steps: 100, steps per second: 65, episode reward: 56.849, mean reward: 0.568 [0.508, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.107], loss: 0.001469, mae: 0.041624, mean_q: 1.168726
 956527/1000000: episode: 9566, duration: 1.020s, episode steps: 100, steps per second: 98, episode reward: 59.036, mean reward: 0.590 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.653, 10.389], loss: 0.001444, mae: 0.041492, mean_q: 1.170533
 956627/1000000: episode: 9567, duration: 1.046s, episode steps: 100, steps per second: 96, episode reward: 57.458, mean reward: 0.575 [0.507, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.229, 10.098], loss: 0.001486, mae: 0.041551, mean_q: 1.171857
 956727/1000000: episode: 9568, duration: 1.136s, episode steps: 100, steps per second: 88, episode reward: 58.620, mean reward: 0.586 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.359, 10.133], loss: 0.001439, mae: 0.040954, mean_q: 1.167090
 956827/1000000: episode: 9569, duration: 1.212s, episode steps: 100, steps per second: 83, episode reward: 58.819, mean reward: 0.588 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.509, 10.098], loss: 0.001294, mae: 0.039158, mean_q: 1.168620
 956927/1000000: episode: 9570, duration: 1.421s, episode steps: 100, steps per second: 70, episode reward: 57.874, mean reward: 0.579 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.190, 10.098], loss: 0.001433, mae: 0.041060, mean_q: 1.165019
 957027/1000000: episode: 9571, duration: 1.672s, episode steps: 100, steps per second: 60, episode reward: 63.272, mean reward: 0.633 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.711, 10.098], loss: 0.001435, mae: 0.041649, mean_q: 1.162337
 957127/1000000: episode: 9572, duration: 1.518s, episode steps: 100, steps per second: 66, episode reward: 58.205, mean reward: 0.582 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.648, 10.282], loss: 0.001307, mae: 0.039950, mean_q: 1.164615
 957227/1000000: episode: 9573, duration: 1.540s, episode steps: 100, steps per second: 65, episode reward: 60.013, mean reward: 0.600 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.501, 10.230], loss: 0.001439, mae: 0.041241, mean_q: 1.165620
 957327/1000000: episode: 9574, duration: 1.484s, episode steps: 100, steps per second: 67, episode reward: 59.142, mean reward: 0.591 [0.509, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.835, 10.098], loss: 0.001414, mae: 0.041368, mean_q: 1.173087
 957427/1000000: episode: 9575, duration: 1.184s, episode steps: 100, steps per second: 84, episode reward: 59.956, mean reward: 0.600 [0.522, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.488, 10.100], loss: 0.001358, mae: 0.040294, mean_q: 1.171728
 957527/1000000: episode: 9576, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 59.135, mean reward: 0.591 [0.515, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.078, 10.184], loss: 0.001414, mae: 0.041026, mean_q: 1.173377
 957627/1000000: episode: 9577, duration: 1.169s, episode steps: 100, steps per second: 86, episode reward: 58.944, mean reward: 0.589 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.827, 10.098], loss: 0.001447, mae: 0.041581, mean_q: 1.172207
 957727/1000000: episode: 9578, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 58.275, mean reward: 0.583 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.213, 10.098], loss: 0.001275, mae: 0.039589, mean_q: 1.171677
 957827/1000000: episode: 9579, duration: 1.500s, episode steps: 100, steps per second: 67, episode reward: 58.661, mean reward: 0.587 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.918, 10.113], loss: 0.001351, mae: 0.040037, mean_q: 1.165509
 957927/1000000: episode: 9580, duration: 1.625s, episode steps: 100, steps per second: 62, episode reward: 58.327, mean reward: 0.583 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.120, 10.161], loss: 0.001389, mae: 0.040970, mean_q: 1.172262
 958027/1000000: episode: 9581, duration: 1.633s, episode steps: 100, steps per second: 61, episode reward: 57.981, mean reward: 0.580 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.991, 10.244], loss: 0.001423, mae: 0.041115, mean_q: 1.165527
 958127/1000000: episode: 9582, duration: 1.388s, episode steps: 100, steps per second: 72, episode reward: 58.039, mean reward: 0.580 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.062, 10.222], loss: 0.001507, mae: 0.042120, mean_q: 1.166634
 958227/1000000: episode: 9583, duration: 1.744s, episode steps: 100, steps per second: 57, episode reward: 58.019, mean reward: 0.580 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.474, 10.173], loss: 0.001469, mae: 0.041414, mean_q: 1.164946
 958327/1000000: episode: 9584, duration: 1.374s, episode steps: 100, steps per second: 73, episode reward: 59.984, mean reward: 0.600 [0.513, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.078, 10.098], loss: 0.001463, mae: 0.041290, mean_q: 1.170571
 958427/1000000: episode: 9585, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 59.979, mean reward: 0.600 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.184, 10.359], loss: 0.001488, mae: 0.041821, mean_q: 1.168176
 958527/1000000: episode: 9586, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 58.107, mean reward: 0.581 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.688, 10.253], loss: 0.001521, mae: 0.042336, mean_q: 1.169677
 958627/1000000: episode: 9587, duration: 1.200s, episode steps: 100, steps per second: 83, episode reward: 58.281, mean reward: 0.583 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.107, 10.098], loss: 0.001352, mae: 0.040004, mean_q: 1.169973
 958727/1000000: episode: 9588, duration: 1.375s, episode steps: 100, steps per second: 73, episode reward: 57.986, mean reward: 0.580 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.751, 10.345], loss: 0.001352, mae: 0.040545, mean_q: 1.168147
 958827/1000000: episode: 9589, duration: 1.379s, episode steps: 100, steps per second: 73, episode reward: 57.942, mean reward: 0.579 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.032, 10.176], loss: 0.001474, mae: 0.041538, mean_q: 1.170493
 958927/1000000: episode: 9590, duration: 1.355s, episode steps: 100, steps per second: 74, episode reward: 61.232, mean reward: 0.612 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.355, 10.098], loss: 0.001449, mae: 0.041197, mean_q: 1.164957
 959027/1000000: episode: 9591, duration: 1.274s, episode steps: 100, steps per second: 79, episode reward: 59.417, mean reward: 0.594 [0.502, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.288], loss: 0.001428, mae: 0.041258, mean_q: 1.166395
 959127/1000000: episode: 9592, duration: 1.260s, episode steps: 100, steps per second: 79, episode reward: 57.463, mean reward: 0.575 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.589, 10.098], loss: 0.001510, mae: 0.041819, mean_q: 1.169607
 959227/1000000: episode: 9593, duration: 1.199s, episode steps: 100, steps per second: 83, episode reward: 60.246, mean reward: 0.602 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.352, 10.098], loss: 0.001486, mae: 0.041845, mean_q: 1.165945
 959327/1000000: episode: 9594, duration: 1.297s, episode steps: 100, steps per second: 77, episode reward: 57.953, mean reward: 0.580 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.137], loss: 0.001392, mae: 0.040495, mean_q: 1.166605
 959427/1000000: episode: 9595, duration: 1.473s, episode steps: 100, steps per second: 68, episode reward: 59.588, mean reward: 0.596 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.221], loss: 0.001383, mae: 0.040592, mean_q: 1.165618
 959527/1000000: episode: 9596, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 59.185, mean reward: 0.592 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.664, 10.098], loss: 0.001412, mae: 0.041277, mean_q: 1.166869
 959627/1000000: episode: 9597, duration: 1.296s, episode steps: 100, steps per second: 77, episode reward: 57.271, mean reward: 0.573 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.683, 10.098], loss: 0.001349, mae: 0.039799, mean_q: 1.166907
 959727/1000000: episode: 9598, duration: 1.292s, episode steps: 100, steps per second: 77, episode reward: 57.712, mean reward: 0.577 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.807, 10.098], loss: 0.001425, mae: 0.041412, mean_q: 1.165517
 959827/1000000: episode: 9599, duration: 1.482s, episode steps: 100, steps per second: 67, episode reward: 57.411, mean reward: 0.574 [0.502, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.260, 10.098], loss: 0.001388, mae: 0.040534, mean_q: 1.164836
 959927/1000000: episode: 9600, duration: 1.748s, episode steps: 100, steps per second: 57, episode reward: 59.383, mean reward: 0.594 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.297, 10.246], loss: 0.001459, mae: 0.041638, mean_q: 1.169712
 960027/1000000: episode: 9601, duration: 1.475s, episode steps: 100, steps per second: 68, episode reward: 57.456, mean reward: 0.575 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.959, 10.228], loss: 0.001517, mae: 0.042107, mean_q: 1.170345
 960127/1000000: episode: 9602, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 65.722, mean reward: 0.657 [0.510, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.008, 10.323], loss: 0.001446, mae: 0.041504, mean_q: 1.166983
 960227/1000000: episode: 9603, duration: 1.334s, episode steps: 100, steps per second: 75, episode reward: 59.365, mean reward: 0.594 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.224, 10.330], loss: 0.001534, mae: 0.042611, mean_q: 1.168671
 960327/1000000: episode: 9604, duration: 1.407s, episode steps: 100, steps per second: 71, episode reward: 61.078, mean reward: 0.611 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.313, 10.339], loss: 0.001524, mae: 0.042253, mean_q: 1.172077
 960427/1000000: episode: 9605, duration: 1.439s, episode steps: 100, steps per second: 70, episode reward: 58.438, mean reward: 0.584 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.256], loss: 0.001468, mae: 0.041786, mean_q: 1.169386
 960527/1000000: episode: 9606, duration: 1.103s, episode steps: 100, steps per second: 91, episode reward: 59.551, mean reward: 0.596 [0.509, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.746, 10.435], loss: 0.001464, mae: 0.041432, mean_q: 1.169708
 960627/1000000: episode: 9607, duration: 1.044s, episode steps: 100, steps per second: 96, episode reward: 58.221, mean reward: 0.582 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.679, 10.098], loss: 0.001495, mae: 0.042094, mean_q: 1.170759
 960727/1000000: episode: 9608, duration: 1.202s, episode steps: 100, steps per second: 83, episode reward: 57.616, mean reward: 0.576 [0.500, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.131], loss: 0.001501, mae: 0.042236, mean_q: 1.167298
 960827/1000000: episode: 9609, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 58.435, mean reward: 0.584 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.951, 10.345], loss: 0.001486, mae: 0.042066, mean_q: 1.167880
 960927/1000000: episode: 9610, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 57.890, mean reward: 0.579 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.186, 10.098], loss: 0.001495, mae: 0.042334, mean_q: 1.165614
 961027/1000000: episode: 9611, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 59.450, mean reward: 0.595 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.098], loss: 0.001404, mae: 0.040610, mean_q: 1.165556
 961127/1000000: episode: 9612, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 59.688, mean reward: 0.597 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.961, 10.098], loss: 0.001350, mae: 0.039822, mean_q: 1.165620
 961227/1000000: episode: 9613, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 61.881, mean reward: 0.619 [0.511, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.095, 10.543], loss: 0.001355, mae: 0.040146, mean_q: 1.166168
 961327/1000000: episode: 9614, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 57.975, mean reward: 0.580 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.567, 10.098], loss: 0.001363, mae: 0.040219, mean_q: 1.167437
 961427/1000000: episode: 9615, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 57.098, mean reward: 0.571 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.553, 10.143], loss: 0.001411, mae: 0.040973, mean_q: 1.166525
 961527/1000000: episode: 9616, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 57.553, mean reward: 0.576 [0.508, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.503, 10.098], loss: 0.001440, mae: 0.041007, mean_q: 1.164682
 961627/1000000: episode: 9617, duration: 1.026s, episode steps: 100, steps per second: 98, episode reward: 58.267, mean reward: 0.583 [0.512, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.178, 10.101], loss: 0.001308, mae: 0.039973, mean_q: 1.164642
 961727/1000000: episode: 9618, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 58.693, mean reward: 0.587 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.891, 10.103], loss: 0.001408, mae: 0.040667, mean_q: 1.167595
 961827/1000000: episode: 9619, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 61.614, mean reward: 0.616 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.005, 10.459], loss: 0.001434, mae: 0.040782, mean_q: 1.166601
 961927/1000000: episode: 9620, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 58.226, mean reward: 0.582 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.237, 10.098], loss: 0.001489, mae: 0.041568, mean_q: 1.173014
 962027/1000000: episode: 9621, duration: 1.212s, episode steps: 100, steps per second: 83, episode reward: 60.587, mean reward: 0.606 [0.513, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.922, 10.098], loss: 0.001319, mae: 0.039559, mean_q: 1.163423
 962127/1000000: episode: 9622, duration: 1.034s, episode steps: 100, steps per second: 97, episode reward: 58.152, mean reward: 0.582 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.511, 10.098], loss: 0.001418, mae: 0.040578, mean_q: 1.167146
 962227/1000000: episode: 9623, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 58.421, mean reward: 0.584 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.137, 10.177], loss: 0.001438, mae: 0.040606, mean_q: 1.166651
 962327/1000000: episode: 9624, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 60.502, mean reward: 0.605 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.710, 10.212], loss: 0.001460, mae: 0.041101, mean_q: 1.170056
 962427/1000000: episode: 9625, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 59.680, mean reward: 0.597 [0.516, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.191, 10.098], loss: 0.001472, mae: 0.041486, mean_q: 1.165790
 962527/1000000: episode: 9626, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 59.041, mean reward: 0.590 [0.498, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.478, 10.403], loss: 0.001501, mae: 0.041864, mean_q: 1.167280
 962627/1000000: episode: 9627, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 57.812, mean reward: 0.578 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.759, 10.098], loss: 0.001487, mae: 0.041742, mean_q: 1.168541
 962727/1000000: episode: 9628, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.717, mean reward: 0.587 [0.504, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.556, 10.179], loss: 0.001542, mae: 0.042114, mean_q: 1.168037
 962827/1000000: episode: 9629, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 59.220, mean reward: 0.592 [0.499, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.541, 10.187], loss: 0.001438, mae: 0.040781, mean_q: 1.165607
 962927/1000000: episode: 9630, duration: 1.047s, episode steps: 100, steps per second: 95, episode reward: 56.989, mean reward: 0.570 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.635, 10.124], loss: 0.001421, mae: 0.040746, mean_q: 1.168335
 963027/1000000: episode: 9631, duration: 1.036s, episode steps: 100, steps per second: 96, episode reward: 62.168, mean reward: 0.622 [0.515, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.204, 10.151], loss: 0.001441, mae: 0.040832, mean_q: 1.169913
 963127/1000000: episode: 9632, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 58.527, mean reward: 0.585 [0.512, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.116], loss: 0.001446, mae: 0.041139, mean_q: 1.169426
 963227/1000000: episode: 9633, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 60.128, mean reward: 0.601 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.269, 10.247], loss: 0.001621, mae: 0.043223, mean_q: 1.169773
 963327/1000000: episode: 9634, duration: 1.110s, episode steps: 100, steps per second: 90, episode reward: 58.559, mean reward: 0.586 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.625, 10.098], loss: 0.001515, mae: 0.041411, mean_q: 1.169765
 963427/1000000: episode: 9635, duration: 1.693s, episode steps: 100, steps per second: 59, episode reward: 62.456, mean reward: 0.625 [0.506, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.794, 10.098], loss: 0.001460, mae: 0.041305, mean_q: 1.164381
 963527/1000000: episode: 9636, duration: 1.673s, episode steps: 100, steps per second: 60, episode reward: 63.928, mean reward: 0.639 [0.498, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.421], loss: 0.001457, mae: 0.040781, mean_q: 1.169631
 963627/1000000: episode: 9637, duration: 1.708s, episode steps: 100, steps per second: 59, episode reward: 58.111, mean reward: 0.581 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.460, 10.098], loss: 0.001479, mae: 0.041513, mean_q: 1.168781
 963727/1000000: episode: 9638, duration: 1.419s, episode steps: 100, steps per second: 70, episode reward: 63.569, mean reward: 0.636 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.023, 10.098], loss: 0.001429, mae: 0.040604, mean_q: 1.168604
 963827/1000000: episode: 9639, duration: 1.300s, episode steps: 100, steps per second: 77, episode reward: 59.086, mean reward: 0.591 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.717, 10.183], loss: 0.001437, mae: 0.040968, mean_q: 1.174179
 963927/1000000: episode: 9640, duration: 1.364s, episode steps: 100, steps per second: 73, episode reward: 58.703, mean reward: 0.587 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.814, 10.147], loss: 0.001465, mae: 0.041173, mean_q: 1.173497
 964027/1000000: episode: 9641, duration: 1.435s, episode steps: 100, steps per second: 70, episode reward: 59.051, mean reward: 0.591 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.898, 10.098], loss: 0.001522, mae: 0.042638, mean_q: 1.174450
 964127/1000000: episode: 9642, duration: 1.327s, episode steps: 100, steps per second: 75, episode reward: 58.122, mean reward: 0.581 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.329, 10.099], loss: 0.001539, mae: 0.042415, mean_q: 1.178291
 964227/1000000: episode: 9643, duration: 1.296s, episode steps: 100, steps per second: 77, episode reward: 60.072, mean reward: 0.601 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.102], loss: 0.001700, mae: 0.044219, mean_q: 1.176667
 964327/1000000: episode: 9644, duration: 1.335s, episode steps: 100, steps per second: 75, episode reward: 58.156, mean reward: 0.582 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.601, 10.192], loss: 0.001451, mae: 0.040798, mean_q: 1.172801
 964427/1000000: episode: 9645, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 58.750, mean reward: 0.588 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.418, 10.175], loss: 0.001459, mae: 0.041060, mean_q: 1.174360
 964527/1000000: episode: 9646, duration: 1.436s, episode steps: 100, steps per second: 70, episode reward: 58.562, mean reward: 0.586 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.091, 10.109], loss: 0.001469, mae: 0.041413, mean_q: 1.169829
 964627/1000000: episode: 9647, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 58.883, mean reward: 0.589 [0.504, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.134, 10.120], loss: 0.001507, mae: 0.041575, mean_q: 1.174490
 964727/1000000: episode: 9648, duration: 1.152s, episode steps: 100, steps per second: 87, episode reward: 58.838, mean reward: 0.588 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.292, 10.102], loss: 0.001468, mae: 0.041439, mean_q: 1.171590
 964827/1000000: episode: 9649, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 58.905, mean reward: 0.589 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.345, 10.215], loss: 0.001559, mae: 0.042695, mean_q: 1.171508
 964927/1000000: episode: 9650, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 59.189, mean reward: 0.592 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.179], loss: 0.001523, mae: 0.042009, mean_q: 1.175721
 965027/1000000: episode: 9651, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 60.873, mean reward: 0.609 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.285, 10.098], loss: 0.001467, mae: 0.041774, mean_q: 1.171261
 965127/1000000: episode: 9652, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.941, mean reward: 0.589 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.947, 10.098], loss: 0.001522, mae: 0.042074, mean_q: 1.175057
 965227/1000000: episode: 9653, duration: 1.116s, episode steps: 100, steps per second: 90, episode reward: 59.376, mean reward: 0.594 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.182, 10.098], loss: 0.001590, mae: 0.043021, mean_q: 1.174767
 965327/1000000: episode: 9654, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 58.381, mean reward: 0.584 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.287], loss: 0.001608, mae: 0.042928, mean_q: 1.177260
 965427/1000000: episode: 9655, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 56.929, mean reward: 0.569 [0.506, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.123], loss: 0.001540, mae: 0.042461, mean_q: 1.171029
 965527/1000000: episode: 9656, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 58.371, mean reward: 0.584 [0.506, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.985, 10.112], loss: 0.001509, mae: 0.042114, mean_q: 1.168698
 965627/1000000: episode: 9657, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 60.877, mean reward: 0.609 [0.528, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.624, 10.243], loss: 0.001445, mae: 0.041308, mean_q: 1.168850
 965727/1000000: episode: 9658, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 56.460, mean reward: 0.565 [0.502, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.098], loss: 0.001585, mae: 0.042592, mean_q: 1.173391
 965827/1000000: episode: 9659, duration: 1.213s, episode steps: 100, steps per second: 82, episode reward: 59.724, mean reward: 0.597 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.323, 10.098], loss: 0.001486, mae: 0.041987, mean_q: 1.171977
 965927/1000000: episode: 9660, duration: 1.125s, episode steps: 100, steps per second: 89, episode reward: 59.809, mean reward: 0.598 [0.505, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.694, 10.346], loss: 0.001508, mae: 0.041256, mean_q: 1.172954
 966027/1000000: episode: 9661, duration: 1.055s, episode steps: 100, steps per second: 95, episode reward: 61.953, mean reward: 0.620 [0.519, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.267, 10.499], loss: 0.001602, mae: 0.043458, mean_q: 1.174025
 966127/1000000: episode: 9662, duration: 1.056s, episode steps: 100, steps per second: 95, episode reward: 57.450, mean reward: 0.575 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.216], loss: 0.001498, mae: 0.040920, mean_q: 1.171763
 966227/1000000: episode: 9663, duration: 1.113s, episode steps: 100, steps per second: 90, episode reward: 61.815, mean reward: 0.618 [0.517, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.628, 10.172], loss: 0.001445, mae: 0.041177, mean_q: 1.175014
 966327/1000000: episode: 9664, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 56.972, mean reward: 0.570 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.818, 10.124], loss: 0.001488, mae: 0.041025, mean_q: 1.171848
 966427/1000000: episode: 9665, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 58.561, mean reward: 0.586 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.198, 10.114], loss: 0.001588, mae: 0.043365, mean_q: 1.173370
 966527/1000000: episode: 9666, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 66.343, mean reward: 0.663 [0.506, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.379, 10.098], loss: 0.001557, mae: 0.042649, mean_q: 1.173900
 966627/1000000: episode: 9667, duration: 1.365s, episode steps: 100, steps per second: 73, episode reward: 58.219, mean reward: 0.582 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.296, 10.280], loss: 0.001493, mae: 0.041339, mean_q: 1.178096
 966727/1000000: episode: 9668, duration: 1.311s, episode steps: 100, steps per second: 76, episode reward: 59.879, mean reward: 0.599 [0.514, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.581, 10.337], loss: 0.001548, mae: 0.042551, mean_q: 1.178214
 966827/1000000: episode: 9669, duration: 1.373s, episode steps: 100, steps per second: 73, episode reward: 58.125, mean reward: 0.581 [0.498, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.384, 10.098], loss: 0.001517, mae: 0.042114, mean_q: 1.175240
 966927/1000000: episode: 9670, duration: 1.568s, episode steps: 100, steps per second: 64, episode reward: 59.014, mean reward: 0.590 [0.498, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: 0.001568, mae: 0.042601, mean_q: 1.171268
 967027/1000000: episode: 9671, duration: 1.521s, episode steps: 100, steps per second: 66, episode reward: 58.230, mean reward: 0.582 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.568, 10.141], loss: 0.001542, mae: 0.042529, mean_q: 1.177046
 967127/1000000: episode: 9672, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 59.484, mean reward: 0.595 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.151], loss: 0.001644, mae: 0.043534, mean_q: 1.179542
 967227/1000000: episode: 9673, duration: 1.347s, episode steps: 100, steps per second: 74, episode reward: 58.593, mean reward: 0.586 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.683, 10.111], loss: 0.001535, mae: 0.042751, mean_q: 1.174585
 967327/1000000: episode: 9674, duration: 1.121s, episode steps: 100, steps per second: 89, episode reward: 56.480, mean reward: 0.565 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.200], loss: 0.001540, mae: 0.042360, mean_q: 1.176383
 967427/1000000: episode: 9675, duration: 1.191s, episode steps: 100, steps per second: 84, episode reward: 57.271, mean reward: 0.573 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.586, 10.279], loss: 0.001467, mae: 0.041605, mean_q: 1.171604
 967527/1000000: episode: 9676, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: 57.764, mean reward: 0.578 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.126], loss: 0.001504, mae: 0.042174, mean_q: 1.174908
 967627/1000000: episode: 9677, duration: 1.108s, episode steps: 100, steps per second: 90, episode reward: 57.376, mean reward: 0.574 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.201], loss: 0.001521, mae: 0.041361, mean_q: 1.169426
 967727/1000000: episode: 9678, duration: 1.488s, episode steps: 100, steps per second: 67, episode reward: 57.801, mean reward: 0.578 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.511, 10.098], loss: 0.001416, mae: 0.040824, mean_q: 1.168969
 967827/1000000: episode: 9679, duration: 1.457s, episode steps: 100, steps per second: 69, episode reward: 57.608, mean reward: 0.576 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.456, 10.130], loss: 0.001563, mae: 0.042473, mean_q: 1.170679
 967927/1000000: episode: 9680, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 59.000, mean reward: 0.590 [0.521, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.867, 10.098], loss: 0.001521, mae: 0.042016, mean_q: 1.171992
 968027/1000000: episode: 9681, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 60.855, mean reward: 0.609 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.098], loss: 0.001398, mae: 0.041071, mean_q: 1.172073
 968127/1000000: episode: 9682, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 59.655, mean reward: 0.597 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.260, 10.098], loss: 0.001475, mae: 0.041812, mean_q: 1.172150
 968227/1000000: episode: 9683, duration: 1.150s, episode steps: 100, steps per second: 87, episode reward: 57.454, mean reward: 0.575 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.098], loss: 0.001470, mae: 0.040882, mean_q: 1.168179
 968327/1000000: episode: 9684, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 60.150, mean reward: 0.602 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.154], loss: 0.001474, mae: 0.041621, mean_q: 1.168948
 968427/1000000: episode: 9685, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 58.695, mean reward: 0.587 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.338, 10.098], loss: 0.001495, mae: 0.041670, mean_q: 1.172281
 968527/1000000: episode: 9686, duration: 1.380s, episode steps: 100, steps per second: 72, episode reward: 58.917, mean reward: 0.589 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.783, 10.098], loss: 0.001472, mae: 0.041409, mean_q: 1.169447
 968627/1000000: episode: 9687, duration: 1.156s, episode steps: 100, steps per second: 87, episode reward: 61.182, mean reward: 0.612 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.592, 10.283], loss: 0.001390, mae: 0.040502, mean_q: 1.168330
 968727/1000000: episode: 9688, duration: 1.154s, episode steps: 100, steps per second: 87, episode reward: 58.114, mean reward: 0.581 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.549, 10.113], loss: 0.001348, mae: 0.039848, mean_q: 1.165730
 968827/1000000: episode: 9689, duration: 1.156s, episode steps: 100, steps per second: 87, episode reward: 57.500, mean reward: 0.575 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.865, 10.098], loss: 0.001485, mae: 0.041279, mean_q: 1.166219
 968927/1000000: episode: 9690, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 57.523, mean reward: 0.575 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.383, 10.102], loss: 0.001448, mae: 0.040550, mean_q: 1.165907
 969027/1000000: episode: 9691, duration: 1.159s, episode steps: 100, steps per second: 86, episode reward: 63.030, mean reward: 0.630 [0.519, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.161, 10.368], loss: 0.001351, mae: 0.040060, mean_q: 1.167443
 969127/1000000: episode: 9692, duration: 1.445s, episode steps: 100, steps per second: 69, episode reward: 57.747, mean reward: 0.577 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.136, 10.098], loss: 0.001433, mae: 0.040949, mean_q: 1.166868
 969227/1000000: episode: 9693, duration: 1.151s, episode steps: 100, steps per second: 87, episode reward: 57.955, mean reward: 0.580 [0.499, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.349, 10.186], loss: 0.001446, mae: 0.041250, mean_q: 1.167075
 969327/1000000: episode: 9694, duration: 1.216s, episode steps: 100, steps per second: 82, episode reward: 61.355, mean reward: 0.614 [0.520, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.699, 10.282], loss: 0.001488, mae: 0.041548, mean_q: 1.168430
 969427/1000000: episode: 9695, duration: 1.145s, episode steps: 100, steps per second: 87, episode reward: 59.203, mean reward: 0.592 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.232], loss: 0.001358, mae: 0.039662, mean_q: 1.170115
 969527/1000000: episode: 9696, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 56.389, mean reward: 0.564 [0.502, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.490, 10.119], loss: 0.001374, mae: 0.039781, mean_q: 1.168084
 969627/1000000: episode: 9697, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 58.551, mean reward: 0.586 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.950, 10.098], loss: 0.001478, mae: 0.041295, mean_q: 1.168980
 969727/1000000: episode: 9698, duration: 1.474s, episode steps: 100, steps per second: 68, episode reward: 61.981, mean reward: 0.620 [0.513, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.998, 10.488], loss: 0.001435, mae: 0.041187, mean_q: 1.169611
 969827/1000000: episode: 9699, duration: 1.492s, episode steps: 100, steps per second: 67, episode reward: 57.382, mean reward: 0.574 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.670, 10.098], loss: 0.001458, mae: 0.041455, mean_q: 1.168544
 969927/1000000: episode: 9700, duration: 1.560s, episode steps: 100, steps per second: 64, episode reward: 58.403, mean reward: 0.584 [0.509, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.123], loss: 0.001411, mae: 0.040358, mean_q: 1.172595
 970027/1000000: episode: 9701, duration: 2.330s, episode steps: 100, steps per second: 43, episode reward: 58.399, mean reward: 0.584 [0.500, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.126, 10.098], loss: 0.001464, mae: 0.041361, mean_q: 1.168082
 970127/1000000: episode: 9702, duration: 2.476s, episode steps: 100, steps per second: 40, episode reward: 58.254, mean reward: 0.583 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.307, 10.182], loss: 0.001430, mae: 0.041033, mean_q: 1.167574
 970227/1000000: episode: 9703, duration: 2.310s, episode steps: 100, steps per second: 43, episode reward: 57.330, mean reward: 0.573 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.215], loss: 0.001485, mae: 0.041301, mean_q: 1.165098
 970327/1000000: episode: 9704, duration: 2.624s, episode steps: 100, steps per second: 38, episode reward: 57.505, mean reward: 0.575 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.679, 10.179], loss: 0.001472, mae: 0.041299, mean_q: 1.164946
 970427/1000000: episode: 9705, duration: 2.003s, episode steps: 100, steps per second: 50, episode reward: 57.568, mean reward: 0.576 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.107, 10.127], loss: 0.001537, mae: 0.042210, mean_q: 1.167173
 970527/1000000: episode: 9706, duration: 2.002s, episode steps: 100, steps per second: 50, episode reward: 58.181, mean reward: 0.582 [0.513, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.098], loss: 0.001469, mae: 0.040691, mean_q: 1.165713
 970627/1000000: episode: 9707, duration: 1.670s, episode steps: 100, steps per second: 60, episode reward: 61.179, mean reward: 0.612 [0.511, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.060, 10.098], loss: 0.001437, mae: 0.040535, mean_q: 1.163470
 970727/1000000: episode: 9708, duration: 1.662s, episode steps: 100, steps per second: 60, episode reward: 59.233, mean reward: 0.592 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.342, 10.098], loss: 0.001533, mae: 0.042014, mean_q: 1.163058
 970827/1000000: episode: 9709, duration: 1.566s, episode steps: 100, steps per second: 64, episode reward: 57.584, mean reward: 0.576 [0.501, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.487, 10.235], loss: 0.001480, mae: 0.041227, mean_q: 1.166913
 970927/1000000: episode: 9710, duration: 1.316s, episode steps: 100, steps per second: 76, episode reward: 57.703, mean reward: 0.577 [0.505, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.083, 10.098], loss: 0.001490, mae: 0.041535, mean_q: 1.163022
 971027/1000000: episode: 9711, duration: 1.237s, episode steps: 100, steps per second: 81, episode reward: 58.365, mean reward: 0.584 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.153, 10.169], loss: 0.001486, mae: 0.041582, mean_q: 1.162646
 971127/1000000: episode: 9712, duration: 1.552s, episode steps: 100, steps per second: 64, episode reward: 58.669, mean reward: 0.587 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.729, 10.098], loss: 0.001587, mae: 0.042717, mean_q: 1.163274
 971227/1000000: episode: 9713, duration: 1.485s, episode steps: 100, steps per second: 67, episode reward: 61.928, mean reward: 0.619 [0.503, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.589, 10.471], loss: 0.001607, mae: 0.042965, mean_q: 1.163523
 971327/1000000: episode: 9714, duration: 1.514s, episode steps: 100, steps per second: 66, episode reward: 59.317, mean reward: 0.593 [0.501, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.755, 10.098], loss: 0.001483, mae: 0.042471, mean_q: 1.168745
 971427/1000000: episode: 9715, duration: 1.537s, episode steps: 100, steps per second: 65, episode reward: 58.680, mean reward: 0.587 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.840, 10.098], loss: 0.001548, mae: 0.042640, mean_q: 1.162632
 971527/1000000: episode: 9716, duration: 1.316s, episode steps: 100, steps per second: 76, episode reward: 57.050, mean reward: 0.571 [0.509, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.112, 10.141], loss: 0.001497, mae: 0.041559, mean_q: 1.162542
 971627/1000000: episode: 9717, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 57.463, mean reward: 0.575 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.594, 10.340], loss: 0.001475, mae: 0.041546, mean_q: 1.161813
 971727/1000000: episode: 9718, duration: 1.225s, episode steps: 100, steps per second: 82, episode reward: 57.940, mean reward: 0.579 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.136, 10.098], loss: 0.001433, mae: 0.041187, mean_q: 1.159522
 971827/1000000: episode: 9719, duration: 1.309s, episode steps: 100, steps per second: 76, episode reward: 59.305, mean reward: 0.593 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.480, 10.098], loss: 0.001454, mae: 0.041091, mean_q: 1.159763
 971927/1000000: episode: 9720, duration: 1.344s, episode steps: 100, steps per second: 74, episode reward: 59.507, mean reward: 0.595 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.562, 10.187], loss: 0.001478, mae: 0.042408, mean_q: 1.165306
 972027/1000000: episode: 9721, duration: 1.192s, episode steps: 100, steps per second: 84, episode reward: 62.550, mean reward: 0.625 [0.520, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.969, 10.317], loss: 0.001536, mae: 0.042220, mean_q: 1.164849
 972127/1000000: episode: 9722, duration: 1.256s, episode steps: 100, steps per second: 80, episode reward: 60.201, mean reward: 0.602 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.333, 10.214], loss: 0.001444, mae: 0.041566, mean_q: 1.162890
 972227/1000000: episode: 9723, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: 59.596, mean reward: 0.596 [0.511, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.051, 10.173], loss: 0.001439, mae: 0.041592, mean_q: 1.166920
 972327/1000000: episode: 9724, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 60.052, mean reward: 0.601 [0.498, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.032, 10.098], loss: 0.001512, mae: 0.041966, mean_q: 1.167772
 972427/1000000: episode: 9725, duration: 1.883s, episode steps: 100, steps per second: 53, episode reward: 58.859, mean reward: 0.589 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.484, 10.098], loss: 0.001495, mae: 0.042250, mean_q: 1.163721
 972527/1000000: episode: 9726, duration: 1.673s, episode steps: 100, steps per second: 60, episode reward: 58.645, mean reward: 0.586 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.263, 10.098], loss: 0.001479, mae: 0.041462, mean_q: 1.163361
 972627/1000000: episode: 9727, duration: 1.515s, episode steps: 100, steps per second: 66, episode reward: 59.039, mean reward: 0.590 [0.515, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.098], loss: 0.001439, mae: 0.041330, mean_q: 1.168003
 972727/1000000: episode: 9728, duration: 1.333s, episode steps: 100, steps per second: 75, episode reward: 60.399, mean reward: 0.604 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.792, 10.370], loss: 0.001467, mae: 0.042071, mean_q: 1.167124
 972827/1000000: episode: 9729, duration: 1.203s, episode steps: 100, steps per second: 83, episode reward: 58.411, mean reward: 0.584 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.578, 10.189], loss: 0.001446, mae: 0.041134, mean_q: 1.166584
 972927/1000000: episode: 9730, duration: 1.270s, episode steps: 100, steps per second: 79, episode reward: 57.579, mean reward: 0.576 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.160], loss: 0.001426, mae: 0.040687, mean_q: 1.166302
 973027/1000000: episode: 9731, duration: 1.686s, episode steps: 100, steps per second: 59, episode reward: 64.719, mean reward: 0.647 [0.508, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.388, 10.397], loss: 0.001416, mae: 0.040247, mean_q: 1.165484
 973127/1000000: episode: 9732, duration: 1.512s, episode steps: 100, steps per second: 66, episode reward: 55.858, mean reward: 0.559 [0.498, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.852, 10.098], loss: 0.001484, mae: 0.041757, mean_q: 1.169078
 973227/1000000: episode: 9733, duration: 1.559s, episode steps: 100, steps per second: 64, episode reward: 60.949, mean reward: 0.609 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.047, 10.098], loss: 0.001459, mae: 0.041482, mean_q: 1.168983
 973327/1000000: episode: 9734, duration: 1.765s, episode steps: 100, steps per second: 57, episode reward: 59.089, mean reward: 0.591 [0.514, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.473, 10.098], loss: 0.001562, mae: 0.042843, mean_q: 1.168612
 973427/1000000: episode: 9735, duration: 1.483s, episode steps: 100, steps per second: 67, episode reward: 57.683, mean reward: 0.577 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.394, 10.149], loss: 0.001499, mae: 0.041919, mean_q: 1.165131
 973527/1000000: episode: 9736, duration: 1.545s, episode steps: 100, steps per second: 65, episode reward: 60.253, mean reward: 0.603 [0.506, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.070, 10.548], loss: 0.001405, mae: 0.041027, mean_q: 1.168711
 973627/1000000: episode: 9737, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 59.043, mean reward: 0.590 [0.514, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.493, 10.217], loss: 0.001527, mae: 0.041470, mean_q: 1.170550
 973727/1000000: episode: 9738, duration: 1.511s, episode steps: 100, steps per second: 66, episode reward: 59.299, mean reward: 0.593 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.408, 10.098], loss: 0.001422, mae: 0.040582, mean_q: 1.166702
 973827/1000000: episode: 9739, duration: 1.585s, episode steps: 100, steps per second: 63, episode reward: 60.940, mean reward: 0.609 [0.509, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.379, 10.098], loss: 0.001482, mae: 0.041373, mean_q: 1.167948
 973927/1000000: episode: 9740, duration: 1.306s, episode steps: 100, steps per second: 77, episode reward: 58.337, mean reward: 0.583 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.755, 10.143], loss: 0.001407, mae: 0.039996, mean_q: 1.170311
 974027/1000000: episode: 9741, duration: 1.596s, episode steps: 100, steps per second: 63, episode reward: 60.660, mean reward: 0.607 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.397, 10.098], loss: 0.001355, mae: 0.039962, mean_q: 1.168391
 974127/1000000: episode: 9742, duration: 1.494s, episode steps: 100, steps per second: 67, episode reward: 57.363, mean reward: 0.574 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.405, 10.098], loss: 0.001429, mae: 0.041046, mean_q: 1.170425
 974227/1000000: episode: 9743, duration: 1.371s, episode steps: 100, steps per second: 73, episode reward: 57.342, mean reward: 0.573 [0.499, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.000, 10.098], loss: 0.001399, mae: 0.040777, mean_q: 1.168877
 974327/1000000: episode: 9744, duration: 1.930s, episode steps: 100, steps per second: 52, episode reward: 59.121, mean reward: 0.591 [0.512, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.480, 10.098], loss: 0.001361, mae: 0.039832, mean_q: 1.165374
 974427/1000000: episode: 9745, duration: 1.787s, episode steps: 100, steps per second: 56, episode reward: 58.510, mean reward: 0.585 [0.512, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.999, 10.098], loss: 0.001373, mae: 0.040433, mean_q: 1.167888
 974527/1000000: episode: 9746, duration: 1.609s, episode steps: 100, steps per second: 62, episode reward: 56.533, mean reward: 0.565 [0.503, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.400, 10.148], loss: 0.001417, mae: 0.040668, mean_q: 1.168605
 974627/1000000: episode: 9747, duration: 1.501s, episode steps: 100, steps per second: 67, episode reward: 58.921, mean reward: 0.589 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.184], loss: 0.001386, mae: 0.040995, mean_q: 1.168698
 974727/1000000: episode: 9748, duration: 1.466s, episode steps: 100, steps per second: 68, episode reward: 62.092, mean reward: 0.621 [0.509, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.239, 10.098], loss: 0.001407, mae: 0.040929, mean_q: 1.164445
 974827/1000000: episode: 9749, duration: 1.243s, episode steps: 100, steps per second: 80, episode reward: 57.776, mean reward: 0.578 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.209, 10.098], loss: 0.001444, mae: 0.041374, mean_q: 1.169887
 974927/1000000: episode: 9750, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 59.413, mean reward: 0.594 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.487, 10.248], loss: 0.001324, mae: 0.039550, mean_q: 1.166302
 975027/1000000: episode: 9751, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 59.689, mean reward: 0.597 [0.517, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.822, 10.098], loss: 0.001287, mae: 0.038575, mean_q: 1.165214
 975127/1000000: episode: 9752, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 59.149, mean reward: 0.591 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.968, 10.333], loss: 0.001303, mae: 0.039484, mean_q: 1.165484
 975227/1000000: episode: 9753, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 59.527, mean reward: 0.595 [0.508, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.098], loss: 0.001289, mae: 0.038978, mean_q: 1.166042
 975327/1000000: episode: 9754, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 57.660, mean reward: 0.577 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.711, 10.185], loss: 0.001307, mae: 0.039348, mean_q: 1.167804
 975427/1000000: episode: 9755, duration: 1.257s, episode steps: 100, steps per second: 80, episode reward: 59.467, mean reward: 0.595 [0.517, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.352, 10.280], loss: 0.001284, mae: 0.038688, mean_q: 1.168294
 975527/1000000: episode: 9756, duration: 1.633s, episode steps: 100, steps per second: 61, episode reward: 63.402, mean reward: 0.634 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.528, 10.098], loss: 0.001425, mae: 0.041172, mean_q: 1.169353
 975627/1000000: episode: 9757, duration: 1.759s, episode steps: 100, steps per second: 57, episode reward: 56.474, mean reward: 0.565 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.604, 10.098], loss: 0.001344, mae: 0.039649, mean_q: 1.167799
 975727/1000000: episode: 9758, duration: 1.994s, episode steps: 100, steps per second: 50, episode reward: 58.310, mean reward: 0.583 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.247, 10.098], loss: 0.001356, mae: 0.040490, mean_q: 1.168788
 975827/1000000: episode: 9759, duration: 1.549s, episode steps: 100, steps per second: 65, episode reward: 60.612, mean reward: 0.606 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.996, 10.218], loss: 0.001367, mae: 0.040355, mean_q: 1.170505
 975927/1000000: episode: 9760, duration: 1.312s, episode steps: 100, steps per second: 76, episode reward: 59.033, mean reward: 0.590 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.771, 10.175], loss: 0.001433, mae: 0.041469, mean_q: 1.171916
 976027/1000000: episode: 9761, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 60.538, mean reward: 0.605 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.410, 10.098], loss: 0.001337, mae: 0.039918, mean_q: 1.169467
 976127/1000000: episode: 9762, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 62.878, mean reward: 0.629 [0.511, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.746, 10.098], loss: 0.001375, mae: 0.040106, mean_q: 1.173333
 976227/1000000: episode: 9763, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 58.807, mean reward: 0.588 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.804, 10.098], loss: 0.001452, mae: 0.040583, mean_q: 1.177136
 976327/1000000: episode: 9764, duration: 1.158s, episode steps: 100, steps per second: 86, episode reward: 60.422, mean reward: 0.604 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.131, 10.154], loss: 0.001390, mae: 0.040756, mean_q: 1.172770
 976427/1000000: episode: 9765, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: 57.073, mean reward: 0.571 [0.504, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.181, 10.204], loss: 0.001402, mae: 0.040912, mean_q: 1.172739
 976527/1000000: episode: 9766, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 60.417, mean reward: 0.604 [0.516, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.513, 10.098], loss: 0.001362, mae: 0.039816, mean_q: 1.175950
 976627/1000000: episode: 9767, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 57.783, mean reward: 0.578 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.511, 10.134], loss: 0.001474, mae: 0.041414, mean_q: 1.176075
 976727/1000000: episode: 9768, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 58.094, mean reward: 0.581 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.271, 10.174], loss: 0.001437, mae: 0.040899, mean_q: 1.177902
 976827/1000000: episode: 9769, duration: 1.721s, episode steps: 100, steps per second: 58, episode reward: 60.352, mean reward: 0.604 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.894, 10.409], loss: 0.001349, mae: 0.040398, mean_q: 1.174029
 976927/1000000: episode: 9770, duration: 1.464s, episode steps: 100, steps per second: 68, episode reward: 58.097, mean reward: 0.581 [0.504, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.699, 10.247], loss: 0.001482, mae: 0.041802, mean_q: 1.176100
 977027/1000000: episode: 9771, duration: 1.473s, episode steps: 100, steps per second: 68, episode reward: 58.246, mean reward: 0.582 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.606, 10.144], loss: 0.001413, mae: 0.040759, mean_q: 1.174954
 977127/1000000: episode: 9772, duration: 1.157s, episode steps: 100, steps per second: 86, episode reward: 56.957, mean reward: 0.570 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.972, 10.105], loss: 0.001410, mae: 0.040901, mean_q: 1.175026
 977227/1000000: episode: 9773, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 57.314, mean reward: 0.573 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.055, 10.098], loss: 0.001380, mae: 0.040219, mean_q: 1.172506
 977327/1000000: episode: 9774, duration: 1.445s, episode steps: 100, steps per second: 69, episode reward: 57.534, mean reward: 0.575 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.794, 10.098], loss: 0.001467, mae: 0.041467, mean_q: 1.167032
 977427/1000000: episode: 9775, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 57.259, mean reward: 0.573 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.043, 10.238], loss: 0.001335, mae: 0.039688, mean_q: 1.170714
 977527/1000000: episode: 9776, duration: 1.164s, episode steps: 100, steps per second: 86, episode reward: 55.791, mean reward: 0.558 [0.501, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.226, 10.256], loss: 0.001325, mae: 0.039415, mean_q: 1.167646
 977627/1000000: episode: 9777, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.300, mean reward: 0.583 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.860, 10.283], loss: 0.001297, mae: 0.039093, mean_q: 1.168952
 977727/1000000: episode: 9778, duration: 1.498s, episode steps: 100, steps per second: 67, episode reward: 58.617, mean reward: 0.586 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.168], loss: 0.001390, mae: 0.040637, mean_q: 1.166926
 977827/1000000: episode: 9779, duration: 1.471s, episode steps: 100, steps per second: 68, episode reward: 61.860, mean reward: 0.619 [0.501, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.534, 10.098], loss: 0.001343, mae: 0.040409, mean_q: 1.166526
 977927/1000000: episode: 9780, duration: 1.405s, episode steps: 100, steps per second: 71, episode reward: 58.108, mean reward: 0.581 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.194], loss: 0.001369, mae: 0.040301, mean_q: 1.166987
 978027/1000000: episode: 9781, duration: 1.280s, episode steps: 100, steps per second: 78, episode reward: 58.840, mean reward: 0.588 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.637, 10.197], loss: 0.001442, mae: 0.041137, mean_q: 1.164139
 978127/1000000: episode: 9782, duration: 1.163s, episode steps: 100, steps per second: 86, episode reward: 57.477, mean reward: 0.575 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.130, 10.180], loss: 0.001439, mae: 0.041318, mean_q: 1.170063
 978227/1000000: episode: 9783, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 57.675, mean reward: 0.577 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.969, 10.098], loss: 0.001400, mae: 0.040528, mean_q: 1.168067
 978327/1000000: episode: 9784, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 58.524, mean reward: 0.585 [0.511, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.098], loss: 0.001384, mae: 0.040236, mean_q: 1.166036
 978427/1000000: episode: 9785, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.847, mean reward: 0.588 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.150], loss: 0.001348, mae: 0.040001, mean_q: 1.165273
 978527/1000000: episode: 9786, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 61.225, mean reward: 0.612 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.098], loss: 0.001361, mae: 0.040691, mean_q: 1.164827
 978627/1000000: episode: 9787, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 63.781, mean reward: 0.638 [0.504, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.057, 10.098], loss: 0.001408, mae: 0.040814, mean_q: 1.166291
 978727/1000000: episode: 9788, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: 59.577, mean reward: 0.596 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.243, 10.445], loss: 0.001343, mae: 0.039703, mean_q: 1.166164
 978827/1000000: episode: 9789, duration: 1.492s, episode steps: 100, steps per second: 67, episode reward: 57.910, mean reward: 0.579 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.593, 10.193], loss: 0.001434, mae: 0.041101, mean_q: 1.165766
 978927/1000000: episode: 9790, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 58.311, mean reward: 0.583 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.008, 10.168], loss: 0.001439, mae: 0.041045, mean_q: 1.165491
 979027/1000000: episode: 9791, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 58.034, mean reward: 0.580 [0.510, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.262, 10.139], loss: 0.001487, mae: 0.042076, mean_q: 1.166098
 979127/1000000: episode: 9792, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 57.151, mean reward: 0.572 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.547, 10.098], loss: 0.001371, mae: 0.040985, mean_q: 1.169257
 979227/1000000: episode: 9793, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 60.206, mean reward: 0.602 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.229, 10.192], loss: 0.001414, mae: 0.041252, mean_q: 1.165237
 979327/1000000: episode: 9794, duration: 1.319s, episode steps: 100, steps per second: 76, episode reward: 59.537, mean reward: 0.595 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.155, 10.466], loss: 0.001387, mae: 0.040303, mean_q: 1.169159
 979427/1000000: episode: 9795, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 60.936, mean reward: 0.609 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.272, 10.258], loss: 0.001484, mae: 0.042535, mean_q: 1.164739
 979527/1000000: episode: 9796, duration: 1.174s, episode steps: 100, steps per second: 85, episode reward: 60.240, mean reward: 0.602 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.384, 10.098], loss: 0.001476, mae: 0.041655, mean_q: 1.167660
 979627/1000000: episode: 9797, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 58.146, mean reward: 0.581 [0.498, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.366, 10.098], loss: 0.001516, mae: 0.042368, mean_q: 1.168966
 979727/1000000: episode: 9798, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 59.099, mean reward: 0.591 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.046, 10.098], loss: 0.001440, mae: 0.041152, mean_q: 1.169077
 979827/1000000: episode: 9799, duration: 1.160s, episode steps: 100, steps per second: 86, episode reward: 58.059, mean reward: 0.581 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.278], loss: 0.001444, mae: 0.041736, mean_q: 1.165282
 979927/1000000: episode: 9800, duration: 1.161s, episode steps: 100, steps per second: 86, episode reward: 59.525, mean reward: 0.595 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.098], loss: 0.001456, mae: 0.041719, mean_q: 1.167458
 980027/1000000: episode: 9801, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 58.100, mean reward: 0.581 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.734, 10.275], loss: 0.001429, mae: 0.041538, mean_q: 1.169422
 980127/1000000: episode: 9802, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 58.424, mean reward: 0.584 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.711, 10.098], loss: 0.001493, mae: 0.042243, mean_q: 1.168107
 980227/1000000: episode: 9803, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 62.957, mean reward: 0.630 [0.517, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.701, 10.098], loss: 0.001489, mae: 0.042369, mean_q: 1.170666
 980327/1000000: episode: 9804, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 59.428, mean reward: 0.594 [0.501, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.188], loss: 0.001483, mae: 0.042394, mean_q: 1.169095
 980427/1000000: episode: 9805, duration: 1.179s, episode steps: 100, steps per second: 85, episode reward: 57.155, mean reward: 0.572 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.764, 10.098], loss: 0.001493, mae: 0.042400, mean_q: 1.168796
 980527/1000000: episode: 9806, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 57.799, mean reward: 0.578 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.308, 10.098], loss: 0.001545, mae: 0.042843, mean_q: 1.165751
 980627/1000000: episode: 9807, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 59.820, mean reward: 0.598 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.216, 10.098], loss: 0.001417, mae: 0.041316, mean_q: 1.166825
 980727/1000000: episode: 9808, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 57.451, mean reward: 0.575 [0.499, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.395, 10.181], loss: 0.001449, mae: 0.041329, mean_q: 1.169823
 980827/1000000: episode: 9809, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 59.833, mean reward: 0.598 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.127, 10.163], loss: 0.001416, mae: 0.040743, mean_q: 1.165606
 980927/1000000: episode: 9810, duration: 1.172s, episode steps: 100, steps per second: 85, episode reward: 58.016, mean reward: 0.580 [0.511, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.599, 10.098], loss: 0.001475, mae: 0.041931, mean_q: 1.164937
 981027/1000000: episode: 9811, duration: 1.171s, episode steps: 100, steps per second: 85, episode reward: 58.796, mean reward: 0.588 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.888, 10.246], loss: 0.001483, mae: 0.042009, mean_q: 1.162962
 981127/1000000: episode: 9812, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: 60.252, mean reward: 0.603 [0.509, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.447, 10.381], loss: 0.001463, mae: 0.041940, mean_q: 1.164461
 981227/1000000: episode: 9813, duration: 1.181s, episode steps: 100, steps per second: 85, episode reward: 58.700, mean reward: 0.587 [0.498, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.569, 10.098], loss: 0.001463, mae: 0.041527, mean_q: 1.164824
 981327/1000000: episode: 9814, duration: 1.460s, episode steps: 100, steps per second: 68, episode reward: 58.861, mean reward: 0.589 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.495, 10.098], loss: 0.001485, mae: 0.041500, mean_q: 1.162635
 981427/1000000: episode: 9815, duration: 1.165s, episode steps: 100, steps per second: 86, episode reward: 57.235, mean reward: 0.572 [0.498, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.750, 10.232], loss: 0.001432, mae: 0.041516, mean_q: 1.165029
 981527/1000000: episode: 9816, duration: 1.166s, episode steps: 100, steps per second: 86, episode reward: 59.909, mean reward: 0.599 [0.518, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.385, 10.098], loss: 0.001478, mae: 0.042111, mean_q: 1.161482
 981627/1000000: episode: 9817, duration: 1.170s, episode steps: 100, steps per second: 85, episode reward: 58.268, mean reward: 0.583 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.202, 10.098], loss: 0.001434, mae: 0.041150, mean_q: 1.160221
 981727/1000000: episode: 9818, duration: 1.176s, episode steps: 100, steps per second: 85, episode reward: 57.554, mean reward: 0.576 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.635, 10.098], loss: 0.001502, mae: 0.042107, mean_q: 1.164816
 981827/1000000: episode: 9819, duration: 2.011s, episode steps: 100, steps per second: 50, episode reward: 59.771, mean reward: 0.598 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.179, 10.098], loss: 0.001441, mae: 0.041347, mean_q: 1.160469
 981927/1000000: episode: 9820, duration: 2.232s, episode steps: 100, steps per second: 45, episode reward: 58.472, mean reward: 0.585 [0.501, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.958, 10.098], loss: 0.001546, mae: 0.043229, mean_q: 1.164328
 982027/1000000: episode: 9821, duration: 1.828s, episode steps: 100, steps per second: 55, episode reward: 59.718, mean reward: 0.597 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.097, 10.098], loss: 0.001522, mae: 0.042664, mean_q: 1.165001
 982127/1000000: episode: 9822, duration: 1.927s, episode steps: 100, steps per second: 52, episode reward: 57.515, mean reward: 0.575 [0.503, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.798, 10.098], loss: 0.001538, mae: 0.042295, mean_q: 1.166112
 982227/1000000: episode: 9823, duration: 1.692s, episode steps: 100, steps per second: 59, episode reward: 56.773, mean reward: 0.568 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.375, 10.098], loss: 0.001437, mae: 0.041588, mean_q: 1.161537
 982327/1000000: episode: 9824, duration: 1.569s, episode steps: 100, steps per second: 64, episode reward: 58.308, mean reward: 0.583 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.366, 10.098], loss: 0.001528, mae: 0.042320, mean_q: 1.163282
 982427/1000000: episode: 9825, duration: 1.525s, episode steps: 100, steps per second: 66, episode reward: 58.256, mean reward: 0.583 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.534, 10.098], loss: 0.001441, mae: 0.041657, mean_q: 1.165092
 982527/1000000: episode: 9826, duration: 1.531s, episode steps: 100, steps per second: 65, episode reward: 60.016, mean reward: 0.600 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.502, 10.098], loss: 0.001424, mae: 0.041309, mean_q: 1.166383
 982627/1000000: episode: 9827, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 58.465, mean reward: 0.585 [0.498, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.994, 10.098], loss: 0.001559, mae: 0.043013, mean_q: 1.168291
 982727/1000000: episode: 9828, duration: 1.485s, episode steps: 100, steps per second: 67, episode reward: 57.351, mean reward: 0.574 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.509, 10.336], loss: 0.001602, mae: 0.043762, mean_q: 1.166196
 982827/1000000: episode: 9829, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 59.724, mean reward: 0.597 [0.507, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.297, 10.285], loss: 0.001518, mae: 0.042274, mean_q: 1.166519
 982927/1000000: episode: 9830, duration: 1.430s, episode steps: 100, steps per second: 70, episode reward: 58.077, mean reward: 0.581 [0.515, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.155, 10.180], loss: 0.001622, mae: 0.043383, mean_q: 1.171249
 983027/1000000: episode: 9831, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 58.652, mean reward: 0.587 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.661, 10.098], loss: 0.001585, mae: 0.042835, mean_q: 1.163268
 983127/1000000: episode: 9832, duration: 1.126s, episode steps: 100, steps per second: 89, episode reward: 58.192, mean reward: 0.582 [0.513, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.469, 10.098], loss: 0.001627, mae: 0.043321, mean_q: 1.165989
 983227/1000000: episode: 9833, duration: 1.329s, episode steps: 100, steps per second: 75, episode reward: 58.910, mean reward: 0.589 [0.504, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.685, 10.252], loss: 0.001525, mae: 0.042427, mean_q: 1.165842
 983327/1000000: episode: 9834, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 57.778, mean reward: 0.578 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.015, 10.186], loss: 0.001461, mae: 0.041511, mean_q: 1.163679
 983427/1000000: episode: 9835, duration: 1.327s, episode steps: 100, steps per second: 75, episode reward: 59.534, mean reward: 0.595 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.158, 10.098], loss: 0.001586, mae: 0.043136, mean_q: 1.166176
 983527/1000000: episode: 9836, duration: 1.087s, episode steps: 100, steps per second: 92, episode reward: 59.355, mean reward: 0.594 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.455, 10.137], loss: 0.001569, mae: 0.043140, mean_q: 1.164327
 983627/1000000: episode: 9837, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 56.577, mean reward: 0.566 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.731, 10.098], loss: 0.001537, mae: 0.042146, mean_q: 1.167019
 983727/1000000: episode: 9838, duration: 1.197s, episode steps: 100, steps per second: 84, episode reward: 59.683, mean reward: 0.597 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.941, 10.098], loss: 0.001470, mae: 0.041621, mean_q: 1.162853
 983827/1000000: episode: 9839, duration: 1.082s, episode steps: 100, steps per second: 92, episode reward: 56.962, mean reward: 0.570 [0.505, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.119, 10.190], loss: 0.001506, mae: 0.042339, mean_q: 1.163617
 983927/1000000: episode: 9840, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 58.526, mean reward: 0.585 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.977, 10.152], loss: 0.001514, mae: 0.042579, mean_q: 1.163373
 984027/1000000: episode: 9841, duration: 1.148s, episode steps: 100, steps per second: 87, episode reward: 59.583, mean reward: 0.596 [0.513, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.718, 10.175], loss: 0.001505, mae: 0.042180, mean_q: 1.162123
 984127/1000000: episode: 9842, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 59.663, mean reward: 0.597 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.098], loss: 0.001436, mae: 0.041270, mean_q: 1.160214
 984227/1000000: episode: 9843, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 60.986, mean reward: 0.610 [0.509, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.270, 10.098], loss: 0.001480, mae: 0.041729, mean_q: 1.164582
 984327/1000000: episode: 9844, duration: 1.537s, episode steps: 100, steps per second: 65, episode reward: 59.670, mean reward: 0.597 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.888, 10.098], loss: 0.001391, mae: 0.040250, mean_q: 1.162404
 984427/1000000: episode: 9845, duration: 1.810s, episode steps: 100, steps per second: 55, episode reward: 58.905, mean reward: 0.589 [0.503, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.398, 10.117], loss: 0.001479, mae: 0.041499, mean_q: 1.164866
 984527/1000000: episode: 9846, duration: 1.658s, episode steps: 100, steps per second: 60, episode reward: 57.035, mean reward: 0.570 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.647, 10.129], loss: 0.001447, mae: 0.040929, mean_q: 1.161421
 984627/1000000: episode: 9847, duration: 1.563s, episode steps: 100, steps per second: 64, episode reward: 58.730, mean reward: 0.587 [0.512, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.167, 10.098], loss: 0.001461, mae: 0.040983, mean_q: 1.165118
 984727/1000000: episode: 9848, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 57.771, mean reward: 0.578 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.424, 10.098], loss: 0.001440, mae: 0.040801, mean_q: 1.161841
 984827/1000000: episode: 9849, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.837, mean reward: 0.588 [0.509, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.847, 10.125], loss: 0.001491, mae: 0.041481, mean_q: 1.161397
 984927/1000000: episode: 9850, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 61.322, mean reward: 0.613 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.098], loss: 0.001560, mae: 0.041992, mean_q: 1.162726
 985027/1000000: episode: 9851, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 62.651, mean reward: 0.627 [0.501, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.915, 10.098], loss: 0.001555, mae: 0.042197, mean_q: 1.163836
 985127/1000000: episode: 9852, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 57.173, mean reward: 0.572 [0.505, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.438, 10.151], loss: 0.001481, mae: 0.041240, mean_q: 1.164852
 985227/1000000: episode: 9853, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 57.551, mean reward: 0.576 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.893, 10.098], loss: 0.001509, mae: 0.041847, mean_q: 1.160502
 985327/1000000: episode: 9854, duration: 1.210s, episode steps: 100, steps per second: 83, episode reward: 59.679, mean reward: 0.597 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.928, 10.337], loss: 0.001488, mae: 0.041408, mean_q: 1.161367
 985427/1000000: episode: 9855, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 57.554, mean reward: 0.576 [0.508, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.947, 10.174], loss: 0.001500, mae: 0.041720, mean_q: 1.165284
 985527/1000000: episode: 9856, duration: 1.224s, episode steps: 100, steps per second: 82, episode reward: 60.203, mean reward: 0.602 [0.511, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.118, 10.098], loss: 0.001406, mae: 0.040672, mean_q: 1.159319
 985627/1000000: episode: 9857, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 60.850, mean reward: 0.609 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.388], loss: 0.001422, mae: 0.040740, mean_q: 1.164987
 985727/1000000: episode: 9858, duration: 1.244s, episode steps: 100, steps per second: 80, episode reward: 59.895, mean reward: 0.599 [0.520, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.327, 10.098], loss: 0.001482, mae: 0.041774, mean_q: 1.162647
 985827/1000000: episode: 9859, duration: 1.204s, episode steps: 100, steps per second: 83, episode reward: 57.570, mean reward: 0.576 [0.508, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.610, 10.098], loss: 0.001460, mae: 0.041491, mean_q: 1.163690
 985927/1000000: episode: 9860, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 58.158, mean reward: 0.582 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.957, 10.200], loss: 0.001441, mae: 0.040725, mean_q: 1.164599
 986027/1000000: episode: 9861, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 57.114, mean reward: 0.571 [0.507, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.716, 10.098], loss: 0.001535, mae: 0.041974, mean_q: 1.160497
 986127/1000000: episode: 9862, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 62.671, mean reward: 0.627 [0.511, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.860, 10.278], loss: 0.001377, mae: 0.040273, mean_q: 1.162047
 986227/1000000: episode: 9863, duration: 1.040s, episode steps: 100, steps per second: 96, episode reward: 57.266, mean reward: 0.573 [0.503, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.216, 10.098], loss: 0.001433, mae: 0.040989, mean_q: 1.164743
 986327/1000000: episode: 9864, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 60.878, mean reward: 0.609 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.730, 10.303], loss: 0.001462, mae: 0.041010, mean_q: 1.165116
 986427/1000000: episode: 9865, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 59.654, mean reward: 0.597 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.418, 10.098], loss: 0.001519, mae: 0.041937, mean_q: 1.166136
 986527/1000000: episode: 9866, duration: 1.038s, episode steps: 100, steps per second: 96, episode reward: 60.879, mean reward: 0.609 [0.503, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.619, 10.098], loss: 0.001533, mae: 0.042421, mean_q: 1.166589
 986627/1000000: episode: 9867, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 60.987, mean reward: 0.610 [0.518, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.579, 10.385], loss: 0.001486, mae: 0.041657, mean_q: 1.170350
 986727/1000000: episode: 9868, duration: 1.231s, episode steps: 100, steps per second: 81, episode reward: 58.621, mean reward: 0.586 [0.507, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.030, 10.098], loss: 0.001494, mae: 0.041566, mean_q: 1.165143
 986827/1000000: episode: 9869, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 58.001, mean reward: 0.580 [0.505, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.023, 10.131], loss: 0.001479, mae: 0.041339, mean_q: 1.165043
 986927/1000000: episode: 9870, duration: 1.214s, episode steps: 100, steps per second: 82, episode reward: 59.948, mean reward: 0.599 [0.505, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.603, 10.098], loss: 0.001550, mae: 0.042917, mean_q: 1.167402
 987027/1000000: episode: 9871, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 63.884, mean reward: 0.639 [0.503, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.382], loss: 0.001606, mae: 0.042990, mean_q: 1.169650
 987127/1000000: episode: 9872, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.736, mean reward: 0.597 [0.511, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.399, 10.098], loss: 0.001522, mae: 0.042006, mean_q: 1.165519
 987227/1000000: episode: 9873, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 57.916, mean reward: 0.579 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.332, 10.098], loss: 0.001505, mae: 0.041723, mean_q: 1.172196
 987327/1000000: episode: 9874, duration: 1.051s, episode steps: 100, steps per second: 95, episode reward: 57.824, mean reward: 0.578 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.215, 10.098], loss: 0.001599, mae: 0.043170, mean_q: 1.170422
 987427/1000000: episode: 9875, duration: 1.042s, episode steps: 100, steps per second: 96, episode reward: 56.469, mean reward: 0.565 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.098], loss: 0.001511, mae: 0.041623, mean_q: 1.170746
 987527/1000000: episode: 9876, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.661, mean reward: 0.597 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.366, 10.220], loss: 0.001522, mae: 0.041305, mean_q: 1.169803
 987627/1000000: episode: 9877, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 56.765, mean reward: 0.568 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.152], loss: 0.001643, mae: 0.043221, mean_q: 1.168283
 987727/1000000: episode: 9878, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 62.578, mean reward: 0.626 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.153, 10.268], loss: 0.001523, mae: 0.042044, mean_q: 1.168425
 987827/1000000: episode: 9879, duration: 1.057s, episode steps: 100, steps per second: 95, episode reward: 58.760, mean reward: 0.588 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.287], loss: 0.001513, mae: 0.042273, mean_q: 1.172651
 987927/1000000: episode: 9880, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: 58.605, mean reward: 0.586 [0.520, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.276, 10.240], loss: 0.001507, mae: 0.042389, mean_q: 1.170792
 988027/1000000: episode: 9881, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 56.765, mean reward: 0.568 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.782, 10.222], loss: 0.001445, mae: 0.040973, mean_q: 1.170919
 988127/1000000: episode: 9882, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 58.675, mean reward: 0.587 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.534, 10.372], loss: 0.001493, mae: 0.041187, mean_q: 1.167199
 988227/1000000: episode: 9883, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 57.366, mean reward: 0.574 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.175], loss: 0.001413, mae: 0.041131, mean_q: 1.168031
 988327/1000000: episode: 9884, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 60.197, mean reward: 0.602 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.353, 10.325], loss: 0.001500, mae: 0.042153, mean_q: 1.171994
 988427/1000000: episode: 9885, duration: 1.083s, episode steps: 100, steps per second: 92, episode reward: 58.321, mean reward: 0.583 [0.510, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.574, 10.456], loss: 0.001458, mae: 0.041386, mean_q: 1.171588
 988527/1000000: episode: 9886, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 58.227, mean reward: 0.582 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.176, 10.146], loss: 0.001503, mae: 0.041482, mean_q: 1.166178
 988627/1000000: episode: 9887, duration: 1.045s, episode steps: 100, steps per second: 96, episode reward: 57.768, mean reward: 0.578 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.697, 10.175], loss: 0.001527, mae: 0.042595, mean_q: 1.169594
 988727/1000000: episode: 9888, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 57.584, mean reward: 0.576 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.285, 10.098], loss: 0.001371, mae: 0.040793, mean_q: 1.166191
 988827/1000000: episode: 9889, duration: 1.058s, episode steps: 100, steps per second: 94, episode reward: 63.595, mean reward: 0.636 [0.513, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.851, 10.098], loss: 0.001475, mae: 0.041770, mean_q: 1.172454
 988927/1000000: episode: 9890, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 60.870, mean reward: 0.609 [0.512, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.783, 10.098], loss: 0.001611, mae: 0.043629, mean_q: 1.168506
 989027/1000000: episode: 9891, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 55.652, mean reward: 0.557 [0.500, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.194], loss: 0.001547, mae: 0.042437, mean_q: 1.174556
 989127/1000000: episode: 9892, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 58.370, mean reward: 0.584 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.611, 10.145], loss: 0.001492, mae: 0.041828, mean_q: 1.173263
 989227/1000000: episode: 9893, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 57.178, mean reward: 0.572 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.730, 10.098], loss: 0.001574, mae: 0.042994, mean_q: 1.174710
 989327/1000000: episode: 9894, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 58.709, mean reward: 0.587 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.934, 10.106], loss: 0.001453, mae: 0.041707, mean_q: 1.167250
 989427/1000000: episode: 9895, duration: 1.090s, episode steps: 100, steps per second: 92, episode reward: 58.830, mean reward: 0.588 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.159, 10.209], loss: 0.001366, mae: 0.040062, mean_q: 1.165321
 989527/1000000: episode: 9896, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 57.856, mean reward: 0.579 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.130, 10.098], loss: 0.001548, mae: 0.042452, mean_q: 1.168164
 989627/1000000: episode: 9897, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 63.189, mean reward: 0.632 [0.503, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.938, 10.197], loss: 0.001513, mae: 0.041977, mean_q: 1.172714
 989727/1000000: episode: 9898, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 58.527, mean reward: 0.585 [0.517, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.956, 10.098], loss: 0.001410, mae: 0.041115, mean_q: 1.169231
 989827/1000000: episode: 9899, duration: 1.231s, episode steps: 100, steps per second: 81, episode reward: 57.452, mean reward: 0.575 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.900, 10.189], loss: 0.001461, mae: 0.041131, mean_q: 1.169654
 989927/1000000: episode: 9900, duration: 1.085s, episode steps: 100, steps per second: 92, episode reward: 59.106, mean reward: 0.591 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.954, 10.166], loss: 0.001423, mae: 0.040930, mean_q: 1.166834
 990027/1000000: episode: 9901, duration: 1.060s, episode steps: 100, steps per second: 94, episode reward: 58.488, mean reward: 0.585 [0.497, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.227, 10.125], loss: 0.001449, mae: 0.041842, mean_q: 1.167690
 990127/1000000: episode: 9902, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 59.889, mean reward: 0.599 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.641, 10.098], loss: 0.001384, mae: 0.040612, mean_q: 1.168762
 990227/1000000: episode: 9903, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 61.476, mean reward: 0.615 [0.501, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.569, 10.384], loss: 0.001480, mae: 0.041535, mean_q: 1.171050
 990327/1000000: episode: 9904, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 58.750, mean reward: 0.588 [0.504, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.411, 10.192], loss: 0.001401, mae: 0.041301, mean_q: 1.170244
 990427/1000000: episode: 9905, duration: 1.062s, episode steps: 100, steps per second: 94, episode reward: 60.276, mean reward: 0.603 [0.510, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.374, 10.219], loss: 0.001526, mae: 0.041931, mean_q: 1.170989
 990527/1000000: episode: 9906, duration: 1.074s, episode steps: 100, steps per second: 93, episode reward: 61.799, mean reward: 0.618 [0.511, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.947, 10.394], loss: 0.001324, mae: 0.039573, mean_q: 1.170594
 990627/1000000: episode: 9907, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 57.807, mean reward: 0.578 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.173, 10.098], loss: 0.001466, mae: 0.041384, mean_q: 1.168516
 990727/1000000: episode: 9908, duration: 1.123s, episode steps: 100, steps per second: 89, episode reward: 56.950, mean reward: 0.569 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.189, 10.202], loss: 0.001495, mae: 0.041499, mean_q: 1.169429
 990827/1000000: episode: 9909, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 60.631, mean reward: 0.606 [0.520, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.158, 10.098], loss: 0.001436, mae: 0.041976, mean_q: 1.168370
 990927/1000000: episode: 9910, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.672, mean reward: 0.587 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.648, 10.284], loss: 0.001496, mae: 0.041534, mean_q: 1.169139
 991027/1000000: episode: 9911, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 61.487, mean reward: 0.615 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.615, 10.188], loss: 0.001539, mae: 0.042751, mean_q: 1.171804
 991127/1000000: episode: 9912, duration: 1.209s, episode steps: 100, steps per second: 83, episode reward: 56.283, mean reward: 0.563 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.693, 10.098], loss: 0.001492, mae: 0.042045, mean_q: 1.173741
 991227/1000000: episode: 9913, duration: 1.219s, episode steps: 100, steps per second: 82, episode reward: 56.831, mean reward: 0.568 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.972, 10.205], loss: 0.001418, mae: 0.041424, mean_q: 1.168209
 991327/1000000: episode: 9914, duration: 1.208s, episode steps: 100, steps per second: 83, episode reward: 59.458, mean reward: 0.595 [0.515, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.334, 10.098], loss: 0.001388, mae: 0.040990, mean_q: 1.167378
 991427/1000000: episode: 9915, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 57.636, mean reward: 0.576 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.346, 10.167], loss: 0.001523, mae: 0.041847, mean_q: 1.168873
 991527/1000000: episode: 9916, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 56.555, mean reward: 0.566 [0.510, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.816, 10.098], loss: 0.001420, mae: 0.041424, mean_q: 1.168091
 991627/1000000: episode: 9917, duration: 1.084s, episode steps: 100, steps per second: 92, episode reward: 60.423, mean reward: 0.604 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.217, 10.296], loss: 0.001317, mae: 0.039827, mean_q: 1.164052
 991727/1000000: episode: 9918, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 57.867, mean reward: 0.579 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.001359, mae: 0.040117, mean_q: 1.165446
 991827/1000000: episode: 9919, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 59.092, mean reward: 0.591 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.010, 10.098], loss: 0.001409, mae: 0.040537, mean_q: 1.164151
 991927/1000000: episode: 9920, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 61.576, mean reward: 0.616 [0.512, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.765, 10.098], loss: 0.001485, mae: 0.041321, mean_q: 1.168365
 992027/1000000: episode: 9921, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 59.497, mean reward: 0.595 [0.511, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.353, 10.098], loss: 0.001367, mae: 0.040486, mean_q: 1.165294
 992127/1000000: episode: 9922, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 63.778, mean reward: 0.638 [0.514, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.110, 10.144], loss: 0.001360, mae: 0.040270, mean_q: 1.165788
 992227/1000000: episode: 9923, duration: 1.039s, episode steps: 100, steps per second: 96, episode reward: 58.171, mean reward: 0.582 [0.502, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.364, 10.106], loss: 0.001317, mae: 0.039581, mean_q: 1.166098
 992327/1000000: episode: 9924, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.675, mean reward: 0.597 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.453, 10.371], loss: 0.001351, mae: 0.040244, mean_q: 1.165766
 992427/1000000: episode: 9925, duration: 1.058s, episode steps: 100, steps per second: 94, episode reward: 57.644, mean reward: 0.576 [0.516, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.805, 10.130], loss: 0.001302, mae: 0.039336, mean_q: 1.163899
 992527/1000000: episode: 9926, duration: 1.221s, episode steps: 100, steps per second: 82, episode reward: 57.946, mean reward: 0.579 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.143], loss: 0.001343, mae: 0.040537, mean_q: 1.167102
 992627/1000000: episode: 9927, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 60.139, mean reward: 0.601 [0.498, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.147, 10.469], loss: 0.001386, mae: 0.040955, mean_q: 1.167346
 992727/1000000: episode: 9928, duration: 1.073s, episode steps: 100, steps per second: 93, episode reward: 59.719, mean reward: 0.597 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.776, 10.348], loss: 0.001235, mae: 0.038473, mean_q: 1.164875
 992827/1000000: episode: 9929, duration: 1.247s, episode steps: 100, steps per second: 80, episode reward: 61.207, mean reward: 0.612 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.248, 10.190], loss: 0.001275, mae: 0.039383, mean_q: 1.168227
 992927/1000000: episode: 9930, duration: 1.250s, episode steps: 100, steps per second: 80, episode reward: 58.450, mean reward: 0.585 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.780, 10.185], loss: 0.001320, mae: 0.039528, mean_q: 1.167808
 993027/1000000: episode: 9931, duration: 1.218s, episode steps: 100, steps per second: 82, episode reward: 58.423, mean reward: 0.584 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.196, 10.098], loss: 0.001342, mae: 0.040593, mean_q: 1.167159
 993127/1000000: episode: 9932, duration: 1.305s, episode steps: 100, steps per second: 77, episode reward: 60.550, mean reward: 0.606 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.507, 10.098], loss: 0.001374, mae: 0.040273, mean_q: 1.170677
 993227/1000000: episode: 9933, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 58.321, mean reward: 0.583 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.081, 10.293], loss: 0.001423, mae: 0.040844, mean_q: 1.167189
 993327/1000000: episode: 9934, duration: 1.217s, episode steps: 100, steps per second: 82, episode reward: 60.249, mean reward: 0.602 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.693, 10.260], loss: 0.001375, mae: 0.040456, mean_q: 1.173638
 993427/1000000: episode: 9935, duration: 1.080s, episode steps: 100, steps per second: 93, episode reward: 59.591, mean reward: 0.596 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.227, 10.098], loss: 0.001327, mae: 0.039962, mean_q: 1.173959
 993527/1000000: episode: 9936, duration: 1.070s, episode steps: 100, steps per second: 93, episode reward: 60.341, mean reward: 0.603 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.914, 10.117], loss: 0.001450, mae: 0.041350, mean_q: 1.172382
 993627/1000000: episode: 9937, duration: 1.240s, episode steps: 100, steps per second: 81, episode reward: 58.661, mean reward: 0.587 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.278], loss: 0.001396, mae: 0.041013, mean_q: 1.174495
 993727/1000000: episode: 9938, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 56.996, mean reward: 0.570 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.265, 10.107], loss: 0.001394, mae: 0.040422, mean_q: 1.174451
 993827/1000000: episode: 9939, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 60.161, mean reward: 0.602 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.370, 10.098], loss: 0.001319, mae: 0.039749, mean_q: 1.172038
 993927/1000000: episode: 9940, duration: 1.058s, episode steps: 100, steps per second: 95, episode reward: 59.132, mean reward: 0.591 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.779, 10.211], loss: 0.001567, mae: 0.043026, mean_q: 1.169001
 994027/1000000: episode: 9941, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 57.049, mean reward: 0.570 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.946, 10.098], loss: 0.001397, mae: 0.040958, mean_q: 1.173263
 994127/1000000: episode: 9942, duration: 1.075s, episode steps: 100, steps per second: 93, episode reward: 59.829, mean reward: 0.598 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.425], loss: 0.001353, mae: 0.040271, mean_q: 1.170973
 994227/1000000: episode: 9943, duration: 1.227s, episode steps: 100, steps per second: 82, episode reward: 60.312, mean reward: 0.603 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.463, 10.098], loss: 0.001511, mae: 0.041582, mean_q: 1.172815
 994327/1000000: episode: 9944, duration: 1.081s, episode steps: 100, steps per second: 93, episode reward: 58.845, mean reward: 0.588 [0.509, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.713, 10.239], loss: 0.001403, mae: 0.040452, mean_q: 1.170909
 994427/1000000: episode: 9945, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 63.122, mean reward: 0.631 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.069, 10.098], loss: 0.001400, mae: 0.040944, mean_q: 1.171351
 994527/1000000: episode: 9946, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 58.878, mean reward: 0.589 [0.502, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.351, 10.098], loss: 0.001485, mae: 0.042263, mean_q: 1.174367
 994627/1000000: episode: 9947, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 58.273, mean reward: 0.583 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.120], loss: 0.001443, mae: 0.041429, mean_q: 1.171600
 994727/1000000: episode: 9948, duration: 1.215s, episode steps: 100, steps per second: 82, episode reward: 57.836, mean reward: 0.578 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.552, 10.308], loss: 0.001528, mae: 0.042038, mean_q: 1.168523
 994827/1000000: episode: 9949, duration: 1.241s, episode steps: 100, steps per second: 81, episode reward: 57.464, mean reward: 0.575 [0.501, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.456, 10.129], loss: 0.001520, mae: 0.042894, mean_q: 1.174286
 994927/1000000: episode: 9950, duration: 1.155s, episode steps: 100, steps per second: 87, episode reward: 57.218, mean reward: 0.572 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.888, 10.098], loss: 0.001342, mae: 0.040033, mean_q: 1.170476
 995027/1000000: episode: 9951, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 58.529, mean reward: 0.585 [0.511, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.759, 10.322], loss: 0.001399, mae: 0.040889, mean_q: 1.168637
 995127/1000000: episode: 9952, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.247, mean reward: 0.592 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.098], loss: 0.001396, mae: 0.040722, mean_q: 1.173916
 995227/1000000: episode: 9953, duration: 1.222s, episode steps: 100, steps per second: 82, episode reward: 59.317, mean reward: 0.593 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.043, 10.364], loss: 0.001531, mae: 0.042612, mean_q: 1.170561
 995327/1000000: episode: 9954, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 59.138, mean reward: 0.591 [0.516, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.126, 10.210], loss: 0.001316, mae: 0.039650, mean_q: 1.169254
 995427/1000000: episode: 9955, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 59.293, mean reward: 0.593 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.941, 10.098], loss: 0.001396, mae: 0.041110, mean_q: 1.171323
 995527/1000000: episode: 9956, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 61.408, mean reward: 0.614 [0.500, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.102], loss: 0.001410, mae: 0.041299, mean_q: 1.174733
 995627/1000000: episode: 9957, duration: 1.231s, episode steps: 100, steps per second: 81, episode reward: 59.789, mean reward: 0.598 [0.500, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.489, 10.233], loss: 0.001317, mae: 0.039433, mean_q: 1.171030
 995727/1000000: episode: 9958, duration: 1.069s, episode steps: 100, steps per second: 94, episode reward: 58.308, mean reward: 0.583 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.788, 10.098], loss: 0.001378, mae: 0.040865, mean_q: 1.168225
 995827/1000000: episode: 9959, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 61.002, mean reward: 0.610 [0.512, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.352, 10.279], loss: 0.001411, mae: 0.040999, mean_q: 1.168729
 995927/1000000: episode: 9960, duration: 1.201s, episode steps: 100, steps per second: 83, episode reward: 60.220, mean reward: 0.602 [0.511, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.338], loss: 0.001402, mae: 0.041065, mean_q: 1.169554
 996027/1000000: episode: 9961, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 58.263, mean reward: 0.583 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.126], loss: 0.001424, mae: 0.040670, mean_q: 1.172278
 996127/1000000: episode: 9962, duration: 1.133s, episode steps: 100, steps per second: 88, episode reward: 57.910, mean reward: 0.579 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.954, 10.141], loss: 0.001433, mae: 0.041147, mean_q: 1.174113
 996227/1000000: episode: 9963, duration: 1.078s, episode steps: 100, steps per second: 93, episode reward: 57.517, mean reward: 0.575 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.410, 10.205], loss: 0.001343, mae: 0.040581, mean_q: 1.172684
 996327/1000000: episode: 9964, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 58.193, mean reward: 0.582 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.371, 10.126], loss: 0.001429, mae: 0.041150, mean_q: 1.172608
 996427/1000000: episode: 9965, duration: 1.068s, episode steps: 100, steps per second: 94, episode reward: 59.354, mean reward: 0.594 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.575, 10.098], loss: 0.001388, mae: 0.041098, mean_q: 1.171128
 996527/1000000: episode: 9966, duration: 1.048s, episode steps: 100, steps per second: 95, episode reward: 59.735, mean reward: 0.597 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.098], loss: 0.001406, mae: 0.041045, mean_q: 1.174682
 996627/1000000: episode: 9967, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 57.622, mean reward: 0.576 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.446, 10.349], loss: 0.001465, mae: 0.041835, mean_q: 1.174142
 996727/1000000: episode: 9968, duration: 1.079s, episode steps: 100, steps per second: 93, episode reward: 60.234, mean reward: 0.602 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.020, 10.224], loss: 0.001359, mae: 0.039952, mean_q: 1.175100
 996827/1000000: episode: 9969, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 59.688, mean reward: 0.597 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.320], loss: 0.001434, mae: 0.040992, mean_q: 1.174239
 996927/1000000: episode: 9970, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 58.930, mean reward: 0.589 [0.523, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.569, 10.387], loss: 0.001461, mae: 0.041155, mean_q: 1.172857
 997027/1000000: episode: 9971, duration: 1.124s, episode steps: 100, steps per second: 89, episode reward: 56.882, mean reward: 0.569 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.501, 10.251], loss: 0.001437, mae: 0.041072, mean_q: 1.174932
 997127/1000000: episode: 9972, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 59.438, mean reward: 0.594 [0.500, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.695, 10.204], loss: 0.001403, mae: 0.040987, mean_q: 1.167339
 997227/1000000: episode: 9973, duration: 1.142s, episode steps: 100, steps per second: 88, episode reward: 56.672, mean reward: 0.567 [0.509, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.565, 10.098], loss: 0.001408, mae: 0.040607, mean_q: 1.169887
 997327/1000000: episode: 9974, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 59.659, mean reward: 0.597 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.730, 10.275], loss: 0.001334, mae: 0.040103, mean_q: 1.164872
 997427/1000000: episode: 9975, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 57.102, mean reward: 0.571 [0.504, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.169, 10.098], loss: 0.001472, mae: 0.042216, mean_q: 1.169587
 997527/1000000: episode: 9976, duration: 1.236s, episode steps: 100, steps per second: 81, episode reward: 60.545, mean reward: 0.605 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.130], loss: 0.001345, mae: 0.039878, mean_q: 1.164396
 997627/1000000: episode: 9977, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 58.763, mean reward: 0.588 [0.511, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.057, 10.098], loss: 0.001468, mae: 0.041374, mean_q: 1.169053
 997727/1000000: episode: 9978, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 58.307, mean reward: 0.583 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.797, 10.109], loss: 0.001433, mae: 0.041036, mean_q: 1.168398
 997827/1000000: episode: 9979, duration: 1.220s, episode steps: 100, steps per second: 82, episode reward: 59.922, mean reward: 0.599 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.847, 10.115], loss: 0.001377, mae: 0.040733, mean_q: 1.168445
 997927/1000000: episode: 9980, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 57.476, mean reward: 0.575 [0.499, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.756, 10.098], loss: 0.001388, mae: 0.040269, mean_q: 1.166521
 998027/1000000: episode: 9981, duration: 1.228s, episode steps: 100, steps per second: 81, episode reward: 58.626, mean reward: 0.586 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.351, 10.098], loss: 0.001534, mae: 0.042164, mean_q: 1.169122
 998127/1000000: episode: 9982, duration: 1.067s, episode steps: 100, steps per second: 94, episode reward: 59.151, mean reward: 0.592 [0.511, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.682, 10.098], loss: 0.001395, mae: 0.040114, mean_q: 1.168697
 998227/1000000: episode: 9983, duration: 1.063s, episode steps: 100, steps per second: 94, episode reward: 61.414, mean reward: 0.614 [0.521, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.973, 10.098], loss: 0.001393, mae: 0.040097, mean_q: 1.166441
 998327/1000000: episode: 9984, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 57.765, mean reward: 0.578 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.677, 10.098], loss: 0.001372, mae: 0.039922, mean_q: 1.167927
 998427/1000000: episode: 9985, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 60.917, mean reward: 0.609 [0.510, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.596, 10.317], loss: 0.001456, mae: 0.041416, mean_q: 1.167163
 998527/1000000: episode: 9986, duration: 1.065s, episode steps: 100, steps per second: 94, episode reward: 60.860, mean reward: 0.609 [0.505, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.991, 10.115], loss: 0.001466, mae: 0.041577, mean_q: 1.168486
 998627/1000000: episode: 9987, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 58.615, mean reward: 0.586 [0.498, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.098], loss: 0.001394, mae: 0.040615, mean_q: 1.170769
 998727/1000000: episode: 9988, duration: 1.076s, episode steps: 100, steps per second: 93, episode reward: 56.524, mean reward: 0.565 [0.504, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.643, 10.163], loss: 0.001511, mae: 0.042624, mean_q: 1.170171
 998827/1000000: episode: 9989, duration: 1.229s, episode steps: 100, steps per second: 81, episode reward: 60.361, mean reward: 0.604 [0.515, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.802, 10.098], loss: 0.001383, mae: 0.040841, mean_q: 1.168551
 998927/1000000: episode: 9990, duration: 1.064s, episode steps: 100, steps per second: 94, episode reward: 57.417, mean reward: 0.574 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.416, 10.214], loss: 0.001442, mae: 0.041202, mean_q: 1.168373
 999027/1000000: episode: 9991, duration: 1.270s, episode steps: 100, steps per second: 79, episode reward: 58.919, mean reward: 0.589 [0.514, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.294, 10.098], loss: 0.001389, mae: 0.040667, mean_q: 1.168263
 999127/1000000: episode: 9992, duration: 1.047s, episode steps: 100, steps per second: 96, episode reward: 58.112, mean reward: 0.581 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.384, 10.098], loss: 0.001436, mae: 0.040656, mean_q: 1.170180
 999227/1000000: episode: 9993, duration: 1.266s, episode steps: 100, steps per second: 79, episode reward: 61.282, mean reward: 0.613 [0.503, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.047, 10.098], loss: 0.001405, mae: 0.040579, mean_q: 1.171018
 999327/1000000: episode: 9994, duration: 1.249s, episode steps: 100, steps per second: 80, episode reward: 59.048, mean reward: 0.590 [0.514, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.984, 10.098], loss: 0.001426, mae: 0.041296, mean_q: 1.169495
 999427/1000000: episode: 9995, duration: 1.258s, episode steps: 100, steps per second: 79, episode reward: 57.464, mean reward: 0.575 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.924, 10.107], loss: 0.001413, mae: 0.040708, mean_q: 1.170845
 999527/1000000: episode: 9996, duration: 1.086s, episode steps: 100, steps per second: 92, episode reward: 56.933, mean reward: 0.569 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.036, 10.098], loss: 0.001413, mae: 0.041084, mean_q: 1.165963
 999627/1000000: episode: 9997, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 57.913, mean reward: 0.579 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.763, 10.098], loss: 0.001411, mae: 0.040587, mean_q: 1.166057
 999727/1000000: episode: 9998, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 57.150, mean reward: 0.571 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.321, 10.160], loss: 0.001428, mae: 0.041077, mean_q: 1.164215
 999827/1000000: episode: 9999, duration: 1.072s, episode steps: 100, steps per second: 93, episode reward: 60.461, mean reward: 0.605 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.708, 10.098], loss: 0.001307, mae: 0.039432, mean_q: 1.161564
 999927/1000000: episode: 10000, duration: 1.271s, episode steps: 100, steps per second: 79, episode reward: 66.476, mean reward: 0.665 [0.504, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.505, 10.240], loss: 0.001467, mae: 0.041515, mean_q: 1.166443
done, took 10089.869 seconds
[Info] End Uniform Random Simulation. Falsification occurred 0 times.
